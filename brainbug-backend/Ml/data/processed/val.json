[
  {
    "code": "def read_parquet(\\n    path,\\n    engine: str = \"auto\",\\n    columns=None,\\n    use_nullable_dtypes: bool = False,\\n    **kwargs,\\n):\\n    impl = get_engine(engine)\\n    return impl.read(\\n        path, columns=columns, use_nullable_dtypes=use_nullable_dtypes, **kwargs\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_parquet(\\n    path,\\n    engine: str = \"auto\",\\n    columns=None,\\n    use_nullable_dtypes: bool = False,\\n    **kwargs,\\n):\\n    impl = get_engine(engine)\\n    return impl.read(\\n        path, columns=columns, use_nullable_dtypes=use_nullable_dtypes, **kwargs\\n    )"
  },
  {
    "code": "def as_string(self, unixfrom=False, maxheaderlen=0):\\n        from email.generator import Generator\\n        fp = StringIO()\\n        g = Generator(fp, mangle_from_=False, maxheaderlen=maxheaderlen)\\n        g.flatten(self, unixfrom=unixfrom)\\n        return fp.getvalue()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#18600: add policy to add_string, and as_bytes and __bytes__ methods.\\n\\nThis was triggered by wanting to make the doctest in email.policy.rst pass;\\nas_bytes and __bytes__ are clearly useful now that we have BytesGenerator.\\nAlso updated the Message docs to document the policy keyword that was\\nadded in 3.3.",
    "fixed_code": "def as_string(self, unixfrom=False, maxheaderlen=0, policy=None):\\n        from email.generator import Generator\\n        policy = self.policy if policy is None else policy\\n        fp = StringIO()\\n        g = Generator(fp,\\n                      mangle_from_=False,\\n                      maxheaderlen=maxheaderlen,\\n                      policy=policy)\\n        g.flatten(self, unixfrom=unixfrom)\\n        return fp.getvalue()"
  },
  {
    "code": "def _get_valid_mysql_name(name):\\n    uname = _get_unicode_name(name)\\n    if not len(uname):\\n        raise ValueError(\"Empty table or column name specified\")\\n    basere = r'[0-9,a-z,A-Z$_]'\\n    for c in uname:\\n        if not re.match(basere, c):\\n            if not (0x80 < ord(c) < 0xFFFF):\\n                raise ValueError(\"Invalid MySQL identifier '%s'\" % uname)\\n    return '`' + uname + '`'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_valid_mysql_name(name):\\n    uname = _get_unicode_name(name)\\n    if not len(uname):\\n        raise ValueError(\"Empty table or column name specified\")\\n    basere = r'[0-9,a-z,A-Z$_]'\\n    for c in uname:\\n        if not re.match(basere, c):\\n            if not (0x80 < ord(c) < 0xFFFF):\\n                raise ValueError(\"Invalid MySQL identifier '%s'\" % uname)\\n    return '`' + uname + '`'"
  },
  {
    "code": "def format_file_in_place(\\n\\tsrc: Path,\\n\\tline_length: int,\\n\\tfast: bool,\\n\\twrite_back: WriteBack = WriteBack.NO,\\n\\tmode: FileMode = FileMode.AUTO_DETECT,\\n\\tlock: Any = None,  \\n) -> bool:\\n\\tif src.suffix == \".pyi\":\\n\\t\\tmode |= FileMode.PYI\\n\\twith open(src, \"rb\") as buf:\\n\\t\\tnewline, encoding, src_contents = prepare_input(buf.read())\\n\\ttry:\\n\\t\\tdst_contents = format_file_contents(\\n\\t\\t\\tsrc_contents, line_length=line_length, fast=fast, mode=mode\\n\\t\\t)\\n\\texcept NothingChanged:\\n\\t\\treturn False\\n\\tif write_back == write_back.YES:\\n\\t\\twith open(src, \"w\", encoding=encoding, newline=newline) as f:\\n\\t\\t\\tf.write(dst_contents)\\n\\telif write_back == write_back.DIFF:\\n\\t\\tsrc_name = f\"{src}  (original)\"\\n\\t\\tdst_name = f\"{src}  (formatted)\"\\n\\t\\tdiff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n\\t\\tif lock:\\n\\t\\t\\tlock.acquire()\\n\\t\\ttry:\\n\\t\\t\\tf = io.TextIOWrapper(\\n\\t\\t\\t\\tsys.stdout.buffer,\\n\\t\\t\\t\\tencoding=encoding,\\n\\t\\t\\t\\tnewline=newline,\\n\\t\\t\\t\\twrite_through=True,\\n\\t\\t\\t)\\n\\t\\t\\tf.write(diff_contents)\\n\\t\\t\\tf.detach()\\n\\t\\tfinally:\\n\\t\\t\\tif lock:\\n\\t\\t\\t\\tlock.release()\\n\\treturn True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_file_in_place(\\n\\tsrc: Path,\\n\\tline_length: int,\\n\\tfast: bool,\\n\\twrite_back: WriteBack = WriteBack.NO,\\n\\tmode: FileMode = FileMode.AUTO_DETECT,\\n\\tlock: Any = None,  \\n) -> bool:\\n\\tif src.suffix == \".pyi\":\\n\\t\\tmode |= FileMode.PYI\\n\\twith open(src, \"rb\") as buf:\\n\\t\\tnewline, encoding, src_contents = prepare_input(buf.read())\\n\\ttry:\\n\\t\\tdst_contents = format_file_contents(\\n\\t\\t\\tsrc_contents, line_length=line_length, fast=fast, mode=mode\\n\\t\\t)\\n\\texcept NothingChanged:\\n\\t\\treturn False\\n\\tif write_back == write_back.YES:\\n\\t\\twith open(src, \"w\", encoding=encoding, newline=newline) as f:\\n\\t\\t\\tf.write(dst_contents)\\n\\telif write_back == write_back.DIFF:\\n\\t\\tsrc_name = f\"{src}  (original)\"\\n\\t\\tdst_name = f\"{src}  (formatted)\"\\n\\t\\tdiff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n\\t\\tif lock:\\n\\t\\t\\tlock.acquire()\\n\\t\\ttry:\\n\\t\\t\\tf = io.TextIOWrapper(\\n\\t\\t\\t\\tsys.stdout.buffer,\\n\\t\\t\\t\\tencoding=encoding,\\n\\t\\t\\t\\tnewline=newline,\\n\\t\\t\\t\\twrite_through=True,\\n\\t\\t\\t)\\n\\t\\t\\tf.write(diff_contents)\\n\\t\\t\\tf.detach()\\n\\t\\tfinally:\\n\\t\\t\\tif lock:\\n\\t\\t\\t\\tlock.release()\\n\\treturn True"
  },
  {
    "code": "def eb_wrapper(failure):\\n\\t\\t\\tcase = _create_testcase(method, 'errback')\\n\\t\\t\\texc_info = failure.value, failure.type, failure.getTracebackObject()\\n\\t\\t\\tresults.addError(case, exc_info)\\n\\t\\trequest.callback = cb_wrapper\\n\\t\\trequest.errback = eb_wrapper",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix contract errback",
    "fixed_code": "def eb_wrapper(failure):\\n\\t\\t\\tcase = _create_testcase(method, 'errback')\\n\\t\\t\\texc_info = failure.type, failure.value, failure.getTracebackObject()\\n\\t\\t\\tresults.addError(case, exc_info)\\n\\t\\trequest.callback = cb_wrapper\\n\\t\\trequest.errback = eb_wrapper"
  },
  {
    "code": "def Time2Internaldate(date_time):\\n    if isinstance(date_time, (int, float)):\\n        tt = time.localtime(date_time)\\n    elif isinstance(date_time, (tuple, time.struct_time)):\\n        tt = date_time\\n    elif isinstance(date_time, str) and (date_time[0],date_time[-1]) == ('\"','\"'):\\n        return date_time        \\n    else:\\n        raise ValueError(\"date_time not of a known type\")\\n    dt = time.strftime(\"%d-%b-%Y %H:%M:%S\", tt)\\n    if dt[0] == '0':\\n        dt = ' ' + dt[1:]\\n    if time.daylight and tt[-1]:\\n        zone = -time.altzone\\n    else:\\n        zone = -time.timezone\\n    return '\"' + dt + \" %+03d%02d\" % divmod(zone//60, 60) + '\"'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issues #11024: Fixes and additional tests for Time2Internaldate.",
    "fixed_code": "def Time2Internaldate(date_time):\\n    if isinstance(date_time, (int, float)):\\n        dt = datetime.fromtimestamp(date_time,\\n                                    timezone.utc).astimezone()\\n    elif isinstance(date_time, tuple):\\n        try:\\n            gmtoff = date_time.tm_gmtoff\\n        except AttributeError:\\n            if time.daylight:\\n                dst = date_time[8]\\n                if dst == -1:\\n                    dst = time.localtime(time.mktime(date_time))[8]\\n                gmtoff = -(time.timezone, time.altzone)[dst]\\n            else:\\n                gmtoff = -time.timezone\\n        delta = timedelta(seconds=gmtoff)\\n        dt = datetime(*date_time[:6], tzinfo=timezone(delta))\\n    elif isinstance(date_time, datetime):\\n        if date_time.tzinfo is None:\\n            raise ValueError(\"date_time must be aware\")\\n        dt = date_time\\n    elif isinstance(date_time, str) and (date_time[0],date_time[-1]) == ('\"','\"'):\\n        return date_time        \\n    else:\\n        raise ValueError(\"date_time not of a known type\")\\n    fmt = '\"%d-{}-%Y %H:%M:%S %z\"'.format(Months[dt.month])\\n    return dt.strftime(fmt)"
  },
  {
    "code": "def _get_groupings(obj, grouper=None, axis=0, level=None):\\n    group_axis = obj._get_axis(axis)\\n    if level is not None and not isinstance(group_axis, MultiIndex):\\n        raise ValueError('can only specify level with multi-level index')\\n    groupings = []\\n    exclusions = []\\n    if isinstance(grouper, (tuple, list)):\\n        if axis != 0:\\n            raise ValueError('multi-grouping only valid with axis=0 for now')\\n        for i, arg in enumerate(grouper):\\n            name = 'key_%d' % i\\n            if isinstance(arg, basestring):\\n                exclusions.append(arg)\\n                name = arg\\n                arg = obj[arg]\\n            ping = Grouping(group_axis, arg, name=name, level=level)\\n            groupings.append(ping)\\n    else:\\n        name = 'key'\\n        if isinstance(grouper, basestring):\\n            exclusions.append(grouper)\\n            name = grouper\\n            grouper = obj[grouper]\\n        ping = Grouping(group_axis, grouper, name=name, level=level)\\n        groupings.append(ping)\\n    return groupings, exclusions",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: multi-grouping with axis > 0",
    "fixed_code": "def _get_groupings(obj, grouper=None, axis=0, level=None):\\n    group_axis = obj._get_axis(axis)\\n    if level is not None and not isinstance(group_axis, MultiIndex):\\n        raise ValueError('can only specify level with multi-level index')\\n    groupings = []\\n    exclusions = []\\n    if isinstance(grouper, (tuple, list)):\\n        for i, arg in enumerate(grouper):\\n            name = 'key_%d' % i\\n            if isinstance(arg, basestring):\\n                exclusions.append(arg)\\n                name = arg\\n                arg = obj[arg]\\n            ping = Grouping(group_axis, arg, name=name, level=level)\\n            groupings.append(ping)\\n    else:\\n        name = 'key'\\n        if isinstance(grouper, basestring):\\n            exclusions.append(grouper)\\n            name = grouper\\n            grouper = obj[grouper]\\n        ping = Grouping(group_axis, grouper, name=name, level=level)\\n        groupings.append(ping)\\n    return groupings, exclusions"
  },
  {
    "code": "def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\\n            self.depth -= 1\\n            self._for_loop_variable -= 1\\n            return True\\n        return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix bracket match bug (#470)",
    "fixed_code": "def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\\n        if (\\n            self._for_loop_depths\\n            and self._for_loop_depths[-1] == self.depth\\n            and leaf.type == token.NAME\\n            and leaf.value == \"in\"\\n        ):\\n            self.depth -= 1\\n            self._for_loop_depths.pop()\\n            return True\\n        return False"
  },
  {
    "code": "def fit(self, X, y=None):\\n        X = check_array(X)\\n        if self.assume_centered:\\n            self.location_ = np.zeros(X.shape[1])\\n        else:\\n            self.location_ = X.mean(0)\\n        emp_cov = empirical_covariance(\\n            X, assume_centered=self.assume_centered)\\n        self.covariance_, self.precision_, self.n_iter_ = graph_lasso(\\n            emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,\\n            max_iter=self.max_iter, verbose=self.verbose,\\n            return_n_iter=True)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, y=None):\\n        X = check_array(X)\\n        if self.assume_centered:\\n            self.location_ = np.zeros(X.shape[1])\\n        else:\\n            self.location_ = X.mean(0)\\n        emp_cov = empirical_covariance(\\n            X, assume_centered=self.assume_centered)\\n        self.covariance_, self.precision_, self.n_iter_ = graph_lasso(\\n            emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,\\n            max_iter=self.max_iter, verbose=self.verbose,\\n            return_n_iter=True)\\n        return self"
  },
  {
    "code": "def _get_label(self, label, axis=0):\\n        if (isinstance(label, tuple) and\\n                isinstance(label[axis], slice)):\\n            raise IndexingError('no slices here')\\n        try:\\n            return self.obj._xs(label, axis=axis, copy=False)\\n        except Exception:\\n            return self.obj._xs(label, axis=axis, copy=True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Regression in Series with a multi-index via ix (GH6018) CLN: Remove references to _SeriesIndexer (vestigial)",
    "fixed_code": "def _get_label(self, label, axis=0):\\n        if self.ndim == 1:\\n            return self.obj[label]\\n        elif (isinstance(label, tuple) and\\n                isinstance(label[axis], slice)):\\n            raise IndexingError('no slices here')\\n        try:\\n            return self.obj._xs(label, axis=axis, copy=False)\\n        except Exception:\\n            return self.obj._xs(label, axis=axis, copy=True)"
  },
  {
    "code": "def prefetch_related_objects(model_instances, *related_lookups):\\n    if not model_instances:\\n        return  \\n    done_queries = {}    \\n    auto_lookups = set()  \\n    followed_descriptors = set()  \\n    all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\\n    while all_lookups:\\n        lookup = all_lookups.pop()\\n        if lookup.prefetch_to in done_queries:\\n            if lookup.queryset:\\n                raise ValueError(\"'%s' lookup was already seen with a different queryset. \"\\n                                 \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\\n            continue\\n        obj_list = model_instances\\n        through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\\n        for level, through_attr in enumerate(through_attrs):\\n            if not obj_list:\\n                break\\n            prefetch_to = lookup.get_current_prefetch_to(level)\\n            if prefetch_to in done_queries:\\n                obj_list = done_queries[prefetch_to]\\n                continue\\n            good_objects = True\\n            for obj in obj_list:\\n                if not hasattr(obj, '_prefetched_objects_cache'):\\n                    try:\\n                        obj._prefetched_objects_cache = {}\\n                    except (AttributeError, TypeError):\\n                        good_objects = False\\n                        break\\n            if not good_objects:\\n                break\\n            first_obj = obj_list[0]\\n            to_attr = lookup.get_current_to_attr(level)[0]\\n            prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)\\n            if not attr_found:\\n                raise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\\n                                     \"parameter to prefetch_related()\" %\\n                                     (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\\n            if level == len(through_attrs) - 1 and prefetcher is None:\\n                raise ValueError(\"'%s' does not resolve to an item that supports \"\\n                                 \"prefetching - this is an invalid parameter to \"\\n                                 \"prefetch_related().\" % lookup.prefetch_through)\\n            if prefetcher is not None and not is_fetched:\\n                obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)\\n                if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):\\n                    done_queries[prefetch_to] = obj_list\\n                    new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)\\n                    auto_lookups.update(new_lookups)\\n                    all_lookups.extend(new_lookups)\\n                followed_descriptors.add(descriptor)\\n            else:\\n                new_obj_list = []\\n                for obj in obj_list:\\n                    if through_attr in getattr(obj, '_prefetched_objects_cache', ()):\\n                        new_obj = list(obj._prefetched_objects_cache.get(through_attr))\\n                    else:\\n                        try:\\n                            new_obj = getattr(obj, through_attr)\\n                        except exceptions.ObjectDoesNotExist:\\n                            continue\\n                    if new_obj is None:\\n                        continue\\n                    if isinstance(new_obj, list):\\n                        new_obj_list.extend(new_obj)\\n                    else:\\n                        new_obj_list.append(new_obj)\\n                obj_list = new_obj_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def prefetch_related_objects(model_instances, *related_lookups):\\n    if not model_instances:\\n        return  \\n    done_queries = {}    \\n    auto_lookups = set()  \\n    followed_descriptors = set()  \\n    all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\\n    while all_lookups:\\n        lookup = all_lookups.pop()\\n        if lookup.prefetch_to in done_queries:\\n            if lookup.queryset:\\n                raise ValueError(\"'%s' lookup was already seen with a different queryset. \"\\n                                 \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\\n            continue\\n        obj_list = model_instances\\n        through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\\n        for level, through_attr in enumerate(through_attrs):\\n            if not obj_list:\\n                break\\n            prefetch_to = lookup.get_current_prefetch_to(level)\\n            if prefetch_to in done_queries:\\n                obj_list = done_queries[prefetch_to]\\n                continue\\n            good_objects = True\\n            for obj in obj_list:\\n                if not hasattr(obj, '_prefetched_objects_cache'):\\n                    try:\\n                        obj._prefetched_objects_cache = {}\\n                    except (AttributeError, TypeError):\\n                        good_objects = False\\n                        break\\n            if not good_objects:\\n                break\\n            first_obj = obj_list[0]\\n            to_attr = lookup.get_current_to_attr(level)[0]\\n            prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)\\n            if not attr_found:\\n                raise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\\n                                     \"parameter to prefetch_related()\" %\\n                                     (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\\n            if level == len(through_attrs) - 1 and prefetcher is None:\\n                raise ValueError(\"'%s' does not resolve to an item that supports \"\\n                                 \"prefetching - this is an invalid parameter to \"\\n                                 \"prefetch_related().\" % lookup.prefetch_through)\\n            if prefetcher is not None and not is_fetched:\\n                obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)\\n                if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):\\n                    done_queries[prefetch_to] = obj_list\\n                    new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)\\n                    auto_lookups.update(new_lookups)\\n                    all_lookups.extend(new_lookups)\\n                followed_descriptors.add(descriptor)\\n            else:\\n                new_obj_list = []\\n                for obj in obj_list:\\n                    if through_attr in getattr(obj, '_prefetched_objects_cache', ()):\\n                        new_obj = list(obj._prefetched_objects_cache.get(through_attr))\\n                    else:\\n                        try:\\n                            new_obj = getattr(obj, through_attr)\\n                        except exceptions.ObjectDoesNotExist:\\n                            continue\\n                    if new_obj is None:\\n                        continue\\n                    if isinstance(new_obj, list):\\n                        new_obj_list.extend(new_obj)\\n                    else:\\n                        new_obj_list.append(new_obj)\\n                obj_list = new_obj_list"
  },
  {
    "code": "def _fit_transform(self, X, y, func, fitted=False,\\n                       column_as_strings=False):\\n        transformers = list(\\n            self._iter(\\n                fitted=fitted, replace_strings=True,\\n                column_as_strings=column_as_strings))\\n        try:\\n            return Parallel(n_jobs=self.n_jobs)(\\n                delayed(func)(\\n                    transformer=clone(trans) if not fitted else trans,\\n                    X=_safe_indexing(X, column, axis=1),\\n                    y=y,\\n                    weight=weight,\\n                    message_clsname='ColumnTransformer',\\n                    message=self._log_message(name, idx, len(transformers)))\\n                for idx, (name, trans, column, weight) in enumerate(\\n                    transformers, 1))\\n        except ValueError as e:\\n            if \"Expected 2D array, got 1D array instead\" in str(e):\\n                raise ValueError(_ERR_MSG_1DCOLUMN) from e\\n            else:\\n                raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fit_transform(self, X, y, func, fitted=False,\\n                       column_as_strings=False):\\n        transformers = list(\\n            self._iter(\\n                fitted=fitted, replace_strings=True,\\n                column_as_strings=column_as_strings))\\n        try:\\n            return Parallel(n_jobs=self.n_jobs)(\\n                delayed(func)(\\n                    transformer=clone(trans) if not fitted else trans,\\n                    X=_safe_indexing(X, column, axis=1),\\n                    y=y,\\n                    weight=weight,\\n                    message_clsname='ColumnTransformer',\\n                    message=self._log_message(name, idx, len(transformers)))\\n                for idx, (name, trans, column, weight) in enumerate(\\n                    transformers, 1))\\n        except ValueError as e:\\n            if \"Expected 2D array, got 1D array instead\" in str(e):\\n                raise ValueError(_ERR_MSG_1DCOLUMN) from e\\n            else:\\n                raise"
  },
  {
    "code": "def platform(aliased=0, terse=0):\\n    result = _platform_cache.get((aliased, terse), None)\\n    if result is not None:\\n        return result\\n    system, node, release, version, machine, processor = uname()\\n    if machine == processor:\\n        processor = ''\\n    if aliased:\\n        system, release, version = system_alias(system, release, version)\\n    if system == 'Windows':\\n        rel, vers, csd, ptype = win32_ver(version)\\n        if terse:\\n            platform = _platform(system, release)\\n        else:\\n            platform = _platform(system, release, version, csd)\\n    elif system in ('Linux',):\\n        libcname, libcversion = libc_ver(sys.executable)\\n        platform = _platform(system, release, machine, processor,\\n                             'with',\\n                             libcname+libcversion)\\n    elif system == 'Java':\\n        r, v, vminfo, (os_name, os_version, os_arch) = java_ver()\\n        if terse or not os_name:\\n            platform = _platform(system, release, version)\\n        else:\\n            platform = _platform(system, release, version,\\n                                 'on',\\n                                 os_name, os_version, os_arch)\\n    elif system == 'MacOS':\\n        if terse:\\n            platform = _platform(system, release)\\n        else:\\n            platform = _platform(system, release, machine)\\n    else:\\n        if terse:\\n            platform = _platform(system, release)\\n        else:\\n            bits, linkage = architecture(sys.executable)\\n            platform = _platform(system, release, machine,\\n                                 processor, bits, linkage)\\n    _platform_cache[(aliased, terse)] = platform\\n    return platform\\nif __name__ == '__main__':\\n    terse = ('terse' in sys.argv or '--terse' in sys.argv)\\n    aliased = (not 'nonaliased' in sys.argv and not '--nonaliased' in sys.argv)\\n    print(platform(aliased, terse))\\n    sys.exit(0)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-35344: platform.platform() uses mac_ver() on macOS (GH-10780)\\n\\nOn macOS, platform.platform() now uses mac_ver(), if it returns a\\nnon-empty release string, to get the macOS version rather than darwin\\nversion.",
    "fixed_code": "def platform(aliased=0, terse=0):\\n    result = _platform_cache.get((aliased, terse), None)\\n    if result is not None:\\n        return result\\n    system, node, release, version, machine, processor = uname()\\n    if machine == processor:\\n        processor = ''\\n    if aliased:\\n        system, release, version = system_alias(system, release, version)\\n    if system == 'Darwin':\\n        macos_release = mac_ver()[0]\\n        if macos_release:\\n            system = 'macOS'\\n            release = macos_release\\n    if system == 'Windows':\\n        rel, vers, csd, ptype = win32_ver(version)\\n        if terse:\\n            platform = _platform(system, release)\\n        else:\\n            platform = _platform(system, release, version, csd)\\n    elif system in ('Linux',):\\n        libcname, libcversion = libc_ver(sys.executable)\\n        platform = _platform(system, release, machine, processor,\\n                             'with',\\n                             libcname+libcversion)\\n    elif system == 'Java':\\n        r, v, vminfo, (os_name, os_version, os_arch) = java_ver()\\n        if terse or not os_name:\\n            platform = _platform(system, release, version)\\n        else:\\n            platform = _platform(system, release, version,\\n                                 'on',\\n                                 os_name, os_version, os_arch)\\n    elif system == 'MacOS':\\n        if terse:\\n            platform = _platform(system, release)\\n        else:\\n            platform = _platform(system, release, machine)\\n    else:\\n        if terse:\\n            platform = _platform(system, release)\\n        else:\\n            bits, linkage = architecture(sys.executable)\\n            platform = _platform(system, release, machine,\\n                                 processor, bits, linkage)\\n    _platform_cache[(aliased, terse)] = platform\\n    return platform\\nif __name__ == '__main__':\\n    terse = ('terse' in sys.argv or '--terse' in sys.argv)\\n    aliased = (not 'nonaliased' in sys.argv and not '--nonaliased' in sys.argv)\\n    print(platform(aliased, terse))\\n    sys.exit(0)"
  },
  {
    "code": "def kneighbors(self, X, n_neighbors=None, return_distance=False):\\n        if X is None:\\n            raise ValueError(\"X cannot be None.\")\\n        if n_neighbors is not None:\\n            self.n_neighbors = n_neighbors\\n        X = safe_asarray(X)\\n        x_dim = X.ndim\\n        if x_dim == 1:\\n            neighbors, distances = self._query(X, self.n_neighbors)\\n            if return_distance:\\n                return np.array([neighbors]), np.array([distances])\\n            else:\\n                return np.array([neighbors])\\n        else:\\n            neighbors, distances = [], []\\n            for i in range(X.shape[0]):\\n                neighs, dists = self._query(X[i], self.n_neighbors)\\n                neighbors.append(neighs)\\n                distances.append(dists)\\n            if return_distance:\\n                return np.array(neighbors), np.array(distances)\\n            else:\\n                return np.array(neighbors)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Added radius_neighbors method to LSHForest.",
    "fixed_code": "def kneighbors(self, X, n_neighbors=None, return_distance=False):\\n        if not hasattr(self, 'hash_functions_'):\\n            raise ValueError(\"estimator should be fitted.\")\\n        if X is None:\\n            raise ValueError(\"X cannot be None.\")\\n        if n_neighbors is not None:\\n            self.n_neighbors = n_neighbors\\n        X = safe_asarray(X)\\n        x_dim = X.ndim\\n        if x_dim == 1:\\n            neighbors, distances = self._query(X, self.n_neighbors)\\n            if return_distance:\\n                return np.array([neighbors]), np.array([distances])\\n            else:\\n                return np.array([neighbors])\\n        else:\\n            neighbors, distances = [], []\\n            for i in range(X.shape[0]):\\n                neighs, dists = self._query(X[i], self.n_neighbors)\\n                neighbors.append(neighs)\\n                distances.append(dists)\\n            if return_distance:\\n                return np.array(neighbors), np.array(distances)\\n            else:\\n                return np.array(neighbors)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        feature=dict(type='str', required=True),\\n        state=dict(choices=['enabled', 'disabled'], default='enabled')\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\\n    warnings = list()\\n    results = dict(changed=False, warnings=warnings)\\n    feature = validate_feature(module)\\n    state = module.params['state'].lower()\\n    available_features = get_available_features(feature, module)\\n    if feature not in available_features:\\n        module.fail_json(\\n            msg='Invalid feature name.',\\n            features_currently_supported=available_features,\\n            invalid_feature=feature)\\n    else:\\n        existstate = available_features[feature]\\n        existing = dict(state=existstate)\\n        proposed = dict(state=state)\\n        results['changed'] = False\\n        cmds = get_commands(proposed, existing, state, module)\\n        if cmds:\\n            if not module.check_mode:\\n                load_config(module, cmds)\\n            results['changed'] = True\\n    results['commands'] = cmds\\n    module.exit_json(**results)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes for the N3500 platform that uses the A8 image (#36261)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        feature=dict(type='str', required=True),\\n        state=dict(choices=['enabled', 'disabled'], default='enabled')\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\\n    warnings = list()\\n    results = dict(changed=False, warnings=warnings)\\n    feature = validate_feature(module)\\n    state = module.params['state'].lower()\\n    available_features = get_available_features(feature, module)\\n    if feature not in available_features:\\n        module.fail_json(\\n            msg='Invalid feature name.',\\n            features_currently_supported=available_features,\\n            invalid_feature=feature)\\n    else:\\n        existstate = available_features[feature]\\n        existing = dict(state=existstate)\\n        proposed = dict(state=state)\\n        results['changed'] = False\\n        cmds = get_commands(proposed, existing, state, module)\\n        if cmds:\\n            cmds.insert(0, 'terminal dont-ask')\\n            if not module.check_mode:\\n                load_config(module, cmds)\\n            results['changed'] = True\\n    results['commands'] = cmds\\n    module.exit_json(**results)"
  },
  {
    "code": "def wrapped_markdown(s, css_class='rich_doc'):\\n    if s is None:\\n        return None\\n    s = '\\n'.join(line.lstrip() for line in s.split('\\n'))\\n    return Markup(f'<div class=\"{css_class}\" >' + markdown.markdown(s, extensions=['tables']) + \"</div>\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Clean Markdown with dedent to respect indents (#16414)",
    "fixed_code": "def wrapped_markdown(s, css_class='rich_doc'):\\n    if s is None:\\n        return None\\n    s = textwrap.dedent(s)\\n    return Markup(f'<div class=\"{css_class}\" >' + markdown.markdown(s, extensions=['tables']) + \"</div>\")"
  },
  {
    "code": "def eof_received(self):\\n        self._stream_reader.feed_eof()\\n        if self._over_ssl:\\n            return False\\n        return True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34638: Store a weak reference to stream reader to break strong references loop (GH-9201)\\n\\nStore a weak reference to stream readerfor breaking strong references\\n\\nIt breaks the strong reference loop between reader and protocol and allows to detect and close the socket if the stream is deleted (garbage collected)",
    "fixed_code": "def eof_received(self):\\n        reader = self._stream_reader\\n        if reader is not None:\\n            reader.feed_eof()\\n        if self._over_ssl:\\n            return False\\n        return True"
  },
  {
    "code": "def do_block_translate(parser, token):\\n    bits = token.split_contents()\\n    options = {}\\n    remaining_bits = bits[1:]\\n    while remaining_bits:\\n        option = remaining_bits.pop(0)\\n        if option in options:\\n            raise TemplateSyntaxError('The %r option was specified more '\\n                                      'than once.' % option)\\n        if option == 'with':\\n            value = token_kwargs(remaining_bits, parser, support_legacy=True)\\n            if not value:\\n                raise TemplateSyntaxError('\"with\" in %r tag needs at least '\\n                                          'one keyword argument.' % bits[0])\\n        elif option == 'count':\\n            value = token_kwargs(remaining_bits, parser, support_legacy=True)\\n            if len(value) != 1:\\n                raise TemplateSyntaxError('\"count\" in %r tag expected exactly '\\n                                          'one keyword argument.' % bits[0])\\n        elif option == \"context\":\\n            try:\\n                value = remaining_bits.pop(0)\\n                value = parser.compile_filter(value)\\n            except Exception:\\n                msg = (\\n                    '\"context\" in %r tag expected '\\n                    'exactly one argument.') % bits[0]\\n                six.reraise(TemplateSyntaxError, TemplateSyntaxError(msg), sys.exc_info()[2])\\n        elif option == \"trimmed\":\\n            value = True\\n        else:\\n            raise TemplateSyntaxError('Unknown argument for %r tag: %r.' %\\n                                      (bits[0], option))\\n        options[option] = value\\n    if 'count' in options:\\n        countervar, counter = list(six.iteritems(options['count']))[0]\\n    else:\\n        countervar, counter = None, None\\n    if 'context' in options:\\n        message_context = options['context']\\n    else:\\n        message_context = None\\n    extra_context = options.get('with', {})\\n    trimmed = options.get(\"trimmed\", False)\\n    singular = []\\n    plural = []\\n    while parser.tokens:\\n        token = parser.next_token()\\n        if token.token_type in (TOKEN_VAR, TOKEN_TEXT):\\n            singular.append(token)\\n        else:\\n            break\\n    if countervar and counter:\\n        if token.contents.strip() != 'plural':\\n            raise TemplateSyntaxError(\"'blocktrans' doesn't allow other block tags inside it\")\\n        while parser.tokens:\\n            token = parser.next_token()\\n            if token.token_type in (TOKEN_VAR, TOKEN_TEXT):\\n                plural.append(token)\\n            else:\\n                break\\n    if token.contents.strip() != 'endblocktrans':\\n        raise TemplateSyntaxError(\"'blocktrans' doesn't allow other block tags (seen %r) inside it\" % token.contents)\\n    return BlockTranslateNode(extra_context, singular, plural, countervar,\\n                              counter, message_context, trimmed=trimmed)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_block_translate(parser, token):\\n    bits = token.split_contents()\\n    options = {}\\n    remaining_bits = bits[1:]\\n    while remaining_bits:\\n        option = remaining_bits.pop(0)\\n        if option in options:\\n            raise TemplateSyntaxError('The %r option was specified more '\\n                                      'than once.' % option)\\n        if option == 'with':\\n            value = token_kwargs(remaining_bits, parser, support_legacy=True)\\n            if not value:\\n                raise TemplateSyntaxError('\"with\" in %r tag needs at least '\\n                                          'one keyword argument.' % bits[0])\\n        elif option == 'count':\\n            value = token_kwargs(remaining_bits, parser, support_legacy=True)\\n            if len(value) != 1:\\n                raise TemplateSyntaxError('\"count\" in %r tag expected exactly '\\n                                          'one keyword argument.' % bits[0])\\n        elif option == \"context\":\\n            try:\\n                value = remaining_bits.pop(0)\\n                value = parser.compile_filter(value)\\n            except Exception:\\n                msg = (\\n                    '\"context\" in %r tag expected '\\n                    'exactly one argument.') % bits[0]\\n                six.reraise(TemplateSyntaxError, TemplateSyntaxError(msg), sys.exc_info()[2])\\n        elif option == \"trimmed\":\\n            value = True\\n        else:\\n            raise TemplateSyntaxError('Unknown argument for %r tag: %r.' %\\n                                      (bits[0], option))\\n        options[option] = value\\n    if 'count' in options:\\n        countervar, counter = list(six.iteritems(options['count']))[0]\\n    else:\\n        countervar, counter = None, None\\n    if 'context' in options:\\n        message_context = options['context']\\n    else:\\n        message_context = None\\n    extra_context = options.get('with', {})\\n    trimmed = options.get(\"trimmed\", False)\\n    singular = []\\n    plural = []\\n    while parser.tokens:\\n        token = parser.next_token()\\n        if token.token_type in (TOKEN_VAR, TOKEN_TEXT):\\n            singular.append(token)\\n        else:\\n            break\\n    if countervar and counter:\\n        if token.contents.strip() != 'plural':\\n            raise TemplateSyntaxError(\"'blocktrans' doesn't allow other block tags inside it\")\\n        while parser.tokens:\\n            token = parser.next_token()\\n            if token.token_type in (TOKEN_VAR, TOKEN_TEXT):\\n                plural.append(token)\\n            else:\\n                break\\n    if token.contents.strip() != 'endblocktrans':\\n        raise TemplateSyntaxError(\"'blocktrans' doesn't allow other block tags (seen %r) inside it\" % token.contents)\\n    return BlockTranslateNode(extra_context, singular, plural, countervar,\\n                              counter, message_context, trimmed=trimmed)"
  },
  {
    "code": "def __repr__(self):\\n\\t\\tret = \"MockCall(\"\\n\\t\\tfor arg in self.args:\\n\\t\\t\\tret += repr(arg) + \", \"\\n\\t\\tif not self.kwargs:\\n\\t\\t\\tif self.args:\\n\\t\\t\\t\\tret = ret[:-2]\\n\\t\\telse:\\n\\t\\t\\tfor key, val in self.kwargs.items():\\n\\t\\t\\t\\tret += f\"{salt.utils.stringutils.to_str(key)}={repr(val)}\"\\n\\t\\tret += \")\"\\n\\t\\treturn ret",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n\\t\\tret = \"MockCall(\"\\n\\t\\tfor arg in self.args:\\n\\t\\t\\tret += repr(arg) + \", \"\\n\\t\\tif not self.kwargs:\\n\\t\\t\\tif self.args:\\n\\t\\t\\t\\tret = ret[:-2]\\n\\t\\telse:\\n\\t\\t\\tfor key, val in self.kwargs.items():\\n\\t\\t\\t\\tret += f\"{salt.utils.stringutils.to_str(key)}={repr(val)}\"\\n\\t\\tret += \")\"\\n\\t\\treturn ret"
  },
  {
    "code": "def load_package(name, path):\\n    if os.path.isdir(path):\\n        extensions = _bootstrap._suffix_list(PY_SOURCE)\\n        extensions += _bootstrap._suffix_list(PY_COMPILED)\\n        for extension in extensions:\\n            path = os.path.join(path, '__init__'+extension)\\n            if os.path.exists(path):\\n                break\\n        else:\\n            raise ValueError('{!r} is not a package'.format(path))\\n    return _bootstrap._SourceFileLoader(name, path).load_module(name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #14605: Expose importlib.abc.FileLoader and importlib.machinery.(FileFinder, SourceFileLoader, _SourcelessFileLoader, ExtensionFileLoader).\\n\\nThis exposes all of importlib's mechanisms that will become public on\\nthe sys module.",
    "fixed_code": "def load_package(name, path):\\n    if os.path.isdir(path):\\n        extensions = _bootstrap._suffix_list(PY_SOURCE)\\n        extensions += _bootstrap._suffix_list(PY_COMPILED)\\n        for extension in extensions:\\n            path = os.path.join(path, '__init__'+extension)\\n            if os.path.exists(path):\\n                break\\n        else:\\n            raise ValueError('{!r} is not a package'.format(path))\\n    return _bootstrap.SourceFileLoader(name, path).load_module(name)"
  },
  {
    "code": "def signature(self, value):\\n        return base64_hmac(self.salt + 'signer', value, self.key)\\n    def sign(self, value):\\n        value = smart_bytes(value)\\n        return '%s%s%s' % (value, self.sep, self.signature(value))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[py3] Made signing infrastructure pass tests with Python 3",
    "fixed_code": "def signature(self, value):\\n        return base64_hmac(self.salt + 'signer', value, self.key)\\n    def sign(self, value):\\n        return '%s%s%s' % (value, self.sep, self.signature(value))"
  },
  {
    "code": "def create_version(self, project_id, model_name, version_spec):\\n        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)\\n        create_request = self._cloudml.projects().models().versions().create(\\n            parent=parent_name, body=version_spec)\\n        response = create_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_version(self, project_id, model_name, version_spec):\\n        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)\\n        create_request = self._cloudml.projects().models().versions().create(\\n            parent=parent_name, body=version_spec)\\n        response = create_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)"
  },
  {
    "code": "def getvalue(self):\\n        return super(PythonSerializer, self).getvalue()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getvalue(self):\\n        return super(PythonSerializer, self).getvalue()"
  },
  {
    "code": "def __rmul__(self, b):\\n        if not isinstance(b, Kernel):\\n            return Product(ConstantKernel.from_literal(b), self)\\n        return Product(b, self)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __rmul__(self, b):\\n        if not isinstance(b, Kernel):\\n            return Product(ConstantKernel.from_literal(b), self)\\n        return Product(b, self)"
  },
  {
    "code": "def _get_data_label(self):\\n        if self.format_version == 118:\\n            strlen = struct.unpack(self.byteorder + \"H\", self.path_or_buf.read(2))[0]\\n            return self._decode(self.path_or_buf.read(strlen))\\n        elif self.format_version == 117:\\n            strlen = struct.unpack(\"b\", self.path_or_buf.read(1))[0]\\n            return self._decode(self.path_or_buf.read(strlen))\\n        elif self.format_version > 105:\\n            return self._decode(self.path_or_buf.read(81))\\n        else:\\n            return self._decode(self.path_or_buf.read(32))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Add dta 119 reading to StataReader (#28542)\\n\\nAdd requirements for reading 119 format files",
    "fixed_code": "def _get_data_label(self):\\n        if self.format_version >= 118:\\n            strlen = struct.unpack(self.byteorder + \"H\", self.path_or_buf.read(2))[0]\\n            return self._decode(self.path_or_buf.read(strlen))\\n        elif self.format_version == 117:\\n            strlen = struct.unpack(\"b\", self.path_or_buf.read(1))[0]\\n            return self._decode(self.path_or_buf.read(strlen))\\n        elif self.format_version > 105:\\n            return self._decode(self.path_or_buf.read(81))\\n        else:\\n            return self._decode(self.path_or_buf.read(32))"
  },
  {
    "code": "def max(self, axis=0):\\n        return self.apply(Series.max, axis=axis)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def max(self, axis=0):\\n        return self.apply(Series.max, axis=axis)"
  },
  {
    "code": "def _generate_cache_header_key(key_prefix, request):\\n    path = hashlib.md5(force_bytes(iri_to_uri(request.get_full_path())))\\n    cache_key = 'views.decorators.cache.cache_header.%s.%s' % (\\n        key_prefix, path.hexdigest())\\n    return _i18n_cache_key_suffix(request, cache_key)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #20346 -- Made cache middleware vary on the full URL.\\n\\nPreviously, only the URL path was included in the cache key.\\n\\nThanks jamey for the suggestion.",
    "fixed_code": "def _generate_cache_header_key(key_prefix, request):\\n    url = hashlib.md5(force_bytes(iri_to_uri(request.build_absolute_uri())))\\n    cache_key = 'views.decorators.cache.cache_header.%s.%s' % (\\n        key_prefix, url.hexdigest())\\n    return _i18n_cache_key_suffix(request, cache_key)"
  },
  {
    "code": "def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,\\n\\t\\t\\t\\t\\t\\t validate_certs, force, parent=None):\\n\\tdep_msg = \"\"\\n\\tif parent:\\n\\t\\tdep_msg = \" - as dependency of %s\" % parent\\n\\tdisplay.vvv(\"Processing requirement collection '%s'%s\" % (to_text(collection), dep_msg))\\n\\tb_tar_path = None\\n\\tif os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\\n\\t\\tb_tar_path = to_bytes(collection, errors='surrogate_or_strict')\\n\\telif urlparse(collection).scheme:\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\\n\\t\\tb_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\\n\\tif b_tar_path:\\n\\t\\treq = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)\\n\\t\\tcollection_name = to_text(req)\\n\\t\\tif collection_name in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection_name]\\n\\t\\t\\tcollection_info.add_requirement(None, req.latest_version)\\n\\t\\telse:\\n\\t\\t\\tcollection_info = req\\n\\telse:\\n\\t\\tvalidate_collection_name(collection)\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is the name of a collection\" % collection)\\n\\t\\tif collection in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection]\\n\\t\\t\\tcollection_info.add_requirement(parent, requirement)\\n\\t\\telse:\\n\\t\\t\\tapis = [source] if source else apis\\n\\t\\t\\tcollection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent)\\n\\texisting = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\\n\\tif existing and not collection_info.force:\\n\\t\\texisting[0].add_requirement(to_text(collection_info), requirement)\\n\\t\\tcollection_info = existing[0]\\n\\tdep_map[to_text(collection_info)] = collection_info",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix using a URL for galaxy collection install (#65272)",
    "fixed_code": "def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,\\n\\t\\t\\t\\t\\t\\t validate_certs, force, parent=None):\\n\\tdep_msg = \"\"\\n\\tif parent:\\n\\t\\tdep_msg = \" - as dependency of %s\" % parent\\n\\tdisplay.vvv(\"Processing requirement collection '%s'%s\" % (to_text(collection), dep_msg))\\n\\tb_tar_path = None\\n\\tif os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\\n\\t\\tb_tar_path = to_bytes(collection, errors='surrogate_or_strict')\\n\\telif urlparse(collection).scheme.lower() in ['http', 'https']:\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\\n\\t\\ttry:\\n\\t\\t\\tb_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\\n\\t\\texcept urllib_error.URLError as err:\\n\\t\\t\\traise AnsibleError(\"Failed to download collection tar from '%s': %s\"\\n\\t\\t\\t\\t\\t\\t\\t   % (to_native(collection), to_native(err)))\\n\\tif b_tar_path:\\n\\t\\treq = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)\\n\\t\\tcollection_name = to_text(req)\\n\\t\\tif collection_name in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection_name]\\n\\t\\t\\tcollection_info.add_requirement(None, req.latest_version)\\n\\t\\telse:\\n\\t\\t\\tcollection_info = req\\n\\telse:\\n\\t\\tvalidate_collection_name(collection)\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is the name of a collection\" % collection)\\n\\t\\tif collection in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection]\\n\\t\\t\\tcollection_info.add_requirement(parent, requirement)\\n\\t\\telse:\\n\\t\\t\\tapis = [source] if source else apis\\n\\t\\t\\tcollection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent)\\n\\texisting = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\\n\\tif existing and not collection_info.force:\\n\\t\\texisting[0].add_requirement(to_text(collection_info), requirement)\\n\\t\\tcollection_info = existing[0]\\n\\tdep_map[to_text(collection_info)] = collection_info"
  },
  {
    "code": "def get_validation_errors(outfile, app=None):\\n    from django.db import models, connection\\n    from django.db.models.loading import get_app_errors\\n    from django.db.models.deletion import SET_NULL, SET_DEFAULT\\n    e = ModelErrorCollection(outfile)\\n    for (app_name, error) in get_app_errors().items():\\n        e.add(app_name, error)\\n    for cls in models.get_models(app, include_swapped=True):\\n        opts = cls._meta\\n        if opts.swapped:\\n            try:\\n                app_label, model_name = opts.swapped.split('.')\\n            except ValueError:\\n                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\\n                continue\\n            if not models.get_model(app_label, model_name):\\n                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\\n            continue\\n        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\\n            if not isinstance(cls.REQUIRED_FIELDS, (list, tuple)):\\n                e.add(opts, 'The REQUIRED_FIELDS must be a list or tuple.')\\n            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\\n                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\\n            if not opts.get_field(cls.USERNAME_FIELD).unique:\\n                e.add(opts, 'The USERNAME_FIELD must be unique. Add unique=True to the field parameters.')\\n        for f in opts.local_fields:\\n            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\\n                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\\n            if f.name.endswith('_'):\\n                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\\n            if (f.primary_key and f.null and\\n                    not connection.features.interprets_empty_strings_as_nulls):\\n                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\\n            if isinstance(f, models.CharField):\\n                try:\\n                    max_length = int(f.max_length)\\n                    if max_length <= 0:\\n                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n                except (ValueError, TypeError):\\n                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n            if isinstance(f, models.DecimalField):\\n                decimalp_ok, mdigits_ok = False, False\\n                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\\n                try:\\n                    decimal_places = int(f.decimal_places)\\n                    if decimal_places < 0:\\n                        e.add(opts, decimalp_msg % f.name)\\n                    else:\\n                        decimalp_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, decimalp_msg % f.name)\\n                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\\n                try:\\n                    max_digits = int(f.max_digits)\\n                    if max_digits <= 0:\\n                        e.add(opts,  mdigits_msg % f.name)\\n                    else:\\n                        mdigits_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, mdigits_msg % f.name)\\n                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\\n                if decimalp_ok and mdigits_ok:\\n                    if decimal_places > max_digits:\\n                        e.add(opts, invalid_values_msg % f.name)\\n            if isinstance(f, models.FileField) and not f.upload_to:\\n                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\\n            if isinstance(f, models.ImageField):\\n                try:\\n                    from django.utils.image import Image\\n                except ImportError:\\n                    e.add(opts, '\"%s\": To use ImageFields, you need to install Pillow. Get it at https://pypi.python.org/pypi/Pillow.' % f.name)\\n            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\\n                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\\n            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\\n                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\\n            if isinstance(f, models.GenericIPAddressField) and not getattr(f, 'null', False) and getattr(f, 'blank', False):\\n                e.add(opts, '\"%s\": GenericIPAddressField can not accept blank values if null values are not allowed, as blank values are stored as null.' % f.name)\\n            if f.choices:\\n                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\\n                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\\n                else:\\n                    for c in f.choices:\\n                        if isinstance(c, six.string_types) or not is_iterable(c) or len(c) != 2:\\n                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).' % f.name)\\n            if f.db_index not in (None, True, False):\\n                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\\n            connection.validation.validate_field(e, opts, f)\\n            if f.rel and hasattr(f.rel, 'on_delete'):\\n                if f.rel.on_delete == SET_NULL and not f.null:\\n                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\\n                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\\n                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\\n            if f.rel:\\n                if f.rel.to not in models.get_models():\\n                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                    else:\\n                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n                if f.requires_unique_target:\\n                    if len(f.foreign_related_fields) > 1:\\n                        has_unique_field = False\\n                        for rel_field in f.foreign_related_fields:\\n                            has_unique_field = has_unique_field or rel_field.unique\\n                        if not has_unique_field:\\n                            e.add(opts, \"Field combination '%s' under model '%s' must have a unique=True constraint\" % (','.join([rel_field.name for rel_field in f.foreign_related_fields]), f.rel.to.__name__))\\n                    else:\\n                        if not f.foreign_related_fields[0].unique:\\n                            e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.foreign_related_fields[0].name, f.rel.to.__name__))\\n                rel_opts = f.rel.to._meta\\n                rel_name = f.related.get_accessor_name()\\n                rel_query_name = f.related_query_name()\\n                if not f.rel.is_hidden():\\n                    for r in rel_opts.fields:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.local_many_to_many:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.get_all_related_many_to_many_objects():\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    for r in rel_opts.get_all_related_objects():\\n                        if r.field is not f:\\n                            if r.get_accessor_name() == rel_name:\\n                                e.add(opts, \"Accessor for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                            if r.get_accessor_name() == rel_query_name:\\n                                e.add(opts, \"Reverse query name for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        seen_intermediary_signatures = []\\n        for i, f in enumerate(opts.local_many_to_many):\\n            if f.rel.to not in models.get_models():\\n                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                else:\\n                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n            if f.unique:\\n                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\\n            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\\n                from_model, to_model = cls, f.rel.to\\n                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\\n                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\\n                seen_from, seen_to, seen_self = False, False, 0\\n                for inter_field in f.rel.through._meta.fields:\\n                    rel_to = getattr(inter_field.rel, 'to', None)\\n                    if from_model == to_model:  \\n                        if rel_to == from_model:\\n                            seen_self += 1\\n                        if seen_self > 2:\\n                            e.add(opts, \"Intermediary model %s has more than \"\\n                                \"two foreign keys to %s, which is ambiguous \"\\n                                \"and is not permitted.\" % (\\n                                    f.rel.through._meta.object_name,\\n                                    from_model._meta.object_name\\n                                )\\n                            )\\n                    else:\\n                        if rel_to == from_model:\\n                            if seen_from:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                         from_model._meta.object_name\\n                                     )\\n                                 )\\n                            else:\\n                                seen_from = True\\n                        elif rel_to == to_model:\\n                            if seen_to:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                        rel_to._meta.object_name\\n                                    )\\n                                )\\n                            else:\\n                                seen_to = True\\n                if f.rel.through not in models.get_models(include_auto_created=True):\\n                    e.add(opts, \"'%s' specifies an m2m relation through model \"\\n                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\\n                    )\\n                signature = (f.rel.to, cls, f.rel.through)\\n                if signature in seen_intermediary_signatures:\\n                    e.add(opts, \"The model %s has two manually-defined m2m \"\\n                        \"relations through the model %s, which is not \"\\n                        \"permitted. Please consider using an extra field on \"\\n                        \"your intermediary model instead.\" % (\\n                            cls._meta.object_name,\\n                            f.rel.through._meta.object_name\\n                        )\\n                    )\\n                else:\\n                    seen_intermediary_signatures.append(signature)\\n                if not f.rel.through._meta.auto_created:\\n                    seen_related_fk, seen_this_fk = False, False\\n                    for field in f.rel.through._meta.fields:\\n                        if field.rel:\\n                            if not seen_related_fk and field.rel.to == f.rel.to:\\n                                seen_related_fk = True\\n                            elif field.rel.to == cls:\\n                                seen_this_fk = True\\n                    if not seen_related_fk or not seen_this_fk:\\n                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\\n                            \"through model %s, which does not have foreign keys \"\\n                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\\n                                f.rel.to._meta.object_name, cls._meta.object_name)\\n                        )\\n            elif isinstance(f.rel.through, six.string_types):\\n                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\\n                    \"which has not been installed\" % (f.name, f.rel.through)\\n                )\\n            rel_opts = f.rel.to._meta\\n            rel_name = f.related.get_accessor_name()\\n            rel_query_name = f.related_query_name()\\n            if rel_name is not None:\\n                for r in rel_opts.fields:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.local_many_to_many:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.get_all_related_many_to_many_objects():\\n                    if r.field is not f:\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                for r in rel_opts.get_all_related_objects():\\n                    if r.get_accessor_name() == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    if r.get_accessor_name() == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        if opts.ordering:\\n            for field_name in opts.ordering:\\n                if field_name == '?':\\n                    continue\\n                if field_name.startswith('-'):\\n                    field_name = field_name[1:]\\n                if opts.order_with_respect_to and field_name == '_order':\\n                    continue\\n                if '__' in field_name:\\n                    continue\\n                if field_name == 'pk':\\n                    continue\\n                try:\\n                    opts.get_field(field_name, many_to_many=False)\\n                except models.FieldDoesNotExist:\\n                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\\n        for ut in opts.unique_together:\\n            validate_local_fields(e, opts, \"unique_together\", ut)\\n        if not isinstance(opts.index_together, collections.Sequence):\\n            e.add(opts, '\"index_together\" must a sequence')\\n        else:\\n            for it in opts.index_together:\\n                validate_local_fields(e, opts, \"index_together\", it)\\n    return len(e.errors)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_validation_errors(outfile, app=None):\\n    from django.db import models, connection\\n    from django.db.models.loading import get_app_errors\\n    from django.db.models.deletion import SET_NULL, SET_DEFAULT\\n    e = ModelErrorCollection(outfile)\\n    for (app_name, error) in get_app_errors().items():\\n        e.add(app_name, error)\\n    for cls in models.get_models(app, include_swapped=True):\\n        opts = cls._meta\\n        if opts.swapped:\\n            try:\\n                app_label, model_name = opts.swapped.split('.')\\n            except ValueError:\\n                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\\n                continue\\n            if not models.get_model(app_label, model_name):\\n                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\\n            continue\\n        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\\n            if not isinstance(cls.REQUIRED_FIELDS, (list, tuple)):\\n                e.add(opts, 'The REQUIRED_FIELDS must be a list or tuple.')\\n            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\\n                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\\n            if not opts.get_field(cls.USERNAME_FIELD).unique:\\n                e.add(opts, 'The USERNAME_FIELD must be unique. Add unique=True to the field parameters.')\\n        for f in opts.local_fields:\\n            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\\n                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\\n            if f.name.endswith('_'):\\n                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\\n            if (f.primary_key and f.null and\\n                    not connection.features.interprets_empty_strings_as_nulls):\\n                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\\n            if isinstance(f, models.CharField):\\n                try:\\n                    max_length = int(f.max_length)\\n                    if max_length <= 0:\\n                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n                except (ValueError, TypeError):\\n                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n            if isinstance(f, models.DecimalField):\\n                decimalp_ok, mdigits_ok = False, False\\n                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\\n                try:\\n                    decimal_places = int(f.decimal_places)\\n                    if decimal_places < 0:\\n                        e.add(opts, decimalp_msg % f.name)\\n                    else:\\n                        decimalp_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, decimalp_msg % f.name)\\n                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\\n                try:\\n                    max_digits = int(f.max_digits)\\n                    if max_digits <= 0:\\n                        e.add(opts,  mdigits_msg % f.name)\\n                    else:\\n                        mdigits_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, mdigits_msg % f.name)\\n                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\\n                if decimalp_ok and mdigits_ok:\\n                    if decimal_places > max_digits:\\n                        e.add(opts, invalid_values_msg % f.name)\\n            if isinstance(f, models.FileField) and not f.upload_to:\\n                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\\n            if isinstance(f, models.ImageField):\\n                try:\\n                    from django.utils.image import Image\\n                except ImportError:\\n                    e.add(opts, '\"%s\": To use ImageFields, you need to install Pillow. Get it at https://pypi.python.org/pypi/Pillow.' % f.name)\\n            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\\n                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\\n            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\\n                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\\n            if isinstance(f, models.GenericIPAddressField) and not getattr(f, 'null', False) and getattr(f, 'blank', False):\\n                e.add(opts, '\"%s\": GenericIPAddressField can not accept blank values if null values are not allowed, as blank values are stored as null.' % f.name)\\n            if f.choices:\\n                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\\n                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\\n                else:\\n                    for c in f.choices:\\n                        if isinstance(c, six.string_types) or not is_iterable(c) or len(c) != 2:\\n                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).' % f.name)\\n            if f.db_index not in (None, True, False):\\n                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\\n            connection.validation.validate_field(e, opts, f)\\n            if f.rel and hasattr(f.rel, 'on_delete'):\\n                if f.rel.on_delete == SET_NULL and not f.null:\\n                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\\n                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\\n                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\\n            if f.rel:\\n                if f.rel.to not in models.get_models():\\n                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                    else:\\n                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n                if f.requires_unique_target:\\n                    if len(f.foreign_related_fields) > 1:\\n                        has_unique_field = False\\n                        for rel_field in f.foreign_related_fields:\\n                            has_unique_field = has_unique_field or rel_field.unique\\n                        if not has_unique_field:\\n                            e.add(opts, \"Field combination '%s' under model '%s' must have a unique=True constraint\" % (','.join([rel_field.name for rel_field in f.foreign_related_fields]), f.rel.to.__name__))\\n                    else:\\n                        if not f.foreign_related_fields[0].unique:\\n                            e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.foreign_related_fields[0].name, f.rel.to.__name__))\\n                rel_opts = f.rel.to._meta\\n                rel_name = f.related.get_accessor_name()\\n                rel_query_name = f.related_query_name()\\n                if not f.rel.is_hidden():\\n                    for r in rel_opts.fields:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.local_many_to_many:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.get_all_related_many_to_many_objects():\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    for r in rel_opts.get_all_related_objects():\\n                        if r.field is not f:\\n                            if r.get_accessor_name() == rel_name:\\n                                e.add(opts, \"Accessor for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                            if r.get_accessor_name() == rel_query_name:\\n                                e.add(opts, \"Reverse query name for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        seen_intermediary_signatures = []\\n        for i, f in enumerate(opts.local_many_to_many):\\n            if f.rel.to not in models.get_models():\\n                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                else:\\n                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n            if f.unique:\\n                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\\n            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\\n                from_model, to_model = cls, f.rel.to\\n                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\\n                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\\n                seen_from, seen_to, seen_self = False, False, 0\\n                for inter_field in f.rel.through._meta.fields:\\n                    rel_to = getattr(inter_field.rel, 'to', None)\\n                    if from_model == to_model:  \\n                        if rel_to == from_model:\\n                            seen_self += 1\\n                        if seen_self > 2:\\n                            e.add(opts, \"Intermediary model %s has more than \"\\n                                \"two foreign keys to %s, which is ambiguous \"\\n                                \"and is not permitted.\" % (\\n                                    f.rel.through._meta.object_name,\\n                                    from_model._meta.object_name\\n                                )\\n                            )\\n                    else:\\n                        if rel_to == from_model:\\n                            if seen_from:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                         from_model._meta.object_name\\n                                     )\\n                                 )\\n                            else:\\n                                seen_from = True\\n                        elif rel_to == to_model:\\n                            if seen_to:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                        rel_to._meta.object_name\\n                                    )\\n                                )\\n                            else:\\n                                seen_to = True\\n                if f.rel.through not in models.get_models(include_auto_created=True):\\n                    e.add(opts, \"'%s' specifies an m2m relation through model \"\\n                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\\n                    )\\n                signature = (f.rel.to, cls, f.rel.through)\\n                if signature in seen_intermediary_signatures:\\n                    e.add(opts, \"The model %s has two manually-defined m2m \"\\n                        \"relations through the model %s, which is not \"\\n                        \"permitted. Please consider using an extra field on \"\\n                        \"your intermediary model instead.\" % (\\n                            cls._meta.object_name,\\n                            f.rel.through._meta.object_name\\n                        )\\n                    )\\n                else:\\n                    seen_intermediary_signatures.append(signature)\\n                if not f.rel.through._meta.auto_created:\\n                    seen_related_fk, seen_this_fk = False, False\\n                    for field in f.rel.through._meta.fields:\\n                        if field.rel:\\n                            if not seen_related_fk and field.rel.to == f.rel.to:\\n                                seen_related_fk = True\\n                            elif field.rel.to == cls:\\n                                seen_this_fk = True\\n                    if not seen_related_fk or not seen_this_fk:\\n                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\\n                            \"through model %s, which does not have foreign keys \"\\n                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\\n                                f.rel.to._meta.object_name, cls._meta.object_name)\\n                        )\\n            elif isinstance(f.rel.through, six.string_types):\\n                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\\n                    \"which has not been installed\" % (f.name, f.rel.through)\\n                )\\n            rel_opts = f.rel.to._meta\\n            rel_name = f.related.get_accessor_name()\\n            rel_query_name = f.related_query_name()\\n            if rel_name is not None:\\n                for r in rel_opts.fields:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.local_many_to_many:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.get_all_related_many_to_many_objects():\\n                    if r.field is not f:\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                for r in rel_opts.get_all_related_objects():\\n                    if r.get_accessor_name() == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    if r.get_accessor_name() == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        if opts.ordering:\\n            for field_name in opts.ordering:\\n                if field_name == '?':\\n                    continue\\n                if field_name.startswith('-'):\\n                    field_name = field_name[1:]\\n                if opts.order_with_respect_to and field_name == '_order':\\n                    continue\\n                if '__' in field_name:\\n                    continue\\n                if field_name == 'pk':\\n                    continue\\n                try:\\n                    opts.get_field(field_name, many_to_many=False)\\n                except models.FieldDoesNotExist:\\n                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\\n        for ut in opts.unique_together:\\n            validate_local_fields(e, opts, \"unique_together\", ut)\\n        if not isinstance(opts.index_together, collections.Sequence):\\n            e.add(opts, '\"index_together\" must a sequence')\\n        else:\\n            for it in opts.index_together:\\n                validate_local_fields(e, opts, \"index_together\", it)\\n    return len(e.errors)"
  },
  {
    "code": "def get_dirs(self):\\n        return self.dirs if self.dirs is not None else self.engine.dirs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_dirs(self):\\n        return self.dirs if self.dirs is not None else self.engine.dirs"
  },
  {
    "code": "def nanmax(a, axis=None):\\n    a = np.asanyarray(a)\\n    if axis is not None:\\n        return np.fmax.reduce(a, axis)\\n    else:\\n        return np.fmax.reduce(a.flat)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT: Refactor nanfunctions.\\n\\nnanmax, nanmin\\n--------------\\nAdd out and keepdims keywords.\\n\\nnanargmin, nanargmax\\n--------------------\\nA NanWarning is raised if an all NaN slice detected. For all such\\nslices np.iingo(np.intp).min is returned as the index value.\\n\\nnansum\\n------\\nThe keywords dtype, out, and keepdims are added.\\n\\nA FutureWarning is raised, as in the future the mean of an empty\\nslice after NaN replacement will be 0 instead of the current NaN.\\n\\nnanmean, nanvar, nanstd\\n-----------------------\\nFor all, if the input array is of inexact type then the dtype and out\\nparameters must be of inexact type if specified.  That insures that NaNs\\ncan be returned when appropriate.\\n\\nThe nanmean function detects empty slices after NaN replacement and\\nraises a NanWarning. NaN is returned as the value for all such slices.\\n\\nThe nanmean and nanstd functions detect degrees of freedom <= 0 after\\nNaN replacement and raise a NanWarning. NaN is returned as the value for\\nall such slices.",
    "fixed_code": "def nanmax(a, axis=None, out=None, keepdims=False):\\n    return np.fmax.reduce(a, axis=axis, out=out, keepdims=keepdims)"
  },
  {
    "code": "def configure_logging(logging_config, logging_settings):\\n    if logging_config:\\n        logging_config_func = import_string(logging_config)\\n        logging.config.dictConfig(DEFAULT_LOGGING)\\n        if logging_settings:\\n            logging_config_func(logging_settings)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def configure_logging(logging_config, logging_settings):\\n    if logging_config:\\n        logging_config_func = import_string(logging_config)\\n        logging.config.dictConfig(DEFAULT_LOGGING)\\n        if logging_settings:\\n            logging_config_func(logging_settings)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        youtube_channel_status=dict(required=False, type=\"str\", choices=[\"disable\", \"blacklist\", \"whitelist\"]),\\n        wisp_servers=dict(required=False, type=\"str\"),\\n        wisp_algorithm=dict(required=False, type=\"str\", choices=[\"auto-learning\", \"primary-secondary\", \"round-robin\"]),\\n        wisp=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_url_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_invalid_domain_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_ftgd_quota_usage=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_ftgd_err_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_vbs_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_unknown_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_referer_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_jscript_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_js_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_cookie_removal_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_cookie_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_command_block_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_applet_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_activex_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_extended_all_action_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_content_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        replacemsg_group=dict(required=False, type=\"str\"),\\n        post_action=dict(required=False, type=\"str\", choices=[\"normal\", \"block\"]),\\n        ovrd_perm=dict(required=False, type=\"list\", choices=[\"bannedword-override\",\\n                                                             \"urlfilter-override\",\\n                                                             \"fortiguard-wf-override\",\\n                                                             \"contenttype-check-override\"]),\\n        options=dict(required=False, type=\"list\", choices=[\"block-invalid-url\",\\n                                                           \"jscript\",\\n                                                           \"js\",\\n                                                           \"vbs\",\\n                                                           \"unknown\",\\n                                                           \"wf-referer\",\\n                                                           \"intrinsic\",\\n                                                           \"wf-cookie\",\\n                                                           \"per-user-bwl\",\\n                                                           \"activexfilter\",\\n                                                           \"cookiefilter\",\\n                                                           \"javafilter\"]),\\n        name=dict(required=False, type=\"str\"),\\n        log_all_url=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        inspection_mode=dict(required=False, type=\"str\", choices=[\"proxy\", \"flow-based\"]),\\n        https_replacemsg=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        extended_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        ftgd_wf=dict(required=False, type=\"list\"),\\n        ftgd_wf_exempt_quota=dict(required=False, type=\"str\"),\\n        ftgd_wf_max_quota_timeout=dict(required=False, type=\"int\"),\\n        ftgd_wf_options=dict(required=False, type=\"str\", choices=[\"error-allow\", \"rate-server-ip\",\\n                                                                  \"connect-request-bypass\", \"ftgd-disable\"]),\\n        ftgd_wf_ovrd=dict(required=False, type=\"str\"),\\n        ftgd_wf_rate_crl_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_rate_css_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_rate_image_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_rate_javascript_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_filters_action=dict(required=False, type=\"str\", choices=[\"block\", \"monitor\",\\n                                                                         \"warning\", \"authenticate\"]),\\n        ftgd_wf_filters_auth_usr_grp=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_category=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_filters_override_replacemsg=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_warn_duration=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_warning_duration_type=dict(required=False, type=\"str\", choices=[\"session\", \"timeout\"]),\\n        ftgd_wf_filters_warning_prompt=dict(required=False, type=\"str\", choices=[\"per-domain\", \"per-category\"]),\\n        ftgd_wf_quota_category=dict(required=False, type=\"str\"),\\n        ftgd_wf_quota_duration=dict(required=False, type=\"str\"),\\n        ftgd_wf_quota_override_replacemsg=dict(required=False, type=\"str\"),\\n        ftgd_wf_quota_type=dict(required=False, type=\"str\", choices=[\"time\", \"traffic\"]),\\n        ftgd_wf_quota_unit=dict(required=False, type=\"str\", choices=[\"B\", \"KB\", \"MB\", \"GB\"]),\\n        ftgd_wf_quota_value=dict(required=False, type=\"int\"),\\n        override=dict(required=False, type=\"list\"),\\n        override_ovrd_cookie=dict(required=False, type=\"str\", choices=[\"deny\", \"allow\"]),\\n        override_ovrd_dur=dict(required=False, type=\"str\"),\\n        override_ovrd_dur_mode=dict(required=False, type=\"str\", choices=[\"constant\", \"ask\"]),\\n        override_ovrd_scope=dict(required=False, type=\"str\", choices=[\"user\", \"user-group\", \"ip\", \"ask\", \"browser\"]),\\n        override_ovrd_user_group=dict(required=False, type=\"str\"),\\n        override_profile=dict(required=False, type=\"str\"),\\n        override_profile_attribute=dict(required=False, type=\"list\", choices=[\"User-Name\",\\n                                                                              \"NAS-IP-Address\",\\n                                                                              \"Framed-IP-Address\",\\n                                                                              \"Framed-IP-Netmask\",\\n                                                                              \"Filter-Id\",\\n                                                                              \"Login-IP-Host\",\\n                                                                              \"Reply-Message\",\\n                                                                              \"Callback-Number\",\\n                                                                              \"Callback-Id\",\\n                                                                              \"Framed-Route\",\\n                                                                              \"Framed-IPX-Network\",\\n                                                                              \"Class\",\\n                                                                              \"Called-Station-Id\",\\n                                                                              \"Calling-Station-Id\",\\n                                                                              \"NAS-Identifier\",\\n                                                                              \"Proxy-State\",\\n                                                                              \"Login-LAT-Service\",\\n                                                                              \"Login-LAT-Node\",\\n                                                                              \"Login-LAT-Group\",\\n                                                                              \"Framed-AppleTalk-Zone\",\\n                                                                              \"Acct-Session-Id\",\\n                                                                              \"Acct-Multi-Session-Id\"]),\\n        override_profile_type=dict(required=False, type=\"str\", choices=[\"list\", \"radius\"]),\\n        url_extraction=dict(required=False, type=\"list\"),\\n        url_extraction_redirect_header=dict(required=False, type=\"str\"),\\n        url_extraction_redirect_no_content=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_extraction_redirect_url=dict(required=False, type=\"str\"),\\n        url_extraction_server_fqdn=dict(required=False, type=\"str\"),\\n        url_extraction_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web=dict(required=False, type=\"list\"),\\n        web_blacklist=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_bword_table=dict(required=False, type=\"str\"),\\n        web_bword_threshold=dict(required=False, type=\"int\"),\\n        web_content_header_list=dict(required=False, type=\"str\"),\\n        web_keyword_match=dict(required=False, type=\"str\"),\\n        web_log_search=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_safe_search=dict(required=False, type=\"str\", choices=[\"url\", \"header\"]),\\n        web_urlfilter_table=dict(required=False, type=\"str\"),\\n        web_whitelist=dict(required=False, type=\"list\", choices=[\"exempt-av\",\\n                                                                 \"exempt-webcontent\",\\n                                                                 \"exempt-activex-java-cookie\",\\n                                                                 \"exempt-dlp\",\\n                                                                 \"exempt-rangeblock\",\\n                                                                 \"extended-log-others\"]),\\n        web_youtube_restrict=dict(required=False, type=\"str\", choices=[\"strict\", \"none\", \"moderate\"]),\\n        youtube_channel_filter=dict(required=False, type=\"list\"),\\n        youtube_channel_filter_channel_id=dict(required=False, type=\"str\"),\\n        youtube_channel_filter_comment=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"youtube-channel-status\": module.params[\"youtube_channel_status\"],\\n        \"wisp-servers\": module.params[\"wisp_servers\"],\\n        \"wisp-algorithm\": module.params[\"wisp_algorithm\"],\\n        \"wisp\": module.params[\"wisp\"],\\n        \"web-url-log\": module.params[\"web_url_log\"],\\n        \"web-invalid-domain-log\": module.params[\"web_invalid_domain_log\"],\\n        \"web-ftgd-quota-usage\": module.params[\"web_ftgd_quota_usage\"],\\n        \"web-ftgd-err-log\": module.params[\"web_ftgd_err_log\"],\\n        \"web-filter-vbs-log\": module.params[\"web_filter_vbs_log\"],\\n        \"web-filter-unknown-log\": module.params[\"web_filter_unknown_log\"],\\n        \"web-filter-referer-log\": module.params[\"web_filter_referer_log\"],\\n        \"web-filter-jscript-log\": module.params[\"web_filter_jscript_log\"],\\n        \"web-filter-js-log\": module.params[\"web_filter_js_log\"],\\n        \"web-filter-cookie-removal-log\": module.params[\"web_filter_cookie_removal_log\"],\\n        \"web-filter-cookie-log\": module.params[\"web_filter_cookie_log\"],\\n        \"web-filter-command-block-log\": module.params[\"web_filter_command_block_log\"],\\n        \"web-filter-applet-log\": module.params[\"web_filter_applet_log\"],\\n        \"web-filter-activex-log\": module.params[\"web_filter_activex_log\"],\\n        \"web-extended-all-action-log\": module.params[\"web_extended_all_action_log\"],\\n        \"web-content-log\": module.params[\"web_content_log\"],\\n        \"replacemsg-group\": module.params[\"replacemsg_group\"],\\n        \"post-action\": module.params[\"post_action\"],\\n        \"ovrd-perm\": module.params[\"ovrd_perm\"],\\n        \"options\": module.params[\"options\"],\\n        \"name\": module.params[\"name\"],\\n        \"log-all-url\": module.params[\"log_all_url\"],\\n        \"inspection-mode\": module.params[\"inspection_mode\"],\\n        \"https-replacemsg\": module.params[\"https_replacemsg\"],\\n        \"extended-log\": module.params[\"extended_log\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"ftgd-wf\": {\\n            \"exempt-quota\": module.params[\"ftgd_wf_exempt_quota\"],\\n            \"max-quota-timeout\": module.params[\"ftgd_wf_max_quota_timeout\"],\\n            \"options\": module.params[\"ftgd_wf_options\"],\\n            \"ovrd\": module.params[\"ftgd_wf_ovrd\"],\\n            \"rate-crl-urls\": module.params[\"ftgd_wf_rate_crl_urls\"],\\n            \"rate-css-urls\": module.params[\"ftgd_wf_rate_css_urls\"],\\n            \"rate-image-urls\": module.params[\"ftgd_wf_rate_image_urls\"],\\n            \"rate-javascript-urls\": module.params[\"ftgd_wf_rate_javascript_urls\"],\\n            \"filters\": {\\n                \"action\": module.params[\"ftgd_wf_filters_action\"],\\n                \"auth-usr-grp\": module.params[\"ftgd_wf_filters_auth_usr_grp\"],\\n                \"category\": module.params[\"ftgd_wf_filters_category\"],\\n                \"log\": module.params[\"ftgd_wf_filters_log\"],\\n                \"override-replacemsg\": module.params[\"ftgd_wf_filters_override_replacemsg\"],\\n                \"warn-duration\": module.params[\"ftgd_wf_filters_warn_duration\"],\\n                \"warning-duration-type\": module.params[\"ftgd_wf_filters_warning_duration_type\"],\\n                \"warning-prompt\": module.params[\"ftgd_wf_filters_warning_prompt\"],\\n            },\\n            \"quota\": {\\n                \"category\": module.params[\"ftgd_wf_quota_category\"],\\n                \"duration\": module.params[\"ftgd_wf_quota_duration\"],\\n                \"override-replacemsg\": module.params[\"ftgd_wf_quota_override_replacemsg\"],\\n                \"type\": module.params[\"ftgd_wf_quota_type\"],\\n                \"unit\": module.params[\"ftgd_wf_quota_unit\"],\\n                \"value\": module.params[\"ftgd_wf_quota_value\"],\\n            },\\n        },\\n        \"override\": {\\n            \"ovrd-cookie\": module.params[\"override_ovrd_cookie\"],\\n            \"ovrd-dur\": module.params[\"override_ovrd_dur\"],\\n            \"ovrd-dur-mode\": module.params[\"override_ovrd_dur_mode\"],\\n            \"ovrd-scope\": module.params[\"override_ovrd_scope\"],\\n            \"ovrd-user-group\": module.params[\"override_ovrd_user_group\"],\\n            \"profile\": module.params[\"override_profile\"],\\n            \"profile-attribute\": module.params[\"override_profile_attribute\"],\\n            \"profile-type\": module.params[\"override_profile_type\"],\\n        },\\n        \"url-extraction\": {\\n            \"redirect-header\": module.params[\"url_extraction_redirect_header\"],\\n            \"redirect-no-content\": module.params[\"url_extraction_redirect_no_content\"],\\n            \"redirect-url\": module.params[\"url_extraction_redirect_url\"],\\n            \"server-fqdn\": module.params[\"url_extraction_server_fqdn\"],\\n            \"status\": module.params[\"url_extraction_status\"],\\n        },\\n        \"web\": {\\n            \"blacklist\": module.params[\"web_blacklist\"],\\n            \"bword-table\": module.params[\"web_bword_table\"],\\n            \"bword-threshold\": module.params[\"web_bword_threshold\"],\\n            \"content-header-list\": module.params[\"web_content_header_list\"],\\n            \"keyword-match\": module.params[\"web_keyword_match\"],\\n            \"log-search\": module.params[\"web_log_search\"],\\n            \"safe-search\": module.params[\"web_safe_search\"],\\n            \"urlfilter-table\": module.params[\"web_urlfilter_table\"],\\n            \"whitelist\": module.params[\"web_whitelist\"],\\n            \"youtube-restrict\": module.params[\"web_youtube_restrict\"],\\n        },\\n        \"youtube-channel-filter\": {\\n            \"channel-id\": module.params[\"youtube_channel_filter_channel_id\"],\\n            \"comment\": module.params[\"youtube_channel_filter_comment\"],\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['ftgd-wf', 'override', 'url-extraction', 'web', 'youtube-channel-filter']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_webfilter_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        youtube_channel_status=dict(required=False, type=\"str\", choices=[\"disable\", \"blacklist\", \"whitelist\"]),\\n        wisp_servers=dict(required=False, type=\"str\"),\\n        wisp_algorithm=dict(required=False, type=\"str\", choices=[\"auto-learning\", \"primary-secondary\", \"round-robin\"]),\\n        wisp=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_url_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_invalid_domain_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_ftgd_quota_usage=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_ftgd_err_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_vbs_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_unknown_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_referer_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_jscript_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_js_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_cookie_removal_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_cookie_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_command_block_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_applet_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_filter_activex_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_extended_all_action_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_content_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        replacemsg_group=dict(required=False, type=\"str\"),\\n        post_action=dict(required=False, type=\"str\", choices=[\"normal\", \"block\"]),\\n        ovrd_perm=dict(required=False, type=\"list\", choices=[\"bannedword-override\",\\n                                                             \"urlfilter-override\",\\n                                                             \"fortiguard-wf-override\",\\n                                                             \"contenttype-check-override\"]),\\n        options=dict(required=False, type=\"list\", choices=[\"block-invalid-url\",\\n                                                           \"jscript\",\\n                                                           \"js\",\\n                                                           \"vbs\",\\n                                                           \"unknown\",\\n                                                           \"wf-referer\",\\n                                                           \"intrinsic\",\\n                                                           \"wf-cookie\",\\n                                                           \"per-user-bwl\",\\n                                                           \"activexfilter\",\\n                                                           \"cookiefilter\",\\n                                                           \"javafilter\"]),\\n        name=dict(required=False, type=\"str\"),\\n        log_all_url=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        inspection_mode=dict(required=False, type=\"str\", choices=[\"proxy\", \"flow-based\"]),\\n        https_replacemsg=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        extended_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        ftgd_wf=dict(required=False, type=\"list\"),\\n        ftgd_wf_exempt_quota=dict(required=False, type=\"str\"),\\n        ftgd_wf_max_quota_timeout=dict(required=False, type=\"int\"),\\n        ftgd_wf_options=dict(required=False, type=\"str\", choices=[\"error-allow\", \"rate-server-ip\",\\n                                                                  \"connect-request-bypass\", \"ftgd-disable\"]),\\n        ftgd_wf_ovrd=dict(required=False, type=\"str\"),\\n        ftgd_wf_rate_crl_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_rate_css_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_rate_image_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_rate_javascript_urls=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_filters_action=dict(required=False, type=\"str\", choices=[\"block\", \"monitor\",\\n                                                                         \"warning\", \"authenticate\"]),\\n        ftgd_wf_filters_auth_usr_grp=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_category=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftgd_wf_filters_override_replacemsg=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_warn_duration=dict(required=False, type=\"str\"),\\n        ftgd_wf_filters_warning_duration_type=dict(required=False, type=\"str\", choices=[\"session\", \"timeout\"]),\\n        ftgd_wf_filters_warning_prompt=dict(required=False, type=\"str\", choices=[\"per-domain\", \"per-category\"]),\\n        ftgd_wf_quota_category=dict(required=False, type=\"str\"),\\n        ftgd_wf_quota_duration=dict(required=False, type=\"str\"),\\n        ftgd_wf_quota_override_replacemsg=dict(required=False, type=\"str\"),\\n        ftgd_wf_quota_type=dict(required=False, type=\"str\", choices=[\"time\", \"traffic\"]),\\n        ftgd_wf_quota_unit=dict(required=False, type=\"str\", choices=[\"B\", \"KB\", \"MB\", \"GB\"]),\\n        ftgd_wf_quota_value=dict(required=False, type=\"int\"),\\n        override=dict(required=False, type=\"list\"),\\n        override_ovrd_cookie=dict(required=False, type=\"str\", choices=[\"deny\", \"allow\"]),\\n        override_ovrd_dur=dict(required=False, type=\"str\"),\\n        override_ovrd_dur_mode=dict(required=False, type=\"str\", choices=[\"constant\", \"ask\"]),\\n        override_ovrd_scope=dict(required=False, type=\"str\", choices=[\"user\", \"user-group\", \"ip\", \"ask\", \"browser\"]),\\n        override_ovrd_user_group=dict(required=False, type=\"str\"),\\n        override_profile=dict(required=False, type=\"str\"),\\n        override_profile_attribute=dict(required=False, type=\"list\", choices=[\"User-Name\",\\n                                                                              \"NAS-IP-Address\",\\n                                                                              \"Framed-IP-Address\",\\n                                                                              \"Framed-IP-Netmask\",\\n                                                                              \"Filter-Id\",\\n                                                                              \"Login-IP-Host\",\\n                                                                              \"Reply-Message\",\\n                                                                              \"Callback-Number\",\\n                                                                              \"Callback-Id\",\\n                                                                              \"Framed-Route\",\\n                                                                              \"Framed-IPX-Network\",\\n                                                                              \"Class\",\\n                                                                              \"Called-Station-Id\",\\n                                                                              \"Calling-Station-Id\",\\n                                                                              \"NAS-Identifier\",\\n                                                                              \"Proxy-State\",\\n                                                                              \"Login-LAT-Service\",\\n                                                                              \"Login-LAT-Node\",\\n                                                                              \"Login-LAT-Group\",\\n                                                                              \"Framed-AppleTalk-Zone\",\\n                                                                              \"Acct-Session-Id\",\\n                                                                              \"Acct-Multi-Session-Id\"]),\\n        override_profile_type=dict(required=False, type=\"str\", choices=[\"list\", \"radius\"]),\\n        url_extraction=dict(required=False, type=\"list\"),\\n        url_extraction_redirect_header=dict(required=False, type=\"str\"),\\n        url_extraction_redirect_no_content=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_extraction_redirect_url=dict(required=False, type=\"str\"),\\n        url_extraction_server_fqdn=dict(required=False, type=\"str\"),\\n        url_extraction_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web=dict(required=False, type=\"list\"),\\n        web_blacklist=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_bword_table=dict(required=False, type=\"str\"),\\n        web_bword_threshold=dict(required=False, type=\"int\"),\\n        web_content_header_list=dict(required=False, type=\"str\"),\\n        web_keyword_match=dict(required=False, type=\"str\"),\\n        web_log_search=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        web_safe_search=dict(required=False, type=\"str\", choices=[\"url\", \"header\"]),\\n        web_urlfilter_table=dict(required=False, type=\"str\"),\\n        web_whitelist=dict(required=False, type=\"list\", choices=[\"exempt-av\",\\n                                                                 \"exempt-webcontent\",\\n                                                                 \"exempt-activex-java-cookie\",\\n                                                                 \"exempt-dlp\",\\n                                                                 \"exempt-rangeblock\",\\n                                                                 \"extended-log-others\"]),\\n        web_youtube_restrict=dict(required=False, type=\"str\", choices=[\"strict\", \"none\", \"moderate\"]),\\n        youtube_channel_filter=dict(required=False, type=\"list\"),\\n        youtube_channel_filter_channel_id=dict(required=False, type=\"str\"),\\n        youtube_channel_filter_comment=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"youtube-channel-status\": module.params[\"youtube_channel_status\"],\\n        \"wisp-servers\": module.params[\"wisp_servers\"],\\n        \"wisp-algorithm\": module.params[\"wisp_algorithm\"],\\n        \"wisp\": module.params[\"wisp\"],\\n        \"web-url-log\": module.params[\"web_url_log\"],\\n        \"web-invalid-domain-log\": module.params[\"web_invalid_domain_log\"],\\n        \"web-ftgd-quota-usage\": module.params[\"web_ftgd_quota_usage\"],\\n        \"web-ftgd-err-log\": module.params[\"web_ftgd_err_log\"],\\n        \"web-filter-vbs-log\": module.params[\"web_filter_vbs_log\"],\\n        \"web-filter-unknown-log\": module.params[\"web_filter_unknown_log\"],\\n        \"web-filter-referer-log\": module.params[\"web_filter_referer_log\"],\\n        \"web-filter-jscript-log\": module.params[\"web_filter_jscript_log\"],\\n        \"web-filter-js-log\": module.params[\"web_filter_js_log\"],\\n        \"web-filter-cookie-removal-log\": module.params[\"web_filter_cookie_removal_log\"],\\n        \"web-filter-cookie-log\": module.params[\"web_filter_cookie_log\"],\\n        \"web-filter-command-block-log\": module.params[\"web_filter_command_block_log\"],\\n        \"web-filter-applet-log\": module.params[\"web_filter_applet_log\"],\\n        \"web-filter-activex-log\": module.params[\"web_filter_activex_log\"],\\n        \"web-extended-all-action-log\": module.params[\"web_extended_all_action_log\"],\\n        \"web-content-log\": module.params[\"web_content_log\"],\\n        \"replacemsg-group\": module.params[\"replacemsg_group\"],\\n        \"post-action\": module.params[\"post_action\"],\\n        \"ovrd-perm\": module.params[\"ovrd_perm\"],\\n        \"options\": module.params[\"options\"],\\n        \"name\": module.params[\"name\"],\\n        \"log-all-url\": module.params[\"log_all_url\"],\\n        \"inspection-mode\": module.params[\"inspection_mode\"],\\n        \"https-replacemsg\": module.params[\"https_replacemsg\"],\\n        \"extended-log\": module.params[\"extended_log\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"ftgd-wf\": {\\n            \"exempt-quota\": module.params[\"ftgd_wf_exempt_quota\"],\\n            \"max-quota-timeout\": module.params[\"ftgd_wf_max_quota_timeout\"],\\n            \"options\": module.params[\"ftgd_wf_options\"],\\n            \"ovrd\": module.params[\"ftgd_wf_ovrd\"],\\n            \"rate-crl-urls\": module.params[\"ftgd_wf_rate_crl_urls\"],\\n            \"rate-css-urls\": module.params[\"ftgd_wf_rate_css_urls\"],\\n            \"rate-image-urls\": module.params[\"ftgd_wf_rate_image_urls\"],\\n            \"rate-javascript-urls\": module.params[\"ftgd_wf_rate_javascript_urls\"],\\n            \"filters\": {\\n                \"action\": module.params[\"ftgd_wf_filters_action\"],\\n                \"auth-usr-grp\": module.params[\"ftgd_wf_filters_auth_usr_grp\"],\\n                \"category\": module.params[\"ftgd_wf_filters_category\"],\\n                \"log\": module.params[\"ftgd_wf_filters_log\"],\\n                \"override-replacemsg\": module.params[\"ftgd_wf_filters_override_replacemsg\"],\\n                \"warn-duration\": module.params[\"ftgd_wf_filters_warn_duration\"],\\n                \"warning-duration-type\": module.params[\"ftgd_wf_filters_warning_duration_type\"],\\n                \"warning-prompt\": module.params[\"ftgd_wf_filters_warning_prompt\"],\\n            },\\n            \"quota\": {\\n                \"category\": module.params[\"ftgd_wf_quota_category\"],\\n                \"duration\": module.params[\"ftgd_wf_quota_duration\"],\\n                \"override-replacemsg\": module.params[\"ftgd_wf_quota_override_replacemsg\"],\\n                \"type\": module.params[\"ftgd_wf_quota_type\"],\\n                \"unit\": module.params[\"ftgd_wf_quota_unit\"],\\n                \"value\": module.params[\"ftgd_wf_quota_value\"],\\n            },\\n        },\\n        \"override\": {\\n            \"ovrd-cookie\": module.params[\"override_ovrd_cookie\"],\\n            \"ovrd-dur\": module.params[\"override_ovrd_dur\"],\\n            \"ovrd-dur-mode\": module.params[\"override_ovrd_dur_mode\"],\\n            \"ovrd-scope\": module.params[\"override_ovrd_scope\"],\\n            \"ovrd-user-group\": module.params[\"override_ovrd_user_group\"],\\n            \"profile\": module.params[\"override_profile\"],\\n            \"profile-attribute\": module.params[\"override_profile_attribute\"],\\n            \"profile-type\": module.params[\"override_profile_type\"],\\n        },\\n        \"url-extraction\": {\\n            \"redirect-header\": module.params[\"url_extraction_redirect_header\"],\\n            \"redirect-no-content\": module.params[\"url_extraction_redirect_no_content\"],\\n            \"redirect-url\": module.params[\"url_extraction_redirect_url\"],\\n            \"server-fqdn\": module.params[\"url_extraction_server_fqdn\"],\\n            \"status\": module.params[\"url_extraction_status\"],\\n        },\\n        \"web\": {\\n            \"blacklist\": module.params[\"web_blacklist\"],\\n            \"bword-table\": module.params[\"web_bword_table\"],\\n            \"bword-threshold\": module.params[\"web_bword_threshold\"],\\n            \"content-header-list\": module.params[\"web_content_header_list\"],\\n            \"keyword-match\": module.params[\"web_keyword_match\"],\\n            \"log-search\": module.params[\"web_log_search\"],\\n            \"safe-search\": module.params[\"web_safe_search\"],\\n            \"urlfilter-table\": module.params[\"web_urlfilter_table\"],\\n            \"whitelist\": module.params[\"web_whitelist\"],\\n            \"youtube-restrict\": module.params[\"web_youtube_restrict\"],\\n        },\\n        \"youtube-channel-filter\": {\\n            \"channel-id\": module.params[\"youtube_channel_filter_channel_id\"],\\n            \"comment\": module.params[\"youtube_channel_filter_comment\"],\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['ftgd-wf', 'override', 'url-extraction', 'web', 'youtube-channel-filter']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_webfilter_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def _check_psd_eigenvalues(lambdas, enable_warnings=False):\\n    lambdas = np.array(lambdas)\\n    is_double_precision = lambdas.dtype == np.float64\\n    significant_imag_ratio = 1e-5\\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\\n    small_pos_ratio = 1e-12\\n    if not np.isreal(lambdas).all():\\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\\n        max_real_abs = np.abs(np.real(lambdas)).max()\\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\\n            raise ValueError(\\n                \"There are significant imaginary parts in eigenvalues (%g \"\\n                \"of the maximum real part). Either the matrix is not PSD, or \"\\n                \"there was an issue while computing the eigendecomposition \"\\n                \"of the matrix.\"\\n                % (max_imag_abs / max_real_abs))\\n        if enable_warnings:\\n            warnings.warn(\"There are imaginary parts in eigenvalues (%g \"\\n                          \"of the maximum real part). Either the matrix is not\"\\n                          \" PSD, or there was an issue while computing the \"\\n                          \"eigendecomposition of the matrix. Only the real \"\\n                          \"parts will be kept.\"\\n                          % (max_imag_abs / max_real_abs),\\n                          PositiveSpectrumWarning)\\n    lambdas = np.real(lambdas)\\n    max_eig = lambdas.max()\\n    if max_eig < 0:\\n        raise ValueError(\"All eigenvalues are negative (maximum is %g). \"\\n                         \"Either the matrix is not PSD, or there was an \"\\n                         \"issue while computing the eigendecomposition of \"\\n                         \"the matrix.\" % max_eig)\\n    else:\\n        min_eig = lambdas.min()\\n        if (min_eig < -significant_neg_ratio * max_eig\\n                and min_eig < -significant_neg_value):\\n            raise ValueError(\"There are significant negative eigenvalues (%g\"\\n                             \" of the maximum positive). Either the matrix is \"\\n                             \"not PSD, or there was an issue while computing \"\\n                             \"the eigendecomposition of the matrix.\"\\n                             % (-min_eig / max_eig))\\n        elif min_eig < 0:\\n            if enable_warnings:\\n                warnings.warn(\"There are negative eigenvalues (%g of the \"\\n                              \"maximum positive). Either the matrix is not \"\\n                              \"PSD, or there was an issue while computing the\"\\n                              \" eigendecomposition of the matrix. Negative \"\\n                              \"eigenvalues will be replaced with 0.\"\\n                              % (-min_eig / max_eig),\\n                              PositiveSpectrumWarning)\\n            lambdas[lambdas < 0] = 0\\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\\n    if too_small_lambdas.any():\\n        if enable_warnings:\\n            warnings.warn(\"Badly conditioned PSD matrix spectrum: the largest \"\\n                          \"eigenvalue is more than %g times the smallest. \"\\n                          \"Small eigenvalues will be replaced with 0.\"\\n                          \"\" % (1 / small_pos_ratio),\\n                          PositiveSpectrumWarning)\\n        lambdas[too_small_lambdas] = 0\\n    return lambdas",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX use specific threshold to discard eigenvalues with 32 bits fp(#18149)",
    "fixed_code": "def _check_psd_eigenvalues(lambdas, enable_warnings=False):\\n    lambdas = np.array(lambdas)\\n    is_double_precision = lambdas.dtype == np.float64\\n    significant_imag_ratio = 1e-5\\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\\n    small_pos_ratio = 1e-12 if is_double_precision else 1e-7\\n    if not np.isreal(lambdas).all():\\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\\n        max_real_abs = np.abs(np.real(lambdas)).max()\\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\\n            raise ValueError(\\n                \"There are significant imaginary parts in eigenvalues (%g \"\\n                \"of the maximum real part). Either the matrix is not PSD, or \"\\n                \"there was an issue while computing the eigendecomposition \"\\n                \"of the matrix.\"\\n                % (max_imag_abs / max_real_abs))\\n        if enable_warnings:\\n            warnings.warn(\"There are imaginary parts in eigenvalues (%g \"\\n                          \"of the maximum real part). Either the matrix is not\"\\n                          \" PSD, or there was an issue while computing the \"\\n                          \"eigendecomposition of the matrix. Only the real \"\\n                          \"parts will be kept.\"\\n                          % (max_imag_abs / max_real_abs),\\n                          PositiveSpectrumWarning)\\n    lambdas = np.real(lambdas)\\n    max_eig = lambdas.max()\\n    if max_eig < 0:\\n        raise ValueError(\"All eigenvalues are negative (maximum is %g). \"\\n                         \"Either the matrix is not PSD, or there was an \"\\n                         \"issue while computing the eigendecomposition of \"\\n                         \"the matrix.\" % max_eig)\\n    else:\\n        min_eig = lambdas.min()\\n        if (min_eig < -significant_neg_ratio * max_eig\\n                and min_eig < -significant_neg_value):\\n            raise ValueError(\"There are significant negative eigenvalues (%g\"\\n                             \" of the maximum positive). Either the matrix is \"\\n                             \"not PSD, or there was an issue while computing \"\\n                             \"the eigendecomposition of the matrix.\"\\n                             % (-min_eig / max_eig))\\n        elif min_eig < 0:\\n            if enable_warnings:\\n                warnings.warn(\"There are negative eigenvalues (%g of the \"\\n                              \"maximum positive). Either the matrix is not \"\\n                              \"PSD, or there was an issue while computing the\"\\n                              \" eigendecomposition of the matrix. Negative \"\\n                              \"eigenvalues will be replaced with 0.\"\\n                              % (-min_eig / max_eig),\\n                              PositiveSpectrumWarning)\\n            lambdas[lambdas < 0] = 0\\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\\n    if too_small_lambdas.any():\\n        if enable_warnings:\\n            warnings.warn(\"Badly conditioned PSD matrix spectrum: the largest \"\\n                          \"eigenvalue is more than %g times the smallest. \"\\n                          \"Small eigenvalues will be replaced with 0.\"\\n                          \"\" % (1 / small_pos_ratio),\\n                          PositiveSpectrumWarning)\\n        lambdas[too_small_lambdas] = 0\\n    return lambdas"
  },
  {
    "code": "def quote(c):\\n    return _QUOPRI_MAP[ord(c)]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quote(c):\\n    return _QUOPRI_MAP[ord(c)]"
  },
  {
    "code": "def _queue_management_worker(executor_reference,\\n                             processes,\\n                             pending_work_items,\\n                             work_ids_queue,\\n                             call_queue,\\n                             result_queue):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #12456: fix a possible hang on shutdown of a concurrent.futures.ProcessPoolExecutor.",
    "fixed_code": "def _queue_management_worker(executor_reference,\\n                             processes,\\n                             pending_work_items,\\n                             work_ids_queue,\\n                             call_queue,\\n                             result_queue):\\n    executor = None"
  },
  {
    "code": "def _findLib_gcc(name):\\n        expr = os.fsencode(r'[^\\(\\)\\s]*lib%s\\.[^\\(\\)\\s]*' % re.escape(name))\\n        c_compiler = shutil.which('gcc')\\n        if not c_compiler:\\n            c_compiler = shutil.which('cc')\\n        if not c_compiler:\\n            return None\\n        temp = tempfile.NamedTemporaryFile()\\n        try:\\n            args = [c_compiler, '-Wl,-t', '-o', temp.name, '-l' + name]\\n            env = dict(os.environ)\\n            env['LC_ALL'] = 'C'\\n            env['LANG'] = 'C'\\n            proc = subprocess.Popen(args,\\n                                    stdout=subprocess.PIPE,\\n                                    stderr=subprocess.STDOUT,\\n                                    env=env)\\n            with proc:\\n                trace = proc.stdout.read()\\n        finally:\\n            try:\\n                temp.close()\\n            except FileNotFoundError:\\n                pass\\n        res = re.search(expr, trace)\\n        if not res:\\n            return None\\n        return os.fsdecode(res.group(0))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _findLib_gcc(name):\\n        expr = os.fsencode(r'[^\\(\\)\\s]*lib%s\\.[^\\(\\)\\s]*' % re.escape(name))\\n        c_compiler = shutil.which('gcc')\\n        if not c_compiler:\\n            c_compiler = shutil.which('cc')\\n        if not c_compiler:\\n            return None\\n        temp = tempfile.NamedTemporaryFile()\\n        try:\\n            args = [c_compiler, '-Wl,-t', '-o', temp.name, '-l' + name]\\n            env = dict(os.environ)\\n            env['LC_ALL'] = 'C'\\n            env['LANG'] = 'C'\\n            proc = subprocess.Popen(args,\\n                                    stdout=subprocess.PIPE,\\n                                    stderr=subprocess.STDOUT,\\n                                    env=env)\\n            with proc:\\n                trace = proc.stdout.read()\\n        finally:\\n            try:\\n                temp.close()\\n            except FileNotFoundError:\\n                pass\\n        res = re.search(expr, trace)\\n        if not res:\\n            return None\\n        return os.fsdecode(res.group(0))"
  },
  {
    "code": "def init_tok2vec(\\n\\tnlp: \"Language\", pretrain_config: Dict[str, Any], init_config: Dict[str, Any]\\n) -> bool:\\n\\tP = pretrain_config\\n\\tI = init_config\\n\\tweights_data = None\\n\\tinit_tok2vec = ensure_path(I[\"init_tok2vec\"])\\n\\tif init_tok2vec is not None:\\n\\t\\tif not init_tok2vec.exists():\\n\\t\\t\\terr = f\"can't find pretrained tok2vec: {init_tok2vec}\"\\n\\t\\t\\terrors = [{\"loc\": [\"initialize\", \"init_tok2vec\"], \"msg\": err}]\\n\\t\\t\\traise ConfigValidationError(config=nlp.config, errors=errors)\\n\\t\\twith init_tok2vec.open(\"rb\") as file_:\\n\\t\\t\\tweights_data = file_.read()\\n\\tif weights_data is not None:\\n\\t\\tlayer = get_tok2vec_ref(nlp, P)\\n\\t\\tlayer.from_bytes(weights_data)\\n\\t\\tlogger.info(f\"Loaded pretrained weights from {init_tok2vec}\")\\n\\t\\treturn True\\n\\treturn False",
    "label": 1,
    "bug_type": "perf",
    "bug_description": "Have logging calls use string formatting types (#12215)",
    "fixed_code": "def init_tok2vec(\\n\\tnlp: \"Language\", pretrain_config: Dict[str, Any], init_config: Dict[str, Any]\\n) -> bool:\\n\\tP = pretrain_config\\n\\tI = init_config\\n\\tweights_data = None\\n\\tinit_tok2vec = ensure_path(I[\"init_tok2vec\"])\\n\\tif init_tok2vec is not None:\\n\\t\\tif not init_tok2vec.exists():\\n\\t\\t\\terr = f\"can't find pretrained tok2vec: {init_tok2vec}\"\\n\\t\\t\\terrors = [{\"loc\": [\"initialize\", \"init_tok2vec\"], \"msg\": err}]\\n\\t\\t\\traise ConfigValidationError(config=nlp.config, errors=errors)\\n\\t\\twith init_tok2vec.open(\"rb\") as file_:\\n\\t\\t\\tweights_data = file_.read()\\n\\tif weights_data is not None:\\n\\t\\tlayer = get_tok2vec_ref(nlp, P)\\n\\t\\tlayer.from_bytes(weights_data)\\n\\t\\tlogger.info(\"Loaded pretrained weights from %s\", init_tok2vec)\\n\\t\\treturn True\\n\\treturn False"
  },
  {
    "code": "def squeeze(self):\\n\\t\\ttry:\\n\\t\\t\\treturn self.ix[tuple([slice(None) if len(a) > 1 else a[0]\\n\\t\\t\\t\\t\\t\\t\\t\\t  for a in self.axes])]\\n\\t\\texcept:\\n\\t\\t\\treturn self",
    "label": 1,
    "bug_type": "corruption - memory",
    "bug_description": "BUG: squeeze works on 0 length arrays, #11299, #8999",
    "fixed_code": "def squeeze(self):\\n\\t\\ttry:\\n\\t\\t\\treturn self.iloc[tuple([0 if len(a) == 1 else slice(None)\\n\\t\\t\\t\\t\\t\\t\\t\\t  for a in self.axes])]\\n\\t\\texcept:\\n\\t\\t\\treturn self"
  },
  {
    "code": "def sitemap(request, sitemaps, section=None,\\n            template_name='sitemap.xml', content_type='application/xml'):\\n    req_protocol = request.scheme\\n    req_site = get_current_site(request)\\n    if section is not None:\\n        if section not in sitemaps:\\n            raise Http404(\"No sitemap available for section: %r\" % section)\\n        maps = [sitemaps[section]]\\n    else:\\n        maps = list(six.itervalues(sitemaps))\\n    page = request.GET.get(\"p\", 1)\\n    urls = []\\n    for site in maps:\\n        try:\\n            if callable(site):\\n                site = site()\\n            urls.extend(site.get_urls(page=page, site=req_site,\\n                                      protocol=req_protocol))\\n        except EmptyPage:\\n            raise Http404(\"Page %s empty\" % page)\\n        except PageNotAnInteger:\\n            raise Http404(\"No page '%s'\" % page)\\n    response = TemplateResponse(request, template_name, {'urlset': urls},\\n                                content_type=content_type)\\n    if hasattr(site, 'latest_lastmod'):\\n        response['Last-Modified'] = http_date(\\n            timegm(site.latest_lastmod.utctimetuple()))\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #23403 -- Fixed crash in contrib.sitemaps if lastmod returned a date rather than datetime.\\n\\nThanks igorcc for the report.",
    "fixed_code": "def sitemap(request, sitemaps, section=None,\\n            template_name='sitemap.xml', content_type='application/xml'):\\n    req_protocol = request.scheme\\n    req_site = get_current_site(request)\\n    if section is not None:\\n        if section not in sitemaps:\\n            raise Http404(\"No sitemap available for section: %r\" % section)\\n        maps = [sitemaps[section]]\\n    else:\\n        maps = list(six.itervalues(sitemaps))\\n    page = request.GET.get(\"p\", 1)\\n    urls = []\\n    for site in maps:\\n        try:\\n            if callable(site):\\n                site = site()\\n            urls.extend(site.get_urls(page=page, site=req_site,\\n                                      protocol=req_protocol))\\n        except EmptyPage:\\n            raise Http404(\"Page %s empty\" % page)\\n        except PageNotAnInteger:\\n            raise Http404(\"No page '%s'\" % page)\\n    response = TemplateResponse(request, template_name, {'urlset': urls},\\n                                content_type=content_type)\\n    if hasattr(site, 'latest_lastmod'):\\n        lastmod = site.latest_lastmod\\n        response['Last-Modified'] = http_date(\\n            timegm(\\n                lastmod.utctimetuple() if isinstance(lastmod, datetime.datetime)\\n                else lastmod.timetuple()\\n            )\\n        )\\n    return response"
  },
  {
    "code": "def getsourcelines(obj):\\n    lines, lineno = inspect.findsource(obj)\\n    if inspect.isframe(obj) and obj.f_globals is obj.f_locals:\\n        return lines, 1\\n    elif inspect.ismodule(obj):\\n        return lines, 1\\n    return inspect.getblock(lines[lineno:]), lineno+1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getsourcelines(obj):\\n    lines, lineno = inspect.findsource(obj)\\n    if inspect.isframe(obj) and obj.f_globals is obj.f_locals:\\n        return lines, 1\\n    elif inspect.ismodule(obj):\\n        return lines, 1\\n    return inspect.getblock(lines[lineno:]), lineno+1"
  },
  {
    "code": "def webserver(args):\\n    print(settings.HEADER)\\n    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')\\n    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')\\n    num_workers = args.workers or conf.get('webserver', 'workers')\\n    worker_timeout = (args.worker_timeout or\\n                      conf.get('webserver', 'web_server_worker_timeout'))\\n    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')\\n    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')\\n    if not ssl_cert and ssl_key:\\n        raise AirflowException(\\n            'An SSL certificate must also be provided for use with ' + ssl_key)\\n    if ssl_cert and not ssl_key:\\n        raise AirflowException(\\n            'An SSL key must also be provided for use with ' + ssl_cert)\\n    if args.debug:\\n        print(\\n            \"Starting the web server on port {0} and host {1}.\".format(\\n                args.port, args.hostname))\\n        app = create_app(testing=conf.getboolean('core', 'unit_test_mode'))\\n        app.run(debug=True, use_reloader=not app.config['TESTING'],\\n                port=args.port, host=args.hostname,\\n                ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None)\\n    else:\\n        os.environ['SKIP_DAGS_PARSING'] = 'True'\\n        app = cached_app(None)\\n        os.environ.pop('SKIP_DAGS_PARSING')\\n        pid_file, stdout, stderr, log_file = setup_locations(\\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file)\\n        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\\n        print(\\n            textwrap.dedent(''.format(num_workers=num_workers, workerclass=args.workerclass,\\n                       hostname=args.hostname, port=args.port,\\n                       worker_timeout=worker_timeout, access_logfile=access_logfile,\\n                       error_logfile=error_logfile)))\\n        run_args = [\\n            'gunicorn',\\n            '-w', str(num_workers),\\n            '-k', str(args.workerclass),\\n            '-t', str(worker_timeout),\\n            '-b', args.hostname + ':' + str(args.port),\\n            '-n', 'airflow-webserver',\\n            '-p', pid_file,\\n            '-c', 'python:airflow.www.gunicorn_config',\\n        ]\\n        if args.access_logfile:\\n            run_args += ['--access-logfile', str(args.access_logfile)]\\n        if args.error_logfile:\\n            run_args += ['--error-logfile', str(args.error_logfile)]\\n        if args.daemon:\\n            run_args += ['-D']\\n        if ssl_cert:\\n            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]\\n        run_args += [\"airflow.www.app:cached_app()\"]\\n        gunicorn_master_proc = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Reload gunicorn when plugins has beeen changed (#8997)",
    "fixed_code": "def webserver(args):\\n    print(settings.HEADER)\\n    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')\\n    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')\\n    num_workers = args.workers or conf.get('webserver', 'workers')\\n    worker_timeout = (args.worker_timeout or\\n                      conf.get('webserver', 'web_server_worker_timeout'))\\n    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')\\n    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')\\n    if not ssl_cert and ssl_key:\\n        raise AirflowException(\\n            'An SSL certificate must also be provided for use with ' + ssl_key)\\n    if ssl_cert and not ssl_key:\\n        raise AirflowException(\\n            'An SSL key must also be provided for use with ' + ssl_cert)\\n    if args.debug:\\n        print(\\n            \"Starting the web server on port {0} and host {1}.\".format(\\n                args.port, args.hostname))\\n        app = create_app(testing=conf.getboolean('core', 'unit_test_mode'))\\n        app.run(debug=True, use_reloader=not app.config['TESTING'],\\n                port=args.port, host=args.hostname,\\n                ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None)\\n    else:\\n        os.environ['SKIP_DAGS_PARSING'] = 'True'\\n        app = cached_app(None)\\n        os.environ.pop('SKIP_DAGS_PARSING')\\n        pid_file, stdout, stderr, log_file = setup_locations(\\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file)\\n        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\\n        print(\\n            textwrap.dedent(''.format(num_workers=num_workers, workerclass=args.workerclass,\\n                       hostname=args.hostname, port=args.port,\\n                       worker_timeout=worker_timeout, access_logfile=access_logfile,\\n                       error_logfile=error_logfile)))\\n        run_args = [\\n            'gunicorn',\\n            '--workers', str(num_workers),\\n            '--worker-class', str(args.workerclass),\\n            '--timeout', str(worker_timeout),\\n            '--bind', args.hostname + ':' + str(args.port),\\n            '--name', 'airflow-webserver',\\n            '--pid', pid_file,\\n            '--config', 'python:airflow.www.gunicorn_config',\\n        ]\\n        if args.access_logfile:\\n            run_args += ['--access-logfile', str(args.access_logfile)]\\n        if args.error_logfile:\\n            run_args += ['--error-logfile', str(args.error_logfile)]\\n        if args.daemon:\\n            run_args += ['--daemon']\\n        if ssl_cert:\\n            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]\\n        run_args += [\"airflow.www.app:cached_app()\"]\\n        gunicorn_master_proc = None"
  },
  {
    "code": "def get_libraries (self, ext):\\n        if sys.platform == \"win32\":\\n            from distutils.msvccompiler import MSVCCompiler\\n            if not isinstance(self.compiler, MSVCCompiler):\\n                template = \"python%d%d\"\\n                if self.debug:\\n                    template = template + '_d'\\n                pythonlib = (template %\\n                       (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries\\n        elif sys.platform == \"os2emx\":\\n            template = \"python%d%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"cygwin\":\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"atheos\":\\n            from distutils import sysconfig\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            extra = []\\n            for lib in sysconfig.get_config_var('SHLIBS').split():\\n                if lib.startswith('-l'):\\n                    extra.append(lib[2:])\\n                else:\\n                    extra.append(lib)\\n            return ext.libraries + [pythonlib, \"m\"] + extra\\n        elif sys.platform == 'darwin':\\n            return ext.libraries\\n        else:\\n            from distutils import sysconfig\\n            if sysconfig.get_config_var('Py_ENABLE_SHARED'):\\n                template = \"python%d.%d\"\\n                pythonlib = (template %\\n                             (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_libraries (self, ext):\\n        if sys.platform == \"win32\":\\n            from distutils.msvccompiler import MSVCCompiler\\n            if not isinstance(self.compiler, MSVCCompiler):\\n                template = \"python%d%d\"\\n                if self.debug:\\n                    template = template + '_d'\\n                pythonlib = (template %\\n                       (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries\\n        elif sys.platform == \"os2emx\":\\n            template = \"python%d%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"cygwin\":\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"atheos\":\\n            from distutils import sysconfig\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            extra = []\\n            for lib in sysconfig.get_config_var('SHLIBS').split():\\n                if lib.startswith('-l'):\\n                    extra.append(lib[2:])\\n                else:\\n                    extra.append(lib)\\n            return ext.libraries + [pythonlib, \"m\"] + extra\\n        elif sys.platform == 'darwin':\\n            return ext.libraries\\n        else:\\n            from distutils import sysconfig\\n            if sysconfig.get_config_var('Py_ENABLE_SHARED'):\\n                template = \"python%d.%d\"\\n                pythonlib = (template %\\n                             (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries"
  },
  {
    "code": "def forward(self, x, y):\\t\\t\\t\\treturn x.repeat(y.size()[0] / 2, y.size()[1] * 2)",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "Updates tests for integer division deprecation (#38621)\\n\\nSummary:\\nUpdates our tests in preparation of integer division using torch.div and torch.addcdiv throwing a runtime error by avoiding integer division using torch.div. This creates a brief period where integer division using torch.div is untested, but that should be OK (since it will soon throw a runtime error).\\n\\n\\nDifferential Revision: D21612823\\n\\nPulled By: mruberry\\n\\nfbshipit-source-id: 749c03a69feae02590b4395335163d9bf047e162",
    "fixed_code": "def forward(self, input):\\n\\t\\t\\t\\treturn self.many_fc(input)\\n\\t\\tmodel = MyModel()\\n\\t\\tinput = torch.randn(3, 4, requires_grad=True)\\n\\t\\tself.run_model_test(model, train=False, batch_size=0, input=input)"
  },
  {
    "code": "def get_features_used(node: Node) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n    return features",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_features_used(node: Node) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n    return features"
  },
  {
    "code": "def load_binput(self):\\n        i = self.read(1)[0]\\n        if i < 0:\\n            raise ValueError(\"negative BINPUT argument\")\\n        self.memo[i] = self.stack[-1]\\n    dispatch[BINPUT[0]] = load_binput",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_binput(self):\\n        i = self.read(1)[0]\\n        if i < 0:\\n            raise ValueError(\"negative BINPUT argument\")\\n        self.memo[i] = self.stack[-1]\\n    dispatch[BINPUT[0]] = load_binput"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        vdom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"execute\", \"set\", \"delete\"], type=\"str\", default=\"add\"),\\n        script_name=dict(required=True, type=\"str\"),\\n        script_type=dict(required=False, type=\"str\"),\\n        script_target=dict(required=False, type=\"str\"),\\n        script_description=dict(required=False, type=\"str\"),\\n        script_content=dict(required=False, type=\"str\"),\\n        script_scope=dict(required=False, type=\"str\"),\\n        script_package=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"script_name\": module.params[\"script_name\"],\\n        \"script_type\": module.params[\"script_type\"],\\n        \"script_target\": module.params[\"script_target\"],\\n        \"script_description\": module.params[\"script_description\"],\\n        \"script_content\": module.params[\"script_content\"],\\n        \"script_scope\": module.params[\"script_scope\"],\\n        \"script_package\": module.params[\"script_package\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"vdom\": module.params[\"vdom\"],\\n        \"mode\": module.params[\"mode\"],\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"mode\"] in ['add', 'set']:\\n            results = set_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"mode\"] == \"execute\":\\n            results = execute_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"mode\"] == \"delete\":\\n            results = delete_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        vdom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"execute\", \"set\", \"delete\"], type=\"str\", default=\"add\"),\\n        script_name=dict(required=True, type=\"str\"),\\n        script_type=dict(required=False, type=\"str\"),\\n        script_target=dict(required=False, type=\"str\"),\\n        script_description=dict(required=False, type=\"str\"),\\n        script_content=dict(required=False, type=\"str\"),\\n        script_scope=dict(required=False, type=\"str\"),\\n        script_package=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"script_name\": module.params[\"script_name\"],\\n        \"script_type\": module.params[\"script_type\"],\\n        \"script_target\": module.params[\"script_target\"],\\n        \"script_description\": module.params[\"script_description\"],\\n        \"script_content\": module.params[\"script_content\"],\\n        \"script_scope\": module.params[\"script_scope\"],\\n        \"script_package\": module.params[\"script_package\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"vdom\": module.params[\"vdom\"],\\n        \"mode\": module.params[\"mode\"],\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"mode\"] in ['add', 'set']:\\n            results = set_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"mode\"] == \"execute\":\\n            results = execute_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"mode\"] == \"delete\":\\n            results = delete_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def cascaded_union(self):\\n        \"Returns a cascaded union of this MultiPolygon.\"\\n        return GEOSGeometry(capi.geos_cascaded_union(self.ptr), self.srid)\\nGeometryCollection._allowed = (Point, LineString, LinearRing, Polygon, MultiPoint, MultiLineString, MultiPolygon)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cascaded_union(self):\\n        \"Returns a cascaded union of this MultiPolygon.\"\\n        return GEOSGeometry(capi.geos_cascaded_union(self.ptr), self.srid)\\nGeometryCollection._allowed = (Point, LineString, LinearRing, Polygon, MultiPoint, MultiLineString, MultiPolygon)"
  },
  {
    "code": "def sort(self, *args, **kwargs):\\n        raise Exception('Cannot sort an Index object')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Make core/index exceptions more descriptive\\n\\n  produce copies that are not the same object. (uses `assert_almost_equal`\\n  under the hood).\\n  after iterable check)",
    "fixed_code": "def sort(self, *args, **kwargs):\\n        raise TypeError('Cannot sort an %r object' % self.__class__.__name__)"
  },
  {
    "code": "def _isnull_ndarraylike(obj):\\n    from pandas import Series\\n    values = np.asarray(obj)\\n    if values.dtype.kind in ('O', 'S', 'U'):\\n        shape = values.shape\\n        if values.dtype.kind in ('S', 'U'):\\n            result = np.zeros(values.shape, dtype=bool)\\n        else:\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(values.ravel())\\n            result[:] = vec.reshape(shape)\\n        if isinstance(obj, Series):\\n            result = Series(result, index=obj.index, copy=False)\\n    elif values.dtype == np.dtype('M8[ns]'):\\n        result = values.view('i8') == tslib.iNaT\\n    elif issubclass(values.dtype.type, np.timedelta64):\\n        result = -np.isfinite(values.view('i8'))\\n    else:\\n        result = -np.isfinite(obj)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: doc for inf_as_null. more testing to ensure inf not excluded by default. #2477",
    "fixed_code": "def _isnull_ndarraylike(obj):\\n    from pandas import Series\\n    values = np.asarray(obj)\\n    if values.dtype.kind in ('O', 'S', 'U'):\\n        shape = values.shape\\n        if values.dtype.kind in ('S', 'U'):\\n            result = np.zeros(values.shape, dtype=bool)\\n        else:\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(values.ravel())\\n            result[:] = vec.reshape(shape)\\n        if isinstance(obj, Series):\\n            result = Series(result, index=obj.index, copy=False)\\n    elif values.dtype == np.dtype('M8[ns]'):\\n        result = values.view('i8') == tslib.iNaT\\n    elif issubclass(values.dtype.type, np.timedelta64):\\n        result = np.ones(values.shape, dtype=bool)\\n    else:\\n        result = np.isnan(obj)\\n    return result"
  },
  {
    "code": "def fit(self, X, y=None):\\n        self._validate_remainder(X)\\n        self._validate_transformers()\\n        transformers = self._fit_transform(X, y, _fit_one_transformer)\\n        self._update_fitted_transformers(transformers)\\n        return self\\n    def fit_transform(self, X, y=None):\\n        self._validate_remainder(X)\\n        self._validate_transformers()\\n        result = self._fit_transform(X, y, _fit_transform_one)\\n        if not result:\\n            return np.zeros((X.shape[0], 0))\\n        Xs, transformers = zip(*result)\\n        self._update_fitted_transformers(transformers)\\n        self._validate_output(Xs)\\n        return _hstack(list(Xs))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add sparse_threshold keyword to ColumnTransformer (#11614)\\n\\nReasoning: when eg OneHotEncoder is used as part of ColumnTransformer it would cause the final result to be a sparse matrix. As this is a typical case when we have mixed dtype data, it means that many pipeline will have to deal with sparse data implicitly, even if you have only some categorical features with low cardinality.",
    "fixed_code": "def fit(self, X, y=None):\\n        self.fit_transform(X, y=y)\\n        return self\\n    def fit_transform(self, X, y=None):\\n        self._validate_remainder(X)\\n        self._validate_transformers()\\n        result = self._fit_transform(X, y, _fit_transform_one)\\n        if not result:\\n            self._update_fitted_transformers([])\\n            return np.zeros((X.shape[0], 0))\\n        Xs, transformers = zip(*result)\\n        if all(sparse.issparse(X) for X in Xs):\\n            self.sparse_output_ = True\\n        elif any(sparse.issparse(X) for X in Xs):\\n            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)\\n            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)\\n                        else X.size for X in Xs)\\n            density = nnz / total\\n            self.sparse_output_ = density < self.sparse_threshold\\n        else:\\n            self.sparse_output_ = False\\n        self._update_fitted_transformers(transformers)\\n        self._validate_output(Xs)\\n        return _hstack(list(Xs), self.sparse_output_)"
  },
  {
    "code": "def connection_made(self, transport):\\n        self._transport = transport\\n        if transport.get_pipe_transport(1):\\n            self.stdout = streams.StreamReader(limit=self._limit,\\n                                               loop=self._loop)\\n        if transport.get_pipe_transport(2):\\n            self.stderr = streams.StreamReader(limit=self._limit,\\n                                               loop=self._loop)\\n        stdin = transport.get_pipe_transport(0)\\n        if stdin is not None:\\n            self.stdin = streams.StreamWriter(stdin,\\n                                              protocol=self,\\n                                              reader=None,\\n                                              loop=self._loop)\\n        self.waiter.set_result(None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Closes #22685, asyncio: Set the transport of stdout and stderr StreamReader objects in the SubprocessStreamProtocol. It allows to pause the transport to not buffer too much stdout or stderr data.",
    "fixed_code": "def connection_made(self, transport):\\n        self._transport = transport\\n        stdout_transport = transport.get_pipe_transport(1)\\n        if stdout_transport is not None:\\n            self.stdout = streams.StreamReader(limit=self._limit,\\n                                               loop=self._loop)\\n            self.stdout.set_transport(stdout_transport)\\n        stderr_transport = transport.get_pipe_transport(2)\\n        if stderr_transport is not None:\\n            self.stderr = streams.StreamReader(limit=self._limit,\\n                                               loop=self._loop)\\n            self.stderr.set_transport(stderr_transport)\\n        stdin_transport = transport.get_pipe_transport(0)\\n        if stdin_transport is not None:\\n            self.stdin = streams.StreamWriter(stdin_transport,\\n                                              protocol=self,\\n                                              reader=None,\\n                                              loop=self._loop)\\n        self.waiter.set_result(None)"
  },
  {
    "code": "def plot_frame(frame=None, subplots=False, sharex=True, sharey=False,\\n               use_index=True,\\n               figsize=None, grid=True, legend=True, rot=None,\\n               ax=None, title=None,\\n               xlim=None, ylim=None, logy=False,\\n               xticks=None, yticks=None,\\n               kind='line',\\n               sort_columns=False, fontsize=None, **kwds):\\n    kind = kind.lower().strip()\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logy=logy,\\n                     sort_columns=sort_columns, **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: alias density to kde in plot methods #1343",
    "fixed_code": "def plot_frame(frame=None, subplots=False, sharex=True, sharey=False,\\n               use_index=True,\\n               figsize=None, grid=True, legend=True, rot=None,\\n               ax=None, title=None,\\n               xlim=None, ylim=None, logy=False,\\n               xticks=None, yticks=None,\\n               kind='line',\\n               sort_columns=False, fontsize=None, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logy=logy,\\n                     sort_columns=sort_columns, **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]"
  },
  {
    "code": "def remove_omit(task_args, omit_token):\\n    ''\\n    new_args = {}\\n    for i in iteritems(task_args):\\n        if i[1] == omit_token:\\n            continue\\n        elif isinstance(i[1], dict):\\n            new_args[i[0]] = remove_omit(i[1], omit_token)\\n        else:\\n            new_args[i[0]] = i[1]\\n    return new_args",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Support list of dicts with omit. Fixes #45907 (#45923)",
    "fixed_code": "def remove_omit(task_args, omit_token):\\n    ''\\n    if not isinstance(task_args, dict):\\n        return task_args\\n    new_args = {}\\n    for i in iteritems(task_args):\\n        if i[1] == omit_token:\\n            continue\\n        elif isinstance(i[1], dict):\\n            new_args[i[0]] = remove_omit(i[1], omit_token)\\n        elif isinstance(i[1], list):\\n            new_args[i[0]] = [remove_omit(v, omit_token) for v in i[1]]\\n        else:\\n            new_args[i[0]] = i[1]\\n    return new_args"
  },
  {
    "code": "def time_center(self, dtype):\\n        self.s.str.center(100)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_center(self, dtype):\\n        self.s.str.center(100)"
  },
  {
    "code": "def path_exists(self, path: str) -> bool:\\n        conn = self.get_conn()\\n        return conn.exists(path)\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def path_exists(self, path: str) -> bool:\\n        conn = self.get_conn()\\n        return conn.exists(path)\\n    @staticmethod"
  },
  {
    "code": "def format_exception_only(etype, value):\\n    list = []\\n    if type(etype) == types.ClassType:\\n        stype = etype.__name__\\n    else:\\n        stype = etype\\n    if value is None:\\n        list.append(str(stype) + '\\n')\\n    else:\\n        if etype is SyntaxError:\\n            try:\\n                msg, (filename, lineno, offset, line) = value\\n            except:\\n                pass\\n            else:\\n                if not filename: filename = \"<string>\"\\n                list.append('  File \"%s\", line %d\\n' %\\n                            (filename, lineno))\\n                if line is not None:\\n                    i = 0\\n                    while i < len(line) and line[i].isspace():\\n                        i = i+1\\n                    list.append('    %s\\n' % line.strip())\\n                    if offset is not None:\\n                        s = '    '\\n                        for c in line[i:offset-1]:\\n                            if c.isspace():\\n                                s = s + c\\n                            else:\\n                                s = s + ' '\\n                        list.append('%s^\\n' % s)\\n                    value = msg\\n        s = _some_str(value)\\n        if s:\\n            list.append('%s: %s\\n' % (str(stype), s))\\n        else:\\n            list.append('%s\\n' % str(stype))\\n    return list",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PEP 352 implementation.  Creates a new base class, BaseException, which has an added message attribute compared to the previous version of Exception.  It is also a new-style class, making all exceptions now new-style.  KeyboardInterrupt and SystemExit inherit from BaseException directly.  String exceptions now raise DeprecationWarning.\\n\\nApplies patch 1104669, and closes bugs 1012952 and 518846.",
    "fixed_code": "def format_exception_only(etype, value):\\n    list = []\\n    if (type(etype) == types.ClassType\\n        or (isinstance(etype, type) and issubclass(etype, Exception))):\\n        stype = etype.__name__\\n    else:\\n        stype = etype\\n    if value is None:\\n        list.append(str(stype) + '\\n')\\n    else:\\n        if etype is SyntaxError:\\n            try:\\n                msg, (filename, lineno, offset, line) = value\\n            except:\\n                pass\\n            else:\\n                if not filename: filename = \"<string>\"\\n                list.append('  File \"%s\", line %d\\n' %\\n                            (filename, lineno))\\n                if line is not None:\\n                    i = 0\\n                    while i < len(line) and line[i].isspace():\\n                        i = i+1\\n                    list.append('    %s\\n' % line.strip())\\n                    if offset is not None:\\n                        s = '    '\\n                        for c in line[i:offset-1]:\\n                            if c.isspace():\\n                                s = s + c\\n                            else:\\n                                s = s + ' '\\n                        list.append('%s^\\n' % s)\\n                    value = msg\\n        s = _some_str(value)\\n        if s:\\n            list.append('%s: %s\\n' % (str(stype), s))\\n        else:\\n            list.append('%s\\n' % str(stype))\\n    return list"
  },
  {
    "code": "def destinsrc(src, dst):\\n    src = abspath(src)\\n    dst = abspath(dst)\\n    if not src.endswith(os.path.sep):\\n        src += os.path.sep\\n    if not dst.endswith(os.path.sep):\\n        dst += os.path.sep\\n    return dst.startswith(src)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def destinsrc(src, dst):\\n    src = abspath(src)\\n    dst = abspath(dst)\\n    if not src.endswith(os.path.sep):\\n        src += os.path.sep\\n    if not dst.endswith(os.path.sep):\\n        dst += os.path.sep\\n    return dst.startswith(src)"
  },
  {
    "code": "def sanitize_array(\\n    data,\\n    index: Optional[Index],\\n    dtype: Optional[DtypeObj] = None,\\n    copy: bool = False,\\n    raise_cast_failure: bool = True,\\n) -> ArrayLike:\\n    if isinstance(data, ma.MaskedArray):\\n        data = sanitize_masked_array(data)\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray) and data.ndim == 0:\\n        if dtype is None:\\n            dtype = data.dtype\\n        data = lib.item_from_zerodim(data)\\n    if isinstance(data, np.ndarray):\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except ValueError:\\n                subarr = np.array(data, copy=copy)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n        return subarr\\n    elif isinstance(data, (list, tuple, abc.Set, abc.ValuesView)) and len(data) > 0:\\n        if isinstance(data, set):\\n            raise TypeError(\"Set type is unordered\")\\n        data = list(data)\\n        if dtype is not None:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n            subarr = maybe_cast_to_datetime(subarr, dtype)\\n    elif isinstance(data, range):\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\\n        subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\\n    elif not is_list_like(data):\\n        if index is None:\\n            raise ValueError(\"index must be specified when data is not list-like\")\\n        subarr = construct_1d_arraylike_from_scalar(data, len(index), dtype)\\n    else:\\n        data = list(data)\\n        subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    subarr = _sanitize_ndim(subarr, data, dtype, index)\\n    if not (is_extension_array_dtype(subarr.dtype) or is_extension_array_dtype(dtype)):\\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\\n        is_object_or_str_dtype = is_object_dtype(dtype) or is_string_dtype(dtype)\\n        if is_object_dtype(subarr.dtype) and not is_object_or_str_dtype:\\n            inferred = lib.infer_dtype(subarr, skipna=False)\\n            if inferred in {\"interval\", \"period\"}:\\n                subarr = array(subarr)\\n    return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DataFrame(frozenset) should raise (#40163)",
    "fixed_code": "def sanitize_array(\\n    data,\\n    index: Optional[Index],\\n    dtype: Optional[DtypeObj] = None,\\n    copy: bool = False,\\n    raise_cast_failure: bool = True,\\n) -> ArrayLike:\\n    if isinstance(data, ma.MaskedArray):\\n        data = sanitize_masked_array(data)\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray) and data.ndim == 0:\\n        if dtype is None:\\n            dtype = data.dtype\\n        data = lib.item_from_zerodim(data)\\n    if isinstance(data, np.ndarray):\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except ValueError:\\n                subarr = np.array(data, copy=copy)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n        return subarr\\n    elif isinstance(data, (list, tuple, abc.Set, abc.ValuesView)) and len(data) > 0:\\n        if isinstance(data, (set, frozenset)):\\n            raise TypeError(f\"'{type(data).__name__}' type is unordered\")\\n        data = list(data)\\n        if dtype is not None:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n            subarr = maybe_cast_to_datetime(subarr, dtype)\\n    elif isinstance(data, range):\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\\n        subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\\n    elif not is_list_like(data):\\n        if index is None:\\n            raise ValueError(\"index must be specified when data is not list-like\")\\n        subarr = construct_1d_arraylike_from_scalar(data, len(index), dtype)\\n    else:\\n        data = list(data)\\n        subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    subarr = _sanitize_ndim(subarr, data, dtype, index)\\n    if not (is_extension_array_dtype(subarr.dtype) or is_extension_array_dtype(dtype)):\\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\\n        is_object_or_str_dtype = is_object_dtype(dtype) or is_string_dtype(dtype)\\n        if is_object_dtype(subarr.dtype) and not is_object_or_str_dtype:\\n            inferred = lib.infer_dtype(subarr, skipna=False)\\n            if inferred in {\"interval\", \"period\"}:\\n                subarr = array(subarr)\\n    return subarr"
  },
  {
    "code": "def getnode(*, getters=None):\\n    global _node\\n    if _node is not None:\\n        return _node\\n    if sys.platform == 'win32':\\n        getters = _NODE_GETTERS_WIN32\\n    else:\\n        getters = _NODE_GETTERS_UNIX\\n    for getter in getters + [_random_getnode]:\\n        try:\\n            _node = getter()\\n        except:\\n            continue\\n        if (_node is not None) and (0 <= _node < (1 << 48)):\\n            return _node\\n    assert False, '_random_getnode() returned invalid value: {}'.format(_node)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getnode(*, getters=None):\\n    global _node\\n    if _node is not None:\\n        return _node\\n    if sys.platform == 'win32':\\n        getters = _NODE_GETTERS_WIN32\\n    else:\\n        getters = _NODE_GETTERS_UNIX\\n    for getter in getters + [_random_getnode]:\\n        try:\\n            _node = getter()\\n        except:\\n            continue\\n        if (_node is not None) and (0 <= _node < (1 << 48)):\\n            return _node\\n    assert False, '_random_getnode() returned invalid value: {}'.format(_node)"
  },
  {
    "code": "def create_stack(module, stack_params, cfn, events_limit):\\n    if 'TemplateBody' not in stack_params and 'TemplateURL' not in stack_params:\\n        module.fail_json(msg=\"Either 'template', 'template_body' or 'template_url' is required when the stack does not exist.\")\\n    if module.params.get('on_create_failure') is None:\\n        stack_params['DisableRollback'] = module.params['disable_rollback']\\n    else:\\n        if module.params['disable_rollback']:\\n            module.fail_json(msg=\"You can specify either 'on_create_failure' or 'disable_rollback', but not both.\")\\n        stack_params['OnFailure'] = module.params['on_create_failure']\\n    if module.params.get('create_timeout') is not None:\\n        stack_params['TimeoutInMinutes'] = module.params['create_timeout']\\n    if module.params.get('termination_protection') is not None:\\n        if boto_supports_termination_protection(cfn):\\n            stack_params['EnableTerminationProtection'] = bool(module.params.get('termination_protection'))\\n        else:\\n            module.fail_json(msg=\"termination_protection parameter requires botocore >= 1.7.18\")\\n    try:\\n        response = cfn.create_stack(**stack_params)\\n        result = stack_operation(cfn, response['StackId'], 'CREATE', events_limit, stack_params.get('ClientRequestToken', None))\\n    except Exception as err:\\n        error_msg = boto_exception(err)\\n        module.fail_json(msg=\"Failed to create stack {0}: {1}.\".format(stack_params.get('StackName'), error_msg), exception=traceback.format_exc())\\n    if not result:\\n        module.fail_json(msg=\"empty result\")\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "cloudformation - use mutually_exclusive for on_create_failure and disable_rollback (#65629)",
    "fixed_code": "def create_stack(module, stack_params, cfn, events_limit):\\n    if 'TemplateBody' not in stack_params and 'TemplateURL' not in stack_params:\\n        module.fail_json(msg=\"Either 'template', 'template_body' or 'template_url' is required when the stack does not exist.\")\\n    if module.params.get('on_create_failure') is not None:\\n        stack_params['OnFailure'] = module.params['on_create_failure']\\n    else:\\n        stack_params['DisableRollback'] = module.params['disable_rollback']\\n    if module.params.get('create_timeout') is not None:\\n        stack_params['TimeoutInMinutes'] = module.params['create_timeout']\\n    if module.params.get('termination_protection') is not None:\\n        if boto_supports_termination_protection(cfn):\\n            stack_params['EnableTerminationProtection'] = bool(module.params.get('termination_protection'))\\n        else:\\n            module.fail_json(msg=\"termination_protection parameter requires botocore >= 1.7.18\")\\n    try:\\n        response = cfn.create_stack(**stack_params)\\n        result = stack_operation(cfn, response['StackId'], 'CREATE', events_limit, stack_params.get('ClientRequestToken', None))\\n    except Exception as err:\\n        error_msg = boto_exception(err)\\n        module.fail_json(msg=\"Failed to create stack {0}: {1}.\".format(stack_params.get('StackName'), error_msg), exception=traceback.format_exc())\\n    if not result:\\n        module.fail_json(msg=\"empty result\")\\n    return result"
  },
  {
    "code": "def _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\\n                  owner=None, group=None, logger=None):\\n    tar_compression = {'gzip': 'gz', None: ''}\\n    compress_ext = {'gzip': '.gz'}\\n    if _BZ2_SUPPORTED:\\n        tar_compression['bzip2'] = 'bz2'\\n        compress_ext['bzip2'] = '.bz2'\\n    if compress is not None and compress not in compress_ext:\\n        raise ValueError(\"bad value for 'compress', or compression format not \"\\n                         \"supported : {0}\".format(compress))\\n    archive_name = base_name + '.tar' + compress_ext.get(compress, '')\\n    archive_dir = os.path.dirname(archive_name)\\n    if not os.path.exists(archive_dir):\\n        if logger is not None:\\n            logger.info(\"creating %s\", archive_dir)\\n        if not dry_run:\\n            os.makedirs(archive_dir)\\n    if logger is not None:\\n        logger.info('Creating tar archive')\\n    uid = _get_uid(owner)\\n    gid = _get_gid(group)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #21280: Fixed a bug in shutil.make_archive() when create an archive of current directory in current directory.",
    "fixed_code": "def _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\\n                  owner=None, group=None, logger=None):\\n    tar_compression = {'gzip': 'gz', None: ''}\\n    compress_ext = {'gzip': '.gz'}\\n    if _BZ2_SUPPORTED:\\n        tar_compression['bzip2'] = 'bz2'\\n        compress_ext['bzip2'] = '.bz2'\\n    if compress is not None and compress not in compress_ext:\\n        raise ValueError(\"bad value for 'compress', or compression format not \"\\n                         \"supported : {0}\".format(compress))\\n    archive_name = base_name + '.tar' + compress_ext.get(compress, '')\\n    archive_dir = os.path.dirname(archive_name)\\n    if archive_dir and not os.path.exists(archive_dir):\\n        if logger is not None:\\n            logger.info(\"creating %s\", archive_dir)\\n        if not dry_run:\\n            os.makedirs(archive_dir)\\n    if logger is not None:\\n        logger.info('Creating tar archive')\\n    uid = _get_uid(owner)\\n    gid = _get_gid(group)"
  },
  {
    "code": "def poke(self, context):\\n        hook = FSHook(self.fs_conn_id)\\n        basepath = hook.get_path()\\n        full_path = os.path.join(basepath, self.filepath)\\n        self.log.info('Poking for file %s', full_path)\\n        for path in glob(full_path):\\n            if os.path.isfile(path):\\n                mod_time = os.path.getmtime(path)\\n                mod_time = datetime.datetime.fromtimestamp(mod_time).strftime('%Y%m%d%H%M%S')\\n                self.log.info('Found File %s last modified: %s', str(path), str(mod_time))\\n                return True\\n            for _, _, files in os.walk(full_path):\\n                if len(files) > 0:\\n                    return True\\n        return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add recursive flag to glob in filesystem sensor (#16894)\\n\\nThis PR aims to fix #16725 by adding the `recursive` flag to `glob` in the filesystem sensor.\\n\\ncloses: #16725",
    "fixed_code": "def poke(self, context):\\n        hook = FSHook(self.fs_conn_id)\\n        basepath = hook.get_path()\\n        full_path = os.path.join(basepath, self.filepath)\\n        self.log.info('Poking for file %s', full_path)\\n        for path in glob(full_path, recursive=self.recursive):\\n            if os.path.isfile(path):\\n                mod_time = os.path.getmtime(path)\\n                mod_time = datetime.datetime.fromtimestamp(mod_time).strftime('%Y%m%d%H%M%S')\\n                self.log.info('Found File %s last modified: %s', str(path), str(mod_time))\\n                return True\\n            for _, _, files in os.walk(full_path):\\n                if len(files) > 0:\\n                    return True\\n        return False"
  },
  {
    "code": "def read_stored_certificate_fingerprint(module, keytool_bin, alias, keystore_path, keystore_password):\\n    stored_certificate_fingerprint_cmd = \"%s -list -alias '%s' -keystore '%s' -storepass '%s'\" % (keytool_bin, alias, keystore_path, keystore_password)\\n    (rc, stored_certificate_fingerprint_out, stored_certificate_fingerprint_err) = run_commands(module, stored_certificate_fingerprint_cmd)\\n    if rc != 0:\\n        if \"keytool error: java.lang.Exception: Alias <%s> does not exist\" % alias not in stored_certificate_fingerprint_out:\\n            return module.fail_json(msg=stored_certificate_fingerprint_out,\\n                                    err=stored_certificate_fingerprint_err,\\n                                    rc=rc,\\n                                    cmd=stored_certificate_fingerprint_cmd)\\n        else:\\n            return None\\n    else:\\n        stored_certificate_match = re.search(r\": ([\\w:]+)\", stored_certificate_fingerprint_out)\\n        if not stored_certificate_match:\\n            return module.fail_json(\\n                msg=\"Unable to find the stored certificate fingerprint in %s\" % stored_certificate_fingerprint_out,\\n                rc=rc,\\n                cmd=stored_certificate_fingerprint_cmd\\n            )\\n        return stored_certificate_match.group(1)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "java_keystore - Prefer SHA256 and solve SHA256 keytool in java11 version (#57302)",
    "fixed_code": "def read_stored_certificate_fingerprint(module, keytool_bin, alias, keystore_path, keystore_password):\\n    stored_certificate_fingerprint_cmd = \"%s -list -alias '%s' -keystore '%s' -storepass '%s' -v\" % (keytool_bin, alias, keystore_path, keystore_password)\\n    (rc, stored_certificate_fingerprint_out, stored_certificate_fingerprint_err) = run_commands(module, stored_certificate_fingerprint_cmd)\\n    if rc != 0:\\n        if \"keytool error: java.lang.Exception: Alias <%s> does not exist\" % alias not in stored_certificate_fingerprint_out:\\n            return module.fail_json(msg=stored_certificate_fingerprint_out,\\n                                    err=stored_certificate_fingerprint_err,\\n                                    rc=rc,\\n                                    cmd=stored_certificate_fingerprint_cmd)\\n        else:\\n            return None\\n    else:\\n        stored_certificate_match = re.search(r\"SHA256: ([\\w:]+)\", stored_certificate_fingerprint_out)\\n        if not stored_certificate_match:\\n            return module.fail_json(\\n                msg=\"Unable to find the stored certificate fingerprint in %s\" % stored_certificate_fingerprint_out,\\n                rc=rc,\\n                cmd=stored_certificate_fingerprint_cmd\\n            )\\n        return stored_certificate_match.group(1)"
  },
  {
    "code": "def mklogical(dict):\\n    return aetypes.Logical(dict['logc'], dict['term'])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "SF patch# 1761465 by Jeffrey Yasskin. Fix test_aepack and test_applesingle.",
    "fixed_code": "def mklogical(dict):\\n    return aetypes.Logical(dict[b2i(b'logc')], dict[b2i(b'term')])"
  },
  {
    "code": "def update(self, pm, CS, lead):\\n    v_ego = CS.vEgo\\n    self.cur_state[0].x_ego = 0.0\\n    if lead is not None and lead.status:\\n      x_lead = lead.dRel\\n      v_lead = max(0.0, lead.vLead)\\n      a_lead = lead.aLeadK\\n      if (v_lead < 0.1 or -a_lead / 2.0 > v_lead):\\n        v_lead = 0.0\\n        a_lead = 0.0\\n      self.a_lead_tau = lead.aLeadTau\\n      self.new_lead = False\\n      if not self.prev_lead_status or abs(x_lead - self.prev_lead_x) > 2.5:\\n        self.libmpc.init_with_simulation(self.v_mpc, x_lead, v_lead, a_lead, self.a_lead_tau)\\n        self.new_lead = True\\n      self.prev_lead_status = True\\n      self.prev_lead_x = x_lead\\n      self.cur_state[0].x_l = x_lead\\n      self.cur_state[0].v_l = v_lead\\n    else:\\n      self.prev_lead_status = False\\n      self.cur_state[0].x_l = 50.0\\n      self.cur_state[0].v_l = v_ego + 10.0\\n      a_lead = 0.0\\n      self.a_lead_tau = _LEAD_ACCEL_TAU\\n    t = sec_since_boot()\\n    n_its = self.libmpc.run_mpc(self.cur_state, self.mpc_solution, self.a_lead_tau, a_lead)\\n    duration = int((sec_since_boot() - t) * 1e9)\\n    if LOG_MPC:\\n      self.send_mpc_solution(pm, n_its, duration)\\n    self.v_mpc = self.mpc_solution[0].v_ego[1]\\n    self.a_mpc = self.mpc_solution[0].a_ego[1]\\n    self.v_mpc_future = self.mpc_solution[0].v_ego[10]\\n    crashing = any(lead - ego < -50 for (lead, ego) in zip(self.mpc_solution[0].x_l, self.mpc_solution[0].x_ego))\\n    nans = any(math.isnan(x) for x in self.mpc_solution[0].v_ego)\\n    backwards = min(self.mpc_solution[0].v_ego) < -0.01\\n    if ((backwards or crashing) and self.prev_lead_status) or nans:\\n      if t > self.last_cloudlog_t + 5.0:\\n        self.last_cloudlog_t = t\\n        cloudlog.warning(\"Longitudinal mpc %d reset - backwards: %s crashing: %s nan: %s\" % (\\n                          self.mpc_id, backwards, crashing, nans))\\n      self.libmpc.init(MPC_COST_LONG.TTC, MPC_COST_LONG.DISTANCE,\\n                       MPC_COST_LONG.ACCELERATION, MPC_COST_LONG.JERK)\\n      self.cur_state[0].v_ego = v_ego\\n      self.cur_state[0].a_ego = 0.0\\n      self.v_mpc = v_ego\\n      self.a_mpc = CS.aEgo\\n      self.prev_lead_status = False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update(self, pm, CS, lead):\\n    v_ego = CS.vEgo\\n    self.cur_state[0].x_ego = 0.0\\n    if lead is not None and lead.status:\\n      x_lead = lead.dRel\\n      v_lead = max(0.0, lead.vLead)\\n      a_lead = lead.aLeadK\\n      if (v_lead < 0.1 or -a_lead / 2.0 > v_lead):\\n        v_lead = 0.0\\n        a_lead = 0.0\\n      self.a_lead_tau = lead.aLeadTau\\n      self.new_lead = False\\n      if not self.prev_lead_status or abs(x_lead - self.prev_lead_x) > 2.5:\\n        self.libmpc.init_with_simulation(self.v_mpc, x_lead, v_lead, a_lead, self.a_lead_tau)\\n        self.new_lead = True\\n      self.prev_lead_status = True\\n      self.prev_lead_x = x_lead\\n      self.cur_state[0].x_l = x_lead\\n      self.cur_state[0].v_l = v_lead\\n    else:\\n      self.prev_lead_status = False\\n      self.cur_state[0].x_l = 50.0\\n      self.cur_state[0].v_l = v_ego + 10.0\\n      a_lead = 0.0\\n      self.a_lead_tau = _LEAD_ACCEL_TAU\\n    t = sec_since_boot()\\n    n_its = self.libmpc.run_mpc(self.cur_state, self.mpc_solution, self.a_lead_tau, a_lead)\\n    duration = int((sec_since_boot() - t) * 1e9)\\n    if LOG_MPC:\\n      self.send_mpc_solution(pm, n_its, duration)\\n    self.v_mpc = self.mpc_solution[0].v_ego[1]\\n    self.a_mpc = self.mpc_solution[0].a_ego[1]\\n    self.v_mpc_future = self.mpc_solution[0].v_ego[10]\\n    crashing = any(lead - ego < -50 for (lead, ego) in zip(self.mpc_solution[0].x_l, self.mpc_solution[0].x_ego))\\n    nans = any(math.isnan(x) for x in self.mpc_solution[0].v_ego)\\n    backwards = min(self.mpc_solution[0].v_ego) < -0.01\\n    if ((backwards or crashing) and self.prev_lead_status) or nans:\\n      if t > self.last_cloudlog_t + 5.0:\\n        self.last_cloudlog_t = t\\n        cloudlog.warning(\"Longitudinal mpc %d reset - backwards: %s crashing: %s nan: %s\" % (\\n                          self.mpc_id, backwards, crashing, nans))\\n      self.libmpc.init(MPC_COST_LONG.TTC, MPC_COST_LONG.DISTANCE,\\n                       MPC_COST_LONG.ACCELERATION, MPC_COST_LONG.JERK)\\n      self.cur_state[0].v_ego = v_ego\\n      self.cur_state[0].a_ego = 0.0\\n      self.v_mpc = v_ego\\n      self.a_mpc = CS.aEgo\\n      self.prev_lead_status = False"
  },
  {
    "code": "def _tuple_str(obj_name, fields):\\n    if not fields:\\n        return '()'\\n    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _tuple_str(obj_name, fields):\\n    if not fields:\\n        return '()'\\n    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'"
  },
  {
    "code": "def auto_dev_build(debug=False):\\n    msg = ''\\n    try:\\n        clean()\\n        html()\\n        latex()\\n        upload_dev()\\n        upload_dev_pdf()\\n        if not debug:\\n            sendmail()\\n    except (Exception, SystemExit), inst:\\n        msg += str(inst) + '\\n'\\n        sendmail(msg)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def auto_dev_build(debug=False):\\n    msg = ''\\n    try:\\n        clean()\\n        html()\\n        latex()\\n        upload_dev()\\n        upload_dev_pdf()\\n        if not debug:\\n            sendmail()\\n    except (Exception, SystemExit), inst:\\n        msg += str(inst) + '\\n'\\n        sendmail(msg)"
  },
  {
    "code": "def _fix_connectivity(X, connectivity, n_components=None):\\n    n_samples = X.shape[0]\\n    if (connectivity.shape[0] != n_samples or\\n        connectivity.shape[1] != n_samples):\\n        raise ValueError('Wrong shape for connectivity matrix: %s '\\n                         'when X is %s' % (connectivity.shape, X.shape))\\n    connectivity = connectivity + connectivity.T\\n    if not sparse.isspmatrix_lil(connectivity):\\n        if not sparse.isspmatrix(connectivity):\\n            connectivity = sparse.lil_matrix(connectivity)\\n        else:\\n            connectivity = connectivity.tolil()\\n    n_components, labels = connected_components(connectivity)\\n    if n_components > 1:\\n        warnings.warn(\"the number of connected components of the \"\\n                      \"connectivity matrix is %d > 1. Completing it to avoid \"\\n                      \"stopping the tree early.\" % n_components,\\n                      stacklevel=2)\\n        for i in xrange(n_components):\\n            idx_i = np.where(labels == i)[0]\\n            Xi = X[idx_i]\\n            for j in xrange(i):\\n                idx_j = np.where(labels == j)[0]\\n                Xj = X[idx_j]\\n                D = euclidean_distances(Xi, Xj)\\n                ii, jj = np.where(D == np.min(D))\\n                ii = ii[0]\\n                jj = jj[0]\\n                connectivity[idx_i[ii], idx_j[jj]] = True\\n                connectivity[idx_j[jj], idx_i[ii]] = True\\n        n_components = 1\\n    return connectivity",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fix_connectivity(X, connectivity, n_components=None):\\n    n_samples = X.shape[0]\\n    if (connectivity.shape[0] != n_samples or\\n        connectivity.shape[1] != n_samples):\\n        raise ValueError('Wrong shape for connectivity matrix: %s '\\n                         'when X is %s' % (connectivity.shape, X.shape))\\n    connectivity = connectivity + connectivity.T\\n    if not sparse.isspmatrix_lil(connectivity):\\n        if not sparse.isspmatrix(connectivity):\\n            connectivity = sparse.lil_matrix(connectivity)\\n        else:\\n            connectivity = connectivity.tolil()\\n    n_components, labels = connected_components(connectivity)\\n    if n_components > 1:\\n        warnings.warn(\"the number of connected components of the \"\\n                      \"connectivity matrix is %d > 1. Completing it to avoid \"\\n                      \"stopping the tree early.\" % n_components,\\n                      stacklevel=2)\\n        for i in xrange(n_components):\\n            idx_i = np.where(labels == i)[0]\\n            Xi = X[idx_i]\\n            for j in xrange(i):\\n                idx_j = np.where(labels == j)[0]\\n                Xj = X[idx_j]\\n                D = euclidean_distances(Xi, Xj)\\n                ii, jj = np.where(D == np.min(D))\\n                ii = ii[0]\\n                jj = jj[0]\\n                connectivity[idx_i[ii], idx_j[jj]] = True\\n                connectivity[idx_j[jj], idx_i[ii]] = True\\n        n_components = 1\\n    return connectivity"
  },
  {
    "code": "def read_excel(\\n    io,\\n    sheet_name: str | int = ...,\\n    header: int | Sequence[int] | None = ...,\\n    names: list[str] | None = ...,\\n    index_col: int | Sequence[int] | None = ...,\\n    usecols: int\\n    | str\\n    | Sequence[int]\\n    | Sequence[str]\\n    | Callable[[str], bool]\\n    | None = ...,\\n    squeeze: bool | None = ...,\\n    dtype: DtypeArg | None = ...,\\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\\n    converters: dict[str, Callable] | dict[int, Callable] | None = ...,\\n    true_values: Iterable[Hashable] | None = ...,\\n    false_values: Iterable[Hashable] | None = ...,\\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\\n    nrows: int | None = ...,\\n    na_values=...,\\n    keep_default_na: bool = ...,\\n    na_filter: bool = ...,\\n    verbose: bool = ...,\\n    parse_dates: list | dict | bool = ...,\\n    date_parser: Callable | None = ...,\\n    thousands: str | None = ...,\\n    decimal: str = ...,\\n    comment: str | None = ...,\\n    skipfooter: int = ...,\\n    convert_float: bool | None = ...,\\n    mangle_dupe_cols: bool = ...,\\n    storage_options: StorageOptions = ...,\\n) -> DataFrame:\\n    ...",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEP: Enforce deprecation of mangle_dup cols and convert_float in read_excel (#49089)",
    "fixed_code": "def read_excel(\\n    io,\\n    sheet_name: str | int = ...,\\n    *,\\n    header: int | Sequence[int] | None = ...,\\n    names: list[str] | None = ...,\\n    index_col: int | Sequence[int] | None = ...,\\n    usecols: int\\n    | str\\n    | Sequence[int]\\n    | Sequence[str]\\n    | Callable[[str], bool]\\n    | None = ...,\\n    squeeze: bool | None = ...,\\n    dtype: DtypeArg | None = ...,\\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\\n    converters: dict[str, Callable] | dict[int, Callable] | None = ...,\\n    true_values: Iterable[Hashable] | None = ...,\\n    false_values: Iterable[Hashable] | None = ...,\\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\\n    nrows: int | None = ...,\\n    na_values=...,\\n    keep_default_na: bool = ...,\\n    na_filter: bool = ...,\\n    verbose: bool = ...,\\n    parse_dates: list | dict | bool = ...,\\n    date_parser: Callable | None = ...,\\n    thousands: str | None = ...,\\n    decimal: str = ...,\\n    comment: str | None = ...,\\n    skipfooter: int = ...,\\n    storage_options: StorageOptions = ...,\\n) -> DataFrame:\\n    ..."
  },
  {
    "code": "def get_near_stock_price(self, above_below=2, call=True, put=False,\\n                             month=None, year=None, expiry=None):\\n        to_ret = Series({'calls': call, 'puts': put})\\n        to_ret = to_ret[to_ret].index\\n        data = {}\\n        for nam in to_ret:\\n            df = self._get_option_data(month, year, expiry, nam)\\n            data[nam] = self.chop_data(df, above_below, self.underlying_price)\\n        return concat([data[nam] for nam in to_ret]).sortlevel()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix pandas.io.data.Options for change in format of Yahoo Option page\\n\\nENH: Automatically choose next expiry if expiry date isn't valid\\n\\nCOMPAT: Remove dictionary comprehension to pass 2.6 test\\n\\nBUG: Add check that tables were downloaded.\\n\\nENH: Replace third_saturday function with Pandas offset\\n\\nBUG: Check to make sure enough option tables are downloaded.\\n\\nTST: Add sample options page during market open.\\n\\nDOC: Add bug fix report to v0.15.1.txt for data.Options\\n\\nBUG: Ensure that the value used for chopping is within the range of strikes.\\n\\nTST: Add test for requesting out of range chop\\n\\nBUG: Fix missing underlying price and quote time in first process data\\n\\nBUG: Fix AM/PM on quote time\\n\\nENH: Refactor to expose available expiry dates\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nBUG: Undo accidental deletion of privat emethods\\n\\nENH: Add ability to use list as expiry parameter.\\n\\nAlso has year & month return all data for the specified year and month.\\n\\nTST: Remove tests for warnings that are no longer in use.\\n\\nDOC: Update docstrings of Options class.",
    "fixed_code": "def get_near_stock_price(self, above_below=2, call=True, put=False,\\n                             month=None, year=None, expiry=None):\\n        expiry = self._try_parse_dates(year, month, expiry)\\n        data = self._get_data_in_date_range(expiry, call=call, put=put)\\n        return self.chop_data(data, above_below, self.underlying_price)"
  },
  {
    "code": "def _try_convert_dates(parser, colspec, data_dict, columns):\\n    colspec = _get_col_names(colspec, columns)\\n    new_name = '_'.join(colspec)\\n    to_parse = [data_dict[c] for c in colspec if c in data_dict]\\n    try:\\n        new_col = parser(*to_parse)\\n    except DateConversionError:\\n        new_col = parser(_concat_date_cols(to_parse))\\n    return new_name, new_col, colspec",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: change default header names in read_* functions from X.1, X.2, ... to X0, X1, ... close #2000",
    "fixed_code": "def _try_convert_dates(parser, colspec, data_dict, columns):\\n    colspec = _get_col_names(colspec, columns)\\n    new_name = '_'.join([str(x) for x in colspec])\\n    to_parse = [data_dict[c] for c in colspec if c in data_dict]\\n    try:\\n        new_col = parser(*to_parse)\\n    except DateConversionError:\\n        new_col = parser(_concat_date_cols(to_parse))\\n    return new_name, new_col, colspec"
  },
  {
    "code": "def irow(self, i):\\n        if isinstance(i, slice):\\n            return self[i]\\n        else:\\n            label = self.index[i]\\n            return self.xs(label)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: can pass sequence of integers to DataFrame.{irow/icol} and Series.iget, GH #654\"",
    "fixed_code": "def irow(self, i):\\n        if isinstance(i, slice):\\n            return self[i]\\n        else:\\n            label = self.index[i]\\n            if isinstance(label, Index):\\n                return self.reindex(label)\\n            else:\\n                return self.xs(label)"
  },
  {
    "code": "def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None,\\n                     normalize=None):\\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\\n    if y_type not in (\"binary\", \"multiclass\"):\\n        raise ValueError(\"%s is not supported\" % y_type)\\n    if labels is None:\\n        labels = unique_labels(y_true, y_pred)\\n    else:\\n        labels = np.asarray(labels)\\n        if np.all([l not in y_true for l in labels]):\\n            raise ValueError(\"At least one label specified must be in y_true\")\\n    if sample_weight is None:\\n        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\\n    else:\\n        sample_weight = np.asarray(sample_weight)\\n    check_consistent_length(y_true, y_pred, sample_weight)\\n    n_labels = labels.size\\n    label_to_ind = {y: x for x, y in enumerate(labels)}\\n    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\\n    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\\n    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\\n    y_pred = y_pred[ind]\\n    y_true = y_true[ind]\\n    sample_weight = sample_weight[ind]\\n    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\\n        dtype = np.int64\\n    else:\\n        dtype = np.float64\\n    cm = coo_matrix((sample_weight, (y_true, y_pred)),\\n                    shape=(n_labels, n_labels), dtype=dtype,\\n                    ).toarray()\\n    with np.errstate(all='ignore'):\\n        if normalize == 'true':\\n            cm = cm / cm.sum(axis=1, keepdims=True)\\n        elif normalize == 'pred':\\n            cm = cm / cm.sum(axis=0, keepdims=True)\\n        elif normalize == 'all':\\n            cm = cm / cm.sum()\\n        cm = np.nan_to_num(cm)\\n    return cm",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None,\\n                     normalize=None):\\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\\n    if y_type not in (\"binary\", \"multiclass\"):\\n        raise ValueError(\"%s is not supported\" % y_type)\\n    if labels is None:\\n        labels = unique_labels(y_true, y_pred)\\n    else:\\n        labels = np.asarray(labels)\\n        if np.all([l not in y_true for l in labels]):\\n            raise ValueError(\"At least one label specified must be in y_true\")\\n    if sample_weight is None:\\n        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\\n    else:\\n        sample_weight = np.asarray(sample_weight)\\n    check_consistent_length(y_true, y_pred, sample_weight)\\n    n_labels = labels.size\\n    label_to_ind = {y: x for x, y in enumerate(labels)}\\n    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\\n    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\\n    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\\n    y_pred = y_pred[ind]\\n    y_true = y_true[ind]\\n    sample_weight = sample_weight[ind]\\n    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\\n        dtype = np.int64\\n    else:\\n        dtype = np.float64\\n    cm = coo_matrix((sample_weight, (y_true, y_pred)),\\n                    shape=(n_labels, n_labels), dtype=dtype,\\n                    ).toarray()\\n    with np.errstate(all='ignore'):\\n        if normalize == 'true':\\n            cm = cm / cm.sum(axis=1, keepdims=True)\\n        elif normalize == 'pred':\\n            cm = cm / cm.sum(axis=0, keepdims=True)\\n        elif normalize == 'all':\\n            cm = cm / cm.sum()\\n        cm = np.nan_to_num(cm)\\n    return cm"
  },
  {
    "code": "def _process_worker(call_queue, result_queue):\\n    while True:\\n        call_item = call_queue.get(block=True)\\n        if call_item is None:\\n            result_queue.put(os.getpid())\\n            return\\n        try:\\n            r = call_item.fn(*call_item.args, **call_item.kwargs)\\n        except BaseException as e:\\n            result_queue.put(_ResultItem(call_item.work_id,\\n                                         exception=e))\\n        else:\\n            result_queue.put(_ResultItem(call_item.work_id,\\n                                         result=r))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #21817: When an exception is raised in a task submitted to a ProcessPoolExecutor, the remote traceback is now displayed in the parent process. Patch by Claudiu Popa.",
    "fixed_code": "def _process_worker(call_queue, result_queue):\\n    while True:\\n        call_item = call_queue.get(block=True)\\n        if call_item is None:\\n            result_queue.put(os.getpid())\\n            return\\n        try:\\n            r = call_item.fn(*call_item.args, **call_item.kwargs)\\n        except BaseException as e:\\n            exc = _ExceptionWithTraceback(e, e.__traceback__)\\n            result_queue.put(_ResultItem(call_item.work_id, exception=exc))\\n        else:\\n            result_queue.put(_ResultItem(call_item.work_id,\\n                                         result=r))"
  },
  {
    "code": "def is_bool_indexer(key: Any) -> bool:\\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\\n        is_array_like(key) and is_extension_array_dtype(key.dtype)\\n    ):\\n        if key.dtype == np.object_:\\n            key = np.asarray(key)\\n            if not lib.is_bool_array(key):\\n                na_msg = \"Cannot mask with non-boolean array containing NA / NaN values\"\\n                if lib.infer_dtype(key) == \"boolean\" and isna(key).any():\\n                    raise ValueError(na_msg)\\n                return False\\n            return True\\n        elif is_bool_dtype(key.dtype):\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  \\n            return False\\n    return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_bool_indexer(key: Any) -> bool:\\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\\n        is_array_like(key) and is_extension_array_dtype(key.dtype)\\n    ):\\n        if key.dtype == np.object_:\\n            key = np.asarray(key)\\n            if not lib.is_bool_array(key):\\n                na_msg = \"Cannot mask with non-boolean array containing NA / NaN values\"\\n                if lib.infer_dtype(key) == \"boolean\" and isna(key).any():\\n                    raise ValueError(na_msg)\\n                return False\\n            return True\\n        elif is_bool_dtype(key.dtype):\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  \\n            return False\\n    return False"
  },
  {
    "code": "def items_for_result(cl, result, form):\\n    first = True\\n    pk = cl.lookup_opts.pk.attname\\n    for field_name in cl.list_display:\\n        row_class = ''\\n        try:\\n            f, attr, value = lookup_field(field_name, result, cl.model_admin)\\n        except (AttributeError, ObjectDoesNotExist):\\n            result_repr = EMPTY_CHANGELIST_VALUE\\n        else:\\n            if f is None:\\n                allow_tags = getattr(attr, 'allow_tags', False)\\n                boolean = getattr(attr, 'boolean', False)\\n                if boolean:\\n                    allow_tags = True\\n                    result_repr = _boolean_icon(value)\\n                else:\\n                    result_repr = smart_unicode(value)\\n                if not allow_tags:\\n                    result_repr = escape(result_repr)\\n                else:\\n                    result_repr = mark_safe(result_repr)\\n            else:\\n                if isinstance(f.rel, models.ManyToOneRel):\\n                    field_val = getattr(result, f.name)\\n                    if field_val is None:\\n                        result_repr = EMPTY_CHANGELIST_VALUE\\n                    else:\\n                        result_repr = escape(field_val)\\n                else:\\n                    result_repr = display_for_field(value, f)\\n                if isinstance(f, models.DateField) or isinstance(f, models.TimeField):\\n                    row_class = ' class=\"nowrap\"'\\n        if force_unicode(result_repr) == '':\\n            result_repr = mark_safe('&nbsp;')\\n        if (first and not cl.list_display_links) or field_name in cl.list_display_links:\\n            table_tag = {True:'th', False:'td'}[first]\\n            first = False\\n            url = cl.url_for_result(result)\\n            if cl.to_field:\\n                attr = str(cl.to_field)\\n            else:\\n                attr = pk\\n            value = result.serializable_value(attr)\\n            result_id = repr(force_unicode(value))[1:]\\n            yield mark_safe(u'<%s%s><a href=\"%s\"%s>%s</a></%s>' % \\\\n                (table_tag, row_class, url, (cl.is_popup and ' onclick=\"opener.dismissRelatedLookupPopup(window, %s); return false;\"' % result_id or ''), conditional_escape(result_repr), table_tag))\\n        else:\\n            if form and field_name in form.fields:\\n                bf = form[field_name]\\n                result_repr = mark_safe(force_unicode(bf.errors) + force_unicode(bf))\\n            else:\\n                result_repr = conditional_escape(result_repr)\\n            yield mark_safe(u'<td%s>%s</td>' % (row_class, result_repr))\\n    if form and not form[cl.model._meta.pk.name].is_hidden:\\n        yield mark_safe(u'<td>%s</td>' % force_unicode(form[cl.model._meta.pk.name]))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #15291 -- Corrected alignment issue when actions are disabled in a ModelAdmin. Thanks to Julien Phalip for the report and patch.",
    "fixed_code": "def items_for_result(cl, result, form):\\n    first = True\\n    pk = cl.lookup_opts.pk.attname\\n    for field_name in cl.list_display:\\n        row_class = ''\\n        try:\\n            f, attr, value = lookup_field(field_name, result, cl.model_admin)\\n        except (AttributeError, ObjectDoesNotExist):\\n            result_repr = EMPTY_CHANGELIST_VALUE\\n        else:\\n            if f is None:\\n                if field_name == u'action_checkbox':\\n                    row_class = ' class=\"action-checkbox\"'\\n                allow_tags = getattr(attr, 'allow_tags', False)\\n                boolean = getattr(attr, 'boolean', False)\\n                if boolean:\\n                    allow_tags = True\\n                    result_repr = _boolean_icon(value)\\n                else:\\n                    result_repr = smart_unicode(value)\\n                if not allow_tags:\\n                    result_repr = escape(result_repr)\\n                else:\\n                    result_repr = mark_safe(result_repr)\\n            else:\\n                if isinstance(f.rel, models.ManyToOneRel):\\n                    field_val = getattr(result, f.name)\\n                    if field_val is None:\\n                        result_repr = EMPTY_CHANGELIST_VALUE\\n                    else:\\n                        result_repr = escape(field_val)\\n                else:\\n                    result_repr = display_for_field(value, f)\\n                if isinstance(f, models.DateField) or isinstance(f, models.TimeField):\\n                    row_class = ' class=\"nowrap\"'\\n        if force_unicode(result_repr) == '':\\n            result_repr = mark_safe('&nbsp;')\\n        if (first and not cl.list_display_links) or field_name in cl.list_display_links:\\n            table_tag = {True:'th', False:'td'}[first]\\n            first = False\\n            url = cl.url_for_result(result)\\n            if cl.to_field:\\n                attr = str(cl.to_field)\\n            else:\\n                attr = pk\\n            value = result.serializable_value(attr)\\n            result_id = repr(force_unicode(value))[1:]\\n            yield mark_safe(u'<%s%s><a href=\"%s\"%s>%s</a></%s>' % \\\\n                (table_tag, row_class, url, (cl.is_popup and ' onclick=\"opener.dismissRelatedLookupPopup(window, %s); return false;\"' % result_id or ''), conditional_escape(result_repr), table_tag))\\n        else:\\n            if form and field_name in form.fields:\\n                bf = form[field_name]\\n                result_repr = mark_safe(force_unicode(bf.errors) + force_unicode(bf))\\n            else:\\n                result_repr = conditional_escape(result_repr)\\n            yield mark_safe(u'<td%s>%s</td>' % (row_class, result_repr))\\n    if form and not form[cl.model._meta.pk.name].is_hidden:\\n        yield mark_safe(u'<td>%s</td>' % force_unicode(form[cl.model._meta.pk.name]))"
  },
  {
    "code": "def call_command(name, *args, **options):\\n    try:\\n        app_name = get_commands()[name]\\n    except KeyError:\\n        raise CommandError(\"Unknown command: %r\" % name)\\n    if isinstance(app_name, BaseCommand):\\n        command = app_name\\n    else:\\n        command = load_command_class(app_name, name)\\n    parser = command.create_parser('', name)\\n    opt_mapping = {\\n        sorted(s_opt.option_strings)[0].lstrip('-').replace('-', '_'): s_opt.dest\\n        for s_opt in parser._actions if s_opt.option_strings\\n    }\\n    arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}\\n    defaults = parser.parse_args(args=args)\\n    defaults = dict(defaults._get_kwargs(), **arg_options)\\n    args = defaults.pop('args', ())\\n    if 'skip_checks' not in options:\\n        defaults['skip_checks'] = True\\n    return command.execute(*args, **defaults)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25483 -- Allowed passing non-string arguments to call_command\\n\\nThanks KS Chan for the report and Tim Graham for the review.",
    "fixed_code": "def call_command(name, *args, **options):\\n    try:\\n        app_name = get_commands()[name]\\n    except KeyError:\\n        raise CommandError(\"Unknown command: %r\" % name)\\n    if isinstance(app_name, BaseCommand):\\n        command = app_name\\n    else:\\n        command = load_command_class(app_name, name)\\n    parser = command.create_parser('', name)\\n    opt_mapping = {\\n        sorted(s_opt.option_strings)[0].lstrip('-').replace('-', '_'): s_opt.dest\\n        for s_opt in parser._actions if s_opt.option_strings\\n    }\\n    arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}\\n    defaults = parser.parse_args(args=[force_text(a) for a in args])\\n    defaults = dict(defaults._get_kwargs(), **arg_options)\\n    args = defaults.pop('args', ())\\n    if 'skip_checks' not in options:\\n        defaults['skip_checks'] = True\\n    return command.execute(*args, **defaults)"
  },
  {
    "code": "def get_config(self, key: str) -> Optional[str]:\\n        if self.config_prefix is None:\\n            return None\\n        return self._get_secret(self.config_prefix, key)\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_config(self, key: str) -> Optional[str]:\\n        if self.config_prefix is None:\\n            return None\\n        return self._get_secret(self.config_prefix, key)\\n    @staticmethod"
  },
  {
    "code": "def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):\\n    from pandas.core.format import print_config\\n    from pandas.tseries.offsets import DateOffset\\n    from pandas.tseries.frequencies import (_get_rule_month, _month_numbers,\\n                                            _get_freq_str)\\n    if not isinstance(arg, basestring):\\n        return arg\\n    arg = arg.upper()\\n    default = datetime(1, 1, 1).replace(hour=0, minute=0,\\n                                      second=0, microsecond=0)\\n    if len(arg) in [4, 6]:\\n        m = ypat.match(arg)\\n        if m:\\n            ret = default.replace(year=int(m.group(1)))\\n            return ret, ret, 'year'\\n        add_century = False\\n        if len(arg) == 4:\\n            add_century = True\\n            qpats = [(qpat1, 1), (qpat2, 0)]\\n        else:\\n            qpats = [(qpat1full, 1), (qpat2full, 0)]\\n        for pat, yfirst in qpats:\\n            qparse = pat.match(arg)\\n            if qparse is not None:\\n                if yfirst:\\n                    yi, qi = 1, 2\\n                else:\\n                    yi, qi = 2, 1\\n                q = int(qparse.group(yi))\\n                y_str = qparse.group(qi)\\n                y = int(y_str)\\n                if add_century:\\n                    y += 2000\\n                if freq is not None:\\n                    mnum = _month_numbers[_get_rule_month(freq)] + 1\\n                    month = (mnum + (q - 1) * 3) % 12 + 1\\n                    if month > mnum:\\n                        y -= 1\\n                else:\\n                    month = (q - 1) * 3 + 1\\n                ret = default.replace(year=y, month=month)\\n                return ret, ret, 'quarter'\\n        is_mo_str = freq is not None and freq == 'M'\\n        is_mo_off = getattr(freq, 'rule_code', None) == 'M'\\n        is_monthly = is_mo_str or is_mo_off\\n        if len(arg) == 6 and is_monthly:\\n            try:\\n                ret = _try_parse_monthly(arg)\\n                if ret is not None:\\n                    return ret, ret, 'month'\\n            except Exception:\\n                pass\\n    mresult = _attempt_monthly(arg)\\n    if mresult:\\n        return mresult\\n    if dayfirst is None:\\n        dayfirst = print_config.date_dayfirst\\n    if yearfirst is None:\\n        yearfirst = print_config.date_yearfirst\\n    try:\\n        parsed = parse(arg, dayfirst=dayfirst, yearfirst=yearfirst)\\n    except Exception, e:\\n        raise DateParseError(e)\\n    if parsed is None:\\n        raise DateParseError(\"Could not parse %s\" % arg)\\n    repl = {}\\n    reso = 'year'\\n    for attr in [\"year\", \"month\", \"day\", \"hour\",\\n                 \"minute\", \"second\", \"microsecond\"]:\\n        value = getattr(parsed, attr)\\n        if value is not None and value != 0:  \\n            repl[attr] = value\\n            reso = attr\\n    ret = default.replace(**repl)\\n    return ret, parsed, reso",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):\\n    from pandas.core.format import print_config\\n    from pandas.tseries.offsets import DateOffset\\n    from pandas.tseries.frequencies import (_get_rule_month, _month_numbers,\\n                                            _get_freq_str)\\n    if not isinstance(arg, basestring):\\n        return arg\\n    arg = arg.upper()\\n    default = datetime(1, 1, 1).replace(hour=0, minute=0,\\n                                      second=0, microsecond=0)\\n    if len(arg) in [4, 6]:\\n        m = ypat.match(arg)\\n        if m:\\n            ret = default.replace(year=int(m.group(1)))\\n            return ret, ret, 'year'\\n        add_century = False\\n        if len(arg) == 4:\\n            add_century = True\\n            qpats = [(qpat1, 1), (qpat2, 0)]\\n        else:\\n            qpats = [(qpat1full, 1), (qpat2full, 0)]\\n        for pat, yfirst in qpats:\\n            qparse = pat.match(arg)\\n            if qparse is not None:\\n                if yfirst:\\n                    yi, qi = 1, 2\\n                else:\\n                    yi, qi = 2, 1\\n                q = int(qparse.group(yi))\\n                y_str = qparse.group(qi)\\n                y = int(y_str)\\n                if add_century:\\n                    y += 2000\\n                if freq is not None:\\n                    mnum = _month_numbers[_get_rule_month(freq)] + 1\\n                    month = (mnum + (q - 1) * 3) % 12 + 1\\n                    if month > mnum:\\n                        y -= 1\\n                else:\\n                    month = (q - 1) * 3 + 1\\n                ret = default.replace(year=y, month=month)\\n                return ret, ret, 'quarter'\\n        is_mo_str = freq is not None and freq == 'M'\\n        is_mo_off = getattr(freq, 'rule_code', None) == 'M'\\n        is_monthly = is_mo_str or is_mo_off\\n        if len(arg) == 6 and is_monthly:\\n            try:\\n                ret = _try_parse_monthly(arg)\\n                if ret is not None:\\n                    return ret, ret, 'month'\\n            except Exception:\\n                pass\\n    mresult = _attempt_monthly(arg)\\n    if mresult:\\n        return mresult\\n    if dayfirst is None:\\n        dayfirst = print_config.date_dayfirst\\n    if yearfirst is None:\\n        yearfirst = print_config.date_yearfirst\\n    try:\\n        parsed = parse(arg, dayfirst=dayfirst, yearfirst=yearfirst)\\n    except Exception, e:\\n        raise DateParseError(e)\\n    if parsed is None:\\n        raise DateParseError(\"Could not parse %s\" % arg)\\n    repl = {}\\n    reso = 'year'\\n    for attr in [\"year\", \"month\", \"day\", \"hour\",\\n                 \"minute\", \"second\", \"microsecond\"]:\\n        value = getattr(parsed, attr)\\n        if value is not None and value != 0:  \\n            repl[attr] = value\\n            reso = attr\\n    ret = default.replace(**repl)\\n    return ret, parsed, reso"
  },
  {
    "code": "def _should_reindex_frame_op(\\n    left: \"DataFrame\", right, op, axis, default_axis, fill_value, level\\n) -> bool:\\n    assert isinstance(left, ABCDataFrame)\\n    if op is operator.pow or op is rpow:\\n        return False\\n    if not isinstance(right, ABCDataFrame):\\n        return False\\n    if fill_value is None and level is None and axis is default_axis:\\n        cols = left.columns.intersection(right.columns)\\n        if not (cols.equals(left.columns) and cols.equals(right.columns)):\\n            return True\\n    return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DTI/TDI.equals with i8 (#36744)",
    "fixed_code": "def _should_reindex_frame_op(\\n    left: \"DataFrame\", right, op, axis, default_axis, fill_value, level\\n) -> bool:\\n    assert isinstance(left, ABCDataFrame)\\n    if op is operator.pow or op is rpow:\\n        return False\\n    if not isinstance(right, ABCDataFrame):\\n        return False\\n    if fill_value is None and level is None and axis is default_axis:\\n        cols = left.columns.intersection(right.columns)\\n        if len(cols) and not (cols.equals(left.columns) and cols.equals(right.columns)):\\n            return True\\n    return False"
  },
  {
    "code": "def dispatch_to_extension_op(op, left, right):\\n    if left.dtype.kind in \"mM\":\\n        left = array(left)\\n    new_left = extract_array(left, extract_numpy=True)\\n    new_right = extract_array(right, extract_numpy=True)\\n    try:\\n        res_values = op(new_left, new_right)\\n    except NullFrequencyError:\\n        raise TypeError(\\n            \"incompatible type for a datetime/timedelta \"\\n            \"operation [{name}]\".format(name=op.__name__)\\n        )\\n    return res_values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def dispatch_to_extension_op(op, left, right):\\n    if left.dtype.kind in \"mM\":\\n        left = array(left)\\n    new_left = extract_array(left, extract_numpy=True)\\n    new_right = extract_array(right, extract_numpy=True)\\n    try:\\n        res_values = op(new_left, new_right)\\n    except NullFrequencyError:\\n        raise TypeError(\\n            \"incompatible type for a datetime/timedelta \"\\n            \"operation [{name}]\".format(name=op.__name__)\\n        )\\n    return res_values"
  },
  {
    "code": "def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,\\n\\t\\t\\t\\t\\t\\t validate_certs, force, parent=None):\\n\\tdep_msg = \"\"\\n\\tif parent:\\n\\t\\tdep_msg = \" - as dependency of %s\" % parent\\n\\tdisplay.vvv(\"Processing requirement collection '%s'%s\" % (to_text(collection), dep_msg))\\n\\tb_tar_path = None\\n\\tif os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\\n\\t\\tb_tar_path = to_bytes(collection, errors='surrogate_or_strict')\\n\\telif urlparse(collection).scheme.lower() in ['http', 'https']:\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\\n\\t\\ttry:\\n\\t\\t\\tb_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\\n\\t\\texcept urllib_error.URLError as err:\\n\\t\\t\\traise AnsibleError(\"Failed to download collection tar from '%s': %s\"\\n\\t\\t\\t\\t\\t\\t\\t   % (to_native(collection), to_native(err)))\\n\\tif b_tar_path:\\n\\t\\treq = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)\\n\\t\\tcollection_name = to_text(req)\\n\\t\\tif collection_name in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection_name]\\n\\t\\t\\tcollection_info.add_requirement(None, req.latest_version)\\n\\t\\telse:\\n\\t\\t\\tcollection_info = req\\n\\telse:\\n\\t\\tvalidate_collection_name(collection)\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is the name of a collection\" % collection)\\n\\t\\tif collection in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection]\\n\\t\\t\\tcollection_info.add_requirement(parent, requirement)\\n\\t\\telse:\\n\\t\\t\\tapis = [source] if source else apis\\n\\t\\t\\tcollection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent)\\n\\texisting = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\\n\\tif existing and not collection_info.force:\\n\\t\\texisting[0].add_requirement(to_text(collection_info), requirement)\\n\\t\\tcollection_info = existing[0]\\n\\tdep_map[to_text(collection_info)] = collection_info",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "galaxy - Fix collection install dep resolver for bad versions (#67405)",
    "fixed_code": "def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,\\n\\t\\t\\t\\t\\t\\t validate_certs, force, parent=None):\\n\\tdep_msg = \"\"\\n\\tif parent:\\n\\t\\tdep_msg = \" - as dependency of %s\" % parent\\n\\tdisplay.vvv(\"Processing requirement collection '%s'%s\" % (to_text(collection), dep_msg))\\n\\tb_tar_path = None\\n\\tif os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\\n\\t\\tb_tar_path = to_bytes(collection, errors='surrogate_or_strict')\\n\\telif urlparse(collection).scheme.lower() in ['http', 'https']:\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\\n\\t\\ttry:\\n\\t\\t\\tb_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\\n\\t\\texcept urllib_error.URLError as err:\\n\\t\\t\\traise AnsibleError(\"Failed to download collection tar from '%s': %s\"\\n\\t\\t\\t\\t\\t\\t\\t   % (to_native(collection), to_native(err)))\\n\\tif b_tar_path:\\n\\t\\treq = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)\\n\\t\\tcollection_name = to_text(req)\\n\\t\\tif collection_name in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection_name]\\n\\t\\t\\tcollection_info.add_requirement(None, req.latest_version)\\n\\t\\telse:\\n\\t\\t\\tcollection_info = req\\n\\telse:\\n\\t\\tvalidate_collection_name(collection)\\n\\t\\tdisplay.vvvv(\"Collection requirement '%s' is the name of a collection\" % collection)\\n\\t\\tif collection in dep_map:\\n\\t\\t\\tcollection_info = dep_map[collection]\\n\\t\\t\\tcollection_info.add_requirement(parent, requirement)\\n\\t\\telse:\\n\\t\\t\\tapis = [source] if source else apis\\n\\t\\t\\tcollection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent)\\n\\texisting = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\\n\\tif existing and not collection_info.force:\\n\\t\\texisting[0].add_requirement(parent, requirement)\\n\\t\\tcollection_info = existing[0]\\n\\tdep_map[to_text(collection_info)] = collection_info"
  },
  {
    "code": "def _rows_to_cols(self, content):\\n        zipped_content = list(lib.to_object_array(content).T)\\n        col_len = len(self.orig_columns)\\n        zip_len = len(zipped_content)\\n        if self._implicit_index:\\n            if np.isscalar(self.index_col):\\n                col_len += 1\\n            else:\\n                col_len += len(self.index_col)\\n        if col_len != zip_len:\\n            row_num = -1\\n            i = 0\\n            for (i, l) in enumerate(content):\\n                if len(l) != col_len:\\n                    break\\n            footers = 0\\n            if self.skip_footer:\\n                footers = self.skip_footer\\n                if footers > 0:\\n                    footers = footers - self.pos\\n            row_num = self.pos - (len(content) - i - footers)\\n            msg = ('Expecting %d columns, got %d in row %d' %\\n                   (col_len, zip_len, row_num))\\n            raise ValueError(msg)\\n        return zipped_content",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: more refactoring.  checkpoint to debug something",
    "fixed_code": "def _rows_to_cols(self, content):\\n        zipped_content = list(lib.to_object_array(content).T)\\n        col_len = len(self.orig_columns)\\n        zip_len = len(zipped_content)\\n        if self._implicit_index:\\n            col_len += len(self.index_col)\\n        if col_len != zip_len:\\n            row_num = -1\\n            i = 0\\n            for (i, l) in enumerate(content):\\n                if len(l) != col_len:\\n                    break\\n            footers = 0\\n            if self.skip_footer:\\n                footers = self.skip_footer\\n                if footers > 0:\\n                    footers = footers - self.pos\\n            row_num = self.pos - (len(content) - i - footers)\\n            msg = ('Expecting %d columns, got %d in row %d' %\\n                   (col_len, zip_len, row_num))\\n            raise ValueError(msg)\\n        return zipped_content"
  },
  {
    "code": "def _get_values(self):\\n        self._consolidate_inplace()\\n        return self._data.as_matrix()\\n    index = property(fget=lambda self: self._get_index(),\\n                     fset=lambda self, x: self._set_index(x))\\n    columns = property(fget=lambda self: self._get_columns(),\\n                     fset=lambda self, x: self._set_columns(x))\\n    values = property(fget=_get_values)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_values(self):\\n        self._consolidate_inplace()\\n        return self._data.as_matrix()\\n    index = property(fget=lambda self: self._get_index(),\\n                     fset=lambda self, x: self._set_index(x))\\n    columns = property(fget=lambda self: self._get_columns(),\\n                     fset=lambda self, x: self._set_columns(x))\\n    values = property(fget=_get_values)"
  },
  {
    "code": "def _format_col(self, i):\\n        col = self.columns[i]\\n        formatter = self.formatters.get(col)\\n        return format_array(self.frame.icol(i).values, formatter,\\n                            float_format=self.float_format,\\n                            na_rep=self.na_rep,\\n                            space=self.col_space)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _format_col(self, i):\\n        col = self.columns[i]\\n        formatter = self.formatters.get(col)\\n        return format_array(self.frame.icol(i).values, formatter,\\n                            float_format=self.float_format,\\n                            na_rep=self.na_rep,\\n                            space=self.col_space)"
  },
  {
    "code": "def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\\n\\tclone_to_dir = os.path.expanduser(clone_to_dir)\\n\\tmake_sure_path_exists(clone_to_dir)\\n\\trepo_type, repo_url = identify_repo(repo_url)\\n\\tif not is_vcs_installed(repo_type):\\n\\t\\tmsg = f\"'{repo_type}' is not installed.\"\\n\\t\\traise VCSNotInstalled(msg)\\n\\trepo_url = repo_url.rstrip('/')\\n\\trepo_name = os.path.split(repo_url)[1]\\n\\tif repo_type == 'git':\\n\\t\\trepo_name = repo_name.split(':')[-1].rsplit('.git')[0]\\n\\t\\trepo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\\n\\tif repo_type == 'hg':\\n\\t\\trepo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\\n\\tlogger.debug(f'repo_dir is {repo_dir}')\\n\\tif os.path.isdir(repo_dir):\\n\\t\\tclone = prompt_and_delete(repo_dir, no_input=no_input)\\n\\telse:\\n\\t\\tclone = True\\n\\tif clone:\\n\\t\\ttry:\\n\\t\\t\\tsubprocess.check_output(  \\n\\t\\t\\t\\t[repo_type, 'clone', repo_url],\\n\\t\\t\\t\\tcwd=clone_to_dir,\\n\\t\\t\\t\\tstderr=subprocess.STDOUT,\\n\\t\\t\\t)\\n\\t\\t\\tif checkout is not None:\\n\\t\\t\\t\\tcheckout_params = [checkout]\\n\\t\\t\\t\\tif repo_type == \"hg\":\\n\\t\\t\\t\\t\\tcheckout_params.insert(0, \"--\")\\n\\t\\t\\t\\tsubprocess.check_output(  \\n\\t\\t\\t\\t\\t[repo_type, 'checkout', *checkout_params],\\n\\t\\t\\t\\t\\tcwd=repo_dir,\\n\\t\\t\\t\\t\\tstderr=subprocess.STDOUT,\\n\\t\\t\\t\\t)\\n\\t\\texcept subprocess.CalledProcessError as clone_error:\\n\\t\\t\\toutput = clone_error.output.decode('utf-8')\\n\\t\\t\\tif 'not found' in output.lower():\\n\\t\\t\\t\\traise RepositoryNotFound(\\n\\t\\t\\t\\t\\tf'The repository {repo_url} could not be found, '\\n\\t\\t\\t\\t\\t'have you made a typo?'\\n\\t\\t\\t\\t)\\n\\t\\t\\tif any(error in output for error in BRANCH_ERRORS):\\n\\t\\t\\t\\traise RepositoryCloneFailed(\\n\\t\\t\\t\\t\\tf'The {checkout} branch of repository '\\n\\t\\t\\t\\t\\tf'{repo_url} could not found, have you made a typo?'\\n\\t\\t\\t\\t)\\n\\t\\t\\tlogger.error('git clone failed with error: %s', output)\\n\\t\\t\\traise\\n\\treturn repo_dir",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\\n\\tclone_to_dir = os.path.expanduser(clone_to_dir)\\n\\tmake_sure_path_exists(clone_to_dir)\\n\\trepo_type, repo_url = identify_repo(repo_url)\\n\\tif not is_vcs_installed(repo_type):\\n\\t\\tmsg = f\"'{repo_type}' is not installed.\"\\n\\t\\traise VCSNotInstalled(msg)\\n\\trepo_url = repo_url.rstrip('/')\\n\\trepo_name = os.path.split(repo_url)[1]\\n\\tif repo_type == 'git':\\n\\t\\trepo_name = repo_name.split(':')[-1].rsplit('.git')[0]\\n\\t\\trepo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\\n\\tif repo_type == 'hg':\\n\\t\\trepo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\\n\\tlogger.debug(f'repo_dir is {repo_dir}')\\n\\tif os.path.isdir(repo_dir):\\n\\t\\tclone = prompt_and_delete(repo_dir, no_input=no_input)\\n\\telse:\\n\\t\\tclone = True\\n\\tif clone:\\n\\t\\ttry:\\n\\t\\t\\tsubprocess.check_output(  \\n\\t\\t\\t\\t[repo_type, 'clone', repo_url],\\n\\t\\t\\t\\tcwd=clone_to_dir,\\n\\t\\t\\t\\tstderr=subprocess.STDOUT,\\n\\t\\t\\t)\\n\\t\\t\\tif checkout is not None:\\n\\t\\t\\t\\tcheckout_params = [checkout]\\n\\t\\t\\t\\tif repo_type == \"hg\":\\n\\t\\t\\t\\t\\tcheckout_params.insert(0, \"--\")\\n\\t\\t\\t\\tsubprocess.check_output(  \\n\\t\\t\\t\\t\\t[repo_type, 'checkout', *checkout_params],\\n\\t\\t\\t\\t\\tcwd=repo_dir,\\n\\t\\t\\t\\t\\tstderr=subprocess.STDOUT,\\n\\t\\t\\t\\t)\\n\\t\\texcept subprocess.CalledProcessError as clone_error:\\n\\t\\t\\toutput = clone_error.output.decode('utf-8')\\n\\t\\t\\tif 'not found' in output.lower():\\n\\t\\t\\t\\traise RepositoryNotFound(\\n\\t\\t\\t\\t\\tf'The repository {repo_url} could not be found, '\\n\\t\\t\\t\\t\\t'have you made a typo?'\\n\\t\\t\\t\\t)\\n\\t\\t\\tif any(error in output for error in BRANCH_ERRORS):\\n\\t\\t\\t\\traise RepositoryCloneFailed(\\n\\t\\t\\t\\t\\tf'The {checkout} branch of repository '\\n\\t\\t\\t\\t\\tf'{repo_url} could not found, have you made a typo?'\\n\\t\\t\\t\\t)\\n\\t\\t\\tlogger.error('git clone failed with error: %s', output)\\n\\t\\t\\traise\\n\\treturn repo_dir"
  },
  {
    "code": "def __invert__(self):\\n        return type(self)(self.queryset, self.output_field, negated=(not self.negated), **self.extra)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #28394 -- Allowed setting BaseExpression.output_field (renamed from _output_field).",
    "fixed_code": "def __invert__(self):\\n        return type(self)(self.queryset, negated=(not self.negated), **self.extra)\\n    @property"
  },
  {
    "code": "def stop_task(self, cluster, task, reason: str) -> dict:\\n        \"\"\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html\\n        ...",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add reattach flag to ECSOperator (#10643)\\n\\n..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and attach to it.",
    "fixed_code": "def stop_task(self, cluster, task, reason: str) -> Dict:\\n        \"\"\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html\\n        ..."
  },
  {
    "code": "def _box_func(self, x: np.datetime64) -> Timestamp | NaTType:\\n        value = x.view(\"i8\")\\n        ts = Timestamp(value, tz=self.tz)\\n        if ts is not NaT:  \\n            ts._set_freq(self.freq)\\n        return ts\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: support non-nano in DTA._box_func (#47299)",
    "fixed_code": "def _box_func(self, x: np.datetime64) -> Timestamp | NaTType:\\n        value = x.view(\"i8\")\\n        ts = Timestamp._from_value_and_reso(value, reso=self._reso, tz=self.tz)\\n        if ts is not NaT:  \\n            ts._set_freq(self.freq)\\n        return ts\\n    @property"
  },
  {
    "code": "def load(self, stream):\\n\\t\\t''\\n\\t\\treturn yaml.load(self.vault.decrypt(stream))",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix security issue refs #4",
    "fixed_code": "def load(self, stream):\\n\\t\\t''\\n\\t\\treturn yaml.safe_load(self.vault.decrypt(stream))"
  },
  {
    "code": "def filter_options(options):\\n    options_to_remove = [\"help\", \"print-logs-live\", \"print-logs\", \"pool\"]\\n    return [option for option in options if option not in options_to_remove]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def filter_options(options):\\n    options_to_remove = [\"help\", \"print-logs-live\", \"print-logs\", \"pool\"]\\n    return [option for option in options if option not in options_to_remove]"
  },
  {
    "code": "def insert_mode(page_html, mode):\\n\\tpage_html = page_html\\n\\tmatch_found = False\\n\\tmatches = re.finditer('workerId={{ workerid }}', page_html)\\n\\tmatch = None\\n\\tfor match in matches:\\n\\t\\tmatch_found = True\\n\\tif match_found:\\n\\t\\tnew_html = page_html[:match.end()] + \"&mode=\" + mode +\\\\n\\t\\t\\tpage_html[match.end():]\\n\\t\\treturn new_html\\n\\telse:\\n\\t\\traise ExperimentError(\"insert_mode_failed\")",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix SSTI vulnerability in ad and consent pages (#517)\\n\\n\\nFixed an issue where users could pass arbitrary Python code to be executed on the server to the mode HTTP arg",
    "fixed_code": "def insert_mode(page_html):\\n\\tpage_html = page_html\\n\\tmatch_found = False\\n\\tmatches = re.finditer('workerId={{ workerid }}', page_html)\\n\\tmatch = None\\n\\tfor match in matches:\\n\\t\\tmatch_found = True\\n\\tif match_found:\\n\\t\\tnew_html = page_html[:match.end()] + '&mode={{ mode }}' +\\\\n\\t\\t\\tpage_html[match.end():]\\n\\t\\treturn new_html\\n\\telse:\\n\\t\\traise ExperimentError(\"insert_mode_failed\")"
  },
  {
    "code": "def __deepcopy__(self, memo={}):\\n        return self._shallow_copy()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Additional keyword arguments for Index.copy()\\n\\n  `dtype` on copy. MultiIndex can set `levels`, `labels`, and `names`.\\n  copies axes as well. Defaults to False.\\n  copy its index.",
    "fixed_code": "def __deepcopy__(self, memo={}):\\n        return self.copy(deep=True)"
  },
  {
    "code": "def _build_y(self, X, y, sample_weight, trim_duplicates=True):\\n        self._check_fit_data(X, y, sample_weight)\\n        if self.increasing == 'auto':\\n            self.increasing_ = check_increasing(X, y)\\n        else:\\n            self.increasing_ = self.increasing\\n        if sample_weight is not None:\\n            sample_weight = check_array(sample_weight, ensure_2d=False,\\n                                        dtype=X.dtype)\\n            mask = sample_weight > 0\\n            X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\\n        else:\\n            sample_weight = np.ones(len(y), dtype=X.dtype)\\n        order = np.lexsort((y, X))\\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\\n        unique_X, unique_y, unique_sample_weight = _make_unique(\\n            X, y, sample_weight)\\n        self._X_ = X = unique_X\\n        self._y_ = y = isotonic_regression(unique_y, unique_sample_weight,\\n                                           self.y_min, self.y_max,\\n                                           increasing=self.increasing_)\\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\\n        if trim_duplicates:\\n            keep_data = np.ones((len(y),), dtype=bool)\\n            keep_data[1:-1] = np.logical_or(\\n                np.not_equal(y[1:-1], y[:-2]),\\n                np.not_equal(y[1:-1], y[2:])\\n            )\\n            return X[keep_data], y[keep_data]\\n        else:\\n            return X, y",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_y(self, X, y, sample_weight, trim_duplicates=True):\\n        self._check_fit_data(X, y, sample_weight)\\n        if self.increasing == 'auto':\\n            self.increasing_ = check_increasing(X, y)\\n        else:\\n            self.increasing_ = self.increasing\\n        if sample_weight is not None:\\n            sample_weight = check_array(sample_weight, ensure_2d=False,\\n                                        dtype=X.dtype)\\n            mask = sample_weight > 0\\n            X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\\n        else:\\n            sample_weight = np.ones(len(y), dtype=X.dtype)\\n        order = np.lexsort((y, X))\\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\\n        unique_X, unique_y, unique_sample_weight = _make_unique(\\n            X, y, sample_weight)\\n        self._X_ = X = unique_X\\n        self._y_ = y = isotonic_regression(unique_y, unique_sample_weight,\\n                                           self.y_min, self.y_max,\\n                                           increasing=self.increasing_)\\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\\n        if trim_duplicates:\\n            keep_data = np.ones((len(y),), dtype=bool)\\n            keep_data[1:-1] = np.logical_or(\\n                np.not_equal(y[1:-1], y[:-2]),\\n                np.not_equal(y[1:-1], y[2:])\\n            )\\n            return X[keep_data], y[keep_data]\\n        else:\\n            return X, y"
  },
  {
    "code": "def put_nowait(self, item):\\n        self._consume_done_getters()\\n        if self._getters:\\n            assert not self._queue, (\\n                'queue non-empty, why are getters waiting?')\\n            getter = self._getters.popleft()\\n            self._put(item)\\n            getter.set_result(self._get())\\n        elif self._maxsize > 0 and self._maxsize == self.qsize():\\n            raise Full\\n        else:\\n            self._put(item)\\n    @coroutine",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio: Rename {Empty,Full} to {QueueEmpty,QueueFull} and no longer get them from queue.py.",
    "fixed_code": "def put_nowait(self, item):\\n        self._consume_done_getters()\\n        if self._getters:\\n            assert not self._queue, (\\n                'queue non-empty, why are getters waiting?')\\n            getter = self._getters.popleft()\\n            self._put(item)\\n            getter.set_result(self._get())\\n        elif self._maxsize > 0 and self._maxsize == self.qsize():\\n            raise QueueFull\\n        else:\\n            self._put(item)\\n    @coroutine"
  },
  {
    "code": "def __contains__(self, key):\\n        return key == DEFAULTSECT or self.has_section(key)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "configparser: the name of the DEFAULT section is now customizable",
    "fixed_code": "def __contains__(self, key):\\n        return key == self._default_section or self.has_section(key)"
  },
  {
    "code": "def member_count(self):\\n        return self._values['stats']['memberCnt']\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes issue with sub collection for pool members (#53955)\\n\\nRefactors main() function and module manager in multiple modules in line with recent changes\\nAdds variable types to docs\\nRefactors unit tests to remove deprecated parameters",
    "fixed_code": "def member_count(self):\\n        if 'memberCnt' in self._values['stats']:\\n            return self._values['stats']['memberCnt']\\n        return None\\n    @property"
  },
  {
    "code": "def __mul__(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        if context is None:\\n            context = getcontext()\\n        resultsign = self._sign ^ other._sign\\n        if self._is_special or other._is_special:\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n            if self._isinfinity():\\n                if not other:\\n                    return context._raise_error(InvalidOperation, '(+-)INF * 0')\\n                return Infsign[resultsign]\\n            if other._isinfinity():\\n                if not self:\\n                    return context._raise_error(InvalidOperation, '0 * (+-)INF')\\n                return Infsign[resultsign]\\n        resultexp = self._exp + other._exp\\n        shouldround = context._rounding_decision == ALWAYS_ROUND\\n        if not self or not other:\\n            ans = Decimal((resultsign, (0,), resultexp))\\n            if shouldround:\\n                ans = ans._fix(context)\\n            return ans\\n        if self._int == (1,):\\n            ans = Decimal((resultsign, other._int, resultexp))\\n            if shouldround:\\n                ans = ans._fix(context)\\n            return ans\\n        if other._int == (1,):\\n            ans = Decimal((resultsign, self._int, resultexp))\\n            if shouldround:\\n                ans = ans._fix(context)\\n            return ans\\n        op1 = _WorkRep(self)\\n        op2 = _WorkRep(other)\\n        ans = Decimal( (resultsign, map(int, str(op1.int * op2.int)), resultexp))\\n        if shouldround:\\n            ans = ans._fix(context)\\n        return ans\\n    __rmul__ = __mul__",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def __mul__(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        if context is None:\\n            context = getcontext()\\n        resultsign = self._sign ^ other._sign\\n        if self._is_special or other._is_special:\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n            if self._isinfinity():\\n                if not other:\\n                    return context._raise_error(InvalidOperation, '(+-)INF * 0')\\n                return Infsign[resultsign]\\n            if other._isinfinity():\\n                if not self:\\n                    return context._raise_error(InvalidOperation, '0 * (+-)INF')\\n                return Infsign[resultsign]\\n        resultexp = self._exp + other._exp\\n        if not self or not other:\\n            ans = _dec_from_triple(resultsign, '0', resultexp)\\n            ans = ans._fix(context)\\n            return ans\\n        if self._int == '1':\\n            ans = _dec_from_triple(resultsign, other._int, resultexp)\\n            ans = ans._fix(context)\\n            return ans\\n        if other._int == '1':\\n            ans = _dec_from_triple(resultsign, self._int, resultexp)\\n            ans = ans._fix(context)\\n            return ans\\n        op1 = _WorkRep(self)\\n        op2 = _WorkRep(other)\\n        ans = _dec_from_triple(resultsign, str(op1.int * op2.int), resultexp)\\n        ans = ans._fix(context)\\n        return ans\\n    __rmul__ = __mul__"
  },
  {
    "code": "def _run_command(self, command, data, *params):\\n\\t\\tcmd = [\"collie\", \"vdi\"]\\n\\t\\tcmd.extend(command)\\n\\t\\tcmd.extend([\"-a\", self.addr, \"-p\", self.port, self.name])\\n\\t\\tcmd.extend(params)\\n\\t\\ttry:\\n\\t\\t\\treturn processutils.execute(*cmd, process_input=data)[0]\\n\\t\\texcept (processutils.ProcessExecutionError, OSError) as exc:\\n\\t\\t\\tLOG.error(exc)\\n\\t\\t\\traise glance.store.BackendException(exc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _run_command(self, command, data, *params):\\n\\t\\tcmd = [\"collie\", \"vdi\"]\\n\\t\\tcmd.extend(command)\\n\\t\\tcmd.extend([\"-a\", self.addr, \"-p\", self.port, self.name])\\n\\t\\tcmd.extend(params)\\n\\t\\ttry:\\n\\t\\t\\treturn processutils.execute(*cmd, process_input=data)[0]\\n\\t\\texcept (processutils.ProcessExecutionError, OSError) as exc:\\n\\t\\t\\tLOG.error(exc)\\n\\t\\t\\traise glance.store.BackendException(exc)"
  },
  {
    "code": "def merge_provider_user_param(self, result, provider):\\n        if self.validate_params('user', provider):\\n            result['user'] = provider['user']\\n        elif self.validate_params('user', self.params):\\n            result['user'] = self.params['user']\\n        elif self.validate_params('F5_USER', os.environ):\\n            result['user'] = os.environ.get('F5_USER')\\n        elif self.validate_params('ANSIBLE_NET_USERNAME', os.environ):\\n            result['user'] = os.environ.get('ANSIBLE_NET_USERNAME')\\n        else:\\n            result['user'] = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "removes args from the code and corrects few missed tests (#58175)",
    "fixed_code": "def merge_provider_user_param(self, result, provider):\\n        if self.validate_params('user', provider):\\n            result['user'] = provider['user']\\n        elif self.validate_params('F5_USER', os.environ):\\n            result['user'] = os.environ.get('F5_USER')\\n        elif self.validate_params('ANSIBLE_NET_USERNAME', os.environ):\\n            result['user'] = os.environ.get('ANSIBLE_NET_USERNAME')\\n        else:\\n            result['user'] = None"
  },
  {
    "code": "def get_field(self, field_name):\\n        field_name = (\\n            self.options['order_with_respect_to']\\n            if field_name == '_order'\\n            else field_name\\n        )\\n        return self.fields[field_name]\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #33449 -- Fixed makemigrations crash on models without Meta.order_with_respect_to but with _order field.\\n\\nRegression in aa4acc164d1247c0de515c959f7b09648b57dc42.",
    "fixed_code": "def get_field(self, field_name):\\n        if field_name == '_order':\\n            field_name = self.options.get('order_with_respect_to', field_name)\\n        return self.fields[field_name]\\n    @classmethod"
  },
  {
    "code": "def __init__(self, attrs=None, years=None, required=True, months=None):\\n        self.attrs = attrs or {}\\n        self.required = required\\n        if years:\\n            self.years = years\\n        else:\\n            this_year = datetime.date.today().year\\n            self.years = range(this_year, this_year + 10)\\n        if months:\\n            self.months = months\\n        else:\\n            self.months = MONTHS",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #13970 -- Made SelectDateWidget use the standard widget is_required attribute\\n\\nThanks mitar for the report and Tim Graham for the review.",
    "fixed_code": "def __init__(self, attrs=None, years=None, months=None):\\n        self.attrs = attrs or {}\\n        if years:\\n            self.years = years\\n        else:\\n            this_year = datetime.date.today().year\\n            self.years = range(this_year, this_year + 10)\\n        if months:\\n            self.months = months\\n        else:\\n            self.months = MONTHS"
  },
  {
    "code": "def naturaltime(value, arg=None):\\n    try:\\n        value = datetime(value.year, value.month, value.day, value.hour, value.minute, value.second)\\n    except AttributeError:\\n        return value\\n    except ValueError:\\n        return value\\n    delta = datetime.now() - value\\n    if delta.days != 0:\\n        value = date(value.year, value.month, value.day)\\n        return naturalday(value, arg)\\n    elif delta.seconds == 0:\\n        return _(u'now')\\n    elif delta.seconds < 60:\\n        return ungettext(u'%s seconds ago', u'%s seconds ago', delta.seconds)\\n    elif delta.seconds / 60 < 60:\\n        return ungettext(u'a minute ago', u'%s minutes ago', delta.seconds/60)\\n    elif delta.seconds / 60 / 60 < 24:\\n        return ungettext(u'an hour ago', u'%s hours ago', delta.seconds/60/60)\\n    return naturalday(value, arg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #15921 -- Refined naturaltime filter added in r16071 to use timesince and timeuntil filters as fallbacks instead of date filter.",
    "fixed_code": "def naturaltime(value):\\n    try:\\n        value = datetime(value.year, value.month, value.day, value.hour, value.minute, value.second)\\n    except AttributeError:\\n        return value\\n    except ValueError:\\n        return value\\n    if getattr(value, 'tzinfo', None):\\n        now = datetime.now(LocalTimezone(value))\\n    else:\\n        now = datetime.now()\\n    now = now - timedelta(0, 0, now.microsecond)\\n    if value < now:\\n        delta = now - value\\n        if delta.days != 0:\\n            return pgettext(\\n                'naturaltime', '%(delta)s ago'\\n            ) % {'delta': defaultfilters.timesince(value)}\\n        elif delta.seconds == 0:\\n            return _(u'now')\\n        elif delta.seconds < 60:\\n            return ungettext(\\n                u'a second ago', u'%(count)s seconds ago', delta.seconds\\n            ) % {'count': delta.seconds}\\n        elif delta.seconds / 60 < 60:\\n            count = delta.seconds / 60\\n            return ungettext(\\n                u'a minute ago', u'%(count)s minutes ago', count\\n            ) % {'count': count}\\n        else:\\n            count = delta.seconds / 60 / 60\\n            return ungettext(\\n                u'an hour ago', u'%(count)s hours ago', count\\n            ) % {'count': count}\\n    else:\\n        delta = value - now\\n        if delta.days != 0:\\n            return pgettext(\\n                'naturaltime', '%(delta)s from now'\\n            ) % {'delta': defaultfilters.timeuntil(value)}\\n        elif delta.seconds == 0:\\n            return _(u'now')\\n        elif delta.seconds < 60:\\n            return ungettext(\\n                u'a second from now', u'%(count)s seconds from now', delta.seconds\\n            ) % {'count': delta.seconds}\\n        elif delta.seconds / 60 < 60:\\n            count = delta.seconds / 60\\n            return ungettext(\\n                u'a minute from now', u'%(count)s minutes from now', count\\n            ) % {'count': count}\\n        else:\\n            count = delta.seconds / 60 / 60\\n            return ungettext(\\n                u'an hour from now', u'%(count)s hours from now', count\\n            ) % {'count': count}"
  },
  {
    "code": "def get_pandas_df(self, bql, parameters=None, dialect=None):\\n        if dialect is None:\\n            dialect = 'legacy' if self.use_legacy_sql else 'standard'\\n        return read_gbq(bql,\\n                        project_id=self._get_field('project'),\\n                        dialect=dialect,\\n                        verbose=False)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2513] Change `bql` to `sql` for BigQuery Hooks & Ops\\n\\n- Change `bql` to `sql` for BigQuery Hooks &\\nOperators for consistency\\n\\nCloses #3454 from kaxil/consistent-bq-lang",
    "fixed_code": "def get_pandas_df(self, sql, parameters=None, dialect=None):\\n        if dialect is None:\\n            dialect = 'legacy' if self.use_legacy_sql else 'standard'\\n        return read_gbq(sql,\\n                        project_id=self._get_field('project'),\\n                        dialect=dialect,\\n                        verbose=False)"
  },
  {
    "code": "def check_secure(line, conf, strict=None, ssh=None):\\n\\toline = line\\n\\tline = line.strip()\\n\\treturncode = 0\\n\\trelist = re.findall(r'[^=]\\\"(.+)\\\"', line)\\n\\trelist2 = re.findall(r'[^=]\\'(.+)\\'', line)\\n\\trelist = relist + relist2\\n\\tfor item in relist:\\n\\t\\tif os.path.exists(item):\\n\\t\\t\\tret_check_path, conf = check_path(item, conf, strict=strict)\\n\\t\\t\\treturncode += ret_check_path\\n\\tif re.findall(r'[\\x01-\\x1F\\x7F]', oline):\\n\\t\\tret, conf = warn_count('control char',\\n\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\treturn ret, conf\\n\\tfor item in conf['forbidden']:\\n\\t\\tif item in ['&', '|']:\\n\\t\\t\\tif re.findall(\"[^\\%s]\\%s[^\\%s]\" % (item, item, item), line):\\n\\t\\t\\t\\tret, conf = warn_count('syntax',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\t\\treturn ret, conf\\n\\t\\telse:\\n\\t\\t\\tif item in line:\\n\\t\\t\\t\\tret, conf = warn_count('syntax',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\t\\treturn ret, conf\\n\\texecutions = re.findall('\\$\\([^)]+[)]', line)\\n\\tfor item in executions:\\n\\t\\tret_check_path, conf = check_path(item[2:-1].strip(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_path\\n\\t\\tret_check_secure, conf = check_secure(item[2:-1].strip(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_secure\\n\\texecutions = re.findall('\\`[^`]+[`]', line)\\n\\tfor item in executions:\\n\\t\\tret_check_secure, conf = check_secure(item[1:-1].strip(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_secure\\n\\tcurly = re.findall('\\$\\{[^}]+[}]', line)\\n\\tfor item in curly:\\n\\t\\tif re.findall(r'=|\\+|\\?|\\-', item):\\n\\t\\t\\tvariable = re.split('=|\\+|\\?|\\-', item, 1)\\n\\t\\telse:\\n\\t\\t\\tvariable = item\\n\\t\\tret_check_path, conf = check_path(variable[1][:-1],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_path\\n\\tif returncode > 0:\\n\\t\\treturn 1, conf\\n\\telif line.startswith('$(') or line.startswith('`'):\\n\\t\\treturn 0, conf\\n\\tlines = []\\n\\tif line[0] in [\"&\", \"|\", \";\"]:\\n\\t\\tstart = 1\\n\\telse:\\n\\t\\tstart = 0\\n\\tfor i in range(1, len(line)):\\n\\t\\tif line[i] in [\"&\", \"|\", \";\"] and line[i - 1] != \"\\\\\":\\n\\t\\t\\tif start != i:\\n\\t\\t\\t\\tlines.append(line[start:i])\\n\\t\\t\\tstart = i + 1\\n\\tif start != len(line):\\n\\t\\tlines.append(line[start:len(line)])\\n\\tline = re.sub('\\)$', '', line)\\n\\tfor separate_line in lines:\\n\\t\\tseparate_line = \" \".join(separate_line.split())\\n\\t\\tsplitcmd = separate_line.strip().split(' ')\\n\\t\\tcommand = splitcmd[0]\\n\\t\\tif len(splitcmd) > 1:\\n\\t\\t\\tcmdargs = splitcmd\\n\\t\\telse:\\n\\t\\t\\tcmdargs = None\\n\\t\\tif command == 'sudo':\\n\\t\\t\\tif type(cmdargs) == list:\\n\\t\\t\\t\\tif cmdargs[1] == '-u' and cmdargs:\\n\\t\\t\\t\\t\\tsudocmd = cmdargs[3]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tsudocmd = cmdargs[1]\\n\\t\\t\\t\\tif sudocmd not in conf['sudo_commands'] and cmdargs:\\n\\t\\t\\t\\t\\tret, conf = warn_count('sudo command',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\t\\t\\treturn ret, conf\\n\\t\\tif ssh:\\n\\t\\t\\tconf['allowed'] = conf['overssh']\\n\\t\\tif command not in conf['allowed'] and command:\\n\\t\\t\\tret, conf = warn_count('command',\\n\\t\\t\\t\\t\\t\\t\\t\\t   command,\\n\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\treturn ret, conf\\n\\treturn 0, conf",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_secure(line, conf, strict=None, ssh=None):\\n\\toline = line\\n\\tline = line.strip()\\n\\treturncode = 0\\n\\trelist = re.findall(r'[^=]\\\"(.+)\\\"', line)\\n\\trelist2 = re.findall(r'[^=]\\'(.+)\\'', line)\\n\\trelist = relist + relist2\\n\\tfor item in relist:\\n\\t\\tif os.path.exists(item):\\n\\t\\t\\tret_check_path, conf = check_path(item, conf, strict=strict)\\n\\t\\t\\treturncode += ret_check_path\\n\\tif re.findall(r'[\\x01-\\x1F\\x7F]', oline):\\n\\t\\tret, conf = warn_count('control char',\\n\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\treturn ret, conf\\n\\tfor item in conf['forbidden']:\\n\\t\\tif item in ['&', '|']:\\n\\t\\t\\tif re.findall(\"[^\\%s]\\%s[^\\%s]\" % (item, item, item), line):\\n\\t\\t\\t\\tret, conf = warn_count('syntax',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\t\\treturn ret, conf\\n\\t\\telse:\\n\\t\\t\\tif item in line:\\n\\t\\t\\t\\tret, conf = warn_count('syntax',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\t\\treturn ret, conf\\n\\texecutions = re.findall('\\$\\([^)]+[)]', line)\\n\\tfor item in executions:\\n\\t\\tret_check_path, conf = check_path(item[2:-1].strip(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_path\\n\\t\\tret_check_secure, conf = check_secure(item[2:-1].strip(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_secure\\n\\texecutions = re.findall('\\`[^`]+[`]', line)\\n\\tfor item in executions:\\n\\t\\tret_check_secure, conf = check_secure(item[1:-1].strip(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_secure\\n\\tcurly = re.findall('\\$\\{[^}]+[}]', line)\\n\\tfor item in curly:\\n\\t\\tif re.findall(r'=|\\+|\\?|\\-', item):\\n\\t\\t\\tvariable = re.split('=|\\+|\\?|\\-', item, 1)\\n\\t\\telse:\\n\\t\\t\\tvariable = item\\n\\t\\tret_check_path, conf = check_path(variable[1][:-1],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=strict)\\n\\t\\treturncode += ret_check_path\\n\\tif returncode > 0:\\n\\t\\treturn 1, conf\\n\\telif line.startswith('$(') or line.startswith('`'):\\n\\t\\treturn 0, conf\\n\\tlines = []\\n\\tif line[0] in [\"&\", \"|\", \";\"]:\\n\\t\\tstart = 1\\n\\telse:\\n\\t\\tstart = 0\\n\\tfor i in range(1, len(line)):\\n\\t\\tif line[i] in [\"&\", \"|\", \";\"] and line[i - 1] != \"\\\\\":\\n\\t\\t\\tif start != i:\\n\\t\\t\\t\\tlines.append(line[start:i])\\n\\t\\t\\tstart = i + 1\\n\\tif start != len(line):\\n\\t\\tlines.append(line[start:len(line)])\\n\\tline = re.sub('\\)$', '', line)\\n\\tfor separate_line in lines:\\n\\t\\tseparate_line = \" \".join(separate_line.split())\\n\\t\\tsplitcmd = separate_line.strip().split(' ')\\n\\t\\tcommand = splitcmd[0]\\n\\t\\tif len(splitcmd) > 1:\\n\\t\\t\\tcmdargs = splitcmd\\n\\t\\telse:\\n\\t\\t\\tcmdargs = None\\n\\t\\tif command == 'sudo':\\n\\t\\t\\tif type(cmdargs) == list:\\n\\t\\t\\t\\tif cmdargs[1] == '-u' and cmdargs:\\n\\t\\t\\t\\t\\tsudocmd = cmdargs[3]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tsudocmd = cmdargs[1]\\n\\t\\t\\t\\tif sudocmd not in conf['sudo_commands'] and cmdargs:\\n\\t\\t\\t\\t\\tret, conf = warn_count('sudo command',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   oline,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\t\\t\\treturn ret, conf\\n\\t\\tif ssh:\\n\\t\\t\\tconf['allowed'] = conf['overssh']\\n\\t\\tif command not in conf['allowed'] and command:\\n\\t\\t\\tret, conf = warn_count('command',\\n\\t\\t\\t\\t\\t\\t\\t\\t   command,\\n\\t\\t\\t\\t\\t\\t\\t\\t   conf,\\n\\t\\t\\t\\t\\t\\t\\t\\t   strict=strict,\\n\\t\\t\\t\\t\\t\\t\\t\\t   ssh=ssh)\\n\\t\\t\\treturn ret, conf\\n\\treturn 0, conf"
  },
  {
    "code": "def _signature_from_callable(obj, *,\\n                             follow_wrapper_chains=True,\\n                             skip_bound_arg=True,\\n                             sigcls):\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = _signature_from_callable(\\n            obj.__func__,\\n            follow_wrapper_chains=follow_wrapper_chains,\\n            skip_bound_arg=skip_bound_arg,\\n            sigcls=sigcls)\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if follow_wrapper_chains:\\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n        if isinstance(obj, types.MethodType):\\n            return _signature_from_callable(\\n                obj,\\n                follow_wrapper_chains=follow_wrapper_chains,\\n                skip_bound_arg=skip_bound_arg,\\n                sigcls=sigcls)\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            if not isinstance(sig, Signature):\\n                raise TypeError(\\n                    'unexpected object {!r} in __signature__ '\\n                    'attribute'.format(sig))\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        if isinstance(partialmethod, functools.partialmethod):\\n            wrapped_sig = _signature_from_callable(\\n                partialmethod.func,\\n                follow_wrapper_chains=follow_wrapper_chains,\\n                skip_bound_arg=skip_bound_arg,\\n                sigcls=sigcls)\\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\\n                return sig\\n            else:\\n                sig_params = tuple(sig.parameters.values())\\n                assert (not sig_params or\\n                        first_wrapped_param is not sig_params[0])\\n                new_params = (first_wrapped_param,) + sig_params\\n                return sig.replace(parameters=new_params)\\n    if isfunction(obj) or _signature_is_functionlike(obj):\\n        return _signature_from_function(sigcls, obj)\\n    if _signature_is_builtin(obj):\\n        return _signature_from_builtin(sigcls, obj,\\n                                       skip_bound_arg=skip_bound_arg)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = _signature_from_callable(\\n            obj.func,\\n            follow_wrapper_chains=follow_wrapper_chains,\\n            skip_bound_arg=skip_bound_arg,\\n            sigcls=sigcls)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = _signature_from_callable(\\n                call,\\n                follow_wrapper_chains=follow_wrapper_chains,\\n                skip_bound_arg=skip_bound_arg,\\n                sigcls=sigcls)\\n        else:\\n            new = _signature_get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = _signature_from_callable(\\n                    new,\\n                    follow_wrapper_chains=follow_wrapper_chains,\\n                    skip_bound_arg=skip_bound_arg,\\n                    sigcls=sigcls)\\n            else:\\n                init = _signature_get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = _signature_from_callable(\\n                        init,\\n                        follow_wrapper_chains=follow_wrapper_chains,\\n                        skip_bound_arg=skip_bound_arg,\\n                        sigcls=sigcls)\\n        if sig is None:\\n            for base in obj.__mro__[:-1]:\\n                try:\\n                    text_sig = base.__text_signature__\\n                except AttributeError:\\n                    pass\\n                else:\\n                    if text_sig:\\n                        return _signature_fromstr(sigcls, obj, text_sig)\\n            if type not in obj.__mro__:\\n                if (obj.__init__ is object.__init__ and\\n                    obj.__new__ is object.__new__):\\n                    return signature(object)\\n                else:\\n                    raise ValueError(\\n                        'no signature found for builtin type {!r}'.format(obj))\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            try:\\n                sig = _signature_from_callable(\\n                    call,\\n                    follow_wrapper_chains=follow_wrapper_chains,\\n                    skip_bound_arg=skip_bound_arg,\\n                    sigcls=sigcls)\\n            except ValueError as ex:\\n                msg = 'no signature found for {!r}'.format(obj)\\n                raise ValueError(msg) from ex\\n    if sig is not None:\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-36542: Allow to overwrite the signature for Python functions. (GH-12705)",
    "fixed_code": "def _signature_from_callable(obj, *,\\n                             follow_wrapper_chains=True,\\n                             skip_bound_arg=True,\\n                             sigcls):\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = _signature_from_callable(\\n            obj.__func__,\\n            follow_wrapper_chains=follow_wrapper_chains,\\n            skip_bound_arg=skip_bound_arg,\\n            sigcls=sigcls)\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if follow_wrapper_chains:\\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n        if isinstance(obj, types.MethodType):\\n            return _signature_from_callable(\\n                obj,\\n                follow_wrapper_chains=follow_wrapper_chains,\\n                skip_bound_arg=skip_bound_arg,\\n                sigcls=sigcls)\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            if not isinstance(sig, Signature):\\n                raise TypeError(\\n                    'unexpected object {!r} in __signature__ '\\n                    'attribute'.format(sig))\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        if isinstance(partialmethod, functools.partialmethod):\\n            wrapped_sig = _signature_from_callable(\\n                partialmethod.func,\\n                follow_wrapper_chains=follow_wrapper_chains,\\n                skip_bound_arg=skip_bound_arg,\\n                sigcls=sigcls)\\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\\n                return sig\\n            else:\\n                sig_params = tuple(sig.parameters.values())\\n                assert (not sig_params or\\n                        first_wrapped_param is not sig_params[0])\\n                new_params = (first_wrapped_param,) + sig_params\\n                return sig.replace(parameters=new_params)\\n    if isfunction(obj) or _signature_is_functionlike(obj):\\n        return _signature_from_function(sigcls, obj,\\n                                        skip_bound_arg=skip_bound_arg)\\n    if _signature_is_builtin(obj):\\n        return _signature_from_builtin(sigcls, obj,\\n                                       skip_bound_arg=skip_bound_arg)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = _signature_from_callable(\\n            obj.func,\\n            follow_wrapper_chains=follow_wrapper_chains,\\n            skip_bound_arg=skip_bound_arg,\\n            sigcls=sigcls)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = _signature_from_callable(\\n                call,\\n                follow_wrapper_chains=follow_wrapper_chains,\\n                skip_bound_arg=skip_bound_arg,\\n                sigcls=sigcls)\\n        else:\\n            new = _signature_get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = _signature_from_callable(\\n                    new,\\n                    follow_wrapper_chains=follow_wrapper_chains,\\n                    skip_bound_arg=skip_bound_arg,\\n                    sigcls=sigcls)\\n            else:\\n                init = _signature_get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = _signature_from_callable(\\n                        init,\\n                        follow_wrapper_chains=follow_wrapper_chains,\\n                        skip_bound_arg=skip_bound_arg,\\n                        sigcls=sigcls)\\n        if sig is None:\\n            for base in obj.__mro__[:-1]:\\n                try:\\n                    text_sig = base.__text_signature__\\n                except AttributeError:\\n                    pass\\n                else:\\n                    if text_sig:\\n                        return _signature_fromstr(sigcls, obj, text_sig)\\n            if type not in obj.__mro__:\\n                if (obj.__init__ is object.__init__ and\\n                    obj.__new__ is object.__new__):\\n                    return signature(object)\\n                else:\\n                    raise ValueError(\\n                        'no signature found for builtin type {!r}'.format(obj))\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            try:\\n                sig = _signature_from_callable(\\n                    call,\\n                    follow_wrapper_chains=follow_wrapper_chains,\\n                    skip_bound_arg=skip_bound_arg,\\n                    sigcls=sigcls)\\n            except ValueError as ex:\\n                msg = 'no signature found for {!r}'.format(obj)\\n                raise ValueError(msg) from ex\\n    if sig is not None:\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))"
  },
  {
    "code": "def load(self):\\n\\t\\ttry:\\n\\t\\t\\tsession_data = self._cache.get(self.cache_key, None)\\n\\t\\texcept Exception:\\n\\t\\t\\tsession_data = None\\n\\t\\tif session_data is not None:\\n\\t\\t\\treturn session_data\\n\\t\\tself.create()\\n\\t\\treturn {}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[1.4.x] Fixed #19324 -- Avoided creating a session record when loading the session.\\n\\nThe session record is now only created if/when the session is modified. This\\nprevents a potential DoS via creation of many empty session records.\\n\\nThis is a security fix; disclosure to follow shortly.",
    "fixed_code": "def load(self):\\n\\t\\ttry:\\n\\t\\t\\tsession_data = self._cache.get(self.cache_key, None)\\n\\t\\texcept Exception:\\n\\t\\t\\tsession_data = None\\n\\t\\tif session_data is not None:\\n\\t\\t\\treturn session_data\\n\\t\\tself._session_key = None\\n\\t\\treturn {}"
  },
  {
    "code": "def weighted_r2_score(y_true, y_pred, weights=None):\\t\\ty_true, y_pred = check_arrays(y_true, y_pred)\\tif len(y_true) == 1:\\t\\traise ValueError(\"r2_score can only be computed given more than one\"\\t\\t\\t\\t\\t\\t \" sample.\")\\tif weights is None:\\t\\tnumerator = ((y_true - y_pred) ** 2).sum()\\t\\tdenominator = ((y_true - y_true.mean()) ** 2).sum()\\telse:\\t\\tnumerator = (weights * (y_true - y_pred) ** 2).sum()\\t\\tdenominator = (weights * (y_true - y_true.mean()) ** 2).sum()\\tif denominator == 0.0:\\t\\tif numerator == 0.0:\\t\\t\\treturn 1.0\\t\\telse:",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "raise ValueError if len(y_true) is less than or equal to 1 add some tests for weighted_r2_score",
    "fixed_code": "def weighted_r2_score(y_true, y_pred, weights=None):\\n\\ty_true, y_pred = check_arrays(y_true, y_pred)\\n\\tif len(y_true) <= 1:\\n\\t\\traise ValueError(\"r2_score can only be computed given more than one\"\\n\\t\\t\\t\\t\\t\\t \" sample.\")\\n\\tif weights is None:\\n\\t\\tnumerator = ((y_true - y_pred) ** 2).sum()\\n\\t\\tdenominator = ((y_true - y_true.mean()) ** 2).sum()\\n\\telse:\\n\\t\\tnumerator = (weights * (y_true - y_pred) ** 2).sum()\\n\\t\\tdenominator = (weights * (y_true - y_true.mean()) ** 2).sum()\\n\\tif denominator == 0.0:\\n\\t\\tif numerator == 0.0:\\n\\t\\t\\treturn 1.0\\n\\t\\telse:\\n\\t\\t\\treturn 0.0\\n\\treturn 1 - numerator / denominator"
  },
  {
    "code": "def generate_commands(vlan_id, to_set, to_remove):\\n\\tcommands = []\\n\\tif \"vlan_id\" in to_remove:\\n\\t\\treturn [\"no vlan {0}\".format(vlan_id)]\\n\\tfor key, value in to_set.items():\\n\\t\\tif key == \"vlan_id\" or value is None:\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"{0} {1}\".format(key, value))\\n\\tfor key in to_remove:\\n\\t\\tcommands.append(\"no {0}\".format(key))\\n\\tif commands:\\n\\t\\tcommands.insert(0, \"vlan {0}\".format(vlan_id))\\n\\treturn commands",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix to - eos_vlans using state parameter replaced is giving odd behavior  (#67318)",
    "fixed_code": "def generate_commands(vlan_id, to_set, to_remove):\\n\\tcommands = []\\n\\tif \"vlan_id\" in to_remove:\\n\\t\\treturn [\"no vlan {0}\".format(vlan_id)]\\n\\tfor key in to_remove:\\n\\t\\tif key in to_set.keys():\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"no {0}\".format(key))\\n\\tfor key, value in to_set.items():\\n\\t\\tif key == \"vlan_id\" or value is None:\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"{0} {1}\".format(key, value))\\n\\tif commands:\\n\\t\\tcommands.insert(0, \"vlan {0}\".format(vlan_id))\\n\\treturn commands"
  },
  {
    "code": "def get_pending_tasks(self, state):\\n\\t\\tif len(self.tasks) < state.num_pending_tasks():\\n\\t\\t\\treturn six.moves.filter(lambda task: task.status in [PENDING, RUNNING],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tself.tasks)\\n\\t\\telse:\\n\\t\\t\\treturn six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_pending_tasks(self, state):\\n\\t\\tif len(self.tasks) < state.num_pending_tasks():\\n\\t\\t\\treturn six.moves.filter(lambda task: task.status in [PENDING, RUNNING],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tself.tasks)\\n\\t\\telse:\\n\\t\\t\\treturn six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())"
  },
  {
    "code": "def _sub_datetime_arraylike(self, other):\\n        if self.dtype.kind != \"M\":\\n            raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\\n        if len(self) != len(other):\\n            raise ValueError(\"cannot add indices of unequal length\")\\n        self = cast(\"DatetimeArray\", self)\\n        other = ensure_wrapped_if_datetimelike(other)\\n        self, other = self._ensure_matching_resos(other)\\n        return self._sub_datetimelike(other)\\n    @final",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: tighter typing in datetimelike arith methods (#48962)",
    "fixed_code": "def _sub_datetime_arraylike(self, other: DatetimeArray):\\n        if self.dtype.kind != \"M\":\\n            raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\\n        if len(self) != len(other):\\n            raise ValueError(\"cannot add indices of unequal length\")\\n        self = cast(\"DatetimeArray\", self)\\n        self, other = self._ensure_matching_resos(other)\\n        return self._sub_datetimelike(other)\\n    @final"
  },
  {
    "code": "def readline(self):\\n        if self._exception is not None:\\n            raise self._exception\\n        parts = []\\n        parts_size = 0\\n        not_enough = True\\n        while not_enough:\\n            while self._buffer and not_enough:\\n                data = self._buffer.popleft()\\n                ichar = data.find(b'\\n')\\n                if ichar < 0:\\n                    parts.append(data)\\n                    parts_size += len(data)\\n                else:\\n                    ichar += 1\\n                    head, tail = data[:ichar], data[ichar:]\\n                    if tail:\\n                        self._buffer.appendleft(tail)\\n                    not_enough = False\\n                    parts.append(head)\\n                    parts_size += len(head)\\n                if parts_size > self._limit:\\n                    self._byte_count -= parts_size\\n                    self._maybe_resume_transport()\\n                    raise ValueError('Line is too long')\\n            if self._eof:\\n                break\\n            if not_enough:\\n                self._waiter = self._create_waiter('readline')\\n                try:\\n                    yield from self._waiter\\n                finally:\\n                    self._waiter = None\\n        line = b''.join(parts)\\n        self._byte_count -= parts_size\\n        self._maybe_resume_transport()\\n        return line\\n    @tasks.coroutine\\n    def read(self, n=-1):\\n        if self._exception is not None:\\n            raise self._exception\\n        if not n:\\n            return b''\\n        if n < 0:\\n            while not self._eof:\\n                self._waiter = self._create_waiter('read')\\n                try:\\n                    yield from self._waiter\\n                finally:\\n                    self._waiter = None\\n        else:\\n            if not self._byte_count and not self._eof:\\n                self._waiter = self._create_waiter('read')\\n                try:\\n                    yield from self._waiter\\n                finally:\\n                    self._waiter = None\\n        if n < 0 or self._byte_count <= n:\\n            data = b''.join(self._buffer)\\n            self._buffer.clear()\\n            self._byte_count = 0\\n            self._maybe_resume_transport()\\n            return data\\n        parts = []\\n        parts_bytes = 0\\n        while self._buffer and parts_bytes < n:\\n            data = self._buffer.popleft()\\n            data_bytes = len(data)\\n            if n < parts_bytes + data_bytes:\\n                data_bytes = n - parts_bytes\\n                data, rest = data[:data_bytes], data[data_bytes:]\\n                self._buffer.appendleft(rest)\\n            parts.append(data)\\n            parts_bytes += data_bytes\\n            self._byte_count -= data_bytes\\n            self._maybe_resume_transport()\\n        return b''.join(parts)\\n    @tasks.coroutine\\n    def readexactly(self, n):\\n        if self._exception is not None:\\n            raise self._exception\\n        blocks = []\\n        while n > 0:\\n            block = yield from self.read(n)\\n            if not block:\\n                partial = b''.join(blocks)\\n                raise IncompleteReadError(partial, len(partial) + n)\\n            blocks.append(block)\\n            n -= len(block)\\n        return b''.join(blocks)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio.streams: Use bytebuffer in StreamReader; Add assertion in feed_data",
    "fixed_code": "def readline(self):\\n        if self._exception is not None:\\n            raise self._exception\\n        line = bytearray()\\n        not_enough = True\\n        while not_enough:\\n            while self._buffer and not_enough:\\n                ichar = self._buffer.find(b'\\n')\\n                if ichar < 0:\\n                    line.extend(self._buffer)\\n                    self._buffer.clear()\\n                else:\\n                    ichar += 1\\n                    line.extend(self._buffer[:ichar])\\n                    del self._buffer[:ichar]\\n                    not_enough = False\\n                if len(line) > self._limit:\\n                    self._maybe_resume_transport()\\n                    raise ValueError('Line is too long')\\n            if self._eof:\\n                break\\n            if not_enough:\\n                self._waiter = self._create_waiter('readline')\\n                try:\\n                    yield from self._waiter\\n                finally:\\n                    self._waiter = None\\n        self._maybe_resume_transport()\\n        return bytes(line)\\n    @tasks.coroutine\\n    def read(self, n=-1):\\n        if self._exception is not None:\\n            raise self._exception\\n        if not n:\\n            return b''\\n        if n < 0:\\n            while not self._eof:\\n                self._waiter = self._create_waiter('read')\\n                try:\\n                    yield from self._waiter\\n                finally:\\n                    self._waiter = None\\n        else:\\n            if not self._buffer and not self._eof:\\n                self._waiter = self._create_waiter('read')\\n                try:\\n                    yield from self._waiter\\n                finally:\\n                    self._waiter = None\\n        if n < 0 or len(self._buffer) <= n:\\n            data = bytes(self._buffer)\\n            self._buffer.clear()\\n        else:\\n            data = bytes(self._buffer[:n])\\n            del self._buffer[:n]\\n        self._maybe_resume_transport()\\n        return data\\n    @tasks.coroutine\\n    def readexactly(self, n):\\n        if self._exception is not None:\\n            raise self._exception\\n        blocks = []\\n        while n > 0:\\n            block = yield from self.read(n)\\n            if not block:\\n                partial = b''.join(blocks)\\n                raise IncompleteReadError(partial, len(partial) + n)\\n            blocks.append(block)\\n            n -= len(block)\\n        return b''.join(blocks)"
  },
  {
    "code": "def add(self, a, b):\\n        return a.__add__(b, context=self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #7633: Context method in the decimal module (with the exception of the 'canonical' and 'is_canonical' methods) now consistently accept integer arguments wherever a Decimal instance is accepted.  Thanks Juan Jos\u00e9 Conti for the patch.",
    "fixed_code": "def add(self, a, b):\\n        a = _convert_other(a, raiseit=True)\\n        r = a.__add__(b, context=self)\\n        if r is NotImplemented:\\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\\n        else:\\n            return r"
  },
  {
    "code": "def _validate_scalar(self, value, msg: Optional[str] = None):\\n        if isinstance(value, str):\\n            try:\\n                value = self._scalar_from_string(value)\\n            except ValueError as err:\\n                raise TypeError(msg) from err\\n        elif is_valid_nat_for_dtype(value, self.dtype):\\n            value = NaT\\n        elif isinstance(value, self._recognized_scalars):\\n            value = self._scalar_type(value)  \\n        else:\\n            if msg is None:\\n                msg = str(value)\\n            raise TypeError(msg)\\n        return value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: de-duplicate DTA/TDA validators by standardizing exception messages (#37293)",
    "fixed_code": "def _validate_scalar(self, value, allow_listlike: bool = False):\\n        if isinstance(value, str):\\n            try:\\n                value = self._scalar_from_string(value)\\n            except ValueError as err:\\n                msg = self._validation_error_message(value, allow_listlike)\\n                raise TypeError(msg) from err\\n        elif is_valid_nat_for_dtype(value, self.dtype):\\n            value = NaT\\n        elif isinstance(value, self._recognized_scalars):\\n            value = self._scalar_type(value)  \\n        else:\\n            msg = self._validation_error_message(value, allow_listlike)\\n            raise TypeError(msg)\\n        return value"
  },
  {
    "code": "def cycle(parser, token):\\n    args = token.split_contents()\\n    if len(args) < 2:\\n        raise TemplateSyntaxError(\"'cycle' tag requires at least two arguments\")\\n    if ',' in args[1]:\\n        warnings.warn(\\n            \"The old {% cycle %} syntax with comma-separated arguments is deprecated.\",\\n            RemovedInDjango20Warning,\\n        )\\n        args[1:2] = ['\"%s\"' % arg for arg in args[1].split(\",\")]\\n    if len(args) == 2:\\n        name = args[1]\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            raise TemplateSyntaxError(\"No named cycles in template. '%s' is not defined\" % name)\\n        if name not in parser._namedCycleNodes:\\n            raise TemplateSyntaxError(\"Named cycle '%s' does not exist\" % name)\\n        return parser._namedCycleNodes[name]\\n    as_form = False\\n    if len(args) > 4:\\n        if args[-3] == \"as\":\\n            if args[-1] != \"silent\":\\n                raise TemplateSyntaxError(\"Only 'silent' flag is allowed after cycle's name, not '%s'.\" % args[-1])\\n            as_form = True\\n            silent = True\\n            args = args[:-1]\\n        elif args[-2] == \"as\":\\n            as_form = True\\n            silent = False\\n    if as_form:\\n        name = args[-1]\\n        values = [parser.compile_filter(arg) for arg in args[1:-2]]\\n        node = CycleNode(values, name, silent=silent)\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            parser._namedCycleNodes = {}\\n        parser._namedCycleNodes[name] = node\\n    else:\\n        values = [parser.compile_filter(arg) for arg in args[1:]]\\n        node = CycleNode(values)\\n    return node",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cycle(parser, token):\\n    args = token.split_contents()\\n    if len(args) < 2:\\n        raise TemplateSyntaxError(\"'cycle' tag requires at least two arguments\")\\n    if ',' in args[1]:\\n        warnings.warn(\\n            \"The old {% cycle %} syntax with comma-separated arguments is deprecated.\",\\n            RemovedInDjango20Warning,\\n        )\\n        args[1:2] = ['\"%s\"' % arg for arg in args[1].split(\",\")]\\n    if len(args) == 2:\\n        name = args[1]\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            raise TemplateSyntaxError(\"No named cycles in template. '%s' is not defined\" % name)\\n        if name not in parser._namedCycleNodes:\\n            raise TemplateSyntaxError(\"Named cycle '%s' does not exist\" % name)\\n        return parser._namedCycleNodes[name]\\n    as_form = False\\n    if len(args) > 4:\\n        if args[-3] == \"as\":\\n            if args[-1] != \"silent\":\\n                raise TemplateSyntaxError(\"Only 'silent' flag is allowed after cycle's name, not '%s'.\" % args[-1])\\n            as_form = True\\n            silent = True\\n            args = args[:-1]\\n        elif args[-2] == \"as\":\\n            as_form = True\\n            silent = False\\n    if as_form:\\n        name = args[-1]\\n        values = [parser.compile_filter(arg) for arg in args[1:-2]]\\n        node = CycleNode(values, name, silent=silent)\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            parser._namedCycleNodes = {}\\n        parser._namedCycleNodes[name] = node\\n    else:\\n        values = [parser.compile_filter(arg) for arg in args[1:]]\\n        node = CycleNode(values)\\n    return node"
  },
  {
    "code": "def getint(self, section, option, vars=None, default=_UNSET):\\n        try:\\n            return self._get(section, int, option, vars)\\n        except (NoSectionError, NoOptionError):\\n            if default is _UNSET:\\n                raise\\n            else:\\n                return default",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getint(self, section, option, vars=None, default=_UNSET):\\n        try:\\n            return self._get(section, int, option, vars)\\n        except (NoSectionError, NoOptionError):\\n            if default is _UNSET:\\n                raise\\n            else:\\n                return default"
  },
  {
    "code": "def from_decimal(cls, dec):\\n        from decimal import Decimal\\n        if not isinstance(dec, Decimal):\\n            raise TypeError(\\n                \"%s.from_decimal() only takes Decimals, not %r (%s)\" %\\n                (cls.__name__, dec, type(dec).__name__))\\n        if not dec.is_finite():\\n            raise TypeError(\"Cannot convert %s to %s.\" % (dec, cls.__name__))\\n        sign, digits, exp = dec.as_tuple()\\n        digits = int(''.join(map(str, digits)))\\n        if sign:\\n            digits = -digits\\n        if exp >= 0:\\n            return cls(digits * 10 ** exp)\\n        else:\\n            return cls(digits, 10 ** -exp)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue 3285: Fractions from_float() and from_decimal() accept Integral arguments.",
    "fixed_code": "def from_decimal(cls, dec):\\n        from decimal import Decimal\\n        if isinstance(dec, numbers.Integral):\\n            dec = Decimal(int(dec))\\n        elif not isinstance(dec, Decimal):\\n            raise TypeError(\\n                \"%s.from_decimal() only takes Decimals, not %r (%s)\" %\\n                (cls.__name__, dec, type(dec).__name__))\\n        if not dec.is_finite():\\n            raise TypeError(\"Cannot convert %s to %s.\" % (dec, cls.__name__))\\n        sign, digits, exp = dec.as_tuple()\\n        digits = int(''.join(map(str, digits)))\\n        if sign:\\n            digits = -digits\\n        if exp >= 0:\\n            return cls(digits * 10 ** exp)\\n        else:\\n            return cls(digits, 10 ** -exp)"
  },
  {
    "code": "def mkinsertionloc(dict):\\n    return aetypes.InsertionLoc(dict[keyAEObject], dict[keyAEPosition])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "SF patch# 1761465 by Jeffrey Yasskin. Fix test_aepack and test_applesingle.",
    "fixed_code": "def mkinsertionloc(dict):\\n    return aetypes.InsertionLoc(dict[b2i(keyAEObject)],\\n                                dict[b2i(keyAEPosition)])"
  },
  {
    "code": "def andrews_curves(data, class_column, ax=None, samples=200):\\n    from math import sqrt, pi, sin, cos\\n    import matplotlib.pyplot as plt\\n    import random",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def andrews_curves(data, class_column, ax=None, samples=200):\\n    from math import sqrt, pi, sin, cos\\n    import matplotlib.pyplot as plt\\n    import random"
  },
  {
    "code": "def to_pytimedelta(self) -> npt.NDArray[np.object_]:\\n        return ints_to_pytimedelta(self._ndarray)\\n    days = _field_accessor(\"days\", \"days\", \"Number of days for each element.\")\\n    seconds = _field_accessor(\\n        \"seconds\",\\n        \"seconds\",\\n        \"Number of seconds (>= 0 and less than 1 day) for each element.\",\\n    )\\n    microseconds = _field_accessor(\\n        \"microseconds\",\\n        \"microseconds\",\\n        \"Number of microseconds (>= 0 and less than 1 second) for each element.\",\\n    )\\n    nanoseconds = _field_accessor(\\n        \"nanoseconds\",\\n        \"nanoseconds\",\\n        \"Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.\",\\n    )\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_pytimedelta(self) -> npt.NDArray[np.object_]:\\n        return ints_to_pytimedelta(self._ndarray)\\n    days = _field_accessor(\"days\", \"days\", \"Number of days for each element.\")\\n    seconds = _field_accessor(\\n        \"seconds\",\\n        \"seconds\",\\n        \"Number of seconds (>= 0 and less than 1 day) for each element.\",\\n    )\\n    microseconds = _field_accessor(\\n        \"microseconds\",\\n        \"microseconds\",\\n        \"Number of microseconds (>= 0 and less than 1 second) for each element.\",\\n    )\\n    nanoseconds = _field_accessor(\\n        \"nanoseconds\",\\n        \"nanoseconds\",\\n        \"Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.\",\\n    )\\n    @property"
  },
  {
    "code": "def __init__(self, base_field, size=None, **kwargs):\\n        self.base_field = base_field\\n        self.db_collation = getattr(self.base_field, \"db_collation\", None)\\n        self.size = size\\n        if self.size:\\n            self.default_validators = [\\n                *self.default_validators,\\n                ArrayMaxLengthValidator(self.size),\\n            ]\\n        if hasattr(self.base_field, \"from_db_value\"):\\n            self.from_db_value = self._from_db_value\\n        super().__init__(**kwargs)\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, base_field, size=None, **kwargs):\\n        self.base_field = base_field\\n        self.db_collation = getattr(self.base_field, \"db_collation\", None)\\n        self.size = size\\n        if self.size:\\n            self.default_validators = [\\n                *self.default_validators,\\n                ArrayMaxLengthValidator(self.size),\\n            ]\\n        if hasattr(self.base_field, \"from_db_value\"):\\n            self.from_db_value = self._from_db_value\\n        super().__init__(**kwargs)\\n    @property"
  },
  {
    "code": "def get_language_info(lang_code):\\n    from django.conf.locale import LANG_INFO\\n    try:\\n        return LANG_INFO[lang_code]\\n    except KeyError:\\n        raise KeyError(\"Unknown language code %r.\" % lang_code)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19811 - Added language code fallback in get_language_info.",
    "fixed_code": "def get_language_info(lang_code):\\n    from django.conf.locale import LANG_INFO\\n    try:\\n        return LANG_INFO[lang_code]\\n    except KeyError:\\n        if '-' in lang_code:\\n            splited_lang_code = lang_code.split('-')[0]\\n            try:\\n                return LANG_INFO[splited_lang_code]\\n            except KeyError:\\n                raise KeyError(\"Unknown language code %r and %r.\" % (lang_code, splited_lang_code))\\n        raise KeyError(\"Unknown language code %r.\" % lang_code)"
  },
  {
    "code": "def populate_interface_descriptions(self, data):\\n        for elem in data:\\n            if 'show_ports_description' not in elem:\\n                continue\\n            key = str(elem['show_ports_description']['port'])\\n            if 'descriptionString' in elem['show_ports_description']:\\n                desc = elem['show_ports_description']['descriptionString']\\n                self.facts['interfaces'][key]['description'] = desc",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def populate_interface_descriptions(self, data):\\n        for elem in data:\\n            if 'show_ports_description' not in elem:\\n                continue\\n            key = str(elem['show_ports_description']['port'])\\n            if 'descriptionString' in elem['show_ports_description']:\\n                desc = elem['show_ports_description']['descriptionString']\\n                self.facts['interfaces'][key]['description'] = desc"
  },
  {
    "code": "def delete(self):\\n\\t\\tself._run_command(\"delete\", None)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "To prevent remote code injection on Sheepdog store",
    "fixed_code": "def delete(self):\\n\\t\\tself._run_command([\"delete\"], None)"
  },
  {
    "code": "def melt(frame, id_vars=None, value_vars=None,\\n         var_name=None, value_name='value'):\\n    if id_vars is not None:\\n        if not isinstance(id_vars, (tuple, list, np.ndarray)):\\n            id_vars = [id_vars]\\n        else:\\n            id_vars = list(id_vars)\\n    else:\\n        id_vars = []\\n    if value_vars is not None:\\n        if not isinstance(value_vars, (tuple, list, np.ndarray)):\\n            value_vars = [value_vars]\\n        frame = frame.ix[:, id_vars + value_vars]\\n    else:\\n        frame = frame.copy()\\n    if var_name is None:\\n        var_name = frame.columns.name if frame.columns.name is not None else 'variable'\\n    N, K = frame.shape\\n    K -= len(id_vars)\\n    mdata = {}\\n    for col in id_vars:\\n        mdata[col] = np.tile(frame.pop(col).values, K)\\n    mcolumns = id_vars + [var_name, value_name]\\n    mdata[value_name] = frame.values.ravel('F')\\n    mdata[var_name] = np.asarray(frame.columns).repeat(N)\\n    return DataFrame(mdata, columns=mcolumns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def melt(frame, id_vars=None, value_vars=None,\\n         var_name=None, value_name='value'):\\n    if id_vars is not None:\\n        if not isinstance(id_vars, (tuple, list, np.ndarray)):\\n            id_vars = [id_vars]\\n        else:\\n            id_vars = list(id_vars)\\n    else:\\n        id_vars = []\\n    if value_vars is not None:\\n        if not isinstance(value_vars, (tuple, list, np.ndarray)):\\n            value_vars = [value_vars]\\n        frame = frame.ix[:, id_vars + value_vars]\\n    else:\\n        frame = frame.copy()\\n    if var_name is None:\\n        var_name = frame.columns.name if frame.columns.name is not None else 'variable'\\n    N, K = frame.shape\\n    K -= len(id_vars)\\n    mdata = {}\\n    for col in id_vars:\\n        mdata[col] = np.tile(frame.pop(col).values, K)\\n    mcolumns = id_vars + [var_name, value_name]\\n    mdata[value_name] = frame.values.ravel('F')\\n    mdata[var_name] = np.asarray(frame.columns).repeat(N)\\n    return DataFrame(mdata, columns=mcolumns)"
  },
  {
    "code": "def _nanmean(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    if skipna and not issubclass(values.dtype.type, np.integer):\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    the_sum = values.sum(axis)\\n    count = _get_counts(mask, axis)\\n    if axis is not None:\\n        the_mean = the_sum / count\\n        ct_mask = count == 0\\n        if ct_mask.any():\\n            the_mean[ct_mask] = np.nan\\n    else:\\n        the_mean = the_sum / count if count > 0 else np.nan\\n    return the_mean",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: catch zero division errors in nanops from object dtype arrays in all NA case, GH #676",
    "fixed_code": "def _nanmean(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    if skipna and not issubclass(values.dtype.type, np.integer):\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    the_sum = _ensure_numeric(values.sum(axis))\\n    count = _get_counts(mask, axis)\\n    if axis is not None:\\n        the_mean = the_sum / count\\n        ct_mask = count == 0\\n        if ct_mask.any():\\n            the_mean[ct_mask] = np.nan\\n    else:\\n        the_mean = the_sum / count if count > 0 else np.nan\\n    return the_mean"
  },
  {
    "code": "def determine_parent(self, caller):\\n        self.msgin(4, \"determine_parent\", caller)\\n        if not caller:\\n            self.msgout(4, \"determine_parent -> None\")\\n            return None\\n        pname = caller.__name__\\n        if caller.__path__:\\n            parent = self.modules[pname]\\n            assert caller is parent\\n            self.msgout(4, \"determine_parent ->\", parent)\\n            return parent\\n        if '.' in pname:\\n            i = pname.rfind('.')\\n            pname = pname[:i]\\n            parent = self.modules[pname]\\n            assert parent.__name__ == pname\\n            self.msgout(4, \"determine_parent ->\", parent)\\n            return parent\\n        self.msgout(4, \"determine_parent -> None\")\\n        return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Modulefinder now handles absolute and relative imports, including tests.\\n\\nWill backport to release25-maint.",
    "fixed_code": "def determine_parent(self, caller, level=-1):\\n        self.msgin(4, \"determine_parent\", caller, level)\\n        if not caller or level == 0:\\n            self.msgout(4, \"determine_parent -> None\")\\n            return None\\n        pname = caller.__name__\\n        if level >= 1: \\n            if caller.__path__:\\n                level -= 1\\n            if level == 0:\\n                parent = self.modules[pname]\\n                assert parent is caller\\n                self.msgout(4, \"determine_parent ->\", parent)\\n                return parent\\n            if pname.count(\".\") < level:\\n                raise ImportError, \"relative importpath too deep\"\\n            pname = \".\".join(pname.split(\".\")[:-level])\\n            parent = self.modules[pname]\\n            self.msgout(4, \"determine_parent ->\", parent)\\n            return parent\\n        if caller.__path__:\\n            parent = self.modules[pname]\\n            assert caller is parent\\n            self.msgout(4, \"determine_parent ->\", parent)\\n            return parent\\n        if '.' in pname:\\n            i = pname.rfind('.')\\n            pname = pname[:i]\\n            parent = self.modules[pname]\\n            assert parent.__name__ == pname\\n            self.msgout(4, \"determine_parent ->\", parent)\\n            return parent\\n        self.msgout(4, \"determine_parent -> None\")\\n        return None"
  },
  {
    "code": "def _ensure_arraylike(values):\\n    if not isinstance(values, (np.ndarray, ABCCategorical,\\n                               ABCIndexClass, ABCSeries)):\\n        inferred = lib.infer_dtype(values)\\n        if inferred in ['mixed', 'string', 'unicode']:\\n            values = lib.list_to_object_array(values)\\n        else:\\n            values = np.asarray(values)\\n    return values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Allow pd.unique to accept tuple of strings (#17108)",
    "fixed_code": "def _ensure_arraylike(values):\\n    if not isinstance(values, (np.ndarray, ABCCategorical,\\n                               ABCIndexClass, ABCSeries)):\\n        inferred = lib.infer_dtype(values)\\n        if inferred in ['mixed', 'string', 'unicode']:\\n            if isinstance(values, tuple):\\n                values = list(values)\\n            values = lib.list_to_object_array(values)\\n        else:\\n            values = np.asarray(values)\\n    return values"
  },
  {
    "code": "def _set_item_frame_value(self, key, value: DataFrame) -> None:\\n        self._ensure_valid_index(value)\\n        if key in self.columns:\\n            loc = self.columns.get_loc(key)\\n            cols = self.columns[loc]\\n            len_cols = 1 if is_scalar(cols) else len(cols)\\n            if len_cols != len(value.columns):\\n                raise ValueError(\"Columns must be same length as key\")\\n            if isinstance(self.columns, MultiIndex) and isinstance(\\n                loc, (slice, Series, np.ndarray, Index)\\n            ):\\n                cols = maybe_droplevels(cols, key)\\n                if len(cols) and not cols.equals(value.columns):\\n                    value = value.reindex(cols, axis=1)\\n        value = _reindex_for_setitem(value, self.index)\\n        value = value.T\\n        self._set_item_mgr(key, value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: move reshaping of array for setitem from DataFrame into BlockManager internals (#39722)",
    "fixed_code": "def _set_item_frame_value(self, key, value: DataFrame) -> None:\\n        self._ensure_valid_index(value)\\n        if key in self.columns:\\n            loc = self.columns.get_loc(key)\\n            cols = self.columns[loc]\\n            len_cols = 1 if is_scalar(cols) else len(cols)\\n            if len_cols != len(value.columns):\\n                raise ValueError(\"Columns must be same length as key\")\\n            if isinstance(self.columns, MultiIndex) and isinstance(\\n                loc, (slice, Series, np.ndarray, Index)\\n            ):\\n                cols = maybe_droplevels(cols, key)\\n                if len(cols) and not cols.equals(value.columns):\\n                    value = value.reindex(cols, axis=1)\\n        value = _reindex_for_setitem(value, self.index)\\n        self._set_item_mgr(key, value)"
  },
  {
    "code": "def read_conllx(input_data, use_morphology=False, n=0):\\n\\ti = 0\\n\\tfor sent in input_data.strip().split(\"\\n\"):\\n\\t\\tlines = sent.strip().split(\"\\n\")\\n\\t\\tif lines:\\n\\t\\t\\twhile lines[0].startswith(\"\\n\\t\\t\\t\\tlines.pop(0)\\n\\t\\t\\ttokens = []\\n\\t\\t\\tfor line in lines:\\n\\t\\t\\t\\tparts = line.split(\"\\t\")\\n\\t\\t\\t\\tid_, word, lemma, pos, tag, morph, head, dep, _1, iob = parts\\n\\t\\t\\t\\tif \"-\" in id_ or \".\" in id_:\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tid_ = int(id_) - 1\\n\\t\\t\\t\\t\\thead = (int(head) - 1) if head not in [\"0\", \"_\"] else id_\\n\\t\\t\\t\\t\\tdep = \"ROOT\" if dep == \"root\" else dep\\n\\t\\t\\t\\t\\ttag = pos if tag == \"_\" else tag\\n\\t\\t\\t\\t\\ttag = tag + \"__\" + morph if use_morphology else tag\\n\\t\\t\\t\\t\\tiob = iob if iob else \"O\"\\n\\t\\t\\t\\t\\ttokens.append((id_, word, tag, head, dep, iob))\\n\\t\\t\\t\\texcept:  \\n\\t\\t\\t\\t\\tprint(line)\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\ttuples = [list(t) for t in zip(*tokens)]\\n\\t\\t\\tyield (None, [[tuples, []]])\\n\\t\\t\\ti += 1\\n\\t\\t\\tif n >= 1 and i >= n:\\n\\t\\t\\t\\tbreak",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_conllx(input_data, use_morphology=False, n=0):\\n\\ti = 0\\n\\tfor sent in input_data.strip().split(\"\\n\"):\\n\\t\\tlines = sent.strip().split(\"\\n\")\\n\\t\\tif lines:\\n\\t\\t\\twhile lines[0].startswith(\"\\n\\t\\t\\t\\tlines.pop(0)\\n\\t\\t\\ttokens = []\\n\\t\\t\\tfor line in lines:\\n\\t\\t\\t\\tparts = line.split(\"\\t\")\\n\\t\\t\\t\\tid_, word, lemma, pos, tag, morph, head, dep, _1, iob = parts\\n\\t\\t\\t\\tif \"-\" in id_ or \".\" in id_:\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tid_ = int(id_) - 1\\n\\t\\t\\t\\t\\thead = (int(head) - 1) if head not in [\"0\", \"_\"] else id_\\n\\t\\t\\t\\t\\tdep = \"ROOT\" if dep == \"root\" else dep\\n\\t\\t\\t\\t\\ttag = pos if tag == \"_\" else tag\\n\\t\\t\\t\\t\\ttag = tag + \"__\" + morph if use_morphology else tag\\n\\t\\t\\t\\t\\tiob = iob if iob else \"O\"\\n\\t\\t\\t\\t\\ttokens.append((id_, word, tag, head, dep, iob))\\n\\t\\t\\t\\texcept:  \\n\\t\\t\\t\\t\\tprint(line)\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\ttuples = [list(t) for t in zip(*tokens)]\\n\\t\\t\\tyield (None, [[tuples, []]])\\n\\t\\t\\ti += 1\\n\\t\\t\\tif n >= 1 and i >= n:\\n\\t\\t\\t\\tbreak"
  },
  {
    "code": "def make_block(values, ref_locs, ref_columns, _columns=None):\\n    dtype = values.dtype\\n    vtype = dtype.type\\n    if issubclass(vtype, np.floating):\\n        klass = FloatBlock\\n    elif issubclass(vtype, np.integer):\\n        klass = IntBlock\\n    elif dtype == np.bool_:\\n        klass = BoolBlock\\n    else:\\n        klass = ObjectBlock\\n    block = klass(values, ref_locs, ref_columns)\\n    block._columns = _columns\\n    return block\\nclass BlockManager(object):\\n    \"\"\"\\n    Core internal data structure to implement DataFrame\\n    Manage a bunch of labeled 2D mixed-type ndarrays. Essentially it's a\\n    lightweight blocked set of labeled data to be manipulated by the DataFrame\\n    public API class\\n    Parameters\\n    ----------",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def make_block(values, columns, ref_columns):\\n    dtype = values.dtype\\n    vtype = dtype.type\\n    if issubclass(vtype, np.floating):\\n        klass = FloatBlock\\n    elif issubclass(vtype, np.integer):\\n        klass = IntBlock\\n    elif dtype == np.bool_:\\n        klass = BoolBlock\\n    else:\\n        klass = ObjectBlock\\n    return klass(values, columns, ref_columns)\\nclass BlockManager(object):\\n    \"\"\"\\n    Core internal data structure to implement DataFrame\\n    Manage a bunch of labeled 2D mixed-type ndarrays. Essentially it's a\\n    lightweight blocked set of labeled data to be manipulated by the DataFrame\\n    public API class\\n    Parameters\\n    ----------"
  },
  {
    "code": "def serializer_factory(value):\\n    if isinstance(value, Promise):\\n        value = str(value)\\n    elif isinstance(value, LazyObject):\\n        value = value.__reduce__()[1][0]\\n    if isinstance(value, models.Field):\\n        return ModelFieldSerializer(value)\\n    if isinstance(value, models.manager.BaseManager):\\n        return ModelManagerSerializer(value)\\n    if isinstance(value, Operation):\\n        return OperationSerializer(value)\\n    if isinstance(value, type):\\n        return TypeSerializer(value)\\n    if hasattr(value, 'deconstruct'):\\n        return DeconstructableSerializer(value)\\n    for type_, serializer_cls in Serializer._registry.items():\\n        if isinstance(value, type_):\\n            return serializer_cls(value)\\n    raise ValueError(\\n        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\\n        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\\n        \"topics/migrations/\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def serializer_factory(value):\\n    if isinstance(value, Promise):\\n        value = str(value)\\n    elif isinstance(value, LazyObject):\\n        value = value.__reduce__()[1][0]\\n    if isinstance(value, models.Field):\\n        return ModelFieldSerializer(value)\\n    if isinstance(value, models.manager.BaseManager):\\n        return ModelManagerSerializer(value)\\n    if isinstance(value, Operation):\\n        return OperationSerializer(value)\\n    if isinstance(value, type):\\n        return TypeSerializer(value)\\n    if hasattr(value, 'deconstruct'):\\n        return DeconstructableSerializer(value)\\n    for type_, serializer_cls in Serializer._registry.items():\\n        if isinstance(value, type_):\\n            return serializer_cls(value)\\n    raise ValueError(\\n        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\\n        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\\n        \"topics/migrations/\\n    )"
  },
  {
    "code": "def patch_response_headers(response, cache_timeout=None):\\n    if cache_timeout is None:\\n        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\\n    if cache_timeout < 0:\\n        cache_timeout = 0 \\n    if settings.USE_ETAGS and not response.has_header('ETag'):\\n        if hasattr(response, 'render') and callable(response.render):\\n            response.add_post_render_callback(_set_response_etag)\\n        else:\\n            response = _set_response_etag(response)\\n    if not response.has_header('Last-Modified'):\\n        response['Last-Modified'] = http_date()\\n    if not response.has_header('Expires'):\\n        response['Expires'] = http_date(time.time() + cache_timeout)\\n    patch_cache_control(response, max_age=cache_timeout)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def patch_response_headers(response, cache_timeout=None):\\n    if cache_timeout is None:\\n        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\\n    if cache_timeout < 0:\\n        cache_timeout = 0 \\n    if settings.USE_ETAGS and not response.has_header('ETag'):\\n        if hasattr(response, 'render') and callable(response.render):\\n            response.add_post_render_callback(_set_response_etag)\\n        else:\\n            response = _set_response_etag(response)\\n    if not response.has_header('Last-Modified'):\\n        response['Last-Modified'] = http_date()\\n    if not response.has_header('Expires'):\\n        response['Expires'] = http_date(time.time() + cache_timeout)\\n    patch_cache_control(response, max_age=cache_timeout)"
  },
  {
    "code": "def unsign(self, signed_value):\\n        if not self.sep in signed_value:\\n            raise BadSignature('No \"%s\" found in value' % self.sep)\\n        value, sig = signed_value.rsplit(self.sep, 1)\\n        if constant_time_compare(sig, self.signature(value)):\\n            return value\\n        raise BadSignature('Signature \"%s\" does not match' % sig)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def unsign(self, signed_value):\\n        if not self.sep in signed_value:\\n            raise BadSignature('No \"%s\" found in value' % self.sep)\\n        value, sig = signed_value.rsplit(self.sep, 1)\\n        if constant_time_compare(sig, self.signature(value)):\\n            return value\\n        raise BadSignature('Signature \"%s\" does not match' % sig)"
  },
  {
    "code": "def _concat_date_cols(date_cols):\\n    if len(date_cols) == 1:\\n        return date_cols[0]\\n    rs = np.array([' '.join([str(y) for y in x])\\n                   for x in zip(*date_cols)], dtype=object)\\n    return rs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _concat_date_cols(date_cols):\\n    if len(date_cols) == 1:\\n        return date_cols[0]\\n    rs = np.array([' '.join([str(y) for y in x])\\n                   for x in zip(*date_cols)], dtype=object)\\n    return rs"
  },
  {
    "code": "def is_safe_url(url, host=None):\\n\\tif not url:\\n\\t\\treturn False\\n\\turl = url.strip()\\n\\turl = url.replace('\\\\', '/')\\n\\tif url.startswith('///'):\\n\\t\\treturn False\\n\\turl_info = urlparse(url)\\n\\tif not url_info.netloc and url_info.scheme:\\n\\t\\treturn False\\n\\treturn ((not url_info.netloc or url_info.netloc == host) and\\n\\t\\t\\t(not url_info.scheme or url_info.scheme in ['http', 'https']))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Made is_safe_url() reject URLs that start with control characters.\\n\\nThis is a security fix; disclosure to follow shortly.",
    "fixed_code": "def is_safe_url(url, host=None):\\n\\tif url is not None:\\n\\t\\turl = url.strip()\\n\\tif not url:\\n\\t\\treturn False\\n\\turl = url.replace('\\\\', '/')\\n\\tif url.startswith('///'):\\n\\t\\treturn False\\n\\turl_info = urlparse(url)\\n\\tif not url_info.netloc and url_info.scheme:\\n\\t\\treturn False\\n\\tif unicodedata.category(url[0])[0] == 'C':\\n\\t\\treturn False\\n\\treturn ((not url_info.netloc or url_info.netloc == host) and\\n\\t\\t\\t(not url_info.scheme or url_info.scheme in ['http', 'https']))"
  },
  {
    "code": "def check_setitem_lengths(indexer, value, values) -> bool:\\n    no_op = False\\n    if isinstance(indexer, (np.ndarray, list)):\\n        if is_list_like(value):\\n            if len(indexer) != len(value) and values.ndim == 1:\\n                if not (\\n                    isinstance(indexer, np.ndarray)\\n                    and indexer.dtype == np.bool_\\n                    and len(indexer[indexer]) == len(value)\\n                ):\\n                    raise ValueError(\\n                        \"cannot set using a list-like indexer \"\\n                        \"with a different length than the value\"\\n                    )\\n            if not len(indexer):\\n                no_op = True\\n    elif isinstance(indexer, slice):\\n        if is_list_like(value):\\n            if len(value) != length_of_indexer(indexer, values) and values.ndim == 1:\\n                raise ValueError(\\n                    \"cannot set using a slice indexer with a \"\\n                    \"different length than the value\"\\n                )\\n            if not len(value):\\n                no_op = True\\n    return no_op",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Bug in loc raised ValueError when setting value via boolean list (#37761)",
    "fixed_code": "def check_setitem_lengths(indexer, value, values) -> bool:\\n    no_op = False\\n    if isinstance(indexer, (np.ndarray, list)):\\n        if is_list_like(value):\\n            if len(indexer) != len(value) and values.ndim == 1:\\n                if isinstance(indexer, list):\\n                    indexer = np.array(indexer)\\n                if not (\\n                    isinstance(indexer, np.ndarray)\\n                    and indexer.dtype == np.bool_\\n                    and len(indexer[indexer]) == len(value)\\n                ):\\n                    raise ValueError(\\n                        \"cannot set using a list-like indexer \"\\n                        \"with a different length than the value\"\\n                    )\\n            if not len(indexer):\\n                no_op = True\\n    elif isinstance(indexer, slice):\\n        if is_list_like(value):\\n            if len(value) != length_of_indexer(indexer, values) and values.ndim == 1:\\n                raise ValueError(\\n                    \"cannot set using a slice indexer with a \"\\n                    \"different length than the value\"\\n                )\\n            if not len(value):\\n                no_op = True\\n    return no_op"
  },
  {
    "code": "def __deepcopy__(self, memo={}):\\n        return self.copy(deep=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __deepcopy__(self, memo={}):\\n        return self.copy(deep=True)"
  },
  {
    "code": "def setdefault(self, key, default=None):\\n        self._assert_mutable()\\n        key = bytes_to_text(key, self.encoding)\\n        default = bytes_to_text(default, self.encoding)\\n        return super(QueryDict, self).setdefault(key, default)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setdefault(self, key, default=None):\\n        self._assert_mutable()\\n        key = bytes_to_text(key, self.encoding)\\n        default = bytes_to_text(default, self.encoding)\\n        return super(QueryDict, self).setdefault(key, default)"
  },
  {
    "code": "def __init__(self, path):\\n        if path == '':\\n            raise ImportError('empty pathname', path='')\\n        elif os.path.isdir(path):\\n            raise ImportError('existing directory', path=path)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, path):\\n        if path == '':\\n            raise ImportError('empty pathname', path='')\\n        elif os.path.isdir(path):\\n            raise ImportError('existing directory', path=path)"
  },
  {
    "code": "def _convert_to_ascii(self, *values):\\n        for value in values:\\n            if not isinstance(value, six.string_types):\\n                value = str(value)\\n            try:\\n                if six.PY3:\\n                    value.encode('us-ascii')\\n                else:\\n                    if isinstance(value, str):\\n                        value.decode('us-ascii')\\n                    else:\\n                        value = value.encode('us-ascii')\\n            except UnicodeError as e:\\n                e.reason += ', HTTP response headers must be in US-ASCII format'\\n                raise\\n            if '\\n' in value or '\\r' in value:\\n                raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\\n            yield value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_to_ascii(self, *values):\\n        for value in values:\\n            if not isinstance(value, six.string_types):\\n                value = str(value)\\n            try:\\n                if six.PY3:\\n                    value.encode('us-ascii')\\n                else:\\n                    if isinstance(value, str):\\n                        value.decode('us-ascii')\\n                    else:\\n                        value = value.encode('us-ascii')\\n            except UnicodeError as e:\\n                e.reason += ', HTTP response headers must be in US-ASCII format'\\n                raise\\n            if '\\n' in value or '\\r' in value:\\n                raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\\n            yield value"
  },
  {
    "code": "def evaluate(\\n\\t\\tself, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None\\n\\t):\\n\\t\\tif scorer is None:\\n\\t\\t\\tscorer = Scorer(pipeline=self.pipeline)\\n\\t\\tif component_cfg is None:\\n\\t\\t\\tcomponent_cfg = {}\\n\\t\\tdocs, golds = zip(*docs_golds)\\n\\t\\tdocs = [\\n\\t\\t\\tself.make_doc(doc) if isinstance(doc, basestring_) else doc for doc in docs\\n\\t\\t]\\n\\t\\tgolds = list(golds)\\n\\t\\tfor name, pipe in self.pipeline:\\n\\t\\t\\tkwargs = component_cfg.get(name, {})\\n\\t\\t\\tkwargs.setdefault(\"batch_size\", batch_size)\\n\\t\\t\\tif not hasattr(pipe, \"pipe\"):\\n\\t\\t\\t\\tdocs = _pipe(docs, pipe, kwargs)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdocs = pipe.pipe(docs, **kwargs)\\n\\t\\tfor doc, gold in zip(docs, golds):\\n\\t\\t\\tif not isinstance(gold, GoldParse):\\n\\t\\t\\t\\tgold = GoldParse(doc, **gold)\\n\\t\\t\\tif verbose:\\n\\t\\t\\t\\tprint(doc)\\n\\t\\t\\tkwargs = component_cfg.get(\"scorer\", {})\\n\\t\\t\\tkwargs.setdefault(\"verbose\", verbose)\\n\\t\\t\\tscorer.score(doc, gold, **kwargs)\\n\\t\\treturn scorer\\n\\t@contextmanager",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def evaluate(\\n\\t\\tself, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None\\n\\t):\\n\\t\\tif scorer is None:\\n\\t\\t\\tscorer = Scorer(pipeline=self.pipeline)\\n\\t\\tif component_cfg is None:\\n\\t\\t\\tcomponent_cfg = {}\\n\\t\\tdocs, golds = zip(*docs_golds)\\n\\t\\tdocs = [\\n\\t\\t\\tself.make_doc(doc) if isinstance(doc, basestring_) else doc for doc in docs\\n\\t\\t]\\n\\t\\tgolds = list(golds)\\n\\t\\tfor name, pipe in self.pipeline:\\n\\t\\t\\tkwargs = component_cfg.get(name, {})\\n\\t\\t\\tkwargs.setdefault(\"batch_size\", batch_size)\\n\\t\\t\\tif not hasattr(pipe, \"pipe\"):\\n\\t\\t\\t\\tdocs = _pipe(docs, pipe, kwargs)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdocs = pipe.pipe(docs, **kwargs)\\n\\t\\tfor doc, gold in zip(docs, golds):\\n\\t\\t\\tif not isinstance(gold, GoldParse):\\n\\t\\t\\t\\tgold = GoldParse(doc, **gold)\\n\\t\\t\\tif verbose:\\n\\t\\t\\t\\tprint(doc)\\n\\t\\t\\tkwargs = component_cfg.get(\"scorer\", {})\\n\\t\\t\\tkwargs.setdefault(\"verbose\", verbose)\\n\\t\\t\\tscorer.score(doc, gold, **kwargs)\\n\\t\\treturn scorer\\n\\t@contextmanager"
  },
  {
    "code": "def to_interval(self, freq=None):\\n        if freq == None:\\n            freq = self.freq\\n        return Interval(self, freq=freq)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_interval(self, freq=None):\\n        if freq == None:\\n            freq = self.freq\\n        return Interval(self, freq=freq)"
  },
  {
    "code": "def drop_duplicates(self, subset=None, keep='first', inplace=False):\\n        duplicated = self.duplicated(subset, keep=keep)\\n        if inplace:\\n            inds, = (-duplicated).nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]\\n    @deprecate_kwarg('take_last', 'keep', mapping={True: 'last',\\n                                                   False: 'first'})\\n    @deprecate_kwarg(old_arg_name='cols', new_arg_name='subset', stacklevel=3)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: some removals\\n\\nDEPR: Removal of cols keyword in favor of subset in\\nDataFrame.duplicated() and DataFrame.drop_duplicates(), xref #6680\\n\\nAuthor: Jeff Reback <jeff@reback.net>\\n\\nCloses #12165 from jreback/deprecate and squashes the following commits:\\n\\n5be6dc6 [Jeff Reback] DOC: small fix on Timestamp doc-string\\ne3579a5 [Jeff Reback] DEPR: Removal of cols keyword in favor of subset in DataFrame.duplicated() and DataFrame.drop_duplicates(), xref #6680",
    "fixed_code": "def drop_duplicates(self, subset=None, keep='first', inplace=False):\\n        duplicated = self.duplicated(subset, keep=keep)\\n        if inplace:\\n            inds, = (-duplicated).nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]\\n    @deprecate_kwarg('take_last', 'keep', mapping={True: 'last',\\n                                                   False: 'first'})"
  },
  {
    "code": "def rpm_release(self):\\n        v = self._parsed_version\\n        match = self._parsed_regex_match\\n        if v.is_prerelease or match.group('dev') or match.group('post'):\\n            if match.group('dev') and match.group('post'):\\n                raise Exception(\"dev and post may not currently be used together\")\\n            if match.group('pre'):\\n                tag_value = match.group('pre')\\n                tag_type = match.group('pre_l')\\n                tag_ver = match.group('pre_n')\\n                if match.group('dev'):\\n                    tag_value += match.group('dev')\\n                if match.group('post'):\\n                    tag_value += match.group('post')\\n            elif match.group('dev'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('dev')\\n                tag_ver = match.group('dev_n')\\n            elif match.group('post'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('post')\\n                tag_ver = match.group('post_n')\\n            else:\\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\\n        else:\\n            tag_type = None\\n            tag_value = ''\\n            tag_ver = 0\\n        if not tag_type:\\n            if self._revision is None:\\n                self._revision = 1\\n            return '{revision}'.format(revision=self._revision)\\n        tag_value = tag_value.strip('.')\\n        tag_ver = int(tag_ver if tag_ver else 0)\\n        if self._revision is None:\\n            tag_offset = self.tag_offsets.get(tag_type)\\n            if tag_offset is None:\\n                raise Exception('no tag offset defined for tag {0}'.format(tag_type))\\n            pkgrel = '0.{0}'.format(tag_offset + tag_ver)\\n        else:\\n            pkgrel = self._revision\\n        return '{pkgrel}.{tag_value}'.format(pkgrel=pkgrel, tag_value=tag_value)\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rpm_release(self):\\n        v = self._parsed_version\\n        match = self._parsed_regex_match\\n        if v.is_prerelease or match.group('dev') or match.group('post'):\\n            if match.group('dev') and match.group('post'):\\n                raise Exception(\"dev and post may not currently be used together\")\\n            if match.group('pre'):\\n                tag_value = match.group('pre')\\n                tag_type = match.group('pre_l')\\n                tag_ver = match.group('pre_n')\\n                if match.group('dev'):\\n                    tag_value += match.group('dev')\\n                if match.group('post'):\\n                    tag_value += match.group('post')\\n            elif match.group('dev'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('dev')\\n                tag_ver = match.group('dev_n')\\n            elif match.group('post'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('post')\\n                tag_ver = match.group('post_n')\\n            else:\\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\\n        else:\\n            tag_type = None\\n            tag_value = ''\\n            tag_ver = 0\\n        if not tag_type:\\n            if self._revision is None:\\n                self._revision = 1\\n            return '{revision}'.format(revision=self._revision)\\n        tag_value = tag_value.strip('.')\\n        tag_ver = int(tag_ver if tag_ver else 0)\\n        if self._revision is None:\\n            tag_offset = self.tag_offsets.get(tag_type)\\n            if tag_offset is None:\\n                raise Exception('no tag offset defined for tag {0}'.format(tag_type))\\n            pkgrel = '0.{0}'.format(tag_offset + tag_ver)\\n        else:\\n            pkgrel = self._revision\\n        return '{pkgrel}.{tag_value}'.format(pkgrel=pkgrel, tag_value=tag_value)\\n    @property"
  },
  {
    "code": "def diff(a: str, b: str, a_name: str, b_name: str) -> str:\\n    import difflib\\n    a_lines = [line for line in a.splitlines(keepends=True)]\\n    b_lines = [line for line in b.splitlines(keepends=True)]\\n    diff_lines = []\\n    for line in difflib.unified_diff(\\n        a_lines, b_lines, fromfile=a_name, tofile=b_name, n=5\\n    ):\\n        if line[-1] == \"\\n\":\\n            diff_lines.append(line)\\n        else:\\n            diff_lines.append(line + \"\\n\")\\n            diff_lines.append(\"\\\\ No newline at end of file\\n\")\\n    return \"\".join(diff_lines)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "add more flake8 lints (#2653)",
    "fixed_code": "def diff(a: str, b: str, a_name: str, b_name: str) -> str:\\n    import difflib\\n    a_lines = a.splitlines(keepends=True)\\n    b_lines = b.splitlines(keepends=True)\\n    diff_lines = []\\n    for line in difflib.unified_diff(\\n        a_lines, b_lines, fromfile=a_name, tofile=b_name, n=5\\n    ):\\n        if line[-1] == \"\\n\":\\n            diff_lines.append(line)\\n        else:\\n            diff_lines.append(line + \"\\n\")\\n            diff_lines.append(\"\\\\ No newline at end of file\\n\")\\n    return \"\".join(diff_lines)"
  },
  {
    "code": "def _parse_header_params(s):\\n    plist = []\\n    while s[:1] == b\";\":\\n        s = s[1:]\\n        end = s.find(b\";\")\\n        while end > 0 and (s.count(b'\"', 0, end) - s.count(b'\\\\\"', 0, end)) % 2:\\n            end = s.find(b\";\", end + 1)\\n        if end < 0:\\n            end = len(s)\\n        f = s[:end]\\n        plist.append(f.strip())\\n        s = s[end:]\\n    return plist",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_header_params(s):\\n    plist = []\\n    while s[:1] == b\";\":\\n        s = s[1:]\\n        end = s.find(b\";\")\\n        while end > 0 and (s.count(b'\"', 0, end) - s.count(b'\\\\\"', 0, end)) % 2:\\n            end = s.find(b\";\", end + 1)\\n        if end < 0:\\n            end = len(s)\\n        f = s[:end]\\n        plist.append(f.strip())\\n        s = s[end:]\\n    return plist"
  },
  {
    "code": "def info(self, buffer=sys.stdout):\\n        if len(self._series) == 0:\\n            print >> buffer, 'DataFrame is empty!'\\n            print >> buffer, repr(self.index)\\n        print >> buffer, 'Index: %s entries, %s to %s' % (len(self.index),\\n                                                          min(self.index),\\n                                                          max(self.index))\\n        print >> buffer, 'Data columns:'\\n        series = self._series\\n        columns = sorted(self.cols())\\n        space = max([len(str(k)) for k in columns]) + 4\\n        for k in columns:\\n            out = _pfixed(k, space)\\n            N = notnull(series[k]).sum()\\n            out += '%d  non-null values' % N\\n            print >> buffer, out",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def info(self, buffer=sys.stdout):\\n        if len(self._series) == 0:\\n            print >> buffer, 'DataFrame is empty!'\\n            print >> buffer, repr(self.index)\\n        print >> buffer, 'Index: %s entries, %s to %s' % (len(self.index),\\n                                                          min(self.index),\\n                                                          max(self.index))\\n        print >> buffer, 'Data columns:'\\n        series = self._series\\n        columns = sorted(self.cols())\\n        space = max([len(str(k)) for k in columns]) + 4\\n        for k in columns:\\n            out = _pfixed(k, space)\\n            N = notnull(series[k]).sum()\\n            out += '%d  non-null values' % N\\n            print >> buffer, out"
  },
  {
    "code": "def _dt_index_cmp(opname):\\n\\tdef wrapper(self, other):\\n\\t\\tif isinstance(other, datetime):\\n\\t\\t\\tfunc = getattr(self, opname)\\n\\t\\t\\tresult = func(_to_m8(other))\\n\\t\\telif isinstance(other, np.ndarray):\\n\\t\\t\\tfunc = getattr(super(DatetimeIndex, self), opname)\\n\\t\\t\\tresult = func(other)\\n\\t\\telse:\\n\\t\\t\\tother = _ensure_datetime64(other)\\n\\t\\t\\tfunc = getattr(super(DatetimeIndex, self), opname)\\n\\t\\t\\tresult = func(other)\\n\\t\\ttry:\\n\\t\\t\\treturn result.view(np.ndarray)\\n\\t\\texcept:\\n\\t\\t\\treturn result\\n\\treturn wrapper",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix DatetimeIndex.groupby bug close #1430",
    "fixed_code": "def _dt_index_cmp(opname):\\n\\tdef wrapper(self, other):\\n\\t\\tfunc = getattr(super(DatetimeIndex, self), opname)\\n\\t\\tif isinstance(other, datetime):\\n\\t\\t\\tfunc = getattr(self, opname)\\n\\t\\t\\tother = _to_m8(other)\\n\\t\\telif isinstance(other, list):\\n\\t\\t\\tother = DatetimeIndex(other)\\n\\t\\telif not isinstance(other, np.ndarray):\\n\\t\\t\\tother = _ensure_datetime64(other)\\n\\t\\tresult = func(other)\\n\\t\\ttry:\\n\\t\\t\\treturn result.view(np.ndarray)\\n\\t\\texcept:\\n\\t\\t\\treturn result\\n\\treturn wrapper"
  },
  {
    "code": "def lookups(self, request):\\n        raise NotImplementedError",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #15960 -- Extended list filer API added in r16144 slightly to pass the current model admin to the SimpleListFilter.lookups method to support finer grained control over what is filtered over. Many thanks to Carl Meyer and Julien Phalip for the suggestion and patch.",
    "fixed_code": "def lookups(self, request, model_admin):\\n        raise NotImplementedError"
  },
  {
    "code": "def is_list_like(arg):\\n    return hasattr(arg, '__iter__') and not isinstance(arg, basestring) or hasattr(arg,'len')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/CLN: remove infer_types",
    "fixed_code": "def is_list_like(arg):\\n    return hasattr(arg, '__iter__') and not isinstance(arg, basestring)"
  },
  {
    "code": "def _free_pending_blocks(self):\\n        while True:\\n            try:\\n                block = self._pending_free_blocks.pop()\\n            except IndexError:\\n                break\\n            self._allocated_blocks.remove(block)\\n            self._free(block)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-32759: Free unused arenas in multiprocessing.heap (GH-5827)\\n\\nLarge shared arrays allocated using multiprocessing would remain allocated\\nuntil the process ends.",
    "fixed_code": "def _free_pending_blocks(self):\\n        while True:\\n            try:\\n                block = self._pending_free_blocks.pop()\\n            except IndexError:\\n                break\\n            self._add_free_block(block)\\n            self._remove_allocated_block(block)"
  },
  {
    "code": "def compare(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        if (self._is_special or other and other._is_special):\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n        return Decimal(self.__cmp__(other, context))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def compare(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if (self._is_special or other and other._is_special):\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n        return Decimal(self.__cmp__(other))"
  },
  {
    "code": "def include_router(\\n\\t\\tself,\\n\\t\\trouter: \"APIRouter\",\\n\\t\\t*,\\n\\t\\tprefix: str = \"\",\\n\\t\\ttags: List[str] = None,\\n\\t\\tresponses: Dict[Union[int, str], Dict[str, Any]] = None,\\n\\t) -> None:\\n\\t\\tif prefix:\\n\\t\\t\\tassert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\\n\\t\\t\\tassert not prefix.endswith(\\n\\t\\t\\t\\t\"/\"\\n\\t\\t\\t), \"A path prefix must not end with '/', as the routes will start with '/'\"\\n\\t\\tfor route in router.routes:\\n\\t\\t\\tif isinstance(route, APIRoute):\\n\\t\\t\\t\\tif responses is None:\\n\\t\\t\\t\\t\\tresponses = {}\\n\\t\\t\\t\\tresponses = {**responses, **route.responses}\\n\\t\\t\\t\\tself.add_api_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tresponse_model=route.response_model,\\n\\t\\t\\t\\t\\tstatus_code=route.status_code,\\n\\t\\t\\t\\t\\ttags=(route.tags or []) + (tags or []),\\n\\t\\t\\t\\t\\tsummary=route.summary,\\n\\t\\t\\t\\t\\tdescription=route.description,\\n\\t\\t\\t\\t\\tresponse_description=route.response_description,\\n\\t\\t\\t\\t\\tresponses=responses,\\n\\t\\t\\t\\t\\tdeprecated=route.deprecated,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\toperation_id=route.operation_id,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tcontent_type=route.content_type,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.Route):\\n\\t\\t\\t\\tself.add_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.WebSocketRoute):\\n\\t\\t\\t\\tself.add_websocket_route(\\n\\t\\t\\t\\t\\tprefix + route.path, route.endpoint, name=route.name\\n\\t\\t\\t\\t)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": ":bug: Fix handling additional responses in include_router (#140)",
    "fixed_code": "def include_router(\\n\\t\\tself,\\n\\t\\trouter: \"APIRouter\",\\n\\t\\t*,\\n\\t\\tprefix: str = \"\",\\n\\t\\ttags: List[str] = None,\\n\\t\\tresponses: Dict[Union[int, str], Dict[str, Any]] = None,\\n\\t) -> None:\\n\\t\\tif prefix:\\n\\t\\t\\tassert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\\n\\t\\t\\tassert not prefix.endswith(\\n\\t\\t\\t\\t\"/\"\\n\\t\\t\\t), \"A path prefix must not end with '/', as the routes will start with '/'\"\\n\\t\\tif responses is None:\\n\\t\\t\\tresponses = {}\\n\\t\\tfor route in router.routes:\\n\\t\\t\\tif isinstance(route, APIRoute):\\n\\t\\t\\t\\tcombined_responses = {**responses, **route.responses}\\n\\t\\t\\t\\tself.add_api_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tresponse_model=route.response_model,\\n\\t\\t\\t\\t\\tstatus_code=route.status_code,\\n\\t\\t\\t\\t\\ttags=(route.tags or []) + (tags or []),\\n\\t\\t\\t\\t\\tsummary=route.summary,\\n\\t\\t\\t\\t\\tdescription=route.description,\\n\\t\\t\\t\\t\\tresponse_description=route.response_description,\\n\\t\\t\\t\\t\\tresponses=combined_responses,\\n\\t\\t\\t\\t\\tdeprecated=route.deprecated,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\toperation_id=route.operation_id,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tcontent_type=route.content_type,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.Route):\\n\\t\\t\\t\\tself.add_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.WebSocketRoute):\\n\\t\\t\\t\\tself.add_websocket_route(\\n\\t\\t\\t\\t\\tprefix + route.path, route.endpoint, name=route.name\\n\\t\\t\\t\\t)"
  },
  {
    "code": "def downgrade():\\n    conn = op.get_bind()\\n    if conn.dialect.name == 'mysql':\\n        conn.execute(\"SET time_zone = '+00:00'\")\\n        op.alter_column(table_name='chart', column_name='last_modified', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag', column_name='last_scheduler_run', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag', column_name='last_pickled', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag', column_name='last_expired', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_pickle', column_name='created_dttm', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_run', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_run', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_run', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='import_error', column_name='DATETIME', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='job', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='job', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='job', column_name='latest_heartbeat', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='log', column_name='dttm', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='log', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='sla_miss', column_name='execution_date', type_=mysql.DATETIME(fsp=6),\\n                        nullable=False)\\n        op.alter_column(table_name='sla_miss', column_name='DATETIME', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_fail', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_fail', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_fail', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_instance', column_name='execution_date', type_=mysql.DATETIME(fsp=6),\\n                        nullable=False)\\n        op.alter_column(table_name='task_instance', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_instance', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_instance', column_name='queued_dttm', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='xcom', column_name='DATETIME', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='xcom', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n    else:\\n        if conn.dialect.name in ('sqlite', 'mssql'):\\n            return\\n        if conn.dialect.name == 'postgresql':\\n            conn.execute(\"set timezone=UTC\")\\n        op.alter_column(table_name='chart', column_name='last_modified', type_=sa.DateTime())\\n        op.alter_column(table_name='dag', column_name='last_scheduler_run', type_=sa.DateTime())\\n        op.alter_column(table_name='dag', column_name='last_pickled', type_=sa.DateTime())\\n        op.alter_column(table_name='dag', column_name='last_expired', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_pickle', column_name='created_dttm', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_run', column_name='execution_date', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_run', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_run', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='import_error', column_name='timestamp', type_=sa.DateTime())\\n        op.alter_column(table_name='job', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='job', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='job', column_name='latest_heartbeat', type_=sa.DateTime())\\n        op.alter_column(table_name='log', column_name='dttm', type_=sa.DateTime())\\n        op.alter_column(table_name='log', column_name='execution_date', type_=sa.DateTime())\\n        op.alter_column(table_name='sla_miss', column_name='execution_date', type_=sa.DateTime(), nullable=False)\\n        op.alter_column(table_name='sla_miss', column_name='timestamp', type_=sa.DateTime())\\n        op.alter_column(table_name='task_fail', column_name='execution_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_fail', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_fail', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_instance', column_name='execution_date', type_=sa.DateTime(), nullable=False)\\n        op.alter_column(table_name='task_instance', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_instance', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_instance', column_name='queued_dttm', type_=sa.DateTime())\\n        op.alter_column(table_name='xcom', column_name='timestamp', type_=sa.DateTime())\\n        op.alter_column(table_name='xcom', column_name='execution_date', type_=sa.DateTime())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def downgrade():\\n    conn = op.get_bind()\\n    if conn.dialect.name == 'mysql':\\n        conn.execute(\"SET time_zone = '+00:00'\")\\n        op.alter_column(table_name='chart', column_name='last_modified', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag', column_name='last_scheduler_run', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag', column_name='last_pickled', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag', column_name='last_expired', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_pickle', column_name='created_dttm', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_run', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_run', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='dag_run', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='import_error', column_name='DATETIME', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='job', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='job', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='job', column_name='latest_heartbeat', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='log', column_name='dttm', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='log', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='sla_miss', column_name='execution_date', type_=mysql.DATETIME(fsp=6),\\n                        nullable=False)\\n        op.alter_column(table_name='sla_miss', column_name='DATETIME', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_fail', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_fail', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_fail', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_instance', column_name='execution_date', type_=mysql.DATETIME(fsp=6),\\n                        nullable=False)\\n        op.alter_column(table_name='task_instance', column_name='start_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_instance', column_name='end_date', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='task_instance', column_name='queued_dttm', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='xcom', column_name='DATETIME', type_=mysql.DATETIME(fsp=6))\\n        op.alter_column(table_name='xcom', column_name='execution_date', type_=mysql.DATETIME(fsp=6))\\n    else:\\n        if conn.dialect.name in ('sqlite', 'mssql'):\\n            return\\n        if conn.dialect.name == 'postgresql':\\n            conn.execute(\"set timezone=UTC\")\\n        op.alter_column(table_name='chart', column_name='last_modified', type_=sa.DateTime())\\n        op.alter_column(table_name='dag', column_name='last_scheduler_run', type_=sa.DateTime())\\n        op.alter_column(table_name='dag', column_name='last_pickled', type_=sa.DateTime())\\n        op.alter_column(table_name='dag', column_name='last_expired', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_pickle', column_name='created_dttm', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_run', column_name='execution_date', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_run', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='dag_run', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='import_error', column_name='timestamp', type_=sa.DateTime())\\n        op.alter_column(table_name='job', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='job', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='job', column_name='latest_heartbeat', type_=sa.DateTime())\\n        op.alter_column(table_name='log', column_name='dttm', type_=sa.DateTime())\\n        op.alter_column(table_name='log', column_name='execution_date', type_=sa.DateTime())\\n        op.alter_column(table_name='sla_miss', column_name='execution_date', type_=sa.DateTime(), nullable=False)\\n        op.alter_column(table_name='sla_miss', column_name='timestamp', type_=sa.DateTime())\\n        op.alter_column(table_name='task_fail', column_name='execution_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_fail', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_fail', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_instance', column_name='execution_date', type_=sa.DateTime(), nullable=False)\\n        op.alter_column(table_name='task_instance', column_name='start_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_instance', column_name='end_date', type_=sa.DateTime())\\n        op.alter_column(table_name='task_instance', column_name='queued_dttm', type_=sa.DateTime())\\n        op.alter_column(table_name='xcom', column_name='timestamp', type_=sa.DateTime())\\n        op.alter_column(table_name='xcom', column_name='execution_date', type_=sa.DateTime())"
  },
  {
    "code": "def __getattr__(self, attr):\\n        if attr in self._internal_names_set:\\n            return object.__getattribute__(self, attr)\\n        if attr in self._attributes:\\n            return getattr(self.groupby, attr)\\n        if attr in self.obj:\\n            return self[attr]\\n        if attr in self._deprecated_invalids:\\n            raise ValueError(\".resample() is now a deferred operation\\n\"\\n                             \"\\tuse .resample(...).mean() instead of \"\\n                             \".resample(...)\")\\n        if attr not in self._deprecated_valids:\\n            self = self._deprecated()\\n        return object.__getattribute__(self, attr)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: resample warnings\\n\\ncloses #13618\\ncloses #13520\\n\\nAuthor: Chris <cbartak@gmail.com>\\n\\nCloses #13675 from chris-b1/resample-warning and squashes the following commits:\\n\\n2185c1f [Chris] whatsnew note\\nc58c70c [Chris] DOC: resample warnings",
    "fixed_code": "def __getattr__(self, attr):\\n        if attr in self._internal_names_set:\\n            return object.__getattribute__(self, attr)\\n        if attr in self._attributes:\\n            return getattr(self.groupby, attr)\\n        if attr in self.obj:\\n            return self[attr]\\n        if attr in self._deprecated_invalids:\\n            raise ValueError(\".resample() is now a deferred operation\\n\"\\n                             \"\\tuse .resample(...).mean() instead of \"\\n                             \".resample(...)\")\\n        matches_pattern = any(attr.startswith(x) for x\\n                              in self._deprecated_valid_patterns)\\n        if not matches_pattern and attr not in self._deprecated_valids:\\n            self = self._deprecated(attr)\\n        return object.__getattribute__(self, attr)"
  },
  {
    "code": "def time_zfill(self):\\n        self.s.str.zfill(10)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_zfill(self, dtype):\\n        self.s.str.zfill(10)"
  },
  {
    "code": "def signature_parameters(self):\\n        if inspect.isclass(self.obj):\\n            if hasattr(self.obj, '_accessors') and (\\n                    self.name.split('.')[-1] in\\n                    self.obj._accessors):\\n                return tuple()\\n        try:\\n            sig = signature(self.obj)\\n        except (TypeError, ValueError):\\n            return tuple()\\n        params = sig.args\\n        if sig.varargs:\\n            params.append(\"*\" + sig.varargs)\\n        if sig.keywords:\\n            params.append(\"**\" + sig.keywords)\\n        params = tuple(params)\\n        if params and params[0] in ('self', 'cls'):\\n            return params[1:]\\n        return params\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def signature_parameters(self):\\n        if inspect.isclass(self.obj):\\n            if hasattr(self.obj, '_accessors') and (\\n                    self.name.split('.')[-1] in\\n                    self.obj._accessors):\\n                return tuple()\\n        try:\\n            sig = signature(self.obj)\\n        except (TypeError, ValueError):\\n            return tuple()\\n        params = sig.args\\n        if sig.varargs:\\n            params.append(\"*\" + sig.varargs)\\n        if sig.keywords:\\n            params.append(\"**\" + sig.keywords)\\n        params = tuple(params)\\n        if params and params[0] in ('self', 'cls'):\\n            return params[1:]\\n        return params\\n    @property"
  },
  {
    "code": "def __str__(self):\\n        return to_native(self.data, errors='surrogate_or_strict')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __str__(self):\\n        return to_native(self.data, errors='surrogate_or_strict')"
  },
  {
    "code": "def iget_value(self, i, j):\\n        warnings.warn(\"iget_value(i, j) is deprecated. Please use .iat[i, j]\",\\n                      FutureWarning, stacklevel=2)\\n        return self.iat[i, j]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def iget_value(self, i, j):\\n        warnings.warn(\"iget_value(i, j) is deprecated. Please use .iat[i, j]\",\\n                      FutureWarning, stacklevel=2)\\n        return self.iat[i, j]"
  },
  {
    "code": "def _dt_box_array(arr, offset=None, tzinfo=None):\\n    if arr is None:\\n        return arr\\n    if not isinstance(arr, np.ndarray):\\n        return arr\\n    boxfunc = lambda x: _dt_box(x, offset=offset, tzinfo=tzinfo)\\n    boxer = np.frompyfunc(boxfunc, 1, 1)\\n    return boxer(arr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _dt_box_array(arr, offset=None, tzinfo=None):\\n    if arr is None:\\n        return arr\\n    if not isinstance(arr, np.ndarray):\\n        return arr\\n    boxfunc = lambda x: _dt_box(x, offset=offset, tzinfo=tzinfo)\\n    boxer = np.frompyfunc(boxfunc, 1, 1)\\n    return boxer(arr)"
  },
  {
    "code": "def __ne__(self, other):\\n        if not isinstance(other, poly1d):\\n            return NotImplemented\\n        return not self.__eq__(other)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __ne__(self, other):\\n        if not isinstance(other, poly1d):\\n            return NotImplemented\\n        return not self.__eq__(other)"
  },
  {
    "code": "def fmgr_get_device(fmg, paramgram):\\n    update_url = '/dvm/cmd/update/device'\\n    update_dict = {\\n        \"adom\": paramgram['adom'],\\n        \"device\": paramgram['device_unique_name'],\\n        \"flags\": \"create_task\"\\n    }\\n    update_call = fmg.execute(update_url, update_dict)\\n    url = '/dvmdb/adom/{adom}/device'.format(adom=paramgram[\"adom\"])\\n    device_found = 0\\n    response = []\\n    if paramgram[\"device_serial\"] is not None:\\n        datagram = {\\n            \"filter\": [\"sn\", \"==\", paramgram[\"device_serial\"]]\\n        }\\n        response = fmg.get(url, datagram)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_unique_name\"] is not None:\\n        datagram = {\\n            \"filter\": [\"name\", \"==\", paramgram[\"device_unique_name\"]]\\n        }\\n        response = fmg.get(url, datagram)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_ip\"] is not None:\\n        datagram = {\\n            \"filter\": [\"ip\", \"==\", paramgram[\"device_ip\"]]\\n        }\\n        response = fmg.get(url, datagram)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_query (#52770)",
    "fixed_code": "def fmgr_get_device(fmgr, paramgram):\\n    response = DEFAULT_RESULT_OBJ\\n    url = \"\"\\n    datagram = {}\\n    update_url = '/dvm/cmd/update/device'\\n    update_dict = {\\n        \"adom\": paramgram['adom'],\\n        \"device\": paramgram['device_unique_name'],\\n        \"flags\": \"create_task\"\\n    }\\n    fmgr.process_request(update_url, update_dict, FMGRMethods.EXEC)\\n    url = '/dvmdb/adom/{adom}/device'.format(adom=paramgram[\"adom\"])\\n    device_found = 0\\n    response = []\\n    if paramgram[\"device_serial\"] is not None:\\n        datagram = {\\n            \"filter\": [\"sn\", \"==\", paramgram[\"device_serial\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_unique_name\"] is not None:\\n        datagram = {\\n            \"filter\": [\"name\", \"==\", paramgram[\"device_unique_name\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_ip\"] is not None:\\n        datagram = {\\n            \"filter\": [\"ip\", \"==\", paramgram[\"device_ip\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    return response"
  },
  {
    "code": "def _create_sql_schema(\\n        self,\\n        frame: DataFrame,\\n        table_name: str,\\n        keys: Optional[List[str]] = None,\\n        dtype: Optional[dict] = None,\\n        schema: Optional[str] = None,\\n    ):\\n        table = SQLTable(\\n            table_name,\\n            self,\\n            frame=frame,\\n            index=False,\\n            keys=keys,\\n            dtype=dtype,\\n            schema=schema,\\n        )\\n        return str(table.sql_schema())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_sql_schema(\\n        self,\\n        frame: DataFrame,\\n        table_name: str,\\n        keys: Optional[List[str]] = None,\\n        dtype: Optional[dict] = None,\\n        schema: Optional[str] = None,\\n    ):\\n        table = SQLTable(\\n            table_name,\\n            self,\\n            frame=frame,\\n            index=False,\\n            keys=keys,\\n            dtype=dtype,\\n            schema=schema,\\n        )\\n        return str(table.sql_schema())"
  },
  {
    "code": "def get_indexer(self, target, method=None, limit=None, tolerance=None):\\n\\t\\tif com.any_not_none(method, tolerance, limit) or not is_list_like(target):\\n\\t\\t\\treturn super().get_indexer(\\n\\t\\t\\t\\ttarget, method=method, tolerance=tolerance, limit=limit\\n\\t\\t\\t)\\n\\t\\tif self.step > 0:\\n\\t\\t\\tstart, stop, step = self.start, self.stop, self.step\\n\\t\\telse:\\n\\t\\t\\tstart, stop, step = (self.stop - self.step, self.start + 1, -self.step)\\n\\t\\ttarget_array = np.asarray(target)\\n\\t\\tif not (is_integer_dtype(target_array) and target_array.ndim == 1):\\n\\t\\t\\treturn super().get_indexer(target, method=method, tolerance=tolerance)\\n\\t\\tlocs = target_array - start\\n\\t\\tvalid = (locs % step == 0) & (locs >= 0) & (target_array < stop)\\n\\t\\tlocs[~valid] = -1\\n\\t\\tlocs[valid] = locs[valid] / step\\n\\t\\tif step != self.step:\\n\\t\\t\\tlocs[valid] = len(self) - 1 - locs[valid]\\n\\t\\treturn ensure_platform_int(locs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_indexer(self, target, method=None, limit=None, tolerance=None):\\n\\t\\tif com.any_not_none(method, tolerance, limit) or not is_list_like(target):\\n\\t\\t\\treturn super().get_indexer(\\n\\t\\t\\t\\ttarget, method=method, tolerance=tolerance, limit=limit\\n\\t\\t\\t)\\n\\t\\tif self.step > 0:\\n\\t\\t\\tstart, stop, step = self.start, self.stop, self.step\\n\\t\\telse:\\n\\t\\t\\tstart, stop, step = (self.stop - self.step, self.start + 1, -self.step)\\n\\t\\ttarget_array = np.asarray(target)\\n\\t\\tif not (is_integer_dtype(target_array) and target_array.ndim == 1):\\n\\t\\t\\treturn super().get_indexer(target, method=method, tolerance=tolerance)\\n\\t\\tlocs = target_array - start\\n\\t\\tvalid = (locs % step == 0) & (locs >= 0) & (target_array < stop)\\n\\t\\tlocs[~valid] = -1\\n\\t\\tlocs[valid] = locs[valid] / step\\n\\t\\tif step != self.step:\\n\\t\\t\\tlocs[valid] = len(self) - 1 - locs[valid]\\n\\t\\treturn ensure_platform_int(locs)"
  },
  {
    "code": "def iterparse(source, events=None, parser=None):\\n    close_source = False\\n    if not hasattr(source, \"read\"):\\n        source = open(source, \"rb\")\\n        close_source = True\\n    return _IterParseIterator(source, events, parser, close_source)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #25688: Fixed file leak in ElementTree.iterparse() raising an error.",
    "fixed_code": "def iterparse(source, events=None, parser=None):\\n    close_source = False\\n    if not hasattr(source, \"read\"):\\n        source = open(source, \"rb\")\\n        close_source = True\\n    try:\\n        return _IterParseIterator(source, events, parser, close_source)\\n    except:\\n        if close_source:\\n            source.close()\\n        raise"
  },
  {
    "code": "def load_lookups_data(lang, tables):\\n\\tutil.logger.debug(\"Loading lookups from spacy-lookups-data: %s\", tables)\\n\\tlookups = load_lookups(lang=lang, tables=tables)\\n\\treturn lookups",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_lookups_data(lang, tables):\\n\\tutil.logger.debug(\"Loading lookups from spacy-lookups-data: %s\", tables)\\n\\tlookups = load_lookups(lang=lang, tables=tables)\\n\\treturn lookups"
  },
  {
    "code": "def check_class_weight_balanced_linear_classifier(name, Classifier):\\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\\n                  [1.0, 1.0], [1.0, 0.0]])\\n    y = np.array([1, 1, 1, -1, -1])\\n    with warnings.catch_warnings(record=True):\\n        classifier = Classifier()\\n    if hasattr(classifier, \"n_iter\"):\\n        classifier.set_params(n_iter=1000)\\n    set_random_state(classifier)\\n    classifier.set_params(class_weight='balanced')\\n    coef_balanced = classifier.fit(X, y).coef_.copy()\\n    n_samples = len(y)\\n    n_classes = float(len(np.unique(y)))\\n    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),\\n                    -1: n_samples / (np.sum(y == -1) * n_classes)}\\n    classifier.set_params(class_weight=class_weight)\\n    coef_manual = classifier.fit(X, y).coef_.copy()\\n    assert_array_almost_equal(coef_balanced, coef_manual)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_class_weight_balanced_linear_classifier(name, Classifier):\\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\\n                  [1.0, 1.0], [1.0, 0.0]])\\n    y = np.array([1, 1, 1, -1, -1])\\n    with ignore_warnings(category=DeprecationWarning):\\n        classifier = Classifier()\\n    if hasattr(classifier, \"n_iter\"):\\n        classifier.set_params(n_iter=1000)\\n    set_random_state(classifier)\\n    classifier.set_params(class_weight='balanced')\\n    coef_balanced = classifier.fit(X, y).coef_.copy()\\n    n_samples = len(y)\\n    n_classes = float(len(np.unique(y)))\\n    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),\\n                    -1: n_samples / (np.sum(y == -1) * n_classes)}\\n    classifier.set_params(class_weight=class_weight)\\n    coef_manual = classifier.fit(X, y).coef_.copy()\\n    assert_array_almost_equal(coef_balanced, coef_manual)"
  },
  {
    "code": "def get(self, key, default=None, version=None):\\n        key = self.make_key(key, version=version)\\n        self.validate_key(key)\\n        db = router.db_for_read(self.cache_model_class)\\n        connection = connections[db]\\n        quote_name = connection.ops.quote_name\\n        table = quote_name(self._table)\\n        with connection.cursor() as cursor:\\n            cursor.execute(\\n                'SELECT %s, %s, %s FROM %s WHERE %s = %%s' % (\\n                    quote_name('cache_key'),\\n                    quote_name('value'),\\n                    quote_name('expires'),\\n                    table,\\n                    quote_name('cache_key'),\\n                ),\\n                [key]\\n            )\\n            row = cursor.fetchone()\\n        if row is None:\\n            return default\\n        expires = row[2]\\n        expression = models.Expression(output_field=models.DateTimeField())\\n        for converter in (connection.ops.get_db_converters(expression) +\\n                          expression.get_db_converters(connection)):\\n            if func_supports_parameter(converter, 'context'):  \\n                expires = converter(expires, expression, connection, {})\\n            else:\\n                expires = converter(expires, expression, connection)\\n        if expires < timezone.now():\\n            db = router.db_for_write(self.cache_model_class)\\n            connection = connections[db]\\n            with connection.cursor() as cursor:\\n                cursor.execute(\\n                    'DELETE FROM %s WHERE %s = %%s' % (\\n                        table,\\n                        quote_name('cache_key'),\\n                    ),\\n                    [key]\\n                )\\n            return default\\n        value = connection.ops.process_clob(row[1])\\n        return pickle.loads(base64.b64decode(value.encode()))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #29626, #29584 -- Added optimized versions of get_many() and delete_many() for the db cache backend.",
    "fixed_code": "def get(self, key, default=None, version=None):\\n        return self.get_many([key], version).get(key, default)\\n    def get_many(self, keys, version=None):\\n        if not keys:\\n            return {}\\n        key_map = {}\\n        for key in keys:\\n            self.validate_key(key)\\n            key_map[self.make_key(key, version)] = key\\n        db = router.db_for_read(self.cache_model_class)\\n        connection = connections[db]\\n        quote_name = connection.ops.quote_name\\n        table = quote_name(self._table)\\n        with connection.cursor() as cursor:\\n            cursor.execute(\\n                'SELECT %s, %s, %s FROM %s WHERE %s IN (%s)' % (\\n                    quote_name('cache_key'),\\n                    quote_name('value'),\\n                    quote_name('expires'),\\n                    table,\\n                    quote_name('cache_key'),\\n                    ', '.join(['%s'] * len(key_map)),\\n                ),\\n                list(key_map),\\n            )\\n            rows = cursor.fetchall()\\n        result = {}\\n        expired_keys = []\\n        expression = models.Expression(output_field=models.DateTimeField())\\n        converters = (connection.ops.get_db_converters(expression) + expression.get_db_converters(connection))\\n        for key, value, expires in rows:\\n            for converter in converters:\\n                if func_supports_parameter(converter, 'context'):  \\n                    expires = converter(expires, expression, connection, {})\\n                else:\\n                    expires = converter(expires, expression, connection)\\n            if expires < timezone.now():\\n                expired_keys.append(key)\\n            else:\\n                value = connection.ops.process_clob(value)\\n                value = pickle.loads(base64.b64decode(value.encode()))\\n                result[key_map.get(key)] = value\\n        self._base_delete_many(expired_keys)\\n        return result"
  },
  {
    "code": "def convert_array_types(cls, value: Union[List[Any], SortedSet]) -> List[Any]:\\n        return [cls.convert_value(nested_value) for nested_value in value]\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[Issue#22846] allow option to encode or not encode UUID when uploading from Cassandra to GCS (#23766)",
    "fixed_code": "def convert_array_types(self, value: Union[List[Any], SortedSet]) -> List[Any]:\\n        return [self.convert_value(nested_value) for nested_value in value]"
  },
  {
    "code": "def _valid_locales(locales, normalize):\\n    return [\\n        loc\\n        for loc in (\\n            locale.normalize(loc.strip()) if normalize else loc.strip()\\n            for loc in locales\\n        )\\n        if can_set_locale(loc)\\n    ]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _valid_locales(locales, normalize):\\n    return [\\n        loc\\n        for loc in (\\n            locale.normalize(loc.strip()) if normalize else loc.strip()\\n            for loc in locales\\n        )\\n        if can_set_locale(loc)\\n    ]"
  },
  {
    "code": "def __getitem__(self, key):\\n        try:\\n            s = self._series[key]\\n            s.name = key\\n            return s\\n        except (TypeError, KeyError):\\n            if isinstance(key, slice):\\n                date_rng = self.index[key]\\n                return self.reindex(date_rng)\\n            elif isinstance(key, (np.ndarray, list)):\\n                if isinstance(key, list):\\n                    key = lib.list_to_object_array(key)\\n                if com._is_bool_indexer(key):\\n                    key = np.asarray(key, dtype=bool)\\n                return self._getitem_array(key)\\n            else:  \\n                raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Various inconsistencies in DataFrame __getitem__ and __setitem__ behavior",
    "fixed_code": "def __getitem__(self, key):\\n        try:\\n            s = self._series[key]\\n            s.name = key\\n            return s\\n        except (TypeError, KeyError):\\n            if isinstance(key, slice):\\n                date_rng = self.index[key]\\n                return self.reindex(date_rng)\\n            elif isinstance(key, (np.ndarray, list)):\\n                return self._getitem_array(key)\\n            else:  \\n                raise"
  },
  {
    "code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        ip=dict(type='str'),\\n        servername=dict(type='str'),\\n        servicetype=dict(\\n            type='str',\\n            choices=[\\n                'HTTP',\\n                'FTP',\\n                'TCP',\\n                'UDP',\\n                'SSL',\\n                'SSL_BRIDGE',\\n                'SSL_TCP',\\n                'DTLS',\\n                'NNTP',\\n                'RPCSVR',\\n                'DNS',\\n                'ADNS',\\n                'SNMP',\\n                'RTSP',\\n                'DHCPRA',\\n                'ANY',\\n                'SIP_UDP',\\n                'SIP_TCP',\\n                'SIP_SSL',\\n                'DNS_TCP',\\n                'ADNS_TCP',\\n                'MYSQL',\\n                'MSSQL',\\n                'ORACLE',\\n                'RADIUS',\\n                'RADIUSListener',\\n                'RDP',\\n                'DIAMETER',\\n                'SSL_DIAMETER',\\n                'TFTP',\\n                'SMPP',\\n                'PPTP',\\n                'GRE',\\n                'SYSLOGTCP',\\n                'SYSLOGUDP',\\n                'FIX',\\n                'SSL_FIX'\\n            ]\\n        ),\\n        port=dict(type='int'),\\n        cleartextport=dict(type='int'),\\n        cachetype=dict(\\n            type='str',\\n            choices=[\\n                'TRANSPARENT',\\n                'REVERSE',\\n                'FORWARD',\\n            ]\\n        ),\\n        maxclient=dict(type='float'),\\n        healthmonitor=dict(\\n            type='bool',\\n            default=True,\\n        ),\\n        maxreq=dict(type='float'),\\n        cacheable=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        cip=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ]\\n        ),\\n        cipheader=dict(type='str'),\\n        usip=dict(type='bool'),\\n        useproxyport=dict(type='bool'),\\n        sp=dict(type='bool'),\\n        rtspsessionidremap=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        clttimeout=dict(type='float'),\\n        svrtimeout=dict(type='float'),\\n        customserverid=dict(\\n            type='str',\\n            default='None',\\n        ),\\n        cka=dict(type='bool'),\\n        tcpb=dict(type='bool'),\\n        cmp=dict(type='bool'),\\n        maxbandwidth=dict(type='float'),\\n        accessdown=dict(\\n            type='bool',\\n            default=False\\n        ),\\n        monthreshold=dict(type='float'),\\n        downstateflush=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        tcpprofilename=dict(type='str'),\\n        httpprofilename=dict(type='str'),\\n        hashid=dict(type='float'),\\n        comment=dict(type='str'),\\n        appflowlog=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        netprofile=dict(type='str'),\\n        processlocal=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='DISABLED',\\n        ),\\n        dnsprofilename=dict(type='str'),\\n        ipaddress=dict(type='str'),\\n        graceful=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    hand_inserted_arguments = dict(\\n        monitor_bindings=dict(type='list'),\\n        disabled=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    argument_spec.update(hand_inserted_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines,\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'maxclient',\\n        'healthmonitor',\\n        'maxreq',\\n        'cacheable',\\n        'cip',\\n        'cipheader',\\n        'usip',\\n        'useproxyport',\\n        'sp',\\n        'rtspsessionidremap',\\n        'clttimeout',\\n        'svrtimeout',\\n        'customserverid',\\n        'cka',\\n        'tcpb',\\n        'cmp',\\n        'maxbandwidth',\\n        'accessdown',\\n        'monthreshold',\\n        'downstateflush',\\n        'tcpprofilename',\\n        'httpprofilename',\\n        'hashid',\\n        'comment',\\n        'appflowlog',\\n        'netprofile',\\n        'processlocal',\\n        'dnsprofilename',\\n        'ipaddress',\\n        'graceful',\\n    ]\\n    readonly_attrs = [\\n        'numofconnections',\\n        'policyname',\\n        'serviceconftype',\\n        'serviceconftype2',\\n        'value',\\n        'gslb',\\n        'dup_state',\\n        'publicip',\\n        'publicport',\\n        'svrstate',\\n        'monitor_state',\\n        'monstatcode',\\n        'lastresponse',\\n        'responsetime',\\n        'riseapbrstatsmsgcode2',\\n        'monstatparam1',\\n        'monstatparam2',\\n        'monstatparam3',\\n        'statechangetimesec',\\n        'statechangetimemsec',\\n        'tickssincelaststatechange',\\n        'stateupdatereason',\\n        'clmonowner',\\n        'clmonview',\\n        'serviceipstr',\\n        'oracleserverversion',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'cipheader',\\n        'serverid',\\n        'state',\\n        'td',\\n        'monitor_name_svc',\\n        'riseapbrstatsmsgcode',\\n        'graceful',\\n        'all',\\n        'Internal',\\n        'newname',\\n    ]\\n    transforms = {\\n        'pathmonitorindv': ['bool_yes_no'],\\n        'cacheable': ['bool_yes_no'],\\n        'cka': ['bool_yes_no'],\\n        'pathmonitor': ['bool_yes_no'],\\n        'tcpb': ['bool_yes_no'],\\n        'sp': ['bool_on_off'],\\n        'graceful': ['bool_yes_no'],\\n        'usip': ['bool_yes_no'],\\n        'healthmonitor': ['bool_yes_no'],\\n        'useproxyport': ['bool_yes_no'],\\n        'rtspsessionidremap': ['bool_on_off'],\\n        'accessdown': ['bool_yes_no'],\\n        'cmp': ['bool_yes_no'],\\n    }\\n    monitor_bindings_rw_attrs = [\\n        'servicename',\\n        'servicegroupname',\\n        'dup_state',\\n        'dup_weight',\\n        'monitorname',\\n        'weight',\\n    ]\\n    if module.params['ip'] is None:\\n        module.params['ip'] = module.params['ipaddress']\\n    service_proxy = ConfigProxy(\\n        actual=service(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.add()\\n                    sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not all_identical(client, module, service_proxy, monitor_bindings_rw_attrs):\\n                diff_dict = diff(client, module, service_proxy)\\n                immutables_changed = get_immutables_intersection(service_proxy, diff_dict.keys())\\n                if immutables_changed != []:\\n                    msg = 'Cannot update immutable attributes %s. Must delete and recreate entity.' % (immutables_changed,)\\n                    module.fail_json(msg=msg, diff=diff_dict, **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    if not module.check_mode:\\n                        service_proxy.update()\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    if not module.check_mode:\\n                        sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                module_result['changed'] = True\\n                if not module.check_mode:\\n                    if module.params['save_config']:\\n                        client.save_config()\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                res = do_state_change(client, module, service_proxy)\\n                if res.errorcode != 0:\\n                    msg = 'Error when setting disabled state. errorcode: %s message: %s' % (res.errorcode, res.message)\\n                    module.fail_json(msg=msg, **module_result)\\n            if not module.check_mode:\\n                log('Sanity checks for state present')\\n                if not service_exists(client, module):\\n                    module.fail_json(msg='Service does not exist', **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    module.fail_json(msg='Service differs from configured', diff=diff(client, module, service_proxy), **module_result)\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    module.fail_json(msg='Monitor bindings are not identical', **module_result)\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if service_exists(client, module):\\n                    module.fail_json(msg='Service still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        ip=dict(type='str'),\\n        servername=dict(type='str'),\\n        servicetype=dict(\\n            type='str',\\n            choices=[\\n                'HTTP',\\n                'FTP',\\n                'TCP',\\n                'UDP',\\n                'SSL',\\n                'SSL_BRIDGE',\\n                'SSL_TCP',\\n                'DTLS',\\n                'NNTP',\\n                'RPCSVR',\\n                'DNS',\\n                'ADNS',\\n                'SNMP',\\n                'RTSP',\\n                'DHCPRA',\\n                'ANY',\\n                'SIP_UDP',\\n                'SIP_TCP',\\n                'SIP_SSL',\\n                'DNS_TCP',\\n                'ADNS_TCP',\\n                'MYSQL',\\n                'MSSQL',\\n                'ORACLE',\\n                'RADIUS',\\n                'RADIUSListener',\\n                'RDP',\\n                'DIAMETER',\\n                'SSL_DIAMETER',\\n                'TFTP',\\n                'SMPP',\\n                'PPTP',\\n                'GRE',\\n                'SYSLOGTCP',\\n                'SYSLOGUDP',\\n                'FIX',\\n                'SSL_FIX'\\n            ]\\n        ),\\n        port=dict(type='int'),\\n        cleartextport=dict(type='int'),\\n        cachetype=dict(\\n            type='str',\\n            choices=[\\n                'TRANSPARENT',\\n                'REVERSE',\\n                'FORWARD',\\n            ]\\n        ),\\n        maxclient=dict(type='float'),\\n        healthmonitor=dict(\\n            type='bool',\\n            default=True,\\n        ),\\n        maxreq=dict(type='float'),\\n        cacheable=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        cip=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ]\\n        ),\\n        cipheader=dict(type='str'),\\n        usip=dict(type='bool'),\\n        useproxyport=dict(type='bool'),\\n        sp=dict(type='bool'),\\n        rtspsessionidremap=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        clttimeout=dict(type='float'),\\n        svrtimeout=dict(type='float'),\\n        customserverid=dict(\\n            type='str',\\n            default='None',\\n        ),\\n        cka=dict(type='bool'),\\n        tcpb=dict(type='bool'),\\n        cmp=dict(type='bool'),\\n        maxbandwidth=dict(type='float'),\\n        accessdown=dict(\\n            type='bool',\\n            default=False\\n        ),\\n        monthreshold=dict(type='float'),\\n        downstateflush=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        tcpprofilename=dict(type='str'),\\n        httpprofilename=dict(type='str'),\\n        hashid=dict(type='float'),\\n        comment=dict(type='str'),\\n        appflowlog=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        netprofile=dict(type='str'),\\n        processlocal=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='DISABLED',\\n        ),\\n        dnsprofilename=dict(type='str'),\\n        ipaddress=dict(type='str'),\\n        graceful=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    hand_inserted_arguments = dict(\\n        monitor_bindings=dict(type='list'),\\n        disabled=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    argument_spec.update(hand_inserted_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines,\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'maxclient',\\n        'healthmonitor',\\n        'maxreq',\\n        'cacheable',\\n        'cip',\\n        'cipheader',\\n        'usip',\\n        'useproxyport',\\n        'sp',\\n        'rtspsessionidremap',\\n        'clttimeout',\\n        'svrtimeout',\\n        'customserverid',\\n        'cka',\\n        'tcpb',\\n        'cmp',\\n        'maxbandwidth',\\n        'accessdown',\\n        'monthreshold',\\n        'downstateflush',\\n        'tcpprofilename',\\n        'httpprofilename',\\n        'hashid',\\n        'comment',\\n        'appflowlog',\\n        'netprofile',\\n        'processlocal',\\n        'dnsprofilename',\\n        'ipaddress',\\n        'graceful',\\n    ]\\n    readonly_attrs = [\\n        'numofconnections',\\n        'policyname',\\n        'serviceconftype',\\n        'serviceconftype2',\\n        'value',\\n        'gslb',\\n        'dup_state',\\n        'publicip',\\n        'publicport',\\n        'svrstate',\\n        'monitor_state',\\n        'monstatcode',\\n        'lastresponse',\\n        'responsetime',\\n        'riseapbrstatsmsgcode2',\\n        'monstatparam1',\\n        'monstatparam2',\\n        'monstatparam3',\\n        'statechangetimesec',\\n        'statechangetimemsec',\\n        'tickssincelaststatechange',\\n        'stateupdatereason',\\n        'clmonowner',\\n        'clmonview',\\n        'serviceipstr',\\n        'oracleserverversion',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'cipheader',\\n        'serverid',\\n        'state',\\n        'td',\\n        'monitor_name_svc',\\n        'riseapbrstatsmsgcode',\\n        'graceful',\\n        'all',\\n        'Internal',\\n        'newname',\\n    ]\\n    transforms = {\\n        'pathmonitorindv': ['bool_yes_no'],\\n        'cacheable': ['bool_yes_no'],\\n        'cka': ['bool_yes_no'],\\n        'pathmonitor': ['bool_yes_no'],\\n        'tcpb': ['bool_yes_no'],\\n        'sp': ['bool_on_off'],\\n        'graceful': ['bool_yes_no'],\\n        'usip': ['bool_yes_no'],\\n        'healthmonitor': ['bool_yes_no'],\\n        'useproxyport': ['bool_yes_no'],\\n        'rtspsessionidremap': ['bool_on_off'],\\n        'accessdown': ['bool_yes_no'],\\n        'cmp': ['bool_yes_no'],\\n    }\\n    monitor_bindings_rw_attrs = [\\n        'servicename',\\n        'servicegroupname',\\n        'dup_state',\\n        'dup_weight',\\n        'monitorname',\\n        'weight',\\n    ]\\n    if module.params['ip'] is None:\\n        module.params['ip'] = module.params['ipaddress']\\n    service_proxy = ConfigProxy(\\n        actual=service(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.add()\\n                    sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not all_identical(client, module, service_proxy, monitor_bindings_rw_attrs):\\n                diff_dict = diff(client, module, service_proxy)\\n                immutables_changed = get_immutables_intersection(service_proxy, diff_dict.keys())\\n                if immutables_changed != []:\\n                    msg = 'Cannot update immutable attributes %s. Must delete and recreate entity.' % (immutables_changed,)\\n                    module.fail_json(msg=msg, diff=diff_dict, **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    if not module.check_mode:\\n                        service_proxy.update()\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    if not module.check_mode:\\n                        sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                module_result['changed'] = True\\n                if not module.check_mode:\\n                    if module.params['save_config']:\\n                        client.save_config()\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                res = do_state_change(client, module, service_proxy)\\n                if res.errorcode != 0:\\n                    msg = 'Error when setting disabled state. errorcode: %s message: %s' % (res.errorcode, res.message)\\n                    module.fail_json(msg=msg, **module_result)\\n            if not module.check_mode:\\n                log('Sanity checks for state present')\\n                if not service_exists(client, module):\\n                    module.fail_json(msg='Service does not exist', **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    module.fail_json(msg='Service differs from configured', diff=diff(client, module, service_proxy), **module_result)\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    module.fail_json(msg='Monitor bindings are not identical', **module_result)\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if service_exists(client, module):\\n                    module.fail_json(msg='Service still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)"
  },
  {
    "code": "def sanitize_array(data, index, dtype=None, copy=False, raise_cast_failure=False):\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n    if isinstance(data, ma.MaskedArray):\\n        mask = ma.getmaskarray(data)\\n        if mask.any():\\n            data, fill_value = maybe_upcast(data, copy=True)\\n            data.soften_mask()  \\n            data[mask] = fill_value\\n        else:\\n            data = data.copy()\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray):\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except ValueError:\\n                if copy:\\n                    subarr = data.copy()\\n                else:\\n                    subarr = np.array(data, copy=False)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n        return subarr\\n    elif isinstance(data, (list, tuple)) and len(data) > 0:\\n        if dtype is not None:\\n            try:\\n                subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n            except Exception:\\n                if raise_cast_failure:  \\n                    raise\\n                subarr = np.array(data, dtype=object, copy=copy)\\n                subarr = lib.maybe_convert_objects(subarr)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n        subarr = maybe_cast_to_datetime(subarr, dtype)\\n    elif isinstance(data, range):\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\\n        subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\\n    else:\\n        subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    if getattr(subarr, \"ndim\", 0) == 0:\\n        if isinstance(data, list):  \\n            subarr = np.array(data, dtype=object)\\n        elif index is not None:\\n            value = data\\n            if dtype is None:\\n                dtype, value = infer_dtype_from_scalar(value)\\n            else:\\n                value = maybe_cast_to_datetime(value, dtype)\\n            subarr = construct_1d_arraylike_from_scalar(value, len(index), dtype)\\n        else:\\n            return subarr.item()\\n    elif subarr.ndim == 1:\\n        if index is not None:\\n            if len(subarr) != len(index) and len(subarr) == 1:\\n                subarr = construct_1d_arraylike_from_scalar(\\n                    subarr[0], len(index), subarr.dtype\\n                )\\n    elif subarr.ndim > 1:\\n        if isinstance(data, np.ndarray):\\n            raise Exception(\"Data must be 1-dimensional\")\\n        else:\\n            subarr = com.asarray_tuplesafe(data, dtype=dtype)\\n    if not (is_extension_array_dtype(subarr.dtype) or is_extension_array_dtype(dtype)):\\n        if issubclass(subarr.dtype.type, str):\\n            if not lib.is_scalar(data):\\n                if not np.all(isna(data)):\\n                    data = np.array(data, dtype=dtype, copy=False)\\n                subarr = np.array(data, dtype=object, copy=copy)\\n        if is_object_dtype(subarr.dtype) and not is_object_dtype(dtype):\\n            inferred = lib.infer_dtype(subarr, skipna=False)\\n            if inferred == \"period\":\\n                from pandas.core.arrays import period_array\\n                try:\\n                    subarr = period_array(subarr)\\n                except IncompatibleFrequency:\\n                    pass\\n    return subarr",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sanitize_array(data, index, dtype=None, copy=False, raise_cast_failure=False):\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n    if isinstance(data, ma.MaskedArray):\\n        mask = ma.getmaskarray(data)\\n        if mask.any():\\n            data, fill_value = maybe_upcast(data, copy=True)\\n            data.soften_mask()  \\n            data[mask] = fill_value\\n        else:\\n            data = data.copy()\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray):\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except ValueError:\\n                if copy:\\n                    subarr = data.copy()\\n                else:\\n                    subarr = np.array(data, copy=False)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n        return subarr\\n    elif isinstance(data, (list, tuple)) and len(data) > 0:\\n        if dtype is not None:\\n            try:\\n                subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n            except Exception:\\n                if raise_cast_failure:  \\n                    raise\\n                subarr = np.array(data, dtype=object, copy=copy)\\n                subarr = lib.maybe_convert_objects(subarr)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n        subarr = maybe_cast_to_datetime(subarr, dtype)\\n    elif isinstance(data, range):\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\\n        subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\\n    else:\\n        subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    if getattr(subarr, \"ndim\", 0) == 0:\\n        if isinstance(data, list):  \\n            subarr = np.array(data, dtype=object)\\n        elif index is not None:\\n            value = data\\n            if dtype is None:\\n                dtype, value = infer_dtype_from_scalar(value)\\n            else:\\n                value = maybe_cast_to_datetime(value, dtype)\\n            subarr = construct_1d_arraylike_from_scalar(value, len(index), dtype)\\n        else:\\n            return subarr.item()\\n    elif subarr.ndim == 1:\\n        if index is not None:\\n            if len(subarr) != len(index) and len(subarr) == 1:\\n                subarr = construct_1d_arraylike_from_scalar(\\n                    subarr[0], len(index), subarr.dtype\\n                )\\n    elif subarr.ndim > 1:\\n        if isinstance(data, np.ndarray):\\n            raise Exception(\"Data must be 1-dimensional\")\\n        else:\\n            subarr = com.asarray_tuplesafe(data, dtype=dtype)\\n    if not (is_extension_array_dtype(subarr.dtype) or is_extension_array_dtype(dtype)):\\n        if issubclass(subarr.dtype.type, str):\\n            if not lib.is_scalar(data):\\n                if not np.all(isna(data)):\\n                    data = np.array(data, dtype=dtype, copy=False)\\n                subarr = np.array(data, dtype=object, copy=copy)\\n        if is_object_dtype(subarr.dtype) and not is_object_dtype(dtype):\\n            inferred = lib.infer_dtype(subarr, skipna=False)\\n            if inferred == \"period\":\\n                from pandas.core.arrays import period_array\\n                try:\\n                    subarr = period_array(subarr)\\n                except IncompatibleFrequency:\\n                    pass\\n    return subarr"
  },
  {
    "code": "def quote(string, safe='/', encoding=None, errors=None):\\n    if isinstance(string, str):\\n        if encoding is None:\\n            encoding = 'utf-8'\\n        if errors is None:\\n            errors = 'strict'\\n        string = string.encode(encoding, errors)\\n    else:\\n        if encoding is not None:\\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\\n        if errors is not None:\\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\\n    return quote_from_bytes(string, safe)\\ndef quote_plus(string, safe='', encoding=None, errors=None):\\n    if ((isinstance(string, str) and ' ' not in string) or\\n        (isinstance(string, bytes) and b' ' not in string)):\\n        return quote(string, safe, encoding, errors)\\n    if isinstance(safe, str):\\n        space = ' '\\n    else:\\n        space = b' '\\n    string = quote(string, safe + space, encoding, errors)\\n    return string.replace(' ', '+')\\ndef quote_from_bytes(bs, safe='/'):\\n    if isinstance(safe, str):\\n        safe = safe.encode('ascii', 'ignore')\\n    cachekey = bytes(safe)  \\n    if not (isinstance(bs, bytes) or isinstance(bs, bytearray)):\\n        raise TypeError(\"quote_from_bytes() expected a bytes\")\\n    try:\\n        quoter = _safe_quoters[cachekey]\\n    except KeyError:\\n        quoter = Quoter(safe)\\n        _safe_quoters[cachekey] = quoter\\n    return ''.join([quoter[char] for char in bs])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 76719,81270-81272,83294,83319,84038-84039 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n................\\n  r76719 | antoine.pitrou | 2009-12-08 20:38:17 +0100 (mar., 08 d\u00e9c. 2009) | 9 lines\\n\\n  Merged revisions 76718 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r76718 | antoine.pitrou | 2009-12-08 20:35:12 +0100 (mar., 08 d\u00e9c. 2009) | 3 lines\\n\\n    Fix transient refleaks in test_urllib. Thanks to Florent Xicluna.\\n  ........\\n................\\n  r81270 | florent.xicluna | 2010-05-17 19:24:07 +0200 (lun., 17 mai 2010) | 9 lines\\n\\n  Merged revision 81259 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r81259 | florent.xicluna | 2010-05-17 12:39:07 +0200 (lun, 17 mai 2010) | 2 lines\\n\\n    Slight style cleanup.\\n  ........\\n................\\n  r81271 | florent.xicluna | 2010-05-17 19:33:07 +0200 (lun., 17 mai 2010) | 11 lines\\n\\n  Issue #1285086: Speed up urllib.parse functions: quote, quote_from_bytes, unquote, unquote_to_bytes.\\n\\n  Recorded merge of revisions 81265 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r81265 | florent.xicluna | 2010-05-17 15:35:09 +0200 (lun, 17 mai 2010) | 2 lines\\n\\n    Issue #1285086: Speed up urllib.quote and urllib.unquote for simple cases.\\n  ........\\n................\\n  r81272 | florent.xicluna | 2010-05-17 20:01:22 +0200 (lun., 17 mai 2010) | 2 lines\\n\\n  Inadvertently removed part of the comment in r81271.\\n................\\n  r83294 | senthil.kumaran | 2010-07-30 21:34:36 +0200 (ven., 30 juil. 2010) | 2 lines\\n\\n  Fix issue9301 - handle unquote({}) kind of case.\\n................\\n  r83319 | florent.xicluna | 2010-07-31 10:56:55 +0200 (sam., 31 juil. 2010) | 2 lines\\n\\n  Fix an oversight in r83294.  unquote() should reject bytes.  Issue #9301.\\n................\\n  r84038 | florent.xicluna | 2010-08-14 20:30:35 +0200 (sam., 14 ao\u00fbt 2010) | 1 line\\n\\n  Silence the BytesWarning, due to patch r83294 for #9301\\n................\\n  r84039 | florent.xicluna | 2010-08-14 22:51:58 +0200 (sam., 14 ao\u00fbt 2010) | 1 line\\n\\n  Silence BytesWarning while testing exception\\n................",
    "fixed_code": "def quote(string, safe='/', encoding=None, errors=None):\\n    if isinstance(string, str):\\n        if not string:\\n            return string\\n        if encoding is None:\\n            encoding = 'utf-8'\\n        if errors is None:\\n            errors = 'strict'\\n        string = string.encode(encoding, errors)\\n    else:\\n        if encoding is not None:\\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\\n        if errors is not None:\\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\\n    return quote_from_bytes(string, safe)\\ndef quote_plus(string, safe='', encoding=None, errors=None):\\n    if ((isinstance(string, str) and ' ' not in string) or\\n        (isinstance(string, bytes) and b' ' not in string)):\\n        return quote(string, safe, encoding, errors)\\n    if isinstance(safe, str):\\n        space = ' '\\n    else:\\n        space = b' '\\n    string = quote(string, safe + space, encoding, errors)\\n    return string.replace(' ', '+')\\ndef quote_from_bytes(bs, safe='/'):\\n    if not isinstance(bs, (bytes, bytearray)):\\n        raise TypeError(\"quote_from_bytes() expected bytes\")\\n    if not bs:\\n        return ''\\n    if isinstance(safe, str):\\n        safe = safe.encode('ascii', 'ignore')\\n    else:\\n        safe = bytes([c for c in safe if c < 128])\\n    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\\n        return bs.decode()\\n    try:\\n        quoter = _safe_quoters[safe]\\n    except KeyError:\\n        _safe_quoters[safe] = quoter = Quoter(safe).__getitem__\\n    return ''.join([quoter(char) for char in bs])"
  },
  {
    "code": "def _get_combined_index(\\n\\tindexes: List[Index],\\n\\tintersect: bool = False,\\n\\tsort: bool = False,\\n\\tcopy: bool = False,\\n) -> Index:\\n\\tindexes = _get_distinct_objs(indexes)\\n\\tif len(indexes) == 0:\\n\\t\\tindex = Index([])\\n\\telif len(indexes) == 1:\\n\\t\\tindex = indexes[0]\\n\\telif intersect:\\n\\t\\tindex = indexes[0]\\n\\t\\tfor other in indexes[1:]:\\n\\t\\t\\tindex = index.intersection(other)\\n\\telse:\\n\\t\\tindex = union_indexes(indexes, sort=sort)\\n\\t\\tindex = ensure_index(index)\\n\\tif sort:\\n\\t\\ttry:\\n\\t\\t\\tindex = index.sort_values()\\n\\t\\texcept TypeError:\\n\\t\\t\\tpass\\n\\tif copy:\\n\\t\\tindex = index.copy()\\n\\treturn index",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_combined_index(\\n\\tindexes: List[Index],\\n\\tintersect: bool = False,\\n\\tsort: bool = False,\\n\\tcopy: bool = False,\\n) -> Index:\\n\\tindexes = _get_distinct_objs(indexes)\\n\\tif len(indexes) == 0:\\n\\t\\tindex = Index([])\\n\\telif len(indexes) == 1:\\n\\t\\tindex = indexes[0]\\n\\telif intersect:\\n\\t\\tindex = indexes[0]\\n\\t\\tfor other in indexes[1:]:\\n\\t\\t\\tindex = index.intersection(other)\\n\\telse:\\n\\t\\tindex = union_indexes(indexes, sort=sort)\\n\\t\\tindex = ensure_index(index)\\n\\tif sort:\\n\\t\\ttry:\\n\\t\\t\\tindex = index.sort_values()\\n\\t\\texcept TypeError:\\n\\t\\t\\tpass\\n\\tif copy:\\n\\t\\tindex = index.copy()\\n\\treturn index"
  },
  {
    "code": "def where(\\n        self, other, cond, errors=\"raise\", try_cast: bool = False, axis: int = 0\\n    ) -> List[\"Block\"]:\\n        import pandas.core.computation.expressions as expressions\\n        cond = _extract_bool_array(cond)\\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\\n        assert errors in [\"raise\", \"ignore\"]\\n        transpose = self.ndim == 2\\n        values = self.values\\n        orig_other = other\\n        if transpose:\\n            values = values.T\\n        if getattr(other, \"ndim\", 0) >= 1:\\n            if values.ndim - 1 == other.ndim and axis == 1:\\n                other = other.reshape(tuple(other.shape + (1,)))\\n            elif transpose and values.ndim == self.ndim - 1:\\n                cond = cond.T\\n        if not hasattr(cond, \"shape\"):\\n            raise ValueError(\"where must have a condition that is ndarray like\")\\n        if cond.ravel(\"K\").all():\\n            result = values\\n        else:\\n            if (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                pass\\n            elif not self._can_hold_element(other):\\n                block = self.coerce_to_target_dtype(other)\\n                blocks = block.where(\\n                    orig_other, cond, errors=errors, try_cast=try_cast, axis=axis\\n                )\\n                return self._maybe_downcast(blocks, \"infer\")\\n            if not (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                other = convert_scalar_for_putitemlike(other, values.dtype)\\n            result = expressions.where(cond, values, other)\\n        if self._can_hold_na or self.ndim == 1:\\n            if transpose:\\n                result = result.T\\n            return [self.make_block(result)]\\n        axis = cond.ndim - 1\\n        cond = cond.swapaxes(axis, 0)\\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])], dtype=bool)\\n        result_blocks: List[\"Block\"] = []\\n        for m in [mask, ~mask]:\\n            if m.any():\\n                result = cast(np.ndarray, result)  \\n                taken = result.take(m.nonzero()[0], axis=axis)\\n                r = maybe_downcast_numeric(taken, self.dtype)\\n                nb = self.make_block(r.T, placement=self.mgr_locs[m])\\n                result_blocks.append(nb)\\n        return result_blocks",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Series.where casting dt64 to int64 (#38073)",
    "fixed_code": "def where(\\n        self, other, cond, errors=\"raise\", try_cast: bool = False, axis: int = 0\\n    ) -> List[\"Block\"]:\\n        import pandas.core.computation.expressions as expressions\\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\\n        assert errors in [\"raise\", \"ignore\"]\\n        transpose = self.ndim == 2\\n        values = self.values\\n        orig_other = other\\n        if transpose:\\n            values = values.T\\n        other, cond = self._maybe_reshape_where_args(values, other, cond, axis)\\n        if cond.ravel(\"K\").all():\\n            result = values\\n        else:\\n            if (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                pass\\n            elif not self._can_hold_element(other):\\n                block = self.coerce_to_target_dtype(other)\\n                blocks = block.where(\\n                    orig_other, cond, errors=errors, try_cast=try_cast, axis=axis\\n                )\\n                return self._maybe_downcast(blocks, \"infer\")\\n            if not (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                other = convert_scalar_for_putitemlike(other, values.dtype)\\n            result = expressions.where(cond, values, other)\\n        if self._can_hold_na or self.ndim == 1:\\n            if transpose:\\n                result = result.T\\n            return [self.make_block(result)]\\n        axis = cond.ndim - 1\\n        cond = cond.swapaxes(axis, 0)\\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])], dtype=bool)\\n        result_blocks: List[\"Block\"] = []\\n        for m in [mask, ~mask]:\\n            if m.any():\\n                result = cast(np.ndarray, result)  \\n                taken = result.take(m.nonzero()[0], axis=axis)\\n                r = maybe_downcast_numeric(taken, self.dtype)\\n                nb = self.make_block(r.T, placement=self.mgr_locs[m])\\n                result_blocks.append(nb)\\n        return result_blocks"
  },
  {
    "code": "def _clear_item_cache(self):\\n        self._item_cache.clear()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fixed a bug in DataFrame/Panel cache insertion and subsequent indexing GH4939",
    "fixed_code": "def _clear_item_cache(self, i=None):\\n        if i is not None:\\n            self._item_cache.pop(i,None)\\n        else:\\n            self._item_cache.clear()"
  },
  {
    "code": "def _sequence_to_dt64ns(\\n    data,\\n    *,\\n    copy: bool = False,\\n    tz: tzinfo | None = None,\\n    dayfirst: bool = False,\\n    yearfirst: bool = False,\\n    ambiguous: TimeAmbiguous = \"raise\",\\n    allow_mixed: bool = False,\\n    require_iso8601: bool = False,\\n):\\n    inferred_freq = None\\n    data, copy = dtl.ensure_arraylike_for_datetimelike(\\n        data, copy, cls_name=\"DatetimeArray\"\\n    )\\n    if isinstance(data, DatetimeArray):\\n        inferred_freq = data.freq\\n    data, copy = maybe_convert_dtype(data, copy, tz=tz)\\n    data_dtype = getattr(data, \"dtype\", None)\\n    if (\\n        is_object_dtype(data_dtype)\\n        or is_string_dtype(data_dtype)\\n        or is_sparse(data_dtype)\\n    ):\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\\n            data = data.astype(np.int64)\\n        else:\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data,\\n                dayfirst=dayfirst,\\n                yearfirst=yearfirst,\\n                allow_object=False,\\n                allow_mixed=allow_mixed,\\n                require_iso8601=require_iso8601,\\n            )\\n            if tz and inferred_tz:\\n                if data.dtype == \"i8\":\\n                    return data.view(DT64NS_DTYPE), tz, None\\n                if timezones.is_utc(tz):\\n                    utc_vals = data.view(\"i8\")\\n                else:\\n                    utc_vals = tz_convert_from_utc(data.view(\"i8\"), tz)\\n                data = utc_vals.view(DT64NS_DTYPE)\\n            elif inferred_tz:\\n                tz = inferred_tz\\n        data_dtype = data.dtype\\n    if is_datetime64tz_dtype(data_dtype):\\n        tz = _maybe_infer_tz(tz, data.tz)\\n        result = data._ndarray\\n    elif is_datetime64_dtype(data_dtype):\\n        data = getattr(data, \"_ndarray\", data)\\n        if data.dtype != DT64NS_DTYPE:\\n            data = astype_overflowsafe(data, dtype=DT64NS_DTYPE)\\n            copy = False\\n        if tz is not None:\\n            data = tzconversion.tz_localize_to_utc(\\n                data.view(\"i8\"), tz, ambiguous=ambiguous\\n            )\\n            data = data.view(DT64NS_DTYPE)\\n        assert data.dtype == DT64NS_DTYPE, data.dtype\\n        result = data\\n    else:\\n        if data.dtype != INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        result = data.view(DT64NS_DTYPE)\\n    if copy:\\n        result = result.copy()\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \"M8[ns]\", result.dtype\\n    return result, tz, inferred_freq",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _sequence_to_dt64ns(\\n    data,\\n    *,\\n    copy: bool = False,\\n    tz: tzinfo | None = None,\\n    dayfirst: bool = False,\\n    yearfirst: bool = False,\\n    ambiguous: TimeAmbiguous = \"raise\",\\n    allow_mixed: bool = False,\\n    require_iso8601: bool = False,\\n):\\n    inferred_freq = None\\n    data, copy = dtl.ensure_arraylike_for_datetimelike(\\n        data, copy, cls_name=\"DatetimeArray\"\\n    )\\n    if isinstance(data, DatetimeArray):\\n        inferred_freq = data.freq\\n    data, copy = maybe_convert_dtype(data, copy, tz=tz)\\n    data_dtype = getattr(data, \"dtype\", None)\\n    if (\\n        is_object_dtype(data_dtype)\\n        or is_string_dtype(data_dtype)\\n        or is_sparse(data_dtype)\\n    ):\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\\n            data = data.astype(np.int64)\\n        else:\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data,\\n                dayfirst=dayfirst,\\n                yearfirst=yearfirst,\\n                allow_object=False,\\n                allow_mixed=allow_mixed,\\n                require_iso8601=require_iso8601,\\n            )\\n            if tz and inferred_tz:\\n                if data.dtype == \"i8\":\\n                    return data.view(DT64NS_DTYPE), tz, None\\n                if timezones.is_utc(tz):\\n                    utc_vals = data.view(\"i8\")\\n                else:\\n                    utc_vals = tz_convert_from_utc(data.view(\"i8\"), tz)\\n                data = utc_vals.view(DT64NS_DTYPE)\\n            elif inferred_tz:\\n                tz = inferred_tz\\n        data_dtype = data.dtype\\n    if is_datetime64tz_dtype(data_dtype):\\n        tz = _maybe_infer_tz(tz, data.tz)\\n        result = data._ndarray\\n    elif is_datetime64_dtype(data_dtype):\\n        data = getattr(data, \"_ndarray\", data)\\n        if data.dtype != DT64NS_DTYPE:\\n            data = astype_overflowsafe(data, dtype=DT64NS_DTYPE)\\n            copy = False\\n        if tz is not None:\\n            data = tzconversion.tz_localize_to_utc(\\n                data.view(\"i8\"), tz, ambiguous=ambiguous\\n            )\\n            data = data.view(DT64NS_DTYPE)\\n        assert data.dtype == DT64NS_DTYPE, data.dtype\\n        result = data\\n    else:\\n        if data.dtype != INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        result = data.view(DT64NS_DTYPE)\\n    if copy:\\n        result = result.copy()\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \"M8[ns]\", result.dtype\\n    return result, tz, inferred_freq"
  },
  {
    "code": "def safe_substitute(self, *args, **kws):\\n        if len(args) > 1:\\n            raise TypeError('Too many positional arguments')\\n        if not args:\\n            mapping = kws\\n        elif kws:\\n            mapping = _multimap(kws, args[0])\\n        else:\\n            mapping = args[0]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix small bugs in Template code.\\n\\n  when braced is not None.\\n  convert code would fall off the end and improperly return None.\\n\\nBeefed-up tests.\\n\\n  tests as they are run).",
    "fixed_code": "def safe_substitute(self, *args, **kws):\\n        if len(args) > 1:\\n            raise TypeError('Too many positional arguments')\\n        if not args:\\n            mapping = kws\\n        elif kws:\\n            mapping = _multimap(kws, args[0])\\n        else:\\n            mapping = args[0]\\n        delimiter = self.delimiter[-1]"
  },
  {
    "code": "def main(args=None):\\n    import argparse\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--output', '-o', default=None,\\n            help=\"The name of the output archive. \"\\n                 \"Required if SOURCE is an archive.\")\\n    parser.add_argument('--python', '-p', default=None,\\n            help=\"The name of the Python interpreter to use \"\\n                 \"(default: no shebang line).\")\\n    parser.add_argument('--main', '-m', default=None,\\n            help=\"The main function of the application \"\\n                 \"(default: use an existing __main__.py).\")\\n    parser.add_argument('--compress', '-c', action='store_true',\\n            help=\"Compress files with the deflate method. \"\\n                 \"Files are stored uncompressed by default.\")\\n    parser.add_argument('--info', default=False, action='store_true',\\n            help=\"Display the interpreter from the archive.\")\\n    parser.add_argument('source',\\n            help=\"Source directory (or existing archive).\")\\n    args = parser.parse_args(args)\\n    if args.info:\\n        if not os.path.isfile(args.source):\\n            raise SystemExit(\"Can only get info for an archive file\")\\n        interpreter = get_interpreter(args.source)\\n        print(\"Interpreter: {}\".format(interpreter or \"<none>\"))\\n        sys.exit(0)\\n    if os.path.isfile(args.source):\\n        if args.output is None or (os.path.exists(args.output) and\\n                                   os.path.samefile(args.source, args.output)):\\n            raise SystemExit(\"In-place editing of archives is not supported\")\\n        if args.main:\\n            raise SystemExit(\"Cannot change the main function when copying\")\\n    create_archive(args.source, args.output,\\n                   interpreter=args.python, main=args.main,\\n                   compressed=args.compress)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main(args=None):\\n    import argparse\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--output', '-o', default=None,\\n            help=\"The name of the output archive. \"\\n                 \"Required if SOURCE is an archive.\")\\n    parser.add_argument('--python', '-p', default=None,\\n            help=\"The name of the Python interpreter to use \"\\n                 \"(default: no shebang line).\")\\n    parser.add_argument('--main', '-m', default=None,\\n            help=\"The main function of the application \"\\n                 \"(default: use an existing __main__.py).\")\\n    parser.add_argument('--compress', '-c', action='store_true',\\n            help=\"Compress files with the deflate method. \"\\n                 \"Files are stored uncompressed by default.\")\\n    parser.add_argument('--info', default=False, action='store_true',\\n            help=\"Display the interpreter from the archive.\")\\n    parser.add_argument('source',\\n            help=\"Source directory (or existing archive).\")\\n    args = parser.parse_args(args)\\n    if args.info:\\n        if not os.path.isfile(args.source):\\n            raise SystemExit(\"Can only get info for an archive file\")\\n        interpreter = get_interpreter(args.source)\\n        print(\"Interpreter: {}\".format(interpreter or \"<none>\"))\\n        sys.exit(0)\\n    if os.path.isfile(args.source):\\n        if args.output is None or (os.path.exists(args.output) and\\n                                   os.path.samefile(args.source, args.output)):\\n            raise SystemExit(\"In-place editing of archives is not supported\")\\n        if args.main:\\n            raise SystemExit(\"Cannot change the main function when copying\")\\n    create_archive(args.source, args.output,\\n                   interpreter=args.python, main=args.main,\\n                   compressed=args.compress)"
  },
  {
    "code": "def _get_blkno_placements(blknos, blk_count, group=True):\\n    blknos = _ensure_int64(blknos)\\n    for blkno, indexer in lib.get_blkno_indexers(blknos, group):\\n        yield blkno, BlockPlacement(indexer)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Implement libinternals\\n\\nAuthor: Brock Mendel <jbrockmendel@gmail.com>\\n\\nCloses #19293 from jbrockmendel/libinternals and squashes the following commits:\\n\\nf0146b20c [Brock Mendel] fix missing cimport\\n70514ce1b [Brock Mendel] revert edits to be pure cut/paste\\n0a94ccac8 [Brock Mendel] Cleanup leftover cimport, use c-api funcs for isinstance\\n3fb0d0a73 [Brock Mendel] implement libinternals",
    "fixed_code": "def _get_blkno_placements(blknos, blk_count, group=True):\\n    blknos = _ensure_int64(blknos)\\n    for blkno, indexer in libinternals.get_blkno_indexers(blknos, group):\\n        yield blkno, BlockPlacement(indexer)"
  },
  {
    "code": "def write(self, s):\\n\\t\\twith open(self.path, 'w' if self.overwrite else 'a') as output_file:\\n\\t\\t\\toutput_file.write(s)\\n\\t\\tself.overwrite = False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix unicode issues and add test, fix #124",
    "fixed_code": "def write(self, s):\\n\\t\\twith open(self.path, 'w' if self.overwrite else 'a',\\n\\t\\t\\t\\t  encoding='utf-8') as output_file:\\n\\t\\t\\toutput_file.write(s)\\n\\t\\tself.overwrite = False"
  },
  {
    "code": "def as_pod(self) -> k8s.V1Pod:\\n        if self.kube_config.pod_template_file:\\n            return PodGenerator(pod_template_file=self.kube_config.pod_template_file).gen_pod()\\n        pod = PodGenerator(\\n            image=self.kube_config.kube_image,\\n            image_pull_policy=self.kube_config.kube_image_pull_policy or 'IfNotPresent',\\n            image_pull_secrets=self.kube_config.image_pull_secrets,\\n            volumes=self._get_volumes(),\\n            volume_mounts=self._get_volume_mounts(),\\n            init_containers=self._get_init_containers(),\\n            annotations=self.kube_config.kube_annotations,\\n            affinity=self.kube_config.kube_affinity,\\n            tolerations=self.kube_config.kube_tolerations,\\n            envs=self._get_environment(),\\n            node_selectors=self.kube_config.kube_node_selectors,\\n            service_account_name=self.kube_config.worker_service_account_name or 'default',\\n            restart_policy='Never',\\n            resources=self._get_resources(),\\n        ).gen_pod()\\n        pod.spec.containers[0].env_from = pod.spec.containers[0].env_from or []\\n        pod.spec.containers[0].env_from.extend(self._get_env_from())\\n        pod.spec.security_context = self._get_security_context()\\n        return append_to_pod(pod, self._get_secrets())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_pod(self) -> k8s.V1Pod:\\n        if self.kube_config.pod_template_file:\\n            return PodGenerator(pod_template_file=self.kube_config.pod_template_file).gen_pod()\\n        pod = PodGenerator(\\n            image=self.kube_config.kube_image,\\n            image_pull_policy=self.kube_config.kube_image_pull_policy or 'IfNotPresent',\\n            image_pull_secrets=self.kube_config.image_pull_secrets,\\n            volumes=self._get_volumes(),\\n            volume_mounts=self._get_volume_mounts(),\\n            init_containers=self._get_init_containers(),\\n            annotations=self.kube_config.kube_annotations,\\n            affinity=self.kube_config.kube_affinity,\\n            tolerations=self.kube_config.kube_tolerations,\\n            envs=self._get_environment(),\\n            node_selectors=self.kube_config.kube_node_selectors,\\n            service_account_name=self.kube_config.worker_service_account_name or 'default',\\n            restart_policy='Never',\\n            resources=self._get_resources(),\\n        ).gen_pod()\\n        pod.spec.containers[0].env_from = pod.spec.containers[0].env_from or []\\n        pod.spec.containers[0].env_from.extend(self._get_env_from())\\n        pod.spec.security_context = self._get_security_context()\\n        return append_to_pod(pod, self._get_secrets())"
  },
  {
    "code": "def __init__(self, input='content', charset='utf-8',\\n                 charset_error='strict', strip_accents='ascii',\\n                 strip_tags=False, lowercase=True, tokenize='word',\\n                 stop_words=None, token_pattern=ur\"\\b\\w\\w+\\b\",\\n                 min_n=1, max_n=1, max_df=1.0, max_features=None,\\n                 fixed_vocabulary=None, binary=False, dtype=long):\\n        self.input = input\\n        self.charset = charset\\n        self.charset_error = charset_error\\n        self.strip_accents = strip_accents\\n        self.strip_tags = strip_tags\\n        self.lowercase = lowercase\\n        self.min_n = min_n\\n        self.max_n = max_n\\n        self.tokenize = tokenize\\n        self.token_pattern = token_pattern\\n        self.stop_words = stop_words\\n        self.max_df = max_df\\n        self.max_features = max_features\\n        if (fixed_vocabulary is not None\\n            and not hasattr(fixed_vocabulary, 'get')):\\n            fixed_vocabulary = dict(\\n                (t, i) for i, t in enumerate(fixed_vocabulary))\\n        self.fixed_vocabulary = fixed_vocabulary\\n        self.binary = binary\\n        self.dtype = dtype",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, input='content', charset='utf-8',\\n                 charset_error='strict', strip_accents='ascii',\\n                 strip_tags=False, lowercase=True, tokenize='word',\\n                 stop_words=None, token_pattern=ur\"\\b\\w\\w+\\b\",\\n                 min_n=1, max_n=1, max_df=1.0, max_features=None,\\n                 fixed_vocabulary=None, binary=False, dtype=long):\\n        self.input = input\\n        self.charset = charset\\n        self.charset_error = charset_error\\n        self.strip_accents = strip_accents\\n        self.strip_tags = strip_tags\\n        self.lowercase = lowercase\\n        self.min_n = min_n\\n        self.max_n = max_n\\n        self.tokenize = tokenize\\n        self.token_pattern = token_pattern\\n        self.stop_words = stop_words\\n        self.max_df = max_df\\n        self.max_features = max_features\\n        if (fixed_vocabulary is not None\\n            and not hasattr(fixed_vocabulary, 'get')):\\n            fixed_vocabulary = dict(\\n                (t, i) for i, t in enumerate(fixed_vocabulary))\\n        self.fixed_vocabulary = fixed_vocabulary\\n        self.binary = binary\\n        self.dtype = dtype"
  },
  {
    "code": "def train(hyp, opt, device, tb_writer=None, wandb=None):\\n    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\\n    save_dir, epochs, batch_size, total_batch_size, weights, rank = \\\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank\\n    wdir = save_dir / 'weights'\\n    wdir.mkdir(parents=True, exist_ok=True)  \\n    last = wdir / 'last.pt'\\n    best = wdir / 'best.pt'\\n    results_file = save_dir / 'results.txt'\\n    with open(save_dir / 'hyp.yaml', 'w') as f:\\n        yaml.dump(hyp, f, sort_keys=False)\\n    with open(save_dir / 'opt.yaml', 'w') as f:\\n        yaml.dump(vars(opt), f, sort_keys=False)\\n    plots = not opt.evolve  \\n    cuda = device.type != 'cpu'\\n    init_seeds(2 + rank)\\n    with open(opt.data) as f:\\n        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  \\n    with torch_distributed_zero_first(rank):\\n        check_dataset(data_dict)  \\n    train_path = data_dict['train']\\n    test_path = data_dict['val']\\n    nc = 1 if opt.single_cls else int(data_dict['nc'])  \\n    names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  \\n    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  \\n    pretrained = weights.endswith('.pt')\\n    if pretrained:\\n        with torch_distributed_zero_first(rank):\\n            attempt_download(weights)  \\n        ckpt = torch.load(weights, map_location=device)  \\n        if hyp.get('anchors'):\\n            ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  \\n        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  \\n        exclude = ['anchor'] if opt.cfg or hyp.get('anchors') else []  \\n        state_dict = ckpt['model'].float().state_dict()  \\n        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  \\n        model.load_state_dict(state_dict, strict=False)  \\n        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  \\n    else:\\n        model = Model(opt.cfg, ch=3, nc=nc).to(device)  \\n    freeze = []  \\n    for k, v in model.named_parameters():\\n        v.requires_grad = True  \\n        if any(x in k for x in freeze):\\n            print('freezing %s' % k)\\n            v.requires_grad = False\\n    nbs = 64  \\n    accumulate = max(round(nbs / total_batch_size), 1)  \\n    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  \\n    logger.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\\n    pg0, pg1, pg2 = [], [], []  \\n    for k, v in model.named_modules():\\n        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\\n            pg2.append(v.bias)  \\n        if isinstance(v, nn.BatchNorm2d):\\n            pg0.append(v.weight)  \\n        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\\n            pg1.append(v.weight)  \\n    if opt.adam:\\n        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  \\n    else:\\n        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\\n    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  \\n    optimizer.add_param_group({'params': pg2})  \\n    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\\n    del pg0, pg1, pg2\\n    if opt.linear_lr:\\n        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  \\n    else:\\n        lf = one_cycle(1, hyp['lrf'], epochs)  \\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\\n    if rank in [-1, 0] and wandb and wandb.run is None:\\n        opt.hyp = hyp  \\n        wandb_run = wandb.init(config=opt, resume=\"allow\",\\n                               project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\\n                               name=save_dir.stem,\\n                               id=ckpt.get('wandb_id') if 'ckpt' in locals() else None)\\n    loggers = {'wandb': wandb}  \\n    start_epoch, best_fitness = 0, 0.0\\n    if pretrained:\\n        if ckpt['optimizer'] is not None:\\n            optimizer.load_state_dict(ckpt['optimizer'])\\n            best_fitness = ckpt['best_fitness']\\n        if ckpt.get('training_results') is not None:\\n            with open(results_file, 'w') as file:\\n                file.write(ckpt['training_results'])  \\n        start_epoch = ckpt['epoch'] + 1\\n        if opt.resume:\\n            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)\\n        if epochs < start_epoch:\\n            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\\n                        (weights, ckpt['epoch'], epochs))\\n            epochs += ckpt['epoch']  \\n        del ckpt, state_dict\\n    gs = int(model.stride.max())  \\n    nl = model.model[-1].nl  \\n    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  \\n    if cuda and rank == -1 and torch.cuda.device_count() > 1:\\n        model = torch.nn.DataParallel(model)\\n    if opt.sync_bn and cuda and rank != -1:\\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\\n        logger.info('Using SyncBatchNorm()')\\n    ema = ModelEMA(model) if rank in [-1, 0] else None\\n    if cuda and rank != -1:\\n        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)\\n    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\\n                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,\\n                                            world_size=opt.world_size, workers=opt.workers,\\n                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))\\n    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  \\n    nb = len(dataloader)  \\n    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)\\n    if rank in [-1, 0]:\\n        ema.updates = start_epoch * nb // accumulate  \\n        testloader = create_dataloader(test_path, imgsz_test, batch_size * 2, gs, opt,  \\n                                       hyp=hyp, cache=opt.cache_images and not opt.notest, rect=True, rank=-1,\\n                                       world_size=opt.world_size, workers=opt.workers,\\n                                       pad=0.5, prefix=colorstr('val: '))[0]\\n        if not opt.resume:\\n            labels = np.concatenate(dataset.labels, 0)\\n            c = torch.tensor(labels[:, 0])  \\n            if plots:\\n                plot_labels(labels, save_dir, loggers)\\n                if tb_writer:\\n                    tb_writer.add_histogram('classes', c, 0)\\n            if not opt.noautoanchor:\\n                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\\n    hyp['box'] *= 3. / nl  \\n    hyp['cls'] *= nc / 80. * 3. / nl  \\n    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  \\n    model.nc = nc  \\n    model.hyp = hyp  \\n    model.gr = 1.0  \\n    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  \\n    model.names = names\\n    t0 = time.time()\\n    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  \\n    maps = np.zeros(nc)  \\n    results = (0, 0, 0, 0, 0, 0, 0)  \\n    scheduler.last_epoch = start_epoch - 1  \\n    scaler = amp.GradScaler(enabled=cuda)\\n    compute_loss = ComputeLoss(model)  \\n    logger.info(f'Image sizes {imgsz} train, {imgsz_test} test\\n'\\n                f'Using {dataloader.num_workers} dataloader workers\\n'\\n                f'Logging results to {save_dir}\\n'\\n                f'Starting training for {epochs} epochs...')\\n    for epoch in range(start_epoch, epochs):  \\n        model.train()\\n        if opt.image_weights:\\n            if rank in [-1, 0]:\\n                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  \\n                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  \\n                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  \\n            if rank != -1:\\n                indices = (torch.tensor(dataset.indices) if rank == 0 else torch.zeros(dataset.n)).int()\\n                dist.broadcast(indices, 0)\\n                if rank != 0:\\n                    dataset.indices = indices.cpu().numpy()\\n        mloss = torch.zeros(4, device=device)  \\n        if rank != -1:\\n            dataloader.sampler.set_epoch(epoch)\\n        pbar = enumerate(dataloader)\\n        logger.info(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'targets', 'img_size'))\\n        if rank in [-1, 0]:\\n            pbar = tqdm(pbar, total=nb)  \\n        optimizer.zero_grad()\\n        for i, (imgs, targets, paths, _) in pbar:  \\n            ni = i + nb * epoch  \\n            imgs = imgs.to(device, non_blocking=True).float() / 255.0  \\n            if ni <= nw:\\n                xi = [0, nw]  \\n                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\\n                for j, x in enumerate(optimizer.param_groups):\\n                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\\n                    if 'momentum' in x:\\n                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\\n            if opt.multi_scale:\\n                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  \\n                sf = sz / max(imgs.shape[2:])  \\n                if sf != 1:\\n                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  \\n                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\\n            with amp.autocast(enabled=cuda):\\n                pred = model(imgs)  \\n                loss, loss_items = compute_loss(pred, targets.to(device))  \\n                if rank != -1:\\n                    loss *= opt.world_size  \\n                if opt.quad:\\n                    loss *= 4.\\n            scaler.scale(loss).backward()\\n            if ni % accumulate == 0:\\n                scaler.step(optimizer)  \\n                scaler.update()\\n                optimizer.zero_grad()\\n                if ema:\\n                    ema.update(model)\\n            if rank in [-1, 0]:\\n                mloss = (mloss * i + loss_items) / (i + 1)  \\n                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  \\n                s = ('%10s' * 2 + '%10.4g' * 6) % (\\n                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\\n                pbar.set_description(s)\\n                if plots and ni < 3:\\n                    f = save_dir / f'train_batch{ni}.jpg'  \\n                    Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()\\n                elif plots and ni == 10 and wandb:\\n                    wandb.log({\"Mosaics\": [wandb.Image(str(x), caption=x.name) for x in save_dir.glob('train*.jpg')\\n                                           if x.exists()]}, commit=False)\\n        lr = [x['lr'] for x in optimizer.param_groups]  \\n        scheduler.step()\\n        if rank in [-1, 0]:\\n            if ema:\\n                ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])\\n            final_epoch = epoch + 1 == epochs\\n            if not opt.notest or final_epoch:  \\n                results, maps, times = test.test(opt.data,\\n                                                 batch_size=batch_size * 2,\\n                                                 imgsz=imgsz_test,\\n                                                 model=ema.ema,\\n                                                 single_cls=opt.single_cls,\\n                                                 dataloader=testloader,\\n                                                 save_dir=save_dir,\\n                                                 verbose=nc < 50 and final_epoch,\\n                                                 plots=plots and final_epoch,\\n                                                 log_imgs=opt.log_imgs if wandb else 0,\\n                                                 compute_loss=compute_loss)\\n            with open(results_file, 'a') as f:\\n                f.write(s + '%10.4g' * 7 % results + '\\n')  \\n            if len(opt.name) and opt.bucket:\\n                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\\n            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  \\n                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\\n                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  \\n                    'x/lr0', 'x/lr1', 'x/lr2']  \\n            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\\n                if tb_writer:\\n                    tb_writer.add_scalar(tag, x, epoch)  \\n                if wandb:\\n                    wandb.log({tag: x}, step=epoch, commit=tag == tags[-1])  \\n            fi = fitness(np.array(results).reshape(1, -1))  \\n            if fi > best_fitness:\\n                best_fitness = fi\\n            save = (not opt.nosave) or (final_epoch and not opt.evolve)\\n            if save:\\n                with open(results_file, 'r') as f:  \\n                    ckpt = {'epoch': epoch,\\n                            'best_fitness': best_fitness,\\n                            'training_results': f.read(),\\n                            'model': ema.ema,\\n                            'optimizer': None if final_epoch else optimizer.state_dict(),\\n                            'wandb_id': wandb_run.id if wandb else None}\\n                torch.save(ckpt, last)\\n                if best_fitness == fi:\\n                    torch.save(ckpt, best)\\n                del ckpt\\n    if rank in [-1, 0]:\\n        final = best if best.exists() else last  \\n        for f in [last, best]:\\n            if f.exists():\\n                strip_optimizer(f)  \\n        if opt.bucket:\\n            os.system(f'gsutil cp {final} gs://{opt.bucket}/weights')  \\n        if plots:\\n            plot_results(save_dir=save_dir)  \\n            if wandb:\\n                files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]\\n                wandb.log({\"Results\": [wandb.Image(str(save_dir / f), caption=f) for f in files\\n                                       if (save_dir / f).exists()]})\\n                if opt.log_artifacts:\\n                    wandb.log_artifact(artifact_or_path=str(final), type='model', name=save_dir.stem)\\n        logger.info('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\\n        if opt.data.endswith('coco.yaml') and nc == 80:  \\n            for conf, iou, save_json in ([0.25, 0.45, False], [0.001, 0.65, True]):  \\n                results, _, _ = test.test(opt.data,\\n                                          batch_size=batch_size * 2,\\n                                          imgsz=imgsz_test,\\n                                          conf_thres=conf,\\n                                          iou_thres=iou,\\n                                          model=attempt_load(final, device).half(),\\n                                          single_cls=opt.single_cls,\\n                                          dataloader=testloader,\\n                                          save_dir=save_dir,\\n                                          save_json=save_json,\\n                                          plots=False)\\n    else:\\n        dist.destroy_process_group()\\n    wandb.run.finish() if wandb and wandb.run else None\\n    torch.cuda.empty_cache()\\n    return results",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Update minimum stride to 32 (#2266)",
    "fixed_code": "def train(hyp, opt, device, tb_writer=None, wandb=None):\\n    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\\n    save_dir, epochs, batch_size, total_batch_size, weights, rank = \\\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank\\n    wdir = save_dir / 'weights'\\n    wdir.mkdir(parents=True, exist_ok=True)  \\n    last = wdir / 'last.pt'\\n    best = wdir / 'best.pt'\\n    results_file = save_dir / 'results.txt'\\n    with open(save_dir / 'hyp.yaml', 'w') as f:\\n        yaml.dump(hyp, f, sort_keys=False)\\n    with open(save_dir / 'opt.yaml', 'w') as f:\\n        yaml.dump(vars(opt), f, sort_keys=False)\\n    plots = not opt.evolve  \\n    cuda = device.type != 'cpu'\\n    init_seeds(2 + rank)\\n    with open(opt.data) as f:\\n        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  \\n    with torch_distributed_zero_first(rank):\\n        check_dataset(data_dict)  \\n    train_path = data_dict['train']\\n    test_path = data_dict['val']\\n    nc = 1 if opt.single_cls else int(data_dict['nc'])  \\n    names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  \\n    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  \\n    pretrained = weights.endswith('.pt')\\n    if pretrained:\\n        with torch_distributed_zero_first(rank):\\n            attempt_download(weights)  \\n        ckpt = torch.load(weights, map_location=device)  \\n        if hyp.get('anchors'):\\n            ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  \\n        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  \\n        exclude = ['anchor'] if opt.cfg or hyp.get('anchors') else []  \\n        state_dict = ckpt['model'].float().state_dict()  \\n        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  \\n        model.load_state_dict(state_dict, strict=False)  \\n        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  \\n    else:\\n        model = Model(opt.cfg, ch=3, nc=nc).to(device)  \\n    freeze = []  \\n    for k, v in model.named_parameters():\\n        v.requires_grad = True  \\n        if any(x in k for x in freeze):\\n            print('freezing %s' % k)\\n            v.requires_grad = False\\n    nbs = 64  \\n    accumulate = max(round(nbs / total_batch_size), 1)  \\n    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  \\n    logger.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\\n    pg0, pg1, pg2 = [], [], []  \\n    for k, v in model.named_modules():\\n        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\\n            pg2.append(v.bias)  \\n        if isinstance(v, nn.BatchNorm2d):\\n            pg0.append(v.weight)  \\n        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\\n            pg1.append(v.weight)  \\n    if opt.adam:\\n        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  \\n    else:\\n        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\\n    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  \\n    optimizer.add_param_group({'params': pg2})  \\n    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\\n    del pg0, pg1, pg2\\n    if opt.linear_lr:\\n        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  \\n    else:\\n        lf = one_cycle(1, hyp['lrf'], epochs)  \\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\\n    if rank in [-1, 0] and wandb and wandb.run is None:\\n        opt.hyp = hyp  \\n        wandb_run = wandb.init(config=opt, resume=\"allow\",\\n                               project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\\n                               name=save_dir.stem,\\n                               id=ckpt.get('wandb_id') if 'ckpt' in locals() else None)\\n    loggers = {'wandb': wandb}  \\n    start_epoch, best_fitness = 0, 0.0\\n    if pretrained:\\n        if ckpt['optimizer'] is not None:\\n            optimizer.load_state_dict(ckpt['optimizer'])\\n            best_fitness = ckpt['best_fitness']\\n        if ckpt.get('training_results') is not None:\\n            with open(results_file, 'w') as file:\\n                file.write(ckpt['training_results'])  \\n        start_epoch = ckpt['epoch'] + 1\\n        if opt.resume:\\n            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)\\n        if epochs < start_epoch:\\n            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\\n                        (weights, ckpt['epoch'], epochs))\\n            epochs += ckpt['epoch']  \\n        del ckpt, state_dict\\n    gs = max(int(model.stride.max()), 32)  \\n    nl = model.model[-1].nl  \\n    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  \\n    if cuda and rank == -1 and torch.cuda.device_count() > 1:\\n        model = torch.nn.DataParallel(model)\\n    if opt.sync_bn and cuda and rank != -1:\\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\\n        logger.info('Using SyncBatchNorm()')\\n    ema = ModelEMA(model) if rank in [-1, 0] else None\\n    if cuda and rank != -1:\\n        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)\\n    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\\n                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,\\n                                            world_size=opt.world_size, workers=opt.workers,\\n                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))\\n    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  \\n    nb = len(dataloader)  \\n    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)\\n    if rank in [-1, 0]:\\n        ema.updates = start_epoch * nb // accumulate  \\n        testloader = create_dataloader(test_path, imgsz_test, batch_size * 2, gs, opt,  \\n                                       hyp=hyp, cache=opt.cache_images and not opt.notest, rect=True, rank=-1,\\n                                       world_size=opt.world_size, workers=opt.workers,\\n                                       pad=0.5, prefix=colorstr('val: '))[0]\\n        if not opt.resume:\\n            labels = np.concatenate(dataset.labels, 0)\\n            c = torch.tensor(labels[:, 0])  \\n            if plots:\\n                plot_labels(labels, save_dir, loggers)\\n                if tb_writer:\\n                    tb_writer.add_histogram('classes', c, 0)\\n            if not opt.noautoanchor:\\n                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\\n    hyp['box'] *= 3. / nl  \\n    hyp['cls'] *= nc / 80. * 3. / nl  \\n    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  \\n    model.nc = nc  \\n    model.hyp = hyp  \\n    model.gr = 1.0  \\n    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  \\n    model.names = names\\n    t0 = time.time()\\n    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  \\n    maps = np.zeros(nc)  \\n    results = (0, 0, 0, 0, 0, 0, 0)  \\n    scheduler.last_epoch = start_epoch - 1  \\n    scaler = amp.GradScaler(enabled=cuda)\\n    compute_loss = ComputeLoss(model)  \\n    logger.info(f'Image sizes {imgsz} train, {imgsz_test} test\\n'\\n                f'Using {dataloader.num_workers} dataloader workers\\n'\\n                f'Logging results to {save_dir}\\n'\\n                f'Starting training for {epochs} epochs...')\\n    for epoch in range(start_epoch, epochs):  \\n        model.train()\\n        if opt.image_weights:\\n            if rank in [-1, 0]:\\n                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  \\n                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  \\n                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  \\n            if rank != -1:\\n                indices = (torch.tensor(dataset.indices) if rank == 0 else torch.zeros(dataset.n)).int()\\n                dist.broadcast(indices, 0)\\n                if rank != 0:\\n                    dataset.indices = indices.cpu().numpy()\\n        mloss = torch.zeros(4, device=device)  \\n        if rank != -1:\\n            dataloader.sampler.set_epoch(epoch)\\n        pbar = enumerate(dataloader)\\n        logger.info(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'targets', 'img_size'))\\n        if rank in [-1, 0]:\\n            pbar = tqdm(pbar, total=nb)  \\n        optimizer.zero_grad()\\n        for i, (imgs, targets, paths, _) in pbar:  \\n            ni = i + nb * epoch  \\n            imgs = imgs.to(device, non_blocking=True).float() / 255.0  \\n            if ni <= nw:\\n                xi = [0, nw]  \\n                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\\n                for j, x in enumerate(optimizer.param_groups):\\n                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\\n                    if 'momentum' in x:\\n                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\\n            if opt.multi_scale:\\n                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  \\n                sf = sz / max(imgs.shape[2:])  \\n                if sf != 1:\\n                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  \\n                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\\n            with amp.autocast(enabled=cuda):\\n                pred = model(imgs)  \\n                loss, loss_items = compute_loss(pred, targets.to(device))  \\n                if rank != -1:\\n                    loss *= opt.world_size  \\n                if opt.quad:\\n                    loss *= 4.\\n            scaler.scale(loss).backward()\\n            if ni % accumulate == 0:\\n                scaler.step(optimizer)  \\n                scaler.update()\\n                optimizer.zero_grad()\\n                if ema:\\n                    ema.update(model)\\n            if rank in [-1, 0]:\\n                mloss = (mloss * i + loss_items) / (i + 1)  \\n                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  \\n                s = ('%10s' * 2 + '%10.4g' * 6) % (\\n                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\\n                pbar.set_description(s)\\n                if plots and ni < 3:\\n                    f = save_dir / f'train_batch{ni}.jpg'  \\n                    Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()\\n                elif plots and ni == 10 and wandb:\\n                    wandb.log({\"Mosaics\": [wandb.Image(str(x), caption=x.name) for x in save_dir.glob('train*.jpg')\\n                                           if x.exists()]}, commit=False)\\n        lr = [x['lr'] for x in optimizer.param_groups]  \\n        scheduler.step()\\n        if rank in [-1, 0]:\\n            if ema:\\n                ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])\\n            final_epoch = epoch + 1 == epochs\\n            if not opt.notest or final_epoch:  \\n                results, maps, times = test.test(opt.data,\\n                                                 batch_size=batch_size * 2,\\n                                                 imgsz=imgsz_test,\\n                                                 model=ema.ema,\\n                                                 single_cls=opt.single_cls,\\n                                                 dataloader=testloader,\\n                                                 save_dir=save_dir,\\n                                                 verbose=nc < 50 and final_epoch,\\n                                                 plots=plots and final_epoch,\\n                                                 log_imgs=opt.log_imgs if wandb else 0,\\n                                                 compute_loss=compute_loss)\\n            with open(results_file, 'a') as f:\\n                f.write(s + '%10.4g' * 7 % results + '\\n')  \\n            if len(opt.name) and opt.bucket:\\n                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\\n            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  \\n                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\\n                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  \\n                    'x/lr0', 'x/lr1', 'x/lr2']  \\n            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\\n                if tb_writer:\\n                    tb_writer.add_scalar(tag, x, epoch)  \\n                if wandb:\\n                    wandb.log({tag: x}, step=epoch, commit=tag == tags[-1])  \\n            fi = fitness(np.array(results).reshape(1, -1))  \\n            if fi > best_fitness:\\n                best_fitness = fi\\n            save = (not opt.nosave) or (final_epoch and not opt.evolve)\\n            if save:\\n                with open(results_file, 'r') as f:  \\n                    ckpt = {'epoch': epoch,\\n                            'best_fitness': best_fitness,\\n                            'training_results': f.read(),\\n                            'model': ema.ema,\\n                            'optimizer': None if final_epoch else optimizer.state_dict(),\\n                            'wandb_id': wandb_run.id if wandb else None}\\n                torch.save(ckpt, last)\\n                if best_fitness == fi:\\n                    torch.save(ckpt, best)\\n                del ckpt\\n    if rank in [-1, 0]:\\n        final = best if best.exists() else last  \\n        for f in [last, best]:\\n            if f.exists():\\n                strip_optimizer(f)  \\n        if opt.bucket:\\n            os.system(f'gsutil cp {final} gs://{opt.bucket}/weights')  \\n        if plots:\\n            plot_results(save_dir=save_dir)  \\n            if wandb:\\n                files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]\\n                wandb.log({\"Results\": [wandb.Image(str(save_dir / f), caption=f) for f in files\\n                                       if (save_dir / f).exists()]})\\n                if opt.log_artifacts:\\n                    wandb.log_artifact(artifact_or_path=str(final), type='model', name=save_dir.stem)\\n        logger.info('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\\n        if opt.data.endswith('coco.yaml') and nc == 80:  \\n            for conf, iou, save_json in ([0.25, 0.45, False], [0.001, 0.65, True]):  \\n                results, _, _ = test.test(opt.data,\\n                                          batch_size=batch_size * 2,\\n                                          imgsz=imgsz_test,\\n                                          conf_thres=conf,\\n                                          iou_thres=iou,\\n                                          model=attempt_load(final, device).half(),\\n                                          single_cls=opt.single_cls,\\n                                          dataloader=testloader,\\n                                          save_dir=save_dir,\\n                                          save_json=save_json,\\n                                          plots=False)\\n    else:\\n        dist.destroy_process_group()\\n    wandb.run.finish() if wandb and wandb.run else None\\n    torch.cuda.empty_cache()\\n    return results"
  },
  {
    "code": "def __init__(self, n_components='auto', density='auto', eps=0.5,\\n                 dense_output=False, random_state=None,\\n                 distribution=\"bernouilli\"):\\n        self.n_components = n_components\\n        self.density = density\\n        self.eps = eps\\n        self.dense_output = dense_output\\n        self.random_state = random_state\\n        self.distribution = distribution\\n        self.components_ = None\\n        self.n_components_ = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, n_components='auto', density='auto', eps=0.5,\\n                 dense_output=False, random_state=None,\\n                 distribution=\"bernouilli\"):\\n        self.n_components = n_components\\n        self.density = density\\n        self.eps = eps\\n        self.dense_output = dense_output\\n        self.random_state = random_state\\n        self.distribution = distribution\\n        self.components_ = None\\n        self.n_components_ = None"
  },
  {
    "code": "def max(self):\\n        import pandas.core.nanops\\n        return pandas.core.nanops.nanmax(self.values)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def max(self):\\n        import pandas.core.nanops\\n        return pandas.core.nanops.nanmax(self.values)"
  },
  {
    "code": "def __init__(\\n        self,\\n        *,\\n        batch_id: Union[int, str],\\n        livy_conn_id: str = 'livy_default',\\n        extra_options: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.batch_id = batch_id\\n        self._livy_conn_id = livy_conn_id\\n        self._livy_hook: Optional[LivyHook] = None\\n        self._extra_options = extra_options or {}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        *,\\n        batch_id: Union[int, str],\\n        livy_conn_id: str = 'livy_default',\\n        extra_options: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.batch_id = batch_id\\n        self._livy_conn_id = livy_conn_id\\n        self._livy_hook: Optional[LivyHook] = None\\n        self._extra_options = extra_options or {}"
  },
  {
    "code": "def _aggregate_item_by_item(self, func, *args, **kwargs):\\n        obj = self._obj_with_exclusions\\n        result = {}\\n        cannot_agg = []\\n        for item in obj:\\n            try:\\n                result[item] = self[item].agg(func, *args, **kwargs)\\n            except (ValueError, TypeError):\\n                cannot_agg.append(item)\\n                continue\\n        return DataFrame(result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_item_by_item(self, func, *args, **kwargs):\\n        obj = self._obj_with_exclusions\\n        result = {}\\n        cannot_agg = []\\n        for item in obj:\\n            try:\\n                result[item] = self[item].agg(func, *args, **kwargs)\\n            except (ValueError, TypeError):\\n                cannot_agg.append(item)\\n                continue\\n        return DataFrame(result)"
  },
  {
    "code": "def parse_distribution_file_Debian(self, name, data, path, collected_facts):\\n\\t\\tdebian_facts = {}\\n\\t\\tif 'Debian' in data or 'Raspbian' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Debian'\\n\\t\\t\\trelease = re.search(r\"PRETTY_NAME=[^(]+ \\(?([^)]+?)\\)\", data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\t\\tif collected_facts['distribution_release'] == 'NA' and 'Debian' in data:\\n\\t\\t\\t\\tdpkg_cmd = self.module.get_bin_path('dpkg')\\n\\t\\t\\t\\tif dpkg_cmd:\\n\\t\\t\\t\\t\\tcmd = \"%s --status tzdata|grep Provides|cut -f2 -d'-'\" % dpkg_cmd\\n\\t\\t\\t\\t\\trc, out, err = self.module.run_command(cmd)\\n\\t\\t\\t\\t\\tif rc == 0:\\n\\t\\t\\t\\t\\t\\tdebian_facts['distribution_release'] = out.strip()\\n\\t\\telif 'Ubuntu' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Ubuntu'\\n\\t\\telif 'SteamOS' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'SteamOS'\\n\\t\\telif path == '/etc/lsb-release' and 'Kali' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Kali'\\n\\t\\t\\trelease = re.search('DISTRIB_RELEASE=(.*)', data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\telif 'Devuan' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Devuan'\\n\\t\\t\\trelease = re.search(r\"PRETTY_NAME=\\\"?[^(\\\"]+ \\(?([^) \\\"]+)\\)?\", data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\t\\tversion = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\\n\\t\\t\\tif version:\\n\\t\\t\\t\\tdebian_facts['distribution_version'] = version.group(1)\\n\\t\\t\\t\\tdebian_facts['distribution_major_version'] = version.group(1)\\n\\t\\telif 'Cumulus' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Cumulus Linux'\\n\\t\\t\\tversion = re.search(r\"VERSION_ID=(.*)\", data)\\n\\t\\t\\tif version:\\n\\t\\t\\t\\tmajor, _minor, _dummy_ver = version.group(1).split(\".\")\\n\\t\\t\\t\\tdebian_facts['distribution_version'] = version.group(1)\\n\\t\\t\\t\\tdebian_facts['distribution_major_version'] = major\\n\\t\\t\\trelease = re.search(r'VERSION=\"(.*)\"', data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\telif \"Mint\" in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Linux Mint'\\n\\t\\t\\tversion = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\\n\\t\\t\\tif version:\\n\\t\\t\\t\\tdebian_facts['distribution_version'] = version.group(1)\\n\\t\\t\\t\\tdebian_facts['distribution_major_version'] = version.group(1).split('.')[0]\\n\\t\\telse:\\n\\t\\t\\treturn False, debian_facts\\n\\t\\treturn True, debian_facts",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix support for Kali Linux detection (#69194)",
    "fixed_code": "def parse_distribution_file_Debian(self, name, data, path, collected_facts):\\n\\t\\tdebian_facts = {}\\n\\t\\tif 'Debian' in data or 'Raspbian' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Debian'\\n\\t\\t\\trelease = re.search(r\"PRETTY_NAME=[^(]+ \\(?([^)]+?)\\)\", data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\t\\tif collected_facts['distribution_release'] == 'NA' and 'Debian' in data:\\n\\t\\t\\t\\tdpkg_cmd = self.module.get_bin_path('dpkg')\\n\\t\\t\\t\\tif dpkg_cmd:\\n\\t\\t\\t\\t\\tcmd = \"%s --status tzdata|grep Provides|cut -f2 -d'-'\" % dpkg_cmd\\n\\t\\t\\t\\t\\trc, out, err = self.module.run_command(cmd)\\n\\t\\t\\t\\t\\tif rc == 0:\\n\\t\\t\\t\\t\\t\\tdebian_facts['distribution_release'] = out.strip()\\n\\t\\telif 'Ubuntu' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Ubuntu'\\n\\t\\telif 'SteamOS' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'SteamOS'\\n\\t\\telif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Kali'\\n\\t\\t\\trelease = re.search('DISTRIB_RELEASE=(.*)', data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\telif 'Devuan' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Devuan'\\n\\t\\t\\trelease = re.search(r\"PRETTY_NAME=\\\"?[^(\\\"]+ \\(?([^) \\\"]+)\\)?\", data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\t\\tversion = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\\n\\t\\t\\tif version:\\n\\t\\t\\t\\tdebian_facts['distribution_version'] = version.group(1)\\n\\t\\t\\t\\tdebian_facts['distribution_major_version'] = version.group(1)\\n\\t\\telif 'Cumulus' in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Cumulus Linux'\\n\\t\\t\\tversion = re.search(r\"VERSION_ID=(.*)\", data)\\n\\t\\t\\tif version:\\n\\t\\t\\t\\tmajor, _minor, _dummy_ver = version.group(1).split(\".\")\\n\\t\\t\\t\\tdebian_facts['distribution_version'] = version.group(1)\\n\\t\\t\\t\\tdebian_facts['distribution_major_version'] = major\\n\\t\\t\\trelease = re.search(r'VERSION=\"(.*)\"', data)\\n\\t\\t\\tif release:\\n\\t\\t\\t\\tdebian_facts['distribution_release'] = release.groups()[0]\\n\\t\\telif \"Mint\" in data:\\n\\t\\t\\tdebian_facts['distribution'] = 'Linux Mint'\\n\\t\\t\\tversion = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\\n\\t\\t\\tif version:\\n\\t\\t\\t\\tdebian_facts['distribution_version'] = version.group(1)\\n\\t\\t\\t\\tdebian_facts['distribution_major_version'] = version.group(1).split('.')[0]\\n\\t\\telse:\\n\\t\\t\\treturn False, debian_facts\\n\\t\\treturn True, debian_facts"
  },
  {
    "code": "def execute(self, context):\\n        if self.bq_cursor is None:\\n            self.log.info('Executing: %s', self.sql)\\n            hook = BigQueryHook(\\n                bigquery_conn_id=self.bigquery_conn_id,\\n                use_legacy_sql=self.use_legacy_sql,\\n                delegate_to=self.delegate_to)\\n            conn = hook.get_conn()\\n            self.bq_cursor = conn.cursor()\\n        self.bq_cursor.run_query(\\n            sql=self.sql,\\n            destination_dataset_table=self.destination_dataset_table,\\n            write_disposition=self.write_disposition,\\n            allow_large_results=self.allow_large_results,\\n            flatten_results=self.flatten_results,\\n            udf_config=self.udf_config,\\n            maximum_billing_tier=self.maximum_billing_tier,\\n            maximum_bytes_billed=self.maximum_bytes_billed,\\n            create_disposition=self.create_disposition,\\n            query_params=self.query_params,\\n            labels=self.labels,\\n            schema_update_options=self.schema_update_options,\\n            priority=self.priority,\\n            time_partitioning=self.time_partitioning,\\n            api_resource_configs=self.api_resource_configs,\\n            cluster_fields=self.cluster_fields,\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(self, context):\\n        if self.bq_cursor is None:\\n            self.log.info('Executing: %s', self.sql)\\n            hook = BigQueryHook(\\n                bigquery_conn_id=self.bigquery_conn_id,\\n                use_legacy_sql=self.use_legacy_sql,\\n                delegate_to=self.delegate_to)\\n            conn = hook.get_conn()\\n            self.bq_cursor = conn.cursor()\\n        self.bq_cursor.run_query(\\n            sql=self.sql,\\n            destination_dataset_table=self.destination_dataset_table,\\n            write_disposition=self.write_disposition,\\n            allow_large_results=self.allow_large_results,\\n            flatten_results=self.flatten_results,\\n            udf_config=self.udf_config,\\n            maximum_billing_tier=self.maximum_billing_tier,\\n            maximum_bytes_billed=self.maximum_bytes_billed,\\n            create_disposition=self.create_disposition,\\n            query_params=self.query_params,\\n            labels=self.labels,\\n            schema_update_options=self.schema_update_options,\\n            priority=self.priority,\\n            time_partitioning=self.time_partitioning,\\n            api_resource_configs=self.api_resource_configs,\\n            cluster_fields=self.cluster_fields,\\n        )"
  },
  {
    "code": "def update_stack(module, stack_params, cfn, events_limit):\\n    if 'TemplateBody' not in stack_params and 'TemplateURL' not in stack_params:\\n        stack_params['UsePreviousTemplate'] = True\\n    try:\\n        cfn.update_stack(**stack_params)\\n        result = stack_operation(cfn, stack_params['StackName'], 'UPDATE', events_limit, stack_params.get('ClientRequestToken', None))\\n    except Exception as err:\\n        error_msg = boto_exception(err)\\n        if 'No updates are to be performed.' in error_msg:\\n            result = dict(changed=False, output='Stack is already up-to-date.')\\n        else:\\n            module.fail_json(msg=\"Failed to update stack {0}: {1}\".format(stack_params.get('StackName'), error_msg), exception=traceback.format_exc())\\n    if not result:\\n        module.fail_json(msg=\"empty result\")\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_stack(module, stack_params, cfn, events_limit):\\n    if 'TemplateBody' not in stack_params and 'TemplateURL' not in stack_params:\\n        stack_params['UsePreviousTemplate'] = True\\n    try:\\n        cfn.update_stack(**stack_params)\\n        result = stack_operation(cfn, stack_params['StackName'], 'UPDATE', events_limit, stack_params.get('ClientRequestToken', None))\\n    except Exception as err:\\n        error_msg = boto_exception(err)\\n        if 'No updates are to be performed.' in error_msg:\\n            result = dict(changed=False, output='Stack is already up-to-date.')\\n        else:\\n            module.fail_json(msg=\"Failed to update stack {0}: {1}\".format(stack_params.get('StackName'), error_msg), exception=traceback.format_exc())\\n    if not result:\\n        module.fail_json(msg=\"empty result\")\\n    return result"
  },
  {
    "code": "def create_volume(self):\\n        ''\\n        if self.parameters.get('aggregate_name') is None:\\n            self.module.fail_json(msg='Error provisioning volume %s: \\\\n                                  aggregate_name is required'\\n                                  % self.parameters['name'])\\n        options = {'volume': self.parameters['name'],\\n                   'containing-aggr-name': self.parameters['aggregate_name'],\\n                   'size': str(self.parameters['size'])}\\n        if self.parameters.get('percent_snapshot_space'):\\n            options['percentage-snapshot-reserve'] = self.parameters['percent_snapshot_space']\\n        if self.parameters.get('type'):\\n            options['volume-type'] = self.parameters['type']\\n        if self.parameters.get('policy'):\\n            options['export-policy'] = self.parameters['policy']\\n        if self.parameters.get('junction_path'):\\n            options['junction-path'] = self.parameters['junction_path']\\n        if self.parameters.get('space_guarantee'):\\n            options['space-reserve'] = self.parameters['space_guarantee']\\n        if self.parameters.get('volume_security_style'):\\n            options['volume-security-style'] = self.parameters['volume_security_style']\\n        volume_create = netapp_utils.zapi.NaElement.create_node_with_children('volume-create', **options)\\n        try:\\n            self.server.invoke_successfully(volume_create,\\n                                            enable_tunneling=True)\\n            self.ems_log_event(\"volume-create\")\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error provisioning volume %s \\\\n                                  of size %s: %s'\\n                                  % (self.parameters['name'], self.parameters['size'], to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "New parameters and bugfixes for na_ontap_volume (#49511)",
    "fixed_code": "def create_volume(self):\\n        ''\\n        if self.parameters.get('aggregate_name') is None:\\n            self.module.fail_json(msg='Error provisioning volume %s: aggregate_name is required'\\n                                  % self.parameters['name'])\\n        options = {'volume': self.parameters['name'],\\n                   'containing-aggr-name': self.parameters['aggregate_name'],\\n                   'size': str(self.parameters['size'])}\\n        if self.parameters.get('percent_snapshot_space'):\\n            options['percentage-snapshot-reserve'] = self.parameters['percent_snapshot_space']\\n        if self.parameters.get('type'):\\n            options['volume-type'] = self.parameters['type']\\n        if self.parameters.get('policy'):\\n            options['export-policy'] = self.parameters['policy']\\n        if self.parameters.get('junction_path'):\\n            options['junction-path'] = self.parameters['junction_path']\\n        if self.parameters.get('space_guarantee'):\\n            options['space-reserve'] = self.parameters['space_guarantee']\\n        if self.parameters.get('volume_security_style'):\\n            options['volume-security-style'] = self.parameters['volume_security_style']\\n        if self.parameters.get('unix_permissions'):\\n            options['unix-permissions'] = self.parameters['unix_permissions']\\n        if self.parameters.get('snapshot_policy'):\\n            options['snapshot-policy'] = self.parameters['snapshot_policy']\\n        volume_create = netapp_utils.zapi.NaElement.create_node_with_children('volume-create', **options)\\n        try:\\n            self.server.invoke_successfully(volume_create,\\n                                            enable_tunneling=True)\\n            self.ems_log_event(\"volume-create\")\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error provisioning volume %s \\\\n                                  of size %s: %s'\\n                                  % (self.parameters['name'], self.parameters['size'], to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def cycle(parser, token):\\n    args = token.split_contents()\\n    if len(args) < 2:\\n        raise TemplateSyntaxError(\"'cycle' tag requires at least two arguments\")\\n    if ',' in args[1]:\\n        args[1:2] = ['\"%s\"' % arg for arg in args[1].split(\",\")]\\n    if len(args) == 2:\\n        name = args[1]\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            raise TemplateSyntaxError(\"No named cycles in template. '%s' is not defined\" % name)\\n        if name not in parser._namedCycleNodes:\\n            raise TemplateSyntaxError(\"Named cycle '%s' does not exist\" % name)\\n        return parser._namedCycleNodes[name]\\n    as_form = False\\n    if len(args) > 4:\\n        if args[-3] == \"as\":\\n            if args[-1] != \"silent\":\\n                raise TemplateSyntaxError(\"Only 'silent' flag is allowed after cycle's name, not '%s'.\" % args[-1])\\n            as_form = True\\n            silent = True\\n            args = args[:-1]\\n        elif args[-2] == \"as\":\\n            as_form = True\\n            silent = False\\n    if as_form:\\n        name = args[-1]\\n        values = [parser.compile_filter(arg) for arg in args[1:-2]]\\n        node = CycleNode(values, name, silent=silent)\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            parser._namedCycleNodes = {}\\n        parser._namedCycleNodes[name] = node\\n    else:\\n        values = [parser.compile_filter(arg) for arg in args[1:]]\\n        node = CycleNode(values)\\n    return node",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24451 -- Deprecated comma-separated {% cycle %} syntax.",
    "fixed_code": "def cycle(parser, token):\\n    args = token.split_contents()\\n    if len(args) < 2:\\n        raise TemplateSyntaxError(\"'cycle' tag requires at least two arguments\")\\n    if ',' in args[1]:\\n        warnings.warn(\\n            \"The old {% cycle %} syntax with comma-separated arguments is deprecated.\",\\n            RemovedInDjango20Warning,\\n        )\\n        args[1:2] = ['\"%s\"' % arg for arg in args[1].split(\",\")]\\n    if len(args) == 2:\\n        name = args[1]\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            raise TemplateSyntaxError(\"No named cycles in template. '%s' is not defined\" % name)\\n        if name not in parser._namedCycleNodes:\\n            raise TemplateSyntaxError(\"Named cycle '%s' does not exist\" % name)\\n        return parser._namedCycleNodes[name]\\n    as_form = False\\n    if len(args) > 4:\\n        if args[-3] == \"as\":\\n            if args[-1] != \"silent\":\\n                raise TemplateSyntaxError(\"Only 'silent' flag is allowed after cycle's name, not '%s'.\" % args[-1])\\n            as_form = True\\n            silent = True\\n            args = args[:-1]\\n        elif args[-2] == \"as\":\\n            as_form = True\\n            silent = False\\n    if as_form:\\n        name = args[-1]\\n        values = [parser.compile_filter(arg) for arg in args[1:-2]]\\n        node = CycleNode(values, name, silent=silent)\\n        if not hasattr(parser, '_namedCycleNodes'):\\n            parser._namedCycleNodes = {}\\n        parser._namedCycleNodes[name] = node\\n    else:\\n        values = [parser.compile_filter(arg) for arg in args[1:]]\\n        node = CycleNode(values)\\n    return node"
  },
  {
    "code": "def locate(path, forceload=0):\\n    parts = [part for part in path.split('.') if part]\\n    module, n = None, 0\\n    while n < len(parts):\\n        nextmodule = safeimport('.'.join(parts[:n+1]), forceload)\\n        if nextmodule: module, n = nextmodule, n + 1\\n        else: break\\n    if module:\\n        object = module\\n        for part in parts[n:]:\\n            try: object = getattr(object, part)\\n            except AttributeError: return None\\n        return object\\n    else:\\n        if hasattr(builtins, path):\\n            return getattr(builtins, path)\\ntext = TextDoc()\\nplaintext = _PlainTextDoc()\\nhtml = HTMLDoc()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make \u201cpydoc somebuiltin.somemethod\u201d work (#8887)",
    "fixed_code": "def locate(path, forceload=0):\\n    parts = [part for part in path.split('.') if part]\\n    module, n = None, 0\\n    while n < len(parts):\\n        nextmodule = safeimport('.'.join(parts[:n+1]), forceload)\\n        if nextmodule: module, n = nextmodule, n + 1\\n        else: break\\n    if module:\\n        object = module\\n    else:\\n        object = builtins\\n    for part in parts[n:]:\\n        try:\\n            object = getattr(object, part)\\n        except AttributeError:\\n            return None\\n    return object\\ntext = TextDoc()\\nplaintext = _PlainTextDoc()\\nhtml = HTMLDoc()"
  },
  {
    "code": "def _explicit_index_names(self, columns):\\n        index_name = None\\n        if np.isscalar(self.index_col):\\n            if isinstance(self.index_col, basestring):\\n                index_name = self.index_col\\n                for i, c in enumerate(list(columns)):\\n                    if c == self.index_col:\\n                        self.index_col = i\\n                        columns.pop(i)\\n                        break\\n            else:\\n                index_name = columns[self.index_col]\\n            if index_name is not None and 'Unnamed' in index_name:\\n                index_name = None\\n        elif self.index_col is not None:\\n            cp_cols = list(columns)\\n            index_name = []\\n            index_col = list(self.index_col)\\n            for i, c in enumerate(index_col):\\n                if isinstance(c, basestring):\\n                    index_name.append(c)\\n                    for j, name in enumerate(cp_cols):\\n                        if name == c:\\n                            index_col[i] = j\\n                            columns.remove(name)\\n                            break\\n                else:\\n                    name = cp_cols[c]\\n                    columns.remove(name)\\n                    index_name.append(name)\\n            self.index_col = index_col\\n        return index_name",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: more refactoring.  checkpoint to debug something",
    "fixed_code": "def _explicit_index_names(self, columns):\\n        if self.index_col is None:\\n            return None\\n        cp_cols = list(columns)\\n        index_name = []\\n        index_col = list(self.index_col)\\n        for i, c in enumerate(index_col):\\n            if isinstance(c, basestring):\\n                index_name.append(c)\\n                for j, name in enumerate(cp_cols):\\n                    if name == c:\\n                        index_col[i] = j\\n                        columns.remove(name)\\n                        break\\n            else:\\n                name = cp_cols[c]\\n                columns.remove(name)\\n                index_name.append(name)\\n        self.index_col = index_col\\n        return index_name"
  },
  {
    "code": "def get_group_labels(self, group):\\n        inds = self.indices[group]\\n        return self.labels.take(inds)\\n    _groups = None\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: pure python multi-grouping working, UI needs work. as does cython integration",
    "fixed_code": "def get_group_labels(self, group):\\n        inds = self.indices[group]\\n        return self.index.take(inds)\\n    _labels = None\\n    _ids = None\\n    _counts = None\\n    @property"
  },
  {
    "code": "def sanitize_address(addr, encoding):\\n\\taddress = None\\n\\tif not isinstance(addr, tuple):\\n\\t\\taddr = force_str(addr)\\n\\t\\ttry:\\n\\t\\t\\ttoken, rest = parser.get_mailbox(addr)\\n\\t\\texcept (HeaderParseError, ValueError, IndexError):\\n\\t\\t\\traise ValueError('Invalid address \"%s\"' % addr)\\n\\t\\telse:\\n\\t\\t\\tif rest:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t'Invalid address; only %s could be parsed from \"%s\"'\\n\\t\\t\\t\\t\\t% (token, addr)\\n\\t\\t\\t\\t)\\n\\t\\t\\tnm = token.display_name or ''\\n\\t\\t\\tlocalpart = token.local_part\\n\\t\\t\\tdomain = token.domain or ''\\n\\telse:\\n\\t\\tnm, address = addr\\n\\t\\tlocalpart, domain = address.rsplit('@', 1)\\n\\taddress_parts = nm + localpart + domain\\n\\tif '\\n' in address_parts or '\\r' in address_parts:\\n\\t\\traise ValueError('Invalid address; address parts cannot contain newlines.')\\n\\ttry:\\n\\t\\tnm.encode('ascii')\\n\\t\\tnm = Header(nm).encode()\\n\\texcept UnicodeEncodeError:\\n\\t\\tnm = Header(nm, encoding).encode()\\n\\ttry:\\n\\t\\tlocalpart.encode('ascii')\\n\\texcept UnicodeEncodeError:\\n\\t\\tlocalpart = Header(localpart, encoding).encode()\\n\\tdomain = punycode(domain)\\n\\tparsed_address = Address(username=localpart, domain=domain)\\n\\treturn formataddr((nm, parsed_address.addr_spec))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sanitize_address(addr, encoding):\\n\\taddress = None\\n\\tif not isinstance(addr, tuple):\\n\\t\\taddr = force_str(addr)\\n\\t\\ttry:\\n\\t\\t\\ttoken, rest = parser.get_mailbox(addr)\\n\\t\\texcept (HeaderParseError, ValueError, IndexError):\\n\\t\\t\\traise ValueError('Invalid address \"%s\"' % addr)\\n\\t\\telse:\\n\\t\\t\\tif rest:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t'Invalid address; only %s could be parsed from \"%s\"'\\n\\t\\t\\t\\t\\t% (token, addr)\\n\\t\\t\\t\\t)\\n\\t\\t\\tnm = token.display_name or ''\\n\\t\\t\\tlocalpart = token.local_part\\n\\t\\t\\tdomain = token.domain or ''\\n\\telse:\\n\\t\\tnm, address = addr\\n\\t\\tlocalpart, domain = address.rsplit('@', 1)\\n\\taddress_parts = nm + localpart + domain\\n\\tif '\\n' in address_parts or '\\r' in address_parts:\\n\\t\\traise ValueError('Invalid address; address parts cannot contain newlines.')\\n\\ttry:\\n\\t\\tnm.encode('ascii')\\n\\t\\tnm = Header(nm).encode()\\n\\texcept UnicodeEncodeError:\\n\\t\\tnm = Header(nm, encoding).encode()\\n\\ttry:\\n\\t\\tlocalpart.encode('ascii')\\n\\texcept UnicodeEncodeError:\\n\\t\\tlocalpart = Header(localpart, encoding).encode()\\n\\tdomain = punycode(domain)\\n\\tparsed_address = Address(username=localpart, domain=domain)\\n\\treturn formataddr((nm, parsed_address.addr_spec))"
  },
  {
    "code": "def validate_host(host, allowed_hosts):\\n\\thost = host[:-1] if host.endswith('.') else host\\n\\tfor pattern in allowed_hosts:\\n\\t\\tif pattern == '*' or is_same_domain(host, pattern):\\n\\t\\t\\treturn True\\n\\treturn False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate_host(host, allowed_hosts):\\n\\thost = host[:-1] if host.endswith('.') else host\\n\\tfor pattern in allowed_hosts:\\n\\t\\tif pattern == '*' or is_same_domain(host, pattern):\\n\\t\\t\\treturn True\\n\\treturn False"
  },
  {
    "code": "def combineFunc(self, other, func):\\n        if isinstance(other, Series):\\n            newIndex = self.index + other.index\\n            newArr = np.empty(len(newIndex), dtype=self.dtype)\\n            for i, idx in enumerate(newIndex):\\n                newArr[i] = func(self.get(idx, NaN), other.get(idx, NaN))\\n        else:\\n            newIndex = self.index\\n            newArr = func(self.values(), other)\\n        return Series(newArr, index=newIndex)\\n    def combineFirst(self, other):\\n        if self.index.equals(other.index):\\n            newIndex = self.index\\n            this = self\\n        else:\\n            newIndex = self.index + other.index\\n            this = self.reindex(newIndex)\\n            other = other.reindex(newIndex)\\n        result = Series(np.where(isnull(this), other, this), index=newIndex)\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def combineFunc(self, other, func):\\n        if isinstance(other, Series):\\n            newIndex = self.index + other.index\\n            newArr = np.empty(len(newIndex), dtype=self.dtype)\\n            for i, idx in enumerate(newIndex):\\n                newArr[i] = func(self.get(idx, NaN), other.get(idx, NaN))\\n        else:\\n            newIndex = self.index\\n            newArr = func(self.values(), other)\\n        return Series(newArr, index=newIndex)\\n    def combineFirst(self, other):\\n        if self.index.equals(other.index):\\n            newIndex = self.index\\n            this = self\\n        else:\\n            newIndex = self.index + other.index\\n            this = self.reindex(newIndex)\\n            other = other.reindex(newIndex)\\n        result = Series(np.where(isnull(this), other, this), index=newIndex)\\n        return result"
  },
  {
    "code": "def check_classifiers_classes(name, Classifier):\\n    X, y = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)\\n    X, y = shuffle(X, y, random_state=7)\\n    X = StandardScaler().fit_transform(X)\\n    X -= X.min() - .1\\n    y_names = np.array([\"one\", \"two\", \"three\"])[y]\\n    for y_names in [y_names, y_names.astype('O')]:\\n        if name in [\"LabelPropagation\", \"LabelSpreading\"]:\\n            y_ = y\\n        else:\\n            y_ = y_names\\n        classes = np.unique(y_)\\n        with warnings.catch_warnings(record=True):\\n            classifier = Classifier()\\n        if name == 'BernoulliNB':\\n            classifier.set_params(binarize=X.mean())\\n        set_testing_parameters(classifier)\\n        set_random_state(classifier)\\n        classifier.fit(X, y_)\\n        y_pred = classifier.predict(X)\\n        assert_array_equal(np.unique(y_), np.unique(y_pred))\\n        if np.any(classifier.classes_ != classes):\\n            print(\"Unexpected classes_ attribute for %r: \"\\n                  \"expected %s, got %s\" %\\n                  (classifier, classes, classifier.classes_))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_classifiers_classes(name, Classifier):\\n    X, y = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)\\n    X, y = shuffle(X, y, random_state=7)\\n    X = StandardScaler().fit_transform(X)\\n    X -= X.min() - .1\\n    y_names = np.array([\"one\", \"two\", \"three\"])[y]\\n    for y_names in [y_names, y_names.astype('O')]:\\n        if name in [\"LabelPropagation\", \"LabelSpreading\"]:\\n            y_ = y\\n        else:\\n            y_ = y_names\\n        classes = np.unique(y_)\\n        with ignore_warnings(category=DeprecationWarning):\\n            classifier = Classifier()\\n        if name == 'BernoulliNB':\\n            classifier.set_params(binarize=X.mean())\\n        set_testing_parameters(classifier)\\n        set_random_state(classifier)\\n        classifier.fit(X, y_)\\n        y_pred = classifier.predict(X)\\n        assert_array_equal(np.unique(y_), np.unique(y_pred))\\n        if np.any(classifier.classes_ != classes):\\n            print(\"Unexpected classes_ attribute for %r: \"\\n                  \"expected %s, got %s\" %\\n                  (classifier, classes, classifier.classes_))"
  },
  {
    "code": "def _normalize(op1, op2, shouldround = 0, prec = 0):\\n    if op1.exp < op2.exp:\\n        tmp = op2\\n        other = op1\\n    else:\\n        tmp = op1\\n        other = op2\\n    if shouldround:\\n        tmp_len = len(str(tmp.int))\\n        other_len = len(str(other.int))\\n        exp = tmp.exp + min(-1, tmp_len - prec - 2)\\n        if other_len + other.exp - 1 < exp:\\n            other.int = 1\\n            other.exp = exp\\n    tmp.int *= 10 ** (tmp.exp - other.exp)\\n    tmp.exp = other.exp\\n    return op1, op2",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 59512-59540 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r59513 | raymond.hettinger | 2007-12-15 01:07:25 +0100 (Sat, 15 Dec 2007) | 6 lines\\n\\n  Optimize PyList_AsTuple(). Improve cache performance by doing the\\n  pointer copy and object increment in one pass.  For small lists,\\n  save the overhead of the call to memcpy() -- this comes up in\\n  calls like f(*listcomp).\\n........\\n  r59519 | christian.heimes | 2007-12-15 06:38:35 +0100 (Sat, 15 Dec 2007) | 2 lines\\n\\n  Fixed #1624: Remove output comparison for test_pep277\\n  I had to modify Brett's patch slightly.\\n........\\n  r59520 | georg.brandl | 2007-12-15 10:34:59 +0100 (Sat, 15 Dec 2007) | 2 lines\\n\\n  Add note about future import needed for with statement.\\n........\\n  r59522 | georg.brandl | 2007-12-15 10:36:37 +0100 (Sat, 15 Dec 2007) | 2 lines\\n\\n  Argh, wrong version.\\n........\\n  r59524 | georg.brandl | 2007-12-16 12:06:09 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Dummy commit to investigate #1617.\\n........\\n  r59525 | georg.brandl | 2007-12-16 12:21:48 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Revert dummy commit now that the build slave is building.\\n........\\n  r59527 | georg.brandl | 2007-12-16 16:47:46 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove orphaned footnote reference.\\n........\\n  r59528 | georg.brandl | 2007-12-16 16:53:49 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove gratuitous unicode character.\\n........\\n  r59529 | georg.brandl | 2007-12-16 16:59:19 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove another unnecessary Unicode character.\\n........\\n  r59530 | georg.brandl | 2007-12-16 17:00:36 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove curious space-like characters.\\n........\\n  r59532 | georg.brandl | 2007-12-16 20:36:51 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Adapt conf.py to new option names.\\n........\\n  r59533 | christian.heimes | 2007-12-16 22:39:43 +0100 (Sun, 16 Dec 2007) | 1 line\\n\\n  Fixed #1638: %zd configure test fails on Linux\\n........\\n  r59536 | georg.brandl | 2007-12-17 00:11:16 +0100 (Mon, 17 Dec 2007) | 2 lines\\n\\n  Simplify.\\n........\\n  r59537 | georg.brandl | 2007-12-17 00:13:29 +0100 (Mon, 17 Dec 2007) | 2 lines\\n\\n  Use PEP 8.\\n........\\n  r59539 | georg.brandl | 2007-12-17 00:15:07 +0100 (Mon, 17 Dec 2007) | 2 lines\\n\\n  Don't use quotes for non-string code.\\n........\\n  r59540 | facundo.batista | 2007-12-17 15:18:42 +0100 (Mon, 17 Dec 2007) | 4 lines\\n\\n\\n  Removed the private _rounding_decision: it was not needed, and the code\\n  is now simpler.  Thanks Mark Dickinson.\\n........",
    "fixed_code": "def _normalize(op1, op2, prec = 0):\\n    if op1.exp < op2.exp:\\n        tmp = op2\\n        other = op1\\n    else:\\n        tmp = op1\\n        other = op2\\n    tmp_len = len(str(tmp.int))\\n    other_len = len(str(other.int))\\n    exp = tmp.exp + min(-1, tmp_len - prec - 2)\\n    if other_len + other.exp - 1 < exp:\\n        other.int = 1\\n        other.exp = exp\\n    tmp.int *= 10 ** (tmp.exp - other.exp)\\n    tmp.exp = other.exp\\n    return op1, op2"
  },
  {
    "code": "def generate_files(repo_dir, context=None, output_dir='.',\\n\\t\\t\\t\\t   overwrite_if_exists=False):\\n\\ttemplate_dir = find_template(repo_dir)\\n\\tlogging.debug('Generating project from {0}...'.format(template_dir))\\n\\tcontext = context or {}\\n\\tunrendered_dir = os.path.split(template_dir)[1]\\n\\tensure_dir_is_templated(unrendered_dir)\\n\\tproject_dir = render_and_create_dir(unrendered_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcontext,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toutput_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toverwrite_if_exists)\\n\\tproject_dir = os.path.abspath(project_dir)\\n\\tlogging.debug('project_dir is {0}'.format(project_dir))\\n\\twith work_in(repo_dir):\\n\\t\\tif run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:\\n\\t\\t\\tlogging.error(\"Stopping generation because pre_gen_project\"\\n\\t\\t\\t\\t\\t\\t  \" hook script didn't exit sucessfully\")\\n\\t\\t\\treturn\\n\\twith work_in(template_dir):\\n\\t\\tenv = Environment(keep_trailing_newline=True)\\n\\t\\tenv.loader = FileSystemLoader('.')\\n\\t\\tfor root, dirs, files in os.walk('.'):\\n\\t\\t\\tcopy_dirs = []\\n\\t\\t\\trender_dirs = []\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\td_ = os.path.normpath(os.path.join(root, d))\\n\\t\\t\\t\\tif copy_without_render(d_, context):\\n\\t\\t\\t\\t\\tcopy_dirs.append(d)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\trender_dirs.append(d)\\n\\t\\t\\tfor copy_dir in copy_dirs:\\n\\t\\t\\t\\tindir = os.path.normpath(os.path.join(root, copy_dir))\\n\\t\\t\\t\\toutdir = os.path.normpath(os.path.join(project_dir, indir))\\n\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t'Copying dir {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t''.format(indir, outdir)\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tshutil.copytree(indir, outdir)\\n\\t\\t\\tdirs[:] = render_dirs\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\tunrendered_dir = os.path.join(project_dir, root, d)\\n\\t\\t\\t\\trender_and_create_dir(unrendered_dir, context, output_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  overwrite_if_exists)\\n\\t\\t\\tfor f in files:\\n\\t\\t\\t\\tinfile = os.path.normpath(os.path.join(root, f))\\n\\t\\t\\t\\tif copy_without_render(infile, context):\\n\\t\\t\\t\\t\\toutfile_tmpl = Template(infile)\\n\\t\\t\\t\\t\\toutfile_rendered = outfile_tmpl.render(**context)\\n\\t\\t\\t\\t\\toutfile = os.path.join(project_dir, outfile_rendered)\\n\\t\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t\\t'Copying file {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t\\t''.format(infile, outfile)\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tshutil.copyfile(infile, outfile)\\n\\t\\t\\t\\t\\tshutil.copymode(infile, outfile)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tlogging.debug('f is {0}'.format(f))\\n\\t\\t\\t\\tgenerate_file(project_dir, infile, context, env)\\n\\twith work_in(repo_dir):\\n\\t\\trun_hook('post_gen_project', project_dir, context)\\n\\treturn project_dir",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "using custom exception instead of exit status and cleanup project dir",
    "fixed_code": "def generate_files(repo_dir, context=None, output_dir='.',\\n\\t\\t\\t\\t   overwrite_if_exists=False):\\n\\ttemplate_dir = find_template(repo_dir)\\n\\tlogging.debug('Generating project from {0}...'.format(template_dir))\\n\\tcontext = context or {}\\n\\tunrendered_dir = os.path.split(template_dir)[1]\\n\\tensure_dir_is_templated(unrendered_dir)\\n\\tproject_dir = render_and_create_dir(unrendered_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcontext,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toutput_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toverwrite_if_exists)\\n\\tproject_dir = os.path.abspath(project_dir)\\n\\tlogging.debug('project_dir is {0}'.format(project_dir))\\n\\twith work_in(repo_dir):\\n\\t\\ttry:\\n\\t\\t\\trun_hook('pre_gen_project', project_dir, context)\\n\\t\\texcept FailedHookException:\\n\\t\\t\\tshutil.rmtree(project_dir, ignore_errors=True)\\n\\t\\t\\tlogging.error(\"Stopping generation because pre_gen_project\"\\n\\t\\t\\t\\t\\t\\t  \" hook script didn't exit sucessfully\")\\n\\t\\t\\treturn\\n\\twith work_in(template_dir):\\n\\t\\tenv = Environment(keep_trailing_newline=True)\\n\\t\\tenv.loader = FileSystemLoader('.')\\n\\t\\tfor root, dirs, files in os.walk('.'):\\n\\t\\t\\tcopy_dirs = []\\n\\t\\t\\trender_dirs = []\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\td_ = os.path.normpath(os.path.join(root, d))\\n\\t\\t\\t\\tif copy_without_render(d_, context):\\n\\t\\t\\t\\t\\tcopy_dirs.append(d)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\trender_dirs.append(d)\\n\\t\\t\\tfor copy_dir in copy_dirs:\\n\\t\\t\\t\\tindir = os.path.normpath(os.path.join(root, copy_dir))\\n\\t\\t\\t\\toutdir = os.path.normpath(os.path.join(project_dir, indir))\\n\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t'Copying dir {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t''.format(indir, outdir)\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tshutil.copytree(indir, outdir)\\n\\t\\t\\tdirs[:] = render_dirs\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\tunrendered_dir = os.path.join(project_dir, root, d)\\n\\t\\t\\t\\trender_and_create_dir(unrendered_dir, context, output_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  overwrite_if_exists)\\n\\t\\t\\tfor f in files:\\n\\t\\t\\t\\tinfile = os.path.normpath(os.path.join(root, f))\\n\\t\\t\\t\\tif copy_without_render(infile, context):\\n\\t\\t\\t\\t\\toutfile_tmpl = Template(infile)\\n\\t\\t\\t\\t\\toutfile_rendered = outfile_tmpl.render(**context)\\n\\t\\t\\t\\t\\toutfile = os.path.join(project_dir, outfile_rendered)\\n\\t\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t\\t'Copying file {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t\\t''.format(infile, outfile)\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tshutil.copyfile(infile, outfile)\\n\\t\\t\\t\\t\\tshutil.copymode(infile, outfile)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tlogging.debug('f is {0}'.format(f))\\n\\t\\t\\t\\tgenerate_file(project_dir, infile, context, env)\\n\\twith work_in(repo_dir):\\n\\t\\trun_hook('post_gen_project', project_dir, context)\\n\\treturn project_dir"
  },
  {
    "code": "def parse(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              thousands=None, chunksize=None):\\n        choose = {True:self._parse_xlsx,\\n                  False:self._parse_xls}\\n        return choose[self.use_xlsx](sheetname, header=header,\\n                                     skiprows=skiprows, index_col=index_col,\\n                                     parse_dates=parse_dates,\\n                                     date_parser=date_parser,\\n                                     na_values=na_values,\\n                                     thousands=thousands,\\n                                     chunksize=chunksize)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              thousands=None, chunksize=None):\\n        choose = {True:self._parse_xlsx,\\n                  False:self._parse_xls}\\n        return choose[self.use_xlsx](sheetname, header=header,\\n                                     skiprows=skiprows, index_col=index_col,\\n                                     parse_dates=parse_dates,\\n                                     date_parser=date_parser,\\n                                     na_values=na_values,\\n                                     thousands=thousands,\\n                                     chunksize=chunksize)"
  },
  {
    "code": "def _end_apply_index(self, dtindex):\\n        off = dtindex.to_perioddelta(\"D\")\\n        base, mult = libfrequencies.get_freq_code(self.freqstr)\\n        base_period = dtindex.to_period(base)\\n        if self.n > 0:\\n            normed = dtindex - off + Timedelta(1, \"D\") - Timedelta(1, \"ns\")\\n            roll = np.where(\\n                base_period.to_timestamp(how=\"end\") == normed, self.n, self.n - 1\\n            )\\n            shifted = base_period._addsub_int_array(roll, operator.add)\\n            base = shifted.to_timestamp(how=\"end\")\\n        else:\\n            roll = self.n\\n            base = base_period._time_shift(roll).to_timestamp(how=\"end\")\\n        return base + off + Timedelta(1, \"ns\") - Timedelta(1, \"D\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _end_apply_index(self, dtindex):\\n        off = dtindex.to_perioddelta(\"D\")\\n        base, mult = libfrequencies.get_freq_code(self.freqstr)\\n        base_period = dtindex.to_period(base)\\n        if self.n > 0:\\n            normed = dtindex - off + Timedelta(1, \"D\") - Timedelta(1, \"ns\")\\n            roll = np.where(\\n                base_period.to_timestamp(how=\"end\") == normed, self.n, self.n - 1\\n            )\\n            shifted = base_period._addsub_int_array(roll, operator.add)\\n            base = shifted.to_timestamp(how=\"end\")\\n        else:\\n            roll = self.n\\n            base = base_period._time_shift(roll).to_timestamp(how=\"end\")\\n        return base + off + Timedelta(1, \"ns\") - Timedelta(1, \"D\")"
  },
  {
    "code": "def _compute_plot_data(self):\\n        try:\\n            numeric_data = self.data._get_numeric_data()\\n        except AttributeError:\\n            numeric_data = self.data.convert_objects()\\n            if numeric_data.dtype == np.object_:\\n                raise TypeError('invalid dtype for plotting')\\n        try:\\n            is_empty = numeric_data.empty\\n        except AttributeError:\\n            is_empty = not len(numeric_data)\\n        if is_empty:\\n            raise TypeError('No numeric data to plot')\\n        self.data = numeric_data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: add release notes/whatsnew",
    "fixed_code": "def _compute_plot_data(self):\\n        from pandas.io.pytables import PerformanceWarning\\n        try:\\n            numeric_data = self.data._get_numeric_data()\\n        except AttributeError:\\n            numeric_data = self.data\\n            orig_dtype = numeric_data.dtype\\n            if orig_dtype == np.object_:\\n                numeric_data = numeric_data.convert_objects()\\n                num_data_dtype = numeric_data.dtype\\n                if num_data_dtype == np.object_:\\n                    raise TypeError('No numeric data to plot')\\n                else:\\n                    warnings.warn('Coerced object dtype to numeric dtype, '\\n                                  'you should avoid object dtyped Series if '\\n                                  'possible', PerformanceWarning)\\n        try:\\n            is_empty = numeric_data.empty\\n        except AttributeError:\\n            is_empty = not len(numeric_data)\\n        if is_empty:\\n            raise TypeError('No numeric data to plot')\\n        self.data = numeric_data"
  },
  {
    "code": "def weighted(y_true, y_pred, weights, mask=None):\\n\\t\\tscore_array = fn(y_true, y_pred)\\n\\t\\tif mask is not None:\\n\\t\\t\\tmask = K.cast(mask, K.floatx())\\n\\t\\t\\tscore_array *= mask\\n\\t\\t\\tscore_array /= K.mean(mask) + K.epsilon()\\n\\t\\tif weights is not None:\\n\\t\\t\\tndim = K.ndim(score_array)\\n\\t\\t\\tweight_ndim = K.ndim(weights)\\n\\t\\t\\tscore_array = K.mean(score_array,\\n\\t\\t\\t\\t\\t\\t\\t\\t axis=list(range(weight_ndim, ndim)))\\n\\t\\t\\tscore_array *= weights\\n\\t\\t\\tscore_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\\n\\t\\treturn K.mean(score_array)\\n\\treturn weighted",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def weighted(y_true, y_pred, weights, mask=None):\\n\\t\\tscore_array = fn(y_true, y_pred)\\n\\t\\tif mask is not None:\\n\\t\\t\\tmask = K.cast(mask, K.floatx())\\n\\t\\t\\tscore_array *= mask\\n\\t\\t\\tscore_array /= K.mean(mask) + K.epsilon()\\n\\t\\tif weights is not None:\\n\\t\\t\\tndim = K.ndim(score_array)\\n\\t\\t\\tweight_ndim = K.ndim(weights)\\n\\t\\t\\tscore_array = K.mean(score_array,\\n\\t\\t\\t\\t\\t\\t\\t\\t axis=list(range(weight_ndim, ndim)))\\n\\t\\t\\tscore_array *= weights\\n\\t\\t\\tscore_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\\n\\t\\treturn K.mean(score_array)\\n\\treturn weighted"
  },
  {
    "code": "def filesizeformat(bytes):\\n    try:\\n        bytes = float(bytes)\\n    except (TypeError,ValueError,UnicodeDecodeError):\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", 0) % {'size': 0}\\n    filesize_number_format = lambda value: formats.number_format(round(value, 1), 1)\\n    if bytes < 1024:\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\\n    if bytes < 1024 * 1024:\\n        return ugettext(\"%s KB\") % filesize_number_format(bytes / 1024)\\n    if bytes < 1024 * 1024 * 1024:\\n        return ugettext(\"%s MB\") % filesize_number_format(bytes / (1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%s GB\") % filesize_number_format(bytes / (1024 * 1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%s TB\") % filesize_number_format(bytes / (1024 * 1024 * 1024 * 1024))\\n    return ugettext(\"%s PB\") % filesize_number_format(bytes / (1024 * 1024 * 1024 * 1024 * 1024))\\nfilesizeformat.is_safe = True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def filesizeformat(bytes):\\n    try:\\n        bytes = float(bytes)\\n    except (TypeError,ValueError,UnicodeDecodeError):\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", 0) % {'size': 0}\\n    filesize_number_format = lambda value: formats.number_format(round(value, 1), 1)\\n    if bytes < 1024:\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\\n    if bytes < 1024 * 1024:\\n        return ugettext(\"%s KB\") % filesize_number_format(bytes / 1024)\\n    if bytes < 1024 * 1024 * 1024:\\n        return ugettext(\"%s MB\") % filesize_number_format(bytes / (1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%s GB\") % filesize_number_format(bytes / (1024 * 1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%s TB\") % filesize_number_format(bytes / (1024 * 1024 * 1024 * 1024))\\n    return ugettext(\"%s PB\") % filesize_number_format(bytes / (1024 * 1024 * 1024 * 1024 * 1024))\\nfilesizeformat.is_safe = True"
  },
  {
    "code": "def wait_for_done(self):\\n        while True:\\n            for job in self._jobs:\\n                if job and 'currentState' in job:\\n                    if self.check_dataflow_job_state(job):\\n                        return True\\n                else:\\n                    time.sleep(self._poll_sleep)\\n            self._jobs = self._get_jobs()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def wait_for_done(self):\\n        while True:\\n            for job in self._jobs:\\n                if job and 'currentState' in job:\\n                    if self.check_dataflow_job_state(job):\\n                        return True\\n                else:\\n                    time.sleep(self._poll_sleep)\\n            self._jobs = self._get_jobs()"
  },
  {
    "code": "def get_conn(self):\\n        conn = self.get_connection(self.postgres_conn_id)\\n        conn_args = dict(\\n            host=conn.host,\\n            user=conn.login,\\n            password=conn.password,\\n            dbname=self.schema or conn.schema,\\n            port=conn.port)\\n        for arg_name, arg_val in conn.extra_dejson.items():\\n            if arg_name in ['sslmode', 'sslcert', 'sslkey',\\n                            'sslrootcert', 'sslcrl', 'application_name',\\n                            'keepalives_idle']:\\n                conn_args[arg_name] = arg_val\\n        self.conn = psycopg2.connect(**conn_args)\\n        return self.conn",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4417] Add AWS IAM authenication for PostgresHook (#5223)\\n\\nEnhance the exisitng PostgresHook to allow for IAM authentication for\\nRDS Postgres and Redshift.",
    "fixed_code": "def get_conn(self):\\n        conn = self.get_connection(self.postgres_conn_id)\\n        if conn.extra_dejson.get('iam', False):\\n            conn.login, conn.password, conn.port = self.get_iam_token(conn)\\n        conn_args = dict(\\n            host=conn.host,\\n            user=conn.login,\\n            password=conn.password,\\n            dbname=self.schema or conn.schema,\\n            port=conn.port)\\n        for arg_name, arg_val in conn.extra_dejson.items():\\n            if arg_name in ['sslmode', 'sslcert', 'sslkey',\\n                            'sslrootcert', 'sslcrl', 'application_name',\\n                            'keepalives_idle']:\\n                conn_args[arg_name] = arg_val\\n        self.conn = psycopg2.connect(**conn_args)\\n        return self.conn"
  },
  {
    "code": "def task_test(args, dag=None):\\n    settings.MASK_SECRETS_IN_LOGS = True\\n    handlers = logging.getLogger('airflow.task').handlers\\n    already_has_stream_handler = False\\n    for handler in handlers:\\n        already_has_stream_handler = isinstance(handler, logging.StreamHandler)\\n        if already_has_stream_handler:\\n            break\\n    if not already_has_stream_handler:\\n        logging.getLogger('airflow.task').propagate = True\\n    env_vars = {'AIRFLOW_TEST_MODE': 'True'}\\n    if args.env_vars:\\n        env_vars.update(args.env_vars)\\n        os.environ.update(env_vars)\\n    dag = dag or get_dag(args.subdir, args.dag_id)\\n    task = dag.get_task(task_id=args.task_id)\\n    if args.task_params:\\n        passed_in_params = json.loads(args.task_params)\\n        task.params.update(passed_in_params)\\n    if task.params:\\n        task.params.validate()\\n    ti, dr_created = _get_ti(task, args.execution_date_or_run_id, args.map_index, create_if_necessary=\"db\")\\n    try:\\n        with redirect_stdout(RedactedIO()):\\n            if args.dry_run:\\n                ti.dry_run()\\n            else:\\n                ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)\\n    except Exception:\\n        if args.post_mortem:\\n            debugger = _guess_debugger()\\n            debugger.post_mortem()\\n        else:\\n            raise\\n    finally:\\n        if not already_has_stream_handler:\\n            logging.getLogger('airflow.task').propagate = False\\n        if dr_created:\\n            with create_session() as session:\\n                session.delete(ti.dag_run)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def task_test(args, dag=None):\\n    settings.MASK_SECRETS_IN_LOGS = True\\n    handlers = logging.getLogger('airflow.task').handlers\\n    already_has_stream_handler = False\\n    for handler in handlers:\\n        already_has_stream_handler = isinstance(handler, logging.StreamHandler)\\n        if already_has_stream_handler:\\n            break\\n    if not already_has_stream_handler:\\n        logging.getLogger('airflow.task').propagate = True\\n    env_vars = {'AIRFLOW_TEST_MODE': 'True'}\\n    if args.env_vars:\\n        env_vars.update(args.env_vars)\\n        os.environ.update(env_vars)\\n    dag = dag or get_dag(args.subdir, args.dag_id)\\n    task = dag.get_task(task_id=args.task_id)\\n    if args.task_params:\\n        passed_in_params = json.loads(args.task_params)\\n        task.params.update(passed_in_params)\\n    if task.params:\\n        task.params.validate()\\n    ti, dr_created = _get_ti(task, args.execution_date_or_run_id, args.map_index, create_if_necessary=\"db\")\\n    try:\\n        with redirect_stdout(RedactedIO()):\\n            if args.dry_run:\\n                ti.dry_run()\\n            else:\\n                ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)\\n    except Exception:\\n        if args.post_mortem:\\n            debugger = _guess_debugger()\\n            debugger.post_mortem()\\n        else:\\n            raise\\n    finally:\\n        if not already_has_stream_handler:\\n            logging.getLogger('airflow.task').propagate = False\\n        if dr_created:\\n            with create_session() as session:\\n                session.delete(ti.dag_run)"
  },
  {
    "code": "def makelist(little_hand, big_hand, seconds_hand):\\n\\ttotal = []\\n\\tif Gl.cyan or not Gl.colorsubset:\\n\\t\\ttotal = total + makesublist(big_hand, Gl.indices[0])\\n\\tif Gl.magenta or not Gl.colorsubset:\\n\\t\\ttotal = total + makesublist(little_hand, Gl.indices[1])\\n\\tif Gl.yellow or not Gl.colorsubset:\\n\\t\\ttotal = total + makesublist(MIDN, Gl.indices[2])\\n\\ttotal.sort()\\n\\treturn total",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "*** empty log message ***",
    "fixed_code": "def makelist(hands):\\n\\tlittle_hand, big_hand, seconds_hand = hands\\n\\ttotal = []\\n\\tif Gl.cyan or not Gl.colorsubset:\\n\\t\\ttotal = total + makesublist(big_hand, Gl.indices[0])\\n\\tif Gl.magenta or not Gl.colorsubset:\\n\\t\\ttotal = total + makesublist(little_hand, Gl.indices[1])\\n\\tif Gl.yellow or not Gl.colorsubset:\\n\\t\\ttotal = total + makesublist(MIDN, Gl.indices[2])\\n\\ttotal.sort()\\n\\treturn total"
  },
  {
    "code": "def __hash__(self):\\n        return hash((\\n            self.creation_counter,\\n            self.model._meta.app_label if hasattr(self, 'model') else None,\\n            self.model._meta.model_name if hasattr(self, 'model') else None,\\n        ))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #33441 -- Restored immutability of models.Field.__hash__().\\n\\nRegression in 502e75f9ed5476ffe8229109acf0c23999d4b533.",
    "fixed_code": "def __hash__(self):\\n        return hash(self.creation_counter)"
  },
  {
    "code": "def __init__(self, required=True, widget=None, label=None):\\n        self.required, self.label = required, label\\n        widget = widget or self.widget\\n        if isinstance(widget, type):\\n            widget = widget()\\n        extra_attrs = self.widget_attrs(widget)\\n        if extra_attrs:\\n            widget.attrs.update(extra_attrs)\\n        self.widget = widget\\n        self.creation_counter = Field.creation_counter\\n        Field.creation_counter += 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, required=True, widget=None, label=None):\\n        self.required, self.label = required, label\\n        widget = widget or self.widget\\n        if isinstance(widget, type):\\n            widget = widget()\\n        extra_attrs = self.widget_attrs(widget)\\n        if extra_attrs:\\n            widget.attrs.update(extra_attrs)\\n        self.widget = widget\\n        self.creation_counter = Field.creation_counter\\n        Field.creation_counter += 1"
  },
  {
    "code": "def make_safe_label_value(string):\\n    safe_label = re.sub(r\"^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\", \"\", string)\\n    if len(safe_label) > MAX_LABEL_LEN or string != safe_label:\\n        safe_hash = hashlib.md5(string.encode()).hexdigest()[:9]\\n        safe_label = safe_label[: MAX_LABEL_LEN - len(safe_hash) - 1] + \"-\" + safe_hash\\n    return safe_label",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "More typing and minor refactor for kubernetes (#24719)",
    "fixed_code": "def make_safe_label_value(string: str) -> str:\\n    safe_label = re.sub(r\"^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\", \"\", string)\\n    if len(safe_label) > MAX_LABEL_LEN or string != safe_label:\\n        safe_hash = hashlib.md5(string.encode()).hexdigest()[:9]\\n        safe_label = safe_label[: MAX_LABEL_LEN - len(safe_hash) - 1] + \"-\" + safe_hash\\n    return safe_label"
  },
  {
    "code": "def loop_fn(i):\\n\\tgathered_elems = nest.map_structure(lambda x: array_ops.gather(x, i), elems)\\n\\treturn fn(gathered_elems)\\n  batch_size = None\\n  first_elem = ops.convert_to_tensor(nest.flatten(elems)[0])\\n  if first_elem.shape.rank is not None:\\n\\tbatch_size = first_elem.shape.as_list()[0]\\n  if batch_size is None:\\n\\tbatch_size = array_ops.shape(first_elem)[0]\\n  return pfor(loop_fn, batch_size)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def loop_fn(i):\\n\\tgathered_elems = nest.map_structure(lambda x: array_ops.gather(x, i), elems)\\n\\treturn fn(gathered_elems)\\n  batch_size = None\\n  first_elem = ops.convert_to_tensor(nest.flatten(elems)[0])\\n  if first_elem.shape.rank is not None:\\n\\tbatch_size = first_elem.shape.as_list()[0]\\n  if batch_size is None:\\n\\tbatch_size = array_ops.shape(first_elem)[0]\\n  return pfor(loop_fn, batch_size)"
  },
  {
    "code": "def as_oracle(self, compiler, connection, **extra_context):\\n        if not self.is_extent:\\n            tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\\n            clone = self.copy()\\n            clone.set_source_expressions([\\n                *self.get_source_expressions(),\\n                Value(tolerance),\\n            ])\\n            template = '%(function)s(SDOAGGRTYPE(%(expressions)s))'\\n            return clone.as_sql(compiler, connection, template=template, **extra_context)\\n        return self.as_sql(compiler, connection, **extra_context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_oracle(self, compiler, connection, **extra_context):\\n        if not self.is_extent:\\n            tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\\n            clone = self.copy()\\n            clone.set_source_expressions([\\n                *self.get_source_expressions(),\\n                Value(tolerance),\\n            ])\\n            template = '%(function)s(SDOAGGRTYPE(%(expressions)s))'\\n            return clone.as_sql(compiler, connection, template=template, **extra_context)\\n        return self.as_sql(compiler, connection, **extra_context)"
  },
  {
    "code": "def readable(self):\\n        if self.closed:\\n            raise ValueError(\"I/O operation on closed socket.\")\\n        return self._reading",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def readable(self):\\n        if self.closed:\\n            raise ValueError(\"I/O operation on closed socket.\")\\n        return self._reading"
  },
  {
    "code": "def getlist(self, key):\\n        for dict_ in self.dicts:\\n            try:\\n                return dict_.getlist(key)\\n            except KeyError:\\n                pass\\n        raise KeyError",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #6465 -- Tweaked MergeDict.getlist() to work with Django's MultiValueDict class. Thanks, Matt McClanahan.",
    "fixed_code": "def getlist(self, key):\\n        for dict_ in self.dicts:\\n            if key in dict_.keys():\\n                return dict_.getlist(key)\\n        return []"
  },
  {
    "code": "def getint(self, section, key, **kwargs):\\n        return int(self.get(section, key, **kwargs))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getint(self, section, key, **kwargs):\\n        return int(self.get(section, key, **kwargs))"
  },
  {
    "code": "def _calltip_window(parent):  \\n    from tkinter import Toplevel, Text, LEFT, BOTH\\n    top = Toplevel(parent)\\n    top.title(\"Test calltips\")\\n    x, y = map(int, parent.geometry().split('+')[1:])\\n    top.geometry(\"200x100+%d+%d\" % (x + 250, y + 175))\\n    text = Text(top)\\n    text.pack(side=LEFT, fill=BOTH, expand=1)\\n    text.insert(\"insert\", \"string.split\")\\n    top.update()\\n    calltip = CallTip(text)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _calltip_window(parent):  \\n    from tkinter import Toplevel, Text, LEFT, BOTH\\n    top = Toplevel(parent)\\n    top.title(\"Test calltips\")\\n    x, y = map(int, parent.geometry().split('+')[1:])\\n    top.geometry(\"200x100+%d+%d\" % (x + 250, y + 175))\\n    text = Text(top)\\n    text.pack(side=LEFT, fill=BOTH, expand=1)\\n    text.insert(\"insert\", \"string.split\")\\n    top.update()\\n    calltip = CallTip(text)"
  },
  {
    "code": "def decodestring(s, header = 0):\\n    if a2b_qp is not None:\\n        return a2b_qp(s, header = header)\\n    from io import BytesIO\\n    infp = BytesIO(s)\\n    outfp = BytesIO()\\n    decode(infp, outfp, header = header)\\n    return outfp.getvalue()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decodestring(s, header = 0):\\n    if a2b_qp is not None:\\n        return a2b_qp(s, header = header)\\n    from io import BytesIO\\n    infp = BytesIO(s)\\n    outfp = BytesIO()\\n    decode(infp, outfp, header = header)\\n    return outfp.getvalue()"
  },
  {
    "code": "def close(self):\\n        self._sock = _closedsocket()\\n        dummy = self._sock._dummy\\n        for method in _delegate_methods:\\n            setattr(self, method, dummy)\\n    close.__doc__ = _realsocket.close.__doc__",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def close(self):\\n        self._sock = _closedsocket()\\n        dummy = self._sock._dummy\\n        for method in _delegate_methods:\\n            setattr(self, method, dummy)\\n    close.__doc__ = _realsocket.close.__doc__"
  },
  {
    "code": "def _create_vm(self, method, *args, **kwargs):\\n\\t\\tconfig_spec = kwargs.get(\"config\")\\n\\t\\tds = _db_content[\"Datastore\"][_db_content[\"Datastore\"].keys()[0]]\\n\\t\\tvm_dict = {\"name\": config_spec.name,\\n\\t\\t\\t\\t  \"ds\": ds,\\n\\t\\t\\t\\t  \"powerstate\": \"poweredOff\",\\n\\t\\t\\t\\t  \"vmPathName\": config_spec.files.vmPathName,\\n\\t\\t\\t\\t  \"numCpu\": config_spec.numCPUs,\\n\\t\\t\\t\\t  \"mem\": config_spec.memoryMB}\\n\\t\\tvirtual_machine = VirtualMachine(**vm_dict)\\n\\t\\t_create_object(\"VirtualMachine\", virtual_machine)\\n\\t\\ttask_mdo = create_task(method, \"success\")\\n\\t\\treturn task_mdo.obj",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "correctly set iface-id in vmware driver\\n\\nbug 1183452\\n\\nCommit 70c659059b8fbef811ce79700aecb01c60242ebd updated the vmware\\ndriver to use the new vif model, but was incorrect in how it tried to\\ngrab the iface-id and set it in the vmx file spec.  This patch fixes\\nthat issue and adds a check for this in the existing unit tests.",
    "fixed_code": "def _create_vm(self, method, *args, **kwargs):\\n\\t\\tconfig_spec = kwargs.get(\"config\")\\n\\t\\tds = _db_content[\"Datastore\"][_db_content[\"Datastore\"].keys()[0]]\\n\\t\\tvm_dict = {\"name\": config_spec.name,\\n\\t\\t\\t\\t  \"ds\": ds,\\n\\t\\t\\t\\t  \"powerstate\": \"poweredOff\",\\n\\t\\t\\t\\t  \"vmPathName\": config_spec.files.vmPathName,\\n\\t\\t\\t\\t  \"numCpu\": config_spec.numCPUs,\\n\\t\\t\\t\\t  \"mem\": config_spec.memoryMB,\\n\\t\\t\\t\\t  \"extra_config\": config_spec.extraConfig}\\n\\t\\tvirtual_machine = VirtualMachine(**vm_dict)\\n\\t\\t_create_object(\"VirtualMachine\", virtual_machine)\\n\\t\\ttask_mdo = create_task(method, \"success\")\\n\\t\\treturn task_mdo.obj"
  },
  {
    "code": "def missing_required_lib(library, reason=None, url=None):\\n    hostname = platform.node()\\n    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\\n    if reason:\\n        msg += \" This is required %s.\" % reason\\n    if url:\\n        msg += \" See %s for more info.\" % url\\n    msg += (\" Please read module documentation and install in the appropriate location.\"\\n            \" If the required library is installed, but Ansible is using the wrong Python interpreter,\"\\n            \" please consult the documentation on ansible_python_interpreter\")\\n    return msg",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def missing_required_lib(library, reason=None, url=None):\\n    hostname = platform.node()\\n    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\\n    if reason:\\n        msg += \" This is required %s.\" % reason\\n    if url:\\n        msg += \" See %s for more info.\" % url\\n    msg += (\" Please read module documentation and install in the appropriate location.\"\\n            \" If the required library is installed, but Ansible is using the wrong Python interpreter,\"\\n            \" please consult the documentation on ansible_python_interpreter\")\\n    return msg"
  },
  {
    "code": "def create_volume(self):\\n        ''\\n        if self.parameters.get('aggregate_name') is None:\\n            self.module.fail_json(msg='Error provisioning volume %s: aggregate_name is required'\\n                                  % self.parameters['name'])\\n        options = {'volume': self.parameters['name'],\\n                   'containing-aggr-name': self.parameters['aggregate_name'],\\n                   'size': str(self.parameters['size'])}\\n        if self.parameters.get('percent_snapshot_space'):\\n            options['percentage-snapshot-reserve'] = self.parameters['percent_snapshot_space']\\n        if self.parameters.get('type'):\\n            options['volume-type'] = self.parameters['type']\\n        if self.parameters.get('policy'):\\n            options['export-policy'] = self.parameters['policy']\\n        if self.parameters.get('junction_path'):\\n            options['junction-path'] = self.parameters['junction_path']\\n        if self.parameters.get('space_guarantee'):\\n            options['space-reserve'] = self.parameters['space_guarantee']\\n        if self.parameters.get('volume_security_style'):\\n            options['volume-security-style'] = self.parameters['volume_security_style']\\n        if self.parameters.get('unix_permissions'):\\n            options['unix-permissions'] = self.parameters['unix_permissions']\\n        if self.parameters.get('snapshot_policy'):\\n            options['snapshot-policy'] = self.parameters['snapshot_policy']\\n        volume_create = netapp_utils.zapi.NaElement.create_node_with_children('volume-create', **options)\\n        try:\\n            self.server.invoke_successfully(volume_create,\\n                                            enable_tunneling=True)\\n            self.ems_log_event(\"volume-create\")\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error provisioning volume %s \\\\n                                  of size %s: %s'\\n                                  % (self.parameters['name'], self.parameters['size'], to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_volume(self):\\n        ''\\n        if self.parameters.get('aggregate_name') is None:\\n            self.module.fail_json(msg='Error provisioning volume %s: aggregate_name is required'\\n                                  % self.parameters['name'])\\n        options = {'volume': self.parameters['name'],\\n                   'containing-aggr-name': self.parameters['aggregate_name'],\\n                   'size': str(self.parameters['size'])}\\n        if self.parameters.get('percent_snapshot_space'):\\n            options['percentage-snapshot-reserve'] = self.parameters['percent_snapshot_space']\\n        if self.parameters.get('type'):\\n            options['volume-type'] = self.parameters['type']\\n        if self.parameters.get('policy'):\\n            options['export-policy'] = self.parameters['policy']\\n        if self.parameters.get('junction_path'):\\n            options['junction-path'] = self.parameters['junction_path']\\n        if self.parameters.get('space_guarantee'):\\n            options['space-reserve'] = self.parameters['space_guarantee']\\n        if self.parameters.get('volume_security_style'):\\n            options['volume-security-style'] = self.parameters['volume_security_style']\\n        if self.parameters.get('unix_permissions'):\\n            options['unix-permissions'] = self.parameters['unix_permissions']\\n        if self.parameters.get('snapshot_policy'):\\n            options['snapshot-policy'] = self.parameters['snapshot_policy']\\n        volume_create = netapp_utils.zapi.NaElement.create_node_with_children('volume-create', **options)\\n        try:\\n            self.server.invoke_successfully(volume_create,\\n                                            enable_tunneling=True)\\n            self.ems_log_event(\"volume-create\")\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error provisioning volume %s \\\\n                                  of size %s: %s'\\n                                  % (self.parameters['name'], self.parameters['size'], to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def _get_grouper(obj, key=None, axis=0, level=None, sort=True):\\n    group_axis = obj._get_axis(axis)\\n    if level is not None:\\n        if not isinstance(group_axis, MultiIndex):\\n            if isinstance(level, basestring):\\n                if obj.index.name != level:\\n                    raise ValueError('level name %s is not the name of the index' % level)\\n            elif level > 0:\\n                raise ValueError('level > 0 only valid with MultiIndex')\\n            level = None\\n            key = group_axis\\n    if isinstance(key, CustomGrouper):\\n        gpr = key.get_grouper(obj)\\n        return gpr, []\\n    elif isinstance(key, Grouper):\\n        return key, []\\n    if not isinstance(key, (tuple, list)):\\n        keys = [key]\\n    else:\\n        keys = key\\n    match_axis_length = len(keys) == len(group_axis)\\n    any_callable = any(callable(g) or isinstance(g, dict) for g in keys)\\n    any_arraylike = any(isinstance(g, (list, tuple, np.ndarray))\\n                        for g in keys)\\n    try:\\n        if isinstance(obj, DataFrame):\\n            all_in_columns = all(g in obj.columns for g in keys)\\n        else:\\n            all_in_columns = False\\n    except Exception:\\n        all_in_columns = False\\n    if (not any_callable and not all_in_columns\\n        and not any_arraylike and match_axis_length\\n            and level is None):\\n        keys = [com._asarray_tuplesafe(keys)]\\n    if isinstance(level, (tuple, list)):\\n        if key is None:\\n            keys = [None] * len(level)\\n        levels = level\\n    else:\\n        levels = [level] * len(keys)\\n    groupings = []\\n    exclusions = []\\n    for i, (gpr, level) in enumerate(izip(keys, levels)):\\n        name = None\\n        try:\\n            obj._data.items.get_loc(gpr)\\n            in_axis = True\\n        except Exception:\\n            in_axis = False\\n        if _is_label_like(gpr) or in_axis:\\n            exclusions.append(gpr)\\n            name = gpr\\n            gpr = obj[gpr]\\n        if (isinstance(gpr,Categorical) and len(gpr) != len(obj)):\\n            errmsg = \"Categorical grouper must have len(grouper) == len(data)\"\\n            raise AssertionError(errmsg)\\n        ping = Grouping(group_axis, gpr, name=name, level=level, sort=sort)\\n        groupings.append(ping)\\n    if len(groupings) == 0:\\n        raise ValueError('No group keys passed!')\\n    grouper = Grouper(group_axis, groupings, sort=sort)\\n    return grouper, exclusions",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_grouper(obj, key=None, axis=0, level=None, sort=True):\\n    group_axis = obj._get_axis(axis)\\n    if level is not None:\\n        if not isinstance(group_axis, MultiIndex):\\n            if isinstance(level, basestring):\\n                if obj.index.name != level:\\n                    raise ValueError('level name %s is not the name of the index' % level)\\n            elif level > 0:\\n                raise ValueError('level > 0 only valid with MultiIndex')\\n            level = None\\n            key = group_axis\\n    if isinstance(key, CustomGrouper):\\n        gpr = key.get_grouper(obj)\\n        return gpr, []\\n    elif isinstance(key, Grouper):\\n        return key, []\\n    if not isinstance(key, (tuple, list)):\\n        keys = [key]\\n    else:\\n        keys = key\\n    match_axis_length = len(keys) == len(group_axis)\\n    any_callable = any(callable(g) or isinstance(g, dict) for g in keys)\\n    any_arraylike = any(isinstance(g, (list, tuple, np.ndarray))\\n                        for g in keys)\\n    try:\\n        if isinstance(obj, DataFrame):\\n            all_in_columns = all(g in obj.columns for g in keys)\\n        else:\\n            all_in_columns = False\\n    except Exception:\\n        all_in_columns = False\\n    if (not any_callable and not all_in_columns\\n        and not any_arraylike and match_axis_length\\n            and level is None):\\n        keys = [com._asarray_tuplesafe(keys)]\\n    if isinstance(level, (tuple, list)):\\n        if key is None:\\n            keys = [None] * len(level)\\n        levels = level\\n    else:\\n        levels = [level] * len(keys)\\n    groupings = []\\n    exclusions = []\\n    for i, (gpr, level) in enumerate(izip(keys, levels)):\\n        name = None\\n        try:\\n            obj._data.items.get_loc(gpr)\\n            in_axis = True\\n        except Exception:\\n            in_axis = False\\n        if _is_label_like(gpr) or in_axis:\\n            exclusions.append(gpr)\\n            name = gpr\\n            gpr = obj[gpr]\\n        if (isinstance(gpr,Categorical) and len(gpr) != len(obj)):\\n            errmsg = \"Categorical grouper must have len(grouper) == len(data)\"\\n            raise AssertionError(errmsg)\\n        ping = Grouping(group_axis, gpr, name=name, level=level, sort=sort)\\n        groupings.append(ping)\\n    if len(groupings) == 0:\\n        raise ValueError('No group keys passed!')\\n    grouper = Grouper(group_axis, groupings, sort=sort)\\n    return grouper, exclusions"
  },
  {
    "code": "def _nanpercentile_1d(\\n    values: np.ndarray,\\n    mask: npt.NDArray[np.bool_],\\n    qs: npt.NDArray[np.float64],\\n    na_value: Scalar,\\n    interpolation,\\n) -> Scalar | np.ndarray:\\n    values = values[~mask]\\n    if len(values) == 0:\\n        return np.full(len(qs), na_value)\\n    return np.percentile(values, qs, **{np_percentile_argname: interpolation})",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _nanpercentile_1d(\\n    values: np.ndarray,\\n    mask: npt.NDArray[np.bool_],\\n    qs: npt.NDArray[np.float64],\\n    na_value: Scalar,\\n    interpolation,\\n) -> Scalar | np.ndarray:\\n    values = values[~mask]\\n    if len(values) == 0:\\n        return np.full(len(qs), na_value)\\n    return np.percentile(values, qs, **{np_percentile_argname: interpolation})"
  },
  {
    "code": "def get_sources(\\n    *,\\n    ctx: click.Context,\\n    src: Tuple[str, ...],\\n    quiet: bool,\\n    verbose: bool,\\n    include: Pattern[str],\\n    exclude: Optional[Pattern[str]],\\n    extend_exclude: Optional[Pattern[str]],\\n    force_exclude: Optional[Pattern[str]],\\n    report: \"Report\",\\n    stdin_filename: Optional[str],\\n) -> Set[Path]:\\n    sources: Set[Path] = set()\\n    path_empty(src, \"No Path provided. Nothing to do \ud83d\ude34\", quiet, verbose, ctx)\\n    if exclude is None:\\n        exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES)\\n        gitignore = get_gitignore(ctx.obj[\"root\"])\\n    else:\\n        gitignore = None\\n    for s in src:\\n        if s == \"-\" and stdin_filename:\\n            p = Path(stdin_filename)\\n            is_stdin = True\\n        else:\\n            p = Path(s)\\n            is_stdin = False\\n        if is_stdin or p.is_file():\\n            normalized_path = normalize_path_maybe_ignore(p, ctx.obj[\"root\"], report)\\n            if normalized_path is None:\\n                continue\\n            normalized_path = \"/\" + normalized_path\\n            if force_exclude:\\n                force_exclude_match = force_exclude.search(normalized_path)\\n            else:\\n                force_exclude_match = None\\n            if force_exclude_match and force_exclude_match.group(0):\\n                report.path_ignored(p, \"matches the --force-exclude regular expression\")\\n                continue\\n            if is_stdin:\\n                p = Path(f\"{STDIN_PLACEHOLDER}{str(p)}\")\\n            if p.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\\n                verbose=verbose, quiet=quiet\\n            ):\\n                continue\\n            sources.add(p)\\n        elif p.is_dir():\\n            sources.update(\\n                gen_python_files(\\n                    p.iterdir(),\\n                    ctx.obj[\"root\"],\\n                    include,\\n                    exclude,\\n                    extend_exclude,\\n                    force_exclude,\\n                    report,\\n                    gitignore,\\n                    verbose=verbose,\\n                    quiet=quiet,\\n                )\\n            )\\n        elif s == \"-\":\\n            sources.add(p)\\n        else:\\n            err(f\"invalid path: {s}\")\\n    return sources",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make SRC or code mandatory and mutually exclusive (#2360) (#2804)\\n\\nCloses #2360: I'd like to make passing SRC or `--code` mandatory and the arguments mutually exclusive. This will change our (partially already broken) promises of CLI behavior, but I'll comment below.",
    "fixed_code": "def get_sources(\\n    *,\\n    ctx: click.Context,\\n    src: Tuple[str, ...],\\n    quiet: bool,\\n    verbose: bool,\\n    include: Pattern[str],\\n    exclude: Optional[Pattern[str]],\\n    extend_exclude: Optional[Pattern[str]],\\n    force_exclude: Optional[Pattern[str]],\\n    report: \"Report\",\\n    stdin_filename: Optional[str],\\n) -> Set[Path]:\\n    sources: Set[Path] = set()\\n    if exclude is None:\\n        exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES)\\n        gitignore = get_gitignore(ctx.obj[\"root\"])\\n    else:\\n        gitignore = None\\n    for s in src:\\n        if s == \"-\" and stdin_filename:\\n            p = Path(stdin_filename)\\n            is_stdin = True\\n        else:\\n            p = Path(s)\\n            is_stdin = False\\n        if is_stdin or p.is_file():\\n            normalized_path = normalize_path_maybe_ignore(p, ctx.obj[\"root\"], report)\\n            if normalized_path is None:\\n                continue\\n            normalized_path = \"/\" + normalized_path\\n            if force_exclude:\\n                force_exclude_match = force_exclude.search(normalized_path)\\n            else:\\n                force_exclude_match = None\\n            if force_exclude_match and force_exclude_match.group(0):\\n                report.path_ignored(p, \"matches the --force-exclude regular expression\")\\n                continue\\n            if is_stdin:\\n                p = Path(f\"{STDIN_PLACEHOLDER}{str(p)}\")\\n            if p.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\\n                verbose=verbose, quiet=quiet\\n            ):\\n                continue\\n            sources.add(p)\\n        elif p.is_dir():\\n            sources.update(\\n                gen_python_files(\\n                    p.iterdir(),\\n                    ctx.obj[\"root\"],\\n                    include,\\n                    exclude,\\n                    extend_exclude,\\n                    force_exclude,\\n                    report,\\n                    gitignore,\\n                    verbose=verbose,\\n                    quiet=quiet,\\n                )\\n            )\\n        elif s == \"-\":\\n            sources.add(p)\\n        else:\\n            err(f\"invalid path: {s}\")\\n    return sources"
  },
  {
    "code": "def __lt__(self, other):\\n        if isinstance(other, timedelta):\\n            return self._cmp(other) < 0\\n        else:\\n            return NotImplemented",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __lt__(self, other):\\n        if isinstance(other, timedelta):\\n            return self._cmp(other) < 0\\n        else:\\n            return NotImplemented"
  },
  {
    "code": "def _unconvert_index_legacy(data, kind, legacy=False):\\n    if kind == 'datetime':\\n        index = lib.time64_to_datetime(data)\\n    elif kind in ('string', 'integer'):\\n        index = np.array(data, dtype=object)\\n    else:  \\n        raise ValueError('unrecognized index type %s' % kind)\\n    return index",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: provide py3k string decoding and compat",
    "fixed_code": "def _unconvert_index_legacy(data, kind, legacy=False):\\n    if kind == u'datetime':\\n        index = lib.time64_to_datetime(data)\\n    elif kind in (u'string', u'integer'):\\n        index = np.array(data, dtype=object)\\n    else:  \\n        raise ValueError('unrecognized index type %s' % kind)\\n    return index"
  },
  {
    "code": "def close(self):\\n        for closable in self._closable_objects:\\n            closable.close()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def close(self):\\n        for closable in self._closable_objects:\\n            closable.close()"
  },
  {
    "code": "def include(arg, namespace=None):\\n    app_name = None\\n    if isinstance(arg, tuple):\\n        try:\\n            urlconf_module, app_name = arg\\n        except ValueError:\\n            if namespace:\\n                raise ImproperlyConfigured(\\n                    'Cannot override the namespace for a dynamic module that provides a namespace'\\n                )\\n            raise ImproperlyConfigured(\\n                'Passing a 3-tuple to django.conf.urls.include() is not supported. '\\n                'Pass a 2-tuple containing the list of patterns and app_name, '\\n                'and provide the namespace argument to include() instead.',\\n            )\\n    else:\\n        urlconf_module = arg\\n    if isinstance(urlconf_module, str):\\n        urlconf_module = import_module(urlconf_module)\\n    patterns = getattr(urlconf_module, 'urlpatterns', urlconf_module)\\n    app_name = getattr(urlconf_module, 'app_name', app_name)\\n    if namespace and not app_name:\\n        raise ImproperlyConfigured(\\n            'Specifying a namespace in django.conf.urls.include() without '\\n            'providing an app_name is not supported. Set the app_name attribute '\\n            'in the included module, or pass a 2-tuple containing the list of '\\n            'patterns and app_name instead.',\\n        )\\n    namespace = namespace or app_name\\n    if isinstance(patterns, (list, tuple)):\\n        for url_pattern in patterns:\\n            if isinstance(url_pattern, LocaleRegexURLResolver):\\n                raise ImproperlyConfigured(\\n                    'Using i18n_patterns in an included URLconf is not allowed.')\\n    return (urlconf_module, app_name, namespace)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Improved test coverage and error messages for conf.urls.__init__.",
    "fixed_code": "def include(arg, namespace=None):\\n    app_name = None\\n    if isinstance(arg, tuple):\\n        try:\\n            urlconf_module, app_name = arg\\n        except ValueError:\\n            if namespace:\\n                raise ImproperlyConfigured(\\n                    'Cannot override the namespace for a dynamic module that '\\n                    'provides a namespace.'\\n                )\\n            raise ImproperlyConfigured(\\n                'Passing a %d-tuple to django.conf.urls.include() is not supported. '\\n                'Pass a 2-tuple containing the list of patterns and app_name, '\\n                'and provide the namespace argument to include() instead.' % len(arg)\\n            )\\n    else:\\n        urlconf_module = arg\\n    if isinstance(urlconf_module, str):\\n        urlconf_module = import_module(urlconf_module)\\n    patterns = getattr(urlconf_module, 'urlpatterns', urlconf_module)\\n    app_name = getattr(urlconf_module, 'app_name', app_name)\\n    if namespace and not app_name:\\n        raise ImproperlyConfigured(\\n            'Specifying a namespace in django.conf.urls.include() without '\\n            'providing an app_name is not supported. Set the app_name attribute '\\n            'in the included module, or pass a 2-tuple containing the list of '\\n            'patterns and app_name instead.',\\n        )\\n    namespace = namespace or app_name\\n    if isinstance(patterns, (list, tuple)):\\n        for url_pattern in patterns:\\n            if isinstance(url_pattern, LocaleRegexURLResolver):\\n                raise ImproperlyConfigured(\\n                    'Using i18n_patterns in an included URLconf is not allowed.')\\n    return (urlconf_module, app_name, namespace)"
  },
  {
    "code": "def as_ul(self):\\n        \"Returns this form rendered as HTML <li>s -- excluding the <ul></ul>.\"\\n        output = []\\n        if self.errors.get(NON_FIELD_ERRORS):\\n            output.append(u'<li>%s</li>' % self.non_field_errors())\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = u'<li>'\\n            if bf.errors:\\n                line += str(bf.errors)\\n            line += u'%s %s</li>' % (bf.label_tag(bf.verbose_name+':'), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_ul(self):\\n        \"Returns this form rendered as HTML <li>s -- excluding the <ul></ul>.\"\\n        output = []\\n        if self.errors.get(NON_FIELD_ERRORS):\\n            output.append(u'<li>%s</li>' % self.non_field_errors())\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = u'<li>'\\n            if bf.errors:\\n                line += str(bf.errors)\\n            line += u'%s %s</li>' % (bf.label_tag(bf.verbose_name+':'), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)"
  },
  {
    "code": "def __init__(self, n_components=None, algorithm='parallel', whiten=True,\\n                fun='logcosh', fun_prime='', fun_args={}, maxit=200, tol=1e-4,\\n                w_init=None):\\n        super(FastICA, self).__init__()\\n        self.n_components = n_components\\n        self.algorithm = algorithm\\n        self.whiten = whiten\\n        self.fun = fun\\n        self.fun_prime = fun_prime\\n        self.fun_args = fun_args\\n        self.maxit = maxit\\n        self.tol = tol\\n        self.w_init = w_init",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, n_components=None, algorithm='parallel', whiten=True,\\n                fun='logcosh', fun_prime='', fun_args={}, maxit=200, tol=1e-4,\\n                w_init=None):\\n        super(FastICA, self).__init__()\\n        self.n_components = n_components\\n        self.algorithm = algorithm\\n        self.whiten = whiten\\n        self.fun = fun\\n        self.fun_prime = fun_prime\\n        self.fun_args = fun_args\\n        self.maxit = maxit\\n        self.tol = tol\\n        self.w_init = w_init"
  },
  {
    "code": "def _execute_helper(self):\\n        self.executor.start()\\n        self.log.info(\"Resetting orphaned tasks for active dag runs\")\\n        self.reset_state_for_orphaned_tasks()\\n        self.processor_agent.start()\\n        execute_start_time = timezone.utcnow()\\n        last_self_heartbeat_time = timezone.utcnow()\\n        while True:\\n            self.log.debug(\"Starting Loop...\")\\n            loop_start_time = time.time()\\n            if self.using_sqlite:\\n                self.processor_agent.heartbeat()\\n                self.log.debug(\\n                    \"Waiting for processors to finish since we're using sqlite\")\\n                self.processor_agent.wait_until_finished()\\n            self.log.debug(\"Harvesting DAG parsing results\")\\n            simple_dags = self._get_simple_dags()\\n            self.log.debug(\"Harvested {} SimpleDAGs\".format(len(simple_dags)))\\n            simple_dag_bag = SimpleDagBag(simple_dags)\\n            if not self._validate_and_run_task_instances(simple_dag_bag=simple_dag_bag):\\n                continue\\n            time_since_last_heartbeat = (timezone.utcnow() -\\n                                         last_self_heartbeat_time).total_seconds()\\n            if time_since_last_heartbeat > self.heartrate:\\n                self.log.debug(\"Heartbeating the scheduler\")\\n                self.heartbeat()\\n                last_self_heartbeat_time = timezone.utcnow()\\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')\\n            loop_end_time = time.time()\\n            loop_duration = loop_end_time - loop_start_time\\n            self.log.debug(\\n                \"Ran scheduling loop in %.2f seconds\",\\n                loop_duration)\\n            if not is_unit_test:\\n                self.log.debug(\"Sleeping for %.2f seconds\", self._processor_poll_interval)\\n                time.sleep(self._processor_poll_interval)\\n            if self.processor_agent.done:\\n                self.log.info(\"Exiting scheduler loop as all files\"\\n                              \" have been processed {} times\".format(self.num_runs))\\n                break\\n            if loop_duration < 1 and not is_unit_test:\\n                sleep_length = 1 - loop_duration\\n                self.log.debug(\\n                    \"Sleeping for {0:.2f} seconds to prevent excessive logging\"\\n                    .format(sleep_length))\\n                sleep(sleep_length)\\n        self.processor_agent.terminate()\\n        if self.processor_agent.all_files_processed:\\n            self.log.info(\\n                \"Deactivating DAGs that haven't been touched since %s\",\\n                execute_start_time.isoformat()\\n            )\\n            models.DAG.deactivate_stale_dags(execute_start_time)\\n        self.executor.end()\\n        settings.Session.remove()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _execute_helper(self):\\n        self.executor.start()\\n        self.log.info(\"Resetting orphaned tasks for active dag runs\")\\n        self.reset_state_for_orphaned_tasks()\\n        self.processor_agent.start()\\n        execute_start_time = timezone.utcnow()\\n        last_self_heartbeat_time = timezone.utcnow()\\n        while True:\\n            self.log.debug(\"Starting Loop...\")\\n            loop_start_time = time.time()\\n            if self.using_sqlite:\\n                self.processor_agent.heartbeat()\\n                self.log.debug(\\n                    \"Waiting for processors to finish since we're using sqlite\")\\n                self.processor_agent.wait_until_finished()\\n            self.log.debug(\"Harvesting DAG parsing results\")\\n            simple_dags = self._get_simple_dags()\\n            self.log.debug(\"Harvested {} SimpleDAGs\".format(len(simple_dags)))\\n            simple_dag_bag = SimpleDagBag(simple_dags)\\n            if not self._validate_and_run_task_instances(simple_dag_bag=simple_dag_bag):\\n                continue\\n            time_since_last_heartbeat = (timezone.utcnow() -\\n                                         last_self_heartbeat_time).total_seconds()\\n            if time_since_last_heartbeat > self.heartrate:\\n                self.log.debug(\"Heartbeating the scheduler\")\\n                self.heartbeat()\\n                last_self_heartbeat_time = timezone.utcnow()\\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')\\n            loop_end_time = time.time()\\n            loop_duration = loop_end_time - loop_start_time\\n            self.log.debug(\\n                \"Ran scheduling loop in %.2f seconds\",\\n                loop_duration)\\n            if not is_unit_test:\\n                self.log.debug(\"Sleeping for %.2f seconds\", self._processor_poll_interval)\\n                time.sleep(self._processor_poll_interval)\\n            if self.processor_agent.done:\\n                self.log.info(\"Exiting scheduler loop as all files\"\\n                              \" have been processed {} times\".format(self.num_runs))\\n                break\\n            if loop_duration < 1 and not is_unit_test:\\n                sleep_length = 1 - loop_duration\\n                self.log.debug(\\n                    \"Sleeping for {0:.2f} seconds to prevent excessive logging\"\\n                    .format(sleep_length))\\n                sleep(sleep_length)\\n        self.processor_agent.terminate()\\n        if self.processor_agent.all_files_processed:\\n            self.log.info(\\n                \"Deactivating DAGs that haven't been touched since %s\",\\n                execute_start_time.isoformat()\\n            )\\n            models.DAG.deactivate_stale_dags(execute_start_time)\\n        self.executor.end()\\n        settings.Session.remove()"
  },
  {
    "code": "def start(self):\\n        cgroups = self._get_cgroup_names()\\n        if ((cgroups.get(\"cpu\") and cgroups.get(\"cpu\") != \"/\") or\\n                (cgroups.get(\"memory\") and cgroups.get(\"memory\") != \"/\")):\\n            self.log.debug(\\n                \"Already running in a cgroup (cpu: %s memory: %s) so not \"\\n                \"creating another one\",\\n                cgroups.get(\"cpu\"), cgroups.get(\"memory\")\\n            )\\n            self.process = self.run_command()\\n            return\\n        cgroup_name = \"airflow/{}/{}\".format(datetime.datetime.utcnow().\\n                                             strftime(\"%Y-%m-%d\"),\\n                                             str(uuid.uuid4()))\\n        self.mem_cgroup_name = \"memory/{}\".format(cgroup_name)\\n        self.cpu_cgroup_name = \"cpu/{}\".format(cgroup_name)\\n        task = self._task_instance.task\\n        resources = task.resources\\n        cpus = resources.cpus.qty\\n        self._cpu_shares = cpus * 1024\\n        self._mem_mb_limit = resources.ram.qty\\n        mem_cgroup_node = self._create_cgroup(self.mem_cgroup_name)\\n        self._created_mem_cgroup = True\\n        if self._mem_mb_limit > 0:\\n            self.log.debug(\\n                \"Setting %s with %s MB of memory\",\\n                self.mem_cgroup_name, self._mem_mb_limit\\n            )\\n            mem_cgroup_node.controller.limit_in_bytes = self._mem_mb_limit * 1024 * 1024\\n        cpu_cgroup_node = self._create_cgroup(self.cpu_cgroup_name)\\n        self._created_cpu_cgroup = True\\n        if self._cpu_shares > 0:\\n            self.log.debug(\\n                \"Setting %s with %s CPU shares\",\\n                self.cpu_cgroup_name, self._cpu_shares\\n            )\\n            cpu_cgroup_node.controller.shares = self._cpu_shares\\n        self.log.debug(\\n            \"Starting task process with cgroups cpu,memory: %s\",\\n            cgroup_name\\n        )\\n        self.process = self.run_command(\\n            ['cgexec', '-g', 'cpu,memory:{}'.format(cgroup_name)]\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def start(self):\\n        cgroups = self._get_cgroup_names()\\n        if ((cgroups.get(\"cpu\") and cgroups.get(\"cpu\") != \"/\") or\\n                (cgroups.get(\"memory\") and cgroups.get(\"memory\") != \"/\")):\\n            self.log.debug(\\n                \"Already running in a cgroup (cpu: %s memory: %s) so not \"\\n                \"creating another one\",\\n                cgroups.get(\"cpu\"), cgroups.get(\"memory\")\\n            )\\n            self.process = self.run_command()\\n            return\\n        cgroup_name = \"airflow/{}/{}\".format(datetime.datetime.utcnow().\\n                                             strftime(\"%Y-%m-%d\"),\\n                                             str(uuid.uuid4()))\\n        self.mem_cgroup_name = \"memory/{}\".format(cgroup_name)\\n        self.cpu_cgroup_name = \"cpu/{}\".format(cgroup_name)\\n        task = self._task_instance.task\\n        resources = task.resources\\n        cpus = resources.cpus.qty\\n        self._cpu_shares = cpus * 1024\\n        self._mem_mb_limit = resources.ram.qty\\n        mem_cgroup_node = self._create_cgroup(self.mem_cgroup_name)\\n        self._created_mem_cgroup = True\\n        if self._mem_mb_limit > 0:\\n            self.log.debug(\\n                \"Setting %s with %s MB of memory\",\\n                self.mem_cgroup_name, self._mem_mb_limit\\n            )\\n            mem_cgroup_node.controller.limit_in_bytes = self._mem_mb_limit * 1024 * 1024\\n        cpu_cgroup_node = self._create_cgroup(self.cpu_cgroup_name)\\n        self._created_cpu_cgroup = True\\n        if self._cpu_shares > 0:\\n            self.log.debug(\\n                \"Setting %s with %s CPU shares\",\\n                self.cpu_cgroup_name, self._cpu_shares\\n            )\\n            cpu_cgroup_node.controller.shares = self._cpu_shares\\n        self.log.debug(\\n            \"Starting task process with cgroups cpu,memory: %s\",\\n            cgroup_name\\n        )\\n        self.process = self.run_command(\\n            ['cgexec', '-g', 'cpu,memory:{}'.format(cgroup_name)]\\n        )"
  },
  {
    "code": "def to_datetime(\\n    arg,\\n    errors=\"raise\",\\n    dayfirst=False,\\n    yearfirst=False,\\n    utc=None,\\n    format=None,\\n    exact=True,\\n    unit=None,\\n    infer_datetime_format=False,\\n    origin=\"unix\",\\n    cache=True,\\n):\\n    if arg is None:\\n        return None\\n    if origin != \"unix\":\\n        arg = _adjust_to_origin(arg, origin, unit)\\n    tz = \"utc\" if utc else None\\n    convert_listlike = partial(\\n        _convert_listlike_datetimes,\\n        tz=tz,\\n        unit=unit,\\n        dayfirst=dayfirst,\\n        yearfirst=yearfirst,\\n        errors=errors,\\n        exact=exact,\\n        infer_datetime_format=infer_datetime_format,\\n    )\\n    if isinstance(arg, Timestamp):\\n        result = arg\\n        if tz is not None:\\n            if arg.tz is not None:\\n                result = result.tz_convert(tz)\\n            else:\\n                result = result.tz_localize(tz)\\n    elif isinstance(arg, ABCSeries):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = arg.map(cache_array)\\n        else:\\n            values = convert_listlike(arg._values, format)\\n            result = arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):\\n        result = _assemble_from_unit_mappings(arg, errors, tz)\\n    elif isinstance(arg, ABCIndexClass):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array, name=arg.name)\\n        else:\\n            convert_listlike = partial(convert_listlike, name=arg.name)\\n            result = convert_listlike(arg, format)\\n    elif is_list_like(arg):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array)\\n        else:\\n            result = convert_listlike(arg, format)\\n    else:\\n        result = convert_listlike(np.array([arg]), format)[0]\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_datetime(\\n    arg,\\n    errors=\"raise\",\\n    dayfirst=False,\\n    yearfirst=False,\\n    utc=None,\\n    format=None,\\n    exact=True,\\n    unit=None,\\n    infer_datetime_format=False,\\n    origin=\"unix\",\\n    cache=True,\\n):\\n    if arg is None:\\n        return None\\n    if origin != \"unix\":\\n        arg = _adjust_to_origin(arg, origin, unit)\\n    tz = \"utc\" if utc else None\\n    convert_listlike = partial(\\n        _convert_listlike_datetimes,\\n        tz=tz,\\n        unit=unit,\\n        dayfirst=dayfirst,\\n        yearfirst=yearfirst,\\n        errors=errors,\\n        exact=exact,\\n        infer_datetime_format=infer_datetime_format,\\n    )\\n    if isinstance(arg, Timestamp):\\n        result = arg\\n        if tz is not None:\\n            if arg.tz is not None:\\n                result = result.tz_convert(tz)\\n            else:\\n                result = result.tz_localize(tz)\\n    elif isinstance(arg, ABCSeries):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = arg.map(cache_array)\\n        else:\\n            values = convert_listlike(arg._values, format)\\n            result = arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):\\n        result = _assemble_from_unit_mappings(arg, errors, tz)\\n    elif isinstance(arg, ABCIndexClass):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array, name=arg.name)\\n        else:\\n            convert_listlike = partial(convert_listlike, name=arg.name)\\n            result = convert_listlike(arg, format)\\n    elif is_list_like(arg):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array)\\n        else:\\n            result = convert_listlike(arg, format)\\n    else:\\n        result = convert_listlike(np.array([arg]), format)[0]\\n    return result"
  },
  {
    "code": "def _rename_index_inplace(self, mapper):\\n        self._data = self._data.rename_axis(mapper, axis=1)\\n        self._clear_caches()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: refactoring and micro-optimizations to support #437, uncovered panel bug",
    "fixed_code": "def _rename_index_inplace(self, mapper):\\n        self._data = self._data.rename_axis(mapper, axis=1)\\n        self._clear_item_cache()"
  },
  {
    "code": "def get_version(version=None):\\n    \"Returns a PEP 386-compliant version number from VERSION.\"\\n    version = get_complete_version(version)\\n    main = get_main_version(version)\\n    sub = ''\\n    if version[3] == 'alpha' and version[4] == 0:\\n        git_changeset = get_git_changeset()\\n        if git_changeset:\\n            sub = '.dev%s' % git_changeset\\n    elif version[3] != 'final':\\n        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'rc'}\\n        sub = mapping[version[3]] + str(version[4])\\n    return str(main + sub)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_version(version=None):\\n    \"Returns a PEP 386-compliant version number from VERSION.\"\\n    version = get_complete_version(version)\\n    main = get_main_version(version)\\n    sub = ''\\n    if version[3] == 'alpha' and version[4] == 0:\\n        git_changeset = get_git_changeset()\\n        if git_changeset:\\n            sub = '.dev%s' % git_changeset\\n    elif version[3] != 'final':\\n        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'rc'}\\n        sub = mapping[version[3]] + str(version[4])\\n    return str(main + sub)"
  },
  {
    "code": "def _maybe_wrap_formatter(\\n    formatter: Optional[BaseFormatter] = None,\\n    na_rep: Optional[str] = None,\\n    precision: Optional[int] = None,\\n) -> Callable:\\n    if isinstance(formatter, str):\\n        formatter_func = lambda x: formatter.format(x)\\n    elif callable(formatter):\\n        formatter_func = formatter\\n    elif formatter is None:\\n        formatter_func = partial(_default_formatter, precision=precision)\\n    else:\\n        raise TypeError(f\"'formatter' expected str or callable, got {type(formatter)}\")\\n    if na_rep is None:\\n        return formatter_func\\n    else:\\n        return lambda x: na_rep if pd.isna(x) else formatter_func(x)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_wrap_formatter(\\n    formatter: Optional[BaseFormatter] = None,\\n    na_rep: Optional[str] = None,\\n    precision: Optional[int] = None,\\n) -> Callable:\\n    if isinstance(formatter, str):\\n        formatter_func = lambda x: formatter.format(x)\\n    elif callable(formatter):\\n        formatter_func = formatter\\n    elif formatter is None:\\n        formatter_func = partial(_default_formatter, precision=precision)\\n    else:\\n        raise TypeError(f\"'formatter' expected str or callable, got {type(formatter)}\")\\n    if na_rep is None:\\n        return formatter_func\\n    else:\\n        return lambda x: na_rep if pd.isna(x) else formatter_func(x)"
  },
  {
    "code": "def _eigen_decompose_covariance(self, X, y, sqrt_sw):\\n        n_samples, n_features = X.shape\\n        cov = np.empty((n_features + 1, n_features + 1), dtype=X.dtype)\\n        cov[:-1, :-1], X_mean = self._compute_covariance(X, sqrt_sw)\\n        if not self.fit_intercept:\\n            cov = cov[:-1, :-1]\\n        else:\\n            cov[-1] = 0\\n            cov[:, -1] = 0\\n            cov[-1, -1] = sqrt_sw.dot(sqrt_sw)\\n        nullspace_dim = max(0, X.shape[1] - X.shape[0])\\n        s, V = linalg.eigh(cov)\\n        s = s[nullspace_dim:]\\n        V = V[:, nullspace_dim:]\\n        return X_mean, s, V, X",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[MRG] RidgeCV minor refactor to improve readability (#13832)\\n\\nThank you very much for this work @thomasjpfan and @jeromedockes.",
    "fixed_code": "def _eigen_decompose_covariance(self, X, y, sqrt_sw):\\n        n_samples, n_features = X.shape\\n        cov = np.empty((n_features + 1, n_features + 1), dtype=X.dtype)\\n        cov[:-1, :-1], X_mean = self._compute_covariance(X, sqrt_sw)\\n        if not self.fit_intercept:\\n            cov = cov[:-1, :-1]\\n        else:\\n            cov[-1] = 0\\n            cov[:, -1] = 0\\n            cov[-1, -1] = sqrt_sw.dot(sqrt_sw)\\n        nullspace_dim = max(0, X.shape[1] - X.shape[0])\\n        eigvals, V = linalg.eigh(cov)\\n        eigvals = eigvals[nullspace_dim:]\\n        V = V[:, nullspace_dim:]\\n        return X_mean, eigvals, V, X"
  },
  {
    "code": "def _add_margins(table, data, values, rows, cols, aggfunc,\\n                 margins_name='All', fill_value=None):\\n    if not isinstance(margins_name, compat.string_types):\\n        raise ValueError('margins_name argument must be a string')\\n    msg = u'Conflicting name \"{name}\" in margins'.format(name=margins_name)\\n    for level in table.index.names:\\n        if margins_name in table.index.get_level_values(level):\\n            raise ValueError(msg)\\n    grand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\\n    if hasattr(table, 'columns'):\\n        for level in table.columns.names[1:]:\\n            if margins_name in table.columns.get_level_values(level):\\n                raise ValueError(msg)\\n    if len(rows) > 1:\\n        key = (margins_name,) + ('',) * (len(rows) - 1)\\n    else:\\n        key = margins_name\\n    if not values and isinstance(table, ABCSeries):\\n        return table.append(Series({key: grand_margin[margins_name]}))\\n    if values:\\n        marginal_result_set = _generate_marginal_results(table, data, values,\\n                                                         rows, cols, aggfunc,\\n                                                         grand_margin,\\n                                                         margins_name)\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    else:\\n        marginal_result_set = _generate_marginal_results_without_values(\\n            table, data, rows, cols, aggfunc, margins_name)\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    row_margin = row_margin.reindex(result.columns, fill_value=fill_value)\\n    for k in margin_keys:\\n        if isinstance(k, compat.string_types):\\n            row_margin[k] = grand_margin[k]\\n        else:\\n            row_margin[k] = grand_margin[k[0]]\\n    from pandas import DataFrame\\n    margin_dummy = DataFrame(row_margin, columns=[key]).T\\n    row_names = result.index.names\\n    try:\\n        for dtype in set(result.dtypes):\\n            cols = result.select_dtypes([dtype]).columns\\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\\n        result = result.append(margin_dummy)\\n    except TypeError:\\n        result.index = result.index._to_safe_for_reshape()\\n        result = result.append(margin_dummy)\\n    result.index.names = row_names\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _add_margins(table, data, values, rows, cols, aggfunc,\\n                 margins_name='All', fill_value=None):\\n    if not isinstance(margins_name, compat.string_types):\\n        raise ValueError('margins_name argument must be a string')\\n    msg = u'Conflicting name \"{name}\" in margins'.format(name=margins_name)\\n    for level in table.index.names:\\n        if margins_name in table.index.get_level_values(level):\\n            raise ValueError(msg)\\n    grand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\\n    if hasattr(table, 'columns'):\\n        for level in table.columns.names[1:]:\\n            if margins_name in table.columns.get_level_values(level):\\n                raise ValueError(msg)\\n    if len(rows) > 1:\\n        key = (margins_name,) + ('',) * (len(rows) - 1)\\n    else:\\n        key = margins_name\\n    if not values and isinstance(table, ABCSeries):\\n        return table.append(Series({key: grand_margin[margins_name]}))\\n    if values:\\n        marginal_result_set = _generate_marginal_results(table, data, values,\\n                                                         rows, cols, aggfunc,\\n                                                         grand_margin,\\n                                                         margins_name)\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    else:\\n        marginal_result_set = _generate_marginal_results_without_values(\\n            table, data, rows, cols, aggfunc, margins_name)\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    row_margin = row_margin.reindex(result.columns, fill_value=fill_value)\\n    for k in margin_keys:\\n        if isinstance(k, compat.string_types):\\n            row_margin[k] = grand_margin[k]\\n        else:\\n            row_margin[k] = grand_margin[k[0]]\\n    from pandas import DataFrame\\n    margin_dummy = DataFrame(row_margin, columns=[key]).T\\n    row_names = result.index.names\\n    try:\\n        for dtype in set(result.dtypes):\\n            cols = result.select_dtypes([dtype]).columns\\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\\n        result = result.append(margin_dummy)\\n    except TypeError:\\n        result.index = result.index._to_safe_for_reshape()\\n        result = result.append(margin_dummy)\\n    result.index.names = row_names\\n    return result"
  },
  {
    "code": "def int_list_validator(sep=',', message=None, code='invalid'):\\n    regexp = _lazy_re_compile('^\\d+(?:%s\\d+)*\\Z' % re.escape(sep))\\n    return RegexValidator(regexp, message=message, code=code)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26107 -- Added option to int_list_validator() to allow negative integers.",
    "fixed_code": "def int_list_validator(sep=',', message=None, code='invalid', allow_negative=False):\\n    regexp = _lazy_re_compile('^%(neg)s\\d+(?:%(sep)s%(neg)s\\d+)*\\Z' % {\\n        'neg': '(-)?' if allow_negative else '',\\n        'sep': re.escape(sep),\\n    })\\n    return RegexValidator(regexp, message=message, code=code)"
  },
  {
    "code": "def populate_obj(self, item):\\n        super().populate_obj(item)\\n        item.execution_date = timezone.make_aware(item.execution_date)\\n        if item.conf:\\n            item.conf = json.loads(item.conf)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-8902] Fix Dag Run UI execution date with timezone cannot be saved issue (#8902)\\n\\nCloses #8842",
    "fixed_code": "def populate_obj(self, item):\\n        super().populate_obj(item)\\n        if item.conf:\\n            item.conf = json.loads(item.conf)"
  },
  {
    "code": "def _tp_tn_fp_fn(y_true, y_pred, labels=None):\\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\\n    if labels is None:\\n        labels = unique_labels(y_true, y_pred)\\n    else:\\n        labels = np.asarray(labels)\\n    n_labels = labels.size\\n    true_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_neg = np.zeros((n_labels,), dtype=np.int)\\n    if y_type == 'multilabel-indicator':\\n        true_pos = np.sum(np.logical_and(y_true == 1,\\n                                         y_pred == 1), axis=0)\\n        false_pos = np.sum(np.logical_and(y_true != 1,\\n                                          y_pred == 1), axis=0)\\n        false_neg = np.sum(np.logical_and(y_true == 1,\\n                                          y_pred != 1), axis=0)\\n    elif y_type == 'multilabel-sequences':\\n        idx_to_label = dict((label_i, i)\\n                            for i, label_i in enumerate(labels))\\n        for true, pred in zip(y_true, y_pred):\\n            true_set = np.array([idx_to_label[l] for l in set(true)],\\n                                dtype=np.int)\\n            pred_set = np.array([idx_to_label[l] for l in set(pred)],\\n                                dtype=np.int)\\n            true_pos[np.intersect1d(true_set, pred_set)] += 1\\n            false_pos[np.setdiff1d(pred_set, true_set)] += 1\\n            false_neg[np.setdiff1d(true_set, pred_set)] += 1\\n    else:\\n        for i, label_i in enumerate(labels):\\n            true_pos[i] = np.sum(y_pred[y_true == label_i] == label_i)\\n            false_pos[i] = np.sum(y_pred[y_true != label_i] == label_i)\\n            false_neg[i] = np.sum(y_pred[y_true == label_i] != label_i)\\n    n_samples = len(y_true)\\n    true_neg = n_samples - true_pos - false_pos - false_neg\\n    return true_pos, true_neg, false_pos, false_neg",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _tp_tn_fp_fn(y_true, y_pred, labels=None):\\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\\n    if labels is None:\\n        labels = unique_labels(y_true, y_pred)\\n    else:\\n        labels = np.asarray(labels)\\n    n_labels = labels.size\\n    true_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_neg = np.zeros((n_labels,), dtype=np.int)\\n    if y_type == 'multilabel-indicator':\\n        true_pos = np.sum(np.logical_and(y_true == 1,\\n                                         y_pred == 1), axis=0)\\n        false_pos = np.sum(np.logical_and(y_true != 1,\\n                                          y_pred == 1), axis=0)\\n        false_neg = np.sum(np.logical_and(y_true == 1,\\n                                          y_pred != 1), axis=0)\\n    elif y_type == 'multilabel-sequences':\\n        idx_to_label = dict((label_i, i)\\n                            for i, label_i in enumerate(labels))\\n        for true, pred in zip(y_true, y_pred):\\n            true_set = np.array([idx_to_label[l] for l in set(true)],\\n                                dtype=np.int)\\n            pred_set = np.array([idx_to_label[l] for l in set(pred)],\\n                                dtype=np.int)\\n            true_pos[np.intersect1d(true_set, pred_set)] += 1\\n            false_pos[np.setdiff1d(pred_set, true_set)] += 1\\n            false_neg[np.setdiff1d(true_set, pred_set)] += 1\\n    else:\\n        for i, label_i in enumerate(labels):\\n            true_pos[i] = np.sum(y_pred[y_true == label_i] == label_i)\\n            false_pos[i] = np.sum(y_pred[y_true != label_i] == label_i)\\n            false_neg[i] = np.sum(y_pred[y_true == label_i] != label_i)\\n    n_samples = len(y_true)\\n    true_neg = n_samples - true_pos - false_pos - false_neg\\n    return true_pos, true_neg, false_pos, false_neg"
  },
  {
    "code": "def _handle_source(self, req, image_id, image_meta, image_data):\\n\\t\\tcopy_from = self._copy_from(req)\\n\\t\\tlocation = image_meta.get('location')\\n\\t\\tsources = filter(lambda x: x, (copy_from, location, image_data))\\n\\t\\tif len(sources) >= 2:\\n\\t\\t\\tmsg = \"It's invalid to provide multiple image sources.\"\\n\\t\\t\\tLOG.debug(msg)\\n\\t\\t\\traise HTTPBadRequest(explanation=msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t content_type=\"text/plain\")\\n\\t\\tif image_data:\\n\\t\\t\\timage_meta = self._validate_image_for_activation(req,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t image_id,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t image_meta)\\n\\t\\t\\timage_meta = self._upload_and_activate(req, image_meta)\\n\\t\\telif copy_from:\\n\\t\\t\\tmsg = _LI('Triggering asynchronous copy from external source')\\n\\t\\t\\tLOG.info(msg)\\n\\t\\t\\tself.pool.spawn_n(self._upload_and_activate, req, image_meta)\\n\\t\\telse:\\n\\t\\t\\tif location:\\n\\t\\t\\t\\tself._validate_image_for_activation(req, image_id, image_meta)\\n\\t\\t\\t\\timage_size_meta = image_meta.get('size')\\n\\t\\t\\t\\tif image_size_meta:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\timage_size_store = store.get_size_from_backend(\\n\\t\\t\\t\\t\\t\\t\\tlocation, req.context)\\n\\t\\t\\t\\t\\texcept (store.BadStoreUri, store.UnknownScheme) as e:\\n\\t\\t\\t\\t\\t\\tLOG.debug(utils.exception_to_str(e))\\n\\t\\t\\t\\t\\t\\traise HTTPBadRequest(explanation=e.msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t content_type=\"text/plain\")\\n\\t\\t\\t\\t\\tif (image_size_store and\\n\\t\\t\\t\\t\\t\\t\\timage_size_store != image_size_meta):\\n\\t\\t\\t\\t\\t\\tmsg = (\"Provided image size must match the stored \"\\n\\t\\t\\t\\t\\t\\t\\t   \"image size. (provided size: %(ps)d, \"\\n\\t\\t\\t\\t\\t\\t\\t   \"stored size: %(ss)d)\" % {\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"ps\": image_size_meta,\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"ss\": image_size_store})\\n\\t\\t\\t\\t\\t\\tLOG.debug(msg)\\n\\t\\t\\t\\t\\t\\traise HTTPConflict(explanation=msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   content_type=\"text/plain\")\\n\\t\\t\\t\\tlocation_data = {'url': location, 'metadata': {},\\n\\t\\t\\t\\t\\t\\t\\t\\t 'status': 'active'}\\n\\t\\t\\t\\timage_meta = self._activate(req, image_id, location_data)\\n\\t\\treturn image_meta",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _handle_source(self, req, image_id, image_meta, image_data):\\n\\t\\tcopy_from = self._copy_from(req)\\n\\t\\tlocation = image_meta.get('location')\\n\\t\\tsources = filter(lambda x: x, (copy_from, location, image_data))\\n\\t\\tif len(sources) >= 2:\\n\\t\\t\\tmsg = \"It's invalid to provide multiple image sources.\"\\n\\t\\t\\tLOG.debug(msg)\\n\\t\\t\\traise HTTPBadRequest(explanation=msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t content_type=\"text/plain\")\\n\\t\\tif image_data:\\n\\t\\t\\timage_meta = self._validate_image_for_activation(req,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t image_id,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t image_meta)\\n\\t\\t\\timage_meta = self._upload_and_activate(req, image_meta)\\n\\t\\telif copy_from:\\n\\t\\t\\tmsg = _LI('Triggering asynchronous copy from external source')\\n\\t\\t\\tLOG.info(msg)\\n\\t\\t\\tself.pool.spawn_n(self._upload_and_activate, req, image_meta)\\n\\t\\telse:\\n\\t\\t\\tif location:\\n\\t\\t\\t\\tself._validate_image_for_activation(req, image_id, image_meta)\\n\\t\\t\\t\\timage_size_meta = image_meta.get('size')\\n\\t\\t\\t\\tif image_size_meta:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\timage_size_store = store.get_size_from_backend(\\n\\t\\t\\t\\t\\t\\t\\tlocation, req.context)\\n\\t\\t\\t\\t\\texcept (store.BadStoreUri, store.UnknownScheme) as e:\\n\\t\\t\\t\\t\\t\\tLOG.debug(utils.exception_to_str(e))\\n\\t\\t\\t\\t\\t\\traise HTTPBadRequest(explanation=e.msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t content_type=\"text/plain\")\\n\\t\\t\\t\\t\\tif (image_size_store and\\n\\t\\t\\t\\t\\t\\t\\timage_size_store != image_size_meta):\\n\\t\\t\\t\\t\\t\\tmsg = (\"Provided image size must match the stored \"\\n\\t\\t\\t\\t\\t\\t\\t   \"image size. (provided size: %(ps)d, \"\\n\\t\\t\\t\\t\\t\\t\\t   \"stored size: %(ss)d)\" % {\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"ps\": image_size_meta,\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"ss\": image_size_store})\\n\\t\\t\\t\\t\\t\\tLOG.debug(msg)\\n\\t\\t\\t\\t\\t\\traise HTTPConflict(explanation=msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   content_type=\"text/plain\")\\n\\t\\t\\t\\tlocation_data = {'url': location, 'metadata': {},\\n\\t\\t\\t\\t\\t\\t\\t\\t 'status': 'active'}\\n\\t\\t\\t\\timage_meta = self._activate(req, image_id, location_data)\\n\\t\\treturn image_meta"
  },
  {
    "code": "def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\\n\\t\\tif mode is None:\\n\\t\\t\\treturn changed\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tif expand:\\n\\t\\t\\tb_path = os.path.expanduser(os.path.expandvars(b_path))\\n\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\tif self.check_file_absent_if_check_mode(b_path):\\n\\t\\t\\treturn True\\n\\t\\tif not isinstance(mode, int):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tmode = int(mode, 8)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tmode = self._symbolic_mode_to_octal(path_stat, mode)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path,\\n\\t\\t\\t\\t\\t\\t\\t\\t   msg=\"mode must be in octal or symbolic form\",\\n\\t\\t\\t\\t\\t\\t\\t\\t   details=to_native(e))\\n\\t\\t\\t\\tif mode != stat.S_IMODE(mode):\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\\n\\t\\tprev_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\tif prev_mode != mode:\\n\\t\\t\\tif diff is not None:\\n\\t\\t\\t\\tif 'before' not in diff:\\n\\t\\t\\t\\t\\tdiff['before'] = {}\\n\\t\\t\\t\\tdiff['before']['mode'] = '0%03o' % prev_mode\\n\\t\\t\\t\\tif 'after' not in diff:\\n\\t\\t\\t\\t\\tdiff['after'] = {}\\n\\t\\t\\t\\tdiff['after']['mode'] = '0%03o' % mode\\n\\t\\t\\tif self.check_mode:\\n\\t\\t\\t\\treturn True\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif hasattr(os, 'lchmod'):\\n\\t\\t\\t\\t\\tos.lchmod(b_path, mode)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not os.path.islink(b_path):\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tunderlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\t\\tnew_underlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tif underlying_stat.st_mode != new_underlying_stat.st_mode:\\n\\t\\t\\t\\t\\t\\t\\tos.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\\n\\t\\t\\texcept OSError as e:\\n\\t\\t\\t\\tif os.path.islink(b_path) and e.errno in (\\n\\t\\t\\t\\t\\terrno.EACCES,  \\n\\t\\t\\t\\t\\terrno.EPERM,  \\n\\t\\t\\t\\t\\terrno.EROFS,  \\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telif e.errno in (errno.ENOENT, errno.ELOOP):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\tself.fail_json(path=path, msg='chmod failed', details=to_native(e),\\n\\t\\t\\t\\t\\t\\t\\t   exception=traceback.format_exc())\\n\\t\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\t\\tnew_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\t\\tif new_mode != prev_mode:\\n\\t\\t\\t\\tchanged = True\\n\\t\\treturn changed",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\\n\\t\\tif mode is None:\\n\\t\\t\\treturn changed\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tif expand:\\n\\t\\t\\tb_path = os.path.expanduser(os.path.expandvars(b_path))\\n\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\tif self.check_file_absent_if_check_mode(b_path):\\n\\t\\t\\treturn True\\n\\t\\tif not isinstance(mode, int):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tmode = int(mode, 8)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tmode = self._symbolic_mode_to_octal(path_stat, mode)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path,\\n\\t\\t\\t\\t\\t\\t\\t\\t   msg=\"mode must be in octal or symbolic form\",\\n\\t\\t\\t\\t\\t\\t\\t\\t   details=to_native(e))\\n\\t\\t\\t\\tif mode != stat.S_IMODE(mode):\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\\n\\t\\tprev_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\tif prev_mode != mode:\\n\\t\\t\\tif diff is not None:\\n\\t\\t\\t\\tif 'before' not in diff:\\n\\t\\t\\t\\t\\tdiff['before'] = {}\\n\\t\\t\\t\\tdiff['before']['mode'] = '0%03o' % prev_mode\\n\\t\\t\\t\\tif 'after' not in diff:\\n\\t\\t\\t\\t\\tdiff['after'] = {}\\n\\t\\t\\t\\tdiff['after']['mode'] = '0%03o' % mode\\n\\t\\t\\tif self.check_mode:\\n\\t\\t\\t\\treturn True\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif hasattr(os, 'lchmod'):\\n\\t\\t\\t\\t\\tos.lchmod(b_path, mode)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not os.path.islink(b_path):\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tunderlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\t\\tnew_underlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tif underlying_stat.st_mode != new_underlying_stat.st_mode:\\n\\t\\t\\t\\t\\t\\t\\tos.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\\n\\t\\t\\texcept OSError as e:\\n\\t\\t\\t\\tif os.path.islink(b_path) and e.errno in (\\n\\t\\t\\t\\t\\terrno.EACCES,  \\n\\t\\t\\t\\t\\terrno.EPERM,  \\n\\t\\t\\t\\t\\terrno.EROFS,  \\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telif e.errno in (errno.ENOENT, errno.ELOOP):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\tself.fail_json(path=path, msg='chmod failed', details=to_native(e),\\n\\t\\t\\t\\t\\t\\t\\t   exception=traceback.format_exc())\\n\\t\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\t\\tnew_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\t\\tif new_mode != prev_mode:\\n\\t\\t\\t\\tchanged = True\\n\\t\\treturn changed"
  },
  {
    "code": "def _concat_frames(frames, index, columns=None, axis=0):\\n    if axis == 0:\\n        all_index = [np.asarray(x.index) for x in frames]\\n        new_index = Index(np.concatenate(all_index))\\n        if columns is None:\\n            new_columns = frames[0].columns\\n        else:\\n            new_columns = columns\\n    else:\\n        all_columns = [np.asarray(x.columns) for x in frames]\\n        new_columns = Index(np.concatenate(all_columns))\\n        new_index = index\\n    new_values = np.concatenate([x.values for x in frames], axis=axis)\\n    result = DataFrame(new_values, index=new_index, columns=new_columns)\\n    return result.reindex(index=index, columns=columns)\\ndef _concat_frames_hierarchical(frames, keys, groupings, axis=0):\\n    if axis == 0:\\n        indexes = [x.index for x in frames]\\n        new_index = _make_concat_multiindex(indexes, keys, groupings)\\n        new_columns = frames[0].columns\\n    else:\\n        all_columns = [x.columns for x in frames]\\n        new_columns = _make_concat_multiindex(all_columns, keys, groupings)\\n        new_index = frames[0].index\\n    new_values = np.concatenate([x.values for x in frames], axis=axis)\\n    return DataFrame(new_values, index=new_index, columns=new_columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix GroupBy.apply bug, GH #237",
    "fixed_code": "def _concat_frames(frames, index, columns=None, axis=0):\\n    if len(frames) == 1:\\n        return frames[0]\\n    if axis == 0:\\n        new_index = _concat_indexes([x.index for x in frames])\\n        if columns is None:\\n            new_columns = frames[0].columns\\n        else:\\n            new_columns = columns\\n    else:\\n        new_columns = _concat_indexes([x.columns for x in frames])\\n        new_index = index\\n    if frames[0]._is_mixed_type:\\n        new_data = {}\\n        for col in new_columns:\\n            new_data[col] = np.concatenate([x[col].values for x in frames])\\n        return DataFrame(new_data, index=new_index, columns=new_columns)\\n    else:\\n        new_values = np.concatenate([x.values for x in frames], axis=axis)\\n        result = DataFrame(new_values, index=new_index, columns=new_columns)\\n        return result.reindex(index=index, columns=columns)"
  },
  {
    "code": "def _aggregate_simple(self, func, *args, **kwargs):\\n        values = self.obj.values\\n        result = {}\\n        for k, v in self.primary.indices.iteritems():\\n            result[k] = func(values.take(v), *args, **kwargs)\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_simple(self, func, *args, **kwargs):\\n        values = self.obj.values\\n        result = {}\\n        for k, v in self.primary.indices.iteritems():\\n            result[k] = func(values.take(v), *args, **kwargs)\\n        return result"
  },
  {
    "code": "def bracket_split_build_line(\\n\\tleaves: List[Leaf], original: Line, opening_bracket: Leaf, *, is_body: bool = False\\n) -> Line:\\n\\tresult = Line(depth=original.depth)\\n\\tif is_body:\\n\\t\\tresult.inside_brackets = True\\n\\t\\tresult.depth += 1\\n\\t\\tif leaves:\\n\\t\\t\\tnormalize_prefix(leaves[0], inside_brackets=True)\\n\\t\\t\\tif original.is_import:\\n\\t\\t\\t\\tif leaves[-1].type != token.COMMA:\\n\\t\\t\\t\\t\\tleaves.append(Leaf(token.COMMA, \",\"))\\n\\tfor leaf in leaves:\\n\\t\\tresult.append(leaf, preformatted=True)\\n\\t\\tfor comment_after in original.comments_after(leaf):\\n\\t\\t\\tresult.append(comment_after, preformatted=True)\\n\\tif is_body:\\n\\t\\tresult.should_explode = should_explode(result, opening_bracket)\\n\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix handling of comments in from imports (#829)\\n\\nFixes #671",
    "fixed_code": "def bracket_split_build_line(\\n\\tleaves: List[Leaf], original: Line, opening_bracket: Leaf, *, is_body: bool = False\\n) -> Line:\\n\\tresult = Line(depth=original.depth)\\n\\tif is_body:\\n\\t\\tresult.inside_brackets = True\\n\\t\\tresult.depth += 1\\n\\t\\tif leaves:\\n\\t\\t\\tnormalize_prefix(leaves[0], inside_brackets=True)\\n\\t\\t\\tif original.is_import:\\n\\t\\t\\t\\tfor i in range(len(leaves) - 1, -1, -1):\\n\\t\\t\\t\\t\\tif leaves[i].type == STANDALONE_COMMENT:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\telif leaves[i].type == token.COMMA:\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tleaves.insert(i + 1, Leaf(token.COMMA, \",\"))\\n\\t\\t\\t\\t\\t\\tbreak\\n\\tfor leaf in leaves:\\n\\t\\tresult.append(leaf, preformatted=True)\\n\\t\\tfor comment_after in original.comments_after(leaf):\\n\\t\\t\\tresult.append(comment_after, preformatted=True)\\n\\tif is_body:\\n\\t\\tresult.should_explode = should_explode(result, opening_bracket)\\n\\treturn result"
  },
  {
    "code": "def load_backend(backend_name):\\n    if backend_name == 'django.db.backends.postgresql_psycopg2':\\n        backend_name = 'django.db.backends.postgresql'\\n    try:\\n        return import_module('%s.base' % backend_name)\\n    except ImportError as e_user:\\n        backend_dir = str(Path(__file__).parent / 'backends')\\n        builtin_backends = [\\n            name for _, name, ispkg in pkgutil.iter_modules([backend_dir])\\n            if ispkg and name not in {'base', 'dummy', 'postgresql_psycopg2'}\\n        ]\\n        if backend_name not in ['django.db.backends.%s' % b for b in builtin_backends]:\\n            backend_reprs = map(repr, sorted(builtin_backends))\\n            raise ImproperlyConfigured(\\n                \"%r isn't an available database backend or couldn't be \"\\n                \"imported. Check the above exception. To use one of the \"\\n                \"built-in backends, use 'django.db.backends.XXX', where XXX \"\\n                \"is one of:\\n\"\\n                \"    %s\" % (backend_name, \", \".join(backend_reprs))\\n            ) from e_user\\n        else:\\n            raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_backend(backend_name):\\n    if backend_name == 'django.db.backends.postgresql_psycopg2':\\n        backend_name = 'django.db.backends.postgresql'\\n    try:\\n        return import_module('%s.base' % backend_name)\\n    except ImportError as e_user:\\n        backend_dir = str(Path(__file__).parent / 'backends')\\n        builtin_backends = [\\n            name for _, name, ispkg in pkgutil.iter_modules([backend_dir])\\n            if ispkg and name not in {'base', 'dummy', 'postgresql_psycopg2'}\\n        ]\\n        if backend_name not in ['django.db.backends.%s' % b for b in builtin_backends]:\\n            backend_reprs = map(repr, sorted(builtin_backends))\\n            raise ImproperlyConfigured(\\n                \"%r isn't an available database backend or couldn't be \"\\n                \"imported. Check the above exception. To use one of the \"\\n                \"built-in backends, use 'django.db.backends.XXX', where XXX \"\\n                \"is one of:\\n\"\\n                \"    %s\" % (backend_name, \", \".join(backend_reprs))\\n            ) from e_user\\n        else:\\n            raise"
  },
  {
    "code": "def _check_generic_foreign_key_existence(self):\\n        target = self.rel.to\\n        if isinstance(target, ModelBase):\\n            fields = target._meta.virtual_fields\\n            if any(isinstance(field, GenericForeignKey) and\\n                    field.ct_field == self.content_type_field_name and\\n                    field.fk_field == self.object_id_field_name\\n                    for field in fields):\\n                return []\\n            else:\\n                return [\\n                    checks.Warning(\\n                        (\"The GenericRelation defines a relation with the model \"\\n                         \"'%s.%s', but that model does not have a GenericForeignKey.\") % (\\n                            target._meta.app_label, target._meta.object_name\\n                        ),\\n                        hint=None,\\n                        obj=self,\\n                        id='contenttypes.E004',\\n                    )\\n                ]\\n        else:\\n            return []",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_generic_foreign_key_existence(self):\\n        target = self.rel.to\\n        if isinstance(target, ModelBase):\\n            fields = target._meta.virtual_fields\\n            if any(isinstance(field, GenericForeignKey) and\\n                    field.ct_field == self.content_type_field_name and\\n                    field.fk_field == self.object_id_field_name\\n                    for field in fields):\\n                return []\\n            else:\\n                return [\\n                    checks.Warning(\\n                        (\"The GenericRelation defines a relation with the model \"\\n                         \"'%s.%s', but that model does not have a GenericForeignKey.\") % (\\n                            target._meta.app_label, target._meta.object_name\\n                        ),\\n                        hint=None,\\n                        obj=self,\\n                        id='contenttypes.E004',\\n                    )\\n                ]\\n        else:\\n            return []"
  },
  {
    "code": "def set_x(self, value):\\n        \"Sets the X component of the Point.\"\\n        self._cs.setOrdinate(0, 0, value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #25665 -- Deprecated getters/setters of Point coordinate properties.",
    "fixed_code": "def set_x(self, value):\\n        warnings.warn(\\n            \"`set_x()` is deprecated, use the `x` property instead.\",\\n            RemovedInDjango20Warning, 2\\n        )\\n        self.x = value"
  },
  {
    "code": "def prefetch_related_objects(model_instances, *related_lookups):\\n    if not model_instances:\\n        return  \\n    done_queries = {}    \\n    auto_lookups = set()  \\n    followed_descriptors = set()  \\n    all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\\n    while all_lookups:\\n        lookup = all_lookups.pop()\\n        if lookup.prefetch_to in done_queries:\\n            if lookup.queryset is not None:\\n                raise ValueError(\"'%s' lookup was already seen with a different queryset. \"\\n                                 \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\\n            continue\\n        obj_list = model_instances\\n        through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\\n        for level, through_attr in enumerate(through_attrs):\\n            if not obj_list:\\n                break\\n            prefetch_to = lookup.get_current_prefetch_to(level)\\n            if prefetch_to in done_queries:\\n                obj_list = done_queries[prefetch_to]\\n                continue\\n            good_objects = True\\n            for obj in obj_list:\\n                if not hasattr(obj, '_prefetched_objects_cache'):\\n                    try:\\n                        obj._prefetched_objects_cache = {}\\n                    except (AttributeError, TypeError):\\n                        good_objects = False\\n                        break\\n            if not good_objects:\\n                break\\n            first_obj = obj_list[0]\\n            to_attr = lookup.get_current_to_attr(level)[0]\\n            prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)\\n            if not attr_found:\\n                raise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\\n                                     \"parameter to prefetch_related()\" %\\n                                     (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\\n            if level == len(through_attrs) - 1 and prefetcher is None:\\n                raise ValueError(\"'%s' does not resolve to an item that supports \"\\n                                 \"prefetching - this is an invalid parameter to \"\\n                                 \"prefetch_related().\" % lookup.prefetch_through)\\n            obj_to_fetch = None\\n            if prefetcher is not None:\\n                obj_to_fetch = [obj for obj in obj_list if not is_fetched(obj)]\\n            if obj_to_fetch:\\n                obj_list, additional_lookups = prefetch_one_level(\\n                    obj_to_fetch,\\n                    prefetcher,\\n                    lookup,\\n                    level,\\n                )\\n                if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):\\n                    done_queries[prefetch_to] = obj_list\\n                    new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)\\n                    auto_lookups.update(new_lookups)\\n                    all_lookups.extend(new_lookups)\\n                followed_descriptors.add(descriptor)\\n            else:\\n                new_obj_list = []\\n                for obj in obj_list:\\n                    if through_attr in getattr(obj, '_prefetched_objects_cache', ()):\\n                        new_obj = list(obj._prefetched_objects_cache.get(through_attr))\\n                    else:\\n                        try:\\n                            new_obj = getattr(obj, through_attr)\\n                        except exceptions.ObjectDoesNotExist:\\n                            continue\\n                    if new_obj is None:\\n                        continue\\n                    if isinstance(new_obj, list):\\n                        new_obj_list.extend(new_obj)\\n                    else:\\n                        new_obj_list.append(new_obj)\\n                obj_list = new_obj_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def prefetch_related_objects(model_instances, *related_lookups):\\n    if not model_instances:\\n        return  \\n    done_queries = {}    \\n    auto_lookups = set()  \\n    followed_descriptors = set()  \\n    all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\\n    while all_lookups:\\n        lookup = all_lookups.pop()\\n        if lookup.prefetch_to in done_queries:\\n            if lookup.queryset is not None:\\n                raise ValueError(\"'%s' lookup was already seen with a different queryset. \"\\n                                 \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\\n            continue\\n        obj_list = model_instances\\n        through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\\n        for level, through_attr in enumerate(through_attrs):\\n            if not obj_list:\\n                break\\n            prefetch_to = lookup.get_current_prefetch_to(level)\\n            if prefetch_to in done_queries:\\n                obj_list = done_queries[prefetch_to]\\n                continue\\n            good_objects = True\\n            for obj in obj_list:\\n                if not hasattr(obj, '_prefetched_objects_cache'):\\n                    try:\\n                        obj._prefetched_objects_cache = {}\\n                    except (AttributeError, TypeError):\\n                        good_objects = False\\n                        break\\n            if not good_objects:\\n                break\\n            first_obj = obj_list[0]\\n            to_attr = lookup.get_current_to_attr(level)[0]\\n            prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)\\n            if not attr_found:\\n                raise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\\n                                     \"parameter to prefetch_related()\" %\\n                                     (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\\n            if level == len(through_attrs) - 1 and prefetcher is None:\\n                raise ValueError(\"'%s' does not resolve to an item that supports \"\\n                                 \"prefetching - this is an invalid parameter to \"\\n                                 \"prefetch_related().\" % lookup.prefetch_through)\\n            obj_to_fetch = None\\n            if prefetcher is not None:\\n                obj_to_fetch = [obj for obj in obj_list if not is_fetched(obj)]\\n            if obj_to_fetch:\\n                obj_list, additional_lookups = prefetch_one_level(\\n                    obj_to_fetch,\\n                    prefetcher,\\n                    lookup,\\n                    level,\\n                )\\n                if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):\\n                    done_queries[prefetch_to] = obj_list\\n                    new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)\\n                    auto_lookups.update(new_lookups)\\n                    all_lookups.extend(new_lookups)\\n                followed_descriptors.add(descriptor)\\n            else:\\n                new_obj_list = []\\n                for obj in obj_list:\\n                    if through_attr in getattr(obj, '_prefetched_objects_cache', ()):\\n                        new_obj = list(obj._prefetched_objects_cache.get(through_attr))\\n                    else:\\n                        try:\\n                            new_obj = getattr(obj, through_attr)\\n                        except exceptions.ObjectDoesNotExist:\\n                            continue\\n                    if new_obj is None:\\n                        continue\\n                    if isinstance(new_obj, list):\\n                        new_obj_list.extend(new_obj)\\n                    else:\\n                        new_obj_list.append(new_obj)\\n                obj_list = new_obj_list"
  },
  {
    "code": "def _fit(self, X, y, step_score=None, **fit_params):\\n        tags = self._get_tags()\\n        X, y = self._validate_data(\\n            X,\\n            y,\\n            accept_sparse=\"csc\",\\n            ensure_min_features=2,\\n            force_all_finite=not tags.get(\"allow_nan\", True),\\n            multi_output=True,\\n        )\\n        error_msg = (\\n            \"n_features_to_select must be either None, a \"\\n            \"positive integer representing the absolute \"\\n            \"number of features or a float in (0.0, 1.0] \"\\n            \"representing a percentage of features to \"\\n            f\"select. Got {self.n_features_to_select}\"\\n        )\\n        n_features = X.shape[1]\\n        if self.n_features_to_select is None:\\n            n_features_to_select = n_features // 2\\n        elif self.n_features_to_select < 0:\\n            raise ValueError(error_msg)\\n        elif isinstance(self.n_features_to_select, numbers.Integral):  \\n            n_features_to_select = self.n_features_to_select\\n        elif self.n_features_to_select > 1.0:  \\n            raise ValueError(error_msg)\\n        else:  \\n            n_features_to_select = int(n_features * self.n_features_to_select)\\n        if 0.0 < self.step < 1.0:\\n            step = int(max(1, self.step * n_features))\\n        else:\\n            step = int(self.step)\\n        if step <= 0:\\n            raise ValueError(\"Step must be >0\")\\n        support_ = np.ones(n_features, dtype=bool)\\n        ranking_ = np.ones(n_features, dtype=int)\\n        if step_score:\\n            self.scores_ = []\\n        while np.sum(support_) > n_features_to_select:\\n            features = np.arange(n_features)[support_]\\n            estimator = clone(self.estimator)\\n            if self.verbose > 0:\\n                print(\"Fitting estimator with %d features.\" % np.sum(support_))\\n            estimator.fit(X[:, features], y, **fit_params)\\n            importances = _get_feature_importances(\\n                estimator,\\n                self.importance_getter,\\n                transform_func=\"square\",\\n            )\\n            ranks = np.argsort(importances)\\n            ranks = np.ravel(ranks)\\n            threshold = min(step, np.sum(support_) - n_features_to_select)\\n            if step_score:\\n                self.scores_.append(step_score(estimator, features))\\n            support_[features[ranks][:threshold]] = False\\n            ranking_[np.logical_not(support_)] += 1\\n        features = np.arange(n_features)[support_]\\n        self.estimator_ = clone(self.estimator)\\n        self.estimator_.fit(X[:, features], y, **fit_params)\\n        if step_score:\\n            self.scores_.append(step_score(self.estimator_, features))\\n        self.n_features_ = support_.sum()\\n        self.support_ = support_\\n        self.ranking_ = ranking_\\n        return self\\n    @if_delegate_has_method(delegate=\"estimator\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fit(self, X, y, step_score=None, **fit_params):\\n        tags = self._get_tags()\\n        X, y = self._validate_data(\\n            X,\\n            y,\\n            accept_sparse=\"csc\",\\n            ensure_min_features=2,\\n            force_all_finite=not tags.get(\"allow_nan\", True),\\n            multi_output=True,\\n        )\\n        error_msg = (\\n            \"n_features_to_select must be either None, a \"\\n            \"positive integer representing the absolute \"\\n            \"number of features or a float in (0.0, 1.0] \"\\n            \"representing a percentage of features to \"\\n            f\"select. Got {self.n_features_to_select}\"\\n        )\\n        n_features = X.shape[1]\\n        if self.n_features_to_select is None:\\n            n_features_to_select = n_features // 2\\n        elif self.n_features_to_select < 0:\\n            raise ValueError(error_msg)\\n        elif isinstance(self.n_features_to_select, numbers.Integral):  \\n            n_features_to_select = self.n_features_to_select\\n        elif self.n_features_to_select > 1.0:  \\n            raise ValueError(error_msg)\\n        else:  \\n            n_features_to_select = int(n_features * self.n_features_to_select)\\n        if 0.0 < self.step < 1.0:\\n            step = int(max(1, self.step * n_features))\\n        else:\\n            step = int(self.step)\\n        if step <= 0:\\n            raise ValueError(\"Step must be >0\")\\n        support_ = np.ones(n_features, dtype=bool)\\n        ranking_ = np.ones(n_features, dtype=int)\\n        if step_score:\\n            self.scores_ = []\\n        while np.sum(support_) > n_features_to_select:\\n            features = np.arange(n_features)[support_]\\n            estimator = clone(self.estimator)\\n            if self.verbose > 0:\\n                print(\"Fitting estimator with %d features.\" % np.sum(support_))\\n            estimator.fit(X[:, features], y, **fit_params)\\n            importances = _get_feature_importances(\\n                estimator,\\n                self.importance_getter,\\n                transform_func=\"square\",\\n            )\\n            ranks = np.argsort(importances)\\n            ranks = np.ravel(ranks)\\n            threshold = min(step, np.sum(support_) - n_features_to_select)\\n            if step_score:\\n                self.scores_.append(step_score(estimator, features))\\n            support_[features[ranks][:threshold]] = False\\n            ranking_[np.logical_not(support_)] += 1\\n        features = np.arange(n_features)[support_]\\n        self.estimator_ = clone(self.estimator)\\n        self.estimator_.fit(X[:, features], y, **fit_params)\\n        if step_score:\\n            self.scores_.append(step_score(self.estimator_, features))\\n        self.n_features_ = support_.sum()\\n        self.support_ = support_\\n        self.ranking_ = ranking_\\n        return self\\n    @if_delegate_has_method(delegate=\"estimator\")"
  },
  {
    "code": "def strip_entities(value):\\n    \"Returns the given HTML with all entities (&something;) stripped\"\\n    return re.sub(r'&(?:\\w+|\\nstrip_entities = allow_lazy(strip_entities, unicode)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def strip_entities(value):\\n    \"Returns the given HTML with all entities (&something;) stripped\"\\n    return re.sub(r'&(?:\\w+|\\nstrip_entities = allow_lazy(strip_entities, unicode)"
  },
  {
    "code": "def _sample_hiddens(self, v, rng):\\n        p = self._mean_hiddens(v)\\n        p[rng.uniform(size=p.shape) < p] = 1.\\n        return np.floor(p, out=p)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix some numpy 1.3 compat issue",
    "fixed_code": "def _sample_hiddens(self, v, rng):\\n        p = self._mean_hiddens(v)\\n        p[rng.uniform(size=p.shape) < p] = 1.\\n        return np.floor(p, p)"
  },
  {
    "code": "def asfreq(self, freq=None):\\n        if isinstance(freq, basestring):\\n            freq = _interval_freq_map[freq]\\n        new_ordinal = lib.interval_freq_conv(self.ordinal, self.freq, freq)\\n        return Interval(new_ordinal, freq)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: made interval constructor test work",
    "fixed_code": "def asfreq(self, freq=None, how='E'):\\n        if how not in ('S', 'E'):\\n            raise ValueError('How must be one of S or E')\\n        if isinstance(freq, basestring):\\n            freq = _interval_freq_map[freq]\\n        new_ordinal = lib.interval_freq_conv(self.ordinal,\\n                                             self.freq, freq, how)\\n        return Interval(new_ordinal, freq)\\n    @classmethod"
  },
  {
    "code": "def load(self, rawdata, ignore_parse_errors=False):\\n                if ignore_parse_errors:\\n                    self.bad_cookies = []\\n                    self._BaseCookie__set = self._loose_set\\n                super(SimpleCookie, self).load(rawdata)\\n                if ignore_parse_errors:\\n                    self._BaseCookie__set = self._strict_set\\n                    for key in self.bad_cookies:\\n                        del self[key]\\n            _strict_set = Cookie.BaseCookie._BaseCookie__set",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #15852 -- Modified cookie parsing so it can handle duplicate invalid cookie names. Thanks goes to Fredrik St\u00e5lnacke for the report and to vung for the patch.",
    "fixed_code": "def load(self, rawdata, ignore_parse_errors=False):\\n                if ignore_parse_errors:\\n                    self.bad_cookies = set()\\n                    self._BaseCookie__set = self._loose_set\\n                super(SimpleCookie, self).load(rawdata)\\n                if ignore_parse_errors:\\n                    self._BaseCookie__set = self._strict_set\\n                    for key in self.bad_cookies:\\n                        del self[key]\\n            _strict_set = Cookie.BaseCookie._BaseCookie__set"
  },
  {
    "code": "def array2d(X, dtype=None, order=None, copy=False):\\n    if sparse.issparse(X):\\n        raise TypeError('A sparse matrix was passed, but dense data '\\n                        'is required. Use X.todense() to convert to dense.')\\n    X_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\\n    if X is X_2d and copy:\\n        X_2d = X_2d.copy()\\n    return X_2d",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def array2d(X, dtype=None, order=None, copy=False):\\n    if sparse.issparse(X):\\n        raise TypeError('A sparse matrix was passed, but dense data '\\n                        'is required. Use X.todense() to convert to dense.')\\n    X_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\\n    if X is X_2d and copy:\\n        X_2d = X_2d.copy()\\n    return X_2d"
  },
  {
    "code": "def _compile(pattern, flags):\\n    if isinstance(pattern, _pattern_type):\\n        if flags:\\n            raise ValueError(\\n                \"Cannot process flags argument with a compiled pattern\")\\n        return pattern\\n    if not sre_compile.isstring(pattern):\\n        raise TypeError(\"first argument must be string or compiled pattern\")\\n    return sre_compile.compile(pattern, flags)\\n@functools.lru_cache(maxsize=500)\\ndef _compile_repl(repl, pattern):\\n    return sre_parse.parse_template(repl, pattern)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _compile(pattern, flags):\\n    if isinstance(pattern, _pattern_type):\\n        if flags:\\n            raise ValueError(\\n                \"Cannot process flags argument with a compiled pattern\")\\n        return pattern\\n    if not sre_compile.isstring(pattern):\\n        raise TypeError(\"first argument must be string or compiled pattern\")\\n    return sre_compile.compile(pattern, flags)\\n@functools.lru_cache(maxsize=500)\\ndef _compile_repl(repl, pattern):\\n    return sre_parse.parse_template(repl, pattern)"
  },
  {
    "code": "def find_paragraph(text, mark):\\n    lineno, col = map(int, mark.split(\".\"))\\n    line = text.get(\"%d.0\" % lineno, \"%d.0 lineend\" % lineno)\\n    while text.compare(\"%d.0\" % lineno, \"<\", \"end\") and is_all_white(line):\\n        lineno = lineno + 1\\n        line = text.get(\"%d.0\" % lineno, \"%d.0 lineend\" % lineno)\\n    first_lineno = lineno\\n    comment_header = get_comment_header(line)\\n    comment_header_len = len(comment_header)\\n    while get_comment_header(line)==comment_header and \\\\n              not is_all_white(line[comment_header_len:]):\\n        lineno = lineno + 1\\n        line = text.get(\"%d.0\" % lineno, \"%d.0 lineend\" % lineno)\\n    last = \"%d.0\" % lineno\\n    lineno = first_lineno - 1\\n    line = text.get(\"%d.0\" % lineno, \"%d.0 lineend\" % lineno)\\n    while lineno > 0 and \\\\n              get_comment_header(line)==comment_header and \\\\n              not is_all_white(line[comment_header_len:]):\\n        lineno = lineno - 1\\n        line = text.get(\"%d.0\" % lineno, \"%d.0 lineend\" % lineno)\\n    first = \"%d.0\" % (lineno+1)\\n    return first, last, comment_header, text.get(first, last)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #18226: Add docstrings and unittests for idlelib/FormatParagraph.py. Move comment code to a separate function so it can be separately tested. Original patches by Todd Rovito and Phil Webster.",
    "fixed_code": "def find_paragraph(text, mark):\\n    lineno, col = map(int, mark.split(\".\"))\\n    line = text.get(\"%d.0\" % lineno, \"%d.end\" % lineno)\\n    while text.compare(\"%d.0\" % lineno, \"<\", \"end\") and is_all_white(line):\\n        lineno = lineno + 1\\n        line = text.get(\"%d.0\" % lineno, \"%d.end\" % lineno)\\n    first_lineno = lineno\\n    comment_header = get_comment_header(line)\\n    comment_header_len = len(comment_header)\\n    while get_comment_header(line)==comment_header and \\\\n              not is_all_white(line[comment_header_len:]):\\n        lineno = lineno + 1\\n        line = text.get(\"%d.0\" % lineno, \"%d.end\" % lineno)\\n    last = \"%d.0\" % lineno\\n    lineno = first_lineno - 1\\n    line = text.get(\"%d.0\" % lineno, \"%d.end\" % lineno)\\n    while lineno > 0 and \\\\n              get_comment_header(line)==comment_header and \\\\n              not is_all_white(line[comment_header_len:]):\\n        lineno = lineno - 1\\n        line = text.get(\"%d.0\" % lineno, \"%d.end\" % lineno)\\n    first = \"%d.0\" % (lineno+1)\\n    return first, last, comment_header, text.get(first, last)"
  },
  {
    "code": "def map_params_to_obj(module):\\n    if module.params['value'] in ['True', 'False']:\\n        module.params['value'] = module.params['value'].lower()\\n    obj = {\\n        'table': module.params['table'],\\n        'record': module.params['record'],\\n        'col': module.params['col'],\\n        'value': module.params['value']\\n    }\\n    key = module.params['key']\\n    if key is not None:\\n        obj['key'] = key\\n    return obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_params_to_obj(module):\\n    if module.params['value'] in ['True', 'False']:\\n        module.params['value'] = module.params['value'].lower()\\n    obj = {\\n        'table': module.params['table'],\\n        'record': module.params['record'],\\n        'col': module.params['col'],\\n        'value': module.params['value']\\n    }\\n    key = module.params['key']\\n    if key is not None:\\n        obj['key'] = key\\n    return obj"
  },
  {
    "code": "def print_tensor(x, message=''):\\n\\treturn tf.Print(x, [x], message)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix initializers and update ops.",
    "fixed_code": "def print_tensor(x, message=''):\\n\\top = tf.print(message, x, output_stream=sys.stdout)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)"
  },
  {
    "code": "def execute(self, **kwargs):\\n        if not self.api_params:\\n            self.construct_api_call_params()\\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\\n        slack.call(self.method, self.api_params)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4543] Update slack operator to support slackclient v2 (#5519)",
    "fixed_code": "def execute(self, **kwargs):\\n        if not self.api_params:\\n            self.construct_api_call_params()\\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\\n        slack.call(self.method, json=self.api_params)"
  },
  {
    "code": "def run(self, terms, variables, **kwargs):\\n\\t\\tret = []\\n\\t\\tfor term in terms:\\n\\t\\t\\tvar = term.split()[0]\\n\\t\\t\\tret.append(os.getenv(var, ''))\\n\\t\\treturn ret",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix env lookup plugin error on utf8 values (#65541)\\n\\nThis commit fixes issue #65297.\\n\\nThe env lookup plugin used to fail when environment variable value\\ncontained any UTF-8 characters (e.g., \u03b4, \u03b6).",
    "fixed_code": "def run(self, terms, variables, **kwargs):\\n\\t\\tret = []\\n\\t\\tfor term in terms:\\n\\t\\t\\tvar = term.split()[0]\\n\\t\\t\\tret.append(py3compat.environ.get(var, ''))\\n\\t\\treturn ret"
  },
  {
    "code": "def _delete(self, req, id):\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tauthorize(context)\\n\\t\\ttry:\\n\\t\\t\\tflavor = flavors.get_flavor_by_flavor_id(\\n\\t\\t\\t\\t\\tid, ctxt=context, read_deleted=\"no\")\\n\\t\\texcept exception.NotFound as e:\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=e.format_message())\\n\\t\\tflavors.destroy(flavor['name'])\\n\\t\\treturn webob.Response(status_int=202)\\n\\t@wsgi.action(\"create\")\\n\\t@wsgi.serializers(xml=flavors_api.FlavorTemplate)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _delete(self, req, id):\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tauthorize(context)\\n\\t\\ttry:\\n\\t\\t\\tflavor = flavors.get_flavor_by_flavor_id(\\n\\t\\t\\t\\t\\tid, ctxt=context, read_deleted=\"no\")\\n\\t\\texcept exception.NotFound as e:\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=e.format_message())\\n\\t\\tflavors.destroy(flavor['name'])\\n\\t\\treturn webob.Response(status_int=202)\\n\\t@wsgi.action(\"create\")\\n\\t@wsgi.serializers(xml=flavors_api.FlavorTemplate)"
  },
  {
    "code": "def _bins_to_cuts(x, bins, right=True, labels=None, retbins=False,\\n                  precision=3, name=None):\\n    if name is None and isinstance(x, Series):\\n        name = x.name\\n    x = np.asarray(x)\\n    side = 'left' if right else 'right'\\n    ids = bins.searchsorted(x, side=side)\\n    na_mask = com.isnull(x) | (ids == len(bins)) | (ids == 0)\\n    has_nas = na_mask.any()\\n    if labels is not False:\\n        if labels is None:\\n            fmt = lambda v: _format_label(v, precision=precision)\\n            if right:\\n                levels = ['(%s, %s]' % (fmt(a), fmt(b))\\n                           for a, b in zip(bins, bins[1:])]\\n            else:\\n                levels = ['[%s, %s)' % (fmt(a), fmt(b))\\n                           for a, b in zip(bins, bins[1:])]\\n        else:\\n            if len(labels) != len(bins) - 1:\\n                raise ValueError('Bin labels must be one fewer than '\\n                                 'the number of bin edges')\\n            levels = labels\\n        levels = np.asarray(levels, dtype=object)\\n        np.putmask(ids, na_mask, 0)\\n        fac = Categorical(ids - 1, levels, name=name)\\n    else:\\n        fac = ids - 1\\n        if has_nas:\\n            fac = ids.astype(np.float64)\\n            np.putmask(fac, na_mask, np.nan)\\n    if not retbins:\\n        return fac\\n    return fac, bins",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _bins_to_cuts(x, bins, right=True, labels=None, retbins=False,\\n                  precision=3, name=None):\\n    if name is None and isinstance(x, Series):\\n        name = x.name\\n    x = np.asarray(x)\\n    side = 'left' if right else 'right'\\n    ids = bins.searchsorted(x, side=side)\\n    na_mask = com.isnull(x) | (ids == len(bins)) | (ids == 0)\\n    has_nas = na_mask.any()\\n    if labels is not False:\\n        if labels is None:\\n            fmt = lambda v: _format_label(v, precision=precision)\\n            if right:\\n                levels = ['(%s, %s]' % (fmt(a), fmt(b))\\n                           for a, b in zip(bins, bins[1:])]\\n            else:\\n                levels = ['[%s, %s)' % (fmt(a), fmt(b))\\n                           for a, b in zip(bins, bins[1:])]\\n        else:\\n            if len(labels) != len(bins) - 1:\\n                raise ValueError('Bin labels must be one fewer than '\\n                                 'the number of bin edges')\\n            levels = labels\\n        levels = np.asarray(levels, dtype=object)\\n        np.putmask(ids, na_mask, 0)\\n        fac = Categorical(ids - 1, levels, name=name)\\n    else:\\n        fac = ids - 1\\n        if has_nas:\\n            fac = ids.astype(np.float64)\\n            np.putmask(fac, na_mask, np.nan)\\n    if not retbins:\\n        return fac\\n    return fac, bins"
  },
  {
    "code": "def _to_original_callable(obj):\\n        while True:\\n            if inspect.isfunction(obj) or inspect.isclass(obj):\\n                f = inspect.getfile(obj)\\n                if f.startswith('<') and f.endswith('>'):\\n                    return None\\n                return obj\\n            if inspect.ismethod(obj):\\n                obj = obj.__func__\\n            elif isinstance(obj, functools.partial):\\n                obj = obj.func\\n            elif isinstance(obj, property):\\n                obj = obj.fget\\n            else:\\n                return None\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _to_original_callable(obj):\\n        while True:\\n            if inspect.isfunction(obj) or inspect.isclass(obj):\\n                f = inspect.getfile(obj)\\n                if f.startswith('<') and f.endswith('>'):\\n                    return None\\n                return obj\\n            if inspect.ismethod(obj):\\n                obj = obj.__func__\\n            elif isinstance(obj, functools.partial):\\n                obj = obj.func\\n            elif isinstance(obj, property):\\n                obj = obj.fget\\n            else:\\n                return None\\n    @property"
  },
  {
    "code": "def _get_terminal_size_tput():\\n    try:\\n        import subprocess\\n        proc = subprocess.Popen([\"tput\", \"cols\"],\\n                                stdin=subprocess.PIPE,\\n                                stdout=subprocess.PIPE)\\n        output = proc.communicate(input=None)\\n        cols = int(output[0])\\n        proc = subprocess.Popen([\"tput\", \"lines\"],\\n                                stdin=subprocess.PIPE,\\n                                stdout=subprocess.PIPE)\\n        output = proc.communicate(input=None)\\n        rows = int(output[0])\\n        return (cols, rows)\\n    except OSError:\\n        return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes Formatting Exception (#25088)",
    "fixed_code": "def _get_terminal_size_tput():\\n    try:\\n        proc = subprocess.Popen([\"tput\", \"cols\"],\\n                                stdin=subprocess.PIPE,\\n                                stdout=subprocess.PIPE)\\n        output_cols = proc.communicate(input=None)\\n        proc = subprocess.Popen([\"tput\", \"lines\"],\\n                                stdin=subprocess.PIPE,\\n                                stdout=subprocess.PIPE)\\n        output_rows = proc.communicate(input=None)\\n    except OSError:\\n        return None\\n    try:\\n        cols = int(output_cols[0])\\n        rows = int(output_rows[0])\\n        return cols, rows\\n    except (ValueError, IndexError):\\n        return None"
  },
  {
    "code": "def setdefault(self, key, value):\\n            if key not in self:\\n                self[key] = value\\n            return self[key]\\n    try:\\n        _putenv = putenv\\n    except NameError:\\n        _putenv = lambda key, value: None\\n    else:\\n        __all__.append(\"putenv\")\\n    try:\\n        _unsetenv = unsetenv\\n    except NameError:\\n        _unsetenv = lambda key: _putenv(key, \"\")\\n    else:\\n        __all__.append(\"unsetenv\")\\n    if name in ('os2', 'nt'): \\n        _keymap = lambda key: key.upper()\\n    else:  \\n        _keymap = lambda key: key\\n    environ = _Environ(environ, _keymap, _putenv, _unsetenv)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Following an idea by Ron Adam, make sure keys and values in the environ dict are strings (in particular, not 8-bit strings).",
    "fixed_code": "def setdefault(self, key, value):\\n            if key not in self:\\n                self[key] = str(value)\\n            return self[key]\\n    try:\\n        _putenv = putenv\\n    except NameError:\\n        _putenv = lambda key, value: None\\n    else:\\n        __all__.append(\"putenv\")\\n    try:\\n        _unsetenv = unsetenv\\n    except NameError:\\n        _unsetenv = lambda key: _putenv(key, \"\")\\n    else:\\n        __all__.append(\"unsetenv\")\\n    if name in ('os2', 'nt'): \\n        _keymap = lambda key: str(key.upper())\\n    else:  \\n        _keymap = lambda key: str(key)\\n    environ = _Environ(environ, _keymap, _putenv, _unsetenv)"
  },
  {
    "code": "def _ensure_data(values, dtype=None):\\n    if (needs_i8_conversion(values) or\\n            is_period_dtype(dtype) or\\n            is_datetime64_any_dtype(dtype) or\\n            is_timedelta64_dtype(dtype)):\\n        if is_period_dtype(values) or is_period_dtype(dtype):\\n            from pandas import PeriodIndex\\n            values = PeriodIndex(values)\\n            dtype = values.dtype\\n        elif is_timedelta64_dtype(values) or is_timedelta64_dtype(dtype):\\n            from pandas import TimedeltaIndex\\n            values = TimedeltaIndex(values)\\n            dtype = values.dtype\\n        else:\\n            from pandas import DatetimeIndex\\n            values = DatetimeIndex(values)\\n            dtype = values.dtype\\n        return values.asi8, dtype, 'int64'\\n    elif is_categorical_dtype(values) or is_categorical_dtype(dtype):\\n        values = getattr(values, 'values', values)\\n        values = values.codes\\n        dtype = 'category'\\n        values = _ensure_int64(values)\\n        return values, dtype, 'int64'\\n    values = np.asarray(values)\\n    try:\\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\\n            values = values.astype('uint64')\\n            dtype = 'bool'\\n            ndtype = 'uint64'\\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\\n            values = _ensure_int64(values)\\n            ndtype = dtype = 'int64'\\n        elif (is_unsigned_integer_dtype(values) or\\n              is_unsigned_integer_dtype(dtype)):\\n            values = _ensure_uint64(values)\\n            ndtype = dtype = 'uint64'\\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\\n            values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        elif is_float_dtype(values) or is_float_dtype(dtype):\\n            values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        else:\\n            values = _ensure_object(values)\\n            ndtype = dtype = 'object'\\n    except (TypeError, ValueError):\\n        values = _ensure_object(values)\\n        ndtype = dtype = 'object'\\n    return values, dtype, ndtype",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: suppress some warnings (#15932)",
    "fixed_code": "def _ensure_data(values, dtype=None):\\n    if (needs_i8_conversion(values) or\\n            is_period_dtype(dtype) or\\n            is_datetime64_any_dtype(dtype) or\\n            is_timedelta64_dtype(dtype)):\\n        if is_period_dtype(values) or is_period_dtype(dtype):\\n            from pandas import PeriodIndex\\n            values = PeriodIndex(values)\\n            dtype = values.dtype\\n        elif is_timedelta64_dtype(values) or is_timedelta64_dtype(dtype):\\n            from pandas import TimedeltaIndex\\n            values = TimedeltaIndex(values)\\n            dtype = values.dtype\\n        else:\\n            from pandas import DatetimeIndex\\n            values = DatetimeIndex(values)\\n            dtype = values.dtype\\n        return values.asi8, dtype, 'int64'\\n    elif is_categorical_dtype(values) or is_categorical_dtype(dtype):\\n        values = getattr(values, 'values', values)\\n        values = values.codes\\n        dtype = 'category'\\n        values = _ensure_int64(values)\\n        return values, dtype, 'int64'\\n    values = np.asarray(values)\\n    try:\\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\\n            values = values.astype('uint64')\\n            dtype = 'bool'\\n            ndtype = 'uint64'\\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\\n            values = _ensure_int64(values)\\n            ndtype = dtype = 'int64'\\n        elif (is_unsigned_integer_dtype(values) or\\n              is_unsigned_integer_dtype(dtype)):\\n            values = _ensure_uint64(values)\\n            ndtype = dtype = 'uint64'\\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\\n            with catch_warnings(record=True):\\n                values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        elif is_float_dtype(values) or is_float_dtype(dtype):\\n            values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        else:\\n            values = _ensure_object(values)\\n            ndtype = dtype = 'object'\\n    except (TypeError, ValueError):\\n        values = _ensure_object(values)\\n        ndtype = dtype = 'object'\\n    return values, dtype, ndtype"
  },
  {
    "code": "def cdate_range(start=None, end=None, periods=None, freq='C', tz=None,\\n                normalize=True, name=None, closed=None, **kwargs):\\n    if freq == 'C':\\n        holidays = kwargs.pop('holidays', [])\\n        weekmask = kwargs.pop('weekmask', 'Mon Tue Wed Thu Fri')\\n        freq = CDay(holidays=holidays, weekmask=weekmask)\\n    return DatetimeIndex(start=start, end=end, periods=periods, freq=freq,\\n                         tz=tz, normalize=normalize, name=name,\\n                         closed=closed, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: Deprecate cdate_range and merge into bdate_range (#17691)",
    "fixed_code": "def cdate_range(start=None, end=None, periods=None, freq='C', tz=None,\\n                normalize=True, name=None, closed=None, **kwargs):\\n    warnings.warn(\"cdate_range is deprecated and will be removed in a future \"\\n                  \"version, instead use pd.bdate_range(..., freq='{freq}')\"\\n                  .format(freq=freq), FutureWarning, stacklevel=2)\\n    if freq == 'C':\\n        holidays = kwargs.pop('holidays', [])\\n        weekmask = kwargs.pop('weekmask', 'Mon Tue Wed Thu Fri')\\n        freq = CDay(holidays=holidays, weekmask=weekmask)\\n    return DatetimeIndex(start=start, end=end, periods=periods, freq=freq,\\n                         tz=tz, normalize=normalize, name=name,\\n                         closed=closed, **kwargs)"
  },
  {
    "code": "def has_limits(self):\\n        return self.limit_cpu is not None or self.limit_memory is not None or self.limit_gpu is not None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5659] Add support for ephemeral storage on KubernetesPodOperator (#6337)",
    "fixed_code": "def has_limits(self):\\n        return self.limit_cpu is not None or \\\\n            self.limit_memory is not None or \\\\n            self.limit_gpu is not None or \\\\n            self.limit_ephemeral_storage is not None"
  },
  {
    "code": "def rollback(self, transaction):\\n        conn = self.get_conn()\\n        conn.projects().rollback(projectId=self.project_id, body={'transaction': transaction}).execute()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rollback(self, transaction):\\n        conn = self.get_conn()\\n        conn.projects().rollback(projectId=self.project_id, body={'transaction': transaction}).execute()"
  },
  {
    "code": "def handle(self, **options):\\n        database = options.get('database')\\n        connection = connections[database]\\n        verbosity = options.get('verbosity')\\n        interactive = options.get('interactive')\\n        reset_sequences = options.get('reset_sequences', True)\\n        allow_cascade = options.get('allow_cascade', False)\\n        inhibit_post_migrate = options.get('inhibit_post_migrate', False)\\n        self.style = no_style()\\n        for app_config in apps.get_app_configs():\\n            try:\\n                import_module('.management', app_config.name)\\n            except ImportError:\\n                pass\\n        sql_list = sql_flush(self.style, connection, only_django=True,\\n                             reset_sequences=reset_sequences,\\n                             allow_cascade=allow_cascade)\\n        if interactive:\\n            confirm = input( % connection.settings_dict['NAME'])\\n        else:\\n            confirm = 'yes'\\n        if confirm == 'yes':\\n            try:\\n                with transaction.atomic(using=database,\\n                                        savepoint=connection.features.can_rollback_ddl):\\n                    with connection.cursor() as cursor:\\n                        for sql in sql_list:\\n                            cursor.execute(sql)\\n            except Exception as e:\\n                new_msg = (\\n                    \"Database %s couldn't be flushed. Possible reasons:\\n\"\\n                    \"  * The database isn't running or isn't configured correctly.\\n\"\\n                    \"  * At least one of the expected database tables doesn't exist.\\n\"\\n                    \"  * The SQL was invalid.\\n\"\\n                    \"Hint: Look at the output of 'django-admin sqlflush'. \"\\n                    \"That's the SQL this command wasn't able to run.\\n\"\\n                    \"The full error: %s\") % (connection.settings_dict['NAME'], e)\\n                six.reraise(CommandError, CommandError(new_msg), sys.exc_info()[2])\\n            if not inhibit_post_migrate:\\n                self.emit_post_migrate(verbosity, interactive, database)\\n        else:\\n            self.stdout.write(\"Flush cancelled.\\n\")\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def handle(self, **options):\\n        database = options.get('database')\\n        connection = connections[database]\\n        verbosity = options.get('verbosity')\\n        interactive = options.get('interactive')\\n        reset_sequences = options.get('reset_sequences', True)\\n        allow_cascade = options.get('allow_cascade', False)\\n        inhibit_post_migrate = options.get('inhibit_post_migrate', False)\\n        self.style = no_style()\\n        for app_config in apps.get_app_configs():\\n            try:\\n                import_module('.management', app_config.name)\\n            except ImportError:\\n                pass\\n        sql_list = sql_flush(self.style, connection, only_django=True,\\n                             reset_sequences=reset_sequences,\\n                             allow_cascade=allow_cascade)\\n        if interactive:\\n            confirm = input( % connection.settings_dict['NAME'])\\n        else:\\n            confirm = 'yes'\\n        if confirm == 'yes':\\n            try:\\n                with transaction.atomic(using=database,\\n                                        savepoint=connection.features.can_rollback_ddl):\\n                    with connection.cursor() as cursor:\\n                        for sql in sql_list:\\n                            cursor.execute(sql)\\n            except Exception as e:\\n                new_msg = (\\n                    \"Database %s couldn't be flushed. Possible reasons:\\n\"\\n                    \"  * The database isn't running or isn't configured correctly.\\n\"\\n                    \"  * At least one of the expected database tables doesn't exist.\\n\"\\n                    \"  * The SQL was invalid.\\n\"\\n                    \"Hint: Look at the output of 'django-admin sqlflush'. \"\\n                    \"That's the SQL this command wasn't able to run.\\n\"\\n                    \"The full error: %s\") % (connection.settings_dict['NAME'], e)\\n                six.reraise(CommandError, CommandError(new_msg), sys.exc_info()[2])\\n            if not inhibit_post_migrate:\\n                self.emit_post_migrate(verbosity, interactive, database)\\n        else:\\n            self.stdout.write(\"Flush cancelled.\\n\")\\n    @staticmethod"
  },
  {
    "code": "def fit_transform(self, X, y=None, **params):\\n        self._set_params(**params)\\n        X = np.asanyarray(X)\\n        U, V, E = sparse_pca(X, self.n_components, self.alpha, tol=self.tol,\\n                             max_iter=self.max_iter, method=self.method,\\n                             n_jobs=self.n_jobs)\\n        self.components_ = V\\n        self.error_ = E\\n        return U",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Doc enhancement, added alpha in transform",
    "fixed_code": "def fit_transform(self, X, y=None, **params):\\n        self._set_params(**params)\\n        X = np.asanyarray(X)\\n        U, V, E = sparse_pca(X, self.n_components, self.alpha, tol=self.tol,\\n                             max_iter=self.max_iter, method=self.method,\\n                             n_jobs=self.n_jobs, verbose=self.verbose)\\n        self.components_ = V\\n        self.error_ = E\\n        return U"
  },
  {
    "code": "def _build_metrics(func_name, namespace):\\n\\tmetrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\\n\\t\\t\\t   'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\\n\\tif not isinstance(namespace, Namespace):\\n\\t\\traise ValueError(\"namespace argument should be argparse.Namespace instance,\"\\n\\t\\t\\t\\t\\t\\t f\"but is {type(namespace)}\")\\n\\ttmp_dic = vars(namespace)\\n\\tmetrics['dag_id'] = tmp_dic.get('dag_id')\\n\\tmetrics['task_id'] = tmp_dic.get('task_id')\\n\\tmetrics['execution_date'] = tmp_dic.get('execution_date')\\n\\tmetrics['host_name'] = socket.gethostname()\\n\\textra = json.dumps({k: metrics[k] for k in ('host_name', 'full_command')})\\n\\tlog = Log(\\n\\t\\tevent='cli_{}'.format(func_name),\\n\\t\\ttask_instance=None,\\n\\t\\towner=metrics['user'],\\n\\t\\textra=extra,\\n\\t\\ttask_id=metrics.get('task_id'),\\n\\t\\tdag_id=metrics.get('dag_id'),\\n\\t\\texecution_date=metrics.get('execution_date'))\\n\\tmetrics['log'] = log\\n\\treturn metrics",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Mask Password in Log table when using the CLI (#11468)",
    "fixed_code": "def _build_metrics(func_name, namespace):\\n\\tsensitive_fields = {'-p', '--password', '--conn-password'}\\n\\tfull_command = list(sys.argv)\\n\\tfor idx, command in enumerate(full_command):  \\n\\t\\tif command in sensitive_fields:\\n\\t\\t\\tfull_command[idx + 1] = \"*\" * 8\\n\\t\\telse:\\n\\t\\t\\tfor sensitive_field in sensitive_fields:\\n\\t\\t\\t\\tif command.startswith(f'{sensitive_field}='):\\n\\t\\t\\t\\t\\tfull_command[idx] = f'{sensitive_field}={\"*\" * 8}'\\n\\tmetrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\\n\\t\\t\\t   'full_command': '{}'.format(full_command), 'user': getpass.getuser()}\\n\\tif not isinstance(namespace, Namespace):\\n\\t\\traise ValueError(\"namespace argument should be argparse.Namespace instance,\"\\n\\t\\t\\t\\t\\t\\t f\"but is {type(namespace)}\")\\n\\ttmp_dic = vars(namespace)\\n\\tmetrics['dag_id'] = tmp_dic.get('dag_id')\\n\\tmetrics['task_id'] = tmp_dic.get('task_id')\\n\\tmetrics['execution_date'] = tmp_dic.get('execution_date')\\n\\tmetrics['host_name'] = socket.gethostname()\\n\\textra = json.dumps({k: metrics[k] for k in ('host_name', 'full_command')})\\n\\tlog = Log(\\n\\t\\tevent='cli_{}'.format(func_name),\\n\\t\\ttask_instance=None,\\n\\t\\towner=metrics['user'],\\n\\t\\textra=extra,\\n\\t\\ttask_id=metrics.get('task_id'),\\n\\t\\tdag_id=metrics.get('dag_id'),\\n\\t\\texecution_date=metrics.get('execution_date'))\\n\\tmetrics['log'] = log\\n\\treturn metrics"
  },
  {
    "code": "def main(args=None):\\n    import argparse\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--output', '-o', default=None,\\n            help=\"The name of the output archive. \"\\n                 \"Required if SOURCE is an archive.\")\\n    parser.add_argument('--python', '-p', default=None,\\n            help=\"The name of the Python interpreter to use \"\\n                 \"(default: no shebang line).\")\\n    parser.add_argument('--main', '-m', default=None,\\n            help=\"The main function of the application \"\\n                 \"(default: use an existing __main__.py).\")\\n    parser.add_argument('--info', default=False, action='store_true',\\n            help=\"Display the interpreter from the archive.\")\\n    parser.add_argument('source',\\n            help=\"Source directory (or existing archive).\")\\n    args = parser.parse_args(args)\\n    if args.info:\\n        if not os.path.isfile(args.source):\\n            raise SystemExit(\"Can only get info for an archive file\")\\n        interpreter = get_interpreter(args.source)\\n        print(\"Interpreter: {}\".format(interpreter or \"<none>\"))\\n        sys.exit(0)\\n    if os.path.isfile(args.source):\\n        if args.output is None or (os.path.exists(args.output) and\\n                                   os.path.samefile(args.source, args.output)):\\n            raise SystemExit(\"In-place editing of archives is not supported\")\\n        if args.main:\\n            raise SystemExit(\"Cannot change the main function when copying\")\\n    create_archive(args.source, args.output,\\n                   interpreter=args.python, main=args.main)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-31638: Add compression support to zipapp (GH-3819)\\n\\nAdd optional argument `compressed` to `zipapp.create_archive`, and add\\noption `--compress` to the command line interface of `zipapp`.",
    "fixed_code": "def main(args=None):\\n    import argparse\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--output', '-o', default=None,\\n            help=\"The name of the output archive. \"\\n                 \"Required if SOURCE is an archive.\")\\n    parser.add_argument('--python', '-p', default=None,\\n            help=\"The name of the Python interpreter to use \"\\n                 \"(default: no shebang line).\")\\n    parser.add_argument('--main', '-m', default=None,\\n            help=\"The main function of the application \"\\n                 \"(default: use an existing __main__.py).\")\\n    parser.add_argument('--compress', '-c', action='store_true',\\n            help=\"Compress files with the deflate method. \"\\n                 \"Files are stored uncompressed by default.\")\\n    parser.add_argument('--info', default=False, action='store_true',\\n            help=\"Display the interpreter from the archive.\")\\n    parser.add_argument('source',\\n            help=\"Source directory (or existing archive).\")\\n    args = parser.parse_args(args)\\n    if args.info:\\n        if not os.path.isfile(args.source):\\n            raise SystemExit(\"Can only get info for an archive file\")\\n        interpreter = get_interpreter(args.source)\\n        print(\"Interpreter: {}\".format(interpreter or \"<none>\"))\\n        sys.exit(0)\\n    if os.path.isfile(args.source):\\n        if args.output is None or (os.path.exists(args.output) and\\n                                   os.path.samefile(args.source, args.output)):\\n            raise SystemExit(\"In-place editing of archives is not supported\")\\n        if args.main:\\n            raise SystemExit(\"Cannot change the main function when copying\")\\n    create_archive(args.source, args.output,\\n                   interpreter=args.python, main=args.main,\\n                   compressed=args.compress)"
  },
  {
    "code": "def __init__(self, obj, func, broadcast, raw, reduce, result_type,\\n\\t\\t\\t\\t ignore_failures, args, kwds):\\n\\t\\tself.obj = obj\\n\\t\\tself.raw = raw\\n\\t\\tself.ignore_failures = ignore_failures\\n\\t\\tself.args = args or ()\\n\\t\\tself.kwds = kwds or {}\\n\\t\\tif result_type not in [None, 'reduce', 'broadcast', 'expand']:\\n\\t\\t\\traise ValueError(\"invalid value for result_type, must be one \"\\n\\t\\t\\t\\t\\t\\t\\t \"of {None, 'reduce', 'broadcast', 'expand'}\")\\n\\t\\tif broadcast is not None:\\n\\t\\t\\twarnings.warn(\"The broadcast argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='broadcast' to broadcast the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif broadcast:\\n\\t\\t\\t\\tresult_type = 'broadcast'\\n\\t\\tif reduce is not None:\\n\\t\\t\\twarnings.warn(\"The reduce argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='reduce' to try to reduce the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif reduce:\\n\\t\\t\\t\\tif result_type is not None:\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\t\"cannot pass both reduce=True and result_type\")\\n\\t\\t\\t\\tresult_type = 'reduce'\\n\\t\\tself.result_type = result_type\\n\\t\\tif kwds or args and not isinstance(func, np.ufunc):\\n\\t\\t\\tdef f(x):\\n\\t\\t\\t\\treturn func(x, *args, **kwds)\\n\\t\\telse:\\n\\t\\t\\tf = func\\n\\t\\tself.f = f\\n\\t\\tself.result = None\\n\\t\\tself.res_index = None\\n\\t\\tself.res_columns = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix DataFrame.apply for string arg with additional args (#22376) (#22377)",
    "fixed_code": "def __init__(self, obj, func, broadcast, raw, reduce, result_type,\\n\\t\\t\\t\\t ignore_failures, args, kwds):\\n\\t\\tself.obj = obj\\n\\t\\tself.raw = raw\\n\\t\\tself.ignore_failures = ignore_failures\\n\\t\\tself.args = args or ()\\n\\t\\tself.kwds = kwds or {}\\n\\t\\tif result_type not in [None, 'reduce', 'broadcast', 'expand']:\\n\\t\\t\\traise ValueError(\"invalid value for result_type, must be one \"\\n\\t\\t\\t\\t\\t\\t\\t \"of {None, 'reduce', 'broadcast', 'expand'}\")\\n\\t\\tif broadcast is not None:\\n\\t\\t\\twarnings.warn(\"The broadcast argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='broadcast' to broadcast the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif broadcast:\\n\\t\\t\\t\\tresult_type = 'broadcast'\\n\\t\\tif reduce is not None:\\n\\t\\t\\twarnings.warn(\"The reduce argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='reduce' to try to reduce the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif reduce:\\n\\t\\t\\t\\tif result_type is not None:\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\t\"cannot pass both reduce=True and result_type\")\\n\\t\\t\\t\\tresult_type = 'reduce'\\n\\t\\tself.result_type = result_type\\n\\t\\tif ((kwds or args) and\\n\\t\\t\\t\\tnot isinstance(func, (np.ufunc, compat.string_types))):\\n\\t\\t\\tdef f(x):\\n\\t\\t\\t\\treturn func(x, *args, **kwds)\\n\\t\\telse:\\n\\t\\t\\tf = func\\n\\t\\tself.f = f\\n\\t\\tself.result = None\\n\\t\\tself.res_index = None\\n\\t\\tself.res_columns = None"
  },
  {
    "code": "def downgrade(migrate_engine):\\n\\tmeta = MetaData()\\n\\tmeta.bind = migrate_engine\\n\\tvolumes = Table('volumes', meta, autoload=True)\\n\\tattach_string = Column('attachtime_string', String(255))\\n\\tattach_string.create(volumes)\\n\\told_attachtime = volumes.c.attach_time\\n\\ttry:\\n\\t\\tvolumes_list = list(volumes.select().execute())\\n\\t\\tfor v in volumes_list:\\n\\t\\t\\tattach_time = select([volumes.c.attach_time],\\n\\t\\t\\t\\tvolumes.c.id == v['id'])\\n\\t\\t\\tvolumes.update().\\\\n\\t\\t\\t\\twhere(volumes.c.id == v['id']).\\\\n\\t\\t\\t\\tvalues(attachtime_string=attach_time).execute()\\n\\texcept Exception:\\n\\t\\tattach_string.drop()\\n\\t\\traise\\n\\told_attachtime.alter(name='attach_time_old')\\n\\tattach_string.alter(name='attach_time')\\n\\told_attachtime.drop()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def downgrade(migrate_engine):\\n\\tmeta = MetaData()\\n\\tmeta.bind = migrate_engine\\n\\tvolumes = Table('volumes', meta, autoload=True)\\n\\tattach_string = Column('attachtime_string', String(255))\\n\\tattach_string.create(volumes)\\n\\told_attachtime = volumes.c.attach_time\\n\\ttry:\\n\\t\\tvolumes_list = list(volumes.select().execute())\\n\\t\\tfor v in volumes_list:\\n\\t\\t\\tattach_time = select([volumes.c.attach_time],\\n\\t\\t\\t\\tvolumes.c.id == v['id'])\\n\\t\\t\\tvolumes.update().\\\\n\\t\\t\\t\\twhere(volumes.c.id == v['id']).\\\\n\\t\\t\\t\\tvalues(attachtime_string=attach_time).execute()\\n\\texcept Exception:\\n\\t\\tattach_string.drop()\\n\\t\\traise\\n\\told_attachtime.alter(name='attach_time_old')\\n\\tattach_string.alter(name='attach_time')\\n\\told_attachtime.drop()"
  },
  {
    "code": "def __init__(self, *args):\\n        self.args = args",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Followup to issue #11140 and r88682: also patch _dummy_thread. Patch by Aymeric Augustin.",
    "fixed_code": "def __init__(self):\\n        self.locked_status = False"
  },
  {
    "code": "def translate_pattern(pattern, anchor=1, prefix=None, is_regex=0):\\n    if is_regex:\\n        if isinstance(pattern, str):\\n            return re.compile(pattern)\\n        else:\\n            return pattern\\n    if pattern:\\n        pattern_re = glob_to_re(pattern)\\n    else:\\n        pattern_re = ''\\n    if prefix is not None:\\n        empty_pattern = glob_to_re('')\\n        prefix_re = (glob_to_re(prefix))[:-len(empty_pattern)]\\n        pattern_re = \"^%s/.*%s\" % (prefix_re, pattern_re)\\n    else:                               \\n        if anchor:\\n            pattern_re = \"^\" + pattern_re\\n    return re.compile(pattern_re)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix long-standing bugs with MANIFEST.in parsing on Windows (#6884).\\n\\nThese regex changes fix a number of issues for distutils on Windows:\\n- #6884: impossible to include a file starting with 'build'\\n- #9691 and #14004: sdist includes too many files\\n- #13193: test_filelist failures\\n\\nThis commit replaces the incorrect changes done in 0a94e2f807c7 and\\n90b30d62caf2 to fix #13193; we were too eager to fix the test failures\\nand I did not study the code enough before greenlighting patches.  This\\ntime we have unit tests from the problems reported by users to be sure\\nwe have the right fix.\\n\\nThanks to Nadeem Vawda for his help.",
    "fixed_code": "def translate_pattern(pattern, anchor=1, prefix=None, is_regex=0):\\n    if is_regex:\\n        if isinstance(pattern, str):\\n            return re.compile(pattern)\\n        else:\\n            return pattern\\n    if pattern:\\n        pattern_re = glob_to_re(pattern)\\n    else:\\n        pattern_re = ''\\n    if prefix is not None:\\n        empty_pattern = glob_to_re('')\\n        prefix_re = glob_to_re(prefix)[:-len(empty_pattern)]\\n        sep = os.sep\\n        if os.sep == '\\\\':\\n            sep = r'\\\\'\\n        pattern_re = \"^\" + sep.join((prefix_re, \".*\" + pattern_re))\\n    else:                               \\n        if anchor:\\n            pattern_re = \"^\" + pattern_re\\n    return re.compile(pattern_re)"
  },
  {
    "code": "def open(filename):\\n    buffer = _builtin_open(filename, 'rb')\\n    encoding, lines = detect_encoding(buffer.readline)\\n    buffer.seek(0)\\n    text = TextIOWrapper(buffer, encoding, line_buffering=True)\\n    text.mode = 'r'\\n    return text",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #23840: tokenize.open() now closes the temporary binary file on error to fix a resource warning.",
    "fixed_code": "def open(filename):\\n    buffer = _builtin_open(filename, 'rb')\\n    try:\\n        encoding, lines = detect_encoding(buffer.readline)\\n        buffer.seek(0)\\n        text = TextIOWrapper(buffer, encoding, line_buffering=True)\\n        text.mode = 'r'\\n        return text\\n    except:\\n        buffer.close()\\n        raise"
  },
  {
    "code": "def EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\\n    if file_encoding is None:\\n        file_encoding = data_encoding\\n    info = lookup(data_encoding)\\n    sr = StreamRecoder(file, info.encode, info.decode,\\n                       info.streamreader, info.streamwriter, errors)\\n    sr.data_encoding = data_encoding\\n    sr.file_encoding = file_encoding\\n    return sr",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\\n    if file_encoding is None:\\n        file_encoding = data_encoding\\n    info = lookup(data_encoding)\\n    sr = StreamRecoder(file, info.encode, info.decode,\\n                       info.streamreader, info.streamwriter, errors)\\n    sr.data_encoding = data_encoding\\n    sr.file_encoding = file_encoding\\n    return sr"
  },
  {
    "code": "def feed_data(self, data):\\n        if not data:\\n            return\\n        self._buffer.append(data)\\n        self._byte_count += len(data)\\n        waiter = self._waiter\\n        if waiter is not None:\\n            self._waiter = None\\n            if not waiter.cancelled():\\n                waiter.set_result(False)\\n        if (self._transport is not None and\\n            not self._paused and\\n            self._byte_count > 2*self._limit):\\n            try:\\n                self._transport.pause_reading()\\n            except NotImplementedError:\\n                self._transport = None\\n            else:\\n                self._paused = True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio.streams: Use bytebuffer in StreamReader; Add assertion in feed_data",
    "fixed_code": "def feed_data(self, data):\\n        assert not self._eof, 'feed_data after feed_eof'\\n        if not data:\\n            return\\n        self._buffer.extend(data)\\n        waiter = self._waiter\\n        if waiter is not None:\\n            self._waiter = None\\n            if not waiter.cancelled():\\n                waiter.set_result(False)\\n        if (self._transport is not None and\\n            not self._paused and\\n            len(self._buffer) > 2*self._limit):\\n            try:\\n                self._transport.pause_reading()\\n            except NotImplementedError:\\n                self._transport = None\\n            else:\\n                self._paused = True"
  },
  {
    "code": "def init_ndarray(values, index, columns, dtype=None, copy=False):\\n\\tif isinstance(values, ABCSeries):\\n\\t\\tif columns is None:\\n\\t\\t\\tif values.name is not None:\\n\\t\\t\\t\\tcolumns = [values.name]\\n\\t\\tif index is None:\\n\\t\\t\\tindex = values.index\\n\\t\\telse:\\n\\t\\t\\tvalues = values.reindex(index)\\n\\t\\tif not len(values) and columns is not None and len(columns):\\n\\t\\t\\tvalues = np.empty((0, 1), dtype=object)\\n\\tif is_categorical_dtype(getattr(values, \"dtype\", None)) or is_categorical_dtype(\\n\\t\\tdtype\\n\\t):\\n\\t\\tif not hasattr(values, \"dtype\"):\\n\\t\\t\\tvalues = prep_ndarray(values, copy=copy)\\n\\t\\t\\tvalues = values.ravel()\\n\\t\\telif copy:\\n\\t\\t\\tvalues = values.copy()\\n\\t\\tindex, columns = _get_axes(len(values), 1, index, columns)\\n\\t\\treturn arrays_to_mgr([values], columns, index, columns, dtype=dtype)\\n\\telif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\\n\\t\\tif columns is None:\\n\\t\\t\\tcolumns = [0]\\n\\t\\treturn arrays_to_mgr([values], columns, index, columns, dtype=dtype)\\n\\tvalues = prep_ndarray(values, copy=copy)\\n\\tif dtype is not None:\\n\\t\\tif not is_dtype_equal(values.dtype, dtype):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tvalues = values.astype(dtype)\\n\\t\\t\\texcept Exception as orig:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\tf\"failed to cast to '{dtype}' (Exception was: {orig})\"\\n\\t\\t\\t\\t) from orig\\n\\tindex, columns = _get_axes(*values.shape, index=index, columns=columns)\\n\\tvalues = values.T\\n\\tif dtype is None and is_object_dtype(values):\\n\\t\\tif values.ndim == 2 and values.shape[0] != 1:\\n\\t\\t\\tdvals_list = [maybe_infer_to_datetimelike(row) for row in values]\\n\\t\\t\\tfor n in range(len(dvals_list)):\\n\\t\\t\\t\\tif isinstance(dvals_list[n], np.ndarray):\\n\\t\\t\\t\\t\\tdvals_list[n] = dvals_list[n].reshape(1, -1)\\n\\t\\t\\tfrom pandas.core.internals.blocks import make_block\\n\\t\\t\\tblock_values = [\\n\\t\\t\\t\\tmake_block(dvals_list[n], placement=[n]) for n in range(len(dvals_list))\\n\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tdatelike_vals = maybe_infer_to_datetimelike(values)\\n\\t\\t\\tblock_values = [datelike_vals]\\n\\telse:\\n\\t\\tblock_values = [values]\\n\\treturn create_block_manager_from_blocks(block_values, [columns, index])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pass 2D ndarray and EA-dtype to DataFrame, closes #12513 (#30507)",
    "fixed_code": "def init_ndarray(values, index, columns, dtype=None, copy=False):\\n\\tif isinstance(values, ABCSeries):\\n\\t\\tif columns is None:\\n\\t\\t\\tif values.name is not None:\\n\\t\\t\\t\\tcolumns = [values.name]\\n\\t\\tif index is None:\\n\\t\\t\\tindex = values.index\\n\\t\\telse:\\n\\t\\t\\tvalues = values.reindex(index)\\n\\t\\tif not len(values) and columns is not None and len(columns):\\n\\t\\t\\tvalues = np.empty((0, 1), dtype=object)\\n\\tif is_categorical_dtype(getattr(values, \"dtype\", None)) or is_categorical_dtype(\\n\\t\\tdtype\\n\\t):\\n\\t\\tif not hasattr(values, \"dtype\"):\\n\\t\\t\\tvalues = prep_ndarray(values, copy=copy)\\n\\t\\t\\tvalues = values.ravel()\\n\\t\\telif copy:\\n\\t\\t\\tvalues = values.copy()\\n\\t\\tindex, columns = _get_axes(len(values), 1, index, columns)\\n\\t\\treturn arrays_to_mgr([values], columns, index, columns, dtype=dtype)\\n\\telif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\\n\\t\\tif isinstance(values, np.ndarray) and values.ndim > 1:\\n\\t\\t\\tvalues = [values[:, n] for n in range(values.shape[1])]\\n\\t\\telse:\\n\\t\\t\\tvalues = [values]\\n\\t\\tif columns is None:\\n\\t\\t\\tcolumns = list(range(len(values)))\\n\\t\\treturn arrays_to_mgr(values, columns, index, columns, dtype=dtype)\\n\\tvalues = prep_ndarray(values, copy=copy)\\n\\tif dtype is not None:\\n\\t\\tif not is_dtype_equal(values.dtype, dtype):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tvalues = values.astype(dtype)\\n\\t\\t\\texcept Exception as orig:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\tf\"failed to cast to '{dtype}' (Exception was: {orig})\"\\n\\t\\t\\t\\t) from orig\\n\\tindex, columns = _get_axes(*values.shape, index=index, columns=columns)\\n\\tvalues = values.T\\n\\tif dtype is None and is_object_dtype(values):\\n\\t\\tif values.ndim == 2 and values.shape[0] != 1:\\n\\t\\t\\tdvals_list = [maybe_infer_to_datetimelike(row) for row in values]\\n\\t\\t\\tfor n in range(len(dvals_list)):\\n\\t\\t\\t\\tif isinstance(dvals_list[n], np.ndarray):\\n\\t\\t\\t\\t\\tdvals_list[n] = dvals_list[n].reshape(1, -1)\\n\\t\\t\\tfrom pandas.core.internals.blocks import make_block\\n\\t\\t\\tblock_values = [\\n\\t\\t\\t\\tmake_block(dvals_list[n], placement=[n]) for n in range(len(dvals_list))\\n\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tdatelike_vals = maybe_infer_to_datetimelike(values)\\n\\t\\t\\tblock_values = [datelike_vals]\\n\\telse:\\n\\t\\tblock_values = [values]\\n\\treturn create_block_manager_from_blocks(block_values, [columns, index])"
  },
  {
    "code": "def getdecoder(encoding):\\n    return lookup(encoding)[1]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Patch #1436130: codecs.lookup() now returns a CodecInfo object (a subclass of tuple) that provides incremental decoders and encoders (a way to use stateful codecs without the stream API). Functions codecs.getincrementaldecoder() and codecs.getincrementalencoder() have been added.",
    "fixed_code": "def getdecoder(encoding):\\n    return lookup(encoding).decode"
  },
  {
    "code": "def update_error_dict(self, error_dict):\\n        if hasattr(self, 'error_dict'):\\n            for field, error_list in self.error_dict.items():\\n                error_dict.setdefault(field, []).extend(error_list)\\n        else:\\n            error_dict.setdefault(NON_FIELD_ERRORS, []).extend(self.error_list)\\n        return error_dict",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_error_dict(self, error_dict):\\n        if hasattr(self, 'error_dict'):\\n            for field, error_list in self.error_dict.items():\\n                error_dict.setdefault(field, []).extend(error_list)\\n        else:\\n            error_dict.setdefault(NON_FIELD_ERRORS, []).extend(self.error_list)\\n        return error_dict"
  },
  {
    "code": "def request(self, method, url, body=None, headers={}):\\n\\t\\tself.req = webob.Request.blank(url, method=method, headers=headers)\\n\\t\\tself.req.body = body",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Uses None instead of mutables for function param defaults\\n\\nAddressing bug 1307878, changes use of mutable lists and dicts as\\ndefault arguments and defaults them within the function. Otherwise,\\nthose defaults can be unexpectedly persisted with the function between\\ninvocations and erupt into mass hysteria on the streets.\\n\\nTo my knowledge there aren't known cases of the current use causing\\nspecific issues, but needs addressing (even stylistically) to avoid\\nproblems in the future -- ones that may crop up as extremely subtle or\\nintermittent bugs...or worse, security vulnerabilities.\\n\\nIn Glance's case there are ACL-related methods using this, so\\nalthough I haven't confirmed one way or the other yet, I've marked it\\nwith SecurityImpact so that a more knowledgeable set of eyes can\\nreview it in this context as well.\\n\\nCloses-Bug: #1307878\\nSecurityImpact",
    "fixed_code": "def request(self, method, url, body=None, headers=None):\\n\\t\\tif headers is None:\\n\\t\\t\\theaders = {}\\n\\t\\tself.req = webob.Request.blank(url, method=method, headers=headers)\\n\\t\\tself.req.body = body"
  },
  {
    "code": "def _get_slice_axis(self, slice_obj, axis=0):\\n        frame = self.frame\\n        axis_name = frame._get_axis_name(axis)\\n        labels = getattr(frame, axis_name)\\n        if _is_label_slice(labels, slice_obj):\\n            i, j = labels.slice_locs(slice_obj.start, slice_obj.stop)\\n            slicer = slice(i, j)\\n        else:\\n            slicer = slice_obj\\n        if not _need_slice(slice_obj):\\n            return frame\\n        if axis == 0:\\n            new_index = frame.index[slicer]\\n            new_columns = frame.columns\\n            new_values = frame.values[slicer]\\n        else:\\n            new_index = frame.index\\n            new_columns = frame.columns[slicer]\\n            new_values = frame.values[:, slicer]\\n        return DataFrame(new_values, index=new_index,\\n                         columns=new_columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_slice_axis(self, slice_obj, axis=0):\\n        frame = self.frame\\n        axis_name = frame._get_axis_name(axis)\\n        labels = getattr(frame, axis_name)\\n        if _is_label_slice(labels, slice_obj):\\n            i, j = labels.slice_locs(slice_obj.start, slice_obj.stop)\\n            slicer = slice(i, j)\\n        else:\\n            slicer = slice_obj\\n        if not _need_slice(slice_obj):\\n            return frame\\n        if axis == 0:\\n            new_index = frame.index[slicer]\\n            new_columns = frame.columns\\n            new_values = frame.values[slicer]\\n        else:\\n            new_index = frame.index\\n            new_columns = frame.columns[slicer]\\n            new_values = frame.values[:, slicer]\\n        return DataFrame(new_values, index=new_index,\\n                         columns=new_columns)"
  },
  {
    "code": "def open(self, mode='a'):\\n        tables = _tables()\\n        if self._mode != mode:\\n            if self._mode in ['a', 'w'] and mode in ['r', 'r+']:\\n                pass\\n            elif mode in ['w']:\\n                if self.is_open:\\n                    raise PossibleDataLossError(\"Re-opening the file [{0}] with mode [{1}] \"\\n                                                \"will delete the current file!\".format(self._path, self._mode))\\n            self._mode = mode\\n        if self.is_open:\\n            self.close()\\n        if self._complib is not None:\\n            if self._complevel is None:\\n                self._complevel = 9\\n            self._filters = _tables().Filters(self._complevel,\\n                                              self._complib,\\n                                              fletcher32=self._fletcher32)\\n        try:\\n            self._handle = tables.openFile(self._path, self._mode)\\n        except (IOError) as e:  \\n            if 'can not be written' in str(e):\\n                print('Opening %s in read-only mode' % self._path)\\n                self._handle = tables.openFile(self._path, 'r')\\n            else:\\n                raise\\n        except (Exception) as e:\\n            if self._mode == 'r' and 'Unable to open/create file' in str(e):\\n                raise IOError(str(e))\\n            raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API/ENH: pass thru store creation arguments for HDFStore; can be used to support in-memory stores",
    "fixed_code": "def open(self, mode='a', **kwargs):\\n        tables = _tables()\\n        if self._mode != mode:\\n            if self._mode in ['a', 'w'] and mode in ['r', 'r+']:\\n                pass\\n            elif mode in ['w']:\\n                if self.is_open:\\n                    raise PossibleDataLossError(\"Re-opening the file [{0}] with mode [{1}] \"\\n                                                \"will delete the current file!\".format(self._path, self._mode))\\n            self._mode = mode\\n        if self.is_open:\\n            self.close()\\n        if self._complib is not None:\\n            if self._complevel is None:\\n                self._complevel = 9\\n            self._filters = _tables().Filters(self._complevel,\\n                                              self._complib,\\n                                              fletcher32=self._fletcher32)\\n        try:\\n            self._handle = tables.openFile(self._path, self._mode, **kwargs)\\n        except (IOError) as e:  \\n            if 'can not be written' in str(e):\\n                print('Opening %s in read-only mode' % self._path)\\n                self._handle = tables.openFile(self._path, 'r', **kwargs)\\n            else:\\n                raise\\n        except (Exception) as e:\\n            if self._mode == 'r' and 'Unable to open/create file' in str(e):\\n                raise IOError(str(e))\\n            raise"
  },
  {
    "code": "def _class_escape(source, escape):\\n    code = ESCAPES.get(escape)\\n    if code:\\n        return code\\n    code = CATEGORIES.get(escape)\\n    if code:\\n        return code\\n    try:\\n        c = escape[1:2]\\n        if c == \"x\":\\n            while source.next in HEXDIGITS and len(escape) < 4:\\n                escape = escape + source.get()\\n            escape = escape[2:]\\n            if len(escape) != 2:\\n                raise error, \"bogus escape: %s\" % repr(\"\\\\\" + escape)\\n            return LITERAL, int(escape, 16) & 0xff\\n        elif c in OCTDIGITS:\\n            while source.next in OCTDIGITS and len(escape) < 4:\\n                escape = escape + source.get()\\n            escape = escape[1:]\\n            return LITERAL, int(escape, 8) & 0xff\\n        elif c in DIGITS:\\n            raise error, \"bogus escape: %s\" % repr(escape)\\n        if len(escape) == 2:\\n            return LITERAL, ord(escape[1])\\n    except ValueError:\\n        pass\\n    raise error, \"bogus escape: %s\" % repr(escape)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#13899: \\A, \\Z, and \\B now correctly match the A, Z, and B literals when used inside character classes (e.g. [A]).  Patch by Matthew Barnett.",
    "fixed_code": "def _class_escape(source, escape):\\n    code = ESCAPES.get(escape)\\n    if code:\\n        return code\\n    code = CATEGORIES.get(escape)\\n    if code and code[0] == IN:\\n        return code\\n    try:\\n        c = escape[1:2]\\n        if c == \"x\":\\n            while source.next in HEXDIGITS and len(escape) < 4:\\n                escape = escape + source.get()\\n            escape = escape[2:]\\n            if len(escape) != 2:\\n                raise error, \"bogus escape: %s\" % repr(\"\\\\\" + escape)\\n            return LITERAL, int(escape, 16) & 0xff\\n        elif c in OCTDIGITS:\\n            while source.next in OCTDIGITS and len(escape) < 4:\\n                escape = escape + source.get()\\n            escape = escape[1:]\\n            return LITERAL, int(escape, 8) & 0xff\\n        elif c in DIGITS:\\n            raise error, \"bogus escape: %s\" % repr(escape)\\n        if len(escape) == 2:\\n            return LITERAL, ord(escape[1])\\n    except ValueError:\\n        pass\\n    raise error, \"bogus escape: %s\" % repr(escape)"
  },
  {
    "code": "def build_analyzer(self):\\n        if hasattr(self.analyzer, '__call__'):\\n            return self.analyzer\\n        preprocess = self.build_preprocessor()\\n        if self.analyzer == 'char':\\n            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))\\n        elif self.analyzer == 'char_nospace':\\n            return lambda doc: self._char_nospace_ngrams(\\n                preprocess(self.decode(doc)))\\n        elif self.analyzer == 'word':\\n            stop_words = self.get_stop_words()\\n            tokenize = self.build_tokenizer()\\n            return lambda doc: self._word_ngrams(\\n                tokenize(preprocess(self.decode(doc))), stop_words)\\n        else:\\n            raise ValueError('%s is not a valid tokenization scheme' %\\n                             self.tokenize)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def build_analyzer(self):\\n        if hasattr(self.analyzer, '__call__'):\\n            return self.analyzer\\n        preprocess = self.build_preprocessor()\\n        if self.analyzer == 'char':\\n            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))\\n        elif self.analyzer == 'char_nospace':\\n            return lambda doc: self._char_nospace_ngrams(\\n                preprocess(self.decode(doc)))\\n        elif self.analyzer == 'word':\\n            stop_words = self.get_stop_words()\\n            tokenize = self.build_tokenizer()\\n            return lambda doc: self._word_ngrams(\\n                tokenize(preprocess(self.decode(doc))), stop_words)\\n        else:\\n            raise ValueError('%s is not a valid tokenization scheme' %\\n                             self.tokenize)"
  },
  {
    "code": "def nankurt(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    if not is_float_dtype(values.dtype):\\n        values = values.astype('f8')\\n        count = _get_counts(mask, axis)\\n    else:\\n        count = _get_counts(mask, axis, dtype=values.dtype)\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    typ = values.dtype.type\\n    A = values.sum(axis) / count\\n    B = (values ** 2).sum(axis) / count - A ** typ(2)\\n    C = (values ** 3).sum(axis) / count - A ** typ(3) - typ(3) * A * B\\n    D = (values ** 4).sum(axis) / count - A ** typ(4) - typ(6) * B * A * A - typ(4) * C * A\\n    B = _zero_out_fperr(B)\\n    D = _zero_out_fperr(D)\\n    if not isinstance(B, np.ndarray):\\n        if count < 4:\\n            return np.nan\\n        if B == 0:\\n            return 0\\n    result = (((count * count - typ(1)) * D / (B * B) - typ(3) * ((count - typ(1)) ** typ(2))) /\\n              ((count - typ(2)) * (count - typ(3))))\\n    if isinstance(result, np.ndarray):\\n        result = np.where(B == 0, 0, result)\\n        result[count < 4] = np.nan\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def nankurt(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    if not is_float_dtype(values.dtype):\\n        values = values.astype('f8')\\n        count = _get_counts(mask, axis)\\n    else:\\n        count = _get_counts(mask, axis, dtype=values.dtype)\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    typ = values.dtype.type\\n    A = values.sum(axis) / count\\n    B = (values ** 2).sum(axis) / count - A ** typ(2)\\n    C = (values ** 3).sum(axis) / count - A ** typ(3) - typ(3) * A * B\\n    D = (values ** 4).sum(axis) / count - A ** typ(4) - typ(6) * B * A * A - typ(4) * C * A\\n    B = _zero_out_fperr(B)\\n    D = _zero_out_fperr(D)\\n    if not isinstance(B, np.ndarray):\\n        if count < 4:\\n            return np.nan\\n        if B == 0:\\n            return 0\\n    result = (((count * count - typ(1)) * D / (B * B) - typ(3) * ((count - typ(1)) ** typ(2))) /\\n              ((count - typ(2)) * (count - typ(3))))\\n    if isinstance(result, np.ndarray):\\n        result = np.where(B == 0, 0, result)\\n        result[count < 4] = np.nan\\n    return result"
  },
  {
    "code": "def _align_series(self, other, join='outer', axis=None, level=None,\\n                      copy=True, fill_value=None):\\n        fdata = self._data\\n        if axis == 0:\\n            join_index = self.index\\n            lidx, ridx = None, None\\n            if not self.index.equals(other.index):\\n                join_index, lidx, ridx = self.index.join(other.index, how=join,\\n                                                         return_indexers=True)\\n            if lidx is not None:\\n                fdata = fdata.reindex_indexer(join_index, lidx, axis=1)\\n        elif axis == 1:\\n            join_index = self.columns\\n            lidx, ridx = None, None\\n            if not self.columns.equals(other.index):\\n                join_index, lidx, ridx = \\\\n                    self.columns.join(other.index, how=join,\\n                                      return_indexers=True)\\n            if lidx is not None:\\n                fdata = fdata.reindex_indexer(join_index, lidx, axis=0)\\n        else:\\n            raise ValueError('Must specify axis=0 or 1')\\n        if copy and fdata is self._data:\\n            fdata = fdata.copy()\\n        left_result = DataFrame(fdata)\\n        right_result = other if ridx is None else other.reindex(join_index)\\n        return left_result.fillna(fill_value), right_result.fillna(fill_value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _align_series(self, other, join='outer', axis=None, level=None,\\n                      copy=True, fill_value=None):\\n        fdata = self._data\\n        if axis == 0:\\n            join_index = self.index\\n            lidx, ridx = None, None\\n            if not self.index.equals(other.index):\\n                join_index, lidx, ridx = self.index.join(other.index, how=join,\\n                                                         return_indexers=True)\\n            if lidx is not None:\\n                fdata = fdata.reindex_indexer(join_index, lidx, axis=1)\\n        elif axis == 1:\\n            join_index = self.columns\\n            lidx, ridx = None, None\\n            if not self.columns.equals(other.index):\\n                join_index, lidx, ridx = \\\\n                    self.columns.join(other.index, how=join,\\n                                      return_indexers=True)\\n            if lidx is not None:\\n                fdata = fdata.reindex_indexer(join_index, lidx, axis=0)\\n        else:\\n            raise ValueError('Must specify axis=0 or 1')\\n        if copy and fdata is self._data:\\n            fdata = fdata.copy()\\n        left_result = DataFrame(fdata)\\n        right_result = other if ridx is None else other.reindex(join_index)\\n        return left_result.fillna(fill_value), right_result.fillna(fill_value)"
  },
  {
    "code": "def isin(self, values):\\n        if isinstance(values, dict):\\n            from collections import defaultdict\\n            from pandas.tools.merge import concat\\n            values = defaultdict(list, values)\\n            return concat((self.iloc[:, [i]].isin(values[col])\\n                           for i, col in enumerate(self.columns)), axis=1)\\n        elif isinstance(values, Series):\\n            if not values.index.is_unique:\\n                raise ValueError(\"ValueError: cannot compute isin with\"\\n                                 \" a duplicate axis.\")\\n            return self.eq(values.reindex_like(self), axis='index')\\n        elif isinstance(values, DataFrame):\\n            if not (values.columns.is_unique and values.index.is_unique):\\n                raise ValueError(\"ValueError: cannot compute isin with\"\\n                                 \" a duplicate axis.\")\\n            return self.eq(values.reindex_like(self))\\n        else:\\n            if not is_list_like(values):\\n                raise TypeError(\"only list-like or dict-like objects are\"\\n                                \" allowed to be passed to DataFrame.isin(), \"\\n                                \"you passed a \"\\n                                \"{0!r}\".format(type(values).__name__))\\n            return DataFrame(lib.ismember(self.values.ravel(),\\n                                          set(values)).reshape(self.shape),\\n                             self.index,\\n                             self.columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def isin(self, values):\\n        if isinstance(values, dict):\\n            from collections import defaultdict\\n            from pandas.tools.merge import concat\\n            values = defaultdict(list, values)\\n            return concat((self.iloc[:, [i]].isin(values[col])\\n                           for i, col in enumerate(self.columns)), axis=1)\\n        elif isinstance(values, Series):\\n            if not values.index.is_unique:\\n                raise ValueError(\"ValueError: cannot compute isin with\"\\n                                 \" a duplicate axis.\")\\n            return self.eq(values.reindex_like(self), axis='index')\\n        elif isinstance(values, DataFrame):\\n            if not (values.columns.is_unique and values.index.is_unique):\\n                raise ValueError(\"ValueError: cannot compute isin with\"\\n                                 \" a duplicate axis.\")\\n            return self.eq(values.reindex_like(self))\\n        else:\\n            if not is_list_like(values):\\n                raise TypeError(\"only list-like or dict-like objects are\"\\n                                \" allowed to be passed to DataFrame.isin(), \"\\n                                \"you passed a \"\\n                                \"{0!r}\".format(type(values).__name__))\\n            return DataFrame(lib.ismember(self.values.ravel(),\\n                                          set(values)).reshape(self.shape),\\n                             self.index,\\n                             self.columns)"
  },
  {
    "code": "def _intersection(self, other: Index, sort=False):\\n        if self.is_monotonic and other.is_monotonic:\\n            try:\\n                result = self._inner_indexer(other)[0]\\n            except TypeError:\\n                pass\\n            else:\\n                res = algos.unique1d(result)\\n                return ensure_wrapped_if_datetimelike(res)\\n        res_values = self._intersection_via_get_indexer(other, sort=sort)\\n        res_values = _maybe_try_sort(res_values, sort)\\n        return res_values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _intersection(self, other: Index, sort=False):\\n        if self.is_monotonic and other.is_monotonic:\\n            try:\\n                result = self._inner_indexer(other)[0]\\n            except TypeError:\\n                pass\\n            else:\\n                res = algos.unique1d(result)\\n                return ensure_wrapped_if_datetimelike(res)\\n        res_values = self._intersection_via_get_indexer(other, sort=sort)\\n        res_values = _maybe_try_sort(res_values, sort)\\n        return res_values"
  },
  {
    "code": "def _replace_nan(a, val):\\n    is_new = not isinstance(a, np.ndarray)\\n    if is_new:\\n        a = np.array(a)\\n    if not issubclass(a.dtype.type, np.inexact):\\n        return a, None\\n    if not is_new:\\n        a = np.array(a, subok=True)\\n    mask = np.isnan(a)\\n    np.copyto(a, val, where=mask)\\n    return a, mask",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT: Change default inplace, ufunc(...,out=x) casting to same_kind.\\n\\nThere has been a warning of this change since numpy 1.7. numpy 1.10\\nis a good time to do it. The nanvar function needed a fix after the\\nchange, and the tests and documentation are updated.",
    "fixed_code": "def _replace_nan(a, val):\\n    is_new = not isinstance(a, np.ndarray)\\n    if is_new:\\n        a = np.array(a)\\n    if not issubclass(a.dtype.type, np.inexact):\\n        return a, None\\n    if not is_new:\\n        a = np.array(a, subok=True)\\n    mask = np.isnan(a)\\n    np.copyto(a, val, where=mask)\\n    return a, mask"
  },
  {
    "code": "def _box_col_values(self, values, loc: int) -> Series:\\n        name = self.columns[loc]\\n        klass = self._constructor_sliced\\n        return klass(values, index=self.index, name=name, fastpath=True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: TODOs and FIXMEs (#44683)",
    "fixed_code": "def _box_col_values(self, values: SingleDataManager, loc: int) -> Series:\\n        name = self.columns[loc]\\n        klass = self._constructor_sliced\\n        return klass(values, index=self.index, name=name, fastpath=True)"
  },
  {
    "code": "def poke(self, context: Dict[str, Any]):\\n        self.log.info('Poking for prefix : %s in bucket s3://%s', self.prefix, self.bucket_name)\\n        return all(self._check_for_prefix(prefix) for prefix in self.prefix)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def poke(self, context: Dict[str, Any]):\\n        self.log.info('Poking for prefix : %s in bucket s3://%s', self.prefix, self.bucket_name)\\n        return all(self._check_for_prefix(prefix) for prefix in self.prefix)"
  },
  {
    "code": "def test_no_jump_from_exception_event(output):\\n\\t\\toutput.append(1)\\n\\t\\t1 // 0\\n\\t@jump_test(3, 2, [2], event='return', error=(ValueError,\\n\\t\\t\\t   \"can't jump from a yield statement\"))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_no_jump_from_exception_event(output):\\n\\t\\toutput.append(1)\\n\\t\\t1 // 0\\n\\t@jump_test(3, 2, [2], event='return', error=(ValueError,\\n\\t\\t\\t   \"can't jump from a yield statement\"))"
  },
  {
    "code": "def decrypt(self, key_name: str, ciphertext: str, authenticated_data: bytes = None) -> bytes:\\n        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()  \\n        body = {'ciphertext': ciphertext}\\n        if authenticated_data:\\n            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\\n        request = keys.decrypt(name=key_name, body=body)\\n        response = request.execute(num_retries=self.num_retries)\\n        plaintext = _b64decode(response['plaintext'])\\n        return plaintext",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5446] Rewrite Google KMS Hook to Google Cloud Python (#6065)",
    "fixed_code": "def decrypt(\\n        self,\\n        key_name: str,\\n        ciphertext: str,\\n        authenticated_data: Optional[bytes] = None,\\n        retry: Optional[Retry] = None,\\n        timeout: Optional[float] = None,\\n        metadata: Optional[Sequence[Tuple[str, str]]] = None,\\n    ) -> bytes:\\n        response = self.get_conn().decrypt(\\n            name=key_name,\\n            ciphertext=_b64decode(ciphertext),\\n            additional_authenticated_data=authenticated_data,\\n            retry=retry,\\n            timeout=timeout,\\n            metadata=metadata,\\n        )\\n        plaintext = response.plaintext\\n        return plaintext"
  },
  {
    "code": "def _set_up_rule_memoization(self, node: Rule, result_type: str) -> None:\\n        self.print(\"{\")\\n        with self.indent():\\n            self.add_level()\\n            self.print(f\"{result_type} _res = NULL;\")\\n            self.print(f\"if (_PyPegen_is_memoized(p, {node.name}_type, &_res)) {{\")\\n            with self.indent():\\n                self.add_return(\"_res\")\\n            self.print(\"}\")\\n            self.print(\"int _mark = p->mark;\")\\n            self.print(\"int _resmark = p->mark;\")\\n            self.print(\"while (1) {\")\\n            with self.indent():\\n                self.call_with_errorcheck_return(\\n                    f\"_PyPegen_update_memo(p, _mark, {node.name}_type, _res)\", \"_res\"\\n                )\\n                self.print(\"p->mark = _mark;\")\\n                self.print(\"p->in_raw_rule++;\")\\n                self.print(f\"void *_raw = {node.name}_raw(p);\")\\n                self.print(\"p->in_raw_rule--;\")\\n                self.print(\"if (p->error_indicator)\")\\n                with self.indent():\\n                    self.print(\"return NULL;\")\\n                self.print(\"if (_raw == NULL || p->mark <= _resmark)\")\\n                with self.indent():\\n                    self.print(\"break;\")\\n                self.print(f\"_resmark = p->mark;\")\\n                self.print(\"_res = _raw;\")\\n            self.print(\"}\")\\n            self.print(f\"p->mark = _resmark;\")\\n            self.add_return(\"_res\")\\n        self.print(\"}\")\\n        self.print(f\"static {result_type}\")\\n        self.print(f\"{node.name}_raw(Parser *p)\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _set_up_rule_memoization(self, node: Rule, result_type: str) -> None:\\n        self.print(\"{\")\\n        with self.indent():\\n            self.add_level()\\n            self.print(f\"{result_type} _res = NULL;\")\\n            self.print(f\"if (_PyPegen_is_memoized(p, {node.name}_type, &_res)) {{\")\\n            with self.indent():\\n                self.add_return(\"_res\")\\n            self.print(\"}\")\\n            self.print(\"int _mark = p->mark;\")\\n            self.print(\"int _resmark = p->mark;\")\\n            self.print(\"while (1) {\")\\n            with self.indent():\\n                self.call_with_errorcheck_return(\\n                    f\"_PyPegen_update_memo(p, _mark, {node.name}_type, _res)\", \"_res\"\\n                )\\n                self.print(\"p->mark = _mark;\")\\n                self.print(\"p->in_raw_rule++;\")\\n                self.print(f\"void *_raw = {node.name}_raw(p);\")\\n                self.print(\"p->in_raw_rule--;\")\\n                self.print(\"if (p->error_indicator)\")\\n                with self.indent():\\n                    self.print(\"return NULL;\")\\n                self.print(\"if (_raw == NULL || p->mark <= _resmark)\")\\n                with self.indent():\\n                    self.print(\"break;\")\\n                self.print(f\"_resmark = p->mark;\")\\n                self.print(\"_res = _raw;\")\\n            self.print(\"}\")\\n            self.print(f\"p->mark = _resmark;\")\\n            self.add_return(\"_res\")\\n        self.print(\"}\")\\n        self.print(f\"static {result_type}\")\\n        self.print(f\"{node.name}_raw(Parser *p)\")"
  },
  {
    "code": "def rnopen(file, flag='c', mode=0666,\\n            rnflags=0, cachesize=None, pgsize=None, lorder=None,\\n            rlen=None, delim=None, source=None, pad=None):\\n    flags = _checkflag(flag)\\n    d = db.DB()\\n    if cachesize is not None: d.set_cachesize(0, cachesize)\\n    if pgsize is not None: d.set_pagesize(pgsize)\\n    if lorder is not None: d.set_lorder(lorder)\\n    d.set_flags(rnflags)\\n    if delim is not None: d.set_re_delim(delim)\\n    if rlen is not None: d.set_re_len(rlen)\\n    if source is not None: d.set_re_source(source)\\n    if pad is not None: d.set_re_pad(pad)\\n    d.open(file, db.DB_RECNO, flags, mode)\\n    return _DBWithCursor(d)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rnopen(file, flag='c', mode=0666,\\n            rnflags=0, cachesize=None, pgsize=None, lorder=None,\\n            rlen=None, delim=None, source=None, pad=None):\\n    flags = _checkflag(flag)\\n    d = db.DB()\\n    if cachesize is not None: d.set_cachesize(0, cachesize)\\n    if pgsize is not None: d.set_pagesize(pgsize)\\n    if lorder is not None: d.set_lorder(lorder)\\n    d.set_flags(rnflags)\\n    if delim is not None: d.set_re_delim(delim)\\n    if rlen is not None: d.set_re_len(rlen)\\n    if source is not None: d.set_re_source(source)\\n    if pad is not None: d.set_re_pad(pad)\\n    d.open(file, db.DB_RECNO, flags, mode)\\n    return _DBWithCursor(d)"
  },
  {
    "code": "def _generate_cache_key(request, headerlist, key_prefix):\\n    ctx = md5_constructor()\\n    for header in headerlist:\\n        value = request.META.get(header, None)\\n        if value is not None:\\n            ctx.update(value)\\n    path = md5_constructor(iri_to_uri(request.path))\\n    cache_key = 'views.decorators.cache.cache_page.%s.%s.%s' % (\\n        key_prefix, path.hexdigest(), ctx.hexdigest())\\n    if settings.USE_I18N:\\n        cache_key += '.%s' % translation.get_language()\\n    return cache_key",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refined changes made in r12546 to also respect the request.LANGUAGE_CODE in case the LocaleMiddleware is used to discover the language preference.",
    "fixed_code": "def _generate_cache_key(request, headerlist, key_prefix):\\n    ctx = md5_constructor()\\n    for header in headerlist:\\n        value = request.META.get(header, None)\\n        if value is not None:\\n            ctx.update(value)\\n    path = md5_constructor(iri_to_uri(request.path))\\n    cache_key = 'views.decorators.cache.cache_page.%s.%s.%s' % (\\n        key_prefix, path.hexdigest(), ctx.hexdigest())\\n    return _i18n_cache_key_suffix(request, cache_key)"
  },
  {
    "code": "def __init__(self, n_components=None, affinity=\"rbf\", gamma=None,\\n                 fit_inverse_transform=False, random_state=None,\\n                 eigen_solver=None, n_neighbors=None):\\n        self.n_components = n_components\\n        if isinstance(affinity, str):\\n            self.affinity = affinity.lower()\\n            if affinity not in {'precomputed', 'rbf', 'nearest_neighbors'}:\\n                raise ValueError(\\n                    \"Only precomputed, rbf,\"\\n                    \"nearest_neighbors graph supported.\")\\n        else:\\n            self.affinity = affinity\\n        if fit_inverse_transform and graph == 'precomputed':\\n            raise ValueError(\\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\\n        self.gamma = gamma\\n        self.fit_inverse_transform = fit_inverse_transform\\n        self.random_state = check_random_state(random_state)\\n        self.eigen_solver = eigen_solver\\n        self.n_neighbors = n_neighbors\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, n_components=None, affinity=\"rbf\", gamma=None,\\n                 fit_inverse_transform=False, random_state=None,\\n                 eigen_solver=None, n_neighbors=None):\\n        self.n_components = n_components\\n        if isinstance(affinity, str):\\n            self.affinity = affinity.lower()\\n            if affinity not in {'precomputed', 'rbf', 'nearest_neighbors'}:\\n                raise ValueError(\\n                    \"Only precomputed, rbf,\"\\n                    \"nearest_neighbors graph supported.\")\\n        else:\\n            self.affinity = affinity\\n        if fit_inverse_transform and graph == 'precomputed':\\n            raise ValueError(\\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\\n        self.gamma = gamma\\n        self.fit_inverse_transform = fit_inverse_transform\\n        self.random_state = check_random_state(random_state)\\n        self.eigen_solver = eigen_solver\\n        self.n_neighbors = n_neighbors\\n    @property"
  },
  {
    "code": "def _average_binary_score(binary_metric, y_true, y_score, average,\\n                          sample_weight=None):\\n    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\\n    if average not in average_options:\\n        raise ValueError('average has to be one of {0}'\\n                         ''.format(average_options))\\n    y_type = type_of_target(y_true)\\n    if y_type not in (\"binary\", \"multilabel-indicator\"):\\n        raise ValueError(\"{0} format is not supported\".format(y_type))\\n    if y_type == \"binary\":\\n        return binary_metric(y_true, y_score, sample_weight=sample_weight)\\n    check_consistent_length(y_true, y_score, sample_weight)\\n    y_true = check_array(y_true)\\n    y_score = check_array(y_score)\\n    not_average_axis = 1\\n    score_weight = sample_weight\\n    average_weight = None\\n    if average == \"micro\":\\n        if score_weight is not None:\\n            score_weight = np.repeat(score_weight, y_true.shape[1])\\n        y_true = y_true.ravel()\\n        y_score = y_score.ravel()\\n    elif average == 'weighted':\\n        if score_weight is not None:\\n            average_weight = np.sum(np.multiply(\\n                y_true, np.reshape(score_weight, (-1, 1))), axis=0)\\n        else:\\n            average_weight = np.sum(y_true, axis=0)\\n        if average_weight.sum() == 0:\\n            return 0\\n    elif average == 'samples':\\n        average_weight = score_weight\\n        score_weight = None\\n        not_average_axis = 0\\n    if y_true.ndim == 1:\\n        y_true = y_true.reshape((-1, 1))\\n    if y_score.ndim == 1:\\n        y_score = y_score.reshape((-1, 1))\\n    n_classes = y_score.shape[not_average_axis]\\n    score = np.zeros((n_classes,))\\n    for c in range(n_classes):\\n        y_true_c = y_true.take([c], axis=not_average_axis).ravel()\\n        y_score_c = y_score.take([c], axis=not_average_axis).ravel()\\n        score[c] = binary_metric(y_true_c, y_score_c,\\n                                 sample_weight=score_weight)\\n    if average is not None:\\n        return np.average(score, weights=average_weight)\\n    else:\\n        return score",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _average_binary_score(binary_metric, y_true, y_score, average,\\n                          sample_weight=None):\\n    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\\n    if average not in average_options:\\n        raise ValueError('average has to be one of {0}'\\n                         ''.format(average_options))\\n    y_type = type_of_target(y_true)\\n    if y_type not in (\"binary\", \"multilabel-indicator\"):\\n        raise ValueError(\"{0} format is not supported\".format(y_type))\\n    if y_type == \"binary\":\\n        return binary_metric(y_true, y_score, sample_weight=sample_weight)\\n    check_consistent_length(y_true, y_score, sample_weight)\\n    y_true = check_array(y_true)\\n    y_score = check_array(y_score)\\n    not_average_axis = 1\\n    score_weight = sample_weight\\n    average_weight = None\\n    if average == \"micro\":\\n        if score_weight is not None:\\n            score_weight = np.repeat(score_weight, y_true.shape[1])\\n        y_true = y_true.ravel()\\n        y_score = y_score.ravel()\\n    elif average == 'weighted':\\n        if score_weight is not None:\\n            average_weight = np.sum(np.multiply(\\n                y_true, np.reshape(score_weight, (-1, 1))), axis=0)\\n        else:\\n            average_weight = np.sum(y_true, axis=0)\\n        if average_weight.sum() == 0:\\n            return 0\\n    elif average == 'samples':\\n        average_weight = score_weight\\n        score_weight = None\\n        not_average_axis = 0\\n    if y_true.ndim == 1:\\n        y_true = y_true.reshape((-1, 1))\\n    if y_score.ndim == 1:\\n        y_score = y_score.reshape((-1, 1))\\n    n_classes = y_score.shape[not_average_axis]\\n    score = np.zeros((n_classes,))\\n    for c in range(n_classes):\\n        y_true_c = y_true.take([c], axis=not_average_axis).ravel()\\n        y_score_c = y_score.take([c], axis=not_average_axis).ravel()\\n        score[c] = binary_metric(y_true_c, y_score_c,\\n                                 sample_weight=score_weight)\\n    if average is not None:\\n        return np.average(score, weights=average_weight)\\n    else:\\n        return score"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        webfilter_profile=dict(required=False, type=\"str\"),\\n        waf_profile=dict(required=False, type=\"str\"),\\n        voip_profile=dict(required=False, type=\"str\"),\\n        ssl_ssh_profile=dict(required=False, type=\"str\"),\\n        ssh_filter_profile=dict(required=False, type=\"str\"),\\n        spamfilter_profile=dict(required=False, type=\"str\"),\\n        profile_protocol_options=dict(required=False, type=\"str\"),\\n        name=dict(required=False, type=\"str\"),\\n        mms_profile=dict(required=False, type=\"str\"),\\n        ips_sensor=dict(required=False, type=\"str\"),\\n        icap_profile=dict(required=False, type=\"str\"),\\n        dnsfilter_profile=dict(required=False, type=\"str\"),\\n        dlp_sensor=dict(required=False, type=\"str\"),\\n        av_profile=dict(required=False, type=\"str\"),\\n        application_list=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"webfilter-profile\": module.params[\"webfilter_profile\"],\\n        \"waf-profile\": module.params[\"waf_profile\"],\\n        \"voip-profile\": module.params[\"voip_profile\"],\\n        \"ssl-ssh-profile\": module.params[\"ssl_ssh_profile\"],\\n        \"ssh-filter-profile\": module.params[\"ssh_filter_profile\"],\\n        \"spamfilter-profile\": module.params[\"spamfilter_profile\"],\\n        \"profile-protocol-options\": module.params[\"profile_protocol_options\"],\\n        \"name\": module.params[\"name\"],\\n        \"mms-profile\": module.params[\"mms_profile\"],\\n        \"ips-sensor\": module.params[\"ips_sensor\"],\\n        \"icap-profile\": module.params[\"icap_profile\"],\\n        \"dnsfilter-profile\": module.params[\"dnsfilter_profile\"],\\n        \"dlp-sensor\": module.params[\"dlp_sensor\"],\\n        \"av-profile\": module.params[\"av_profile\"],\\n        \"application-list\": module.params[\"application_list\"],\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_firewall_profile_group_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        webfilter_profile=dict(required=False, type=\"str\"),\\n        waf_profile=dict(required=False, type=\"str\"),\\n        voip_profile=dict(required=False, type=\"str\"),\\n        ssl_ssh_profile=dict(required=False, type=\"str\"),\\n        ssh_filter_profile=dict(required=False, type=\"str\"),\\n        spamfilter_profile=dict(required=False, type=\"str\"),\\n        profile_protocol_options=dict(required=False, type=\"str\"),\\n        name=dict(required=False, type=\"str\"),\\n        mms_profile=dict(required=False, type=\"str\"),\\n        ips_sensor=dict(required=False, type=\"str\"),\\n        icap_profile=dict(required=False, type=\"str\"),\\n        dnsfilter_profile=dict(required=False, type=\"str\"),\\n        dlp_sensor=dict(required=False, type=\"str\"),\\n        av_profile=dict(required=False, type=\"str\"),\\n        application_list=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"webfilter-profile\": module.params[\"webfilter_profile\"],\\n        \"waf-profile\": module.params[\"waf_profile\"],\\n        \"voip-profile\": module.params[\"voip_profile\"],\\n        \"ssl-ssh-profile\": module.params[\"ssl_ssh_profile\"],\\n        \"ssh-filter-profile\": module.params[\"ssh_filter_profile\"],\\n        \"spamfilter-profile\": module.params[\"spamfilter_profile\"],\\n        \"profile-protocol-options\": module.params[\"profile_protocol_options\"],\\n        \"name\": module.params[\"name\"],\\n        \"mms-profile\": module.params[\"mms_profile\"],\\n        \"ips-sensor\": module.params[\"ips_sensor\"],\\n        \"icap-profile\": module.params[\"icap_profile\"],\\n        \"dnsfilter-profile\": module.params[\"dnsfilter_profile\"],\\n        \"dlp-sensor\": module.params[\"dlp_sensor\"],\\n        \"av-profile\": module.params[\"av_profile\"],\\n        \"application-list\": module.params[\"application_list\"],\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_firewall_profile_group_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def groups(self):\\n        if self._groups is None:\\n            self._groups = _tseries.groupby(self.labels, self.grouper)\\n        return self._groups",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: pure python multi-grouping working, UI needs work. as does cython integration",
    "fixed_code": "def groups(self):\\n        if self._groups is None:\\n            self._groups = _tseries.groupby(self.index, self.grouper)\\n        return self._groups"
  },
  {
    "code": "def patterns(prefix, *args):\\n    pattern_list = []\\n    for t in args:\\n        if isinstance(t, (list, tuple)):\\n            pattern_list.append(url(prefix=prefix, *t))\\n        else:\\n            pattern_list.append(t)\\n    return pattern_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def patterns(prefix, *args):\\n    pattern_list = []\\n    for t in args:\\n        if isinstance(t, (list, tuple)):\\n            pattern_list.append(url(prefix=prefix, *t))\\n        else:\\n            pattern_list.append(t)\\n    return pattern_list"
  },
  {
    "code": "def get_group(db, group_name):\\n\\tgroups_table = db.get_table('groups')\\n\\tgroup = groups_table.find(name=group_name)\\n\\trows = [x for x in group]\\n\\tif not rows:\\n\\t\\treturn error(404, {'error': 'Not a valid group'})\\n\\tuserids = [x['userid'] for x in rows if x['userid']]\\n\\tif not userids:\\n\\t\\treturn {group_name: []}\\n\\tparams = {}\\n\\tfor i, userid in enumerate(userids,1):\\n\\t\\tparams['userid_' + str(i)] = str(userid)\\n\\twhere_clause = 'userid IN(:' + \",:\".join(params.keys()) + ')' \\n\\tq = \"SELECT * FROM users WHERE \" + where_clause\\n\\tusers = db.executable.execute(q, params).fetchall()\\n\\tret = {group_name: [dict(x.items()) for x in users] }\\n\\treturn ret",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_group(db, group_name):\\n\\tgroups_table = db.get_table('groups')\\n\\tgroup = groups_table.find(name=group_name)\\n\\trows = [x for x in group]\\n\\tif not rows:\\n\\t\\treturn error(404, {'error': 'Not a valid group'})\\n\\tuserids = [x['userid'] for x in rows if x['userid']]\\n\\tif not userids:\\n\\t\\treturn {group_name: []}\\n\\tparams = {}\\n\\tfor i, userid in enumerate(userids,1):\\n\\t\\tparams['userid_' + str(i)] = str(userid)\\n\\twhere_clause = 'userid IN(:' + \",:\".join(params.keys()) + ')' \\n\\tq = \"SELECT * FROM users WHERE \" + where_clause\\n\\tusers = db.executable.execute(q, params).fetchall()\\n\\tret = {group_name: [dict(x.items()) for x in users] }\\n\\treturn ret"
  },
  {
    "code": "def __init__(\\n        self,\\n        name: str | None = None,\\n        mode: Literal[\"r\", \"a\", \"w\", \"x\"] = \"r\",\\n        fileobj: ReadBuffer[bytes] | WriteBuffer[bytes] | None = None,\\n        archive_name: str | None = None,\\n        **kwargs,\\n    ) -> None:\\n        super().__init__()\\n        self.archive_name = archive_name\\n        self.name = name\\n        self.buffer = tarfile.TarFile.open(\\n            name=name,\\n            mode=self.extend_mode(mode),\\n            fileobj=fileobj,  \\n            **kwargs,\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        name: str | None = None,\\n        mode: Literal[\"r\", \"a\", \"w\", \"x\"] = \"r\",\\n        fileobj: ReadBuffer[bytes] | WriteBuffer[bytes] | None = None,\\n        archive_name: str | None = None,\\n        **kwargs,\\n    ) -> None:\\n        super().__init__()\\n        self.archive_name = archive_name\\n        self.name = name\\n        self.buffer = tarfile.TarFile.open(\\n            name=name,\\n            mode=self.extend_mode(mode),\\n            fileobj=fileobj,  \\n            **kwargs,\\n        )"
  },
  {
    "code": "def update(x, new_x):\\n\\treturn tf_state_ops.assign(x, new_x)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix initializers and update ops.",
    "fixed_code": "def update(x, new_x):\\n\\top = tf_state_ops.assign(x, new_x)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)"
  },
  {
    "code": "def _aggregate_item_by_item(self, func, *args, **kwargs):\\n        obj = self._obj_with_exclusions\\n        result = {}\\n        cannot_agg = []\\n        for item in obj:\\n            try:\\n                colg = SeriesGroupBy(obj[item], column=item,\\n                                     grouper=self.grouper)\\n                result[item] = colg.agg(func, *args, **kwargs)\\n            except (ValueError, TypeError):\\n                cannot_agg.append(item)\\n                continue\\n        result_columns = obj.columns\\n        if cannot_agg:\\n            result_columns = result_columns.drop(cannot_agg)\\n        return DataFrame(result, columns=result_columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: hack job but panel resampling with NumPy function works, close #1149",
    "fixed_code": "def _aggregate_item_by_item(self, func, *args, **kwargs):\\n        obj = self._obj_with_exclusions\\n        result = {}\\n        cannot_agg = []\\n        for item in obj:\\n            try:\\n                colg = SeriesGroupBy(obj[item], column=item,\\n                                     grouper=self.grouper)\\n                result[item] = colg.aggregate(func, *args, **kwargs)\\n            except (ValueError, TypeError):\\n                cannot_agg.append(item)\\n                continue\\n        result_columns = obj.columns\\n        if cannot_agg:\\n            result_columns = result_columns.drop(cannot_agg)\\n        return DataFrame(result, columns=result_columns)"
  },
  {
    "code": "def _trigger_dag(\\n        dag_id: str,\\n        dag_bag: DagBag,\\n        dag_run: DagModel,\\n        run_id: Optional[str],\\n        conf: Optional[Union[dict, str]],\\n        execution_date: Optional[datetime],\\n        replace_microseconds: bool,\\n) -> List[DagRun]:  \\n    dag = dag_bag.get_dag(dag_id)  \\n    if dag_id not in dag_bag.dags:\\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\\n    execution_date = execution_date if execution_date else timezone.utcnow()\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"The execution_date should be localized\")\\n    if replace_microseconds:\\n        execution_date = execution_date.replace(microsecond=0)\\n    if dag.default_args and 'start_date' in dag.default_args:\\n        min_dag_start_date = dag.default_args[\"start_date\"]\\n        if min_dag_start_date and execution_date < min_dag_start_date:\\n            raise ValueError(\\n                \"The execution_date [{0}] should be >= start_date [{1}] from DAG's default_args\".format(\\n                    execution_date.isoformat(),\\n                    min_dag_start_date.isoformat()))\\n    if not run_id:\\n        run_id = \"manual__{0}\".format(execution_date.isoformat())\\n    dag_run_id = dag_run.find(dag_id=dag_id, run_id=run_id)\\n    if dag_run_id:\\n        raise DagRunAlreadyExists(\"Run id {} already exists for dag id {}\".format(\\n            run_id,\\n            dag_id\\n        ))\\n    run_conf = None\\n    if conf:\\n        if isinstance(conf, dict):\\n            run_conf = conf\\n        else:\\n            run_conf = json.loads(conf)\\n    triggers = []\\n    dags_to_trigger = []\\n    dags_to_trigger.append(dag)\\n    while dags_to_trigger:\\n        dag = dags_to_trigger.pop()\\n        trigger = dag.create_dagrun(\\n            run_id=run_id,\\n            execution_date=execution_date,\\n            state=State.RUNNING,\\n            conf=run_conf,\\n            external_trigger=True,\\n        )\\n        triggers.append(trigger)\\n        if dag.subdags:\\n            dags_to_trigger.extend(dag.subdags)\\n    return triggers",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _trigger_dag(\\n        dag_id: str,\\n        dag_bag: DagBag,\\n        dag_run: DagModel,\\n        run_id: Optional[str],\\n        conf: Optional[Union[dict, str]],\\n        execution_date: Optional[datetime],\\n        replace_microseconds: bool,\\n) -> List[DagRun]:  \\n    dag = dag_bag.get_dag(dag_id)  \\n    if dag_id not in dag_bag.dags:\\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\\n    execution_date = execution_date if execution_date else timezone.utcnow()\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"The execution_date should be localized\")\\n    if replace_microseconds:\\n        execution_date = execution_date.replace(microsecond=0)\\n    if dag.default_args and 'start_date' in dag.default_args:\\n        min_dag_start_date = dag.default_args[\"start_date\"]\\n        if min_dag_start_date and execution_date < min_dag_start_date:\\n            raise ValueError(\\n                \"The execution_date [{0}] should be >= start_date [{1}] from DAG's default_args\".format(\\n                    execution_date.isoformat(),\\n                    min_dag_start_date.isoformat()))\\n    if not run_id:\\n        run_id = \"manual__{0}\".format(execution_date.isoformat())\\n    dag_run_id = dag_run.find(dag_id=dag_id, run_id=run_id)\\n    if dag_run_id:\\n        raise DagRunAlreadyExists(\"Run id {} already exists for dag id {}\".format(\\n            run_id,\\n            dag_id\\n        ))\\n    run_conf = None\\n    if conf:\\n        if isinstance(conf, dict):\\n            run_conf = conf\\n        else:\\n            run_conf = json.loads(conf)\\n    triggers = []\\n    dags_to_trigger = []\\n    dags_to_trigger.append(dag)\\n    while dags_to_trigger:\\n        dag = dags_to_trigger.pop()\\n        trigger = dag.create_dagrun(\\n            run_id=run_id,\\n            execution_date=execution_date,\\n            state=State.RUNNING,\\n            conf=run_conf,\\n            external_trigger=True,\\n        )\\n        triggers.append(trigger)\\n        if dag.subdags:\\n            dags_to_trigger.extend(dag.subdags)\\n    return triggers"
  },
  {
    "code": "def handle_inspection(self, options):\\n        connection = connections[options.get('database')]\\n        table2model = lambda table_name: table_name.title().replace('_', '').replace(' ', '').replace('-', '')\\n        cursor = connection.cursor()\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield ''\\n        yield 'from %s import models' % self.db_module\\n        yield ''\\n        known_models = []\\n        for table_name in connection.introspection.table_names(cursor):\\n            yield 'class %s(models.Model):' % table2model(table_name)\\n            known_models.append(table2model(table_name))\\n            try:\\n                relations = connection.introspection.get_relations(cursor, table_name)\\n            except NotImplementedError:\\n                relations = {}\\n            try:\\n                indexes = connection.introspection.get_indexes(cursor, table_name)\\n            except NotImplementedError:\\n                indexes = {}\\n            for i, row in enumerate(connection.introspection.get_table_description(cursor, table_name)):\\n                column_name = row[0]\\n                att_name = column_name.lower()\\n                comment_notes = [] \\n                extra_params = {}  \\n                if ' ' in att_name or '-' in att_name or keyword.iskeyword(att_name) or column_name != att_name:\\n                    extra_params['db_column'] = column_name\\n                if column_name in indexes:\\n                    if indexes[column_name]['primary_key']:\\n                        extra_params['primary_key'] = True\\n                    elif indexes[column_name]['unique']:\\n                        extra_params['unique'] = True\\n                if ' ' in att_name:\\n                    att_name = att_name.replace(' ', '_')\\n                    comment_notes.append('Field renamed to remove spaces.')\\n                if '-' in att_name:\\n                    att_name = att_name.replace('-', '_')\\n                    comment_notes.append('Field renamed to remove dashes.')\\n                if column_name != att_name:\\n                    comment_notes.append('Field name made lowercase.')\\n                if i in relations:\\n                    rel_to = relations[i][1] == table_name and \"'self'\" or table2model(relations[i][1])\\n                    if rel_to in known_models:\\n                        field_type = 'ForeignKey(%s' % rel_to\\n                    else:\\n                        field_type = \"ForeignKey('%s'\" % rel_to\\n                    if att_name.endswith('_id'):\\n                        att_name = att_name[:-3]\\n                    else:\\n                        extra_params['db_column'] = column_name\\n                else:\\n                    field_type, field_params, field_notes = self.get_field_type(connection, table_name, row)\\n                    extra_params.update(field_params)\\n                    comment_notes.extend(field_notes)\\n                    field_type += '('\\n                if keyword.iskeyword(att_name):\\n                    att_name += '_field'\\n                    comment_notes.append('Field renamed because it was a Python reserved word.')\\n                if att_name[0].isdigit():\\n                    att_name = 'number_%s' % att_name\\n                    extra_params['db_column'] = unicode(column_name)\\n                    comment_notes.append(\"Field renamed because it wasn't a \"\\n                        \"valid Python identifier.\")\\n                if att_name == 'id' and field_type == 'AutoField(' and extra_params == {'primary_key': True}:\\n                    continue\\n                if row[6]: \\n                    extra_params['blank'] = True\\n                    if not field_type in ('TextField(', 'CharField('):\\n                        extra_params['null'] = True\\n                field_desc = '%s = models.%s' % (att_name, field_type)\\n                if extra_params:\\n                    if not field_desc.endswith('('):\\n                        field_desc += ', '\\n                    field_desc += ', '.join(['%s=%r' % (k, v) for k, v in extra_params.items()])\\n                field_desc += ')'\\n                if comment_notes:\\n                    field_desc += ' \\n                yield '    %s' % field_desc\\n            for meta_line in self.get_meta(table_name):\\n                yield meta_line",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Made inspectdb tests deal with a smaller generated models.py file.\\n\\nImplemented this by adding a stealth table_name_filter option for the\\ncommand.",
    "fixed_code": "def handle_inspection(self, options):\\n        connection = connections[options.get('database')]\\n        table_name_filter = options.get('table_name_filter')\\n        table2model = lambda table_name: table_name.title().replace('_', '').replace(' ', '').replace('-', '')\\n        cursor = connection.cursor()\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield \"\\n        yield ''\\n        yield 'from %s import models' % self.db_module\\n        yield ''\\n        known_models = []\\n        for table_name in connection.introspection.table_names(cursor):\\n            if table_name_filter is not None and callable(table_name_filter):\\n                if not table_name_filter(table_name):\\n                    continue\\n            yield 'class %s(models.Model):' % table2model(table_name)\\n            known_models.append(table2model(table_name))\\n            try:\\n                relations = connection.introspection.get_relations(cursor, table_name)\\n            except NotImplementedError:\\n                relations = {}\\n            try:\\n                indexes = connection.introspection.get_indexes(cursor, table_name)\\n            except NotImplementedError:\\n                indexes = {}\\n            for i, row in enumerate(connection.introspection.get_table_description(cursor, table_name)):\\n                column_name = row[0]\\n                att_name = column_name.lower()\\n                comment_notes = [] \\n                extra_params = {}  \\n                if ' ' in att_name or '-' in att_name or keyword.iskeyword(att_name) or column_name != att_name:\\n                    extra_params['db_column'] = column_name\\n                if column_name in indexes:\\n                    if indexes[column_name]['primary_key']:\\n                        extra_params['primary_key'] = True\\n                    elif indexes[column_name]['unique']:\\n                        extra_params['unique'] = True\\n                if ' ' in att_name:\\n                    att_name = att_name.replace(' ', '_')\\n                    comment_notes.append('Field renamed to remove spaces.')\\n                if '-' in att_name:\\n                    att_name = att_name.replace('-', '_')\\n                    comment_notes.append('Field renamed to remove dashes.')\\n                if column_name != att_name:\\n                    comment_notes.append('Field name made lowercase.')\\n                if i in relations:\\n                    rel_to = relations[i][1] == table_name and \"'self'\" or table2model(relations[i][1])\\n                    if rel_to in known_models:\\n                        field_type = 'ForeignKey(%s' % rel_to\\n                    else:\\n                        field_type = \"ForeignKey('%s'\" % rel_to\\n                    if att_name.endswith('_id'):\\n                        att_name = att_name[:-3]\\n                    else:\\n                        extra_params['db_column'] = column_name\\n                else:\\n                    field_type, field_params, field_notes = self.get_field_type(connection, table_name, row)\\n                    extra_params.update(field_params)\\n                    comment_notes.extend(field_notes)\\n                    field_type += '('\\n                if keyword.iskeyword(att_name):\\n                    att_name += '_field'\\n                    comment_notes.append('Field renamed because it was a Python reserved word.')\\n                if att_name[0].isdigit():\\n                    att_name = 'number_%s' % att_name\\n                    extra_params['db_column'] = unicode(column_name)\\n                    comment_notes.append(\"Field renamed because it wasn't a \"\\n                        \"valid Python identifier.\")\\n                if att_name == 'id' and field_type == 'AutoField(' and extra_params == {'primary_key': True}:\\n                    continue\\n                if row[6]: \\n                    extra_params['blank'] = True\\n                    if not field_type in ('TextField(', 'CharField('):\\n                        extra_params['null'] = True\\n                field_desc = '%s = models.%s' % (att_name, field_type)\\n                if extra_params:\\n                    if not field_desc.endswith('('):\\n                        field_desc += ', '\\n                    field_desc += ', '.join(['%s=%r' % (k, v) for k, v in extra_params.items()])\\n                field_desc += ')'\\n                if comment_notes:\\n                    field_desc += ' \\n                yield '    %s' % field_desc\\n            for meta_line in self.get_meta(table_name):\\n                yield meta_line"
  },
  {
    "code": "def regen_pcbuild(modules):\\n    projlines = []\\n    filterlines = []\\n    for src in _iter_sources(modules):\\n        if src.id not in ESSENTIAL and src.id != '__hello__':\\n            continue\\n        pyfile = relpath_for_windows_display(src.pyfile, ROOT_DIR)\\n        header = relpath_for_windows_display(src.frozenfile, ROOT_DIR)\\n        intfile = ntpath.splitext(ntpath.basename(header))[0] + '.g.h'\\n        projlines.append(f'    <None Include=\"..\\\\{pyfile}\">')\\n        projlines.append(f'      <ModName>{src.frozenid}</ModName>')\\n        projlines.append(f'      <IntFile>$(IntDir){intfile}</IntFile>')\\n        projlines.append(f'      <OutFile>$(PySourcePath){header}</OutFile>')\\n        projlines.append(f'    </None>')\\n        filterlines.append(f'    <None Include=\"..\\\\{pyfile}\">')\\n        filterlines.append('      <Filter>Python Files</Filter>')\\n        filterlines.append('    </None>')\\n    print(f'\\n    with updating_file_with_tmpfile(PCBUILD_PROJECT) as (infile, outfile):\\n        lines = infile.readlines()\\n        lines = replace_block(\\n            lines,\\n            '<!-- BEGIN frozen modules -->',\\n            '<!-- END frozen modules -->',\\n            projlines,\\n            PCBUILD_PROJECT,\\n        )\\n        outfile.writelines(lines)\\n    print(f'\\n    with updating_file_with_tmpfile(PCBUILD_FILTERS) as (infile, outfile):\\n        lines = infile.readlines()\\n        lines = replace_block(\\n            lines,\\n            '<!-- BEGIN frozen modules -->',\\n            '<!-- END frozen modules -->',\\n            filterlines,\\n            PCBUILD_FILTERS,\\n        )\\n        outfile.writelines(lines)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def regen_pcbuild(modules):\\n    projlines = []\\n    filterlines = []\\n    for src in _iter_sources(modules):\\n        if src.id not in ESSENTIAL and src.id != '__hello__':\\n            continue\\n        pyfile = relpath_for_windows_display(src.pyfile, ROOT_DIR)\\n        header = relpath_for_windows_display(src.frozenfile, ROOT_DIR)\\n        intfile = ntpath.splitext(ntpath.basename(header))[0] + '.g.h'\\n        projlines.append(f'    <None Include=\"..\\\\{pyfile}\">')\\n        projlines.append(f'      <ModName>{src.frozenid}</ModName>')\\n        projlines.append(f'      <IntFile>$(IntDir){intfile}</IntFile>')\\n        projlines.append(f'      <OutFile>$(PySourcePath){header}</OutFile>')\\n        projlines.append(f'    </None>')\\n        filterlines.append(f'    <None Include=\"..\\\\{pyfile}\">')\\n        filterlines.append('      <Filter>Python Files</Filter>')\\n        filterlines.append('    </None>')\\n    print(f'\\n    with updating_file_with_tmpfile(PCBUILD_PROJECT) as (infile, outfile):\\n        lines = infile.readlines()\\n        lines = replace_block(\\n            lines,\\n            '<!-- BEGIN frozen modules -->',\\n            '<!-- END frozen modules -->',\\n            projlines,\\n            PCBUILD_PROJECT,\\n        )\\n        outfile.writelines(lines)\\n    print(f'\\n    with updating_file_with_tmpfile(PCBUILD_FILTERS) as (infile, outfile):\\n        lines = infile.readlines()\\n        lines = replace_block(\\n            lines,\\n            '<!-- BEGIN frozen modules -->',\\n            '<!-- END frozen modules -->',\\n            filterlines,\\n            PCBUILD_FILTERS,\\n        )\\n        outfile.writelines(lines)"
  },
  {
    "code": "def technical_404_response(request, exception):\\n    try:\\n        error_url = exception.args[0]['path']\\n    except (IndexError, TypeError, KeyError):\\n        error_url = request.path_info[1:]  \\n    try:\\n        tried = exception.args[0]['tried']\\n    except (IndexError, TypeError, KeyError):\\n        tried = []\\n    else:\\n        if (not tried or (                  \\n            request.path == '/' and\\n            len(tried) == 1 and             \\n            len(tried[0]) == 1 and\\n            getattr(tried[0][0], 'app_name', '') == getattr(tried[0][0], 'namespace', '') == 'admin'\\n        )):\\n            return default_urlconf(request)\\n    urlconf = getattr(request, 'urlconf', settings.ROOT_URLCONF)\\n    if isinstance(urlconf, types.ModuleType):\\n        urlconf = urlconf.__name__\\n    caller = ''\\n    try:\\n        resolver_match = resolve(request.path)\\n    except Resolver404:\\n        pass\\n    else:\\n        obj = resolver_match.func\\n        if hasattr(obj, '__name__'):\\n            caller = obj.__name__\\n        elif hasattr(obj, '__class__') and hasattr(obj.__class__, '__name__'):\\n            caller = obj.__class__.__name__\\n        if hasattr(obj, '__module__'):\\n            module = obj.__module__\\n            caller = '%s.%s' % (module, caller)\\n    with Path(CURRENT_DIR, 'templates', 'technical_404.html').open(encoding='utf-8') as fh:\\n        t = DEBUG_ENGINE.from_string(fh.read())\\n    c = Context({\\n        'urlconf': urlconf,\\n        'root_urlconf': settings.ROOT_URLCONF,\\n        'request_path': error_url,\\n        'urlpatterns': tried,\\n        'reason': str(exception),\\n        'request': request,\\n        'settings': get_safe_settings(),\\n        'raising_view_name': caller,\\n    })\\n    return HttpResponseNotFound(t.render(c), content_type='text/html')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def technical_404_response(request, exception):\\n    try:\\n        error_url = exception.args[0]['path']\\n    except (IndexError, TypeError, KeyError):\\n        error_url = request.path_info[1:]  \\n    try:\\n        tried = exception.args[0]['tried']\\n    except (IndexError, TypeError, KeyError):\\n        tried = []\\n    else:\\n        if (not tried or (                  \\n            request.path == '/' and\\n            len(tried) == 1 and             \\n            len(tried[0]) == 1 and\\n            getattr(tried[0][0], 'app_name', '') == getattr(tried[0][0], 'namespace', '') == 'admin'\\n        )):\\n            return default_urlconf(request)\\n    urlconf = getattr(request, 'urlconf', settings.ROOT_URLCONF)\\n    if isinstance(urlconf, types.ModuleType):\\n        urlconf = urlconf.__name__\\n    caller = ''\\n    try:\\n        resolver_match = resolve(request.path)\\n    except Resolver404:\\n        pass\\n    else:\\n        obj = resolver_match.func\\n        if hasattr(obj, '__name__'):\\n            caller = obj.__name__\\n        elif hasattr(obj, '__class__') and hasattr(obj.__class__, '__name__'):\\n            caller = obj.__class__.__name__\\n        if hasattr(obj, '__module__'):\\n            module = obj.__module__\\n            caller = '%s.%s' % (module, caller)\\n    with Path(CURRENT_DIR, 'templates', 'technical_404.html').open(encoding='utf-8') as fh:\\n        t = DEBUG_ENGINE.from_string(fh.read())\\n    c = Context({\\n        'urlconf': urlconf,\\n        'root_urlconf': settings.ROOT_URLCONF,\\n        'request_path': error_url,\\n        'urlpatterns': tried,\\n        'reason': str(exception),\\n        'request': request,\\n        'settings': get_safe_settings(),\\n        'raising_view_name': caller,\\n    })\\n    return HttpResponseNotFound(t.render(c), content_type='text/html')"
  },
  {
    "code": "def set_installed_apps(self, installed):\\n        self.stored_app_configs.append((self.app_configs, self._apps_loaded, self._models_loaded))\\n        self.app_configs = OrderedDict()\\n        self.get_models.cache_clear()\\n        self._apps_loaded = False\\n        self.populate_apps(installed)\\n        self._models_loaded = False\\n        self.populate_models()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added Apps.clear_cache().\\n\\nThis avoid leaking implementation details to tests that swap models.",
    "fixed_code": "def set_installed_apps(self, installed):\\n        self.stored_app_configs.append((self.app_configs, self._apps_loaded, self._models_loaded))\\n        self.app_configs = OrderedDict()\\n        self.clear_cache()\\n        self._apps_loaded = False\\n        self.populate_apps(installed)\\n        self._models_loaded = False\\n        self.populate_models()"
  },
  {
    "code": "def users_export(args):\\n    appbuilder = cached_appbuilder()\\n    users = appbuilder.sm.get_all_users()\\n    fields = ['id', 'username', 'email', 'first_name', 'last_name', 'roles']",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def users_export(args):\\n    appbuilder = cached_appbuilder()\\n    users = appbuilder.sm.get_all_users()\\n    fields = ['id', 'username', 'email', 'first_name', 'last_name', 'roles']"
  },
  {
    "code": "def is_initial(self, value):\\n\\t\\treturn bool(value and hasattr(value, 'url'))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24727 -- Prevented ClearableFileInput from masking exceptions on Python 2",
    "fixed_code": "def is_initial(self, value):\\n\\t\\treturn bool(value and getattr(value, 'url', False))"
  },
  {
    "code": "def hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\\n    import matplotlib.pyplot as plt\\n    if column is not None:\\n        if not isinstance(column, (list, np.ndarray)):\\n            column = [column]\\n        data = data[column]\\n    if by is not None:\\n        axes = grouped_hist(data, by=by, ax=ax, grid=grid, figsize=figsize,\\n                            sharex=sharex, sharey=sharey, layout=layout, bins=bins,\\n                            **kwds)\\n        for ax in axes.ravel():\\n            if xlabelsize is not None:\\n                plt.setp(ax.get_xticklabels(), fontsize=xlabelsize)\\n            if xrot is not None:\\n                plt.setp(ax.get_xticklabels(), rotation=xrot)\\n            if ylabelsize is not None:\\n                plt.setp(ax.get_yticklabels(), fontsize=ylabelsize)\\n            if yrot is not None:\\n                plt.setp(ax.get_yticklabels(), rotation=yrot)\\n        return axes\\n    n = len(data.columns)\\n    if layout is not None:\\n        if not isinstance(layout, (tuple, list)) or len(layout) != 2:\\n            raise ValueError('Layout must be a tuple of (rows, columns)')\\n        rows, cols = layout\\n        if rows * cols < n:\\n            raise ValueError('Layout of %sx%s is incompatible with %s columns' % (rows, cols, n))\\n    else:\\n        rows, cols = 1, 1\\n        while rows * cols < n:\\n            if cols > rows:\\n                rows += 1\\n            else:\\n                cols += 1\\n    fig, axes = _subplots(nrows=rows, ncols=cols, ax=ax, squeeze=False,\\n                          sharex=sharex, sharey=sharey, figsize=figsize)\\n    for i, col in enumerate(com._try_sort(data.columns)):\\n        ax = axes[i / cols, i % cols]\\n        ax.xaxis.set_visible(True)\\n        ax.yaxis.set_visible(True)\\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\\n        ax.set_title(col)\\n        ax.grid(grid)\\n        if xlabelsize is not None:\\n            plt.setp(ax.get_xticklabels(), fontsize=xlabelsize)\\n        if xrot is not None:\\n            plt.setp(ax.get_xticklabels(), rotation=xrot)\\n        if ylabelsize is not None:\\n            plt.setp(ax.get_yticklabels(), fontsize=ylabelsize)\\n        if yrot is not None:\\n            plt.setp(ax.get_yticklabels(), rotation=yrot)\\n    for j in range(i + 1, rows * cols):\\n        ax = axes[j / cols, j % cols]\\n        ax.set_visible(False)\\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\\n    return axes",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "release conflict",
    "fixed_code": "def hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\\n    import matplotlib.pyplot as plt\\n    if by is not None:\\n        axes = grouped_hist(data, column=column, by=by, ax=ax, grid=grid, figsize=figsize,\\n                            sharex=sharex, sharey=sharey, layout=layout, bins=bins,\\n                            **kwds)\\n        for ax in axes.ravel():\\n            if xlabelsize is not None:\\n                plt.setp(ax.get_xticklabels(), fontsize=xlabelsize)\\n            if xrot is not None:\\n                plt.setp(ax.get_xticklabels(), rotation=xrot)\\n            if ylabelsize is not None:\\n                plt.setp(ax.get_yticklabels(), fontsize=ylabelsize)\\n            if yrot is not None:\\n                plt.setp(ax.get_yticklabels(), rotation=yrot)\\n        return axes\\n    if column is not None:\\n        if not isinstance(column, (list, np.ndarray)):\\n            column = [column]\\n        data = data[column]\\n    naxes = len(data.columns)\\n    nrows, ncols = _get_layout(naxes, layout=layout)\\n    fig, axes = _subplots(nrows=nrows, ncols=ncols, naxes=naxes, ax=ax, squeeze=False,\\n                          sharex=sharex, sharey=sharey, figsize=figsize)\\n    for i, col in enumerate(com._try_sort(data.columns)):\\n        ax = axes[i / ncols, i % ncols]\\n        ax.xaxis.set_visible(True)\\n        ax.yaxis.set_visible(True)\\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\\n        ax.set_title(col)\\n        ax.grid(grid)\\n        if xlabelsize is not None:\\n            plt.setp(ax.get_xticklabels(), fontsize=xlabelsize)\\n        if xrot is not None:\\n            plt.setp(ax.get_xticklabels(), rotation=xrot)\\n        if ylabelsize is not None:\\n            plt.setp(ax.get_yticklabels(), fontsize=ylabelsize)\\n        if yrot is not None:\\n            plt.setp(ax.get_yticklabels(), rotation=yrot)\\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\\n    return axes"
  },
  {
    "code": "def isrecursive(object):\\n    return _safe_repr(object, {}, None, 0, True)[2]\\nclass _safe_key:\\n    __slots__ = ['obj']",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-28850: Fix PrettyPrinter.format overrides ignored for contents of small containers (GH-22120)",
    "fixed_code": "def isrecursive(object):\\n    return PrettyPrinter()._safe_repr(object, {}, None, 0)[2]\\nclass _safe_key:\\n    __slots__ = ['obj']"
  },
  {
    "code": "def is_monotonic(self):\\n        return self._is_monotonic(self.values)\\n    _indexMap = None\\n    _integrity = False\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_monotonic(self):\\n        return self._is_monotonic(self.values)\\n    _indexMap = None\\n    _integrity = False\\n    @property"
  },
  {
    "code": "def _possibly_cast_to_datetime(value, dtype, coerce=False):\\n    from pandas.tseries.timedeltas import to_timedelta\\n    from pandas.tseries.tools import to_datetime\\n    if dtype is not None:\\n        if isinstance(dtype, compat.string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if is_datetime64 and dtype != _NS_DTYPE:\\n                if dtype.name == 'datetime64[ns]':\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert datetimelike to dtype [%s]\" % dtype)\\n            elif is_timedelta64 and dtype != _TD_DTYPE:\\n                if dtype.name == 'timedelta64[ns]':\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert timedeltalike to dtype [%s]\" % dtype)\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value,copy=False)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, coerce=coerce).values\\n                    except (AttributeError, ValueError):\\n                        pass\\n    else:\\n        is_array = isinstance(value, np.ndarray)\\n        if is_array and value.dtype.kind in ['M', 'm']:\\n            dtype = value.dtype\\n            if dtype.kind == 'M' and dtype != _NS_DTYPE:\\n                value = value.astype(_NS_DTYPE)\\n            elif dtype.kind == 'm' and dtype != _TD_DTYPE:\\n                value = to_timedelta(value)\\n        elif not (is_array and not (issubclass(value.dtype.type, np.integer) or\\n                                    value.dtype == np.object_)):\\n            value = _possibly_infer_to_datetimelike(value)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _possibly_cast_to_datetime(value, dtype, coerce=False):\\n    from pandas.tseries.timedeltas import to_timedelta\\n    from pandas.tseries.tools import to_datetime\\n    if dtype is not None:\\n        if isinstance(dtype, compat.string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if is_datetime64 and dtype != _NS_DTYPE:\\n                if dtype.name == 'datetime64[ns]':\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert datetimelike to dtype [%s]\" % dtype)\\n            elif is_timedelta64 and dtype != _TD_DTYPE:\\n                if dtype.name == 'timedelta64[ns]':\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert timedeltalike to dtype [%s]\" % dtype)\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value,copy=False)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, coerce=coerce).values\\n                    except (AttributeError, ValueError):\\n                        pass\\n    else:\\n        is_array = isinstance(value, np.ndarray)\\n        if is_array and value.dtype.kind in ['M', 'm']:\\n            dtype = value.dtype\\n            if dtype.kind == 'M' and dtype != _NS_DTYPE:\\n                value = value.astype(_NS_DTYPE)\\n            elif dtype.kind == 'm' and dtype != _TD_DTYPE:\\n                value = to_timedelta(value)\\n        elif not (is_array and not (issubclass(value.dtype.type, np.integer) or\\n                                    value.dtype == np.object_)):\\n            value = _possibly_infer_to_datetimelike(value)\\n    return value"
  },
  {
    "code": "def infer_freq(index, warn=True):\\n    import pandas as pd\\n    if isinstance(index, com.ABCSeries):\\n        values = index.values\\n        if not (com.is_datetime64_dtype(index.values) or values.dtype == object):\\n            raise TypeError(\"cannot infer freq from a non-convertible dtype on a Series of {0}\".format(index.dtype))\\n        index = values\\n    if isinstance(index, pd.PeriodIndex):\\n        raise TypeError(\"PeriodIndex given. Check the `freq` attribute \"\\n                         \"instead of using infer_freq.\")\\n    if not isinstance(index, pd.DatetimeIndex) and isinstance(index, pd.Index):\\n        raise TypeError(\"cannot infer freq from a non-convertible index type {0}\".format(type(index)))\\n    index = pd.DatetimeIndex(index)\\n    inferer = _FrequencyInferer(index, warn=warn)\\n    return inferer.get_freq()\\n_ONE_MICRO = long(1000)\\n_ONE_MILLI = _ONE_MICRO * 1000\\n_ONE_SECOND = _ONE_MILLI * 1000\\n_ONE_MINUTE = 60 * _ONE_SECOND\\n_ONE_HOUR = 60 * _ONE_MINUTE\\n_ONE_DAY = 24 * _ONE_HOUR",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def infer_freq(index, warn=True):\\n    import pandas as pd\\n    if isinstance(index, com.ABCSeries):\\n        values = index.values\\n        if not (com.is_datetime64_dtype(index.values) or values.dtype == object):\\n            raise TypeError(\"cannot infer freq from a non-convertible dtype on a Series of {0}\".format(index.dtype))\\n        index = values\\n    if isinstance(index, pd.PeriodIndex):\\n        raise TypeError(\"PeriodIndex given. Check the `freq` attribute \"\\n                         \"instead of using infer_freq.\")\\n    if not isinstance(index, pd.DatetimeIndex) and isinstance(index, pd.Index):\\n        raise TypeError(\"cannot infer freq from a non-convertible index type {0}\".format(type(index)))\\n    index = pd.DatetimeIndex(index)\\n    inferer = _FrequencyInferer(index, warn=warn)\\n    return inferer.get_freq()\\n_ONE_MICRO = long(1000)\\n_ONE_MILLI = _ONE_MICRO * 1000\\n_ONE_SECOND = _ONE_MILLI * 1000\\n_ONE_MINUTE = 60 * _ONE_SECOND\\n_ONE_HOUR = 60 * _ONE_MINUTE\\n_ONE_DAY = 24 * _ONE_HOUR"
  },
  {
    "code": "def visit(self, node, **kwargs):\\n        parse = ast.parse\\n        if isinstance(node, string_types):\\n            clean = self.preparser(node)\\n        elif isinstance(node, ast.AST):\\n            clean = node\\n        else:\\n            raise TypeError(\"Cannot visit objects of type {0!r}\"\\n                            \"\".format(node.__class__.__name__))\\n        node = parse(clean)\\n        method = 'visit_' + node.__class__.__name__\\n        visitor = getattr(self, method, None)\\n        return visitor(node, **kwargs)\\n    def visit_Module(self, node, **kwargs):\\n        if len(node.body) != 1:\\n            raise SyntaxError('only a single expression is allowed')\\n        expr = node.body[0]\\n        return self.visit(expr, **kwargs)\\n    def visit_Expr(self, node, **kwargs):\\n        return self.visit(node.value, **kwargs)\\n    def visit_BinOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        left = self.visit(node.left, side='left')\\n        right = self.visit(node.right, side='right')\\n        return op(left, right)\\n    def visit_Div(self, node, **kwargs):\\n        return lambda lhs, rhs: Div(lhs, rhs,\\n                                    truediv=self.env.locals['truediv'])\\n    def visit_UnaryOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        operand = self.visit(node.operand)\\n        return op(operand)\\n    def visit_Name(self, node, **kwargs):\\n        return self.term_type(node.id, self.env, **kwargs)\\n    def visit_Num(self, node, **kwargs):\\n        return self.const_type(node.n, self.env)\\n    def visit_Str(self, node, **kwargs):\\n        return self.const_type(node.s, self.env)\\n    def visit_List(self, node, **kwargs):\\n        return self.const_type([self.visit(e).value for e in node.elts],\\n                               self.env)\\n    visit_Tuple = visit_List\\n    def visit_Index(self, node, **kwargs):\\n        return self.visit(node.value)\\n    def visit_Subscript(self, node, **kwargs):\\n        value = self.visit(node.value)\\n        slobj = self.visit(node.slice)\\n        expr = com.pprint_thing(slobj)\\n        result = pd.eval(expr, local_dict=self.env, engine=self.engine,\\n                         parser=self.parser)\\n        try:\\n            v = value.value[result]\\n        except AttributeError:\\n            lhs = pd.eval(com.pprint_thing(value), local_dict=self.env,\\n                          engine=self.engine, parser=self.parser)\\n            v = lhs[result]\\n        name = self.env.add_tmp(v)\\n        return self.term_type(name, env=self.env)\\n    def visit_Slice(self, node, **kwargs):\\n        lower = node.lower\\n        if lower is not None:\\n            lower = self.visit(lower).value\\n        upper = node.upper\\n        if upper is not None:\\n            upper = self.visit(upper).value\\n        step = node.step\\n        if step is not None:\\n            step = self.visit(step).value\\n        return slice(lower, upper, step)\\n    def visit_Assign(self, node, **kwargs):\\n        cmpr = ast.Compare(ops=[ast.Eq()], left=node.targets[0],\\n                           comparators=[node.value])\\n        return self.visit(cmpr)\\n    def visit_Attribute(self, node, **kwargs):\\n        attr = node.attr\\n        value = node.value\\n        ctx = node.ctx\\n        if isinstance(ctx, ast.Load):\\n            resolved = self.visit(value).value\\n            try:\\n                v = getattr(resolved, attr)\\n                name = self.env.add_tmp(v)\\n                return self.term_type(name, self.env)\\n            except AttributeError:\\n                if isinstance(value, ast.Name) and value.id == attr:\\n                    return resolved\\n        raise ValueError(\"Invalid Attribute context {0}\".format(ctx.__name__))\\n    def visit_Call(self, node, **kwargs):\\n        if isinstance(node.func, ast.Attribute):\\n            res = self.visit_Attribute(node.func)\\n        elif not isinstance(node.func, ast.Name):\\n            raise TypeError(\"Only named functions are supported\")\\n        else:\\n            res = self.visit(node.func)\\n        if res is None:\\n            raise ValueError(\"Invalid function call {0}\".format(node.func.id))\\n        if hasattr(res, 'value'):\\n            res = res.value\\n        args = [self.visit(targ).value for targ in node.args]\\n        if node.starargs is not None:\\n            args = args + self.visit(node.starargs).value\\n        keywords = {}\\n        for key in node.keywords:\\n            if not isinstance(key, ast.keyword):\\n                raise ValueError(\\n                    \"keyword error in function call '{0}'\".format(node.func.id))\\n            keywords[key.arg] = self.visit(key.value).value\\n        if node.kwargs is not None:\\n            keywords.update(self.visit(node.kwargs).value)\\n        return self.const_type(res(*args, **keywords), self.env)\\n    def visit_Compare(self, node, **kwargs):\\n        ops = node.ops\\n        comps = node.comparators",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add DataFrame.eval method",
    "fixed_code": "def visit(self, node, **kwargs):\\n        if isinstance(node, string_types):\\n            clean = self.preparser(node)\\n            node = ast.fix_missing_locations(ast.parse(clean))\\n        elif not isinstance(node, ast.AST):\\n            raise TypeError(\"Cannot visit objects of type {0!r}\"\\n                            \"\".format(node.__class__.__name__))\\n        method = 'visit_' + node.__class__.__name__\\n        visitor = getattr(self, method)\\n        return visitor(node, **kwargs)\\n    def visit_Module(self, node, **kwargs):\\n        if len(node.body) != 1:\\n            raise SyntaxError('only a single expression is allowed')\\n        expr = node.body[0]\\n        return self.visit(expr, **kwargs)\\n    def visit_Expr(self, node, **kwargs):\\n        return self.visit(node.value, **kwargs)\\n    def visit_BinOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        left = self.visit(node.left, side='left')\\n        right = self.visit(node.right, side='right')\\n        return op(left, right)\\n    def visit_Div(self, node, **kwargs):\\n        return lambda lhs, rhs: Div(lhs, rhs,\\n                                    truediv=self.env.locals['truediv'])\\n    def visit_UnaryOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        operand = self.visit(node.operand)\\n        return op(operand)\\n    def visit_Name(self, node, **kwargs):\\n        return self.term_type(node.id, self.env, **kwargs)\\n    def visit_Num(self, node, **kwargs):\\n        return self.const_type(node.n, self.env)\\n    def visit_Str(self, node, **kwargs):\\n        name = self.env.add_tmp(node.s)\\n        return self.term_type(name, self.env)\\n    def visit_List(self, node, **kwargs):\\n        name = self.env.add_tmp([self.visit(e).value for e in node.elts])\\n        return self.term_type(name, self.env)\\n    visit_Tuple = visit_List\\n    def visit_Index(self, node, **kwargs):\\n        return self.visit(node.value)\\n    def visit_Subscript(self, node, **kwargs):\\n        value = self.visit(node.value)\\n        slobj = self.visit(node.slice)\\n        expr = com.pprint_thing(slobj)\\n        result = pd.eval(expr, local_dict=self.env, engine=self.engine,\\n                         parser=self.parser)\\n        try:\\n            v = value.value[result]\\n        except AttributeError:\\n            lhs = pd.eval(com.pprint_thing(value), local_dict=self.env,\\n                          engine=self.engine, parser=self.parser)\\n            v = lhs[result]\\n        name = self.env.add_tmp(v)\\n        return self.term_type(name, env=self.env)\\n    def visit_Slice(self, node, **kwargs):\\n        lower = node.lower\\n        if lower is not None:\\n            lower = self.visit(lower).value\\n        upper = node.upper\\n        if upper is not None:\\n            upper = self.visit(upper).value\\n        step = node.step\\n        if step is not None:\\n            step = self.visit(step).value\\n        return slice(lower, upper, step)\\n    def visit_Assign(self, node, **kwargs):\\n        cmpr = ast.Compare(ops=[ast.Eq()], left=node.targets[0],\\n                           comparators=[node.value])\\n        return self.visit(cmpr)\\n    def visit_Attribute(self, node, **kwargs):\\n        attr = node.attr\\n        value = node.value\\n        ctx = node.ctx\\n        if isinstance(ctx, ast.Load):\\n            resolved = self.visit(value).value\\n            try:\\n                v = getattr(resolved, attr)\\n                name = self.env.add_tmp(v)\\n                return self.term_type(name, self.env)\\n            except AttributeError:\\n                if isinstance(value, ast.Name) and value.id == attr:\\n                    return resolved\\n        raise ValueError(\"Invalid Attribute context {0}\".format(ctx.__name__))\\n    def visit_Call(self, node, **kwargs):\\n        if isinstance(node.func, ast.Attribute):\\n            res = self.visit_Attribute(node.func)\\n        elif not isinstance(node.func, ast.Name):\\n            raise TypeError(\"Only named functions are supported\")\\n        else:\\n            res = self.visit(node.func)\\n        if res is None:\\n            raise ValueError(\"Invalid function call {0}\".format(node.func.id))\\n        if hasattr(res, 'value'):\\n            res = res.value\\n        args = [self.visit(targ).value for targ in node.args]\\n        if node.starargs is not None:\\n            args = args + self.visit(node.starargs).value\\n        keywords = {}\\n        for key in node.keywords:\\n            if not isinstance(key, ast.keyword):\\n                raise ValueError(\\n                    \"keyword error in function call '{0}'\".format(node.func.id))\\n            keywords[key.arg] = self.visit(key.value).value\\n        if node.kwargs is not None:\\n            keywords.update(self.visit(node.kwargs).value)\\n        return self.const_type(res(*args, **keywords), self.env)\\n    def visit_Compare(self, node, **kwargs):\\n        ops = node.ops\\n        comps = node.comparators"
  },
  {
    "code": "def _aggregate_generic(self, agger, axis=0):\\n        result = {}\\n        obj = self._get_obj_with_exclusions()\\n        try:\\n            for name in self.primary:\\n                data = self.get_group(name, obj=obj)\\n                try:\\n                    result[name] = agger(data)\\n                except Exception:\\n                    result[name] = data.apply(agger, axis=axis)\\n        except Exception, e1:\\n            if axis == 0:\\n                try:\\n                    return self._aggregate_item_by_item(agger)\\n                except Exception:\\n                    raise e1\\n            else:\\n                raise e1\\n        result = DataFrame(result)\\n        if axis == 0:\\n            result = result.T\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_generic(self, agger, axis=0):\\n        result = {}\\n        obj = self._get_obj_with_exclusions()\\n        try:\\n            for name in self.primary:\\n                data = self.get_group(name, obj=obj)\\n                try:\\n                    result[name] = agger(data)\\n                except Exception:\\n                    result[name] = data.apply(agger, axis=axis)\\n        except Exception, e1:\\n            if axis == 0:\\n                try:\\n                    return self._aggregate_item_by_item(agger)\\n                except Exception:\\n                    raise e1\\n            else:\\n                raise e1\\n        result = DataFrame(result)\\n        if axis == 0:\\n            result = result.T\\n        return result"
  },
  {
    "code": "def fetch_role_related(self, related, role_id):\\n\\t\\tresults = []\\n\\t\\ttry:\\n\\t\\t\\turl = _urljoin(self.api_server, self.available_api_versions['v1'], \"roles\", role_id, related,\\n\\t\\t\\t\\t\\t\\t   \"?page_size=50\")\\n\\t\\t\\tdata = self._call_galaxy(url)\\n\\t\\t\\tresults = data['results']\\n\\t\\t\\tdone = (data.get('next_link', None) is None)\\n\\t\\t\\turl_info = urlparse(self.api_server)\\n\\t\\t\\tbase_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\\n\\t\\t\\twhile not done:\\n\\t\\t\\t\\turl = _urljoin(base_url, data['next_link'])\\n\\t\\t\\t\\tdata = self._call_galaxy(url)\\n\\t\\t\\t\\tresults += data['results']\\n\\t\\t\\t\\tdone = (data.get('next_link', None) is None)\\n\\t\\texcept Exception as e:\\n\\t\\t\\tdisplay.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\\n\\t\\t\\t\\t\\t\\t\\t% (role_id, related, to_text(e)))\\n\\t\\treturn results\\n\\t@g_connect(['v1'])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fetch_role_related(self, related, role_id):\\n\\t\\tresults = []\\n\\t\\ttry:\\n\\t\\t\\turl = _urljoin(self.api_server, self.available_api_versions['v1'], \"roles\", role_id, related,\\n\\t\\t\\t\\t\\t\\t   \"?page_size=50\")\\n\\t\\t\\tdata = self._call_galaxy(url)\\n\\t\\t\\tresults = data['results']\\n\\t\\t\\tdone = (data.get('next_link', None) is None)\\n\\t\\t\\turl_info = urlparse(self.api_server)\\n\\t\\t\\tbase_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\\n\\t\\t\\twhile not done:\\n\\t\\t\\t\\turl = _urljoin(base_url, data['next_link'])\\n\\t\\t\\t\\tdata = self._call_galaxy(url)\\n\\t\\t\\t\\tresults += data['results']\\n\\t\\t\\t\\tdone = (data.get('next_link', None) is None)\\n\\t\\texcept Exception as e:\\n\\t\\t\\tdisplay.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\\n\\t\\t\\t\\t\\t\\t\\t% (role_id, related, to_text(e)))\\n\\t\\treturn results\\n\\t@g_connect(['v1'])"
  },
  {
    "code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_dates=False, date_parser=None, na_values=None,\\n                    thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: selectively parse columns in ExcelFile.parse #873",
    "fixed_code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_cols=None, parse_dates=False, date_parser=None,\\n                    na_values=None, thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        should_parse = {}\\n        for row in sheet.iter_rows():\\n            row_data = []\\n            for j, cell in enumerate(row):\\n                if parse_cols is not None and j not in should_parse:\\n                    should_parse[j] = self._should_parse(j, parse_cols)\\n                if parse_cols is None or should_parse[j]:\\n                    row_data.append(cell.internal_value)\\n            data.append(row_data)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()"
  },
  {
    "code": "def get_conn(self) -> DataFactoryManagementClient:\\n        if not self._conn:\\n            self._conn = super().get_conn()\\n        return self._conn\\n    @provide_targeted_factory",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix AzureDataFactoryHook failing to instantiate its connection (#14565)\\n\\ncloses #14557",
    "fixed_code": "def get_conn(self) -> DataFactoryManagementClient:\\n        if self._conn is not None:\\n            return self._conn\\n        conn = self.get_connection(self.conn_id)\\n        self._conn = DataFactoryManagementClient(\\n            credential=ClientSecretCredential(\\n                client_id=conn.login, client_secret=conn.password, tenant_id=conn.extra_dejson.get(\"tenantId\")\\n            ),\\n            subscription_id=conn.extra_dejson.get(\"subscriptionId\"),\\n        )\\n        return self._conn\\n    @provide_targeted_factory"
  },
  {
    "code": "def sort(self, columns=None, column=None, axis=0, ascending=True,\\n             inplace=False):\\n        if column is not None: \\n            import warnings\\n            warnings.warn(\"column is deprecated, use columns\", FutureWarning)\\n            columns = column\\n        return self.sort_index(by=columns, axis=axis, ascending=ascending,\\n                               inplace=inplace)\\n    def sort_index(self, axis=0, by=None, ascending=True, inplace=False):\\n        from pandas.core.groupby import _lexsort_indexer\\n        labels = self._get_axis(axis)\\n        if by is not None:\\n            assert(axis == 0)\\n            if isinstance(by, (tuple, list)):\\n                keys = [self[x].values for x in by]\\n                indexer = _lexsort_indexer(keys)\\n            else:\\n                indexer = self[by].values.argsort()\\n        else:\\n            indexer = labels.argsort()\\n        if not ascending:\\n            indexer = indexer[::-1]\\n        if inplace:\\n            if axis == 1:\\n                self._data = self._data.reindex_items(self._data.items[indexer],\\n                                                      copy=False)\\n            elif axis == 0:\\n                self._data = self._data.take(indexer)\\n            else:\\n                raise ValueError('Axis must be 0 or 1, got %s' % str(axis))\\n            self._clear_item_cache()\\n            return self\\n        else:\\n            return self.take(indexer, axis=axis)\\n    def sortlevel(self, level=0, axis=0, ascending=True):\\n        the_axis = self._get_axis(axis)\\n        if not isinstance(the_axis, MultiIndex):\\n            raise Exception('can only sort by level with a hierarchical index')\\n        new_axis, indexer = the_axis.sortlevel(level, ascending=ascending)\\n        if self._data.is_mixed_dtype():\\n            if axis == 0:\\n                return self.reindex(index=new_axis)\\n            else:\\n                return self.reindex(columns=new_axis)\\n        if axis == 0:\\n            index = new_axis\\n            columns = self.columns\\n        else:\\n            index = self.index\\n            columns = new_axis\\n        new_values = self.values.take(indexer, axis=axis)\\n        return self._constructor(new_values, index=index, columns=columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sort(self, columns=None, column=None, axis=0, ascending=True,\\n             inplace=False):\\n        if column is not None: \\n            import warnings\\n            warnings.warn(\"column is deprecated, use columns\", FutureWarning)\\n            columns = column\\n        return self.sort_index(by=columns, axis=axis, ascending=ascending,\\n                               inplace=inplace)\\n    def sort_index(self, axis=0, by=None, ascending=True, inplace=False):\\n        from pandas.core.groupby import _lexsort_indexer\\n        labels = self._get_axis(axis)\\n        if by is not None:\\n            assert(axis == 0)\\n            if isinstance(by, (tuple, list)):\\n                keys = [self[x].values for x in by]\\n                indexer = _lexsort_indexer(keys)\\n            else:\\n                indexer = self[by].values.argsort()\\n        else:\\n            indexer = labels.argsort()\\n        if not ascending:\\n            indexer = indexer[::-1]\\n        if inplace:\\n            if axis == 1:\\n                self._data = self._data.reindex_items(self._data.items[indexer],\\n                                                      copy=False)\\n            elif axis == 0:\\n                self._data = self._data.take(indexer)\\n            else:\\n                raise ValueError('Axis must be 0 or 1, got %s' % str(axis))\\n            self._clear_item_cache()\\n            return self\\n        else:\\n            return self.take(indexer, axis=axis)\\n    def sortlevel(self, level=0, axis=0, ascending=True):\\n        the_axis = self._get_axis(axis)\\n        if not isinstance(the_axis, MultiIndex):\\n            raise Exception('can only sort by level with a hierarchical index')\\n        new_axis, indexer = the_axis.sortlevel(level, ascending=ascending)\\n        if self._data.is_mixed_dtype():\\n            if axis == 0:\\n                return self.reindex(index=new_axis)\\n            else:\\n                return self.reindex(columns=new_axis)\\n        if axis == 0:\\n            index = new_axis\\n            columns = self.columns\\n        else:\\n            index = self.index\\n            columns = new_axis\\n        new_values = self.values.take(indexer, axis=axis)\\n        return self._constructor(new_values, index=index, columns=columns)"
  },
  {
    "code": "def update(self, req, id, type_name, type_version, changes, **kwargs):\\n\\t\\tartifact_repo = self.gateway.get_artifact_repo(req.context)\\n\\t\\ttry:\\n\\t\\t\\tartifact = self._get_artifact_with_dependencies(artifact_repo, id,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttype_name,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttype_version)\\n\\t\\t\\tself._ensure_write_access(artifact, req.context)\\n\\t\\t\\tupdated = artifact\\n\\t\\t\\tfor change in changes:\\n\\t\\t\\t\\tif artifact.metadata.attributes.blobs.get(change['path']):\\n\\t\\t\\t\\t\\tmsg = _('Invalid request PATCH for work with blob')\\n\\t\\t\\t\\t\\traise webob.exc.HTTPBadRequest(explanation=msg)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tupdated = self._do_update_op(updated, change)\\n\\t\\t\\tartifact_repo.save(updated)\\n\\t\\t\\treturn self._get_artifact_with_dependencies(artifact_repo, id)\\n\\t\\texcept (exception.InvalidJsonPatchPath,\\n\\t\\t\\t\\texception.Invalid) as e:\\n\\t\\t\\traise webob.exc.HTTPBadRequest(explanation=e.msg)\\n\\t\\texcept exception.NotFound as e:\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=e.msg)\\n\\t\\texcept exception.Forbidden as e:\\n\\t\\t\\traise webob.exc.HTTPForbidden(explanation=e.msg)\\n\\t\\texcept exception.StorageQuotaFull as e:\\n\\t\\t\\tmsg = (_(\"Denying attempt to upload artifact because it exceeds \"\\n\\t\\t\\t\\t\\t \"the quota: %s\") % encodeutils.exception_to_unicode(e))\\n\\t\\t\\traise webob.exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=msg, request=req, content_type='text/plain')\\n\\t\\texcept exception.LimitExceeded as e:\\n\\t\\t\\traise webob.exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=e.msg, request=req, content_type='text/plain')\\n\\t\\texcept exception.NotAuthenticated as e:\\n\\t\\t\\traise webob.exc.HTTPUnauthorized(explanation=e.msg)\\n\\t@utils.mutating",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update(self, req, id, type_name, type_version, changes, **kwargs):\\n\\t\\tartifact_repo = self.gateway.get_artifact_repo(req.context)\\n\\t\\ttry:\\n\\t\\t\\tartifact = self._get_artifact_with_dependencies(artifact_repo, id,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttype_name,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttype_version)\\n\\t\\t\\tself._ensure_write_access(artifact, req.context)\\n\\t\\t\\tupdated = artifact\\n\\t\\t\\tfor change in changes:\\n\\t\\t\\t\\tif artifact.metadata.attributes.blobs.get(change['path']):\\n\\t\\t\\t\\t\\tmsg = _('Invalid request PATCH for work with blob')\\n\\t\\t\\t\\t\\traise webob.exc.HTTPBadRequest(explanation=msg)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tupdated = self._do_update_op(updated, change)\\n\\t\\t\\tartifact_repo.save(updated)\\n\\t\\t\\treturn self._get_artifact_with_dependencies(artifact_repo, id)\\n\\t\\texcept (exception.InvalidJsonPatchPath,\\n\\t\\t\\t\\texception.Invalid) as e:\\n\\t\\t\\traise webob.exc.HTTPBadRequest(explanation=e.msg)\\n\\t\\texcept exception.NotFound as e:\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=e.msg)\\n\\t\\texcept exception.Forbidden as e:\\n\\t\\t\\traise webob.exc.HTTPForbidden(explanation=e.msg)\\n\\t\\texcept exception.StorageQuotaFull as e:\\n\\t\\t\\tmsg = (_(\"Denying attempt to upload artifact because it exceeds \"\\n\\t\\t\\t\\t\\t \"the quota: %s\") % encodeutils.exception_to_unicode(e))\\n\\t\\t\\traise webob.exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=msg, request=req, content_type='text/plain')\\n\\t\\texcept exception.LimitExceeded as e:\\n\\t\\t\\traise webob.exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=e.msg, request=req, content_type='text/plain')\\n\\t\\texcept exception.NotAuthenticated as e:\\n\\t\\t\\traise webob.exc.HTTPUnauthorized(explanation=e.msg)\\n\\t@utils.mutating"
  },
  {
    "code": "def _dump_svmlight(X, y, f, one_based, comment, query_id):\\n    is_sp = int(hasattr(X, \"tocsr\"))\\n    if X.dtype == np.float64:\\n        value_pattern = u\"%d:%0.16e\"\\n    else:\\n        value_pattern = u\"%d:%f\"\\n    if y.dtype.kind == 'i':\\n        line_pattern = u\"%d\"\\n    else:\\n        line_pattern = u\"%f\"\\n    if query_id is not None:\\n        line_pattern += u\" qid:%d\"\\n    line_pattern += u\" %s\\n\"\\n    if comment:\\n        f.write(\"\\n\"\\n                % __version__)\\n        f.write(\"\\n\" % [\"zero\", \"one\"][one_based])\\n        f.write(\"\\n\")\\n        f.writelines(\"\\n\" % line for line in comment.splitlines())\\n    for i in xrange(X.shape[0]):\\n        s = u\" \".join([value_pattern % (j + one_based, X[i, j])\\n                       for j in X[i].nonzero()[is_sp]])\\n        if query_id is not None:\\n            feat = (y[i], query_id[i], s)\\n        else:\\n            feat = (y[i], s)\\n        f.write((line_pattern % feat).encode('ascii'))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _dump_svmlight(X, y, f, one_based, comment, query_id):\\n    is_sp = int(hasattr(X, \"tocsr\"))\\n    if X.dtype == np.float64:\\n        value_pattern = u\"%d:%0.16e\"\\n    else:\\n        value_pattern = u\"%d:%f\"\\n    if y.dtype.kind == 'i':\\n        line_pattern = u\"%d\"\\n    else:\\n        line_pattern = u\"%f\"\\n    if query_id is not None:\\n        line_pattern += u\" qid:%d\"\\n    line_pattern += u\" %s\\n\"\\n    if comment:\\n        f.write(\"\\n\"\\n                % __version__)\\n        f.write(\"\\n\" % [\"zero\", \"one\"][one_based])\\n        f.write(\"\\n\")\\n        f.writelines(\"\\n\" % line for line in comment.splitlines())\\n    for i in xrange(X.shape[0]):\\n        s = u\" \".join([value_pattern % (j + one_based, X[i, j])\\n                       for j in X[i].nonzero()[is_sp]])\\n        if query_id is not None:\\n            feat = (y[i], query_id[i], s)\\n        else:\\n            feat = (y[i], s)\\n        f.write((line_pattern % feat).encode('ascii'))"
  },
  {
    "code": "def run_luks_create(self, device, keyfile, keysize):\\n        luks_type = self._module.params['type']\\n        label = self._module.params['label']\\n        options = []\\n        if keysize is not None:\\n            options.append('--key-size=' + str(keysize))\\n        if label is not None:\\n            options.extend(['--label', label])\\n            luks_type = 'luks2'\\n        if luks_type is not None:\\n            options.extend(['--type', luks_type])\\n        args = [self._cryptsetup_bin, 'luksFormat']\\n        args.extend(options)\\n        args.extend(['-q', device, keyfile])\\n        result = self._run_command(args)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while creating LUKS on %s: %s'\\n                             % (device, result[STDERR]))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add passphrase support for luks_device  (#65050)\\n\\n\\nSeveral tests in `key-management.yml` don't `become` before executing,\\ndespite needing elevated privileges.  This commit fixes that.\\n\\n\\nPreviously, the luks_device module only worked with keyfiles.  The\\nimplication was that the key had to be written to disk before the module\\ncould be used.\\n\\nThis commit implements support for opening, adding and removing\\npassphrases supplied as strings to the module.\\n\\nCloses #52408",
    "fixed_code": "def run_luks_create(self, device, keyfile, passphrase, keysize):\\n        luks_type = self._module.params['type']\\n        label = self._module.params['label']\\n        options = []\\n        if keysize is not None:\\n            options.append('--key-size=' + str(keysize))\\n        if label is not None:\\n            options.extend(['--label', label])\\n            luks_type = 'luks2'\\n        if luks_type is not None:\\n            options.extend(['--type', luks_type])\\n        args = [self._cryptsetup_bin, 'luksFormat']\\n        args.extend(options)\\n        args.extend(['-q', device])\\n        if keyfile:\\n            args.append(keyfile)\\n        result = self._run_command(args, data=passphrase)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while creating LUKS on %s: %s'\\n                             % (device, result[STDERR]))"
  },
  {
    "code": "def truncate(self, before=None, after=None):\\n        i, j = self._get_axis_bounds(before, after)\\n        left, right = self._get_label_bounds(i, j)\\n        new_levels = list(self.levels)\\n        new_levels[0] = new_levels[0][i:j]\\n        new_labels = [lab[left:right] for lab in self.labels]\\n        new_labels[0] = new_labels[0] - i\\n        return MultiIndex(levels=new_levels, labels=new_labels)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def truncate(self, before=None, after=None):\\n        i, j = self._get_axis_bounds(before, after)\\n        left, right = self._get_label_bounds(i, j)\\n        new_levels = list(self.levels)\\n        new_levels[0] = new_levels[0][i:j]\\n        new_labels = [lab[left:right] for lab in self.labels]\\n        new_labels[0] = new_labels[0] - i\\n        return MultiIndex(levels=new_levels, labels=new_labels)"
  },
  {
    "code": "def get_necessary_tasks(self):\\n\\t\\tnecessary_tasks = set()\\n\\t\\tfor task in self.get_active_tasks():\\n\\t\\t\\tif task.status not in (DONE, DISABLED) or \\\\n\\t\\t\\t\\t\\tgetattr(task, 'scheduler_disable_time', None) is not None:\\n\\t\\t\\t\\tnecessary_tasks.update(task.deps)\\n\\t\\t\\t\\tnecessary_tasks.add(task.id)\\n\\t\\treturn necessary_tasks",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "assistants bugfix: Also prune UNKNOWN tasks\\n\\nI'll only motivate this lightly as I assume the previous mistake was a\\n(serious) oversight.  This bug is a bit hard to realize, because when\\nyou visit the scheduler in the browser the UNKNOWN tasks are not shown\\nso you don't see them accumulating.  You only notice that the page loads\\nslower for each day.\\n\\nIn our scheduler instance, there are currently 90% UNKNOWN tasks, most\\nof which would be pruned after this patch is applied. 90% because its\\ncurrent uptime is 10 days. The 9 previous days have almost exclusively\\nUNKNOWN tasks meanwhile today has almost exclusively known tasks.",
    "fixed_code": "def get_necessary_tasks(self):\\n\\t\\tnecessary_tasks = set()\\n\\t\\tfor task in self.get_active_tasks():\\n\\t\\t\\tif task.status not in (DONE, DISABLED, UNKNOWN) or \\\\n\\t\\t\\t\\t\\ttask.scheduler_disable_time is not None:\\n\\t\\t\\t\\tnecessary_tasks.update(task.deps)\\n\\t\\t\\t\\tnecessary_tasks.add(task.id)\\n\\t\\treturn necessary_tasks"
  },
  {
    "code": "def to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None, **kwargs):\\n    f = lambda store: store.put(key, value, **kwargs)\\n    if isinstance(path_or_buf, basestring):\\n        with get_store(path_or_buf, mode=mode, complevel=complevel, complib=complib) as store:\\n            f(store)\\n    else:\\n        f(path_or_buf)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: support timezone data_columns in HDFStore (GH2852)\\n\\nDOC: update release notes/whatsnew, added whatsnew 0.11.1 to index.rst\\n\\nENH: warn a FrequencyWarning if appending with a different frequency that existing",
    "fixed_code": "def to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None, append=None, **kwargs):\\n    if append:\\n        f = lambda store: store.append(key, value, **kwargs)\\n    else:\\n        f = lambda store: store.put(key, value, **kwargs)\\n    if isinstance(path_or_buf, basestring):\\n        with get_store(path_or_buf, mode=mode, complevel=complevel, complib=complib) as store:\\n            f(store)\\n    else:\\n        f(path_or_buf)"
  },
  {
    "code": "def validate(self, value):\\n        super(SimpleArrayField, self).validate(value)\\n        errors = []\\n        for index, item in enumerate(value):\\n            try:\\n                self.base_field.validate(item)\\n            except ValidationError as error:\\n                errors.append(prefix_validation_error(\\n                    error,\\n                    prefix=self.error_messages['item_invalid'],\\n                    code='item_invalid',\\n                    params={'nth': index},\\n                ))\\n        if errors:\\n            raise ValidationError(errors)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate(self, value):\\n        super(SimpleArrayField, self).validate(value)\\n        errors = []\\n        for index, item in enumerate(value):\\n            try:\\n                self.base_field.validate(item)\\n            except ValidationError as error:\\n                errors.append(prefix_validation_error(\\n                    error,\\n                    prefix=self.error_messages['item_invalid'],\\n                    code='item_invalid',\\n                    params={'nth': index},\\n                ))\\n        if errors:\\n            raise ValidationError(errors)"
  },
  {
    "code": "def WINFUNCTYPE(restype, *argtypes, **kw):\\n        flags = _FUNCFLAG_STDCALL\\n        if kw.pop(\"use_errno\", False):\\n            flags |= _FUNCFLAG_USE_ERRNO\\n        if kw.pop(\"use_last_error\", False):\\n            flags |= _FUNCFLAG_USE_LASTERROR\\n        if kw:\\n            raise ValueError(\"unexpected keyword argument(s) %s\" % kw.keys())\\n        try:\\n            return _win_functype_cache[(restype, argtypes, flags)]\\n        except KeyError:\\n            class WinFunctionType(_CFuncPtr):\\n                _argtypes_ = argtypes\\n                _restype_ = restype\\n                _flags_ = flags\\n            _win_functype_cache[(restype, argtypes, flags)] = WinFunctionType\\n            return WinFunctionType\\n    if WINFUNCTYPE.__doc__:\\n        WINFUNCTYPE.__doc__ = CFUNCTYPE.__doc__.replace(\"CFUNCTYPE\", \"WINFUNCTYPE\")\\nelif _os.name == \"posix\":\\n    from _ctypes import dlopen as _dlopen\\nfrom _ctypes import sizeof, byref, addressof, alignment, resize\\nfrom _ctypes import get_errno, set_errno\\nfrom _ctypes import _SimpleCData",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def WINFUNCTYPE(restype, *argtypes, **kw):\\n        flags = _FUNCFLAG_STDCALL\\n        if kw.pop(\"use_errno\", False):\\n            flags |= _FUNCFLAG_USE_ERRNO\\n        if kw.pop(\"use_last_error\", False):\\n            flags |= _FUNCFLAG_USE_LASTERROR\\n        if kw:\\n            raise ValueError(\"unexpected keyword argument(s) %s\" % kw.keys())\\n        try:\\n            return _win_functype_cache[(restype, argtypes, flags)]\\n        except KeyError:\\n            class WinFunctionType(_CFuncPtr):\\n                _argtypes_ = argtypes\\n                _restype_ = restype\\n                _flags_ = flags\\n            _win_functype_cache[(restype, argtypes, flags)] = WinFunctionType\\n            return WinFunctionType\\n    if WINFUNCTYPE.__doc__:\\n        WINFUNCTYPE.__doc__ = CFUNCTYPE.__doc__.replace(\"CFUNCTYPE\", \"WINFUNCTYPE\")\\nelif _os.name == \"posix\":\\n    from _ctypes import dlopen as _dlopen\\nfrom _ctypes import sizeof, byref, addressof, alignment, resize\\nfrom _ctypes import get_errno, set_errno\\nfrom _ctypes import _SimpleCData"
  },
  {
    "code": "def add_operation(self, app_label, operation, dependencies=None):\\n        operation._auto_deps = dependencies or []\\n        self.generated_operations.setdefault(app_label, []).append(operation)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #23315: Operational dependency fail with mixed create/add",
    "fixed_code": "def add_operation(self, app_label, operation, dependencies=None, beginning=False):\\n        operation._auto_deps = dependencies or []\\n        if beginning:\\n            self.generated_operations.setdefault(app_label, []).insert(0, operation)\\n        else:\\n            self.generated_operations.setdefault(app_label, []).append(operation)"
  },
  {
    "code": "def setInterfaceOption(module, lines, iface, option, raw_value, state, address_family=None):\\n    value = str(raw_value)\\n    changed = False\\n    iface_lines = [item for item in lines if \"iface\" in item and item[\"iface\"] == iface]\\n    if address_family is not None:\\n        iface_lines = [item for item in iface_lines\\n                       if \"address_family\" in item and item[\"address_family\"] == address_family]\\n    if len(iface_lines) < 1:\\n        module.fail_json(msg=\"Error: interface %s not found\" % iface)\\n        return changed, None\\n    iface_options = list(filter(lambda i: i['line_type'] == 'option', iface_lines))\\n    target_options = list(filter(lambda i: i['option'] == option, iface_options))\\n    if state == \"present\":\\n        if len(target_options) < 1:\\n            changed = True\\n            last_line_dict = iface_lines[-1]\\n            changed, lines = addOptionAfterLine(option, value, iface, lines, last_line_dict, iface_options, address_family)\\n        else:\\n            if option in [\"pre-up\", \"up\", \"down\", \"post-up\"]:\\n                if len(list(filter(lambda i: i['value'] == value, target_options))) < 1:\\n                    changed, lines = addOptionAfterLine(option, value, iface, lines, target_options[-1], iface_options, address_family)\\n            else:\\n                if target_options[-1]['value'] != value:\\n                    changed = True\\n                    target_option = target_options[-1]\\n                    old_line = target_option['line']\\n                    old_value = target_option['value']\\n                    address_family = target_option['address_family']\\n                    prefix_start = old_line.find(option)\\n                    optionLen = len(option)\\n                    old_value_position = re.search(r\"\\s+\".join(old_value.split()), old_line[prefix_start + optionLen:])\\n                    start = old_value_position.start() + prefix_start + optionLen\\n                    end = old_value_position.end() + prefix_start + optionLen\\n                    line = old_line[:start] + value + old_line[end:]\\n                    index = len(lines) - lines[::-1].index(target_option) - 1\\n                    lines[index] = optionDict(line, iface, option, value, address_family)\\n    elif state == \"absent\":\\n        if len(target_options) >= 1:\\n            if option in [\"pre-up\", \"up\", \"down\", \"post-up\"] and value is not None and value != \"None\":\\n                for target_option in filter(lambda i: i['value'] == value, target_options):\\n                    changed = True\\n                    lines = list(filter(lambda ln: ln != target_option, lines))\\n            else:\\n                changed = True\\n                for target_option in target_options:\\n                    lines = list(filter(lambda ln: ln != target_option, lines))\\n    else:\\n        module.fail_json(msg=\"Error: unsupported state %s, has to be either present or absent\" % state)\\n    return changed, lines",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setInterfaceOption(module, lines, iface, option, raw_value, state, address_family=None):\\n    value = str(raw_value)\\n    changed = False\\n    iface_lines = [item for item in lines if \"iface\" in item and item[\"iface\"] == iface]\\n    if address_family is not None:\\n        iface_lines = [item for item in iface_lines\\n                       if \"address_family\" in item and item[\"address_family\"] == address_family]\\n    if len(iface_lines) < 1:\\n        module.fail_json(msg=\"Error: interface %s not found\" % iface)\\n        return changed, None\\n    iface_options = list(filter(lambda i: i['line_type'] == 'option', iface_lines))\\n    target_options = list(filter(lambda i: i['option'] == option, iface_options))\\n    if state == \"present\":\\n        if len(target_options) < 1:\\n            changed = True\\n            last_line_dict = iface_lines[-1]\\n            changed, lines = addOptionAfterLine(option, value, iface, lines, last_line_dict, iface_options, address_family)\\n        else:\\n            if option in [\"pre-up\", \"up\", \"down\", \"post-up\"]:\\n                if len(list(filter(lambda i: i['value'] == value, target_options))) < 1:\\n                    changed, lines = addOptionAfterLine(option, value, iface, lines, target_options[-1], iface_options, address_family)\\n            else:\\n                if target_options[-1]['value'] != value:\\n                    changed = True\\n                    target_option = target_options[-1]\\n                    old_line = target_option['line']\\n                    old_value = target_option['value']\\n                    address_family = target_option['address_family']\\n                    prefix_start = old_line.find(option)\\n                    optionLen = len(option)\\n                    old_value_position = re.search(r\"\\s+\".join(old_value.split()), old_line[prefix_start + optionLen:])\\n                    start = old_value_position.start() + prefix_start + optionLen\\n                    end = old_value_position.end() + prefix_start + optionLen\\n                    line = old_line[:start] + value + old_line[end:]\\n                    index = len(lines) - lines[::-1].index(target_option) - 1\\n                    lines[index] = optionDict(line, iface, option, value, address_family)\\n    elif state == \"absent\":\\n        if len(target_options) >= 1:\\n            if option in [\"pre-up\", \"up\", \"down\", \"post-up\"] and value is not None and value != \"None\":\\n                for target_option in filter(lambda i: i['value'] == value, target_options):\\n                    changed = True\\n                    lines = list(filter(lambda ln: ln != target_option, lines))\\n            else:\\n                changed = True\\n                for target_option in target_options:\\n                    lines = list(filter(lambda ln: ln != target_option, lines))\\n    else:\\n        module.fail_json(msg=\"Error: unsupported state %s, has to be either present or absent\" % state)\\n    return changed, lines"
  },
  {
    "code": "def write_frame(frame, name, con, flavor='sqlite', if_exists='fail', **kwargs):\\n    if 'append' in kwargs:\\n        import warnings\\n        warnings.warn(\"append is deprecated, use if_exists instead\",\\n                      FutureWarning)\\n        if kwargs['append']:\\n            if_exists='append'\\n        else:\\n            if_exists='fail'\\n    exists = table_exists(name, con, flavor)\\n    if if_exists == 'fail' and exists:\\n        raise ValueError, \"Table '%s' already exists.\" % name\\n    create = None\\n    if exists and if_exists == 'replace':\\n        create = \"DROP TABLE %s\" % name\\n    elif not exists:\\n        create = get_schema(frame, name, flavor)\\n    if create is not None:\\n        cur = con.cursor()\\n        cur.execute(create)\\n        cur.close()\\n    cur = con.cursor()\\n    safe_names = [s.replace(' ', '_').strip() for s in frame.columns]\\n    flavor_picker = {'sqlite' : _write_sqlite,\\n                     'mysql' : _write_mysql}\\n    func = flavor_picker.get(flavor, None)\\n    if func is None:\\n        raise NotImplementedError\\n    func(frame, name, safe_names, cur)\\n    cur.close()\\n    con.commit()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert \"Merging in MySQL support #2482\"",
    "fixed_code": "def write_frame(frame, name=None, con=None, flavor='sqlite', append=False):\\n    if flavor == 'sqlite':\\n        schema = get_sqlite_schema(frame, name)\\n    else:\\n        raise NotImplementedError\\n    if not append and not has_table(name, con):\\n        con.execute(schema)\\n    wildcards = ','.join(['?'] * len(frame.columns))\\n    insert_sql = 'INSERT INTO %s VALUES (%s)' % (name, wildcards)\\n    data = [tuple(x) for x in frame.values]\\n    con.executemany(insert_sql, data)"
  },
  {
    "code": "class HTTPSClientAuthConnection(httplib.HTTPSConnection):\\n\\tdef __init__(self, host, port, key_file, cert_file,\\n\\t\\t\\t\\t ca_file, timeout=None, insecure=False):\\n\\t\\thttplib.HTTPSConnection.__init__(self, host, port, key_file=key_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t cert_file=cert_file)\\n\\t\\tself.key_file = key_file\\n\\t\\tself.cert_file = cert_file\\n\\t\\tself.ca_file = ca_file\\n\\t\\tself.timeout = timeout\\n\\t\\tself.insecure = insecure",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class HTTPSClientAuthConnection(httplib.HTTPSConnection):\\n\\tdef __init__(self, host, port, key_file, cert_file,\\n\\t\\t\\t\\t ca_file, timeout=None, insecure=False):\\n\\t\\thttplib.HTTPSConnection.__init__(self, host, port, key_file=key_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t cert_file=cert_file)\\n\\t\\tself.key_file = key_file\\n\\t\\tself.cert_file = cert_file\\n\\t\\tself.ca_file = ca_file\\n\\t\\tself.timeout = timeout\\n\\t\\tself.insecure = insecure"
  },
  {
    "code": "def convert_value(cls, value: Optional[Any]) -> Optional[Any]:\\n        if not value:\\n            return value\\n        elif isinstance(value, (str, int, float, bool, dict)):\\n            return value\\n        elif isinstance(value, bytes):\\n            return b64encode(value).decode('ascii')\\n        elif isinstance(value, UUID):\\n            return b64encode(value.bytes).decode('ascii')\\n        elif isinstance(value, (datetime, Date)):\\n            return str(value)\\n        elif isinstance(value, Decimal):\\n            return float(value)\\n        elif isinstance(value, Time):\\n            return str(value).split('.')[0]\\n        elif isinstance(value, (list, SortedSet)):\\n            return cls.convert_array_types(value)\\n        elif hasattr(value, '_fields'):\\n            return cls.convert_user_type(value)\\n        elif isinstance(value, tuple):\\n            return cls.convert_tuple_type(value)\\n        elif isinstance(value, OrderedMapSerializedKey):\\n            return cls.convert_map_type(value)\\n        else:\\n            raise AirflowException('Unexpected value: ' + str(value))\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[Issue#22846] allow option to encode or not encode UUID when uploading from Cassandra to GCS (#23766)",
    "fixed_code": "def convert_value(self, value: Optional[Any]) -> Optional[Any]:\\n        if not value:\\n            return value\\n        elif isinstance(value, (str, int, float, bool, dict)):\\n            return value\\n        elif isinstance(value, bytes):\\n            return b64encode(value).decode('ascii')\\n        elif isinstance(value, UUID):\\n            if self.encode_uuid:\\n                return b64encode(value.bytes).decode('ascii')\\n            else:\\n                return str(value)\\n        elif isinstance(value, (datetime, Date)):\\n            return str(value)\\n        elif isinstance(value, Decimal):\\n            return float(value)\\n        elif isinstance(value, Time):\\n            return str(value).split('.')[0]\\n        elif isinstance(value, (list, SortedSet)):\\n            return self.convert_array_types(value)\\n        elif hasattr(value, '_fields'):\\n            return self.convert_user_type(value)\\n        elif isinstance(value, tuple):\\n            return self.convert_tuple_type(value)\\n        elif isinstance(value, OrderedMapSerializedKey):\\n            return self.convert_map_type(value)\\n        else:\\n            raise AirflowException('Unexpected value: ' + str(value))"
  },
  {
    "code": "def __init__(self, iterable=None, **kwds):\\n        gallahad')                 \\n        >>> c = Counter({'a': 4, 'b': 2})           \\n        >>> c = Counter(a=4, b=2)                   \\n        '''\\n        self.update(iterable, **kwds)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Backport PEP 372: OrderedDict()",
    "fixed_code": "def __init__(self, *args, **kwds):\\n        if len(args) > 1:\\n            raise TypeError('expected at most 1 arguments, got %d' % len(args))\\n        if not hasattr(self, '_keys'):\\n            self._keys = []\\n        self.update(*args, **kwds)"
  },
  {
    "code": "def warn_dropping_nuisance_columns_deprecated(cls, how: str, numeric_only) -> None:\\n    if numeric_only is not lib.no_default and not numeric_only:\\n        warnings.warn(\\n            \"Dropping invalid columns in \"\\n            f\"{cls.__name__}.{how} is deprecated. \"\\n            \"In a future version, a TypeError will be raised. \"\\n            f\"Before calling .{how}, select only columns which \"\\n            \"should be valid for the function.\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(),\\n        )\\n    elif numeric_only is lib.no_default:\\n        warnings.warn(\\n            \"The default value of numeric_only in \"\\n            f\"{cls.__name__}.{how} is deprecated. \"\\n            \"In a future version, numeric_only will default to False. \"\\n            f\"Either specify numeric_only or select only columns which \"\\n            \"should be valid for the function.\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(),\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def warn_dropping_nuisance_columns_deprecated(cls, how: str, numeric_only) -> None:\\n    if numeric_only is not lib.no_default and not numeric_only:\\n        warnings.warn(\\n            \"Dropping invalid columns in \"\\n            f\"{cls.__name__}.{how} is deprecated. \"\\n            \"In a future version, a TypeError will be raised. \"\\n            f\"Before calling .{how}, select only columns which \"\\n            \"should be valid for the function.\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(),\\n        )\\n    elif numeric_only is lib.no_default:\\n        warnings.warn(\\n            \"The default value of numeric_only in \"\\n            f\"{cls.__name__}.{how} is deprecated. \"\\n            \"In a future version, numeric_only will default to False. \"\\n            f\"Either specify numeric_only or select only columns which \"\\n            \"should be valid for the function.\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(),\\n        )"
  },
  {
    "code": "def convert_tuple_type(cls, values: Tuple[Any]) -> Dict[str, Any]:\\n        names = ['field_' + str(i) for i in range(len(values))]\\n        return cls.generate_data_dict(names, values)\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[Issue#22846] allow option to encode or not encode UUID when uploading from Cassandra to GCS (#23766)",
    "fixed_code": "def convert_tuple_type(self, values: Tuple[Any]) -> Dict[str, Any]:\\n        names = ['field_' + str(i) for i in range(len(values))]\\n        return self.generate_data_dict(names, values)"
  },
  {
    "code": "def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\\n\\t\\tinline_admin_formsets = []\\n\\t\\tfor inline, formset in zip(inline_instances, formsets):\\n\\t\\t\\tfieldsets = list(inline.get_fieldsets(request, obj))\\n\\t\\t\\treadonly = list(inline.get_readonly_fields(request, obj))\\n\\t\\t\\thas_add_permission = inline.has_add_permission(request, obj)\\n\\t\\t\\thas_change_permission = inline.has_change_permission(request, obj)\\n\\t\\t\\thas_delete_permission = inline.has_delete_permission(request, obj)\\n\\t\\t\\thas_view_permission = inline.has_view_permission(request, obj)\\n\\t\\t\\tprepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n\\t\\t\\tinline_admin_formset = helpers.InlineAdminFormSet(\\n\\t\\t\\t\\tinline, formset, fieldsets, prepopulated, readonly, model_admin=self,\\n\\t\\t\\t\\thas_add_permission=has_add_permission, has_change_permission=has_change_permission,\\n\\t\\t\\t\\thas_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\\n\\t\\t\\t)\\n\\t\\t\\tinline_admin_formsets.append(inline_admin_formset)\\n\\t\\treturn inline_admin_formsets",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed CVE-2019-19118 -- Required edit permissions on parent model for editable inlines in admin.\\n\\nThank you to Shen Ying for reporting this issue.",
    "fixed_code": "def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\\n\\t\\tcan_edit_parent = self.has_change_permission(request, obj) if obj else self.has_add_permission(request)\\n\\t\\tinline_admin_formsets = []\\n\\t\\tfor inline, formset in zip(inline_instances, formsets):\\n\\t\\t\\tfieldsets = list(inline.get_fieldsets(request, obj))\\n\\t\\t\\treadonly = list(inline.get_readonly_fields(request, obj))\\n\\t\\t\\tif can_edit_parent:\\n\\t\\t\\t\\thas_add_permission = inline.has_add_permission(request, obj)\\n\\t\\t\\t\\thas_change_permission = inline.has_change_permission(request, obj)\\n\\t\\t\\t\\thas_delete_permission = inline.has_delete_permission(request, obj)\\n\\t\\t\\telse:\\n\\t\\t\\t\\thas_add_permission = has_change_permission = has_delete_permission = False\\n\\t\\t\\t\\tformset.extra = formset.max_num = 0\\n\\t\\t\\thas_view_permission = inline.has_view_permission(request, obj)\\n\\t\\t\\tprepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n\\t\\t\\tinline_admin_formset = helpers.InlineAdminFormSet(\\n\\t\\t\\t\\tinline, formset, fieldsets, prepopulated, readonly, model_admin=self,\\n\\t\\t\\t\\thas_add_permission=has_add_permission, has_change_permission=has_change_permission,\\n\\t\\t\\t\\thas_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\\n\\t\\t\\t)\\n\\t\\t\\tinline_admin_formsets.append(inline_admin_formset)\\n\\t\\treturn inline_admin_formsets"
  },
  {
    "code": "def send_email_smtp(\\n    to: Union[str, Iterable[str]],\\n    subject: str,\\n    html_content: str,\\n    files: Optional[List[str]] = None,\\n    dryrun: bool = False,\\n    cc: Optional[Union[str, Iterable[str]]] = None,\\n    bcc: Optional[Union[str, Iterable[str]]] = None,\\n    mime_subtype: str = 'mixed',\\n    mime_charset: str = 'utf-8',\\n    conn_id: str = \"smtp_default\",\\n    from_email: Optional[str] = None,\\n    custom_headers: Optional[Dict[str, Any]] = None,\\n    **kwargs,\\n):\\n    smtp_mail_from = conf.get('smtp', 'SMTP_MAIL_FROM')\\n    if smtp_mail_from:\\n        mail_from = smtp_mail_from\\n    else:\\n        mail_from = from_email\\n    msg, recipients = build_mime_message(\\n        mail_from=mail_from,\\n        to=to,\\n        subject=subject,\\n        html_content=html_content,\\n        files=files,\\n        cc=cc,\\n        bcc=bcc,\\n        mime_subtype=mime_subtype,\\n        mime_charset=mime_charset,\\n        custom_headers=custom_headers,\\n    )\\n    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def send_email_smtp(\\n    to: Union[str, Iterable[str]],\\n    subject: str,\\n    html_content: str,\\n    files: Optional[List[str]] = None,\\n    dryrun: bool = False,\\n    cc: Optional[Union[str, Iterable[str]]] = None,\\n    bcc: Optional[Union[str, Iterable[str]]] = None,\\n    mime_subtype: str = 'mixed',\\n    mime_charset: str = 'utf-8',\\n    conn_id: str = \"smtp_default\",\\n    from_email: Optional[str] = None,\\n    custom_headers: Optional[Dict[str, Any]] = None,\\n    **kwargs,\\n):\\n    smtp_mail_from = conf.get('smtp', 'SMTP_MAIL_FROM')\\n    if smtp_mail_from:\\n        mail_from = smtp_mail_from\\n    else:\\n        mail_from = from_email\\n    msg, recipients = build_mime_message(\\n        mail_from=mail_from,\\n        to=to,\\n        subject=subject,\\n        html_content=html_content,\\n        files=files,\\n        cc=cc,\\n        bcc=bcc,\\n        mime_subtype=mime_subtype,\\n        mime_charset=mime_charset,\\n        custom_headers=custom_headers,\\n    )\\n    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)"
  },
  {
    "code": "def fit_transform(self, raw_documents, y=None):\\n        if not self.fit_vocabulary:\\n            return self.transform(raw_documents)\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_current = Counter(self.analyzer.analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.vocabulary = dict(((t, i) for i, t in enumerate(terms)))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Flatten the feature extraction API",
    "fixed_code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(terms)))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)"
  },
  {
    "code": "def _write_array_header(fp, d, version=None):\\n    import struct\\n    header = [\"{\"]\\n    for key, value in sorted(d.items()):\\n        header.append(\"'%s': %s, \" % (key, repr(value)))\\n    header.append(\"}\")\\n    header = \"\".join(header)\\n    header = asbytes(_filter_header(header))\\n    hlen = len(header) + 1 \\n    padlen_v1 = ARRAY_ALIGN - ((MAGIC_LEN + struct.calcsize('<H') + hlen) % ARRAY_ALIGN)\\n    padlen_v2 = ARRAY_ALIGN - ((MAGIC_LEN + struct.calcsize('<I') + hlen) % ARRAY_ALIGN)\\n    if hlen + padlen_v1 < 2**16 and version in (None, (1, 0)):\\n        version = (1, 0)\\n        header_prefix = magic(1, 0) + struct.pack('<H', hlen + padlen_v1)\\n        topad = padlen_v1\\n    elif hlen + padlen_v2 < 2**32 and version in (None, (2, 0)):\\n        version = (2, 0)\\n        header_prefix = magic(2, 0) + struct.pack('<I', hlen + padlen_v2)\\n        topad = padlen_v2\\n    else:\\n        msg = \"Header length %s too big for version=%s\"\\n        msg %= (hlen, version)\\n        raise ValueError(msg)\\n    header = header + b' '*topad + b'\\n'\\n    fp.write(header_prefix)\\n    fp.write(header)\\n    return version",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _write_array_header(fp, d, version=None):\\n    import struct\\n    header = [\"{\"]\\n    for key, value in sorted(d.items()):\\n        header.append(\"'%s': %s, \" % (key, repr(value)))\\n    header.append(\"}\")\\n    header = \"\".join(header)\\n    header = asbytes(_filter_header(header))\\n    hlen = len(header) + 1 \\n    padlen_v1 = ARRAY_ALIGN - ((MAGIC_LEN + struct.calcsize('<H') + hlen) % ARRAY_ALIGN)\\n    padlen_v2 = ARRAY_ALIGN - ((MAGIC_LEN + struct.calcsize('<I') + hlen) % ARRAY_ALIGN)\\n    if hlen + padlen_v1 < 2**16 and version in (None, (1, 0)):\\n        version = (1, 0)\\n        header_prefix = magic(1, 0) + struct.pack('<H', hlen + padlen_v1)\\n        topad = padlen_v1\\n    elif hlen + padlen_v2 < 2**32 and version in (None, (2, 0)):\\n        version = (2, 0)\\n        header_prefix = magic(2, 0) + struct.pack('<I', hlen + padlen_v2)\\n        topad = padlen_v2\\n    else:\\n        msg = \"Header length %s too big for version=%s\"\\n        msg %= (hlen, version)\\n        raise ValueError(msg)\\n    header = header + b' '*topad + b'\\n'\\n    fp.write(header_prefix)\\n    fp.write(header)\\n    return version"
  },
  {
    "code": "def serializer_factory(value):\\n    from django.db.migrations.writer import SettingsReference\\n    if isinstance(value, Promise):\\n        value = force_text(value)\\n    elif isinstance(value, LazyObject):\\n        value = value.__reduce__()[1][0]\\n    if isinstance(value, models.Field):\\n        return ModelFieldSerializer(value)\\n    if isinstance(value, models.manager.BaseManager):\\n        return ModelManagerSerializer(value)\\n    if isinstance(value, Operation):\\n        return OperationSerializer(value)\\n    if isinstance(value, type):\\n        return TypeSerializer(value)\\n    if hasattr(value, 'deconstruct'):\\n        return DeconstructableSerializer(value)\\n    if isinstance(value, frozenset):\\n        return FrozensetSerializer(value)\\n    if isinstance(value, list):\\n        return SequenceSerializer(value)\\n    if isinstance(value, set):\\n        return SetSerializer(value)\\n    if isinstance(value, tuple):\\n        return TupleSerializer(value)\\n    if isinstance(value, dict):\\n        return DictionarySerializer(value)\\n    if isinstance(value, enum.Enum):\\n        return EnumSerializer(value)\\n    if isinstance(value, datetime.datetime):\\n        return DatetimeSerializer(value)\\n    if isinstance(value, datetime.date):\\n        return DateSerializer(value)\\n    if isinstance(value, datetime.time):\\n        return TimeSerializer(value)\\n    if isinstance(value, datetime.timedelta):\\n        return TimedeltaSerializer(value)\\n    if isinstance(value, SettingsReference):\\n        return SettingsReferenceSerializer(value)\\n    if isinstance(value, float):\\n        return FloatSerializer(value)\\n    if isinstance(value, (bool, int, type(None))):\\n        return BaseSimpleSerializer(value)\\n    if isinstance(value, bytes):\\n        return ByteTypeSerializer(value)\\n    if isinstance(value, str):\\n        return TextTypeSerializer(value)\\n    if isinstance(value, decimal.Decimal):\\n        return DecimalSerializer(value)\\n    if isinstance(value, functools.partial):\\n        return FunctoolsPartialSerializer(value)\\n    if isinstance(value, (types.FunctionType, types.BuiltinFunctionType, types.MethodType)):\\n        return FunctionTypeSerializer(value)\\n    if isinstance(value, collections.Iterable):\\n        return IterableSerializer(value)\\n    if isinstance(value, (COMPILED_REGEX_TYPE, RegexObject)):\\n        return RegexSerializer(value)\\n    if isinstance(value, uuid.UUID):\\n        return UUIDSerializer(value)\\n    raise ValueError(\\n        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\\n        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\\n        \"topics/migrations/\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def serializer_factory(value):\\n    from django.db.migrations.writer import SettingsReference\\n    if isinstance(value, Promise):\\n        value = force_text(value)\\n    elif isinstance(value, LazyObject):\\n        value = value.__reduce__()[1][0]\\n    if isinstance(value, models.Field):\\n        return ModelFieldSerializer(value)\\n    if isinstance(value, models.manager.BaseManager):\\n        return ModelManagerSerializer(value)\\n    if isinstance(value, Operation):\\n        return OperationSerializer(value)\\n    if isinstance(value, type):\\n        return TypeSerializer(value)\\n    if hasattr(value, 'deconstruct'):\\n        return DeconstructableSerializer(value)\\n    if isinstance(value, frozenset):\\n        return FrozensetSerializer(value)\\n    if isinstance(value, list):\\n        return SequenceSerializer(value)\\n    if isinstance(value, set):\\n        return SetSerializer(value)\\n    if isinstance(value, tuple):\\n        return TupleSerializer(value)\\n    if isinstance(value, dict):\\n        return DictionarySerializer(value)\\n    if isinstance(value, enum.Enum):\\n        return EnumSerializer(value)\\n    if isinstance(value, datetime.datetime):\\n        return DatetimeSerializer(value)\\n    if isinstance(value, datetime.date):\\n        return DateSerializer(value)\\n    if isinstance(value, datetime.time):\\n        return TimeSerializer(value)\\n    if isinstance(value, datetime.timedelta):\\n        return TimedeltaSerializer(value)\\n    if isinstance(value, SettingsReference):\\n        return SettingsReferenceSerializer(value)\\n    if isinstance(value, float):\\n        return FloatSerializer(value)\\n    if isinstance(value, (bool, int, type(None))):\\n        return BaseSimpleSerializer(value)\\n    if isinstance(value, bytes):\\n        return ByteTypeSerializer(value)\\n    if isinstance(value, str):\\n        return TextTypeSerializer(value)\\n    if isinstance(value, decimal.Decimal):\\n        return DecimalSerializer(value)\\n    if isinstance(value, functools.partial):\\n        return FunctoolsPartialSerializer(value)\\n    if isinstance(value, (types.FunctionType, types.BuiltinFunctionType, types.MethodType)):\\n        return FunctionTypeSerializer(value)\\n    if isinstance(value, collections.Iterable):\\n        return IterableSerializer(value)\\n    if isinstance(value, (COMPILED_REGEX_TYPE, RegexObject)):\\n        return RegexSerializer(value)\\n    if isinstance(value, uuid.UUID):\\n        return UUIDSerializer(value)\\n    raise ValueError(\\n        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\\n        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\\n        \"topics/migrations/\\n    )"
  },
  {
    "code": "def __init__(self, module):\\n        super(VmAttributeDefManager, self).__init__(module)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, module):\\n        super(VmAttributeDefManager, self).__init__(module)"
  },
  {
    "code": "def _homogenize_series(data, index, dtype=None):\\n    homogenized = {}\\n    for k, v in data.iteritems():\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index)\\n        else:\\n            if isinstance(v, dict):\\n                v = [v.get(i, nan) for i in index]\\n            elif np.isscalar(v):\\n                _v = np.empty(len(index), dtype=_infer_dtype(v))\\n                _v.fill(v)\\n                v = _v\\n            else:\\n                assert(len(v) == len(index))\\n            try:\\n                v = Series(v, dtype=dtype, index=index)\\n            except Exception:\\n                v = Series(v, index=index)\\n        homogenized[k] = v\\n    return homogenized",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: DataFrame.__init__ will accept structured arrays. general performance tweak in constructor too",
    "fixed_code": "def _homogenize_series(data, index, dtype=None):\\n    homogenized = {}\\n    for k, v in data.iteritems():\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index)\\n        else:\\n            if isinstance(v, dict):\\n                v = [v.get(i, nan) for i in index]\\n            elif np.isscalar(v):\\n                _v = np.empty(len(index), dtype=_infer_dtype(v))\\n                _v.fill(v)\\n                v = _v\\n            else:\\n                assert(len(v) == len(index))\\n            try:\\n                v = np.asarray(v, dtype=dtype)\\n            except Exception:\\n                v = np.asarray(v)\\n        homogenized[k] = v\\n    return homogenized"
  },
  {
    "code": "def _consolidate_inplace(self):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _consolidate_inplace(self):"
  },
  {
    "code": "def as_json(self, escape_html=False):\\n\\t\\terrors = {f: json.loads(e.as_json(escape_html=escape_html)) for f, e in self.items()}\\n\\t\\treturn json.dumps(errors)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_json(self, escape_html=False):\\n\\t\\terrors = {f: json.loads(e.as_json(escape_html=escape_html)) for f, e in self.items()}\\n\\t\\treturn json.dumps(errors)"
  },
  {
    "code": "def fetch_openml(name=None, version='active', data_id=None, data_home=None,\\n                 target_column='default-target', cache=True):\\n    data_home = get_data_home(data_home=data_home)\\n    data_home = join(data_home, 'openml')\\n    if cache is False:\\n        data_home = None\\n    if name is not None:\\n        name = name.lower()\\n        if data_id is not None:\\n            raise ValueError(\\n                \"Dataset data_id={} and name={} passed, but you can only \"\\n                \"specify a numeric data_id or a name, not \"\\n                \"both.\".format(data_id, name))\\n        data_info = _get_data_info_by_name(name, version, data_home)\\n        data_id = data_info['did']\\n    elif data_id is not None:\\n        if version is not \"active\":\\n            raise ValueError(\\n                \"Dataset data_id={} and version={} passed, but you can only \"\\n                \"specify a numeric data_id or a version, not \"\\n                \"both.\".format(data_id, name))\\n    else:\\n        raise ValueError(\\n            \"Neither name nor data_id are provided. Please provide name or \"\\n            \"data_id.\")\\n    data_description = _get_data_description_by_id(data_id, data_home)\\n    if data_description['status'] != \"active\":\\n        warn(\"Version {} of dataset {} is inactive, meaning that issues have \"\\n             \"been found in the dataset. Try using a newer version from \"\\n             \"this URL: {}\".format(\\n                data_description['version'],\\n                data_description['name'],\\n                data_description['url']))\\n    features_list = _get_data_features(data_id, data_home)\\n    for feature in features_list:\\n        if 'true' in (feature['is_ignore'], feature['is_row_identifier']):\\n            continue\\n        if feature['data_type'] == 'string':\\n            raise ValueError('STRING attributes are not yet supported')\\n    if target_column == \"default-target\":\\n        target_column = [feature['name'] for feature in features_list\\n                         if feature['is_target'] == 'true']\\n    elif isinstance(target_column, string_types):\\n        target_column = [target_column]\\n    elif target_column is None:\\n        target_column = []\\n    elif not isinstance(target_column, list):\\n        raise TypeError(\"Did not recognize type of target_column\"\\n                        \"Should be six.string_type, list or None. Got: \"\\n                        \"{}\".format(type(target_column)))\\n    data_columns = [feature['name'] for feature in features_list\\n                    if (feature['name'] not in target_column and\\n                        feature['is_ignore'] != 'true' and\\n                        feature['is_row_identifier'] != 'true')]\\n    features_dict = {feature['name']: feature for feature in features_list}\\n    _verify_target_data_type(features_dict, target_column)\\n    col_slice_y = [int(features_dict[col_name]['index'])\\n                   for col_name in target_column]\\n    col_slice_x = [int(features_dict[col_name]['index'])\\n                   for col_name in data_columns]\\n    for col_idx in col_slice_y:\\n        feat = features_list[col_idx]\\n        nr_missing = int(feat['number_of_missing_values'])\\n        if nr_missing > 0:\\n            raise ValueError('Target column {} has {} missing values. '\\n                             'Missing values are not supported for target '\\n                             'columns. '.format(feat['name'], nr_missing))\\n    return_sparse = False\\n    if data_description['format'].lower() == 'sparse_arff':\\n        return_sparse = True\\n    arff = _download_data_arff(data_description['file_id'], return_sparse,\\n                               data_home)\\n    arff_data = arff['data']\\n    nominal_attributes = {k: v for k, v in arff['attributes']\\n                          if isinstance(v, list)}\\n    for feature in features_list:\\n        if 'true' in (feature['is_row_identifier'],\\n                      feature['is_ignore']) and (feature['name'] not in\\n                                                 target_column):\\n            del nominal_attributes[feature['name']]\\n    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)\\n    is_classification = {col_name in nominal_attributes\\n                         for col_name in target_column}\\n    if not is_classification:\\n        pass\\n    elif all(is_classification):\\n        y = np.hstack([np.take(np.asarray(nominal_attributes.pop(col_name),\\n                                          dtype='O'),\\n                               y[:, i:i+1].astype(int))\\n                       for i, col_name in enumerate(target_column)])\\n    elif any(is_classification):\\n        raise ValueError('Mix of nominal and non-nominal targets is not '\\n                         'currently supported')\\n    description = u\"{}\\nDownloaded from openml.org.\".format(\\n        data_description.pop('description'))\\n    if y.shape[1] == 1:\\n        y = y.reshape((-1,))\\n    elif y.shape[1] == 0:\\n        y = None\\n    bunch = Bunch(\\n        data=X, target=y, feature_names=data_columns,\\n        DESCR=description, details=data_description,\\n        categories=nominal_attributes,\\n        url=\"https://www.openml.org/d/{}\".format(data_id))\\n    return bunch",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH fetch_openml should support return_X_y (#11840)",
    "fixed_code": "def fetch_openml(name=None, version='active', data_id=None, data_home=None,\\n                 target_column='default-target', cache=True, return_X_y=False):\\n    data_home = get_data_home(data_home=data_home)\\n    data_home = join(data_home, 'openml')\\n    if cache is False:\\n        data_home = None\\n    if name is not None:\\n        name = name.lower()\\n        if data_id is not None:\\n            raise ValueError(\\n                \"Dataset data_id={} and name={} passed, but you can only \"\\n                \"specify a numeric data_id or a name, not \"\\n                \"both.\".format(data_id, name))\\n        data_info = _get_data_info_by_name(name, version, data_home)\\n        data_id = data_info['did']\\n    elif data_id is not None:\\n        if version is not \"active\":\\n            raise ValueError(\\n                \"Dataset data_id={} and version={} passed, but you can only \"\\n                \"specify a numeric data_id or a version, not \"\\n                \"both.\".format(data_id, name))\\n    else:\\n        raise ValueError(\\n            \"Neither name nor data_id are provided. Please provide name or \"\\n            \"data_id.\")\\n    data_description = _get_data_description_by_id(data_id, data_home)\\n    if data_description['status'] != \"active\":\\n        warn(\"Version {} of dataset {} is inactive, meaning that issues have \"\\n             \"been found in the dataset. Try using a newer version from \"\\n             \"this URL: {}\".format(\\n                data_description['version'],\\n                data_description['name'],\\n                data_description['url']))\\n    features_list = _get_data_features(data_id, data_home)\\n    for feature in features_list:\\n        if 'true' in (feature['is_ignore'], feature['is_row_identifier']):\\n            continue\\n        if feature['data_type'] == 'string':\\n            raise ValueError('STRING attributes are not yet supported')\\n    if target_column == \"default-target\":\\n        target_column = [feature['name'] for feature in features_list\\n                         if feature['is_target'] == 'true']\\n    elif isinstance(target_column, string_types):\\n        target_column = [target_column]\\n    elif target_column is None:\\n        target_column = []\\n    elif not isinstance(target_column, list):\\n        raise TypeError(\"Did not recognize type of target_column\"\\n                        \"Should be six.string_type, list or None. Got: \"\\n                        \"{}\".format(type(target_column)))\\n    data_columns = [feature['name'] for feature in features_list\\n                    if (feature['name'] not in target_column and\\n                        feature['is_ignore'] != 'true' and\\n                        feature['is_row_identifier'] != 'true')]\\n    features_dict = {feature['name']: feature for feature in features_list}\\n    _verify_target_data_type(features_dict, target_column)\\n    col_slice_y = [int(features_dict[col_name]['index'])\\n                   for col_name in target_column]\\n    col_slice_x = [int(features_dict[col_name]['index'])\\n                   for col_name in data_columns]\\n    for col_idx in col_slice_y:\\n        feat = features_list[col_idx]\\n        nr_missing = int(feat['number_of_missing_values'])\\n        if nr_missing > 0:\\n            raise ValueError('Target column {} has {} missing values. '\\n                             'Missing values are not supported for target '\\n                             'columns. '.format(feat['name'], nr_missing))\\n    return_sparse = False\\n    if data_description['format'].lower() == 'sparse_arff':\\n        return_sparse = True\\n    arff = _download_data_arff(data_description['file_id'], return_sparse,\\n                               data_home)\\n    arff_data = arff['data']\\n    nominal_attributes = {k: v for k, v in arff['attributes']\\n                          if isinstance(v, list)}\\n    for feature in features_list:\\n        if 'true' in (feature['is_row_identifier'],\\n                      feature['is_ignore']) and (feature['name'] not in\\n                                                 target_column):\\n            del nominal_attributes[feature['name']]\\n    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)\\n    is_classification = {col_name in nominal_attributes\\n                         for col_name in target_column}\\n    if not is_classification:\\n        pass\\n    elif all(is_classification):\\n        y = np.hstack([np.take(np.asarray(nominal_attributes.pop(col_name),\\n                                          dtype='O'),\\n                               y[:, i:i+1].astype(int))\\n                       for i, col_name in enumerate(target_column)])\\n    elif any(is_classification):\\n        raise ValueError('Mix of nominal and non-nominal targets is not '\\n                         'currently supported')\\n    description = u\"{}\\nDownloaded from openml.org.\".format(\\n        data_description.pop('description'))\\n    if y.shape[1] == 1:\\n        y = y.reshape((-1,))\\n    elif y.shape[1] == 0:\\n        y = None\\n    if return_X_y:\\n        return X, y\\n    bunch = Bunch(\\n        data=X, target=y, feature_names=data_columns,\\n        DESCR=description, details=data_description,\\n        categories=nominal_attributes,\\n        url=\"https://www.openml.org/d/{}\".format(data_id))\\n    return bunch"
  },
  {
    "code": "def mktime_tz(data):\\n    if data[9] is None:\\n        return time.mktime(data[:8] + (-1,))\\n    else:\\n        t = calendar.timegm(data)\\n        return t - data[9]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mktime_tz(data):\\n    if data[9] is None:\\n        return time.mktime(data[:8] + (-1,))\\n    else:\\n        t = calendar.timegm(data)\\n        return t - data[9]"
  },
  {
    "code": "def destroy_firewall_policy(self):\\n        net_firewall_policy_obj = netapp_utils.zapi.NaElement(\"net-firewall-policy-destroy\")\\n        net_firewall_policy_obj.add_new_child('policy', self.parameters['policy'])\\n        net_firewall_policy_obj.add_new_child('service', self.parameters['service'])\\n        net_firewall_policy_obj.add_new_child('vserver', self.parameters['vserver'])\\n        try:\\n            self.server.invoke_successfully(net_firewall_policy_obj, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg=\"Error destroying Firewall Policy: %s\" % (to_native(error)), exception=traceback.format_exc())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Update to na_ontap_firewall_policy (#51976)",
    "fixed_code": "def destroy_firewall_policy(self):\\n        net_firewall_policy_obj = netapp_utils.zapi.NaElement(\"net-firewall-policy-destroy\")\\n        net_firewall_policy_obj.translate_struct(self.firewall_policy_attributes())\\n        try:\\n            self.server.invoke_successfully(net_firewall_policy_obj, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg=\"Error destroying Firewall Policy: %s\" % (to_native(error)), exception=traceback.format_exc())"
  },
  {
    "code": "def corr(self):\\n        cols = self._get_numeric_columns()\\n        mat = self.as_matrix(cols).T\\n        baseCov = np.cov(mat)\\n        sigma = np.sqrt(np.diag(baseCov))\\n        correl = baseCov / np.outer(sigma, sigma)\\n        for i, j, ac, bc in self._cov_helper(mat):\\n            c = np.corrcoef(ac, bc)[0, 1]\\n            correl[i, j] = c\\n            correl[j, i] = c\\n        return self._constructor(correl, index=cols, columns=cols)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add kendall/spearman correlation methods, GH #428",
    "fixed_code": "def corr(self, method='pearson'):\\n        cols = self._get_numeric_columns()\\n        mat = self.as_matrix(cols).T\\n        corrf = nanops.get_corr_func(method)\\n        K = len(cols)\\n        correl = np.empty((K, K), dtype=float)\\n        mask = np.isfinite(mat)\\n        for i, ac in enumerate(mat):\\n            for j, bc  in enumerate(mat):\\n                valid = mask[i] & mask[j]\\n                if not valid.all():\\n                    c = corrf(ac[valid], bc[valid])\\n                else:\\n                    c = corrf(ac, bc)\\n                correl[i, j] = c\\n                correl[j, i] = c\\n        return self._constructor(correl, index=cols, columns=cols)"
  },
  {
    "code": "def cmd_async(self, low):\\n\\t\\tfun': 'key.finger',\\n\\t\\t\\t\\t'match': 'jerry',\\n\\t\\t\\t\\t'eauth': 'auto',\\n\\t\\t\\t\\t'username': 'saltdev',\\n\\t\\t\\t\\t'password': 'saltdev',\\n\\t\\t\\t})\\n\\t\\t\\t{'jid': '20131219224744416681', 'tag': 'salt/wheel/20131219224744416681'}\\n\\t\\tfun')\\n\\t\\treturn self.asynchronous(fun, low)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Tests and fix for CVE-2021-25281",
    "fixed_code": "def cmd_async(self, low):\\n\\t\\tfun': 'key.finger',\\n\\t\\t\\t\\t'match': 'jerry',\\n\\t\\t\\t\\t'eauth': 'auto',\\n\\t\\t\\t\\t'username': 'saltdev',\\n\\t\\t\\t\\t'password': 'saltdev',\\n\\t\\t\\t})\\n\\t\\t\\t{'jid': '20131219224744416681', 'tag': 'salt/wheel/20131219224744416681'}\\n\\t\\tfun')\\n\\t\\treturn self.asynchronous(fun, low)"
  },
  {
    "code": "def normalize_invisible_parens(\\n    node: Node, parens_after: Set[str], *, preview: bool\\n) -> None:\\n    for pc in list_comments(node.prefix, is_endmarker=False, preview=preview):\\n        if pc.value in FMT_OFF:\\n            return\\n    check_lpar = False\\n    for index, child in enumerate(list(node.children)):\\n        if isinstance(child, Node) and child.type == syms.annassign:\\n            normalize_invisible_parens(\\n                child, parens_after=parens_after, preview=preview\\n            )\\n        if (\\n            index == 0\\n            and isinstance(child, Node)\\n            and child.type == syms.testlist_star_expr\\n        ):\\n            check_lpar = True\\n        if check_lpar:\\n            if child.type == syms.atom:\\n                if maybe_make_parens_invisible_in_atom(\\n                    child,\\n                    parent=node,\\n                    preview=preview,\\n                ):\\n                    wrap_in_parentheses(node, child, visible=False)\\n            elif is_one_tuple(child):\\n                wrap_in_parentheses(node, child, visible=True)\\n            elif node.type == syms.import_from:\\n                if is_lpar_token(child):\\n                    assert is_rpar_token(node.children[-1])\\n                    child.value = \"\"\\n                    node.children[-1].value = \"\"\\n                elif child.type != token.STAR:\\n                    node.insert_child(index, Leaf(token.LPAR, \"\"))\\n                    node.append_child(Leaf(token.RPAR, \"\"))\\n                break\\n            elif not (isinstance(child, Leaf) and is_multiline_string(child)):\\n                wrap_in_parentheses(node, child, visible=False)\\n        check_lpar = isinstance(child, Leaf) and child.value in parens_after",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove unnecessary parentheses from `with` statements (#2926)",
    "fixed_code": "def normalize_invisible_parens(\\n    node: Node, parens_after: Set[str], *, preview: bool\\n) -> None:\\n    for pc in list_comments(node.prefix, is_endmarker=False, preview=preview):\\n        if pc.value in FMT_OFF:\\n            return\\n    check_lpar = False\\n    for index, child in enumerate(list(node.children)):\\n        if isinstance(child, Node) and child.type == syms.annassign:\\n            normalize_invisible_parens(\\n                child, parens_after=parens_after, preview=preview\\n            )\\n        if (\\n            index == 0\\n            and isinstance(child, Node)\\n            and child.type == syms.testlist_star_expr\\n        ):\\n            check_lpar = True\\n        if check_lpar:\\n            if (\\n                preview\\n                and child.type == syms.atom\\n                and node.type == syms.for_stmt\\n                and isinstance(child.prev_sibling, Leaf)\\n                and child.prev_sibling.type == token.NAME\\n                and child.prev_sibling.value == \"for\"\\n            ):\\n                if maybe_make_parens_invisible_in_atom(\\n                    child,\\n                    parent=node,\\n                    remove_brackets_around_comma=True,\\n                ):\\n                    wrap_in_parentheses(node, child, visible=False)\\n            elif preview and isinstance(child, Node) and node.type == syms.with_stmt:\\n                remove_with_parens(child, node)\\n            elif child.type == syms.atom:\\n                if maybe_make_parens_invisible_in_atom(\\n                    child,\\n                    parent=node,\\n                ):\\n                    wrap_in_parentheses(node, child, visible=False)\\n            elif is_one_tuple(child):\\n                wrap_in_parentheses(node, child, visible=True)\\n            elif node.type == syms.import_from:\\n                if is_lpar_token(child):\\n                    assert is_rpar_token(node.children[-1])\\n                    child.value = \"\"\\n                    node.children[-1].value = \"\"\\n                elif child.type != token.STAR:\\n                    node.insert_child(index, Leaf(token.LPAR, \"\"))\\n                    node.append_child(Leaf(token.RPAR, \"\"))\\n                break\\n            elif not (isinstance(child, Leaf) and is_multiline_string(child)):\\n                wrap_in_parentheses(node, child, visible=False)\\n        comma_check = child.type == token.COMMA if preview else False\\n        check_lpar = isinstance(child, Leaf) and (\\n            child.value in parens_after or comma_check\\n        )"
  },
  {
    "code": "def _cursor(self):\\n        cursor = None\\n        if not self._valid_connection():\\n            conn_string = self._connect_string()\\n            self.connection = Database.connect(conn_string, **self.settings_dict['DATABASE_OPTIONS'])\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n            cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS' \"\\n                           \"NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF' \"\\n                           \"NLS_TERRITORY = 'AMERICA'\")\\n            try:\\n                self.oracle_version = int(self.connection.version.split('.')[0])\\n                if self.oracle_version <= 9:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_9\\n                else:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_10\\n            except ValueError:\\n                pass\\n            try:\\n                self.connection.stmtcachesize = 20\\n            except:\\n                pass\\n            connection_created.send(sender=self.__class__)\\n        if not cursor:\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n        return cursor",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #10566: Added support for cx_Oracle compiled with the WITH_UNICODE flag.",
    "fixed_code": "def _cursor(self):\\n        cursor = None\\n        if not self._valid_connection():\\n            conn_string = convert_unicode(self._connect_string())\\n            self.connection = Database.connect(conn_string, **self.settings_dict['DATABASE_OPTIONS'])\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n            cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS' \"\\n                           \"NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF' \"\\n                           \"NLS_TERRITORY = 'AMERICA'\")\\n            try:\\n                self.oracle_version = int(self.connection.version.split('.')[0])\\n                if self.oracle_version <= 9:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_9\\n                else:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_10\\n            except ValueError:\\n                pass\\n            try:\\n                self.connection.stmtcachesize = 20\\n            except:\\n                pass\\n            connection_created.send(sender=self.__class__)\\n        if not cursor:\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n        return cursor"
  },
  {
    "code": "def insert_statement(self):\\n        names = list(map(str, self.frame.columns))\\n        wld = \"?\"  \\n        escape = _get_valid_sqlite_name\\n        if self.index is not None:\\n            for idx in self.index[::-1]:\\n                names.insert(0, idx)\\n        bracketed_names = [escape(column) for column in names]\\n        col_names = \",\".join(bracketed_names)\\n        wildcards = \",\".join([wld] * len(names))\\n        insert_statement = (\\n            f\"INSERT INTO {escape(self.name)} ({col_names}) VALUES ({wildcards})\"\\n        )\\n        return insert_statement",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Support multi row inserts in to_sql when using the sqlite fallback (#30743)",
    "fixed_code": "def insert_statement(self, *, num_rows):\\n        names = list(map(str, self.frame.columns))\\n        wld = \"?\"  \\n        escape = _get_valid_sqlite_name\\n        if self.index is not None:\\n            for idx in self.index[::-1]:\\n                names.insert(0, idx)\\n        bracketed_names = [escape(column) for column in names]\\n        col_names = \",\".join(bracketed_names)\\n        row_wildcards = \",\".join([wld] * len(names))\\n        wildcards = \",\".join(f\"({row_wildcards})\" for _ in range(num_rows))\\n        insert_statement = (\\n            f\"INSERT INTO {escape(self.name)} ({col_names}) VALUES {wildcards}\"\\n        )\\n        return insert_statement"
  },
  {
    "code": "def reset_printoptions():\\n    global GlobalPrintConfig\\n    GlobalPrintConfig = _GlobalPrintConfig()\\nclass EngFormatter(object):\\n    ENG_PREFIXES = {\\n        -24: \"y\",\\n        -21: \"z\",\\n        -18: \"a\",\\n        -15: \"f\",\\n        -12: \"p\",\\n         -9: \"n\",\\n         -6: \"u\",\\n         -3: \"m\",\\n          0: \"\",\\n          3: \"k\",\\n          6: \"M\",\\n          9: \"G\",\\n         12: \"T\",\\n         15: \"P\",\\n         18: \"E\",\\n         21: \"Z\",\\n         24: \"Y\"\\n      }",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: common.py cleanup, test coverage",
    "fixed_code": "def reset_printoptions():\\n    global GlobalPrintConfig\\n    GlobalPrintConfig.reset()\\nclass EngFormatter(object):\\n    ENG_PREFIXES = {\\n        -24: \"y\",\\n        -21: \"z\",\\n        -18: \"a\",\\n        -15: \"f\",\\n        -12: \"p\",\\n         -9: \"n\",\\n         -6: \"u\",\\n         -3: \"m\",\\n          0: \"\",\\n          3: \"k\",\\n          6: \"M\",\\n          9: \"G\",\\n         12: \"T\",\\n         15: \"P\",\\n         18: \"E\",\\n         21: \"Z\",\\n         24: \"Y\"\\n      }"
  },
  {
    "code": "def create_snapshot_policy(self):\\n        self.validate_parameters()\\n        options = {'policy': self.parameters['name'],\\n                   'enabled': str(self.parameters['enabled']),\\n                   }\\n        positions = [str(i) for i in range(1, len(self.parameters['schedule']) + 1)]\\n        for schedule, count, position in zip(self.parameters['schedule'], self.parameters['count'], positions):\\n            options['count' + position] = str(count)\\n            options['schedule' + position] = schedule\\n        snapshot_obj = netapp_utils.zapi.NaElement.create_node_with_children('snapshot-policy-create', **options)\\n        if self.parameters.get('comment'):\\n            snapshot_obj.add_new_child(\"comment\", self.parameters['comment'])\\n        try:\\n            self.server.invoke_successfully(snapshot_obj, True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error creating snapshot policy %s: %s' %\\n                                  (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "add modify to snapshot policy (#59149)",
    "fixed_code": "def create_snapshot_policy(self):\\n        self.validate_parameters()\\n        options = {'policy': self.parameters['name'],\\n                   'enabled': str(self.parameters['enabled']),\\n                   }\\n        if 'snapmirror_label' in self.parameters:\\n            snapmirror_labels = self.parameters['snapmirror_label']\\n        else:\\n            snapmirror_labels = [None] * len(self.parameters['schedule'])\\n        positions = [str(i) for i in range(1, len(self.parameters['schedule']) + 1)]\\n        for schedule, count, snapmirror_label, position in zip(self.parameters['schedule'], self.parameters['count'], snapmirror_labels, positions):\\n            schedule = schedule.strip()\\n            options['count' + position] = str(count)\\n            options['schedule' + position] = schedule\\n            if snapmirror_label is not None:\\n                snapmirror_label = snapmirror_label.strip()\\n                if snapmirror_label != '':\\n                    options['snapmirror-label' + position] = snapmirror_label\\n        snapshot_obj = netapp_utils.zapi.NaElement.create_node_with_children('snapshot-policy-create', **options)\\n        if self.parameters.get('comment'):\\n            snapshot_obj.add_new_child(\"comment\", self.parameters['comment'])\\n        try:\\n            self.server.invoke_successfully(snapshot_obj, True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error creating snapshot policy %s: %s' %\\n                                  (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def map_obj_to_commands(updates, module):\\n\\tcommands = list()\\n\\twant, have = updates\\n\\tstate = module.params['state']\\n\\tif state == 'absent' and have.get('text'):\\n\\t\\tif isinstance(have['text'], str):\\n\\t\\t\\tcommands.append('no banner %s' % module.params['banner'])\\n\\t\\telif have['text'].get('loginBanner') or have['text'].get('motd'):\\n\\t\\t\\tcommands.append({'cmd': 'no banner %s' % module.params['banner']})\\n\\telif state == 'present':\\n\\t\\tif isinstance(have['text'], string_types):\\n\\t\\t\\tif want['text'] != have['text']:\\n\\t\\t\\t\\tcommands.append('banner %s' % module.params['banner'])\\n\\t\\t\\t\\tcommands.extend(want['text'].strip().split('\\n'))\\n\\t\\t\\t\\tcommands.append('EOF')\\n\\t\\telse:\\n\\t\\t\\thave_text = have['text'].get('loginBanner') or have['text'].get('motd')\\n\\t\\t\\tif have_text:\\n\\t\\t\\t\\thave_text = have_text.strip()\\n\\t\\t\\tif to_text(want['text']) != have_text or not have_text:\\n\\t\\t\\t\\tcommands.append({'cmd': 'banner %s' % module.params['banner'],\\n\\t\\t\\t\\t\\t\\t\\t\\t 'input': want['text'].strip('\\n')})\\n\\treturn commands",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_obj_to_commands(updates, module):\\n\\tcommands = list()\\n\\twant, have = updates\\n\\tstate = module.params['state']\\n\\tif state == 'absent' and have.get('text'):\\n\\t\\tif isinstance(have['text'], str):\\n\\t\\t\\tcommands.append('no banner %s' % module.params['banner'])\\n\\t\\telif have['text'].get('loginBanner') or have['text'].get('motd'):\\n\\t\\t\\tcommands.append({'cmd': 'no banner %s' % module.params['banner']})\\n\\telif state == 'present':\\n\\t\\tif isinstance(have['text'], string_types):\\n\\t\\t\\tif want['text'] != have['text']:\\n\\t\\t\\t\\tcommands.append('banner %s' % module.params['banner'])\\n\\t\\t\\t\\tcommands.extend(want['text'].strip().split('\\n'))\\n\\t\\t\\t\\tcommands.append('EOF')\\n\\t\\telse:\\n\\t\\t\\thave_text = have['text'].get('loginBanner') or have['text'].get('motd')\\n\\t\\t\\tif have_text:\\n\\t\\t\\t\\thave_text = have_text.strip()\\n\\t\\t\\tif to_text(want['text']) != have_text or not have_text:\\n\\t\\t\\t\\tcommands.append({'cmd': 'banner %s' % module.params['banner'],\\n\\t\\t\\t\\t\\t\\t\\t\\t 'input': want['text'].strip('\\n')})\\n\\treturn commands"
  },
  {
    "code": "def _validate_remainder(self, X):\\n        is_transformer = ((hasattr(self.remainder, \"fit\")\\n                           or hasattr(self.remainder, \"fit_transform\"))\\n                          and hasattr(self.remainder, \"transform\"))\\n        if (self.remainder not in ('drop', 'passthrough')\\n                and not is_transformer):\\n            raise ValueError(\\n                \"The remainder keyword needs to be one of 'drop', \"\\n                \"'passthrough', or estimator. '%s' was passed instead\" %\\n                self.remainder)\\n        self._n_features = X.shape[1]\\n        cols = set(chain(*self._transformer_to_input_indices.values()))\\n        remaining = sorted(set(range(self._n_features)) - cols)\\n        self._remainder = ('remainder', self.remainder, remaining)\\n        self._transformer_to_input_indices['remainder'] = remaining\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _validate_remainder(self, X):\\n        is_transformer = ((hasattr(self.remainder, \"fit\")\\n                           or hasattr(self.remainder, \"fit_transform\"))\\n                          and hasattr(self.remainder, \"transform\"))\\n        if (self.remainder not in ('drop', 'passthrough')\\n                and not is_transformer):\\n            raise ValueError(\\n                \"The remainder keyword needs to be one of 'drop', \"\\n                \"'passthrough', or estimator. '%s' was passed instead\" %\\n                self.remainder)\\n        self._n_features = X.shape[1]\\n        cols = set(chain(*self._transformer_to_input_indices.values()))\\n        remaining = sorted(set(range(self._n_features)) - cols)\\n        self._remainder = ('remainder', self.remainder, remaining)\\n        self._transformer_to_input_indices['remainder'] = remaining\\n    @property"
  },
  {
    "code": "def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\\n                             max_iter=100, tol=1e-4, verbose=0,\\n                             solver='lbfgs', coef=None, copy=True,\\n                             class_weight=None, dual=False, penalty='l2',\\n                             intercept_scaling=1., multi_class='ovr'):\\n    if isinstance(Cs, numbers.Integral):\\n        Cs = np.logspace(-4, 4, Cs)\\n    if multi_class not in ['multinomial', 'ovr']:\\n        raise ValueError(\"multi_class can be either 'multinomial' or 'ovr'\"\\n                         \"got %s\" % multi_class)\\n    if multi_class == 'multinomial' and solver != 'lbfgs':\\n        raise ValueError(\"Solver %s cannot solve problems with \"\\n                         \"a multinomial backend.\" % solver)\\n    X = check_array(X, accept_sparse='csc', dtype=np.float64)\\n    y = check_array(y, ensure_2d=False, copy=copy)\\n    _, n_features = X.shape\\n    check_consistent_length(X, y)\\n    classes = np.unique(y)\\n    if pos_class is None and multi_class != 'multinomial':\\n        if (classes.size > 2):\\n            raise ValueError('To fit OvA, use the pos_class argument')\\n        pos_class = classes[1]\\n    sample_weight = np.ones(X.shape[0])\\n    le = LabelEncoder()\\n    if isinstance(class_weight, dict):\\n        if solver == \"liblinear\":\\n            if classes.size == 2:\\n                temp = {}\\n                temp[1] = class_weight[pos_class]\\n                temp[-1] = class_weight[classes[0]]\\n                class_weight = temp.copy()\\n            else:\\n                raise ValueError(\"In LogisticRegressionCV the liblinear \"\\n                                 \"solver cannot handle multiclass with \"\\n                                 \"class_weight of type dict. Use the lbfgs, \"\\n                                 \"newton-cg solvers or set \"\\n                                 \"class_weight='auto'\")\\n        else:\\n            class_weight_ = compute_class_weight(class_weight, classes, y)\\n            sample_weight = class_weight_[le.fit_transform(y)]\\n    if multi_class == 'ovr':\\n        w0 = np.zeros(n_features + int(fit_intercept))\\n        mask_classes = [-1, 1]\\n        mask = (y == pos_class)\\n        y[mask] = 1\\n        y[~mask] = -1\\n        y = as_float_array(y, copy=False)\\n    else:\\n        lbin = LabelBinarizer()\\n        Y = lbin.fit_transform(y)\\n        if Y.shape[1] == 1:\\n            Y = np.hstack([1 - Y, Y])\\n        w0 = np.zeros((Y.shape[1], n_features + int(fit_intercept)), order='F')\\n        mask_classes = classes\\n    if class_weight == \"auto\":\\n        class_weight_ = compute_class_weight(class_weight, mask_classes, y)\\n        sample_weight = class_weight_[le.fit_transform(y)]\\n    if coef is not None:\\n        if multi_class == 'ovr':\\n            if not coef.size in (n_features, w0.size):\\n                raise ValueError('Initialization coef is not of correct shape')\\n            w0[:coef.size] = coef\\n        else:\\n            if coef.shape[0] != classes.size or coef.shape[1] not in (\\n                n_features, n_features + 1):\\n                raise ValueError('Initialization coef is not of correct shape')\\n            w0[:, :coef.shape[1]] = coef\\n    if multi_class == 'multinomial':\\n        w0 = w0.ravel()\\n    coefs = list()\\n    for C in Cs:\\n        if solver == 'lbfgs':\\n            if multi_class == 'multinomial':\\n                target = Y\\n                func = _multinomial_loss_grad\\n            else:\\n                target = y\\n                func = _logistic_loss_and_grad\\n            try:\\n                w0, loss, info = optimize.fmin_l_bfgs_b(\\n                    func, w0, fprime=None,\\n                    args=(X, target, 1. / C, sample_weight),\\n                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter\\n                    )\\n            except TypeError:\\n                w0, loss, info = optimize.fmin_l_bfgs_b(\\n                    func, w0, fprime=None,\\n                    args=(X, target, 1. / C, sample_weight),\\n                    iprint=(verbose > 0) - 1, pgtol=tol\\n                    )\\n            if info[\"warnflag\"] == 1:\\n                warnings.warn(\"lbfgs failed to converge. Increase the number \"\\n                              \"of iterations.\")\\n        elif solver == 'newton-cg':\\n            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]\\n            w0 = newton_cg(_logistic_loss_grad_hess, _logistic_loss, grad, w0,\\n                           args=(X, y, 1. / C, sample_weight),\\n                           maxiter=max_iter, tol=tol)\\n        elif solver == 'liblinear':\\n            lr = LogisticRegression(C=C, fit_intercept=fit_intercept, tol=tol,\\n                                    class_weight=class_weight, dual=dual,\\n                                    penalty=penalty,\\n                                    intercept_scaling=intercept_scaling)\\n            lr.fit(X, y)\\n            if fit_intercept:\\n                w0 = np.concatenate([lr.coef_.ravel(), lr.intercept_])\\n            else:\\n                w0 = lr.coef_.ravel()\\n        else:\\n            raise ValueError(\"solver must be one of {'liblinear', 'lbfgs', \"\\n                             \"'newton-cg'}, got '%s' instead\" % solver)\\n        if multi_class == 'multinomial':\\n            multi_w0 = np.reshape(w0, (classes.size, -1))\\n            if classes.size == 2:\\n                multi_w0 = multi_w0[1][np.newaxis, :]\\n            coefs.append(multi_w0)\\n        else:\\n            coefs.append(w0)\\n    return coefs, np.array(Cs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: Tests for multinomial logistic regression",
    "fixed_code": "def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\\n                             max_iter=100, tol=1e-4, verbose=0,\\n                             solver='lbfgs', coef=None, copy=True,\\n                             class_weight=None, dual=False, penalty='l2',\\n                             intercept_scaling=1., multi_class='ovr'):\\n    if isinstance(Cs, numbers.Integral):\\n        Cs = np.logspace(-4, 4, Cs)\\n    if multi_class not in ['multinomial', 'ovr']:\\n        raise ValueError(\"multi_class can be either 'multinomial' or 'ovr'\"\\n                         \"got %s\" % multi_class)\\n    if multi_class == 'multinomial' and solver != 'lbfgs':\\n        raise ValueError(\"Solver %s cannot solve problems with \"\\n                         \"a multinomial backend.\" % solver)\\n    X = check_array(X, accept_sparse='csc', dtype=np.float64)\\n    y = check_array(y, ensure_2d=False, copy=copy)\\n    _, n_features = X.shape\\n    check_consistent_length(X, y)\\n    classes = np.unique(y)\\n    if pos_class is None and multi_class != 'multinomial':\\n        if (classes.size > 2):\\n            raise ValueError('To fit OvA, use the pos_class argument')\\n        pos_class = classes[1]\\n    sample_weight = np.ones(X.shape[0])\\n    le = LabelEncoder()\\n    if isinstance(class_weight, dict):\\n        if solver == \"liblinear\":\\n            if classes.size == 2:\\n                temp = {1: class_weight[pos_class],\\n                        -1: class_weight[classes[0]]}\\n                class_weight = temp.copy()\\n            else:\\n                raise ValueError(\"In LogisticRegressionCV the liblinear \"\\n                                 \"solver cannot handle multiclass with \"\\n                                 \"class_weight of type dict. Use the lbfgs, \"\\n                                 \"newton-cg solvers or set \"\\n                                 \"class_weight='auto'\")\\n        else:\\n            class_weight_ = compute_class_weight(class_weight, classes, y)\\n            sample_weight = class_weight_[le.fit_transform(y)]\\n    if multi_class == 'ovr':\\n        w0 = np.zeros(n_features + int(fit_intercept))\\n        mask_classes = [-1, 1]\\n        mask = (y == pos_class)\\n        y[mask] = 1\\n        y[~mask] = -1\\n        y = as_float_array(y, copy=False)\\n    else:\\n        lbin = LabelBinarizer()\\n        Y = lbin.fit_transform(y)\\n        if Y.shape[1] == 1:\\n            Y = np.hstack([1 - Y, Y])\\n        w0 = np.zeros((Y.shape[1], n_features + int(fit_intercept)), order='F')\\n        mask_classes = classes\\n    if class_weight == \"auto\":\\n        class_weight_ = compute_class_weight(class_weight, mask_classes, y)\\n        sample_weight = class_weight_[le.fit_transform(y)]\\n    if coef is not None:\\n        if multi_class == 'ovr':\\n            if not coef.size in (n_features, w0.size):\\n                raise ValueError('Initialization coef is not of correct shape')\\n            w0[:coef.size] = coef\\n        else:\\n            if coef.shape[0] != classes.size or coef.shape[1] not in (\\n                n_features, n_features + 1):\\n                raise ValueError('Initialization coef is not of correct shape')\\n            w0[:, :coef.shape[1]] = coef\\n    if multi_class == 'multinomial':\\n        w0 = w0.ravel()\\n    coefs = list()\\n    for C in Cs:\\n        if solver == 'lbfgs':\\n            if multi_class == 'multinomial':\\n                target = Y\\n                func = _multinomial_loss_grad\\n            else:\\n                target = y\\n                func = _logistic_loss_and_grad\\n            try:\\n                w0, loss, info = optimize.fmin_l_bfgs_b(\\n                    func, w0, fprime=None,\\n                    args=(X, target, 1. / C, sample_weight),\\n                    iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter\\n                    )\\n            except TypeError:\\n                w0, loss, info = optimize.fmin_l_bfgs_b(\\n                    func, w0, fprime=None,\\n                    args=(X, target, 1. / C, sample_weight),\\n                    iprint=(verbose > 0) - 1, pgtol=tol\\n                    )\\n            if info[\"warnflag\"] == 1:\\n                warnings.warn(\"lbfgs failed to converge. Increase the number \"\\n                              \"of iterations.\")\\n        elif solver == 'newton-cg':\\n            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]\\n            w0 = newton_cg(_logistic_loss_grad_hess, _logistic_loss, grad, w0,\\n                           args=(X, y, 1. / C, sample_weight),\\n                           maxiter=max_iter, tol=tol)\\n        elif solver == 'liblinear':\\n            lr = LogisticRegression(C=C, fit_intercept=fit_intercept, tol=tol,\\n                                    class_weight=class_weight, dual=dual,\\n                                    penalty=penalty,\\n                                    intercept_scaling=intercept_scaling)\\n            lr.fit(X, y)\\n            if fit_intercept:\\n                w0 = np.concatenate([lr.coef_.ravel(), lr.intercept_])\\n            else:\\n                w0 = lr.coef_.ravel()\\n        else:\\n            raise ValueError(\"solver must be one of {'liblinear', 'lbfgs', \"\\n                             \"'newton-cg'}, got '%s' instead\" % solver)\\n        if multi_class == 'multinomial':\\n            multi_w0 = np.reshape(w0, (classes.size, -1))\\n            if classes.size == 2:\\n                multi_w0 = multi_w0[1][np.newaxis, :]\\n            coefs.append(multi_w0)\\n        else:\\n            coefs.append(w0)\\n    return coefs, np.array(Cs)"
  },
  {
    "code": "def is_updated_after(self, bucket_name, object_name, ts):\\n        blob_update_time = self.get_blob_update_time(bucket_name, object_name)\\n        if blob_update_time is not None:\\n            import dateutil.tz\\n            if not ts.tzinfo:\\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\\n            if blob_update_time > ts:\\n                return True\\n        return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_updated_after(self, bucket_name, object_name, ts):\\n        blob_update_time = self.get_blob_update_time(bucket_name, object_name)\\n        if blob_update_time is not None:\\n            import dateutil.tz\\n            if not ts.tzinfo:\\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\\n            if blob_update_time > ts:\\n                return True\\n        return False"
  },
  {
    "code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\\n    fields = {}\\n    if cls.__module__ in sys.modules:\\n        globals = sys.modules[cls.__module__].__dict__\\n    else:\\n        globals = {}\\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n                                           unsafe_hash, frozen))\\n    any_frozen_base = False\\n    has_dataclass_bases = False\\n    for b in cls.__mro__[-1:0:-1]:\\n        base_fields = getattr(b, _FIELDS, None)\\n        if base_fields:\\n            has_dataclass_bases = True\\n            for f in base_fields.values():\\n                fields[f.name] = f\\n            if getattr(b, _PARAMS).frozen:\\n                any_frozen_base = True\\n    cls_annotations = cls.__dict__.get('__annotations__', {})\\n    cls_fields = [_get_field(cls, name, type)\\n                  for name, type in cls_annotations.items()]\\n    for f in cls_fields:\\n        fields[f.name] = f\\n        if isinstance(getattr(cls, f.name, None), Field):\\n            if f.default is MISSING:\\n                delattr(cls, f.name)\\n            else:\\n                setattr(cls, f.name, f.default)\\n    for name, value in cls.__dict__.items():\\n        if isinstance(value, Field) and not name in cls_annotations:\\n            raise TypeError(f'{name!r} is a field but has no type annotation')\\n    if has_dataclass_bases:\\n        if any_frozen_base and not frozen:\\n            raise TypeError('cannot inherit non-frozen dataclass from a '\\n                            'frozen one')\\n        if not any_frozen_base and frozen:\\n            raise TypeError('cannot inherit frozen dataclass from a '\\n                            'non-frozen one')\\n    setattr(cls, _FIELDS, fields)\\n    class_hash = cls.__dict__.get('__hash__', MISSING)\\n    has_explicit_hash = not (class_hash is MISSING or\\n                             (class_hash is None and '__eq__' in cls.__dict__))\\n    if order and not eq:\\n        raise ValueError('eq must be true if order is true')\\n    if init:\\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\\n        flds = [f for f in fields.values()\\n                if f._field_type in (_FIELD, _FIELD_INITVAR)]\\n        _set_new_attribute(cls, '__init__',\\n                           _init_fn(flds,\\n                                    frozen,\\n                                    has_post_init,\\n                                    '__dataclass_self__' if 'self' in fields\\n                                            else 'self',\\n                                    globals,\\n                          ))\\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\\n    if repr:\\n        flds = [f for f in field_list if f.repr]\\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n    if eq:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        _set_new_attribute(cls, '__eq__',\\n                           _cmp_fn('__eq__', '==',\\n                                   self_tuple, other_tuple,\\n                                   globals=globals))\\n    if order:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        for name, op in [('__lt__', '<'),\\n                         ('__le__', '<='),\\n                         ('__gt__', '>'),\\n                         ('__ge__', '>='),\\n                         ]:\\n            if _set_new_attribute(cls, name,\\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\\n                                          globals=globals)):\\n                raise TypeError(f'Cannot overwrite attribute {name} '\\n                                f'in class {cls.__name__}. Consider using '\\n                                'functools.total_ordering')\\n    if frozen:\\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\\n            if _set_new_attribute(cls, fn.__name__, fn):\\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n                                f'in class {cls.__name__}')\\n    hash_action = _hash_action[bool(unsafe_hash),\\n                               bool(eq),\\n                               bool(frozen),\\n                               has_explicit_hash]\\n    if hash_action:\\n        cls.__hash__ = hash_action(cls, field_list, globals)\\n    if not getattr(cls, '__doc__'):\\n        cls.__doc__ = (cls.__name__ +\\n                       str(inspect.signature(cls)).replace(' -> None', ''))\\n    return cls",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\\n    fields = {}\\n    if cls.__module__ in sys.modules:\\n        globals = sys.modules[cls.__module__].__dict__\\n    else:\\n        globals = {}\\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n                                           unsafe_hash, frozen))\\n    any_frozen_base = False\\n    has_dataclass_bases = False\\n    for b in cls.__mro__[-1:0:-1]:\\n        base_fields = getattr(b, _FIELDS, None)\\n        if base_fields:\\n            has_dataclass_bases = True\\n            for f in base_fields.values():\\n                fields[f.name] = f\\n            if getattr(b, _PARAMS).frozen:\\n                any_frozen_base = True\\n    cls_annotations = cls.__dict__.get('__annotations__', {})\\n    cls_fields = [_get_field(cls, name, type)\\n                  for name, type in cls_annotations.items()]\\n    for f in cls_fields:\\n        fields[f.name] = f\\n        if isinstance(getattr(cls, f.name, None), Field):\\n            if f.default is MISSING:\\n                delattr(cls, f.name)\\n            else:\\n                setattr(cls, f.name, f.default)\\n    for name, value in cls.__dict__.items():\\n        if isinstance(value, Field) and not name in cls_annotations:\\n            raise TypeError(f'{name!r} is a field but has no type annotation')\\n    if has_dataclass_bases:\\n        if any_frozen_base and not frozen:\\n            raise TypeError('cannot inherit non-frozen dataclass from a '\\n                            'frozen one')\\n        if not any_frozen_base and frozen:\\n            raise TypeError('cannot inherit frozen dataclass from a '\\n                            'non-frozen one')\\n    setattr(cls, _FIELDS, fields)\\n    class_hash = cls.__dict__.get('__hash__', MISSING)\\n    has_explicit_hash = not (class_hash is MISSING or\\n                             (class_hash is None and '__eq__' in cls.__dict__))\\n    if order and not eq:\\n        raise ValueError('eq must be true if order is true')\\n    if init:\\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\\n        flds = [f for f in fields.values()\\n                if f._field_type in (_FIELD, _FIELD_INITVAR)]\\n        _set_new_attribute(cls, '__init__',\\n                           _init_fn(flds,\\n                                    frozen,\\n                                    has_post_init,\\n                                    '__dataclass_self__' if 'self' in fields\\n                                            else 'self',\\n                                    globals,\\n                          ))\\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\\n    if repr:\\n        flds = [f for f in field_list if f.repr]\\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n    if eq:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        _set_new_attribute(cls, '__eq__',\\n                           _cmp_fn('__eq__', '==',\\n                                   self_tuple, other_tuple,\\n                                   globals=globals))\\n    if order:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        for name, op in [('__lt__', '<'),\\n                         ('__le__', '<='),\\n                         ('__gt__', '>'),\\n                         ('__ge__', '>='),\\n                         ]:\\n            if _set_new_attribute(cls, name,\\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\\n                                          globals=globals)):\\n                raise TypeError(f'Cannot overwrite attribute {name} '\\n                                f'in class {cls.__name__}. Consider using '\\n                                'functools.total_ordering')\\n    if frozen:\\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\\n            if _set_new_attribute(cls, fn.__name__, fn):\\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n                                f'in class {cls.__name__}')\\n    hash_action = _hash_action[bool(unsafe_hash),\\n                               bool(eq),\\n                               bool(frozen),\\n                               has_explicit_hash]\\n    if hash_action:\\n        cls.__hash__ = hash_action(cls, field_list, globals)\\n    if not getattr(cls, '__doc__'):\\n        cls.__doc__ = (cls.__name__ +\\n                       str(inspect.signature(cls)).replace(' -> None', ''))\\n    return cls"
  },
  {
    "code": "def _clean_na_values(na_values, keep_default_na=True):\\n    if na_values is None:\\n        if keep_default_na:\\n            na_values = STR_NA_VALUES\\n        else:\\n            na_values = set()\\n        na_fvalues = set()\\n    elif isinstance(na_values, dict):\\n        old_na_values = na_values.copy()\\n        na_values = {}  \\n        for k, v in old_na_values.items():\\n            if not is_list_like(v):\\n                v = [v]\\n            if keep_default_na:\\n                v = set(v) | STR_NA_VALUES\\n            na_values[k] = v\\n        na_fvalues = {k: _floatify_na_values(v) for k, v in na_values.items()}\\n    else:\\n        if not is_list_like(na_values):\\n            na_values = [na_values]\\n        na_values = _stringify_na_values(na_values)\\n        if keep_default_na:\\n            na_values = na_values | STR_NA_VALUES\\n        na_fvalues = _floatify_na_values(na_values)\\n    return na_values, na_fvalues",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _clean_na_values(na_values, keep_default_na=True):\\n    if na_values is None:\\n        if keep_default_na:\\n            na_values = STR_NA_VALUES\\n        else:\\n            na_values = set()\\n        na_fvalues = set()\\n    elif isinstance(na_values, dict):\\n        old_na_values = na_values.copy()\\n        na_values = {}  \\n        for k, v in old_na_values.items():\\n            if not is_list_like(v):\\n                v = [v]\\n            if keep_default_na:\\n                v = set(v) | STR_NA_VALUES\\n            na_values[k] = v\\n        na_fvalues = {k: _floatify_na_values(v) for k, v in na_values.items()}\\n    else:\\n        if not is_list_like(na_values):\\n            na_values = [na_values]\\n        na_values = _stringify_na_values(na_values)\\n        if keep_default_na:\\n            na_values = na_values | STR_NA_VALUES\\n        na_fvalues = _floatify_na_values(na_values)\\n    return na_values, na_fvalues"
  },
  {
    "code": "def handle(self, app_or_project, name, target=None, **options):\\n        self.app_or_project = app_or_project\\n        self.a_or_an = 'an' if app_or_project == 'app' else 'a'\\n        self.paths_to_remove = []\\n        self.verbosity = options['verbosity']\\n        self.validate_name(name)\\n        if target is None:\\n            top_dir = os.path.join(os.getcwd(), name)\\n            try:\\n                os.makedirs(top_dir)\\n            except FileExistsError:\\n                raise CommandError(\"'%s' already exists\" % top_dir)\\n            except OSError as e:\\n                raise CommandError(e)\\n        else:\\n            top_dir = os.path.abspath(os.path.expanduser(target))\\n            if app_or_project == 'app':\\n                self.validate_name(os.path.basename(top_dir), 'directory')\\n            if not os.path.exists(top_dir):\\n                raise CommandError(\"Destination directory '%s' does not \"\\n                                   \"exist, please create it first.\" % top_dir)\\n        extensions = tuple(handle_extensions(options['extensions']))\\n        extra_files = []\\n        excluded_directories = ['.git', '__pycache__']\\n        for file in options['files']:\\n            extra_files.extend(map(lambda x: x.strip(), file.split(',')))\\n        if exclude := options.get('exclude'):\\n            for directory in exclude:\\n                excluded_directories.append(directory.strip())\\n        if self.verbosity >= 2:\\n            self.stdout.write(\\n                'Rendering %s template files with extensions: %s'\\n                % (app_or_project, ', '.join(extensions))\\n            )\\n            self.stdout.write(\\n                'Rendering %s template files with filenames: %s'\\n                % (app_or_project, ', '.join(extra_files))\\n            )\\n        base_name = '%s_name' % app_or_project\\n        base_subdir = '%s_template' % app_or_project\\n        base_directory = '%s_directory' % app_or_project\\n        camel_case_name = 'camel_case_%s_name' % app_or_project\\n        camel_case_value = ''.join(x for x in name.title() if x != '_')\\n        context = Context({\\n            **options,\\n            base_name: name,\\n            base_directory: top_dir,\\n            camel_case_name: camel_case_value,\\n            'docs_version': get_docs_version(),\\n            'django_version': django.__version__,\\n        }, autoescape=False)\\n        if not settings.configured:\\n            settings.configure()\\n            django.setup()\\n        template_dir = self.handle_template(options['template'],\\n                                            base_subdir)\\n        prefix_length = len(template_dir) + 1\\n        for root, dirs, files in os.walk(template_dir):\\n            path_rest = root[prefix_length:]\\n            relative_dir = path_rest.replace(base_name, name)\\n            if relative_dir:\\n                target_dir = os.path.join(top_dir, relative_dir)\\n                os.makedirs(target_dir, exist_ok=True)\\n            for dirname in dirs[:]:\\n                if 'exclude' not in options:\\n                    if dirname.startswith('.') or dirname == '__pycache__':\\n                        dirs.remove(dirname)\\n                elif dirname in excluded_directories:\\n                    dirs.remove(dirname)\\n            for filename in files:\\n                if filename.endswith(('.pyo', '.pyc', '.py.class')):\\n                    continue\\n                old_path = os.path.join(root, filename)\\n                new_path = os.path.join(\\n                    top_dir, relative_dir, filename.replace(base_name, name)\\n                )\\n                for old_suffix, new_suffix in self.rewrite_template_suffixes:\\n                    if new_path.endswith(old_suffix):\\n                        new_path = new_path[:-len(old_suffix)] + new_suffix\\n                        break  \\n                if os.path.exists(new_path):\\n                    raise CommandError(\\n                        \"%s already exists. Overlaying %s %s into an existing \"\\n                        \"directory won't replace conflicting files.\" % (\\n                            new_path, self.a_or_an, app_or_project,\\n                        )\\n                    )\\n                if new_path.endswith(extensions) or filename in extra_files:\\n                    with open(old_path, encoding='utf-8') as template_file:\\n                        content = template_file.read()\\n                    template = Engine().from_string(content)\\n                    content = template.render(context)\\n                    with open(new_path, 'w', encoding='utf-8') as new_file:\\n                        new_file.write(content)\\n                else:\\n                    shutil.copyfile(old_path, new_path)\\n                if self.verbosity >= 2:\\n                    self.stdout.write('Creating %s' % new_path)\\n                try:\\n                    shutil.copymode(old_path, new_path)\\n                    self.make_writeable(new_path)\\n                except OSError:\\n                    self.stderr.write(\\n                        \"Notice: Couldn't set permission bits on %s. You're \"\\n                        \"probably using an uncommon filesystem setup. No \"\\n                        \"problem.\" % new_path, self.style.NOTICE)\\n        if self.paths_to_remove:\\n            if self.verbosity >= 2:\\n                self.stdout.write('Cleaning up temporary files.')\\n            for path_to_remove in self.paths_to_remove:\\n                if os.path.isfile(path_to_remove):\\n                    os.remove(path_to_remove)\\n                else:\\n                    shutil.rmtree(path_to_remove)\\n    def handle_template(self, template, subdir):\\n        if template is None:\\n            return os.path.join(django.__path__[0], 'conf', subdir)\\n        else:\\n            if template.startswith('file://'):\\n                template = template[7:]\\n            expanded_template = os.path.expanduser(template)\\n            expanded_template = os.path.normpath(expanded_template)\\n            if os.path.isdir(expanded_template):\\n                return expanded_template\\n            if self.is_url(template):\\n                absolute_path = self.download(template)\\n            else:\\n                absolute_path = os.path.abspath(expanded_template)\\n            if os.path.exists(absolute_path):\\n                return self.extract(absolute_path)\\n        raise CommandError(\"couldn't handle %s template %s.\" %\\n                           (self.app_or_project, template))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def handle(self, app_or_project, name, target=None, **options):\\n        self.app_or_project = app_or_project\\n        self.a_or_an = 'an' if app_or_project == 'app' else 'a'\\n        self.paths_to_remove = []\\n        self.verbosity = options['verbosity']\\n        self.validate_name(name)\\n        if target is None:\\n            top_dir = os.path.join(os.getcwd(), name)\\n            try:\\n                os.makedirs(top_dir)\\n            except FileExistsError:\\n                raise CommandError(\"'%s' already exists\" % top_dir)\\n            except OSError as e:\\n                raise CommandError(e)\\n        else:\\n            top_dir = os.path.abspath(os.path.expanduser(target))\\n            if app_or_project == 'app':\\n                self.validate_name(os.path.basename(top_dir), 'directory')\\n            if not os.path.exists(top_dir):\\n                raise CommandError(\"Destination directory '%s' does not \"\\n                                   \"exist, please create it first.\" % top_dir)\\n        extensions = tuple(handle_extensions(options['extensions']))\\n        extra_files = []\\n        excluded_directories = ['.git', '__pycache__']\\n        for file in options['files']:\\n            extra_files.extend(map(lambda x: x.strip(), file.split(',')))\\n        if exclude := options.get('exclude'):\\n            for directory in exclude:\\n                excluded_directories.append(directory.strip())\\n        if self.verbosity >= 2:\\n            self.stdout.write(\\n                'Rendering %s template files with extensions: %s'\\n                % (app_or_project, ', '.join(extensions))\\n            )\\n            self.stdout.write(\\n                'Rendering %s template files with filenames: %s'\\n                % (app_or_project, ', '.join(extra_files))\\n            )\\n        base_name = '%s_name' % app_or_project\\n        base_subdir = '%s_template' % app_or_project\\n        base_directory = '%s_directory' % app_or_project\\n        camel_case_name = 'camel_case_%s_name' % app_or_project\\n        camel_case_value = ''.join(x for x in name.title() if x != '_')\\n        context = Context({\\n            **options,\\n            base_name: name,\\n            base_directory: top_dir,\\n            camel_case_name: camel_case_value,\\n            'docs_version': get_docs_version(),\\n            'django_version': django.__version__,\\n        }, autoescape=False)\\n        if not settings.configured:\\n            settings.configure()\\n            django.setup()\\n        template_dir = self.handle_template(options['template'],\\n                                            base_subdir)\\n        prefix_length = len(template_dir) + 1\\n        for root, dirs, files in os.walk(template_dir):\\n            path_rest = root[prefix_length:]\\n            relative_dir = path_rest.replace(base_name, name)\\n            if relative_dir:\\n                target_dir = os.path.join(top_dir, relative_dir)\\n                os.makedirs(target_dir, exist_ok=True)\\n            for dirname in dirs[:]:\\n                if 'exclude' not in options:\\n                    if dirname.startswith('.') or dirname == '__pycache__':\\n                        dirs.remove(dirname)\\n                elif dirname in excluded_directories:\\n                    dirs.remove(dirname)\\n            for filename in files:\\n                if filename.endswith(('.pyo', '.pyc', '.py.class')):\\n                    continue\\n                old_path = os.path.join(root, filename)\\n                new_path = os.path.join(\\n                    top_dir, relative_dir, filename.replace(base_name, name)\\n                )\\n                for old_suffix, new_suffix in self.rewrite_template_suffixes:\\n                    if new_path.endswith(old_suffix):\\n                        new_path = new_path[:-len(old_suffix)] + new_suffix\\n                        break  \\n                if os.path.exists(new_path):\\n                    raise CommandError(\\n                        \"%s already exists. Overlaying %s %s into an existing \"\\n                        \"directory won't replace conflicting files.\" % (\\n                            new_path, self.a_or_an, app_or_project,\\n                        )\\n                    )\\n                if new_path.endswith(extensions) or filename in extra_files:\\n                    with open(old_path, encoding='utf-8') as template_file:\\n                        content = template_file.read()\\n                    template = Engine().from_string(content)\\n                    content = template.render(context)\\n                    with open(new_path, 'w', encoding='utf-8') as new_file:\\n                        new_file.write(content)\\n                else:\\n                    shutil.copyfile(old_path, new_path)\\n                if self.verbosity >= 2:\\n                    self.stdout.write('Creating %s' % new_path)\\n                try:\\n                    shutil.copymode(old_path, new_path)\\n                    self.make_writeable(new_path)\\n                except OSError:\\n                    self.stderr.write(\\n                        \"Notice: Couldn't set permission bits on %s. You're \"\\n                        \"probably using an uncommon filesystem setup. No \"\\n                        \"problem.\" % new_path, self.style.NOTICE)\\n        if self.paths_to_remove:\\n            if self.verbosity >= 2:\\n                self.stdout.write('Cleaning up temporary files.')\\n            for path_to_remove in self.paths_to_remove:\\n                if os.path.isfile(path_to_remove):\\n                    os.remove(path_to_remove)\\n                else:\\n                    shutil.rmtree(path_to_remove)\\n    def handle_template(self, template, subdir):\\n        if template is None:\\n            return os.path.join(django.__path__[0], 'conf', subdir)\\n        else:\\n            if template.startswith('file://'):\\n                template = template[7:]\\n            expanded_template = os.path.expanduser(template)\\n            expanded_template = os.path.normpath(expanded_template)\\n            if os.path.isdir(expanded_template):\\n                return expanded_template\\n            if self.is_url(template):\\n                absolute_path = self.download(template)\\n            else:\\n                absolute_path = os.path.abspath(expanded_template)\\n            if os.path.exists(absolute_path):\\n                return self.extract(absolute_path)\\n        raise CommandError(\"couldn't handle %s template %s.\" %\\n                           (self.app_or_project, template))"
  },
  {
    "code": "def get_field(self, field_name):\\n        if field_name == '_order':\\n            field_name = self.options.get('order_with_respect_to', field_name)\\n        return self.fields[field_name]\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_field(self, field_name):\\n        if field_name == '_order':\\n            field_name = self.options.get('order_with_respect_to', field_name)\\n        return self.fields[field_name]\\n    @classmethod"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        http=dict(aliases=['enable_http'], type='bool'),\\n        http_port=dict(type='int'),\\n        https=dict(aliases=['enable_https'], type='bool'),\\n        https_port=dict(type='int'),\\n        local_http=dict(aliases=['enable_local_http'], type='bool'),\\n        local_http_port=dict(type='int'),\\n        socket=dict(aliases=['enable_socket'], type='bool'),\\n        timeout=dict(type=\"int\", default=30),\\n        vrf=dict(default='default'),\\n        config=dict(),\\n        state=dict(default='started', choices=['stopped', 'started']),\\n    )\\n    argument_spec.update(eos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)\\n    check_transport(module)\\n    result = {'changed': False}\\n    warnings = list()\\n    if module.params['config']:\\n        warnings.append('config parameter is no longer necessary and will be ignored')\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands((want, have), module, warnings)\\n    result['commands'] = commands\\n    if commands:\\n        commit = not module.check_mode\\n        response = load_config(module, commands, commit=commit)\\n        if response.get('diff') and module._diff:\\n            result['diff'] = {'prepared': response.get('diff')}\\n        result['session_name'] = response.get('session')\\n        result['changed'] = True\\n    if result['changed']:\\n        verify_state((want, have), module)\\n    collect_facts(module, result)\\n    if warnings:\\n        result['warnings'] = warnings\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        http=dict(aliases=['enable_http'], type='bool'),\\n        http_port=dict(type='int'),\\n        https=dict(aliases=['enable_https'], type='bool'),\\n        https_port=dict(type='int'),\\n        local_http=dict(aliases=['enable_local_http'], type='bool'),\\n        local_http_port=dict(type='int'),\\n        socket=dict(aliases=['enable_socket'], type='bool'),\\n        timeout=dict(type=\"int\", default=30),\\n        vrf=dict(default='default'),\\n        config=dict(),\\n        state=dict(default='started', choices=['stopped', 'started']),\\n    )\\n    argument_spec.update(eos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)\\n    check_transport(module)\\n    result = {'changed': False}\\n    warnings = list()\\n    if module.params['config']:\\n        warnings.append('config parameter is no longer necessary and will be ignored')\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands((want, have), module, warnings)\\n    result['commands'] = commands\\n    if commands:\\n        commit = not module.check_mode\\n        response = load_config(module, commands, commit=commit)\\n        if response.get('diff') and module._diff:\\n            result['diff'] = {'prepared': response.get('diff')}\\n        result['session_name'] = response.get('session')\\n        result['changed'] = True\\n    if result['changed']:\\n        verify_state((want, have), module)\\n    collect_facts(module, result)\\n    if warnings:\\n        result['warnings'] = warnings\\n    module.exit_json(**result)"
  },
  {
    "code": "def _bool_method_SERIES(cls, op, special):\\n    op_name = _get_op_name(op, special)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _bool_method_SERIES(cls, op, special):\\n    op_name = _get_op_name(op, special)"
  },
  {
    "code": "def insert(self, loc, item):\\n        indexes=[self[:loc],\\n                 Index([item]),\\n                 self[loc:]]\\n        return indexes[0].append(indexes[1]).append(indexes[2])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def insert(self, loc, item):\\n        indexes=[self[:loc],\\n                 Index([item]),\\n                 self[loc:]]\\n        return indexes[0].append(indexes[1]).append(indexes[2])"
  },
  {
    "code": "def _ica_def(X, tol, g, fun_args, max_iter, w_init):\\n    n_components = w_init.shape[0]\\n    W = np.zeros((n_components, n_components), dtype=float)\\n    for j in range(n_components):\\n        w = w_init[j, :].copy()\\n        w /= np.sqrt((w ** 2).sum())\\n        for _ in moves.xrange(max_iter):\\n            wtx = np.dot(w.T, X)\\n            gwtx, g_wtx = g(wtx, fun_args)\\n            w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\\n            _gs_decorrelation(w1, W, j)\\n            w1 /= np.sqrt((w1 ** 2).sum())\\n            lim = np.abs(np.abs((w1 * w).sum()) - 1)\\n            w = w1\\n            if lim < tol:\\n                break\\n        W[j, :] = w\\n    return W",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ica_def(X, tol, g, fun_args, max_iter, w_init):\\n    n_components = w_init.shape[0]\\n    W = np.zeros((n_components, n_components), dtype=float)\\n    for j in range(n_components):\\n        w = w_init[j, :].copy()\\n        w /= np.sqrt((w ** 2).sum())\\n        for _ in moves.xrange(max_iter):\\n            wtx = np.dot(w.T, X)\\n            gwtx, g_wtx = g(wtx, fun_args)\\n            w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\\n            _gs_decorrelation(w1, W, j)\\n            w1 /= np.sqrt((w1 ** 2).sum())\\n            lim = np.abs(np.abs((w1 * w).sum()) - 1)\\n            w = w1\\n            if lim < tol:\\n                break\\n        W[j, :] = w\\n    return W"
  },
  {
    "code": "def config_realtime_process(cores: Union[int, List[int]], priority: int) -> None:\\n  gc.disable()\\n  set_realtime_priority(priority)\\n  c = cores if isinstance(cores, list) else [cores, ]\\n  set_core_affinity(c)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def config_realtime_process(cores: Union[int, List[int]], priority: int) -> None:\\n  gc.disable()\\n  set_realtime_priority(priority)\\n  c = cores if isinstance(cores, list) else [cores, ]\\n  set_core_affinity(c)"
  },
  {
    "code": "def set_levels(\\n        self, levels, level=None, inplace=None, verify_integrity: bool = True\\n    ):\\n        if inplace is not None:\\n            warnings.warn(\\n                \"inplace is deprecated and will be removed in a future version.\",\\n                FutureWarning,\\n                stacklevel=find_stack_level(),\\n            )\\n        else:\\n            inplace = False\\n        if is_list_like(levels) and not isinstance(levels, Index):\\n            levels = list(levels)\\n        level, levels = _require_listlike(level, levels, \"Levels\")\\n        if inplace:\\n            idx = self\\n        else:\\n            idx = self._view()\\n        idx._reset_identity()\\n        idx._set_levels(\\n            levels, level=level, validate=True, verify_integrity=verify_integrity\\n        )\\n        if not inplace:\\n            return idx\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEP: Enforce set_values and set_codes inplace and positional args deprecation (#49084)",
    "fixed_code": "def set_levels(self, levels, *, level=None, verify_integrity: bool = True):\\n        if is_list_like(levels) and not isinstance(levels, Index):\\n            levels = list(levels)\\n        level, levels = _require_listlike(level, levels, \"Levels\")\\n        idx = self._view()\\n        idx._reset_identity()\\n        idx._set_levels(\\n            levels, level=level, validate=True, verify_integrity=verify_integrity\\n        )\\n        return idx\\n    @property"
  },
  {
    "code": "def make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0):\\n    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: '', 'compress': ''}\\n    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'compress': '.Z'}\\n    if compress is not None and compress not in compress_ext.keys():\\n        raise ValueError(\\n              \"bad value for 'compress': must be None, 'gzip', 'bzip2' \"\\n              \"or 'compress'\")\\n    archive_name = base_name + '.tar'\\n    if compress != 'compress':\\n        archive_name += compress_ext.get(compress, '')\\n    mkpath(os.path.dirname(archive_name), dry_run=dry_run)\\n    import tarfile  \\n    log.info('Creating tar archive')\\n    if not dry_run:\\n        tar = tarfile.open(archive_name, 'w|%s' % tar_compression[compress])\\n        try:\\n            tar.add(base_dir)\\n        finally:\\n            tar.close()\\n    if compress == 'compress':\\n        warn(\"'compress' will be deprecated.\", PendingDeprecationWarning)\\n        compressed_name = archive_name + compress_ext[compress]\\n        if sys.platform == 'win32':\\n            cmd = [compress, archive_name, compressed_name]\\n        else:\\n            cmd = [compress, '-f', archive_name]\\n        spawn(cmd, dry_run=dry_run)\\n        return compressed_name\\n    return archive_name",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 75192 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r75192 | tarek.ziade | 2009-10-03 01:49:48 +0200 (Sat, 03 Oct 2009) | 1 line\\n\\n  #6516 added owner/group support for tarfiles in Distutils\\n........",
    "fixed_code": "def make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\\n                 owner=None, group=None):\\n    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: '', 'compress': ''}\\n    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'compress': '.Z'}\\n    if compress is not None and compress not in compress_ext.keys():\\n        raise ValueError(\\n              \"bad value for 'compress': must be None, 'gzip', 'bzip2' \"\\n              \"or 'compress'\")\\n    archive_name = base_name + '.tar'\\n    if compress != 'compress':\\n        archive_name += compress_ext.get(compress, '')\\n    mkpath(os.path.dirname(archive_name), dry_run=dry_run)\\n    import tarfile  \\n    log.info('Creating tar archive')\\n    uid = _get_uid(owner)\\n    gid = _get_gid(group)"
  },
  {
    "code": "def new_name(self, template=u\"xxx_todo_changeme\"):\\n        name = template\\n        while name in self.used_names:\\n            name = template + unicode(self.numbers.next())\\n        self.used_names.add(name)\\n        return name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def new_name(self, template=u\"xxx_todo_changeme\"):\\n        name = template\\n        while name in self.used_names:\\n            name = template + unicode(self.numbers.next())\\n        self.used_names.add(name)\\n        return name"
  },
  {
    "code": "def _homogenize(data, index, columns, dtype=None):\\n    from pandas.core.series import _sanitize_array\\n    homogenized = {}\\n    if dtype is not None:\\n        dtype = np.dtype(dtype)\\n    oindex = None\\n    for k in columns:\\n        if k not in data:\\n            if dtype is not None and issubclass(dtype.type, np.integer):\\n                continue\\n            v = np.empty(len(index), dtype=dtype)\\n            v.fill(nan)\\n        else:\\n            v = data[k]\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index, copy=False)\\n        else:\\n            if isinstance(v, dict):\\n                if oindex is None:\\n                    oindex = index.astype('O')\\n                if type(v) != dict:\\n                    v = dict(v)\\n                v = lib.fast_multiget(v, oindex, default=np.nan)\\n            v = _sanitize_array(v, index, dtype=dtype, copy=False,\\n                                raise_cast_failure=False)\\n        homogenized[k] = v\\n    return homogenized",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _homogenize(data, index, columns, dtype=None):\\n    from pandas.core.series import _sanitize_array\\n    homogenized = {}\\n    if dtype is not None:\\n        dtype = np.dtype(dtype)\\n    oindex = None\\n    for k in columns:\\n        if k not in data:\\n            if dtype is not None and issubclass(dtype.type, np.integer):\\n                continue\\n            v = np.empty(len(index), dtype=dtype)\\n            v.fill(nan)\\n        else:\\n            v = data[k]\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index, copy=False)\\n        else:\\n            if isinstance(v, dict):\\n                if oindex is None:\\n                    oindex = index.astype('O')\\n                if type(v) != dict:\\n                    v = dict(v)\\n                v = lib.fast_multiget(v, oindex, default=np.nan)\\n            v = _sanitize_array(v, index, dtype=dtype, copy=False,\\n                                raise_cast_failure=False)\\n        homogenized[k] = v\\n    return homogenized"
  },
  {
    "code": "def _convert_to_indexer(self, obj, axis=0):\\n        labels = self.obj._get_axis(axis)\\n        is_int_index = _is_integer_index(labels)\\n        if com.is_integer(obj) and not is_int_index:\\n            return obj\\n        try:\\n            return labels.get_loc(obj)\\n        except (KeyError, TypeError):\\n            pass\\n        if isinstance(obj, slice):\\n            ltype = labels.inferred_type\\n            float_slice = (labels.inferred_type == 'floating'\\n                           and _is_float_slice(obj))\\n            int_slice = _is_index_slice(obj)\\n            null_slice = obj.start is None and obj.stop is None\\n            position_slice = (int_slice\\n                              and not ltype == 'integer'\\n                              and not isinstance(labels, MultiIndex)\\n                              and not float_slice)\\n            start, stop = obj.start, obj.stop\\n            try:\\n                if position_slice and 'mixed' in ltype:\\n                    if start is not None:\\n                        i = labels.get_loc(start)\\n                    if stop is not None:\\n                        j = labels.get_loc(stop)\\n                    position_slice = False\\n            except KeyError:\\n                if ltype == 'mixed-integer-float':\\n                    raise\\n            if null_slice or position_slice:\\n                indexer = obj\\n            else:\\n                try:\\n                    indexer = labels.slice_indexer(start, stop, obj.step)\\n                except Exception:\\n                    if _is_index_slice(obj):\\n                        if ltype == 'integer':\\n                            raise\\n                        indexer = obj\\n                    else:\\n                        raise\\n            return indexer\\n        elif _is_list_like(obj):\\n            if com._is_bool_indexer(obj):\\n                obj = _check_bool_indexer(labels, obj)\\n                inds, = obj.nonzero()\\n                return inds\\n            else:\\n                if isinstance(obj, Index):\\n                    objarr = obj.values\\n                else:\\n                    objarr = _asarray_tuplesafe(obj)\\n                if _is_integer_dtype(objarr) and not is_int_index:\\n                    if labels.inferred_type != 'integer':\\n                        objarr = np.where(objarr < 0,\\n                                          len(labels) + objarr, objarr)\\n                    return objarr\\n                if (isinstance(labels, MultiIndex) and\\n                        not isinstance(objarr[0], tuple)):\\n                    level = 0\\n                    _, indexer = labels.reindex(objarr, level=level)\\n                    check = labels.levels[0].get_indexer(objarr)\\n                else:\\n                    level = None\\n                    if labels.is_unique:\\n                        indexer = check = labels.get_indexer(objarr)\\n                    else:\\n                        indexer, missing = labels.get_indexer_non_unique(objarr)\\n                        check = indexer\\n                mask = check == -1\\n                if mask.any():\\n                    raise KeyError('%s not in index' % objarr[mask])\\n                return indexer\\n        else:\\n            return labels.get_loc(obj)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH:  GH2578, allow ix and friends to partially set when the key is not contained       in the object",
    "fixed_code": "def _convert_to_indexer(self, obj, axis=0, is_setter=False):\\n        labels = self.obj._get_axis(axis)\\n        is_int_index = _is_integer_index(labels)\\n        if com.is_integer(obj) and not is_int_index:\\n            return obj\\n        try:\\n            return labels.get_loc(obj)\\n        except (KeyError, TypeError):\\n            pass\\n        if isinstance(obj, slice):\\n            ltype = labels.inferred_type\\n            float_slice = (labels.inferred_type == 'floating'\\n                           and _is_float_slice(obj))\\n            int_slice = _is_index_slice(obj)\\n            null_slice = obj.start is None and obj.stop is None\\n            position_slice = (int_slice\\n                              and not ltype == 'integer'\\n                              and not isinstance(labels, MultiIndex)\\n                              and not float_slice)\\n            start, stop = obj.start, obj.stop\\n            try:\\n                if position_slice and 'mixed' in ltype:\\n                    if start is not None:\\n                        i = labels.get_loc(start)\\n                    if stop is not None:\\n                        j = labels.get_loc(stop)\\n                    position_slice = False\\n            except KeyError:\\n                if ltype == 'mixed-integer-float':\\n                    raise\\n            if null_slice or position_slice:\\n                indexer = obj\\n            else:\\n                try:\\n                    indexer = labels.slice_indexer(start, stop, obj.step)\\n                except Exception:\\n                    if _is_index_slice(obj):\\n                        if ltype == 'integer':\\n                            raise\\n                        indexer = obj\\n                    else:\\n                        raise\\n            return indexer\\n        elif _is_list_like(obj):\\n            if com._is_bool_indexer(obj):\\n                obj = _check_bool_indexer(labels, obj)\\n                inds, = obj.nonzero()\\n                return inds\\n            else:\\n                if isinstance(obj, Index):\\n                    objarr = obj.values\\n                else:\\n                    objarr = _asarray_tuplesafe(obj)\\n                if _is_integer_dtype(objarr) and not is_int_index:\\n                    if labels.inferred_type != 'integer':\\n                        objarr = np.where(objarr < 0,\\n                                          len(labels) + objarr, objarr)\\n                    return objarr\\n                if (isinstance(labels, MultiIndex) and\\n                        not isinstance(objarr[0], tuple)):\\n                    level = 0\\n                    _, indexer = labels.reindex(objarr, level=level)\\n                    check = labels.levels[0].get_indexer(objarr)\\n                else:\\n                    level = None\\n                    if labels.is_unique:\\n                        indexer = check = labels.get_indexer(objarr)\\n                    else:\\n                        indexer, missing = labels.get_indexer_non_unique(objarr)\\n                        check = indexer\\n                mask = check == -1\\n                if mask.any():\\n                    raise KeyError('%s not in index' % objarr[mask])\\n                return indexer\\n        else:\\n            try:\\n                return labels.get_loc(obj)\\n            except (KeyError):\\n                if np.isscalar(obj) and is_setter:\\n                    return { 'key' : obj }\\n                raise"
  },
  {
    "code": "def load_module(name, file, filename, details):\\n    suffix, mode, type_ = details\\n    with warnings.catch_warnings():\\n        warnings.simplefilter('ignore')\\n        if mode and (not mode.startswith(('r', 'U')) or '+' in mode):\\n            raise ValueError('invalid file open mode {!r}'.format(mode))\\n        elif file is None and type_ in {PY_SOURCE, PY_COMPILED, C_EXTENSION}:\\n            msg = 'file object required for import (type code {})'.format(type_)\\n            raise ValueError(msg)\\n        elif type_ == PY_SOURCE:\\n            return load_source(name, filename, file)\\n        elif type_ == PY_COMPILED:\\n            return load_compiled(name, filename, file)\\n        elif type_ == C_EXTENSION and load_dynamic is not None:\\n            return load_dynamic(name, filename, file)\\n        elif type_ == PKG_DIRECTORY:\\n            return load_package(name, filename)\\n        elif type_ == C_BUILTIN:\\n            return init_builtin(name)\\n        elif type_ == PY_FROZEN:\\n            return init_frozen(name)\\n        else:\\n            msg =  \"Don't know how to import {} (type code {})\".format(name, type_)\\n            raise ImportError(msg, name=name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #15902: Fix imp.load_module() to accept None as a file when trying to load an extension module.\\n\\nWhile at it, also add a proper unittest.skipIf() guard to another test\\ninvolving imp.load_dynamic().",
    "fixed_code": "def load_module(name, file, filename, details):\\n    suffix, mode, type_ = details\\n    with warnings.catch_warnings():\\n        warnings.simplefilter('ignore')\\n        if mode and (not mode.startswith(('r', 'U')) or '+' in mode):\\n            raise ValueError('invalid file open mode {!r}'.format(mode))\\n        elif file is None and type_ in {PY_SOURCE, PY_COMPILED}:\\n            msg = 'file object required for import (type code {})'.format(type_)\\n            raise ValueError(msg)\\n        elif type_ == PY_SOURCE:\\n            return load_source(name, filename, file)\\n        elif type_ == PY_COMPILED:\\n            return load_compiled(name, filename, file)\\n        elif type_ == C_EXTENSION and load_dynamic is not None:\\n            if file is None:\\n                with open(filename, 'rb') as opened_file:\\n                    return load_dynamic(name, filename, opened_file)\\n            else:\\n                return load_dynamic(name, filename, file)\\n        elif type_ == PKG_DIRECTORY:\\n            return load_package(name, filename)\\n        elif type_ == C_BUILTIN:\\n            return init_builtin(name)\\n        elif type_ == PY_FROZEN:\\n            return init_frozen(name)\\n        else:\\n            msg =  \"Don't know how to import {} (type code {})\".format(name, type_)\\n            raise ImportError(msg, name=name)"
  },
  {
    "code": "def connect(self):\\n\\t\\tsock = socket.create_connection((self.host, self.port), self.timeout)\\n\\t\\tif self._tunnel_host:\\n\\t\\t\\tself.sock = sock\\n\\t\\t\\tself._tunnel()\\n\\t\\tif self.insecure is True:\\n\\t\\t\\tself.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcert_reqs=ssl.CERT_NONE)\\n\\t\\telse:\\n\\t\\t\\tself.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tca_certs=self.ca_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcert_reqs=ssl.CERT_REQUIRED)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def connect(self):\\n\\t\\tsock = socket.create_connection((self.host, self.port), self.timeout)\\n\\t\\tif self._tunnel_host:\\n\\t\\t\\tself.sock = sock\\n\\t\\t\\tself._tunnel()\\n\\t\\tif self.insecure is True:\\n\\t\\t\\tself.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcert_reqs=ssl.CERT_NONE)\\n\\t\\telse:\\n\\t\\t\\tself.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tca_certs=self.ca_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcert_reqs=ssl.CERT_REQUIRED)"
  },
  {
    "code": "def zeros_like(a, dtype=None, order='K', subok=True, maskna=False):\\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, maskna=maskna)\\n    multiarray.copyto(res, 0, casting='unsafe')\\n    return res",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def zeros_like(a, dtype=None, order='K', subok=True, maskna=False):\\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, maskna=maskna)\\n    multiarray.copyto(res, 0, casting='unsafe')\\n    return res"
  },
  {
    "code": "def _bins_to_cuts(x, bins, right=True, labels=None,\\n                  precision=3, include_lowest=False,\\n                  dtype=None, duplicates='raise'):\\n    if duplicates not in ['raise', 'drop']:\\n        raise ValueError(\"invalid value for 'duplicates' parameter, \"\\n                         \"valid options are: raise, drop\")\\n    unique_bins = algos.unique(bins)\\n    if len(unique_bins) < len(bins):\\n        if duplicates == 'raise':\\n            raise ValueError(\"Bin edges must be unique: {}.\\nYou \"\\n                             \"can drop duplicate edges by setting \"\\n                             \"the 'duplicates' kwarg\".format(repr(bins)))\\n        else:\\n            bins = unique_bins\\n    side = 'left' if right else 'right'\\n    ids = bins.searchsorted(x, side=side)\\n    if include_lowest:\\n        ids[x == bins[0]] = 1\\n    na_mask = isnull(x) | (ids == len(bins)) | (ids == 0)\\n    has_nas = na_mask.any()\\n    if labels is not False:\\n        if labels is None:\\n            increases = 0\\n            while True:\\n                try:\\n                    levels = _format_levels(bins, precision, right=right,\\n                                            include_lowest=include_lowest,\\n                                            dtype=dtype)\\n                except ValueError:\\n                    increases += 1\\n                    precision += 1\\n                    if increases >= 20:\\n                        raise\\n                else:\\n                    break\\n        else:\\n            if len(labels) != len(bins) - 1:\\n                raise ValueError('Bin labels must be one fewer than '\\n                                 'the number of bin edges')\\n            levels = labels\\n        levels = np.asarray(levels, dtype=object)\\n        np.putmask(ids, na_mask, 0)\\n        fac = Categorical(ids - 1, levels, ordered=True, fastpath=True)\\n    else:\\n        fac = ids - 1\\n        if has_nas:\\n            fac = fac.astype(np.float64)\\n            np.putmask(fac, na_mask, np.nan)\\n    return fac, bins",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pd.cut with bins=1 and input all 0s\\n\\nThe special case of running pd.cut() qith bins=1 an input containing\\nall  0s raises a ValueError\\n\\ncloses #15428\\ncloses #15431\\n\\nAuthor: Luca Scarabello <luca.scarabello@sed.ethz.ch>\\nAuthor: Luca <luca.scarabello@gmail.com>\\n\\nCloses #15437 from luca-s/issue_15428 and squashes the following commits:\\n\\n1248987 [Luca] rebased on master\\ndef84ba [Luca] Yet another implementation attempt\\n692503a [Luca Scarabello] Improved solution: using same approach as pd.cut\\nb7d92dc [Luca] Added 'allow' duplicates option to _bins_to_cuts\\nf56a27f [Luca Scarabello] Issue #15431\\n55806cf [Luca Scarabello] BUG: pd.cut with bins=1 and input all 0s",
    "fixed_code": "def _bins_to_cuts(x, bins, right=True, labels=None,\\n                  precision=3, include_lowest=False,\\n                  dtype=None, duplicates='raise'):\\n    if duplicates not in ['raise', 'drop']:\\n        raise ValueError(\"invalid value for 'duplicates' parameter, \"\\n                         \"valid options are: raise, drop\")\\n    unique_bins = algos.unique(bins)\\n    if len(unique_bins) < len(bins) and len(bins) != 2:\\n        if duplicates == 'raise':\\n            raise ValueError(\"Bin edges must be unique: {}.\\nYou \"\\n                             \"can drop duplicate edges by setting \"\\n                             \"the 'duplicates' kwarg\".format(repr(bins)))\\n        else:\\n            bins = unique_bins\\n    side = 'left' if right else 'right'\\n    ids = bins.searchsorted(x, side=side)\\n    if include_lowest:\\n        ids[x == bins[0]] = 1\\n    na_mask = isnull(x) | (ids == len(bins)) | (ids == 0)\\n    has_nas = na_mask.any()\\n    if labels is not False:\\n        if labels is None:\\n            increases = 0\\n            while True:\\n                try:\\n                    levels = _format_levels(bins, precision, right=right,\\n                                            include_lowest=include_lowest,\\n                                            dtype=dtype)\\n                except ValueError:\\n                    increases += 1\\n                    precision += 1\\n                    if increases >= 20:\\n                        raise\\n                else:\\n                    break\\n        else:\\n            if len(labels) != len(bins) - 1:\\n                raise ValueError('Bin labels must be one fewer than '\\n                                 'the number of bin edges')\\n            levels = labels\\n        levels = np.asarray(levels, dtype=object)\\n        np.putmask(ids, na_mask, 0)\\n        fac = Categorical(ids - 1, levels, ordered=True, fastpath=True)\\n    else:\\n        fac = ids - 1\\n        if has_nas:\\n            fac = fac.astype(np.float64)\\n            np.putmask(fac, na_mask, np.nan)\\n    return fac, bins"
  },
  {
    "code": "def _isnull_old(obj):\\n    ''\\n    if lib.isscalar(obj):\\n        return lib.checknull_old(obj)\\n    elif isinstance(obj, pd.MultiIndex):\\n        raise NotImplementedError(\"isnull is not defined for MultiIndex\")\\n    elif isinstance(obj, (ABCSeries, np.ndarray)):\\n        return _isnull_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj.apply(_isnull_old)\\n    elif isinstance(obj, list) or hasattr(obj, '__array__'):\\n        return _isnull_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None\\n_isnull = _isnull_new",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _isnull_old(obj):\\n    ''\\n    if lib.isscalar(obj):\\n        return lib.checknull_old(obj)\\n    elif isinstance(obj, pd.MultiIndex):\\n        raise NotImplementedError(\"isnull is not defined for MultiIndex\")\\n    elif isinstance(obj, (ABCSeries, np.ndarray)):\\n        return _isnull_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj.apply(_isnull_old)\\n    elif isinstance(obj, list) or hasattr(obj, '__array__'):\\n        return _isnull_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None\\n_isnull = _isnull_new"
  },
  {
    "code": "def as_table(self):\\n        \"Returns this form rendered as HTML <tr>s -- excluding the <table></table>.\"\\n        return u'\\n'.join(['<tr><td>%s:</td><td>%s</td></tr>' % (pretty_name(name), BoundField(self, field, name)) for name, field in self.fields.items()])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #3065 -- newforms: Fixed rendering problem with RadioSelect as a member of a Form. Also fixed some Unicode issues and added unit tests. Thanks for reporting, Derek Hoy",
    "fixed_code": "def as_table(self):\\n        \"Returns this form rendered as HTML <tr>s -- excluding the <table></table>.\"\\n        return u'\\n'.join([u'<tr><td>%s:</td><td>%s</td></tr>' % (pretty_name(name), BoundField(self, field, name)) for name, field in self.fields.items()])"
  },
  {
    "code": "def get_parser(cls, dag_parser=False):\\n        parser = argparse.ArgumentParser()\\n        subparsers = parser.add_subparsers(\\n            help='sub-command help', dest='subcommand')\\n        subparsers.required = True\\n        subparser_list = cls.dag_subparsers if dag_parser else cls.subparsers_dict.keys()\\n        for sub in sorted(subparser_list):\\n            sub = cls.subparsers_dict[sub]\\n            sp = subparsers.add_parser(sub['func'].__name__, help=sub['help'])\\n            sp.formatter_class = RawTextHelpFormatter\\n            for arg in sub['args']:\\n                if 'dag_id' in arg and dag_parser:\\n                    continue\\n                arg = cls.args[arg]\\n                kwargs = {\\n                    f: v\\n                    for f, v in vars(arg).items() if f != 'flags' and v}\\n                sp.add_argument(*arg.flags, **kwargs)\\n            sp.set_defaults(func=sub['func'])\\n        return parser",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3998] Use nested commands in cli. (#4821)",
    "fixed_code": "def get_parser(cls, dag_parser=False):\\n        parser = argparse.ArgumentParser()\\n        subparsers = parser.add_subparsers(\\n            help='sub-command help', dest='subcommand')\\n        subparsers.required = True\\n        subparser_list = cls.dag_subparsers if dag_parser else cls.subparsers_dict.keys()\\n        for sub in sorted(subparser_list):\\n            sub = cls.subparsers_dict[sub]\\n            cls._add_subcommand(subparsers, sub)\\n        return parser\\n    @classmethod"
  },
  {
    "code": "def execute(self, context: Dict[Any, Any]) -> None:\\n        hook = DruidHook(druid_ingest_conn_id=self.conn_id, max_ingestion_time=self.max_ingestion_time)\\n        self.log.info(\"Submitting %s\", self.json_index_file)\\n        hook.submit_indexing_job(self.json_index_file)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add timeout parameter to DruidOperator (#19984)",
    "fixed_code": "def execute(self, context: Dict[Any, Any]) -> None:\\n        hook = DruidHook(\\n            druid_ingest_conn_id=self.conn_id,\\n            timeout=self.timeout,\\n            max_ingestion_time=self.max_ingestion_time,\\n        )\\n        self.log.info(\"Submitting %s\", self.json_index_file)\\n        hook.submit_indexing_job(self.json_index_file)"
  },
  {
    "code": "def build_pattern(mapping=MAPPING):\\n    mod_list = ' | '.join([\"module='\" + key + \"'\" for key in mapping.keys()])\\n    mod_name_list = ' | '.join([\"module_name='\" + key + \"'\" for key in mapping.keys()])\\n    yield  % (mod_list, mod_list)\\n    yield  % mod_name_list\\n    yield  % mod_name_list\\n    yield  % mod_name_list\\n    yield  % alternates(mapping.keys())\\nclass FixImports(fixer_base.BaseFix):\\n    PATTERN = \"|\".join(build_pattern())\\n    order = \"pre\" \\n    mapping = MAPPING",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def build_pattern(mapping=MAPPING):\\n    mod_list = ' | '.join([\"module='\" + key + \"'\" for key in mapping.keys()])\\n    mod_name_list = ' | '.join([\"module_name='\" + key + \"'\" for key in mapping.keys()])\\n    yield  % (mod_list, mod_list)\\n    yield  % mod_name_list\\n    yield  % mod_name_list\\n    yield  % mod_name_list\\n    yield  % alternates(mapping.keys())\\nclass FixImports(fixer_base.BaseFix):\\n    PATTERN = \"|\".join(build_pattern())\\n    order = \"pre\" \\n    mapping = MAPPING"
  },
  {
    "code": "def get(self, *args, **kw):\\n        data = self.db.get(*args, **kw)\\n        try:\\n            return pickle.loads(data)\\n        except (TypeError, pickle.UnpicklingError, EOFError):\\n            return data  \\n    def get_both(self, key, value, txn=None, flags=0):\\n        data = pickle.dumps(value, self.binary)\\n        data = self.db.get(key, data, txn, flags)\\n        return pickle.loads(data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix dbshelve and much of dbtables.",
    "fixed_code": "def get(self, key, default=_unspecified, txn=None, flags=0):\\n        if default is _unspecified:\\n            data = self.db.get(key, txn=txn, flags=flags)\\n            default = None\\n        else:\\n            data = self.db.get(key, default, txn=txn, flags=flags)\\n        if data is default:\\n            return data\\n        return pickle.loads(data)\\n    def get_both(self, key, value, txn=None, flags=0):\\n        data = pickle.dumps(value, self.binary)\\n        data = self.db.get(key, data, txn, flags)\\n        return pickle.loads(data)"
  },
  {
    "code": "def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\\n        if self._lambda_arguments and leaf.type == token.COLON:\\n            self.depth -= 1\\n            self._lambda_arguments -= 1\\n            return True\\n        return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix bracket match bug (#470)",
    "fixed_code": "def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\\n        if (\\n            self._lambda_argument_depths\\n            and self._lambda_argument_depths[-1] == self.depth\\n            and leaf.type == token.COLON\\n        ):\\n            self.depth -= 1\\n            self._lambda_argument_depths.pop()\\n            return True\\n        return False"
  },
  {
    "code": "def _deprecate(self, attr: str) -> None:\\n        warnings.warn(\\n            f\"{attr} is not part of the public API, usage can give unexpected \"\\n            \"results and will be removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    def _deprecate_set_book(self) -> None:\\n        warnings.warn(\\n            \"Setting the `book` attribute is not part of the public API, \"\\n            \"usage can give unexpected or corrupted results and will be \"\\n            \"removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _deprecate(self, attr: str) -> None:\\n        warnings.warn(\\n            f\"{attr} is not part of the public API, usage can give unexpected \"\\n            \"results and will be removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    def _deprecate_set_book(self) -> None:\\n        warnings.warn(\\n            \"Setting the `book` attribute is not part of the public API, \"\\n            \"usage can give unexpected or corrupted results and will be \"\\n            \"removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    @property"
  },
  {
    "code": "def unsign(self, signed_value):\\n        signed_value = smart_bytes(signed_value)\\n        if not self.sep in signed_value:\\n            raise BadSignature('No \"%s\" found in value' % self.sep)\\n        value, sig = signed_value.rsplit(self.sep, 1)\\n        if constant_time_compare(sig, self.signature(value)):\\n            return force_text(value)\\n        raise BadSignature('Signature \"%s\" does not match' % sig)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[py3] Made signing infrastructure pass tests with Python 3",
    "fixed_code": "def unsign(self, signed_value):\\n        if not self.sep in signed_value:\\n            raise BadSignature('No \"%s\" found in value' % self.sep)\\n        value, sig = signed_value.rsplit(self.sep, 1)\\n        if constant_time_compare(sig, self.signature(value)):\\n            return value\\n        raise BadSignature('Signature \"%s\" does not match' % sig)"
  },
  {
    "code": "def create_widgets(self):\\n        frame = Frame(self, borderwidth=2, relief=SUNKEN)\\n        frame.pack(side=TOP, expand=True, fill=BOTH)\\n        frame_buttons = Frame(self)\\n        frame_buttons.pack(side=BOTTOM, fill=X)\\n        self.button_ok = Button(frame_buttons, text='OK',\\n                                width=8, command=self.ok)\\n        self.button_ok.grid(row=0, column=0, padx=5, pady=5)\\n        self.button_cancel = Button(frame_buttons, text='Cancel',\\n                                   width=8, command=self.cancel)\\n        self.button_cancel.grid(row=0, column=1, padx=5, pady=5)\\n        self.frame_keyseq_basic = Frame(frame)\\n        self.frame_keyseq_basic.grid(row=0, column=0, sticky=NSEW,\\n                                      padx=5, pady=5)\\n        basic_title = Label(self.frame_keyseq_basic,\\n                            text=f\"New keys for '{self.action}' :\")\\n        basic_title.pack(anchor=W)\\n        basic_keys = Label(self.frame_keyseq_basic, justify=LEFT,\\n                           textvariable=self.key_string, relief=GROOVE,\\n                           borderwidth=2)\\n        basic_keys.pack(ipadx=5, ipady=5, fill=X)\\n        self.frame_controls_basic = Frame(frame)\\n        self.frame_controls_basic.grid(row=1, column=0, sticky=NSEW, padx=5)\\n        self.modifier_checkbuttons = {}\\n        column = 0\\n        for modifier, variable in zip(self.modifiers, self.modifier_vars):\\n            label = self.modifier_label.get(modifier, modifier)\\n            check = Checkbutton(self.frame_controls_basic,\\n                                command=self.build_key_string, text=label,\\n                                variable=variable, onvalue=modifier, offvalue='')\\n            check.grid(row=0, column=column, padx=2, sticky=W)\\n            self.modifier_checkbuttons[modifier] = check\\n            column += 1\\n        help_basic = Label(self.frame_controls_basic, justify=LEFT,\\n                           text=\"Select the desired modifier keys\\n\"+\\n                                \"above, and the final key from the\\n\"+\\n                                \"list on the right.\\n\" +\\n                                \"Use upper case Symbols when using\\n\" +\\n                                \"the Shift modifier.  (Letters will be\\n\" +\\n                                \"converted automatically.)\")\\n        help_basic.grid(row=1, column=0, columnspan=4, padx=2, sticky=W)\\n        self.list_keys_final = Listbox(self.frame_controls_basic, width=15,\\n                                       height=10, selectmode=SINGLE)\\n        self.list_keys_final.bind('<ButtonRelease-1>', self.final_key_selected)\\n        self.list_keys_final.grid(row=0, column=4, rowspan=4, sticky=NS)\\n        scroll_keys_final = Scrollbar(self.frame_controls_basic,\\n                                      orient=VERTICAL,\\n                                      command=self.list_keys_final.yview)\\n        self.list_keys_final.config(yscrollcommand=scroll_keys_final.set)\\n        scroll_keys_final.grid(row=0, column=5, rowspan=4, sticky=NS)\\n        self.button_clear = Button(self.frame_controls_basic,\\n                                   text='Clear Keys',\\n                                   command=self.clear_key_seq)\\n        self.button_clear.grid(row=2, column=0, columnspan=4)\\n        self.frame_keyseq_advanced = Frame(frame)\\n        self.frame_keyseq_advanced.grid(row=0, column=0, sticky=NSEW,\\n                                         padx=5, pady=5)\\n        advanced_title = Label(self.frame_keyseq_advanced, justify=LEFT,\\n                               text=f\"Enter new binding(s) for '{self.action}' :\\n\" +\\n                                     \"(These bindings will not be checked for validity!)\")\\n        advanced_title.pack(anchor=W)\\n        self.advanced_keys = Entry(self.frame_keyseq_advanced,\\n                                   textvariable=self.key_string)\\n        self.advanced_keys.pack(fill=X)\\n        self.frame_help_advanced = Frame(frame)\\n        self.frame_help_advanced.grid(row=1, column=0, sticky=NSEW, padx=5)\\n        help_advanced = Label(self.frame_help_advanced, justify=LEFT,\\n            text=\"Key bindings are specified using Tkinter keysyms as\\n\"+\\n                 \"in these samples: <Control-f>, <Shift-F2>, <F12>,\\n\"\\n                 \"<Control-space>, <Meta-less>, <Control-Alt-Shift-X>.\\n\"\\n                 \"Upper case is used when the Shift modifier is present!\\n\" +\\n                 \"'Emacs style' multi-keystroke bindings are specified as\\n\" +\\n                 \"follows: <Control-x><Control-y>, where the first key\\n\" +\\n                 \"is the 'do-nothing' keybinding.\\n\" +\\n                 \"Multiple separate bindings for one action should be\\n\"+\\n                 \"separated by a space, eg., <Alt-v> <Meta-v>.\" )\\n        help_advanced.grid(row=0, column=0, sticky=NSEW)\\n        self.button_level = Button(frame, command=self.toggle_level,\\n                                  text='<< Basic Key Binding Entry')\\n        self.button_level.grid(row=2, column=0, stick=EW, padx=5, pady=5)\\n        self.toggle_level()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-35598: IDLE: Increase test coverage for config_key.py (#11360)",
    "fixed_code": "def create_widgets(self):\\n        self.frame = frame = Frame(self, borderwidth=2, relief=SUNKEN)\\n        frame.pack(side=TOP, expand=True, fill=BOTH)\\n        frame_buttons = Frame(self)\\n        frame_buttons.pack(side=BOTTOM, fill=X)\\n        self.button_ok = Button(frame_buttons, text='OK',\\n                                width=8, command=self.ok)\\n        self.button_ok.grid(row=0, column=0, padx=5, pady=5)\\n        self.button_cancel = Button(frame_buttons, text='Cancel',\\n                                   width=8, command=self.cancel)\\n        self.button_cancel.grid(row=0, column=1, padx=5, pady=5)\\n        self.frame_keyseq_basic = Frame(frame, name='keyseq_basic')\\n        self.frame_keyseq_basic.grid(row=0, column=0, sticky=NSEW,\\n                                      padx=5, pady=5)\\n        basic_title = Label(self.frame_keyseq_basic,\\n                            text=f\"New keys for '{self.action}' :\")\\n        basic_title.pack(anchor=W)\\n        basic_keys = Label(self.frame_keyseq_basic, justify=LEFT,\\n                           textvariable=self.key_string, relief=GROOVE,\\n                           borderwidth=2)\\n        basic_keys.pack(ipadx=5, ipady=5, fill=X)\\n        self.frame_controls_basic = Frame(frame)\\n        self.frame_controls_basic.grid(row=1, column=0, sticky=NSEW, padx=5)\\n        self.modifier_checkbuttons = {}\\n        column = 0\\n        for modifier, variable in zip(self.modifiers, self.modifier_vars):\\n            label = self.modifier_label.get(modifier, modifier)\\n            check = Checkbutton(self.frame_controls_basic,\\n                                command=self.build_key_string, text=label,\\n                                variable=variable, onvalue=modifier, offvalue='')\\n            check.grid(row=0, column=column, padx=2, sticky=W)\\n            self.modifier_checkbuttons[modifier] = check\\n            column += 1\\n        help_basic = Label(self.frame_controls_basic, justify=LEFT,\\n                           text=\"Select the desired modifier keys\\n\"+\\n                                \"above, and the final key from the\\n\"+\\n                                \"list on the right.\\n\" +\\n                                \"Use upper case Symbols when using\\n\" +\\n                                \"the Shift modifier.  (Letters will be\\n\" +\\n                                \"converted automatically.)\")\\n        help_basic.grid(row=1, column=0, columnspan=4, padx=2, sticky=W)\\n        self.list_keys_final = Listbox(self.frame_controls_basic, width=15,\\n                                       height=10, selectmode=SINGLE)\\n        self.list_keys_final.bind('<ButtonRelease-1>', self.final_key_selected)\\n        self.list_keys_final.grid(row=0, column=4, rowspan=4, sticky=NS)\\n        scroll_keys_final = Scrollbar(self.frame_controls_basic,\\n                                      orient=VERTICAL,\\n                                      command=self.list_keys_final.yview)\\n        self.list_keys_final.config(yscrollcommand=scroll_keys_final.set)\\n        scroll_keys_final.grid(row=0, column=5, rowspan=4, sticky=NS)\\n        self.button_clear = Button(self.frame_controls_basic,\\n                                   text='Clear Keys',\\n                                   command=self.clear_key_seq)\\n        self.button_clear.grid(row=2, column=0, columnspan=4)\\n        self.frame_keyseq_advanced = Frame(frame, name='keyseq_advanced')\\n        self.frame_keyseq_advanced.grid(row=0, column=0, sticky=NSEW,\\n                                         padx=5, pady=5)\\n        advanced_title = Label(self.frame_keyseq_advanced, justify=LEFT,\\n                               text=f\"Enter new binding(s) for '{self.action}' :\\n\" +\\n                                     \"(These bindings will not be checked for validity!)\")\\n        advanced_title.pack(anchor=W)\\n        self.advanced_keys = Entry(self.frame_keyseq_advanced,\\n                                   textvariable=self.key_string)\\n        self.advanced_keys.pack(fill=X)\\n        self.frame_help_advanced = Frame(frame)\\n        self.frame_help_advanced.grid(row=1, column=0, sticky=NSEW, padx=5)\\n        help_advanced = Label(self.frame_help_advanced, justify=LEFT,\\n            text=\"Key bindings are specified using Tkinter keysyms as\\n\"+\\n                 \"in these samples: <Control-f>, <Shift-F2>, <F12>,\\n\"\\n                 \"<Control-space>, <Meta-less>, <Control-Alt-Shift-X>.\\n\"\\n                 \"Upper case is used when the Shift modifier is present!\\n\" +\\n                 \"'Emacs style' multi-keystroke bindings are specified as\\n\" +\\n                 \"follows: <Control-x><Control-y>, where the first key\\n\" +\\n                 \"is the 'do-nothing' keybinding.\\n\" +\\n                 \"Multiple separate bindings for one action should be\\n\"+\\n                 \"separated by a space, eg., <Alt-v> <Meta-v>.\" )\\n        help_advanced.grid(row=0, column=0, sticky=NSEW)\\n        self.button_level = Button(frame, command=self.toggle_level,\\n                                  text='<< Basic Key Binding Entry')\\n        self.button_level.grid(row=2, column=0, stick=EW, padx=5, pady=5)\\n        self.toggle_level()"
  },
  {
    "code": "def test_graphviz_toy():\\n\\tclf = DecisionTreeClassifier(max_depth=3,\\n\\t\\t\\t\\t\\t\\t\\t\\t min_samples_split=1,\\n\\t\\t\\t\\t\\t\\t\\t\\t criterion=\"gini\",\\n\\t\\t\\t\\t\\t\\t\\t\\t random_state=1)\\n\\tclf.fit(X, y)\\n\\tout = StringIO()\\n\\texport_graphviz(clf, out_file=out)\\n\\tcontents1 = out.getvalue()\\n\\tcontents2 = \"digraph Tree {\\n\" \\\\n\\t\\t\\t\\t\"0 [label=\\\"X[0] <= 0.0000\\\\ngini = 0.5\\\\n\" \\\\n\\t\\t\\t\\t\"samples = 6\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"1 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 3.  0.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 1 ;\\n\" \\\\n\\t\\t\\t\\t\"2 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 0.  3.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 2 ;\\n\" \\\\n\\t\\t\\t\\t\"}\"\\n\\tassert_equal(contents1, contents2)\\n\\tout = StringIO()\\n\\tout = export_graphviz(clf, out_file=out, feature_names=[\"feature0\", \"feature1\"])\\n\\tcontents1 = out.getvalue()\\n\\tcontents2 = \"digraph Tree {\\n\" \\\\n\\t\\t\\t\\t\"0 [label=\\\"feature0 <= 0.0000\\\\ngini = 0.5\\\\n\" \\\\n\\t\\t\\t\\t\"samples = 6\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"1 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 3.  0.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 1 ;\\n\" \\\\n\\t\\t\\t\\t\"2 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 0.  3.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 2 ;\\n\" \\\\n\\t\\t\\t\\t\"}\"\\n\\tassert_equal(contents1, contents2)\\n\\tout = StringIO()\\n\\texport_graphviz(clf, out_file=out, max_depth=0)\\n\\tcontents1 = out.getvalue()\\n\\tcontents2 = \"digraph Tree {\\n\" \\\\n\\t\\t\\t\\t\"0 [label=\\\"X[0] <= 0.0000\\\\ngini = 0.5\\\\n\" \\\\n\\t\\t\\t\\t\"samples = 6\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"1 [label=\\\"(...)\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 1 ;\\n\" \\\\n\\t\\t\\t\\t\"2 [label=\\\"(...)\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 2 ;\\n\" \\\\n\\t\\t\\t\\t\"}\"\\n\\tassert_equal(contents1, contents2)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_graphviz_toy():\\n\\tclf = DecisionTreeClassifier(max_depth=3,\\n\\t\\t\\t\\t\\t\\t\\t\\t min_samples_split=1,\\n\\t\\t\\t\\t\\t\\t\\t\\t criterion=\"gini\",\\n\\t\\t\\t\\t\\t\\t\\t\\t random_state=1)\\n\\tclf.fit(X, y)\\n\\tout = StringIO()\\n\\texport_graphviz(clf, out_file=out)\\n\\tcontents1 = out.getvalue()\\n\\tcontents2 = \"digraph Tree {\\n\" \\\\n\\t\\t\\t\\t\"0 [label=\\\"X[0] <= 0.0000\\\\ngini = 0.5\\\\n\" \\\\n\\t\\t\\t\\t\"samples = 6\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"1 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 3.  0.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 1 ;\\n\" \\\\n\\t\\t\\t\\t\"2 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 0.  3.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 2 ;\\n\" \\\\n\\t\\t\\t\\t\"}\"\\n\\tassert_equal(contents1, contents2)\\n\\tout = StringIO()\\n\\tout = export_graphviz(clf, out_file=out, feature_names=[\"feature0\", \"feature1\"])\\n\\tcontents1 = out.getvalue()\\n\\tcontents2 = \"digraph Tree {\\n\" \\\\n\\t\\t\\t\\t\"0 [label=\\\"feature0 <= 0.0000\\\\ngini = 0.5\\\\n\" \\\\n\\t\\t\\t\\t\"samples = 6\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"1 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 3.  0.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 1 ;\\n\" \\\\n\\t\\t\\t\\t\"2 [label=\\\"gini = 0.0000\\\\nsamples = 3\\\\n\" \\\\n\\t\\t\\t\\t\"value = [ 0.  3.]\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 2 ;\\n\" \\\\n\\t\\t\\t\\t\"}\"\\n\\tassert_equal(contents1, contents2)\\n\\tout = StringIO()\\n\\texport_graphviz(clf, out_file=out, max_depth=0)\\n\\tcontents1 = out.getvalue()\\n\\tcontents2 = \"digraph Tree {\\n\" \\\\n\\t\\t\\t\\t\"0 [label=\\\"X[0] <= 0.0000\\\\ngini = 0.5\\\\n\" \\\\n\\t\\t\\t\\t\"samples = 6\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"1 [label=\\\"(...)\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 1 ;\\n\" \\\\n\\t\\t\\t\\t\"2 [label=\\\"(...)\\\", shape=\\\"box\\\"] ;\\n\" \\\\n\\t\\t\\t\\t\"0 -> 2 ;\\n\" \\\\n\\t\\t\\t\\t\"}\"\\n\\tassert_equal(contents1, contents2)"
  },
  {
    "code": "def __init__(self, data=None, files=None, auto_id='id_%s', prefix=None,\\n                 initial=None, error_class=ErrorList, label_suffix=None,\\n                 empty_permitted=False, field_order=None):\\n        self.is_bound = data is not None or files is not None\\n        self.data = data or {}\\n        self.files = files or {}\\n        self.auto_id = auto_id\\n        if prefix is not None:\\n            self.prefix = prefix\\n        self.initial = initial or {}\\n        self.error_class = error_class\\n        self.label_suffix = label_suffix if label_suffix is not None else _(':')\\n        self.empty_permitted = empty_permitted\\n        self._errors = None  \\n        self._changed_data = None\\n        self.fields = copy.deepcopy(self.base_fields)\\n        self._bound_fields_cache = {}\\n        self.order_fields(self.field_order if field_order is None else field_order)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, data=None, files=None, auto_id='id_%s', prefix=None,\\n                 initial=None, error_class=ErrorList, label_suffix=None,\\n                 empty_permitted=False, field_order=None):\\n        self.is_bound = data is not None or files is not None\\n        self.data = data or {}\\n        self.files = files or {}\\n        self.auto_id = auto_id\\n        if prefix is not None:\\n            self.prefix = prefix\\n        self.initial = initial or {}\\n        self.error_class = error_class\\n        self.label_suffix = label_suffix if label_suffix is not None else _(':')\\n        self.empty_permitted = empty_permitted\\n        self._errors = None  \\n        self._changed_data = None\\n        self.fields = copy.deepcopy(self.base_fields)\\n        self._bound_fields_cache = {}\\n        self.order_fields(self.field_order if field_order is None else field_order)"
  },
  {
    "code": "def get_avatar_urls_uncached(self, user, size):\\n\\t\\treturn {\\n\\t\\t\\t'%dx' % resolution: mark_safe(get_gravatar_url_for_email(\\n\\t\\t\\t\\temail=user.email, size=(size * resolution)))\\n\\t\\t\\tfor resolution in (1, 2, 3)\\n\\t\\t}",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Prevent gravatars from exploding if users don't have an email address.\\n\\nThe recent changes to the gravatar backend were causing uncaught\\nValueErrors to percolate upwards for users with empty email addresses,\\nbreaking a variety of things. Not having an avatar shouldn't be a fatal\\ncondition, so I've changed it to return an empty dict instead.\\n\\nTesting done:\\n- Ran unit tests.\\n- Tested in conjunction with some other stuff that was previously\\n  breaking.",
    "fixed_code": "def get_avatar_urls_uncached(self, user, size):\\n\\t\\tif user.email:\\n\\t\\t\\treturn {\\n\\t\\t\\t\\t'%dx' % resolution: mark_safe(get_gravatar_url_for_email(\\n\\t\\t\\t\\t\\temail=user.email, size=(size * resolution)))\\n\\t\\t\\t\\tfor resolution in (1, 2, 3)\\n\\t\\t\\t}\\n\\t\\telse:\\n\\t\\t\\treturn {}"
  },
  {
    "code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    _validate_flavor_parameter(flavor)\\n    con = _engine_builder(con)\\n    if _is_sqlalchemy_connectable(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    else:\\n        return SQLiteDatabase(con, is_cursor=is_cursor)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: raise ImporError if conn is string and sqlalchemy not installed (#11920)",
    "fixed_code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    _validate_flavor_parameter(flavor)\\n    con = _engine_builder(con)\\n    if _is_sqlalchemy_connectable(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    elif isinstance(con, string_types):\\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\\n    else:\\n        return SQLiteDatabase(con, is_cursor=is_cursor)"
  },
  {
    "code": "def asfreq(self, freq, fillMethod=None):\\n        if isinstance(freq, datetools.DateOffset):\\n            dateRange = DateRange(self.index[0], self.index[-1], offset=freq)\\n        else:\\n            dateRange = DateRange(self.index[0], self.index[-1], timeRule=freq)\\n        return self.reindex(dateRange, fillMethod=fillMethod)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def asfreq(self, freq, fillMethod=None):\\n        if isinstance(freq, datetools.DateOffset):\\n            dateRange = DateRange(self.index[0], self.index[-1], offset=freq)\\n        else:\\n            dateRange = DateRange(self.index[0], self.index[-1], timeRule=freq)\\n        return self.reindex(dateRange, fillMethod=fillMethod)"
  },
  {
    "code": "def _get_valid_mysql_name(name):\\n    uname = _get_unicode_name(name)\\n    if not len(uname):\\n        raise ValueError(\"Empty table or column name specified\")\\n    basere = r'[0-9,a-z,A-Z$_]'\\n    for c in uname:\\n        if not re.match(basere, c):\\n            if not (0x80 < ord(c) < 0xFFFF):\\n                raise ValueError(\"Invalid MySQL identifier '%s'\" % uname)\\n    if not re.match(r'[^0-9]', uname):\\n        raise ValueError('MySQL identifier cannot be entirely numeric')\\n    return '`' + uname + '`'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Should allow numeric mysql table/column names\\n\\nDoc fix",
    "fixed_code": "def _get_valid_mysql_name(name):\\n    uname = _get_unicode_name(name)\\n    if not len(uname):\\n        raise ValueError(\"Empty table or column name specified\")\\n    basere = r'[0-9,a-z,A-Z$_]'\\n    for c in uname:\\n        if not re.match(basere, c):\\n            if not (0x80 < ord(c) < 0xFFFF):\\n                raise ValueError(\"Invalid MySQL identifier '%s'\" % uname)\\n    return '`' + uname + '`'"
  },
  {
    "code": "def mklogical(dict):\\n    return aetypes.Logical(dict[b2i(b'logc')], dict[b2i(b'term')])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mklogical(dict):\\n    return aetypes.Logical(dict[b2i(b'logc')], dict[b2i(b'term')])"
  },
  {
    "code": "def _resolution(self):\\n        return libperiod.resolution(self.asi8, self.tz)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Implement get_day_of_year, tests (#19555)",
    "fixed_code": "def _resolution(self):\\n        return libresolution.resolution(self.asi8, self.tz)"
  },
  {
    "code": "def replace(self, to_replace, value, inplace=False, filter=None,\\n\\t\\t\\t\\tregex=False, convert=True, mgr=None):\\n\\t\\toriginal_to_replace = to_replace\\n\\t\\ttry:\\n\\t\\t\\tvalues, _, to_replace, _ = self._try_coerce_args(self.values, to_replace)\\n\\t\\t\\tmask = com.mask_missing(values, to_replace)\\n\\t\\t\\tif filter is not None:\\n\\t\\t\\t\\tfiltered_out = ~self.mgr_locs.isin(filter)\\n\\t\\t\\t\\tmask[filtered_out.nonzero()[0]] = False\\n\\t\\t\\tblocks = self.putmask(mask, value, inplace=inplace)\\n\\t\\t\\tif convert:\\n\\t\\t\\t\\tblocks = [ b.convert(by_item=True, numeric=False, copy=not inplace) for b in blocks ]\\n\\t\\t\\treturn blocks\\n\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\tif not mask.any():\\n\\t\\t\\t\\treturn self if inplace else self.copy()\\n\\t\\t\\treturn self.to_object_block(mgr=mgr).replace(to_replace=original_to_replace,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t value=value,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t inplace=inplace,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t filter=filter,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t regex=regex,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t convert=convert)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes GH11698. added default value of mask and a test case. #11698",
    "fixed_code": "def replace(self, to_replace, value, inplace=False, filter=None,\\n\\t\\t\\t\\tregex=False, convert=True, mgr=None):\\n\\t\\toriginal_to_replace = to_replace\\n\\t\\tmask = isnull(self.values)\\n\\t\\ttry:\\n\\t\\t\\tvalues, _, to_replace, _ = self._try_coerce_args(self.values, to_replace)\\n\\t\\t\\tmask = com.mask_missing(values, to_replace)\\n\\t\\t\\tif filter is not None:\\n\\t\\t\\t\\tfiltered_out = ~self.mgr_locs.isin(filter)\\n\\t\\t\\t\\tmask[filtered_out.nonzero()[0]] = False\\n\\t\\t\\tblocks = self.putmask(mask, value, inplace=inplace)\\n\\t\\t\\tif convert:\\n\\t\\t\\t\\tblocks = [ b.convert(by_item=True, numeric=False, copy=not inplace) for b in blocks ]\\n\\t\\t\\treturn blocks\\n\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\tif not mask.any():\\n\\t\\t\\t\\treturn self if inplace else self.copy()\\n\\t\\t\\treturn self.to_object_block(mgr=mgr).replace(to_replace=original_to_replace,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t value=value,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t inplace=inplace,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t filter=filter,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t regex=regex,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t convert=convert)"
  },
  {
    "code": "def _get_time_bins(self, ax):\\n\\t\\tif not isinstance(ax, DatetimeIndex):\\n\\t\\t\\traise TypeError('axis must be a DatetimeIndex, but got '\\n\\t\\t\\t\\t\\t\\t\\t'an instance of %r' % type(ax).__name__)\\n\\t\\tif len(ax) == 0:\\n\\t\\t\\tbinner = labels = DatetimeIndex(\\n\\t\\t\\t\\tdata=[], freq=self.freq, name=ax.name)\\n\\t\\t\\treturn binner, [], labels\\n\\t\\tfirst, last = ax.min(), ax.max()\\n\\t\\tfirst, last = _get_range_edges(first, last, self.freq,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   closed=self.closed,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   base=self.base)\\n\\t\\ttz = ax.tz\\n\\t\\tbinner = labels = DatetimeIndex(freq=self.freq,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tstart=first.replace(tzinfo=None),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend=last.replace(tzinfo=None),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttz=tz,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tname=ax.name)\\n\\t\\ttrimmed = False\\n\\t\\tif (len(binner) > 2 and binner[-2] == last and\\n\\t\\t\\t\\tself.closed == 'right'):\\n\\t\\t\\tbinner = binner[:-1]\\n\\t\\t\\ttrimmed = True\\n\\t\\tax_values = ax.asi8\\n\\t\\tbinner, bin_edges = self._adjust_bin_edges(binner, ax_values)\\n\\t\\tbins = lib.generate_bins_dt64(\\n\\t\\t\\tax_values, bin_edges, self.closed, hasnans=ax.hasnans)\\n\\t\\tif self.closed == 'right':\\n\\t\\t\\tlabels = binner\\n\\t\\t\\tif self.label == 'right':\\n\\t\\t\\t\\tlabels = labels[1:]\\n\\t\\t\\telif not trimmed:\\n\\t\\t\\t\\tlabels = labels[:-1]\\n\\t\\telse:\\n\\t\\t\\tif self.label == 'right':\\n\\t\\t\\t\\tlabels = labels[1:]\\n\\t\\t\\telif not trimmed:\\n\\t\\t\\t\\tlabels = labels[:-1]\\n\\t\\tif ax.hasnans:\\n\\t\\t\\tbinner = binner.insert(0, tslib.NaT)\\n\\t\\t\\tlabels = labels.insert(0, tslib.NaT)\\n\\t\\tif len(bins) < len(labels):\\n\\t\\t\\tlabels = labels[:len(bins)]\\n\\t\\treturn binner, bins, labels",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix #12037 Error when Resampling using pd.tseries.offsets.Nano as period\\n\\nCloses #12037\\n\\nAuthor: Bran Yang <snowolfy@163.com>\\n\\nCloses #12270 from BranYang/nanosec and squashes the following commits:\\n\\nbff0c85 [Bran Yang] Add to whatsnew and some comments\\nfd0b307 [Bran Yang] Fix #12037 Error when Resampling using pd.tseries.offsets.Nano as period",
    "fixed_code": "def _get_time_bins(self, ax):\\n\\t\\tif not isinstance(ax, DatetimeIndex):\\n\\t\\t\\traise TypeError('axis must be a DatetimeIndex, but got '\\n\\t\\t\\t\\t\\t\\t\\t'an instance of %r' % type(ax).__name__)\\n\\t\\tif len(ax) == 0:\\n\\t\\t\\tbinner = labels = DatetimeIndex(\\n\\t\\t\\t\\tdata=[], freq=self.freq, name=ax.name)\\n\\t\\t\\treturn binner, [], labels\\n\\t\\tfirst, last = ax.min(), ax.max()\\n\\t\\tfirst, last = _get_range_edges(first, last, self.freq,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   closed=self.closed,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   base=self.base)\\n\\t\\ttz = ax.tz\\n\\t\\tbinner = labels = DatetimeIndex(freq=self.freq,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tstart=first,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend=last,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttz=tz,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tname=ax.name)\\n\\t\\ttrimmed = False\\n\\t\\tif (len(binner) > 2 and binner[-2] == last and\\n\\t\\t\\t\\tself.closed == 'right'):\\n\\t\\t\\tbinner = binner[:-1]\\n\\t\\t\\ttrimmed = True\\n\\t\\tax_values = ax.asi8\\n\\t\\tbinner, bin_edges = self._adjust_bin_edges(binner, ax_values)\\n\\t\\tbins = lib.generate_bins_dt64(\\n\\t\\t\\tax_values, bin_edges, self.closed, hasnans=ax.hasnans)\\n\\t\\tif self.closed == 'right':\\n\\t\\t\\tlabels = binner\\n\\t\\t\\tif self.label == 'right':\\n\\t\\t\\t\\tlabels = labels[1:]\\n\\t\\t\\telif not trimmed:\\n\\t\\t\\t\\tlabels = labels[:-1]\\n\\t\\telse:\\n\\t\\t\\tif self.label == 'right':\\n\\t\\t\\t\\tlabels = labels[1:]\\n\\t\\t\\telif not trimmed:\\n\\t\\t\\t\\tlabels = labels[:-1]\\n\\t\\tif ax.hasnans:\\n\\t\\t\\tbinner = binner.insert(0, tslib.NaT)\\n\\t\\t\\tlabels = labels.insert(0, tslib.NaT)\\n\\t\\tif len(bins) < len(labels):\\n\\t\\t\\tlabels = labels[:len(bins)]\\n\\t\\treturn binner, bins, labels"
  },
  {
    "code": "def truncate(self, before=None, after=None, periods=None):\\n        beg_slice, end_slice = self._getIndices(before, after)\\n        return self[beg_slice:end_slice]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def truncate(self, before=None, after=None, periods=None):\\n        beg_slice, end_slice = self._getIndices(before, after)\\n        return self[beg_slice:end_slice]"
  },
  {
    "code": "def as_string(self, unixfrom=False):\\n        fp = six.StringIO()\\n        g = Generator(fp, mangle_from_=False)\\n        g.flatten(self, unixfrom=unixfrom)\\n        return fp.getvalue()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_string(self, unixfrom=False):\\n        fp = six.StringIO()\\n        g = Generator(fp, mangle_from_=False)\\n        g.flatten(self, unixfrom=unixfrom)\\n        return fp.getvalue()"
  },
  {
    "code": "def read_text(self, *args, **kwargs):\\n        kwargs[\"encoding\"] = io.text_encoding(kwargs.get(\"encoding\"))\\n        with self.open('r', *args, **kwargs) as strm:\\n            return strm.read()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_text(self, *args, **kwargs):\\n        kwargs[\"encoding\"] = io.text_encoding(kwargs.get(\"encoding\"))\\n        with self.open('r', *args, **kwargs) as strm:\\n            return strm.read()"
  },
  {
    "code": "def as_oracle(self, compiler, connection, **extra_context):\\n        tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\\n        template = None if self.is_extent else '%(function)s(SDOAGGRTYPE(%(expressions)s,%(tolerance)s))'\\n        return self.as_sql(compiler, connection, template=template, tolerance=tolerance, **extra_context)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed CVE-2020-9402 -- Properly escaped tolerance parameter in GIS functions and aggregates on Oracle.\\n\\nThanks to Norbert Szetei for the report.",
    "fixed_code": "def as_oracle(self, compiler, connection, **extra_context):\\n        if not self.is_extent:\\n            tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\\n            clone = self.copy()\\n            clone.set_source_expressions([\\n                *self.get_source_expressions(),\\n                Value(tolerance),\\n            ])\\n            template = '%(function)s(SDOAGGRTYPE(%(expressions)s))'\\n            return clone.as_sql(compiler, connection, template=template, **extra_context)\\n        return self.as_sql(compiler, connection, **extra_context)"
  },
  {
    "code": "def collect(self, objs, source_attr=None, **kwargs):\\n        for obj in objs:\\n            if source_attr:\\n                self.add_edge(getattr(obj, source_attr), obj)\\n            else:\\n                self.add_edge(None, obj)\\n        try:\\n            return super(NestedObjects, self).collect(objs, source_attr=source_attr, **kwargs)\\n        except models.ProtectedError as e:\\n            self.protected.update(e.protected_objects)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #21846 -- Made NestedObjects handle related_name with %(app_label)s or %(class)s.",
    "fixed_code": "def collect(self, objs, source=None, source_attr=None, **kwargs):\\n        for obj in objs:\\n            if source_attr:\\n                related_name = source_attr % {\\n                    'class': source._meta.model_name,\\n                    'app_label': source._meta.app_label,\\n                }\\n                self.add_edge(getattr(obj, related_name), obj)\\n            else:\\n                self.add_edge(None, obj)\\n        try:\\n            return super(NestedObjects, self).collect(objs, source_attr=source_attr, **kwargs)\\n        except models.ProtectedError as e:\\n            self.protected.update(e.protected_objects)"
  },
  {
    "code": "def _start_dataflow(self, variables, name, command_prefix, label_formatter):\\n        variables = self._set_variables(variables)\\n        cmd = command_prefix + self._build_cmd(variables, label_formatter)\\n        job_id = _Dataflow(cmd).wait_for_done()\\n        _DataflowJob(self.get_conn(), variables['project'], name,\\n                     variables['region'],\\n                     self.poll_sleep, job_id,\\n                     self.num_retries).wait_for_done()\\n    @staticmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "AIRFLOW-3791: Dataflow - Support check status if pipeline spans on multiple jobs (#4633)\\n\\nSupport to check if job is already running before starting java job\\nIn case dataflow creates more than one job, we need to track all jobs for status\\n\\nSupport to check if job is already running before starting java job\\nIn case dataflow creates more than one job, we need to track all jobs for status\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nchange default for check if running\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name",
    "fixed_code": "def _start_dataflow(self, variables, name, command_prefix, label_formatter, multiple_jobs=False):\\n        variables = self._set_variables(variables)\\n        cmd = command_prefix + self._build_cmd(variables, label_formatter)\\n        job_id = _Dataflow(cmd).wait_for_done()\\n        _DataflowJob(self.get_conn(), variables['project'], name,\\n                     variables['region'], self.poll_sleep, job_id, self.num_retries, multiple_jobs) \\\\n            .wait_for_done()\\n    @staticmethod"
  },
  {
    "code": "def sum(self, axis=0, *args, **kwargs):\\n        nv.validate_sum(args, kwargs)\\n        valid_vals = self._valid_sp_values\\n        sp_sum = valid_vals.sum()\\n        if self._null_fill_value:\\n            return sp_sum\\n        else:\\n            nsparse = self.sp_index.ngaps\\n            return sp_sum + self.fill_value * nsparse",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Summing a sparse boolean series raises TypeError (#34220)",
    "fixed_code": "def sum(self, axis: int = 0, min_count: int = 0, *args, **kwargs) -> Scalar:\\n        nv.validate_sum(args, kwargs)\\n        valid_vals = self._valid_sp_values\\n        sp_sum = valid_vals.sum()\\n        if self._null_fill_value:\\n            if check_below_min_count(valid_vals.shape, None, min_count):\\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\\n            return sp_sum\\n        else:\\n            nsparse = self.sp_index.ngaps\\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\\n            return sp_sum + self.fill_value * nsparse"
  },
  {
    "code": "def user_exception(self, frame, exc_info):\\n        exc_type, exc_value, exc_traceback = exc_info\\n        frame.f_locals['__exception__'] = exc_type, exc_value\\n        if type(exc_type) == type(''):\\n            exc_type_name = exc_type\\n        else: exc_type_name = exc_type.__name__\\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\\n        self.interaction(frame, exc_traceback)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 83259,83261,83264-83265,83268-83269,83271-83272,83281 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n........\\n  r83259 | georg.brandl | 2010-07-30 09:03:39 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  Clarification.\\n........\\n  r83261 | georg.brandl | 2010-07-30 09:21:26 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  #9230: allow Pdb.checkline() to be called without a current frame, for setting breakpoints before starting debugging.\\n........\\n  r83264 | georg.brandl | 2010-07-30 10:45:26 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  Document the \"jump\" command in pdb.__doc__, and add a version tag for \"until X\".\\n........\\n  r83265 | georg.brandl | 2010-07-30 10:54:49 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  #8015: fix crash when entering an empty line for breakpoint commands.  Also restore environment properly when an exception occurs during the definition of commands.\\n........\\n  r83268 | georg.brandl | 2010-07-30 11:23:23 +0200 (Fr, 30 Jul 2010) | 2 lines\\n\\n  Issue #8048: Prevent doctests from failing when sys.displayhook has\\n  been reassigned.\\n........\\n  r83269 | georg.brandl | 2010-07-30 11:43:00 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  #6719: In pdb, do not stop somewhere in the encodings machinery if the source file to be debugged is in a non-builtin encoding.\\n........\\n  r83271 | georg.brandl | 2010-07-30 11:59:28 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  #5727: Restore the ability to use readline when calling into pdb in doctests.\\n........\\n  r83272 | georg.brandl | 2010-07-30 12:29:19 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  #5294: Fix the behavior of pdb \"continue\" command when called in the top-level debugged frame.\\n........\\n  r83281 | georg.brandl | 2010-07-30 15:36:43 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n  Add myself for pdb.\\n........",
    "fixed_code": "def user_exception(self, frame, exc_info):\\n        if self._wait_for_mainpyfile:\\n            return\\n        exc_type, exc_value, exc_traceback = exc_info\\n        frame.f_locals['__exception__'] = exc_type, exc_value\\n        if type(exc_type) == type(''):\\n            exc_type_name = exc_type\\n        else: exc_type_name = exc_type.__name__\\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\\n        self.interaction(frame, exc_traceback)"
  },
  {
    "code": "def check_password(password, encoded, setter=None, preferred='default'):\\n    if password is None or not is_password_usable(encoded):\\n        return False\\n    preferred = get_hasher(preferred)\\n    hasher = identify_hasher(encoded)\\n    must_update = hasher.algorithm != preferred.algorithm\\n    is_correct = hasher.verify(password, encoded)\\n    if setter and is_correct and must_update:\\n        setter(password)\\n    return is_correct",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Force update of the password on iteration count changes.",
    "fixed_code": "def check_password(password, encoded, setter=None, preferred='default'):\\n    if password is None or not is_password_usable(encoded):\\n        return False\\n    preferred = get_hasher(preferred)\\n    hasher = identify_hasher(encoded)\\n    must_update = hasher.algorithm != preferred.algorithm\\n    if not must_update:\\n        must_update = hasher.must_update(encoded)\\n    is_correct = hasher.verify(password, encoded)\\n    if setter and is_correct and must_update:\\n        setter(password)\\n    return is_correct"
  },
  {
    "code": "def cleanse_setting(key, value):\\n    try:\\n        if HIDDEN_SETTINGS.search(key):\\n            cleansed = CLEANSED_SUBSTITUTE\\n        else:\\n            if isinstance(value, dict):\\n                cleansed = dict((k, cleanse_setting(k, v)) for k, v in value.items())\\n            else:\\n                cleansed = value\\n    except TypeError:\\n        cleansed = value\\n    if callable(cleansed):\\n        cleansed.do_not_call_in_templates = True\\n    return cleansed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed code to solve #23070 problem\\n\\nAdded a class to wrap callable in settings:\\n\\nThanks @bmispelon for giving me some advice.",
    "fixed_code": "def cleanse_setting(key, value):\\n    try:\\n        if HIDDEN_SETTINGS.search(key):\\n            cleansed = CLEANSED_SUBSTITUTE\\n        else:\\n            if isinstance(value, dict):\\n                cleansed = dict((k, cleanse_setting(k, v)) for k, v in value.items())\\n            else:\\n                cleansed = value\\n    except TypeError:\\n        cleansed = value\\n    if callable(cleansed):\\n        cleansed = CallableSettingWrapper(cleansed)\\n    return cleansed"
  },
  {
    "code": "def build_extensions(self):\\n        self.check_extensions_list(self.extensions)\\n        for ext in self.extensions:\\n            try:\\n                self.build_extension(ext)\\n            except (CCompilerError, DistutilsError, CompileError), e:\\n                if not ext.optional:\\n                    raise\\n                self.warn('building extension \"%s\" failed: %s' %\\n                          (ext.name, e))\\n    def build_extension(self, ext):\\n        sources = ext.sources\\n        if sources is None or not isinstance(sources, (list, tuple)):\\n            raise DistutilsSetupError, \\\\n                  (\"in 'ext_modules' option (extension '%s'), \" +\\n                   \"'sources' must be present and must be \" +\\n                   \"a list of source filenames\") % ext.name\\n        sources = list(sources)\\n        ext_path = self.get_ext_fullpath(ext.name)\\n        depends = sources + ext.depends\\n        if not (self.force or newer_group(depends, ext_path, 'newer')):\\n            log.debug(\"skipping '%s' extension (up-to-date)\", ext.name)\\n            return\\n        else:\\n            log.info(\"building '%s' extension\", ext.name)\\n        sources = self.swig_sources(sources, ext)\\n        extra_args = ext.extra_compile_args or []\\n        macros = ext.define_macros[:]\\n        for undef in ext.undef_macros:\\n            macros.append((undef,))\\n        objects = self.compiler_obj.compile(sources,\\n                                            output_dir=self.build_temp,\\n                                            macros=macros,\\n                                            include_dirs=ext.include_dirs,\\n                                            debug=self.debug,\\n                                            extra_postargs=extra_args,\\n                                            depends=ext.depends)\\n        self._built_objects = objects[:]\\n        if ext.extra_objects:\\n            objects.extend(ext.extra_objects)\\n        extra_args = ext.extra_link_args or []\\n        language = ext.language or self.compiler_obj.detect_language(sources)\\n        self.compiler_obj.link_shared_object(\\n            objects, ext_path,\\n            libraries=self.get_libraries(ext),\\n            library_dirs=ext.library_dirs,\\n            runtime_library_dirs=ext.runtime_library_dirs,\\n            extra_postargs=extra_args,\\n            export_symbols=self.get_export_symbols(ext),\\n            debug=self.debug,\\n            build_temp=self.build_temp,\\n            target_lang=language)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "reverting partially distutils to its 2.6.x state so 2.7a4 looks more like the 2.7b1 in this. the whole revert will occur after a4 is tagged",
    "fixed_code": "def build_extensions(self):\\n        self.check_extensions_list(self.extensions)\\n        for ext in self.extensions:\\n            self.build_extension(ext)\\n    def build_extension(self, ext):\\n        sources = ext.sources\\n        if sources is None or type(sources) not in (ListType, TupleType):\\n            raise DistutilsSetupError, \\\\n                  (\"in 'ext_modules' option (extension '%s'), \" +\\n                   \"'sources' must be present and must be \" +\\n                   \"a list of source filenames\") % ext.name\\n        sources = list(sources)\\n        ext_path = self.get_ext_fullpath(ext.name)\\n        depends = sources + ext.depends\\n        if not (self.force or newer_group(depends, ext_path, 'newer')):\\n            log.debug(\"skipping '%s' extension (up-to-date)\", ext.name)\\n            return\\n        else:\\n            log.info(\"building '%s' extension\", ext.name)\\n        sources = self.swig_sources(sources, ext)\\n        extra_args = ext.extra_compile_args or []\\n        macros = ext.define_macros[:]\\n        for undef in ext.undef_macros:\\n            macros.append((undef,))\\n        objects = self.compiler.compile(sources,\\n                                         output_dir=self.build_temp,\\n                                         macros=macros,\\n                                         include_dirs=ext.include_dirs,\\n                                         debug=self.debug,\\n                                         extra_postargs=extra_args,\\n                                         depends=ext.depends)\\n        self._built_objects = objects[:]\\n        if ext.extra_objects:\\n            objects.extend(ext.extra_objects)\\n        extra_args = ext.extra_link_args or []\\n        language = ext.language or self.compiler.detect_language(sources)\\n        self.compiler.link_shared_object(\\n            objects, ext_path,\\n            libraries=self.get_libraries(ext),\\n            library_dirs=ext.library_dirs,\\n            runtime_library_dirs=ext.runtime_library_dirs,\\n            extra_postargs=extra_args,\\n            export_symbols=self.get_export_symbols(ext),\\n            debug=self.debug,\\n            build_temp=self.build_temp,\\n            target_lang=language)"
  },
  {
    "code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    values = Series(values).values\\n    is_category = com.is_categorical_dtype(values.dtype)\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    elif is_category:\\n        bins = values.levels\\n        cat = values\\n        values = cat.codes\\n    dtype = values.dtype\\n    if com.is_integer_dtype(dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    elif issubclass(values.dtype.type, (np.datetime64, np.timedelta64)):\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        if dropna:\\n            from pandas.tslib import iNaT\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n        keys = keys.astype(dtype)\\n    else:\\n        values = com._ensure_object(values)\\n        mask = com.isnull(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n        if not dropna:\\n            keys = np.insert(keys, 0, np.NaN)\\n            counts = np.insert(counts, 0, mask.sum())\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.levels)), fill_value=0)\\n        if not is_category:\\n            result.index = bins[:-1]\\n        else:\\n            result.index = cat.levels\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DTI.value_counts doesnt preserve tz",
    "fixed_code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    is_period = getattr(values, 'inferred_type', None) == 'period'\\n    values = Series(values).values\\n    is_category = com.is_categorical_dtype(values.dtype)\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    elif is_category:\\n        bins = values.levels\\n        cat = values\\n        values = cat.codes\\n    dtype = values.dtype\\n    if issubclass(values.dtype.type, (np.datetime64, np.timedelta64)) or is_period:\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        if dropna:\\n            from pandas.tslib import iNaT\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n        keys = keys.astype(dtype)\\n    elif com.is_integer_dtype(dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    else:\\n        values = com._ensure_object(values)\\n        mask = com.isnull(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n        if not dropna:\\n            keys = np.insert(keys, 0, np.NaN)\\n            counts = np.insert(counts, 0, mask.sum())\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.levels)), fill_value=0)\\n        if not is_category:\\n            result.index = bins[:-1]\\n        else:\\n            result.index = cat.levels\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result"
  },
  {
    "code": "def _fix_connectivity(X, connectivity, n_components=None):\\n    n_samples = X.shape[0]\\n    if (connectivity.shape[0] != n_samples or\\n        connectivity.shape[1] != n_samples):\\n        raise ValueError('Wrong shape for connectivity matrix: %s '\\n                         'when X is %s' % (connectivity.shape, X.shape))\\n    connectivity = connectivity + connectivity.T\\n    n_components, labels = connected_components(connectivity)\\n    if not sparse.isspmatrix_lil(connectivity):\\n        if not sparse.isspmatrix(connectivity):\\n            connectivity = sparse.lil_matrix(connectivity)\\n        else:\\n            connectivity = connectivity.tolil()\\n    if n_components > 1:\\n        warnings.warn(\"the number of connected components of the \"\\n                      \"connectivity matrix is %d > 1. Completing it to avoid \"\\n                      \"stopping the tree early.\" % n_components,\\n                      stacklevel=2)\\n        for i in xrange(n_components):\\n            idx_i = np.where(labels == i)[0]\\n            Xi = X[idx_i]\\n            for j in xrange(i):\\n                idx_j = np.where(labels == j)[0]\\n                Xj = X[idx_j]\\n                D = euclidean_distances(Xi, Xj)\\n                ii, jj = np.where(D == np.min(D))\\n                ii = ii[0]\\n                jj = jj[0]\\n                connectivity[idx_i[ii], idx_j[jj]] = True\\n                connectivity[idx_j[jj], idx_i[ii]] = True\\n        n_components = 1\\n    return connectivity",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TEST: increase test coverage\\n\\nAnd fix a few bugs and remove redundant code",
    "fixed_code": "def _fix_connectivity(X, connectivity, n_components=None):\\n    n_samples = X.shape[0]\\n    if (connectivity.shape[0] != n_samples or\\n        connectivity.shape[1] != n_samples):\\n        raise ValueError('Wrong shape for connectivity matrix: %s '\\n                         'when X is %s' % (connectivity.shape, X.shape))\\n    connectivity = connectivity + connectivity.T\\n    if not sparse.isspmatrix_lil(connectivity):\\n        if not sparse.isspmatrix(connectivity):\\n            connectivity = sparse.lil_matrix(connectivity)\\n        else:\\n            connectivity = connectivity.tolil()\\n    n_components, labels = connected_components(connectivity)\\n    if n_components > 1:\\n        warnings.warn(\"the number of connected components of the \"\\n                      \"connectivity matrix is %d > 1. Completing it to avoid \"\\n                      \"stopping the tree early.\" % n_components,\\n                      stacklevel=2)\\n        for i in xrange(n_components):\\n            idx_i = np.where(labels == i)[0]\\n            Xi = X[idx_i]\\n            for j in xrange(i):\\n                idx_j = np.where(labels == j)[0]\\n                Xj = X[idx_j]\\n                D = euclidean_distances(Xi, Xj)\\n                ii, jj = np.where(D == np.min(D))\\n                ii = ii[0]\\n                jj = jj[0]\\n                connectivity[idx_i[ii], idx_j[jj]] = True\\n                connectivity[idx_j[jj], idx_i[ii]] = True\\n        n_components = 1\\n    return connectivity"
  },
  {
    "code": "def run_module(mod_name, init_globals=None,\\n                         run_name=None, alter_sys=False):\\n    loader = get_loader(mod_name)\\n    if loader is None:\\n        raise ImportError(\"No module named \" + mod_name)\\n    code = loader.get_code(mod_name)\\n    if code is None:\\n        raise ImportError(\"No code object available for \" + mod_name)\\n    filename = _get_filename(loader, mod_name)\\n    if run_name is None:\\n        run_name = mod_name\\n    return _run_module_code(code, init_globals, run_name,\\n                            filename, loader, alter_sys)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run_module(mod_name, init_globals=None,\\n                         run_name=None, alter_sys=False):\\n    loader = get_loader(mod_name)\\n    if loader is None:\\n        raise ImportError(\"No module named \" + mod_name)\\n    code = loader.get_code(mod_name)\\n    if code is None:\\n        raise ImportError(\"No code object available for \" + mod_name)\\n    filename = _get_filename(loader, mod_name)\\n    if run_name is None:\\n        run_name = mod_name\\n    return _run_module_code(code, init_globals, run_name,\\n                            filename, loader, alter_sys)"
  },
  {
    "code": "def _zip_axes_from_type(typ, new_axes):\\n    axes = {}\\n    for ax_ind, ax_name in typ._AXIS_NAMES.iteritems():\\n        axes[ax_name] = new_axes[ax_ind]\\n    return axes",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: added interpretation of 'in' in pytables Term syntax\\n\\nwhich is syntactically equivalent of '='",
    "fixed_code": "def _zip_axes_from_type(typ, new_axes):\\n    axes = {}\\n    for ax_ind, ax_name in compat.iteritems(typ._AXIS_NAMES):\\n        axes[ax_name] = new_axes[ax_ind]\\n    return axes"
  },
  {
    "code": "def sql_indexes(app, style, connection):\\n    \"Returns a list of the CREATE INDEX SQL statements for all models in the given app.\"\\n    output = []\\n    for model in models.get_models(app, include_auto_created=True):\\n        output.extend(connection.creation.sql_indexes_for_model(model, style))\\n    return output",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sql_indexes(app, style, connection):\\n    \"Returns a list of the CREATE INDEX SQL statements for all models in the given app.\"\\n    output = []\\n    for model in models.get_models(app, include_auto_created=True):\\n        output.extend(connection.creation.sql_indexes_for_model(model, style))\\n    return output"
  },
  {
    "code": "def _compute_plot_data(self):\\n        data = self.data\\n        if isinstance(data, Series):\\n            label = self.label\\n            if label is None and data.name is None:\\n                label = 'None'\\n            data = data.to_frame(name=label)\\n        numeric_data = data.convert_objects()._get_numeric_data()\\n        try:\\n            is_empty = numeric_data.empty\\n        except AttributeError:\\n            is_empty = not len(numeric_data)\\n        if is_empty:\\n            raise TypeError('Empty {0!r}: no numeric data to '\\n                            'plot'.format(numeric_data.__class__.__name__))\\n        self.data = numeric_data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _compute_plot_data(self):\\n        data = self.data\\n        if isinstance(data, Series):\\n            label = self.label\\n            if label is None and data.name is None:\\n                label = 'None'\\n            data = data.to_frame(name=label)\\n        numeric_data = data.convert_objects()._get_numeric_data()\\n        try:\\n            is_empty = numeric_data.empty\\n        except AttributeError:\\n            is_empty = not len(numeric_data)\\n        if is_empty:\\n            raise TypeError('Empty {0!r}: no numeric data to '\\n                            'plot'.format(numeric_data.__class__.__name__))\\n        self.data = numeric_data"
  },
  {
    "code": "def invalid_shape_exception(csize, xsize):\\n\\t\\t\\treturn ValueError(\\n\\t\\t\\t\\tf\"'c' argument has {csize} elements, which is inconsistent \"\\n\\t\\t\\t\\tf\"with 'x' and 'y' with size {xsize}.\")\\n\\t\\tc_is_mapped = False  \\n\\t\\tvalid_shape = True  \\n\\t\\tif not c_was_none and kwcolor is None and not c_is_string_or_strings:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tc = np.asanyarray(c, dtype=float)\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass  \\n\\t\\t\\telse:\\n\\t\\t\\t\\tif c.shape == (1, 4) or c.shape == (1, 3):\\n\\t\\t\\t\\t\\tc_is_mapped = False\\n\\t\\t\\t\\t\\tif c.size != xsize:\\n\\t\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\t\\t\\telif c.size == xsize:\\n\\t\\t\\t\\t\\tc = c.ravel()\\n\\t\\t\\t\\t\\tc_is_mapped = True\\n\\t\\t\\t\\telse:  \\n\\t\\t\\t\\t\\tif c.shape in ((3,), (4,)):\\n\\t\\t\\t\\t\\t\\t_log.warning(\\n\\t\\t\\t\\t\\t\\t\\t\"'c' argument looks like a single numeric RGB or \"\\n\\t\\t\\t\\t\\t\\t\\t\"RGBA sequence, which should be avoided as value-\"\\n\\t\\t\\t\\t\\t\\t\\t\"mapping will have precedence in case its length \"\\n\\t\\t\\t\\t\\t\\t\\t\"matches with 'x' & 'y'.  Please use a 2-D array \"\\n\\t\\t\\t\\t\\t\\t\\t\"with a single row if you really want to specify \"\\n\\t\\t\\t\\t\\t\\t\\t\"the same RGB or RGBA value for all points.\")\\n\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\tif not c_is_mapped:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tcolors = mcolors.to_rgba_array(c)\\n\\t\\t\\texcept (TypeError, ValueError) as err:\\n\\t\\t\\t\\tif \"RGBA values should be within 0-1 range\" in str(err):\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not valid_shape:\\n\\t\\t\\t\\t\\t\\traise invalid_shape_exception(c.size, xsize) from err\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\tf\"'c' argument must be a color, a sequence of colors, \"\\n\\t\\t\\t\\t\\t\\tf\"or a sequence of numbers, not {c}\") from err\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif len(colors) not in (0, 1, xsize):\\n\\t\\t\\t\\t\\traise invalid_shape_exception(len(colors), xsize)\\n\\t\\telse:\\n\\t\\t\\tcolors = None  \\n\\t\\treturn c, colors, edgecolors\\n\\t@_preprocess_data(replace_names=[\"x\", \"y\", \"s\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"edgecolors\", \"c\", \"facecolor\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"facecolors\", \"color\"],\\n\\t\\t\\t\\t\\t  label_namer=\"y\")\\n\\t@cbook._delete_parameter(\"3.2\", \"verts\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def invalid_shape_exception(csize, xsize):\\n\\t\\t\\treturn ValueError(\\n\\t\\t\\t\\tf\"'c' argument has {csize} elements, which is inconsistent \"\\n\\t\\t\\t\\tf\"with 'x' and 'y' with size {xsize}.\")\\n\\t\\tc_is_mapped = False  \\n\\t\\tvalid_shape = True  \\n\\t\\tif not c_was_none and kwcolor is None and not c_is_string_or_strings:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tc = np.asanyarray(c, dtype=float)\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass  \\n\\t\\t\\telse:\\n\\t\\t\\t\\tif c.shape == (1, 4) or c.shape == (1, 3):\\n\\t\\t\\t\\t\\tc_is_mapped = False\\n\\t\\t\\t\\t\\tif c.size != xsize:\\n\\t\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\t\\t\\telif c.size == xsize:\\n\\t\\t\\t\\t\\tc = c.ravel()\\n\\t\\t\\t\\t\\tc_is_mapped = True\\n\\t\\t\\t\\telse:  \\n\\t\\t\\t\\t\\tif c.shape in ((3,), (4,)):\\n\\t\\t\\t\\t\\t\\t_log.warning(\\n\\t\\t\\t\\t\\t\\t\\t\"'c' argument looks like a single numeric RGB or \"\\n\\t\\t\\t\\t\\t\\t\\t\"RGBA sequence, which should be avoided as value-\"\\n\\t\\t\\t\\t\\t\\t\\t\"mapping will have precedence in case its length \"\\n\\t\\t\\t\\t\\t\\t\\t\"matches with 'x' & 'y'.  Please use a 2-D array \"\\n\\t\\t\\t\\t\\t\\t\\t\"with a single row if you really want to specify \"\\n\\t\\t\\t\\t\\t\\t\\t\"the same RGB or RGBA value for all points.\")\\n\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\tif not c_is_mapped:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tcolors = mcolors.to_rgba_array(c)\\n\\t\\t\\texcept (TypeError, ValueError) as err:\\n\\t\\t\\t\\tif \"RGBA values should be within 0-1 range\" in str(err):\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not valid_shape:\\n\\t\\t\\t\\t\\t\\traise invalid_shape_exception(c.size, xsize) from err\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\tf\"'c' argument must be a color, a sequence of colors, \"\\n\\t\\t\\t\\t\\t\\tf\"or a sequence of numbers, not {c}\") from err\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif len(colors) not in (0, 1, xsize):\\n\\t\\t\\t\\t\\traise invalid_shape_exception(len(colors), xsize)\\n\\t\\telse:\\n\\t\\t\\tcolors = None  \\n\\t\\treturn c, colors, edgecolors\\n\\t@_preprocess_data(replace_names=[\"x\", \"y\", \"s\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"edgecolors\", \"c\", \"facecolor\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"facecolors\", \"color\"],\\n\\t\\t\\t\\t\\t  label_namer=\"y\")\\n\\t@cbook._delete_parameter(\"3.2\", \"verts\")"
  },
  {
    "code": "def delete_vserver(self):\\n        vserver_delete = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'vserver-destroy', **{'vserver-name': self.name})\\n        try:\\n            self.server.invoke_successfully(vserver_delete,\\n                                            enable_tunneling=False)\\n        except netapp_utils.zapi.NaApiError as e:\\n            self.module.fail_json(msg='Error deleting SVM %s \\\\n                                  with root volume %s on aggregate %s: %s'\\n                                  % (self.name, self.root_volume,\\n                                     self.root_volume_aggregate, to_native(e)),\\n                                  exception=traceback.format_exc())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Bug Fix and rewrite (#49508)",
    "fixed_code": "def delete_vserver(self):\\n        vserver_delete = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'vserver-destroy', **{'vserver-name': self.parameters['name']})\\n        try:\\n            self.server.invoke_successfully(vserver_delete,\\n                                            enable_tunneling=False)\\n        except netapp_utils.zapi.NaApiError as e:\\n            self.module.fail_json(msg='Error deleting SVM %s: %s'\\n                                      % (self.parameters['name'], to_native(e)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\\n    X, Y, dtype_float = _return_float_dtype(X, Y)\\n    warn_on_dtype = dtype is not None\\n    estimator = 'check_pairwise_arrays'\\n    if dtype is None:\\n        dtype = dtype_float\\n    if Y is X or Y is None:\\n        X = Y = check_array(X, accept_sparse='csr', dtype=dtype,\\n                            warn_on_dtype=warn_on_dtype, estimator=estimator)\\n    else:\\n        X = check_array(X, accept_sparse='csr', dtype=dtype,\\n                        warn_on_dtype=warn_on_dtype, estimator=estimator)\\n        Y = check_array(Y, accept_sparse='csr', dtype=dtype,\\n                        warn_on_dtype=warn_on_dtype, estimator=estimator)\\n    if precomputed:\\n        if X.shape[1] != Y.shape[0]:\\n            raise ValueError(\"Precomputed metric requires shape \"\\n                             \"(n_queries, n_indexed). Got (%d, %d) \"\\n                             \"for %d indexed.\" %\\n                             (X.shape[0], X.shape[1], Y.shape[0]))\\n    elif X.shape[1] != Y.shape[1]:\\n        raise ValueError(\"Incompatible dimension for X and Y matrices: \"\\n                         \"X.shape[1] == %d while Y.shape[1] == %d\" % (\\n                             X.shape[1], Y.shape[1]))\\n    return X, Y",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\\n    X, Y, dtype_float = _return_float_dtype(X, Y)\\n    warn_on_dtype = dtype is not None\\n    estimator = 'check_pairwise_arrays'\\n    if dtype is None:\\n        dtype = dtype_float\\n    if Y is X or Y is None:\\n        X = Y = check_array(X, accept_sparse='csr', dtype=dtype,\\n                            warn_on_dtype=warn_on_dtype, estimator=estimator)\\n    else:\\n        X = check_array(X, accept_sparse='csr', dtype=dtype,\\n                        warn_on_dtype=warn_on_dtype, estimator=estimator)\\n        Y = check_array(Y, accept_sparse='csr', dtype=dtype,\\n                        warn_on_dtype=warn_on_dtype, estimator=estimator)\\n    if precomputed:\\n        if X.shape[1] != Y.shape[0]:\\n            raise ValueError(\"Precomputed metric requires shape \"\\n                             \"(n_queries, n_indexed). Got (%d, %d) \"\\n                             \"for %d indexed.\" %\\n                             (X.shape[0], X.shape[1], Y.shape[0]))\\n    elif X.shape[1] != Y.shape[1]:\\n        raise ValueError(\"Incompatible dimension for X and Y matrices: \"\\n                         \"X.shape[1] == %d while Y.shape[1] == %d\" % (\\n                             X.shape[1], Y.shape[1]))\\n    return X, Y"
  },
  {
    "code": "def is_usable(self):\\n        if self.connection is None:\\n            return False\\n        try:\\n            self.connection.cursor().execute(\"SELECT 1\")\\n        except Database.Error:\\n            return False\\n        else:\\n            return True\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #30193, Refs #28478 -- Avoided PostgreSQL connection health checks on initialization.\\n\\nThis addressed a regression introduced by a96b9019320ed8236659ee520a7a017c1bafbc6f as identified by Ran Benita.",
    "fixed_code": "def is_usable(self):\\n        try:\\n            self.connection.cursor().execute(\"SELECT 1\")\\n        except Database.Error:\\n            return False\\n        else:\\n            return True\\n    @property"
  },
  {
    "code": "def _get_slice_axis(self, slice_obj, axis=0):\\n        _check_step(slice_obj)\\n        frame = self.frame\\n        axis_name = frame._get_axis_name(axis)\\n        labels = getattr(frame, axis_name)\\n        if _is_label_slice(labels, slice_obj):\\n            i, j = labels.slice_locs(slice_obj.start, slice_obj.stop)\\n            slicer = slice(i, j)\\n        else:\\n            slicer = slice_obj\\n        if not _need_slice(slice_obj):\\n            return frame\\n        if axis == 0:\\n            new_index = frame.index[slicer]\\n            new_columns = frame.columns\\n            new_values = frame.values[slicer]\\n        else:\\n            new_index = frame.index\\n            new_columns = frame.columns[slicer]\\n            new_values = frame.values[:, slicer]\\n        return DataFrame(new_values, index=new_index,\\n                         columns=new_columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fancy indexing test coverage",
    "fixed_code": "def _get_slice_axis(self, slice_obj, axis=0):\\n        frame = self.frame\\n        axis_name = frame._get_axis_name(axis)\\n        labels = getattr(frame, axis_name)\\n        if _is_label_slice(labels, slice_obj):\\n            i, j = labels.slice_locs(slice_obj.start, slice_obj.stop)\\n            slicer = slice(i, j)\\n        else:\\n            slicer = slice_obj\\n        if not _need_slice(slice_obj):\\n            return frame\\n        if axis == 0:\\n            new_index = frame.index[slicer]\\n            new_columns = frame.columns\\n            new_values = frame.values[slicer]\\n        else:\\n            new_index = frame.index\\n            new_columns = frame.columns[slicer]\\n            new_values = frame.values[:, slicer]\\n        return DataFrame(new_values, index=new_index,\\n                         columns=new_columns)"
  },
  {
    "code": "def maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = fill_value.dtype.type(\"NaT\", \"ns\")\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, np.datetime64):\\n        try:\\n            fill_value = tslibs.Timestamp(fill_value).to_datetime64()\\n        except (TypeError, ValueError):\\n            dtype = np.dtype(np.object_)\\n    elif issubclass(dtype.type, np.timedelta64):\\n        try:\\n            fv = tslibs.Timedelta(fill_value)\\n        except ValueError:\\n            dtype = np.dtype(np.object_)\\n        else:\\n            if fv is NaT:\\n                fill_value = np.timedelta64(\"NaT\", \"ns\")\\n            else:\\n                fill_value = fv.to_timedelta64()\\n    elif is_datetime64tz_dtype(dtype):\\n        if isna(fill_value):\\n            fill_value = NaT\\n    elif is_extension_array_dtype(dtype) and isna(fill_value):\\n        fill_value = dtype.na_value\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.dtype(np.float64)\\n            if not isna(fill_value):\\n                fill_value = dtype.type(fill_value)\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        else:\\n            fill_value = np.bool_(fill_value)\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n        elif issubclass(dtype.type, np.floating):\\n            if _check_lossless_cast(fill_value, dtype):\\n                fill_value = dtype.type(fill_value)\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    elif fill_value is None:\\n        if is_float_dtype(dtype) or is_complex_dtype(dtype):\\n            fill_value = np.nan\\n        elif is_integer_dtype(dtype):\\n            dtype = np.float64\\n            fill_value = np.nan\\n        elif is_datetime_or_timedelta_dtype(dtype):\\n            fill_value = dtype.type(\"NaT\", \"ns\")\\n        else:\\n            dtype = np.object_\\n            fill_value = np.nan\\n    else:\\n        dtype = np.object_\\n    if is_extension_array_dtype(dtype):\\n        pass\\n    elif is_datetime64tz_dtype(dtype):\\n        pass\\n    elif issubclass(np.dtype(dtype).type, (bytes, str)):\\n        dtype = np.object_\\n    return dtype, fill_value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: Fix maybe_promote xfails (#28754)",
    "fixed_code": "def maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = fill_value.dtype.type(\"NaT\", \"ns\")\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n        if dtype == np.object_ or dtype.kind in [\"U\", \"S\"]:\\n            fill_value = np.nan\\n            dtype = np.dtype(np.object_)\\n    if issubclass(dtype.type, np.datetime64):\\n        if isinstance(fill_value, datetime) and fill_value.tzinfo is not None:\\n            dtype = np.dtype(np.object_)\\n        else:\\n            try:\\n                fill_value = tslibs.Timestamp(fill_value).to_datetime64()\\n            except (TypeError, ValueError):\\n                dtype = np.dtype(np.object_)\\n    elif issubclass(dtype.type, np.timedelta64):\\n        try:\\n            fv = tslibs.Timedelta(fill_value)\\n        except ValueError:\\n            dtype = np.dtype(np.object_)\\n        else:\\n            if fv is NaT:\\n                fill_value = np.timedelta64(\"NaT\", \"ns\")\\n            else:\\n                fill_value = fv.to_timedelta64()\\n    elif is_datetime64tz_dtype(dtype):\\n        if isna(fill_value):\\n            fill_value = NaT\\n    elif is_extension_array_dtype(dtype) and isna(fill_value):\\n        fill_value = dtype.na_value\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.dtype(np.float64)\\n            if not isna(fill_value):\\n                fill_value = dtype.type(fill_value)\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        else:\\n            fill_value = np.bool_(fill_value)\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n        elif issubclass(dtype.type, np.floating):\\n            if _check_lossless_cast(fill_value, dtype):\\n                fill_value = dtype.type(fill_value)\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    elif fill_value is None:\\n        if is_float_dtype(dtype) or is_complex_dtype(dtype):\\n            fill_value = np.nan\\n        elif is_integer_dtype(dtype):\\n            dtype = np.float64\\n            fill_value = np.nan\\n        elif is_datetime_or_timedelta_dtype(dtype):\\n            fill_value = dtype.type(\"NaT\", \"ns\")\\n        else:\\n            dtype = np.object_\\n            fill_value = np.nan\\n    else:\\n        dtype = np.object_\\n    if is_extension_array_dtype(dtype):\\n        pass\\n    elif issubclass(np.dtype(dtype).type, (bytes, str)):\\n        dtype = np.object_\\n    return dtype, fill_value"
  },
  {
    "code": "def _can_hold_na(self):\\n        return self._holder._can_hold_na\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: remove FloatBlock, share _can_hold_na (#40526)",
    "fixed_code": "def _can_hold_na(self) -> bool:\\n        values = self.values\\n        if isinstance(values, np.ndarray):\\n            return values.dtype.kind not in [\"b\", \"i\", \"u\"]\\n        return values._can_hold_na\\n    @final\\n    @property"
  },
  {
    "code": "def fmgr_get_device(fmgr, paramgram):\\n    response = DEFAULT_RESULT_OBJ\\n    url = \"\"\\n    datagram = {}\\n    update_url = '/dvm/cmd/update/device'\\n    update_dict = {\\n        \"adom\": paramgram['adom'],\\n        \"device\": paramgram['device_unique_name'],\\n        \"flags\": \"create_task\"\\n    }\\n    fmgr.process_request(update_url, update_dict, FMGRMethods.EXEC)\\n    url = '/dvmdb/adom/{adom}/device'.format(adom=paramgram[\"adom\"])\\n    device_found = 0\\n    response = []\\n    if paramgram[\"device_serial\"] is not None:\\n        datagram = {\\n            \"filter\": [\"sn\", \"==\", paramgram[\"device_serial\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_unique_name\"] is not None:\\n        datagram = {\\n            \"filter\": [\"name\", \"==\", paramgram[\"device_unique_name\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_ip\"] is not None:\\n        datagram = {\\n            \"filter\": [\"ip\", \"==\", paramgram[\"device_ip\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    return response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fmgr_get_device(fmgr, paramgram):\\n    response = DEFAULT_RESULT_OBJ\\n    url = \"\"\\n    datagram = {}\\n    update_url = '/dvm/cmd/update/device'\\n    update_dict = {\\n        \"adom\": paramgram['adom'],\\n        \"device\": paramgram['device_unique_name'],\\n        \"flags\": \"create_task\"\\n    }\\n    fmgr.process_request(update_url, update_dict, FMGRMethods.EXEC)\\n    url = '/dvmdb/adom/{adom}/device'.format(adom=paramgram[\"adom\"])\\n    device_found = 0\\n    response = []\\n    if paramgram[\"device_serial\"] is not None:\\n        datagram = {\\n            \"filter\": [\"sn\", \"==\", paramgram[\"device_serial\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_unique_name\"] is not None:\\n        datagram = {\\n            \"filter\": [\"name\", \"==\", paramgram[\"device_unique_name\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    if device_found == 0 and paramgram[\"device_ip\"] is not None:\\n        datagram = {\\n            \"filter\": [\"ip\", \"==\", paramgram[\"device_ip\"]]\\n        }\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n        if len(response[1]) >= 0:\\n            device_found = 1\\n    return response"
  },
  {
    "code": "def __call__(self, *args, **kwargs):\\n        if self.func is None:\\n            self.func = self.get_func(*self.args, **self.kwargs)\\n        return self.func(*args, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25961 -- Removed handling of thread-non-safe GEOS functions.",
    "fixed_code": "def __call__(self, *args, **kwargs):\\n        if self.func is None:\\n            self.func = self.get_func(*self.args, **self.kwargs)\\n        return self.func(self.thread_context.ptr, *args)"
  },
  {
    "code": "def execute(self, operation, parameters=None):\\n        bql = _bind_parameters(operation,\\n                               parameters) if parameters else operation\\n        self.job_id = self.run_query(bql)\\n    def executemany(self, operation, seq_of_parameters):\\n        for parameters in seq_of_parameters:\\n            self.execute(operation, parameters)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2513] Change `bql` to `sql` for BigQuery Hooks & Ops\\n\\n- Change `bql` to `sql` for BigQuery Hooks &\\nOperators for consistency\\n\\nCloses #3454 from kaxil/consistent-bq-lang",
    "fixed_code": "def execute(self, operation, parameters=None):\\n        sql = _bind_parameters(operation,\\n                               parameters) if parameters else operation\\n        self.job_id = self.run_query(sql)\\n    def executemany(self, operation, seq_of_parameters):\\n        for parameters in seq_of_parameters:\\n            self.execute(operation, parameters)"
  },
  {
    "code": "def transform(self, node, results):\\n        prefix = None\\n        func = results['func'][0]\\n        if 'it' in results and func.value != 'ifilterfalse':\\n            dot, it = (results['dot'], results['it'])\\n            prefix = it.prefix\\n            it.remove()\\n            dot.remove()\\n            func.parent.replace(func)\\n        prefix = prefix or func.prefix\\n        func.replace(Name(func.value[1:], prefix=prefix))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "transform izip_longest #11424",
    "fixed_code": "def transform(self, node, results):\\n        prefix = None\\n        func = results['func'][0]\\n        if ('it' in results and\\n            func.value not in ('ifilterfalse', 'izip_longest')):\\n            dot, it = (results['dot'], results['it'])\\n            prefix = it.prefix\\n            it.remove()\\n            dot.remove()\\n            func.parent.replace(func)\\n        prefix = prefix or func.prefix\\n        func.replace(Name(func.value[1:], prefix=prefix))"
  },
  {
    "code": "def quantile(self, q=0.5, axis=0, numeric_only=True, interpolation=\"linear\"):\\n\\t\\tself._check_percentile(q)\\n\\t\\tdata = self._get_numeric_data() if numeric_only else self\\n\\t\\taxis = self._get_axis_number(axis)\\n\\t\\tis_transposed = axis == 1\\n\\t\\tif is_transposed:\\n\\t\\t\\tdata = data.T\\n\\t\\tif len(data.columns) == 0:\\n\\t\\t\\tcols = Index([], name=self.columns.name)\\n\\t\\t\\tif is_list_like(q):\\n\\t\\t\\t\\treturn self._constructor([], index=q, columns=cols)\\n\\t\\t\\treturn self._constructor_sliced([], index=cols, name=q)\\n\\t\\tresult = data._data.quantile(\\n\\t\\t\\tqs=q, axis=1, interpolation=interpolation, transposed=is_transposed\\n\\t\\t)\\n\\t\\tif result.ndim == 2:\\n\\t\\t\\tresult = self._constructor(result)\\n\\t\\telse:\\n\\t\\t\\tresult = self._constructor_sliced(result, name=q)\\n\\t\\tif is_transposed:\\n\\t\\t\\tresult = result.T\\n\\t\\treturn result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quantile(self, q=0.5, axis=0, numeric_only=True, interpolation=\"linear\"):\\n\\t\\tself._check_percentile(q)\\n\\t\\tdata = self._get_numeric_data() if numeric_only else self\\n\\t\\taxis = self._get_axis_number(axis)\\n\\t\\tis_transposed = axis == 1\\n\\t\\tif is_transposed:\\n\\t\\t\\tdata = data.T\\n\\t\\tif len(data.columns) == 0:\\n\\t\\t\\tcols = Index([], name=self.columns.name)\\n\\t\\t\\tif is_list_like(q):\\n\\t\\t\\t\\treturn self._constructor([], index=q, columns=cols)\\n\\t\\t\\treturn self._constructor_sliced([], index=cols, name=q)\\n\\t\\tresult = data._data.quantile(\\n\\t\\t\\tqs=q, axis=1, interpolation=interpolation, transposed=is_transposed\\n\\t\\t)\\n\\t\\tif result.ndim == 2:\\n\\t\\t\\tresult = self._constructor(result)\\n\\t\\telse:\\n\\t\\t\\tresult = self._constructor_sliced(result, name=q)\\n\\t\\tif is_transposed:\\n\\t\\t\\tresult = result.T\\n\\t\\treturn result"
  },
  {
    "code": "def _ica_par(X, tol, g, fun_args, max_iter, w_init):\\n    n, p = X.shape\\n    W = _sym_decorrelation(w_init)\\n    for _ in moves.xrange(max_iter):\\n        wtx = np.dot(W, X)\\n        gwtx, g_wtx = g(wtx, fun_args)\\n        W1 = (np.dot(gwtx, X.T) / float(p)\\n              - np.dot(np.diag(g_wtx.mean(axis=1)), W))\\n        W1 = _sym_decorrelation(W1)\\n        lim = max(abs(abs(np.diag(np.dot(W1, W.T))) - 1))\\n        W = W1\\n        if lim < tol:\\n            break\\n    return W",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ica_par(X, tol, g, fun_args, max_iter, w_init):\\n    n, p = X.shape\\n    W = _sym_decorrelation(w_init)\\n    for _ in moves.xrange(max_iter):\\n        wtx = np.dot(W, X)\\n        gwtx, g_wtx = g(wtx, fun_args)\\n        W1 = (np.dot(gwtx, X.T) / float(p)\\n              - np.dot(np.diag(g_wtx.mean(axis=1)), W))\\n        W1 = _sym_decorrelation(W1)\\n        lim = max(abs(abs(np.diag(np.dot(W1, W.T))) - 1))\\n        W = W1\\n        if lim < tol:\\n            break\\n    return W"
  },
  {
    "code": "def _handle_existing_file(self, conn, source, dest, proto, timeout):\\n        if not os.path.exists(dest):\\n            return True\\n        cwd = self._loader.get_basedir()\\n        filename = str(uuid.uuid4())\\n        tmp_dest_file = os.path.join(cwd, filename)\\n        try:\\n            out = conn.get_file(\\n                source=source, destination=tmp_dest_file,\\n                proto=proto, timeout=timeout\\n            )\\n        except Exception as exc:\\n            os.remove(tmp_dest_file)\\n            raise Exception(exc)\\n        try:\\n            with open(tmp_dest_file, 'r') as f:\\n                new_content = f.read()\\n            with open(dest, 'r') as f:\\n                old_content = f.read()\\n        except (IOError, OSError) as ioexc:\\n            raise IOError(ioexc)\\n        sha1 = hashlib.sha1()\\n        old_content_b = to_bytes(old_content, errors='surrogate_or_strict')\\n        sha1.update(old_content_b)\\n        checksum_old = sha1.digest()\\n        sha1 = hashlib.sha1()\\n        new_content_b = to_bytes(new_content, errors='surrogate_or_strict')\\n        sha1.update(new_content_b)\\n        checksum_new = sha1.digest()\\n        os.remove(tmp_dest_file)\\n        if checksum_old == checksum_new:\\n            return False\\n        else:\\n            return True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Allow the use of _paramiko_conn even if the connection hasn't been started. (#61570)\\n\\n\\nI'm not sure what the benefit is of Noneing paramiko_conn on close, but will keep for now",
    "fixed_code": "def _handle_existing_file(self, conn, source, dest, proto, timeout):\\n        if not os.path.exists(dest):\\n            return True\\n        cwd = self._loader.get_basedir()\\n        filename = str(uuid.uuid4())\\n        tmp_dest_file = os.path.join(cwd, filename)\\n        try:\\n            conn.get_file(\\n                source=source, destination=tmp_dest_file,\\n                proto=proto, timeout=timeout\\n            )\\n        except ConnectionError as exc:\\n            error = to_text(exc)\\n            if error.endswith(\"No such file or directory\"):\\n                if os.path.exists(tmp_dest_file):\\n                    os.remove(tmp_dest_file)\\n                return True\\n        try:\\n            with open(tmp_dest_file, 'r') as f:\\n                new_content = f.read()\\n            with open(dest, 'r') as f:\\n                old_content = f.read()\\n        except (IOError, OSError):\\n            os.remove(tmp_dest_file)\\n            raise\\n        sha1 = hashlib.sha1()\\n        old_content_b = to_bytes(old_content, errors='surrogate_or_strict')\\n        sha1.update(old_content_b)\\n        checksum_old = sha1.digest()\\n        sha1 = hashlib.sha1()\\n        new_content_b = to_bytes(new_content, errors='surrogate_or_strict')\\n        sha1.update(new_content_b)\\n        checksum_new = sha1.digest()\\n        os.remove(tmp_dest_file)\\n        if checksum_old == checksum_new:\\n            return False\\n        return True"
  },
  {
    "code": "def glob_to_re (pattern):\\n    pattern_re = fnmatch.translate(pattern)\\n    pattern_re = re.sub(r'(^|[^\\\\])\\.', r'\\1[^/]', pattern_re)\\n    return pattern_re",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #1491431: distutils.filelist.glob_to_re was broken for some edge cases (detailed in the test",
    "fixed_code": "def glob_to_re(pattern):\\n    pattern_re = fnmatch.translate(pattern)\\n    pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', r'\\1[^/]', pattern_re)\\n    return pattern_re"
  },
  {
    "code": "def _make_self_pipe(self):\\n        self._ssock, self._csock = socket.socketpair()\\n        self._ssock.setblocking(False)\\n        self._csock.setblocking(False)\\n        self._internal_fds += 1\\n        self.call_soon(self._loop_self_reading)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _make_self_pipe(self):\\n        self._ssock, self._csock = socket.socketpair()\\n        self._ssock.setblocking(False)\\n        self._csock.setblocking(False)\\n        self._internal_fds += 1\\n        self.call_soon(self._loop_self_reading)"
  },
  {
    "code": "def submit_job(\\n        self, jobName, jobQueue, jobDefinition, arrayProperties, parameters, containerOverrides\\n    ) -> dict:\\n        ...",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def submit_job(\\n        self, jobName, jobQueue, jobDefinition, arrayProperties, parameters, containerOverrides\\n    ) -> dict:\\n        ..."
  },
  {
    "code": "def process_view(self, request, callback, callback_args, callback_kwargs):\\n\\t\\tif getattr(request, 'csrf_processing_done', False):\\n\\t\\t\\treturn None\\n\\t\\ttry:\\n\\t\\t\\tcsrf_token = _sanitize_token(\\n\\t\\t\\t\\trequest.COOKIES[settings.CSRF_COOKIE_NAME])\\n\\t\\t\\trequest.META['CSRF_COOKIE'] = csrf_token\\n\\t\\texcept KeyError:\\n\\t\\t\\tcsrf_token = None\\n\\t\\tif getattr(callback, 'csrf_exempt', False):\\n\\t\\t\\treturn None\\n\\t\\tif request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):\\n\\t\\t\\tif getattr(request, '_dont_enforce_csrf_checks', False):\\n\\t\\t\\t\\treturn self._accept(request)\\n\\t\\t\\tif request.is_secure():\\n\\t\\t\\t\\treferer = force_text(\\n\\t\\t\\t\\t\\trequest.META.get('HTTP_REFERER'),\\n\\t\\t\\t\\t\\tstrings_only=True,\\n\\t\\t\\t\\t\\terrors='replace'\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tif referer is None:\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_NO_REFERER)\\n\\t\\t\\t\\treferer = urlparse(referer)\\n\\t\\t\\t\\tif '' in (referer.scheme, referer.netloc):\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_MALFORMED_REFERER)\\n\\t\\t\\t\\tif referer.scheme != 'https':\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_INSECURE_REFERER)\\n\\t\\t\\t\\tif settings.CSRF_COOKIE_DOMAIN is None:\\n\\t\\t\\t\\t\\tgood_referer = request.get_host()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tgood_referer = settings.CSRF_COOKIE_DOMAIN\\n\\t\\t\\t\\t\\tserver_port = request.META['SERVER_PORT']\\n\\t\\t\\t\\t\\tif server_port not in ('443', '80'):\\n\\t\\t\\t\\t\\t\\tgood_referer = '%s:%s' % (good_referer, server_port)\\n\\t\\t\\t\\tgood_hosts = list(settings.CSRF_TRUSTED_ORIGINS)\\n\\t\\t\\t\\tgood_hosts.append(good_referer)\\n\\t\\t\\t\\tif not any(is_same_domain(referer.netloc, host) for host in good_hosts):\\n\\t\\t\\t\\t\\treason = REASON_BAD_REFERER % referer.geturl()\\n\\t\\t\\t\\t\\treturn self._reject(request, reason)\\n\\t\\t\\tif csrf_token is None:\\n\\t\\t\\t\\treturn self._reject(request, REASON_NO_CSRF_COOKIE)\\n\\t\\t\\trequest_csrf_token = \"\"\\n\\t\\t\\tif request.method == \"POST\":\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\trequest_csrf_token = request.POST.get('csrfmiddlewaretoken', '')\\n\\t\\t\\t\\texcept IOError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\tif request_csrf_token == \"\":\\n\\t\\t\\t\\trequest_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')\\n\\t\\t\\tif not constant_time_compare(request_csrf_token, csrf_token):\\n\\t\\t\\t\\treturn self._reject(request, REASON_BAD_TOKEN)\\n\\t\\treturn self._accept(request)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_view(self, request, callback, callback_args, callback_kwargs):\\n\\t\\tif getattr(request, 'csrf_processing_done', False):\\n\\t\\t\\treturn None\\n\\t\\ttry:\\n\\t\\t\\tcsrf_token = _sanitize_token(\\n\\t\\t\\t\\trequest.COOKIES[settings.CSRF_COOKIE_NAME])\\n\\t\\t\\trequest.META['CSRF_COOKIE'] = csrf_token\\n\\t\\texcept KeyError:\\n\\t\\t\\tcsrf_token = None\\n\\t\\tif getattr(callback, 'csrf_exempt', False):\\n\\t\\t\\treturn None\\n\\t\\tif request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):\\n\\t\\t\\tif getattr(request, '_dont_enforce_csrf_checks', False):\\n\\t\\t\\t\\treturn self._accept(request)\\n\\t\\t\\tif request.is_secure():\\n\\t\\t\\t\\treferer = force_text(\\n\\t\\t\\t\\t\\trequest.META.get('HTTP_REFERER'),\\n\\t\\t\\t\\t\\tstrings_only=True,\\n\\t\\t\\t\\t\\terrors='replace'\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tif referer is None:\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_NO_REFERER)\\n\\t\\t\\t\\treferer = urlparse(referer)\\n\\t\\t\\t\\tif '' in (referer.scheme, referer.netloc):\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_MALFORMED_REFERER)\\n\\t\\t\\t\\tif referer.scheme != 'https':\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_INSECURE_REFERER)\\n\\t\\t\\t\\tif settings.CSRF_COOKIE_DOMAIN is None:\\n\\t\\t\\t\\t\\tgood_referer = request.get_host()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tgood_referer = settings.CSRF_COOKIE_DOMAIN\\n\\t\\t\\t\\t\\tserver_port = request.META['SERVER_PORT']\\n\\t\\t\\t\\t\\tif server_port not in ('443', '80'):\\n\\t\\t\\t\\t\\t\\tgood_referer = '%s:%s' % (good_referer, server_port)\\n\\t\\t\\t\\tgood_hosts = list(settings.CSRF_TRUSTED_ORIGINS)\\n\\t\\t\\t\\tgood_hosts.append(good_referer)\\n\\t\\t\\t\\tif not any(is_same_domain(referer.netloc, host) for host in good_hosts):\\n\\t\\t\\t\\t\\treason = REASON_BAD_REFERER % referer.geturl()\\n\\t\\t\\t\\t\\treturn self._reject(request, reason)\\n\\t\\t\\tif csrf_token is None:\\n\\t\\t\\t\\treturn self._reject(request, REASON_NO_CSRF_COOKIE)\\n\\t\\t\\trequest_csrf_token = \"\"\\n\\t\\t\\tif request.method == \"POST\":\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\trequest_csrf_token = request.POST.get('csrfmiddlewaretoken', '')\\n\\t\\t\\t\\texcept IOError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\tif request_csrf_token == \"\":\\n\\t\\t\\t\\trequest_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')\\n\\t\\t\\tif not constant_time_compare(request_csrf_token, csrf_token):\\n\\t\\t\\t\\treturn self._reject(request, REASON_BAD_TOKEN)\\n\\t\\treturn self._accept(request)"
  },
  {
    "code": "def isnull(obj):\\n    ''\\n    if np.isscalar(obj) or obj is None:\\n        return lib.checknull(obj)\\n    from pandas.core.generic import PandasObject\\n    from pandas import Series\\n    if isinstance(obj, np.ndarray):\\n        if obj.dtype.kind in ('O', 'S'):\\n            shape = obj.shape\\n            result = np.empty(shape, dtype=bool)\\n            raveled = obj.ravel()\\n            vec = lib.isnullobj(raveled)\\n            result[:] = vec.reshape(shape)\\n            if isinstance(obj, Series):\\n                result = Series(result, index=obj.index, copy=False)\\n        elif obj.dtype == np.datetime64:\\n            result = obj.ravel().view('i8') == 0x8000000000000000\\n        else:\\n            result = -np.isfinite(obj)\\n        return result\\n    elif isinstance(obj, PandasObject):\\n        return obj.apply(isnull)\\n    else:\\n        return obj is None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "killed some bugs from merge",
    "fixed_code": "def isnull(obj):\\n    ''\\n    if np.isscalar(obj) or obj is None:\\n        return lib.checknull(obj)\\n    from pandas.core.generic import PandasObject\\n    from pandas import Series\\n    if isinstance(obj, np.ndarray):\\n        if obj.dtype.kind in ('O', 'S'):\\n            shape = obj.shape\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(obj.ravel())\\n            result[:] = vec.reshape(shape)\\n            if isinstance(obj, Series):\\n                result = Series(result, index=obj.index, copy=False)\\n        elif obj.dtype == np.datetime64:\\n            result = obj.ravel().view('i8') == 0x8000000000000000\\n        else:\\n            result = -np.isfinite(obj)\\n        return result\\n    elif isinstance(obj, PandasObject):\\n        return obj.apply(isnull)\\n    else:\\n        return obj is None"
  },
  {
    "code": "def partial_fit(self, X, y, classes=None):\\n        X, y = check_arrays(X, y, sparse_format='dense')\\n        y = column_or_1d(y, warn=True)\\n        epsilon = 1e-9\\n        if _check_partial_fit_first_call(self, classes):\\n            n_features = X.shape[1]\\n            n_classes = len(self.classes_)\\n            self.theta_ = np.zeros((n_classes, n_features))\\n            self.sigma_ = np.zeros((n_classes, n_features))\\n            self.class_prior_ = np.zeros(n_classes)\\n            self.class_count_ = np.zeros(n_classes)\\n        else:\\n            self.sigma_[:, :] -= epsilon\\n        class2idx = dict((cls, idx) for idx, cls in enumerate(self.classes_))\\n        for y_i in np.unique(y):\\n            i = class2idx[y_i]\\n            X_i = X[y == y_i, :]\\n            N_i = X_i.shape[0]\\n            new_theta, new_sigma = self._update_mean_variance(\\n                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],\\n                X_i)\\n            self.theta_[i, :] = new_theta\\n            self.sigma_[i, :] = new_sigma\\n            self.class_count_[i] += N_i\\n        self.sigma_[:, :] += epsilon\\n        self.class_prior_[:] = self.class_count_ / np.sum(self.class_count_)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def partial_fit(self, X, y, classes=None):\\n        X, y = check_arrays(X, y, sparse_format='dense')\\n        y = column_or_1d(y, warn=True)\\n        epsilon = 1e-9\\n        if _check_partial_fit_first_call(self, classes):\\n            n_features = X.shape[1]\\n            n_classes = len(self.classes_)\\n            self.theta_ = np.zeros((n_classes, n_features))\\n            self.sigma_ = np.zeros((n_classes, n_features))\\n            self.class_prior_ = np.zeros(n_classes)\\n            self.class_count_ = np.zeros(n_classes)\\n        else:\\n            self.sigma_[:, :] -= epsilon\\n        class2idx = dict((cls, idx) for idx, cls in enumerate(self.classes_))\\n        for y_i in np.unique(y):\\n            i = class2idx[y_i]\\n            X_i = X[y == y_i, :]\\n            N_i = X_i.shape[0]\\n            new_theta, new_sigma = self._update_mean_variance(\\n                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],\\n                X_i)\\n            self.theta_[i, :] = new_theta\\n            self.sigma_[i, :] = new_sigma\\n            self.class_count_[i] += N_i\\n        self.sigma_[:, :] += epsilon\\n        self.class_prior_[:] = self.class_count_ / np.sum(self.class_count_)\\n        return self"
  },
  {
    "code": "def savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='',\\n\\t\\tfooter='', comments='\\n\\tif isinstance(fmt, bytes):\\n\\t\\tfmt = asstr(fmt)\\n\\tdelimiter = asstr(delimiter)\\n\\town_fh = False\\n\\tif _is_string_like(fname):\\n\\t\\town_fh = True\\n\\t\\tif fname.endswith('.gz'):\\n\\t\\t\\timport gzip\\n\\t\\t\\tfh = gzip.open(fname, 'wb')\\n\\t\\telse:\\n\\t\\t\\tif sys.version_info[0] >= 3:\\n\\t\\t\\t\\tfh = open(fname, 'wb')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfh = open(fname, 'w')\\n\\telif hasattr(fname, 'seek'):\\n\\t\\tfh = fname\\n\\telse:\\n\\t\\traise ValueError('fname must be a string or file handle')\\n\\ttry:\\n\\t\\tX = np.asarray(X)\\n\\t\\tif X.ndim == 1:\\n\\t\\t\\tif X.dtype.names is None:\\n\\t\\t\\t\\tX = np.atleast_2d(X).T\\n\\t\\t\\t\\tncol = 1\\n\\t\\t\\telse:\\n\\t\\t\\t\\tncol = len(X.dtype.descr)\\n\\t\\telse:\\n\\t\\t\\tncol = X.shape[1]\\n\\t\\tiscomplex_X = np.iscomplexobj(X)\\n\\t\\tif type(fmt) in (list, tuple):\\n\\t\\t\\tif len(fmt) != ncol:\\n\\t\\t\\t\\traise AttributeError('fmt has wrong shape.  %s' % str(fmt))\\n\\t\\t\\tformat = asstr(delimiter).join(map(asstr, fmt))\\n\\t\\telif type(fmt) is str:\\n\\t\\t\\tn_fmt_chars = fmt.count('%')\\n\\t\\t\\terror = ValueError('fmt has wrong number of %% formats:  %s' % fmt)\\n\\t\\t\\tif n_fmt_chars == 1:\\n\\t\\t\\t\\tif iscomplex_X:\\n\\t\\t\\t\\t\\tfmt = [' (%s+%sj)' % (fmt, fmt),] * ncol\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tfmt = [fmt, ] * ncol\\n\\t\\t\\t\\tformat = delimiter.join(fmt)\\n\\t\\t\\telif iscomplex_X and n_fmt_chars != (2 * ncol):\\n\\t\\t\\t\\traise error\\n\\t\\t\\telif ((not iscomplex_X) and n_fmt_chars != ncol):\\n\\t\\t\\t\\traise error\\n\\t\\t\\telse:\\n\\t\\t\\t\\tformat = fmt\\n\\t\\tif len(header) > 0:\\n\\t\\t\\theader = header.replace('\\n', '\\n' + comments)\\n\\t\\t\\tfh.write(asbytes(comments + header + newline))\\n\\t\\tif iscomplex_X:\\n\\t\\t\\tfor row in X:\\n\\t\\t\\t\\trow2 = []\\n\\t\\t\\t\\tfor number in row:\\n\\t\\t\\t\\t\\trow2.append(number.real)\\n\\t\\t\\t\\t\\trow2.append(number.imag)\\n\\t\\t\\t\\tfh.write(asbytes(format % tuple(row2) + newline))\\n\\t\\telse:\\n\\t\\t\\tfor row in X:\\n\\t\\t\\t\\tfh.write(asbytes(format % tuple(row) + newline))\\n\\t\\tif len(footer) > 0:\\n\\t\\t\\tfooter = footer.replace('\\n', '\\n' + comments)\\n\\t\\t\\tfh.write(asbytes(comments + footer + newline))\\n\\tfinally:\\n\\t\\tif own_fh:\\n\\t\\t\\tfh.close()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='',\\n\\t\\tfooter='', comments='\\n\\tif isinstance(fmt, bytes):\\n\\t\\tfmt = asstr(fmt)\\n\\tdelimiter = asstr(delimiter)\\n\\town_fh = False\\n\\tif _is_string_like(fname):\\n\\t\\town_fh = True\\n\\t\\tif fname.endswith('.gz'):\\n\\t\\t\\timport gzip\\n\\t\\t\\tfh = gzip.open(fname, 'wb')\\n\\t\\telse:\\n\\t\\t\\tif sys.version_info[0] >= 3:\\n\\t\\t\\t\\tfh = open(fname, 'wb')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfh = open(fname, 'w')\\n\\telif hasattr(fname, 'seek'):\\n\\t\\tfh = fname\\n\\telse:\\n\\t\\traise ValueError('fname must be a string or file handle')\\n\\ttry:\\n\\t\\tX = np.asarray(X)\\n\\t\\tif X.ndim == 1:\\n\\t\\t\\tif X.dtype.names is None:\\n\\t\\t\\t\\tX = np.atleast_2d(X).T\\n\\t\\t\\t\\tncol = 1\\n\\t\\t\\telse:\\n\\t\\t\\t\\tncol = len(X.dtype.descr)\\n\\t\\telse:\\n\\t\\t\\tncol = X.shape[1]\\n\\t\\tiscomplex_X = np.iscomplexobj(X)\\n\\t\\tif type(fmt) in (list, tuple):\\n\\t\\t\\tif len(fmt) != ncol:\\n\\t\\t\\t\\traise AttributeError('fmt has wrong shape.  %s' % str(fmt))\\n\\t\\t\\tformat = asstr(delimiter).join(map(asstr, fmt))\\n\\t\\telif type(fmt) is str:\\n\\t\\t\\tn_fmt_chars = fmt.count('%')\\n\\t\\t\\terror = ValueError('fmt has wrong number of %% formats:  %s' % fmt)\\n\\t\\t\\tif n_fmt_chars == 1:\\n\\t\\t\\t\\tif iscomplex_X:\\n\\t\\t\\t\\t\\tfmt = [' (%s+%sj)' % (fmt, fmt),] * ncol\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tfmt = [fmt, ] * ncol\\n\\t\\t\\t\\tformat = delimiter.join(fmt)\\n\\t\\t\\telif iscomplex_X and n_fmt_chars != (2 * ncol):\\n\\t\\t\\t\\traise error\\n\\t\\t\\telif ((not iscomplex_X) and n_fmt_chars != ncol):\\n\\t\\t\\t\\traise error\\n\\t\\t\\telse:\\n\\t\\t\\t\\tformat = fmt\\n\\t\\tif len(header) > 0:\\n\\t\\t\\theader = header.replace('\\n', '\\n' + comments)\\n\\t\\t\\tfh.write(asbytes(comments + header + newline))\\n\\t\\tif iscomplex_X:\\n\\t\\t\\tfor row in X:\\n\\t\\t\\t\\trow2 = []\\n\\t\\t\\t\\tfor number in row:\\n\\t\\t\\t\\t\\trow2.append(number.real)\\n\\t\\t\\t\\t\\trow2.append(number.imag)\\n\\t\\t\\t\\tfh.write(asbytes(format % tuple(row2) + newline))\\n\\t\\telse:\\n\\t\\t\\tfor row in X:\\n\\t\\t\\t\\tfh.write(asbytes(format % tuple(row) + newline))\\n\\t\\tif len(footer) > 0:\\n\\t\\t\\tfooter = footer.replace('\\n', '\\n' + comments)\\n\\t\\t\\tfh.write(asbytes(comments + footer + newline))\\n\\tfinally:\\n\\t\\tif own_fh:\\n\\t\\t\\tfh.close()"
  },
  {
    "code": "def time_from_datetime_timedelta(self):\\n        Timedelta(self.dttimedelta)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_from_datetime_timedelta(self):\\n        Timedelta(self.dttimedelta)"
  },
  {
    "code": "def tell(self):\\n        if self.level > 0:\\n            return self.lastpos\\n        return self.fp.tell() - len(self.readahead) - self.start",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Back out multifile.py 1.19 and 1.20. Fixes #514676.",
    "fixed_code": "def tell(self):\\n        if self.level > 0:\\n            return self.lastpos\\n        return self.fp.tell() - self.start"
  },
  {
    "code": "def copy(self):\\n        return Categorical(values=self._codes.copy(),categories=self.categories,\\n                           name=self.name, ordered=self.ordered, fastpath=True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove Categorical.name to make it more numpy.ndarray like\\n\\n`name` was initialy introduced to save the name of a Series/column during\\na groupby, when categorical was mostly a helper for that.\\n\\n\\nCloses: #10482",
    "fixed_code": "def copy(self):\\n        return Categorical(values=self._codes.copy(),categories=self.categories,\\n                           ordered=self.ordered, fastpath=True)"
  },
  {
    "code": "def _stack_multi_columns(frame, level=-1, dropna=True):\\n    this = frame.copy()\\n    if level != frame.columns.nlevels - 1:\\n        roll_columns = this.columns\\n        for i in range(level, frame.columns.nlevels - 1):\\n            roll_columns = roll_columns.swaplevel(i, i + 1)\\n        this.columns = roll_columns\\n    if not this.columns.is_lexsorted():\\n        this = this.sortlevel(0, axis=1)\\n    if len(frame.columns.levels) > 2:\\n        tuples = list(zip(*[\\n            lev.values.take(lab) for lev, lab in\\n            zip(this.columns.levels[:-1], this.columns.labels[:-1])\\n        ]))\\n        unique_groups = [key for key, _ in itertools.groupby(tuples)]\\n        new_names = this.columns.names[:-1]\\n        new_columns = MultiIndex.from_tuples(unique_groups, names=new_names)\\n    else:\\n        new_columns = unique_groups = this.columns.levels[0]\\n    new_data = {}\\n    level_vals = this.columns.levels[-1]\\n    levsize = len(level_vals)\\n    drop_cols = []\\n    for key in unique_groups:\\n        loc = this.columns.get_loc(key)\\n        slice_len = loc.stop - loc.start\\n        if slice_len == 0:\\n            drop_cols.append(key)\\n            continue\\n        elif slice_len != levsize:\\n            chunk = this.ix[:, this.columns[loc]]\\n            chunk.columns = level_vals.take(chunk.columns.labels[-1])\\n            value_slice = chunk.reindex(columns=level_vals).values\\n        else:\\n            if frame._is_mixed_type:\\n                value_slice = this.ix[:, this.columns[loc]].values\\n            else:\\n                value_slice = this.values[:, loc]\\n        new_data[key] = value_slice.ravel()\\n    if len(drop_cols) > 0:\\n        new_columns = new_columns - drop_cols\\n    N = len(this)\\n    if isinstance(this.index, MultiIndex):\\n        new_levels = list(this.index.levels)\\n        new_names = list(this.index.names)\\n        new_labels = [lab.repeat(levsize) for lab in this.index.labels]\\n    else:\\n        new_levels = [this.index]\\n        new_labels = [np.arange(N).repeat(levsize)]\\n        new_names = [this.index.name]  \\n    new_levels.append(frame.columns.levels[level])\\n    new_labels.append(np.tile(np.arange(levsize), N))\\n    new_names.append(frame.columns.names[level])\\n    new_index = MultiIndex(levels=new_levels, labels=new_labels,\\n                           names=new_names, verify_integrity=False)\\n    result = DataFrame(new_data, index=new_index, columns=new_columns)\\n    if dropna:\\n        result = result.dropna(axis=0, how='all')\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: stack with datetimes\\n\\nJust take from index itself, not levels.",
    "fixed_code": "def _stack_multi_columns(frame, level=-1, dropna=True):\\n    this = frame.copy()\\n    if level != frame.columns.nlevels - 1:\\n        roll_columns = this.columns\\n        for i in range(level, frame.columns.nlevels - 1):\\n            roll_columns = roll_columns.swaplevel(i, i + 1)\\n        this.columns = roll_columns\\n    if not this.columns.is_lexsorted():\\n        this = this.sortlevel(0, axis=1)\\n    if len(frame.columns.levels) > 2:\\n        tuples = list(zip(*[\\n            lev.take(lab) for lev, lab in\\n            zip(this.columns.levels[:-1], this.columns.labels[:-1])\\n        ]))\\n        unique_groups = [key for key, _ in itertools.groupby(tuples)]\\n        new_names = this.columns.names[:-1]\\n        new_columns = MultiIndex.from_tuples(unique_groups, names=new_names)\\n    else:\\n        new_columns = unique_groups = this.columns.levels[0]\\n    new_data = {}\\n    level_vals = this.columns.levels[-1]\\n    levsize = len(level_vals)\\n    drop_cols = []\\n    for key in unique_groups:\\n        loc = this.columns.get_loc(key)\\n        slice_len = loc.stop - loc.start\\n        if slice_len == 0:\\n            drop_cols.append(key)\\n            continue\\n        elif slice_len != levsize:\\n            chunk = this.ix[:, this.columns[loc]]\\n            chunk.columns = level_vals.take(chunk.columns.labels[-1])\\n            value_slice = chunk.reindex(columns=level_vals).values\\n        else:\\n            if frame._is_mixed_type:\\n                value_slice = this.ix[:, this.columns[loc]].values\\n            else:\\n                value_slice = this.values[:, loc]\\n        new_data[key] = value_slice.ravel()\\n    if len(drop_cols) > 0:\\n        new_columns = new_columns - drop_cols\\n    N = len(this)\\n    if isinstance(this.index, MultiIndex):\\n        new_levels = list(this.index.levels)\\n        new_names = list(this.index.names)\\n        new_labels = [lab.repeat(levsize) for lab in this.index.labels]\\n    else:\\n        new_levels = [this.index]\\n        new_labels = [np.arange(N).repeat(levsize)]\\n        new_names = [this.index.name]  \\n    new_levels.append(frame.columns.levels[level])\\n    new_labels.append(np.tile(np.arange(levsize), N))\\n    new_names.append(frame.columns.names[level])\\n    new_index = MultiIndex(levels=new_levels, labels=new_labels,\\n                           names=new_names, verify_integrity=False)\\n    result = DataFrame(new_data, index=new_index, columns=new_columns)\\n    if dropna:\\n        result = result.dropna(axis=0, how='all')\\n    return result"
  },
  {
    "code": "def create_version(self, project_name, model_name, version_spec):\\n        parent_name = 'projects/{}/models/{}'.format(project_name, model_name)\\n        create_request = self._cloudml.projects().models().versions().create(\\n            parent=parent_name, body=version_spec)\\n        response = create_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-1401] Standardize cloud ml operator arguments\\n\\nStandardize on project_id, to be consistent with\\nother cloud operators,\\nbetter-supporting default arguments.\\n\\nThis is one of multiple commits that will be\\nrequired to resolve\\nAIRFLOW-1401.\\n\\nCloses #2439 from peterjdolan/cloudml_project_id",
    "fixed_code": "def create_version(self, project_id, model_name, version_spec):\\n        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)\\n        create_request = self._cloudml.projects().models().versions().create(\\n            parent=parent_name, body=version_spec)\\n        response = create_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)"
  },
  {
    "code": "def validate_ip(module, cidr_ip):\\n    split_addr = cidr_ip.split('/')\\n    if len(split_addr) == 2:\\n        try:\\n            ip = to_subnet(split_addr[0], split_addr[1])\\n        except ValueError:\\n            ip = to_ipv6_subnet(split_addr[0]) + \"/\" + split_addr[1]\\n        if ip != cidr_ip:\\n            module.warn(\"One of your CIDR addresses ({0}) has host bits set. To get rid of this warning, \"\\n                        \"check the network mask and make sure that only network bits are set: {1}.\".format(cidr_ip, ip))\\n        return ip\\n    return cidr_ip",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Don't truncate cidr_ipv6 addresses in ec2_group.py (#59106)",
    "fixed_code": "def validate_ip(module, cidr_ip):\\n    split_addr = cidr_ip.split('/')\\n    if len(split_addr) == 2:\\n        try:\\n            ip = to_subnet(split_addr[0], split_addr[1])\\n            if ip != cidr_ip:\\n                module.warn(\"One of your CIDR addresses ({0}) has host bits set. To get rid of this warning, \"\\n                            \"check the network mask and make sure that only network bits are set: {1}.\".format(\\n                                cidr_ip, ip))\\n        except ValueError:\\n            try:\\n                isinstance(ip_network(to_text(cidr_ip)), IPv6Network)\\n                ip = cidr_ip\\n            except ValueError:\\n                ip6 = to_ipv6_subnet(split_addr[0]) + \"/\" + split_addr[1]\\n                if ip6 != cidr_ip:\\n                    module.warn(\"One of your IPv6 CIDR addresses ({0}) has host bits set. To get rid of this warning, \"\\n                                \"check the network mask and make sure that only network bits are set: {1}.\".format(cidr_ip, ip6))\\n                return ip6\\n        return ip\\n    return cidr_ip"
  },
  {
    "code": "def to_locale(language):\\n    language = language.lower()\\n    parts = language.split('-')\\n    try:\\n        country = parts[1]\\n    except IndexError:\\n        return language\\n    else:\\n        parts[1] = country.title() if len(country) > 2 else country.upper()\\n    return parts[0] + '_' + '-'.join(parts[1:])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_locale(language):\\n    language = language.lower()\\n    parts = language.split('-')\\n    try:\\n        country = parts[1]\\n    except IndexError:\\n        return language\\n    else:\\n        parts[1] = country.title() if len(country) > 2 else country.upper()\\n    return parts[0] + '_' + '-'.join(parts[1:])"
  },
  {
    "code": "def extend(self, values):\\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\\n        if values is self:\\n            values = list(values)\\n        for v in values:\\n            self.append(v)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def extend(self, values):\\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\\n        if values is self:\\n            values = list(values)\\n        for v in values:\\n            self.append(v)"
  },
  {
    "code": "def __init__(self,\\n                 jira_conn_id='jira_default',\\n                 method_name=None,\\n                 method_params=None,\\n                 result_processor=None,\\n                 *args,\\n                 **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.jira_conn_id = jira_conn_id\\n        self.result_processor = None\\n        if result_processor is not None:\\n            self.result_processor = result_processor\\n        self.method_name = method_name\\n        self.method_params = method_params\\n        self.jira_operator = JiraOperator(task_id=self.task_id,\\n                                          jira_conn_id=self.jira_conn_id,\\n                                          jira_method=self.method_name,\\n                                          jira_method_args=self.method_params,\\n                                          result_processor=self.result_processor)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add typing for jira provider (#10005)",
    "fixed_code": "def __init__(self,\\n                 method_name: str,\\n                 jira_conn_id: str = 'jira_default',\\n                 method_params: Optional[dict] = None,\\n                 result_processor: Optional[Callable] = None,\\n                 *args,\\n                 **kwargs) -> None:\\n        super().__init__(*args, **kwargs)\\n        self.jira_conn_id = jira_conn_id\\n        self.result_processor = None\\n        if result_processor is not None:\\n            self.result_processor = result_processor\\n        self.method_name = method_name\\n        self.method_params = method_params\\n        self.jira_operator = JiraOperator(task_id=self.task_id,\\n                                          jira_conn_id=self.jira_conn_id,\\n                                          jira_method=self.method_name,\\n                                          jira_method_args=self.method_params,\\n                                          result_processor=self.result_processor)"
  },
  {
    "code": "def select(self, key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, **kwargs):\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError('No object named %s in the file' % key)\\n        s = self._create_storer(group)\\n        s.infer_axes()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def select(self, key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, **kwargs):\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError('No object named %s in the file' % key)\\n        s = self._create_storer(group)\\n        s.infer_axes()"
  },
  {
    "code": "def add_ordering(self, *ordering):\\n\\t\\terrors = []\\n\\t\\tfor item in ordering:\\n\\t\\t\\tif isinstance(item, str):\\n\\t\\t\\t\\tif '.' in item:\\n\\t\\t\\t\\t\\twarnings.warn(\\n\\t\\t\\t\\t\\t\\t'Passing column raw column aliases to order_by() is '\\n\\t\\t\\t\\t\\t\\t'deprecated. Wrap %r in a RawSQL expression before '\\n\\t\\t\\t\\t\\t\\t'passing it to order_by().' % item,\\n\\t\\t\\t\\t\\t\\tcategory=RemovedInDjango40Warning,\\n\\t\\t\\t\\t\\t\\tstacklevel=3,\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tif item == '?':\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tif item.startswith('-'):\\n\\t\\t\\t\\t\\titem = item[1:]\\n\\t\\t\\t\\tif item in self.annotations:\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tif self.extra and item in self.extra:\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tself.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\\n\\t\\t\\telif not hasattr(item, 'resolve_expression'):\\n\\t\\t\\t\\terrors.append(item)\\n\\t\\t\\tif getattr(item, 'contains_aggregate', False):\\n\\t\\t\\t\\traise FieldError(\\n\\t\\t\\t\\t\\t'Using an aggregate in order_by() without also including '\\n\\t\\t\\t\\t\\t'it in annotate() is not allowed: %s' % item\\n\\t\\t\\t\\t)\\n\\t\\tif errors:\\n\\t\\t\\traise FieldError('Invalid order_by arguments: %s' % errors)\\n\\t\\tif ordering:\\n\\t\\t\\tself.order_by += ordering\\n\\t\\telse:\\n\\t\\t\\tself.default_ordering = False",
    "label": 1,
    "bug_type": "security",
    "bug_description": "[3.1.x] Fixed CVE-2021-35042 -- Prevented SQL injection in QuerySet.order_by().\\n\\nRegression introduced in 513948735b799239f3ef8c89397592445e1a0cd5\\nby marking the raw SQL column reference feature for deprecation in\\nDjango 4.0 while lifting the column format validation.\\n\\nIn retrospective the validation should have been kept around and the\\nuser should have been pointed at using RawSQL expressions during the\\ndeprecation period.\\n\\nThe main branch is not affected because the raw SQL column reference\\nsupport has been removed in 06eec3197009b88e3a633128bbcbd76eea0b46ff\\nper the 4.0 deprecation life cycle.\\n\\nThanks Joel Saunders for the report.",
    "fixed_code": "def add_ordering(self, *ordering):\\n\\t\\terrors = []\\n\\t\\tfor item in ordering:\\n\\t\\t\\tif isinstance(item, str):\\n\\t\\t\\t\\tif '.' in item and ORDER_PATTERN.match(item):\\n\\t\\t\\t\\t\\twarnings.warn(\\n\\t\\t\\t\\t\\t\\t'Passing column raw column aliases to order_by() is '\\n\\t\\t\\t\\t\\t\\t'deprecated. Wrap %r in a RawSQL expression before '\\n\\t\\t\\t\\t\\t\\t'passing it to order_by().' % item,\\n\\t\\t\\t\\t\\t\\tcategory=RemovedInDjango40Warning,\\n\\t\\t\\t\\t\\t\\tstacklevel=3,\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tif item == '?':\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tif item.startswith('-'):\\n\\t\\t\\t\\t\\titem = item[1:]\\n\\t\\t\\t\\tif item in self.annotations:\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tif self.extra and item in self.extra:\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tself.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\\n\\t\\t\\telif not hasattr(item, 'resolve_expression'):\\n\\t\\t\\t\\terrors.append(item)\\n\\t\\t\\tif getattr(item, 'contains_aggregate', False):\\n\\t\\t\\t\\traise FieldError(\\n\\t\\t\\t\\t\\t'Using an aggregate in order_by() without also including '\\n\\t\\t\\t\\t\\t'it in annotate() is not allowed: %s' % item\\n\\t\\t\\t\\t)\\n\\t\\tif errors:\\n\\t\\t\\traise FieldError('Invalid order_by arguments: %s' % errors)\\n\\t\\tif ordering:\\n\\t\\t\\tself.order_by += ordering\\n\\t\\telse:\\n\\t\\t\\tself.default_ordering = False"
  },
  {
    "code": "def _create_instances_here(self, ctxt, instance_uuids, instance_properties,\\n\\t\\t\\tinstance_type, image, security_groups, block_device_mapping):\\n\\t\\tinstance_values = copy.copy(instance_properties)\\n\\t\\tinstance_values['metadata'] = utils.instance_meta(instance_values)\\n\\t\\tsys_metadata = utils.instance_sys_meta(instance_values)\\n\\t\\tsys_metadata = flavors.save_flavor_info(sys_metadata, instance_type)\\n\\t\\tinstance_values['system_metadata'] = sys_metadata\\n\\t\\tinstance_values.pop('id')\\n\\t\\tinstance_values.pop('name')\\n\\t\\tinstance_values.pop('info_cache')\\n\\t\\tinstance_values.pop('security_groups')\\n\\t\\tnum_instances = len(instance_uuids)\\n\\t\\tfor i, instance_uuid in enumerate(instance_uuids):\\n\\t\\t\\tinstance = instance_obj.Instance()\\n\\t\\t\\tinstance.update(instance_values)\\n\\t\\t\\tinstance.uuid = instance_uuid\\n\\t\\t\\tinstance = self.compute_api.create_db_entry_for_new_instance(\\n\\t\\t\\t\\t\\tctxt,\\n\\t\\t\\t\\t\\tinstance_type,\\n\\t\\t\\t\\t\\timage,\\n\\t\\t\\t\\t\\tinstance,\\n\\t\\t\\t\\t\\tsecurity_groups,\\n\\t\\t\\t\\t\\tblock_device_mapping,\\n\\t\\t\\t\\t\\tnum_instances, i)\\n\\t\\t\\tinstance = obj_base.obj_to_primitive(instance)\\n\\t\\t\\tself.msg_runner.instance_update_at_top(ctxt, instance)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_instances_here(self, ctxt, instance_uuids, instance_properties,\\n\\t\\t\\tinstance_type, image, security_groups, block_device_mapping):\\n\\t\\tinstance_values = copy.copy(instance_properties)\\n\\t\\tinstance_values['metadata'] = utils.instance_meta(instance_values)\\n\\t\\tsys_metadata = utils.instance_sys_meta(instance_values)\\n\\t\\tsys_metadata = flavors.save_flavor_info(sys_metadata, instance_type)\\n\\t\\tinstance_values['system_metadata'] = sys_metadata\\n\\t\\tinstance_values.pop('id')\\n\\t\\tinstance_values.pop('name')\\n\\t\\tinstance_values.pop('info_cache')\\n\\t\\tinstance_values.pop('security_groups')\\n\\t\\tnum_instances = len(instance_uuids)\\n\\t\\tfor i, instance_uuid in enumerate(instance_uuids):\\n\\t\\t\\tinstance = instance_obj.Instance()\\n\\t\\t\\tinstance.update(instance_values)\\n\\t\\t\\tinstance.uuid = instance_uuid\\n\\t\\t\\tinstance = self.compute_api.create_db_entry_for_new_instance(\\n\\t\\t\\t\\t\\tctxt,\\n\\t\\t\\t\\t\\tinstance_type,\\n\\t\\t\\t\\t\\timage,\\n\\t\\t\\t\\t\\tinstance,\\n\\t\\t\\t\\t\\tsecurity_groups,\\n\\t\\t\\t\\t\\tblock_device_mapping,\\n\\t\\t\\t\\t\\tnum_instances, i)\\n\\t\\t\\tinstance = obj_base.obj_to_primitive(instance)\\n\\t\\t\\tself.msg_runner.instance_update_at_top(ctxt, instance)"
  },
  {
    "code": "def check(file):\\n    if os.path.isdir(file) and not os.path.islink(file):\\n        if verbose:\\n            print \"%r: listing directory\" % (file,)\\n        names = os.listdir(file)\\n        for name in names:\\n            fullname = os.path.join(file, name)\\n            if (os.path.isdir(fullname) and\\n                not os.path.islink(fullname) or\\n                os.path.normcase(name[-3:]) == \".py\"):\\n                check(fullname)\\n        return\\n    try:\\n        f = open(file)\\n    except IOError, msg:\\n        errprint(\"%r: I/O Error: %s\" % (file, msg))\\n        return\\n    if verbose > 1:\\n        print \"checking %r ...\" % file\\n    try:\\n        process_tokens(tokenize.generate_tokens(f.readline))\\n    except tokenize.TokenError, msg:\\n        errprint(\"%r: Token Error: %s\" % (file, msg))\\n        return\\n    except IndentationError, msg:\\n        errprint(\"%r: Indentation Error: %s\" % (file, msg))\\n        return\\n    except NannyNag, nag:\\n        badline = nag.get_lineno()\\n        line = nag.get_line()\\n        if verbose:\\n            print \"%r: *** Line %d: trouble in tab city! ***\" % (file, badline)\\n            print \"offending line: %r\" % (line,)\\n            print nag.get_msg()\\n        else:\\n            if ' ' in file: file = '\"' + file + '\"'\\n            if filename_only: print file\\n            else: print file, badline, repr(line)\\n        return\\n    if verbose:\\n        print \"%r: Clean bill of health.\" % (file,)\\nclass Whitespace:\\n    S, T = ' \\t'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check(file):\\n    if os.path.isdir(file) and not os.path.islink(file):\\n        if verbose:\\n            print \"%r: listing directory\" % (file,)\\n        names = os.listdir(file)\\n        for name in names:\\n            fullname = os.path.join(file, name)\\n            if (os.path.isdir(fullname) and\\n                not os.path.islink(fullname) or\\n                os.path.normcase(name[-3:]) == \".py\"):\\n                check(fullname)\\n        return\\n    try:\\n        f = open(file)\\n    except IOError, msg:\\n        errprint(\"%r: I/O Error: %s\" % (file, msg))\\n        return\\n    if verbose > 1:\\n        print \"checking %r ...\" % file\\n    try:\\n        process_tokens(tokenize.generate_tokens(f.readline))\\n    except tokenize.TokenError, msg:\\n        errprint(\"%r: Token Error: %s\" % (file, msg))\\n        return\\n    except IndentationError, msg:\\n        errprint(\"%r: Indentation Error: %s\" % (file, msg))\\n        return\\n    except NannyNag, nag:\\n        badline = nag.get_lineno()\\n        line = nag.get_line()\\n        if verbose:\\n            print \"%r: *** Line %d: trouble in tab city! ***\" % (file, badline)\\n            print \"offending line: %r\" % (line,)\\n            print nag.get_msg()\\n        else:\\n            if ' ' in file: file = '\"' + file + '\"'\\n            if filename_only: print file\\n            else: print file, badline, repr(line)\\n        return\\n    if verbose:\\n        print \"%r: Clean bill of health.\" % (file,)\\nclass Whitespace:\\n    S, T = ' \\t'"
  },
  {
    "code": "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\\n\\tlabel_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\\n\\tinput_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\\n\\tsparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\\n\\ty_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\\n\\treturn tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   labels=sparse_labels,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   sequence_length=input_length), 1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\\n\\tlabel_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\\n\\tinput_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\\n\\tsparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\\n\\ty_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\\n\\treturn tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   labels=sparse_labels,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   sequence_length=input_length), 1)"
  },
  {
    "code": "def get_conn_uri(self, conn_id: str) -> Optional[str]:\\n        session = boto3.Session(profile_name=self.profile_name)\\n        client = session.client(\"ssm\")\\n        ssm_path = self.build_ssm_path(conn_id=conn_id)\\n        try:\\n            response = client.get_parameter(\\n                Name=ssm_path, WithDecryption=False\\n            )\\n            value = response[\"Parameter\"][\"Value\"]\\n            return value\\n        except client.exceptions.ParameterNotFound:\\n            self.log.info(\\n                \"An error occurred (ParameterNotFound) when calling the GetParameter operation: \"\\n                \"Parameter %s not found.\", ssm_path\\n            )\\n            return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5705] Make AwsSsmSecretsBackend consistent with VaultBackend (#7753)",
    "fixed_code": "def get_conn_uri(self, conn_id: str) -> Optional[str]:\\n        ssm_path = self.build_ssm_path(conn_id=conn_id)\\n        try:\\n            response = self.client.get_parameter(\\n                Name=ssm_path, WithDecryption=False\\n            )\\n            value = response[\"Parameter\"][\"Value\"]\\n            return value\\n        except self.client.exceptions.ParameterNotFound:\\n            self.log.info(\\n                \"An error occurred (ParameterNotFound) when calling the GetParameter operation: \"\\n                \"Parameter %s not found.\", ssm_path\\n            )\\n            return None"
  },
  {
    "code": "def subtract(self, a, b):\\n        a = _convert_other(a, raiseit=True)\\n        r = a.__sub__(b, context=self)\\n        if r is NotImplemented:\\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\\n        else:\\n            return r",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def subtract(self, a, b):\\n        a = _convert_other(a, raiseit=True)\\n        r = a.__sub__(b, context=self)\\n        if r is NotImplemented:\\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\\n        else:\\n            return r"
  },
  {
    "code": "def _get_varlist(self):\\n        if self.format_version == 117:\\n            b = 33\\n        elif self.format_version >= 118:\\n            b = 129\\n        return [self._decode(self.path_or_buf.read(b)) for i in range(self.nvar)]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_varlist(self):\\n        if self.format_version == 117:\\n            b = 33\\n        elif self.format_version >= 118:\\n            b = 129\\n        return [self._decode(self.path_or_buf.read(b)) for i in range(self.nvar)]"
  },
  {
    "code": "def get_indexer(self, target, method=None, limit=None, tolerance=None):\\n\\t\\tif com.any_not_none(method, tolerance, limit) or not is_list_like(target):\\n\\t\\t\\treturn super().get_indexer(\\n\\t\\t\\t\\ttarget, method=method, tolerance=tolerance, limit=limit\\n\\t\\t\\t)\\n\\t\\tif self.step > 0:\\n\\t\\t\\tstart, stop, step = self.start, self.stop, self.step\\n\\t\\telse:\\n\\t\\t\\treverse = self._range[::-1]\\n\\t\\t\\tstart, stop, step = reverse.start, reverse.stop, reverse.step\\n\\t\\ttarget_array = np.asarray(target)\\n\\t\\tif not (is_integer_dtype(target_array) and target_array.ndim == 1):\\n\\t\\t\\treturn super().get_indexer(target, method=method, tolerance=tolerance)\\n\\t\\tlocs = target_array - start\\n\\t\\tvalid = (locs % step == 0) & (locs >= 0) & (target_array < stop)\\n\\t\\tlocs[~valid] = -1\\n\\t\\tlocs[valid] = locs[valid] / step\\n\\t\\tif step != self.step:\\n\\t\\t\\tlocs[valid] = len(self) - 1 - locs[valid]\\n\\t\\treturn ensure_platform_int(locs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_indexer(self, target, method=None, limit=None, tolerance=None):\\n\\t\\tif com.any_not_none(method, tolerance, limit) or not is_list_like(target):\\n\\t\\t\\treturn super().get_indexer(\\n\\t\\t\\t\\ttarget, method=method, tolerance=tolerance, limit=limit\\n\\t\\t\\t)\\n\\t\\tif self.step > 0:\\n\\t\\t\\tstart, stop, step = self.start, self.stop, self.step\\n\\t\\telse:\\n\\t\\t\\treverse = self._range[::-1]\\n\\t\\t\\tstart, stop, step = reverse.start, reverse.stop, reverse.step\\n\\t\\ttarget_array = np.asarray(target)\\n\\t\\tif not (is_integer_dtype(target_array) and target_array.ndim == 1):\\n\\t\\t\\treturn super().get_indexer(target, method=method, tolerance=tolerance)\\n\\t\\tlocs = target_array - start\\n\\t\\tvalid = (locs % step == 0) & (locs >= 0) & (target_array < stop)\\n\\t\\tlocs[~valid] = -1\\n\\t\\tlocs[valid] = locs[valid] / step\\n\\t\\tif step != self.step:\\n\\t\\t\\tlocs[valid] = len(self) - 1 - locs[valid]\\n\\t\\treturn ensure_platform_int(locs)"
  },
  {
    "code": "def _chop(self, sdata, slice_obj):\\n        return sdata[slice_obj]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: groupby with a Float like index misbehaving when the index is non-monotonic (related GH5375)",
    "fixed_code": "def _chop(self, sdata, slice_obj):\\n        return sdata.iloc[slice_obj]"
  },
  {
    "code": "def pop(self) -> None:\\n        if self.is_backtracking:\\n            self.stack.pop()\\n        else:\\n            popdfa, popstate, popnode = self.stack.pop()\\n            newnode = convert(self.grammar, popnode)\\n            if self.stack:\\n                dfa, state, node = self.stack[-1]\\n                assert node[-1] is not None\\n                node[-1].append(newnode)\\n            else:\\n                self.rootnode = newnode\\n                self.rootnode.used_names = self.used_names",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pop(self) -> None:\\n        if self.is_backtracking:\\n            self.stack.pop()\\n        else:\\n            popdfa, popstate, popnode = self.stack.pop()\\n            newnode = convert(self.grammar, popnode)\\n            if self.stack:\\n                dfa, state, node = self.stack[-1]\\n                assert node[-1] is not None\\n                node[-1].append(newnode)\\n            else:\\n                self.rootnode = newnode\\n                self.rootnode.used_names = self.used_names"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        host=dict(required=True, type=\"str\"),\\n        username=dict(fallback=(env_fallback, [\"ANSIBLE_NET_USERNAME\"])),\\n        password=dict(fallback=(env_fallback, [\"ANSIBLE_NET_PASSWORD\"]), no_log=True),\\n        state=dict(choices=[\"absent\", \"present\"], type=\"str\", default=\"present\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_username=dict(required=False, type=\"str\"),\\n        device_password=dict(required=False, type=\"str\", no_log=True),\\n        device_unique_name=dict(required=True, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec, supports_check_mode=True,)\\n    paramgram = {\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_username\": module.params[\"device_username\"],\\n        \"device_password\": module.params[\"device_password\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"state\": module.params[\"state\"]\\n    }\\n    if module.params[\"host\"] is None or module.params[\"username\"] is None or module.params[\"password\"] is None:\\n        module.fail_json(msg=\"Host and username are required for connection\")\\n    fmg = AnsibleFortiManager(module, module.params[\"host\"], module.params[\"username\"], module.params[\"password\"])\\n    response = fmg.login()\\n    if response[1]['status']['code'] != 0:\\n        module.fail_json(msg=\"Connection to FortiManager Failed\")\\n    else:\\n        results = (-100000, {\"msg\": \"Nothing Happened.\"})\\n        if paramgram[\"state\"] == \"present\":\\n            results = discover_device(fmg, paramgram)\\n            if results[0] != 0:\\n                if results[0] == -20042:\\n                    fmgr_logout(fmg, module, msg=\"Couldn't contact device on network\", results=results, good_codes=[0])\\n                else:\\n                    fmgr_logout(fmg, module, msg=\"Discovering Device Failed\", results=results, good_codes=[0])\\n            if results[0] == 0:\\n                results = add_device(fmg, paramgram)\\n                if results[0] != 0 and results[0] != -20010:\\n                    fmgr_logout(fmg, module, msg=\"Adding Device Failed\", results=results, good_codes=[0])\\n        if paramgram[\"state\"] == \"absent\":\\n            results = delete_device(fmg, paramgram)\\n            if results[0] != 0:\\n                fmgr_logout(fmg, module, msg=\"Deleting Device Failed\", results=results, good_codes=[0])\\n    fmg.logout()\\n    return module.exit_json(**results[1])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_device (#52767)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"delete\"], type=\"str\", default=\"add\"),\\n        blind_add=dict(choices=[\"enable\", \"disable\"], type=\"str\", default=\"disable\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_username=dict(required=False, type=\"str\"),\\n        device_password=dict(required=False, type=\"str\", no_log=True),\\n        device_unique_name=dict(required=True, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_username\": module.params[\"device_username\"],\\n        \"device_password\": module.params[\"device_password\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"mode\": module.params[\"mode\"]\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"mode\"] == \"add\":\\n            if module.params[\"blind_add\"] == \"disable\":\\n                exists_results = get_device(fmgr, paramgram)\\n                fmgr.govern_response(module=module, results=exists_results, good_codes=(0, -3), changed=False,\\n                                     ansible_facts=fmgr.construct_ansible_facts(exists_results,\\n                                                                                module.params, paramgram))\\n            discover_results = discover_device(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=discover_results, stop_on_success=False,\\n                                 ansible_facts=fmgr.construct_ansible_facts(discover_results,\\n                                                                            module.params, paramgram))\\n            if discover_results[0] == 0:\\n                results = add_device(fmgr, paramgram)\\n                fmgr.govern_response(module=module, results=discover_results, stop_on_success=True,\\n                                     changed_if_success=True,\\n                                     ansible_facts=fmgr.construct_ansible_facts(discover_results,\\n                                                                                module.params, paramgram))\\n        if paramgram[\"mode\"] == \"delete\":\\n            results = delete_device(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results,\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def create_transfer_job(self, project_id, description, schedule, transfer_spec):\\n        transfer_job = {\\n            'status': 'ENABLED',\\n            'projectId': project_id,\\n            'description': description,\\n            'transferSpec': transfer_spec,\\n            'schedule': schedule or self._schedule_once_now(),\\n        }\\n        return self.get_conn().transferJobs().create(body=transfer_job).execute()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3531] Add gcs to gcs transfer operator. (#4331)",
    "fixed_code": "def create_transfer_job(self, description, schedule, transfer_spec, project_id=None):\\n        transfer_job = {\\n            'status': 'ENABLED',\\n            'projectId': project_id or self.project_id,\\n            'description': description,\\n            'transferSpec': transfer_spec,\\n            'schedule': schedule or self._schedule_once_now(),\\n        }\\n        return self.get_conn().transferJobs().create(body=transfer_job).execute()"
  },
  {
    "code": "def coerce_to_array(\\n    values, mask=None, copy: bool = False\\n) -> tuple[np.ndarray, np.ndarray]:\\n    if isinstance(values, BooleanArray):\\n        if mask is not None:\\n            raise ValueError(\"cannot pass mask for BooleanArray input\")\\n        values, mask = values._data, values._mask\\n        if copy:\\n            values = values.copy()\\n            mask = mask.copy()\\n        return values, mask\\n    mask_values = None\\n    if isinstance(values, np.ndarray) and values.dtype == np.bool_:\\n        if copy:\\n            values = values.copy()\\n    elif isinstance(values, np.ndarray) and is_numeric_dtype(values.dtype):\\n        mask_values = isna(values)\\n        values_bool = np.zeros(len(values), dtype=bool)\\n        values_bool[~mask_values] = values[~mask_values].astype(bool)\\n        if not np.all(\\n            values_bool[~mask_values].astype(values.dtype) == values[~mask_values]\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n        values = values_bool\\n    else:\\n        values_object = np.asarray(values, dtype=object)\\n        inferred_dtype = lib.infer_dtype(values_object, skipna=True)\\n        integer_like = (\"floating\", \"integer\", \"mixed-integer-float\")\\n        if inferred_dtype not in (\"boolean\", \"empty\") + integer_like:\\n            raise TypeError(\"Need to pass bool-like values\")\\n        mask_values = isna(values_object)\\n        values = np.zeros(len(values), dtype=bool)\\n        values[~mask_values] = values_object[~mask_values].astype(bool)\\n        if (inferred_dtype in integer_like) and not (\\n            np.all(\\n                values[~mask_values].astype(float)\\n                == values_object[~mask_values].astype(float)\\n            )\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n    if mask is None and mask_values is None:\\n        mask = np.zeros(values.shape, dtype=bool)\\n    elif mask is None:\\n        mask = mask_values\\n    else:\\n        if isinstance(mask, np.ndarray) and mask.dtype == np.bool_:\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n            else:\\n                if copy:\\n                    mask = mask.copy()\\n        else:\\n            mask = np.array(mask, dtype=bool)\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n    if values.shape != mask.shape:\\n        raise ValueError(\"values.shape and mask.shape must match\")\\n    return values, mask",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def coerce_to_array(\\n    values, mask=None, copy: bool = False\\n) -> tuple[np.ndarray, np.ndarray]:\\n    if isinstance(values, BooleanArray):\\n        if mask is not None:\\n            raise ValueError(\"cannot pass mask for BooleanArray input\")\\n        values, mask = values._data, values._mask\\n        if copy:\\n            values = values.copy()\\n            mask = mask.copy()\\n        return values, mask\\n    mask_values = None\\n    if isinstance(values, np.ndarray) and values.dtype == np.bool_:\\n        if copy:\\n            values = values.copy()\\n    elif isinstance(values, np.ndarray) and is_numeric_dtype(values.dtype):\\n        mask_values = isna(values)\\n        values_bool = np.zeros(len(values), dtype=bool)\\n        values_bool[~mask_values] = values[~mask_values].astype(bool)\\n        if not np.all(\\n            values_bool[~mask_values].astype(values.dtype) == values[~mask_values]\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n        values = values_bool\\n    else:\\n        values_object = np.asarray(values, dtype=object)\\n        inferred_dtype = lib.infer_dtype(values_object, skipna=True)\\n        integer_like = (\"floating\", \"integer\", \"mixed-integer-float\")\\n        if inferred_dtype not in (\"boolean\", \"empty\") + integer_like:\\n            raise TypeError(\"Need to pass bool-like values\")\\n        mask_values = isna(values_object)\\n        values = np.zeros(len(values), dtype=bool)\\n        values[~mask_values] = values_object[~mask_values].astype(bool)\\n        if (inferred_dtype in integer_like) and not (\\n            np.all(\\n                values[~mask_values].astype(float)\\n                == values_object[~mask_values].astype(float)\\n            )\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n    if mask is None and mask_values is None:\\n        mask = np.zeros(values.shape, dtype=bool)\\n    elif mask is None:\\n        mask = mask_values\\n    else:\\n        if isinstance(mask, np.ndarray) and mask.dtype == np.bool_:\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n            else:\\n                if copy:\\n                    mask = mask.copy()\\n        else:\\n            mask = np.array(mask, dtype=bool)\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n    if values.shape != mask.shape:\\n        raise ValueError(\"values.shape and mask.shape must match\")\\n    return values, mask"
  },
  {
    "code": "def get_traceback_data(self):\\n\\t\\tif self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\\n\\t\\t\\tself.template_does_not_exist = True\\n\\t\\t\\tself.postmortem = self.exc_value.chain or [self.exc_value]\\n\\t\\tframes = self.get_traceback_frames()\\n\\t\\tfor i, frame in enumerate(frames):\\n\\t\\t\\tif 'vars' in frame:\\n\\t\\t\\t\\tframe_vars = []\\n\\t\\t\\t\\tfor k, v in frame['vars']:\\n\\t\\t\\t\\t\\tv = pprint(v)\\n\\t\\t\\t\\t\\tif len(v) > 4096:\\n\\t\\t\\t\\t\\t\\tv = '%s... <trimmed %d bytes string>' % (v[0:4096], len(v))\\n\\t\\t\\t\\t\\tframe_vars.append((k, force_escape(v)))\\n\\t\\t\\t\\tframe['vars'] = frame_vars\\n\\t\\t\\tframes[i] = frame\\n\\t\\tunicode_hint = ''\\n\\t\\tif self.exc_type and issubclass(self.exc_type, UnicodeError):\\n\\t\\t\\tstart = getattr(self.exc_value, 'start', None)\\n\\t\\t\\tend = getattr(self.exc_value, 'end', None)\\n\\t\\t\\tif start is not None and end is not None:\\n\\t\\t\\t\\tunicode_str = self.exc_value.args[1]\\n\\t\\t\\t\\tunicode_hint = force_text(\\n\\t\\t\\t\\t\\tunicode_str[max(start - 5, 0):min(end + 5, len(unicode_str))],\\n\\t\\t\\t\\t\\t'ascii', errors='replace'\\n\\t\\t\\t\\t)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed CVE-2017-12794 -- Fixed XSS possibility in traceback section of technical 500 debug page.\\n\\nThis is a security fix.",
    "fixed_code": "def get_traceback_data(self):\\n\\t\\tif self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\\n\\t\\t\\tself.template_does_not_exist = True\\n\\t\\t\\tself.postmortem = self.exc_value.chain or [self.exc_value]\\n\\t\\tframes = self.get_traceback_frames()\\n\\t\\tfor i, frame in enumerate(frames):\\n\\t\\t\\tif 'vars' in frame:\\n\\t\\t\\t\\tframe_vars = []\\n\\t\\t\\t\\tfor k, v in frame['vars']:\\n\\t\\t\\t\\t\\tv = pprint(v)\\n\\t\\t\\t\\t\\tif len(v) > 4096:\\n\\t\\t\\t\\t\\t\\tv = '%s... <trimmed %d bytes string>' % (v[0:4096], len(v))\\n\\t\\t\\t\\t\\tframe_vars.append((k, v))\\n\\t\\t\\t\\tframe['vars'] = frame_vars\\n\\t\\t\\tframes[i] = frame\\n\\t\\tunicode_hint = ''\\n\\t\\tif self.exc_type and issubclass(self.exc_type, UnicodeError):\\n\\t\\t\\tstart = getattr(self.exc_value, 'start', None)\\n\\t\\t\\tend = getattr(self.exc_value, 'end', None)\\n\\t\\t\\tif start is not None and end is not None:\\n\\t\\t\\t\\tunicode_str = self.exc_value.args[1]\\n\\t\\t\\t\\tunicode_hint = force_text(\\n\\t\\t\\t\\t\\tunicode_str[max(start - 5, 0):min(end + 5, len(unicode_str))],\\n\\t\\t\\t\\t\\t'ascii', errors='replace'\\n\\t\\t\\t\\t)"
  },
  {
    "code": "def test_streamplot_limits():\\n\\tax = plt.axes()\\n\\tx = np.linspace(-5, 10, 20)\\n\\ty = np.linspace(-2, 4, 10)\\n\\ty, x = np.meshgrid(y, x)\\n\\ttrans = mtransforms.Affine2D().translate(25, 32) + ax.transData\\n\\tplt.barbs(x, y, np.sin(x), np.cos(y), transform=trans)\\n\\tassert_array_almost_equal(ax.dataLim.bounds, (20, 30, 15, 6),\\n\\t\\t\\t\\t\\t\\t\\t  decimal=1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_streamplot_limits():\\n\\tax = plt.axes()\\n\\tx = np.linspace(-5, 10, 20)\\n\\ty = np.linspace(-2, 4, 10)\\n\\ty, x = np.meshgrid(y, x)\\n\\ttrans = mtransforms.Affine2D().translate(25, 32) + ax.transData\\n\\tplt.barbs(x, y, np.sin(x), np.cos(y), transform=trans)\\n\\tassert_array_almost_equal(ax.dataLim.bounds, (20, 30, 15, 6),\\n\\t\\t\\t\\t\\t\\t\\t  decimal=1)"
  },
  {
    "code": "def _exact_ratio(x):\\n    try:\\n        if type(x) is float:\\n            return x.as_integer_ratio()\\n        try:\\n            return (x.numerator, x.denominator)\\n        except AttributeError:\\n            try:\\n                return x.as_integer_ratio()\\n            except AttributeError:\\n                try:\\n                    return _decimal_to_ratio(x)\\n                except AttributeError:\\n                    pass\\n    except (OverflowError, ValueError):\\n        assert not math.isfinite(x)\\n        return (x, None)\\n    msg = \"can't convert type '{}' to numerator/denominator\"\\n    raise TypeError(msg.format(type(x).__name__))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _exact_ratio(x):\\n    try:\\n        if type(x) is float:\\n            return x.as_integer_ratio()\\n        try:\\n            return (x.numerator, x.denominator)\\n        except AttributeError:\\n            try:\\n                return x.as_integer_ratio()\\n            except AttributeError:\\n                try:\\n                    return _decimal_to_ratio(x)\\n                except AttributeError:\\n                    pass\\n    except (OverflowError, ValueError):\\n        assert not math.isfinite(x)\\n        return (x, None)\\n    msg = \"can't convert type '{}' to numerator/denominator\"\\n    raise TypeError(msg.format(type(x).__name__))"
  },
  {
    "code": "def _rescale(self, exp, rounding=None, context=None, watchexp=1):\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special:\\n            if self._isinfinity():\\n                return context._raise_error(InvalidOperation, 'rescale with an INF')\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n        if watchexp and (context.Emax  < exp or context.Etiny() > exp):\\n            return context._raise_error(InvalidOperation, 'rescale(a, INF)')\\n        if not self:\\n            ans = Decimal(self)\\n            ans._int = (0,)\\n            ans._exp = exp\\n            return ans\\n        diff = self._exp - exp\\n        digits = len(self._int) + diff\\n        if watchexp and digits > context.prec:\\n            return context._raise_error(InvalidOperation, 'Rescale > prec')\\n        tmp = Decimal(self)\\n        tmp._int = (0,) + tmp._int\\n        digits += 1\\n        if digits < 0:\\n            tmp._exp = -digits + tmp._exp\\n            tmp._int = (0,1)\\n            digits = 1\\n        tmp = tmp._round(digits, rounding, context=context)\\n        if tmp._int[0] == 0 and len(tmp._int) > 1:\\n            tmp._int = tmp._int[1:]\\n        tmp._exp = exp\\n        tmp_adjusted = tmp.adjusted()\\n        if tmp and tmp_adjusted < context.Emin:\\n            context._raise_error(Subnormal)\\n        elif tmp and tmp_adjusted > context.Emax:\\n            return context._raise_error(InvalidOperation, 'rescale(a, INF)')\\n        return tmp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _rescale(self, exp, rounding):\\n        if self._is_special:\\n            return Decimal(self)\\n        if not self:\\n            return _dec_from_triple(self._sign, '0', exp)\\n        if self._exp >= exp:\\n            return _dec_from_triple(self._sign,\\n                                        self._int + '0'*(self._exp - exp), exp)\\n        digits = len(self._int) + self._exp - exp\\n        if digits < 0:\\n            self = _dec_from_triple(self._sign, '1', exp-1)\\n            digits = 0\\n        this_function = getattr(self, self._pick_rounding_function[rounding])\\n        changed = this_function(digits)\\n        coeff = self._int[:digits] or '0'\\n        if changed == 1:\\n            coeff = str(int(coeff)+1)\\n        return _dec_from_triple(self._sign, coeff, exp)"
  },
  {
    "code": "def check_pairwise_arrays(X, Y, precomputed=False):\\n    X, Y, dtype = _return_float_dtype(X, Y)\\n    if Y is X or Y is None:\\n        X = Y = check_array(X, accept_sparse='csr', dtype=dtype)\\n    else:\\n        X = check_array(X, accept_sparse='csr', dtype=dtype)\\n        Y = check_array(Y, accept_sparse='csr', dtype=dtype)\\n    if precomputed:\\n        if X.shape[1] != Y.shape[0]:\\n            raise ValueError(\"Precomputed metric requires shape \"\\n                             \"(n_queries, n_indexed). Got (%d, %d) \"\\n                             \"for %d indexed.\" %\\n                             (X.shape[0], X.shape[1], Y.shape[0]))\\n    elif X.shape[1] != Y.shape[1]:\\n        raise ValueError(\"Incompatible dimension for X and Y matrices: \"\\n                         \"X.shape[1] == %d while Y.shape[1] == %d\" % (\\n                             X.shape[1], Y.shape[1]))\\n    return X, Y",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[MRG+2] convert to boolean arrays for boolean distances (#6932)\\n\\nfor example jaccard.",
    "fixed_code": "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\\n    X, Y, dtype_float = _return_float_dtype(X, Y)\\n    warn_on_dtype = dtype is not None\\n    estimator = 'check_pairwise_arrays'\\n    if dtype is None:\\n        dtype = dtype_float\\n    if Y is X or Y is None:\\n        X = Y = check_array(X, accept_sparse='csr', dtype=dtype,\\n                            warn_on_dtype=warn_on_dtype, estimator=estimator)\\n    else:\\n        X = check_array(X, accept_sparse='csr', dtype=dtype,\\n                        warn_on_dtype=warn_on_dtype, estimator=estimator)\\n        Y = check_array(Y, accept_sparse='csr', dtype=dtype,\\n                        warn_on_dtype=warn_on_dtype, estimator=estimator)\\n    if precomputed:\\n        if X.shape[1] != Y.shape[0]:\\n            raise ValueError(\"Precomputed metric requires shape \"\\n                             \"(n_queries, n_indexed). Got (%d, %d) \"\\n                             \"for %d indexed.\" %\\n                             (X.shape[0], X.shape[1], Y.shape[0]))\\n    elif X.shape[1] != Y.shape[1]:\\n        raise ValueError(\"Incompatible dimension for X and Y matrices: \"\\n                         \"X.shape[1] == %d while Y.shape[1] == %d\" % (\\n                             X.shape[1], Y.shape[1]))\\n    return X, Y"
  },
  {
    "code": "def update_sub(x, decrement):\\n\\treturn tf_state_ops.assign_sub(x, decrement)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix initializers and update ops.",
    "fixed_code": "def update_sub(x, decrement):\\n\\top = tf_state_ops.assign_sub(x, decrement)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)"
  },
  {
    "code": "def eig(a):\\t\"\"\"\\tCompute the eigenvalues and right eigenvectors of a square array.\\tParameters\\t----------\\ta : array_like, shape (M, M)\\t\\tA square array of real or complex elements.\\tReturns\\t-------\\tw : ndarray, shape (M,)\\t\\tThe eigenvalues, each repeated according to its multiplicity.\\t\\tThe eigenvalues are not necessarily ordered, nor are they\\t\\tnecessarily real for real arrays (though for real arrays\\t\\tcomplex-valued eigenvalues should occur in conjugate pairs).\\tv : ndarray, shape (M, M)\\t\\tThe normalized (unit \"length\") eigenvectors, such that the\\t\\tcolumn ``v[:,i]`` is the eigenvector corresponding to the\\t\\teigenvalue ``w[i]``.\\tRaises\\t------\\tLinAlgError\\t\\tIf the eigenvalue computation does not converge.\\tSee Also\\t--------\\teigvalsh : eigenvalues of a symmetric or Hermitian (conjugate symmetric)\\t   array.\\teigvals : eigenvalues of a non-symmetric array.\\tNotes\\t-----\\tThis is a simple interface to the LAPACK routines dgeev and zgeev\\twhich compute the eigenvalues and eigenvectors of, respectively,\\tgeneral real- and complex-valued square arrays.\\tThe number `w` is an eigenvalue of `a` if there exists a vector\\t`v` such that ``dot(a,v) = w * v``. Thus, the arrays `a`, `w`, and\\t`v` satisfy the equations ``dot(a[i,:], v[i]) = w[i] * v[:,i]``\\tfor :math:`i \\\\in \\\\{0,...,M-1\\\\}`.\\tThe array `v` of eigenvectors may not be of maximum rank, that is, some\\tof the columns may be linearly dependent, although round-off error may\\tobscure that fact. If the eigenvalues are all different, then theoretically\\tthe eigenvectors are linearly independent. Likewise, the (complex-valued)\\tmatrix of eigenvectors `v` is unitary if the matrix `a` is normal, i.e.,\\tif ``dot(a, a.H) = dot(a.H, a)``, where `a.H` denotes the conjugate\\ttranspose of `a`.\\tFinally, it is emphasized that `v` consists of the *right* (as in\\tright-hand side) eigenvectors of `a`.  A vector `y` satisfying\\t``dot(y.T, a) = z * y.T`` for some number `z` is called a *left*\\teigenvector of `a`, and, in general, the left and right eigenvectors\\tof a matrix are not necessarily the (perhaps conjugate) transposes\\tof each other.\\tReferences\\t----------\\tG. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL,\\tAcademic Press, Inc., 1980, Various pp.\\tExamples\\t--------\\t>>> from numpy import linalg as LA\\t(Almost) trivial example with real e-values and e-vectors.\\t>>> w, v = LA.eig(np.diag((1, 2, 3)))\\t>>> w; v\\tarray([ 1.,  2.,  3.])\\tarray([[ 1.,  0.,  0.],\\t\\t   [ 0.,  1.,  0.],\\t\\t   [ 0.,  0.,  1.]])\\tReal matrix possessing complex e-values and e-vectors; note that the\\te-values are complex conjugates of each other.\\t>>> w, v = LA.eig(np.array([[1, -1], [1, 1]]))\\t>>> w; v\\tarray([ 1. + 1.j,  1. - 1.j])\\tarray([[ 0.70710678+0.j\\t\\t,  0.70710678+0.j\\t\\t],\\t\\t   [ 0.00000000-0.70710678j,  0.00000000+0.70710678j]])\\tComplex-valued matrix with real e-values (but complex-valued e-vectors);\\tnote that a.conj().T = a, i.e., a is Hermitian.\\t>>> a = np.array([[1, 1j], [-1j, 1]])\\t>>> w, v = LA.eig(a)\\t>>> w; v\\tarray([  2.00000000e+00+0.j,   5.98651912e-36+0.j])",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "3K: linalg: fix integer division issue and tests",
    "fixed_code": "def eig(a):\\n\\ta, wrap = _makearray(a)\\n\\t_assertRank2(a)\\n\\t_assertSquareness(a)\\n\\t_assertFinite(a)\\n\\ta, t, result_t = _convertarray(a) \\n\\treal_t = _linalgRealType(t)\\n\\tn = a.shape[0]\\n\\tdummy = zeros((1,), t)\\n\\tif isComplexType(t):\\n\\t\\tlapack_routine = lapack_lite.zgeev\\n\\t\\tw = zeros((n,), t)\\n\\t\\tv = zeros((n, n), t)\\n\\t\\tlwork = 1\\n\\t\\twork = zeros((lwork,), t)\\n\\t\\trwork = zeros((2*n,), real_t)\\n\\t\\tresults = lapack_routine(_N, _V, n, a, n, w,\\n\\t\\t\\t\\t\\t\\t\\t\\t dummy, 1, v, n, work, -1, rwork, 0)\\n\\t\\tlwork = int(abs(work[0]))\\n\\t\\twork = zeros((lwork,), t)\\n\\t\\tresults = lapack_routine(_N, _V, n, a, n, w,\\n\\t\\t\\t\\t\\t\\t\\t\\t dummy, 1, v, n, work, lwork, rwork, 0)\\n\\telse:\\n\\t\\tlapack_routine = lapack_lite.dgeev\\n\\t\\twr = zeros((n,), t)\\n\\t\\twi = zeros((n,), t)\\n\\t\\tvr = zeros((n, n), t)\\n\\t\\tlwork = 1\\n\\t\\twork = zeros((lwork,), t)\\n\\t\\tresults = lapack_routine(_N, _V, n, a, n, wr, wi,\\n\\t\\t\\t\\t\\t\\t\\t\\t  dummy, 1, vr, n, work, -1, 0)\\n\\t\\tlwork = int(work[0])\\n\\t\\twork = zeros((lwork,), t)\\n\\t\\tresults = lapack_routine(_N, _V, n, a, n, wr, wi,\\n\\t\\t\\t\\t\\t\\t\\t\\t  dummy, 1, vr, n, work, lwork, 0)\\n\\t\\tif all(wi == 0.0):\\n\\t\\t\\tw = wr\\n\\t\\t\\tv = vr\\n\\t\\t\\tresult_t = _realType(result_t)\\n\\t\\telse:\\n\\t\\t\\tw = wr+1j*wi\\n\\t\\t\\tv = array(vr, w.dtype)\\n\\t\\t\\tind = flatnonzero(wi != 0.0)\\t  \\n\\t\\t\\tfor i in range(len(ind)//2):\\n\\t\\t\\t\\tv[ind[2*i]] = vr[ind[2*i]] + 1j*vr[ind[2*i+1]]\\n\\t\\t\\t\\tv[ind[2*i+1]] = vr[ind[2*i]] - 1j*vr[ind[2*i+1]]\\n\\t\\t\\tresult_t = _complexType(result_t)\\n\\tif results['info'] > 0:\\n\\t\\traise LinAlgError, 'Eigenvalues did not converge'\\n\\tvt = v.transpose().astype(result_t)\\n\\treturn w.astype(result_t), wrap(vt)"
  },
  {
    "code": "def decode_header(header):\\n    if not ecre.search(header):\\n        return [(header, None)]\\n    words = []\\n    for line in header.splitlines():\\n        parts = ecre.split(line)\\n        while parts:\\n            unencoded = parts.pop(0).strip()\\n            if unencoded:\\n                words.append((unencoded, None, None))\\n            if parts:\\n                charset = parts.pop(0).lower()\\n                encoding = parts.pop(0).lower()\\n                encoded = parts.pop(0)\\n                words.append((encoded, encoding, charset))\\n    decoded_words = []\\n    for encoded_string, encoding, charset in words:\\n        if encoding is None:\\n            decoded_words.append((encoded_string, charset))\\n        elif encoding == 'q':\\n            word = email.quoprimime.header_decode(encoded_string)\\n            decoded_words.append((word, charset))\\n        elif encoding == 'b':\\n            try:\\n                word = email.base64mime.decode(encoded_string)\\n            except binascii.Error:\\n                raise HeaderParseError('Base64 decoding error')\\n            else:\\n                decoded_words.append((word, charset))\\n        else:\\n            raise AssertionError('Unexpected encoding: ' + encoding)\\n    collapsed = []\\n    last_word = last_charset = None\\n    for word, charset in decoded_words:\\n        if isinstance(word, str):\\n            word = bytes(word, 'raw-unicode-escape')\\n        if last_word is None:\\n            last_word = word\\n            last_charset = charset\\n        elif charset != last_charset:\\n            collapsed.append((last_word, last_charset))\\n            last_word = word\\n            last_charset = charset\\n        elif last_charset is None:\\n            last_word += BSPACE + word\\n        else:\\n            last_word += word\\n    collapsed.append((last_word, last_charset))\\n    return collapsed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 83690 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n........\\n  r83690 | r.david.murray | 2010-08-03 18:14:10 -0400 (Tue, 03 Aug 2010) | 10 lines\\n\\n  #3196: if needed pad a short base64 encoded word before trying to decode.\\n\\n  The RFCs encourage following Postel's law: be liberal in what you accept.\\n  So if someone forgot to pad the base64 encoded word payload to an\\n  even four bytes, we add the padding before handing it to base64mime.decode.\\n  Previously, missing padding resulted in a HeaderParseError.\\n\\n  Patch by Jason Williams.\\n........",
    "fixed_code": "def decode_header(header):\\n    if not ecre.search(header):\\n        return [(header, None)]\\n    words = []\\n    for line in header.splitlines():\\n        parts = ecre.split(line)\\n        while parts:\\n            unencoded = parts.pop(0).strip()\\n            if unencoded:\\n                words.append((unencoded, None, None))\\n            if parts:\\n                charset = parts.pop(0).lower()\\n                encoding = parts.pop(0).lower()\\n                encoded = parts.pop(0)\\n                words.append((encoded, encoding, charset))\\n    decoded_words = []\\n    for encoded_string, encoding, charset in words:\\n        if encoding is None:\\n            decoded_words.append((encoded_string, charset))\\n        elif encoding == 'q':\\n            word = email.quoprimime.header_decode(encoded_string)\\n            decoded_words.append((word, charset))\\n        elif encoding == 'b':\\n            paderr = len(encoded_string) % 4   \\n            if paderr:\\n                encoded_string += '==='[:4 - paderr]\\n            try:\\n                word = email.base64mime.decode(encoded_string)\\n            except binascii.Error:\\n                raise HeaderParseError('Base64 decoding error')\\n            else:\\n                decoded_words.append((word, charset))\\n        else:\\n            raise AssertionError('Unexpected encoding: ' + encoding)\\n    collapsed = []\\n    last_word = last_charset = None\\n    for word, charset in decoded_words:\\n        if isinstance(word, str):\\n            word = bytes(word, 'raw-unicode-escape')\\n        if last_word is None:\\n            last_word = word\\n            last_charset = charset\\n        elif charset != last_charset:\\n            collapsed.append((last_word, last_charset))\\n            last_word = word\\n            last_charset = charset\\n        elif last_charset is None:\\n            last_word += BSPACE + word\\n        else:\\n            last_word += word\\n    collapsed.append((last_word, last_charset))\\n    return collapsed"
  },
  {
    "code": "def load_short_binstring(self):\\n        len = self.read(1)[0]\\n        data = self.read(len)\\n        self.append(self._decode_string(data))\\n    dispatch[SHORT_BINSTRING[0]] = load_short_binstring",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_short_binstring(self):\\n        len = self.read(1)[0]\\n        data = self.read(len)\\n        self.append(self._decode_string(data))\\n    dispatch[SHORT_BINSTRING[0]] = load_short_binstring"
  },
  {
    "code": "def _missing_double(self, vec):\\n        v = vec.view(dtype='u1,u1,u2,u4')\\n        miss = (v['f1'] == 0) & (v['f2'] == 0) & (v['f3'] == 0)\\n        miss1 = (((v['f0'] >= 0x41) & (v['f0'] <= 0x5a)) |\\n                 (v['f0'] == 0x5f) | (v['f0'] == 0x2e))\\n        miss &= miss1\\n        return miss",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _missing_double(self, vec):\\n        v = vec.view(dtype='u1,u1,u2,u4')\\n        miss = (v['f1'] == 0) & (v['f2'] == 0) & (v['f3'] == 0)\\n        miss1 = (((v['f0'] >= 0x41) & (v['f0'] <= 0x5a)) |\\n                 (v['f0'] == 0x5f) | (v['f0'] == 0x2e))\\n        miss &= miss1\\n        return miss"
  },
  {
    "code": "def isnull(obj):\\n    ''\\n    if np.isscalar(obj) or obj is None:\\n        return lib.checknull(obj)\\n    from pandas.core.generic import PandasObject\\n    from pandas import Series\\n    if isinstance(obj, np.ndarray):\\n        if obj.dtype.kind in ('O', 'S'):\\n            shape = obj.shape\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(obj.ravel())\\n            result[:] = vec.reshape(shape)\\n            if isinstance(obj, Series):\\n                result = Series(result, index=obj.index, copy=False)\\n        elif obj.dtype == np.datetime64:\\n            result = np.array(obj).view('i8') == lib.NaT\\n        else:\\n            result = -np.isfinite(obj)\\n        return result\\n    elif isinstance(obj, PandasObject):\\n        return obj.apply(isnull)\\n    else:\\n        return obj is None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def isnull(obj):\\n    ''\\n    if np.isscalar(obj) or obj is None:\\n        return lib.checknull(obj)\\n    from pandas.core.generic import PandasObject\\n    from pandas import Series\\n    if isinstance(obj, np.ndarray):\\n        if obj.dtype.kind in ('O', 'S'):\\n            shape = obj.shape\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(obj.ravel())\\n            result[:] = vec.reshape(shape)\\n            if isinstance(obj, Series):\\n                result = Series(result, index=obj.index, copy=False)\\n        elif obj.dtype == np.datetime64:\\n            result = np.array(obj).view('i8') == lib.NaT\\n        else:\\n            result = -np.isfinite(obj)\\n        return result\\n    elif isinstance(obj, PandasObject):\\n        return obj.apply(isnull)\\n    else:\\n        return obj is None"
  },
  {
    "code": "def tolist(self):\\n        return list(self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "COMPAT: Convert to native datatypes for Series.tolist()\\n\\ncloses #10904\\n\\nAuthor: G\u00e1bor Lipt\u00e1k <gliptak@gmail.com>\\n\\nCloses #13050 from gliptak/npcompat1 and squashes the following commits:\\n\\n4da83ac [G\u00e1bor Lipt\u00e1k] Convert to native datatypes for Series.tolist()",
    "fixed_code": "def tolist(self):\\n        return list(self.asobject)"
  },
  {
    "code": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\\n    trim_url = lambda x, limit=trim_url_limit: limit is not None and (len(x) > limit and ('%s...' % x[:max(0, limit - 3)])) or x\\n    safe_input = isinstance(text, SafeData)\\n    words = word_split_re.split(force_unicode(text))\\n    nofollow_attr = nofollow and ' rel=\"nofollow\"' or ''\\n    for i, word in enumerate(words):\\n        match = None\\n        if '.' in word or '@' in word or ':' in word:\\n            match = punctuation_re.match(word)\\n        if match:\\n            lead, middle, trail = match.groups()\\n            url = None\\n            if middle.startswith('http://') or middle.startswith('https://'):\\n                url = smart_urlquote(middle)\\n            elif middle.startswith('www.') or ('@' not in middle and \\\\n                    middle and middle[0] in string.ascii_letters + string.digits and \\\\n                    (middle.endswith('.org') or middle.endswith('.net') or middle.endswith('.com'))):\\n                url = smart_urlquote('http://%s' % middle)\\n            elif '@' in middle and not ':' in middle and simple_email_re.match(middle):\\n                url = 'mailto:%s' % middle\\n                nofollow_attr = ''\\n            if url:\\n                trimmed = trim_url(middle)\\n                if autoescape and not safe_input:\\n                    lead, trail = escape(lead), escape(trail)\\n                    url, trimmed = escape(url), escape(trimmed)\\n                middle = '<a href=\"%s\"%s>%s</a>' % (url, nofollow_attr, trimmed)\\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\\n            else:\\n                if safe_input:\\n                    words[i] = mark_safe(word)\\n                elif autoescape:\\n                    words[i] = escape(word)\\n        elif safe_input:\\n            words[i] = mark_safe(word)\\n        elif autoescape:\\n            words[i] = escape(word)\\n    return u''.join(words)\\nurlize = allow_lazy(urlize, unicode)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\\n    trim_url = lambda x, limit=trim_url_limit: limit is not None and (len(x) > limit and ('%s...' % x[:max(0, limit - 3)])) or x\\n    safe_input = isinstance(text, SafeData)\\n    words = word_split_re.split(force_unicode(text))\\n    nofollow_attr = nofollow and ' rel=\"nofollow\"' or ''\\n    for i, word in enumerate(words):\\n        match = None\\n        if '.' in word or '@' in word or ':' in word:\\n            match = punctuation_re.match(word)\\n        if match:\\n            lead, middle, trail = match.groups()\\n            url = None\\n            if middle.startswith('http://') or middle.startswith('https://'):\\n                url = smart_urlquote(middle)\\n            elif middle.startswith('www.') or ('@' not in middle and \\\\n                    middle and middle[0] in string.ascii_letters + string.digits and \\\\n                    (middle.endswith('.org') or middle.endswith('.net') or middle.endswith('.com'))):\\n                url = smart_urlquote('http://%s' % middle)\\n            elif '@' in middle and not ':' in middle and simple_email_re.match(middle):\\n                url = 'mailto:%s' % middle\\n                nofollow_attr = ''\\n            if url:\\n                trimmed = trim_url(middle)\\n                if autoescape and not safe_input:\\n                    lead, trail = escape(lead), escape(trail)\\n                    url, trimmed = escape(url), escape(trimmed)\\n                middle = '<a href=\"%s\"%s>%s</a>' % (url, nofollow_attr, trimmed)\\n                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))\\n            else:\\n                if safe_input:\\n                    words[i] = mark_safe(word)\\n                elif autoescape:\\n                    words[i] = escape(word)\\n        elif safe_input:\\n            words[i] = mark_safe(word)\\n        elif autoescape:\\n            words[i] = escape(word)\\n    return u''.join(words)\\nurlize = allow_lazy(urlize, unicode)"
  },
  {
    "code": "def __init__(\\n        self,\\n        n_components=None,\\n        *,\\n        algorithm=\"parallel\",\\n        whiten=\"warn\",\\n        fun=\"logcosh\",\\n        fun_args=None,\\n        max_iter=200,\\n        tol=1e-4,\\n        w_init=None,\\n        whiten_solver=\"svd\",\\n        random_state=None,\\n    ):\\n        super().__init__()\\n        self.n_components = n_components\\n        self.algorithm = algorithm\\n        self.whiten = whiten\\n        self.fun = fun\\n        self.fun_args = fun_args\\n        self.max_iter = max_iter\\n        self.tol = tol\\n        self.w_init = w_init\\n        self.whiten_solver = whiten_solver\\n        self.random_state = random_state",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        n_components=None,\\n        *,\\n        algorithm=\"parallel\",\\n        whiten=\"warn\",\\n        fun=\"logcosh\",\\n        fun_args=None,\\n        max_iter=200,\\n        tol=1e-4,\\n        w_init=None,\\n        whiten_solver=\"svd\",\\n        random_state=None,\\n    ):\\n        super().__init__()\\n        self.n_components = n_components\\n        self.algorithm = algorithm\\n        self.whiten = whiten\\n        self.fun = fun\\n        self.fun_args = fun_args\\n        self.max_iter = max_iter\\n        self.tol = tol\\n        self.w_init = w_init\\n        self.whiten_solver = whiten_solver\\n        self.random_state = random_state"
  },
  {
    "code": "def json_normalize(data, record_path=None, meta=None,\\n                   meta_prefix=None,\\n                   record_prefix=None,\\n                   errors='raise'):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: GH14883: json_normalize now takes a user-specified separator\\n\\ncloses #14883\\n\\nAuthor: Jeff Reback <jeff@reback.net>\\nAuthor: John Owens <jowens@ece.ucdavis.edu>\\n\\nCloses #14950 from jowens/json_normalize-separator and squashes the following commits:\\n\\n0327dd1 [Jeff Reback] compare sorted columns\\nbc5aae8 [Jeff Reback] CLN: fixup json_normalize with sep\\n8edc40e [John Owens] ENH: json_normalize now takes a user-specified separator",
    "fixed_code": "def json_normalize(data, record_path=None, meta=None,\\n                   meta_prefix=None,\\n                   record_prefix=None,\\n                   errors='raise',\\n                   sep='.'):\\n    \"\"\"\\n    \"Normalize\" semi-structured JSON data into a flat table\\n    Parameters\\n    ----------\\n    data : dict or list of dicts\\n        Unserialized JSON objects\\n    record_path : string or list of strings, default None\\n        Path in each object to list of records. If not passed, data will be\\n        assumed to be an array of records\\n    meta : list of paths (string or list of strings), default None\\n        Fields to use as metadata for each record in resulting table\\n    record_prefix : string, default None\\n        If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\n        path to records is ['foo', 'bar']\\n    meta_prefix : string, default None\\n    errors : {'raise', 'ignore'}, default 'raise'\\n        * ignore : will ignore KeyError if keys listed in meta are not\\n        always present\\n        * raise : will raise KeyError if keys listed in meta are not\\n        always present\\n        .. versionadded:: 0.20.0\\n    sep : string, default '.'\\n        Nested records will generate names separated by sep,\\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\\n        .. versionadded:: 0.20.0"
  },
  {
    "code": "def fq_list_names(partition, list_names):\\n    if list_names is None:\\n        return None\\n    return map(lambda x: fqdn_name(partition, x), list_names)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "removes args from the code and corrects few missed tests (#58175)",
    "fixed_code": "def fq_list_names(partition, list_names):\\n    if list_names is None:\\n        return None\\n    return map(lambda x: fq_name(partition, x), list_names)"
  },
  {
    "code": "def state_present(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if existing_commands.get(key):\\n            if key == 'ip router ospf':\\n                if proposed['area'] == existing['area']:\\n                    continue\\n            if existing_commands[key] == proposed_commands[key]:\\n                continue\\n        if key == 'ip ospf passive-interface' and module.params.get('interface').upper().startswith('LO'):\\n            module.fail_json(msg='loopback interface does not support passive_interface')\\n        if key == 'ip ospf network' and value == 'broadcast' and module.params.get('interface').upper().startswith('LO'):\\n            module.fail_json(msg='loopback interface does not support ospf network type broadcast')\\n        if key == 'ip ospf bfd':\\n            cmd = key\\n            if 'disable' in value:\\n                cmd += ' disable'\\n            elif 'default' in value and existing.get('bfd') is not None:\\n                cmd = 'no ' + cmd\\n            commands.append(cmd)\\n            continue\\n        if value is True:\\n            commands.append(key)\\n        elif value is False:\\n            commands.append('no {0}'.format(key))\\n        elif value == 'default':\\n            if existing_commands.get(key):\\n                commands.extend(get_default_commands(existing, proposed,\\n                                                     existing_commands, key,\\n                                                     module))\\n        else:\\n            if (key == 'ip router ospf' or\\n                    key.startswith('ip ospf message-digest-key')):\\n                commands.extend(get_custom_command(commands, proposed,\\n                                                   key, module))\\n            else:\\n                command = '{0} {1}'.format(key, value.lower())\\n                commands.append(command)\\n    if commands:\\n        parents = ['interface {0}'.format(module.params['interface'].capitalize())]\\n        candidate.add(commands, parents=parents)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def state_present(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if existing_commands.get(key):\\n            if key == 'ip router ospf':\\n                if proposed['area'] == existing['area']:\\n                    continue\\n            if existing_commands[key] == proposed_commands[key]:\\n                continue\\n        if key == 'ip ospf passive-interface' and module.params.get('interface').upper().startswith('LO'):\\n            module.fail_json(msg='loopback interface does not support passive_interface')\\n        if key == 'ip ospf network' and value == 'broadcast' and module.params.get('interface').upper().startswith('LO'):\\n            module.fail_json(msg='loopback interface does not support ospf network type broadcast')\\n        if key == 'ip ospf bfd':\\n            cmd = key\\n            if 'disable' in value:\\n                cmd += ' disable'\\n            elif 'default' in value and existing.get('bfd') is not None:\\n                cmd = 'no ' + cmd\\n            commands.append(cmd)\\n            continue\\n        if value is True:\\n            commands.append(key)\\n        elif value is False:\\n            commands.append('no {0}'.format(key))\\n        elif value == 'default':\\n            if existing_commands.get(key):\\n                commands.extend(get_default_commands(existing, proposed,\\n                                                     existing_commands, key,\\n                                                     module))\\n        else:\\n            if (key == 'ip router ospf' or\\n                    key.startswith('ip ospf message-digest-key')):\\n                commands.extend(get_custom_command(commands, proposed,\\n                                                   key, module))\\n            else:\\n                command = '{0} {1}'.format(key, value.lower())\\n                commands.append(command)\\n    if commands:\\n        parents = ['interface {0}'.format(module.params['interface'].capitalize())]\\n        candidate.add(commands, parents=parents)"
  },
  {
    "code": "def predict_margin(self, X):\\n        pass",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def predict_margin(self, X):\\n        pass"
  },
  {
    "code": "def build_extensions(self):\\n        missing = self.detect_modules()\\n        extensions = [ext for ext in self.extensions\\n                      if ext.name not in disabled_module_list]\\n        ext_map = dict((ext.name, i) for i, ext in enumerate(extensions))\\n        if \"_ctypes\" in ext_map:\\n            ctypes = extensions.pop(ext_map[\"_ctypes\"])\\n            extensions.append(ctypes)\\n        self.extensions = extensions\\n        srcdir = sysconfig.get_config_var('srcdir')\\n        if not srcdir:\\n            raise ValueError(\"No source directory; cannot proceed.\")\\n        srcdir = os.path.abspath(srcdir)\\n        moddirlist = [os.path.join(srcdir, 'Modules')]\\n        platform = self.get_platform()\\n        self.distribution.scripts = [os.path.join(srcdir, filename)\\n                                     for filename in self.distribution.scripts]\\n        headers = [sysconfig.get_config_h_filename()]\\n        headers += glob(os.path.join(sysconfig.get_python_inc(), \"*.h\"))\\n        for ext in self.extensions[:]:\\n            ext.sources = [ find_module_file(filename, moddirlist)\\n                            for filename in ext.sources ]\\n            if ext.depends is not None:\\n                ext.depends = [find_module_file(filename, moddirlist)\\n                               for filename in ext.depends]\\n            else:\\n                ext.depends = []\\n            ext.depends.extend(headers)\\n            if ext.name in sys.builtin_module_names:\\n                self.extensions.remove(ext)\\n        if platform != 'mac':\\n            remove_modules = []\\n            for filename in ('Modules/Setup', 'Modules/Setup.local'):\\n                input = text_file.TextFile(filename, join_lines=1)\\n                while 1:\\n                    line = input.readline()\\n                    if not line: break\\n                    line = line.split()\\n                    remove_modules.append(line[0])\\n                input.close()\\n            for ext in self.extensions[:]:\\n                if ext.name in remove_modules:\\n                    self.extensions.remove(ext)\\n        compiler = os.environ.get('CC')\\n        args = {}\\n        if compiler is not None:\\n            (ccshared,cflags) = sysconfig.get_config_vars('CCSHARED','CFLAGS')\\n            args['compiler_so'] = compiler + ' ' + ccshared + ' ' + cflags\\n        self.compiler.set_executables(**args)\\n        build_ext.build_extensions(self)\\n        longest = max([len(e.name) for e in self.extensions])\\n        if self.failed:\\n            longest = max(longest, max([len(name) for name in self.failed]))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 73864 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r73864 | tarek.ziade | 2009-07-06 14:50:46 +0200 (Mon, 06 Jul 2009) | 1 line\\n\\n  Fixed #6377: distutils compiler switch ignored (and added a deprecation warning if compiler is not used as supposed = a string option)\\n........",
    "fixed_code": "def build_extensions(self):\\n        missing = self.detect_modules()\\n        extensions = [ext for ext in self.extensions\\n                      if ext.name not in disabled_module_list]\\n        ext_map = dict((ext.name, i) for i, ext in enumerate(extensions))\\n        if \"_ctypes\" in ext_map:\\n            ctypes = extensions.pop(ext_map[\"_ctypes\"])\\n            extensions.append(ctypes)\\n        self.extensions = extensions\\n        srcdir = sysconfig.get_config_var('srcdir')\\n        if not srcdir:\\n            raise ValueError(\"No source directory; cannot proceed.\")\\n        srcdir = os.path.abspath(srcdir)\\n        moddirlist = [os.path.join(srcdir, 'Modules')]\\n        platform = self.get_platform()\\n        self.distribution.scripts = [os.path.join(srcdir, filename)\\n                                     for filename in self.distribution.scripts]\\n        headers = [sysconfig.get_config_h_filename()]\\n        headers += glob(os.path.join(sysconfig.get_python_inc(), \"*.h\"))\\n        for ext in self.extensions[:]:\\n            ext.sources = [ find_module_file(filename, moddirlist)\\n                            for filename in ext.sources ]\\n            if ext.depends is not None:\\n                ext.depends = [find_module_file(filename, moddirlist)\\n                               for filename in ext.depends]\\n            else:\\n                ext.depends = []\\n            ext.depends.extend(headers)\\n            if ext.name in sys.builtin_module_names:\\n                self.extensions.remove(ext)\\n        if platform != 'mac':\\n            remove_modules = []\\n            for filename in ('Modules/Setup', 'Modules/Setup.local'):\\n                input = text_file.TextFile(filename, join_lines=1)\\n                while 1:\\n                    line = input.readline()\\n                    if not line: break\\n                    line = line.split()\\n                    remove_modules.append(line[0])\\n                input.close()\\n            for ext in self.extensions[:]:\\n                if ext.name in remove_modules:\\n                    self.extensions.remove(ext)\\n        compiler = os.environ.get('CC')\\n        args = {}\\n        if compiler is not None:\\n            (ccshared,cflags) = sysconfig.get_config_vars('CCSHARED','CFLAGS')\\n            args['compiler_so'] = compiler + ' ' + ccshared + ' ' + cflags\\n        self.compiler_obj.set_executables(**args)\\n        build_ext.build_extensions(self)\\n        longest = max([len(e.name) for e in self.extensions])\\n        if self.failed:\\n            longest = max(longest, max([len(name) for name in self.failed]))"
  },
  {
    "code": "def __getitem__(self, idx):\\n\\t\\tif not isinstance(idx, six.integer_types):\\n\\t\\t\\traise TypeError\\n\\t\\treturn list(self.__iter__())[idx]\\n\\t@property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26267 -- Fixed BoundField to reallow slices of subwidgets.",
    "fixed_code": "def __getitem__(self, idx):\\n\\t\\tif not isinstance(idx, six.integer_types + (slice,)):\\n\\t\\t\\traise TypeError\\n\\t\\treturn list(self.__iter__())[idx]\\n\\t@property"
  },
  {
    "code": "def to_timestamp(self, freq=None, how=\"start\", copy=True) -> \"Series\":\\n\\t\\tnew_values = self._values\\n\\t\\tif copy:\\n\\t\\t\\tnew_values = new_values.copy()\\n\\t\\tassert isinstance(self.index, PeriodIndex)\\n\\t\\tnew_index = self.index.to_timestamp(freq=freq, how=how)  \\n\\t\\treturn self._constructor(new_values, index=new_index).__finalize__(\\n\\t\\t\\tself, method=\"to_timestamp\"\\n\\t\\t)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG : Series.to_timestamp and Series.to_period raise user-facing AssertionError (#34067)",
    "fixed_code": "def to_timestamp(self, freq=None, how=\"start\", copy=True) -> \"Series\":\\n\\t\\tnew_values = self._values\\n\\t\\tif copy:\\n\\t\\t\\tnew_values = new_values.copy()\\n\\t\\tif not isinstance(self.index, PeriodIndex):\\n\\t\\t\\traise TypeError(f\"unsupported Type {type(self.index).__name__}\")\\n\\t\\tnew_index = self.index.to_timestamp(freq=freq, how=how)  \\n\\t\\treturn self._constructor(new_values, index=new_index).__finalize__(\\n\\t\\t\\tself, method=\"to_timestamp\"\\n\\t\\t)"
  },
  {
    "code": "def rename(self, index=None, columns=None, copy=True, inplace=False):\\n        from pandas.core.series import _get_rename_function\\n        if index is None and columns is None:\\n            raise Exception('must pass either index or columns')\\n        index_f = _get_rename_function(index)\\n        columns_f = _get_rename_function(columns)\\n        self._consolidate_inplace()\\n        result = self if inplace else self.copy(deep=copy)\\n        if index is not None:\\n            result._rename_index_inplace(index_f)\\n        if columns is not None:\\n            result._rename_columns_inplace(columns_f)\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rename(self, index=None, columns=None, copy=True, inplace=False):\\n        from pandas.core.series import _get_rename_function\\n        if index is None and columns is None:\\n            raise Exception('must pass either index or columns')\\n        index_f = _get_rename_function(index)\\n        columns_f = _get_rename_function(columns)\\n        self._consolidate_inplace()\\n        result = self if inplace else self.copy(deep=copy)\\n        if index is not None:\\n            result._rename_index_inplace(index_f)\\n        if columns is not None:\\n            result._rename_columns_inplace(columns_f)\\n        return result"
  },
  {
    "code": "def _interval_str_to_code(freqstr):\\n    try:\\n        freqstr = freqstr.upper()\\n        return _interval_code_map[freqstr]\\n    except:\\n        alias = _skts_alias_dict[freqstr]\\n        try:\\n            return _interval_code_map[alias]\\n        except:\\n            raise \"Could not interpret frequency %s\" % freqstr\\n_gfc = _get_freq_code",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: legacy time rule support and refactoring, better alias handling. misc tests, #1041",
    "fixed_code": "def _interval_str_to_code(freqstr):\\n    freqstr = _rule_aliases.get(freqstr, freqstr)\\n    freqstr = _rule_aliases.get(freqstr.lower(), freqstr)\\n    try:\\n        freqstr = freqstr.upper()\\n        return _interval_code_map[freqstr]\\n    except:\\n        alias = _skts_alias_dict[freqstr]\\n        try:\\n            return _interval_code_map[alias]\\n        except:\\n            raise \"Could not interpret frequency %s\" % freqstr\\n_gfc = _get_freq_code"
  },
  {
    "code": "def __cmp__(self, other, *rest):\\n' % (self, (other,) + rest))\\n        if type(other) != type(self):\\n            other = bitvec(other, *rest)\\n        length = self._len\\n        if length == 0 or other._len == 0:\\n            return cmp(length, other._len)\\n        if length != other._len:\\n            min_length = min(length, other._len)\\n            return cmp(self[:min_length], other[:min_length]) or \\\\n                      cmp(self[min_length:], other[min_length:])\\n        if self._data == other._data:\\n            return 0\\n        if length == 1:\\n            return cmp(self[0], other[0])\\n        else:\\n            length = length >> 1\\n            return cmp(self[:length], other[:length]) or \\\\n                      cmp(self[length:], other[length:])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __cmp__(self, other, *rest):\\n' % (self, (other,) + rest))\\n        if type(other) != type(self):\\n            other = bitvec(other, *rest)\\n        length = self._len\\n        if length == 0 or other._len == 0:\\n            return cmp(length, other._len)\\n        if length != other._len:\\n            min_length = min(length, other._len)\\n            return cmp(self[:min_length], other[:min_length]) or \\\\n                      cmp(self[min_length:], other[min_length:])\\n        if self._data == other._data:\\n            return 0\\n        if length == 1:\\n            return cmp(self[0], other[0])\\n        else:\\n            length = length >> 1\\n            return cmp(self[:length], other[:length]) or \\\\n                      cmp(self[length:], other[length:])"
  },
  {
    "code": "def appendlist(self, key, value):\\n        self._assert_mutable()\\n        key = str_to_unicode(key, self.encoding)\\n        value = str_to_unicode(value, self.encoding)\\n        super(QueryDict, self).appendlist(key, value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[py3] Ported django.http according to PEP 3333.\\n\\nPerfomed some style cleanup while I was in the area.",
    "fixed_code": "def appendlist(self, key, value):\\n        self._assert_mutable()\\n        key = bytes_to_text(key, self.encoding)\\n        value = bytes_to_text(value, self.encoding)\\n        super(QueryDict, self).appendlist(key, value)"
  },
  {
    "code": "def ewmvar(arg, com=None, span=None, halflife=None, min_periods=0, bias=False,\\n           freq=None, how=None):\\n    com = _get_center_of_mass(com, span, halflife)\\n    arg = _conv_timerule(arg, freq, how)\\n    moment2nd = ewma(arg * arg, com=com, min_periods=min_periods)\\n    moment1st = ewma(arg, com=com, min_periods=min_periods)\\n    result = moment2nd - moment1st ** 2\\n    if not bias:\\n        result *= (1.0 + 2.0 * com) / (2.0 * com)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: ewma() weights incorrect when some values are missing (GH7543)",
    "fixed_code": "def ewmvar(arg, com=None, span=None, halflife=None, min_periods=0, bias=False,\\n           freq=None, how=None, ignore_na=False):\\n    com = _get_center_of_mass(com, span, halflife)\\n    arg = _conv_timerule(arg, freq, how)\\n    moment2nd = ewma(arg * arg, com=com, min_periods=min_periods, ignore_na=ignore_na)\\n    moment1st = ewma(arg, com=com, min_periods=min_periods, ignore_na=ignore_na)\\n    result = moment2nd - moment1st ** 2\\n    if not bias:\\n        result *= (1.0 + 2.0 * com) / (2.0 * com)\\n    return result"
  },
  {
    "code": "def _aggregate_named(self, applyfunc):\\n        result = {}\\n        for k, v in self.primary.indices.iteritems():\\n            grp = self[k]\\n            grp.groupName = k\\n            output = applyfunc(grp)\\n            if isinstance(output, Series):\\n                raise Exception('Given applyfunc did not return a '\\n                                'value from the subseries as expected!')\\n            result[k] = output\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: tests pass except for multi-column. renamed self.klass to DataFrame in test_frame.py",
    "fixed_code": "def _aggregate_named(self, applyfunc):\\n        result = {}\\n        for name in self.primary:\\n            grp = self.get_group(name)\\n            grp.groupName = name\\n            output = applyfunc(grp)\\n            if isinstance(output, Series):\\n                raise Exception('Given applyfunc did not return a '\\n                                'value from the subseries as expected!')\\n            result[name] = output\\n        return result"
  },
  {
    "code": "def as_ul(self):\\n        \"Returns this form rendered as HTML <li>s -- excluding the <ul></ul>.\"\\n        output = []\\n        if self.errors.get(NON_FIELD_ERRORS):\\n            output.append(u'<li>%s</li>' % self.non_field_errors())\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = u'<li>'\\n            if bf.errors:\\n                line += str(bf.errors)\\n            line += u'%s: %s</li>' % (bf.label, bf)\\n            output.append(line)\\n        return u'\\n'.join(output)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #3064 -- newforms: Added <label> support through BoundField.label_tag() method. Also added BoundField.verbose_name and added/updated unit tests. Thanks, SmileyChris",
    "fixed_code": "def as_ul(self):\\n        \"Returns this form rendered as HTML <li>s -- excluding the <ul></ul>.\"\\n        output = []\\n        if self.errors.get(NON_FIELD_ERRORS):\\n            output.append(u'<li>%s</li>' % self.non_field_errors())\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = u'<li>'\\n            if bf.errors:\\n                line += str(bf.errors)\\n            line += u'%s %s</li>' % (bf.label_tag(bf.verbose_name+':'), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)"
  },
  {
    "code": "def _possibly_cast_to_datetime(value, dtype, coerce=False):\\n    if dtype is not None:\\n        if isinstance(dtype, compat.string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if is_datetime64 and dtype != _NS_DTYPE:\\n                if dtype.name == 'datetime64[ns]':\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert datetimelike to dtype [%s]\" % dtype)\\n            elif is_timedelta64 and dtype != _TD_DTYPE:\\n                if dtype.name == 'timedelta64[ns]':\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert timedeltalike to dtype [%s]\" % dtype)\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            from pandas.tseries.tools import to_datetime\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            from pandas.tseries.timedeltas import \\\\n                                _possibly_cast_to_timedelta\\n                            value = _possibly_cast_to_timedelta(value, coerce='compat')\\n                    except:\\n                        pass\\n    else:\\n        if (isinstance(value, np.ndarray) and not\\n                (issubclass(value.dtype.type, np.integer) or\\n                 value.dtype == np.object_)):\\n            pass\\n        else:\\n            v = value\\n            if not is_list_like(v):\\n                v = [v]\\n            if len(v):\\n                inferred_type = lib.infer_dtype(v)\\n                if inferred_type in ['datetime', 'datetime64']:\\n                    try:\\n                        value = tslib.array_to_datetime(np.array(v))\\n                    except:\\n                        pass\\n                elif inferred_type in ['timedelta', 'timedelta64']:\\n                    from pandas.tseries.timedeltas import \\\\n                        _possibly_cast_to_timedelta\\n                    value = _possibly_cast_to_timedelta(value, coerce='compat')\\n    return value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Bug in DataFrame construction with recarray and non-ns datetime dtype (GH6140)",
    "fixed_code": "def _possibly_cast_to_datetime(value, dtype, coerce=False):\\n    if dtype is not None:\\n        if isinstance(dtype, compat.string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if is_datetime64 and dtype != _NS_DTYPE:\\n                if dtype.name == 'datetime64[ns]':\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert datetimelike to dtype [%s]\" % dtype)\\n            elif is_timedelta64 and dtype != _TD_DTYPE:\\n                if dtype.name == 'timedelta64[ns]':\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert timedeltalike to dtype [%s]\" % dtype)\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            from pandas.tseries.tools import to_datetime\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            from pandas.tseries.timedeltas import \\\\n                                _possibly_cast_to_timedelta\\n                            value = _possibly_cast_to_timedelta(value, coerce='compat')\\n                    except:\\n                        pass\\n    else:\\n        is_array = isinstance(value, np.ndarray)\\n        if (is_array and value.dtype.kind in ['M','m']):\\n            dtype = value.dtype\\n            if dtype.kind == 'M' and dtype != _NS_DTYPE:\\n                try:\\n                    value = tslib.array_to_datetime(value)\\n                except:\\n                    raise\\n            elif dtype.kind == 'm' and dtype != _TD_DTYPE:\\n                from pandas.tseries.timedeltas import \\\\n                     _possibly_cast_to_timedelta\\n                value = _possibly_cast_to_timedelta(value, coerce='compat')\\n        elif (is_array and not (\\n            issubclass(value.dtype.type, np.integer) or\\n            value.dtype == np.object_)):\\n            pass\\n        else:\\n            v = value\\n            if not is_list_like(v):\\n                v = [v]\\n            if len(v):\\n                inferred_type = lib.infer_dtype(v)\\n                if inferred_type in ['datetime', 'datetime64']:\\n                    try:\\n                        value = tslib.array_to_datetime(np.array(v))\\n                    except:\\n                        pass\\n                elif inferred_type in ['timedelta', 'timedelta64']:\\n                    from pandas.tseries.timedeltas import \\\\n                        _possibly_cast_to_timedelta\\n                    value = _possibly_cast_to_timedelta(value, coerce='compat')\\n    return value"
  },
  {
    "code": "def process_form(self, form, is_created):\\n\\t\\tconn_id = form.data[\"conn_id\"]\\n\\t\\tconn_type = form.data[\"conn_type\"]\\n\\t\\textra = {}\\n\\t\\textra_json = form.data.get(\"extra\")\\n\\t\\tif extra_json:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\textra.update(json.loads(extra_json))\\n\\t\\t\\texcept (JSONDecodeError, TypeError):\\n\\t\\t\\t\\tflash(\\n\\t\\t\\t\\t\\tMarkup(\\n\\t\\t\\t\\t\\t\\t\"<p>The <em>Extra</em> connection field contained an invalid value for Conn ID: \"\\n\\t\\t\\t\\t\\t\\t\"<q>{conn_id}</q>.</p>\"\\n\\t\\t\\t\\t\\t\\t\"<p>If connection parameters need to be added to <em>Extra</em>, \"\\n\\t\\t\\t\\t\\t\\t\"please make sure they are in the form of a single, valid JSON object.</p><br>\"\\n\\t\\t\\t\\t\\t\\t\"The following <em>Extra</em> parameters were <b>not</b> added to the connection:<br>\"\\n\\t\\t\\t\\t\\t\\t\"{extra_json}\"\\n\\t\\t\\t\\t\\t).format(conn_id=conn_id, extra_json=extra_json),\\n\\t\\t\\t\\t\\tcategory=\"error\",\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tdel form.extra\\n\\t\\tdel extra_json\\n\\t\\tfor key, field_name in self._iter_extra_field_names():\\n\\t\\t\\tif key in form.data and key.startswith(\"extra__\"):\\n\\t\\t\\t\\tconn_type_from_extra_field = key.split(\"__\")[1]\\n\\t\\t\\t\\tif conn_type_from_extra_field == conn_type:\\n\\t\\t\\t\\t\\tvalue = form.data[key]\\n\\t\\t\\t\\t\\tif value != \"\":\\n\\t\\t\\t\\t\\t\\textra[field_name] = value\\n\\t\\tif extra.keys():\\n\\t\\t\\tform.extra.data = json.dumps(extra)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Hide sensitive values from extra in connection edit form (#32309)\\n\\nThe fields that are sensitive (i.e password field is used\\nto show them in Connection edit view) should also be hidden\\nwhen they are stored as \"extra\" in the form extra field.\\n\\nThis PR handles both - replacing such values with a\\nplaceholder as well as not updating the value if the\\nplaceholder has not been modified.",
    "fixed_code": "def process_form(self, form, is_created):\\n\\t\\tconn_id = form.data[\"conn_id\"]\\n\\t\\tconn_type = form.data[\"conn_type\"]\\n\\t\\textra = {}\\n\\t\\textra_json = form.data.get(\"extra\")\\n\\t\\tif extra_json:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\textra.update(json.loads(extra_json))\\n\\t\\t\\texcept (JSONDecodeError, TypeError):\\n\\t\\t\\t\\tflash(\\n\\t\\t\\t\\t\\tMarkup(\\n\\t\\t\\t\\t\\t\\t\"<p>The <em>Extra</em> connection field contained an invalid value for Conn ID: \"\\n\\t\\t\\t\\t\\t\\t\"<q>{conn_id}</q>.</p>\"\\n\\t\\t\\t\\t\\t\\t\"<p>If connection parameters need to be added to <em>Extra</em>, \"\\n\\t\\t\\t\\t\\t\\t\"please make sure they are in the form of a single, valid JSON object.</p><br>\"\\n\\t\\t\\t\\t\\t\\t\"The following <em>Extra</em> parameters were <b>not</b> added to the connection:<br>\"\\n\\t\\t\\t\\t\\t\\t\"{extra_json}\"\\n\\t\\t\\t\\t\\t).format(conn_id=conn_id, extra_json=extra_json),\\n\\t\\t\\t\\t\\tcategory=\"error\",\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tdel form.extra\\n\\t\\tdel extra_json\\n\\t\\tfor key, field_name, is_sensitive in self._iter_extra_field_names_and_sensitivity():\\n\\t\\t\\tif key in form.data and key.startswith(\"extra__\"):\\n\\t\\t\\t\\tconn_type_from_extra_field = key.split(\"__\")[1]\\n\\t\\t\\t\\tif conn_type_from_extra_field == conn_type:\\n\\t\\t\\t\\t\\tvalue = form.data[key]\\n\\t\\t\\t\\t\\tif value != \"\":\\n\\t\\t\\t\\t\\t\\textra[field_name] = value\\n\\t\\tif extra.keys():\\n\\t\\t\\tsensitive_unchanged_keys = set()\\n\\t\\t\\tfor key, value in extra.items():\\n\\t\\t\\t\\tif value == SENSITIVE_FIELD_PLACEHOLDER:\\n\\t\\t\\t\\t\\tsensitive_unchanged_keys.add(key)\\n\\t\\t\\tif sensitive_unchanged_keys:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tconn = BaseHook.get_connection(conn_id)\\n\\t\\t\\t\\texcept AirflowNotFoundException:\\n\\t\\t\\t\\t\\tconn = None\\n\\t\\t\\t\\tfor key in sensitive_unchanged_keys:\\n\\t\\t\\t\\t\\tif conn and conn.extra_dejson.get(key):\\n\\t\\t\\t\\t\\t\\textra[key] = conn.extra_dejson.get(key)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tdel extra[key]\\n\\t\\t\\tform.extra.data = json.dumps(extra)"
  },
  {
    "code": "def _str_extract_noexpand(arr, pat, flags=0):\\n    from pandas import DataFrame\\n    regex = re.compile(pat, flags=flags)\\n    groups_or_na = _groups_or_na_fun(regex)\\n    if regex.groups == 1:\\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\\n        name = _get_single_group_name(regex)\\n    else:\\n        if isinstance(arr, ABCIndexClass):\\n            raise ValueError(\"only one regex group is supported with Index\")\\n        name = None\\n        names = dict(zip(regex.groupindex.values(), regex.groupindex.keys()))\\n        columns = [names.get(1 + i, i) for i in range(regex.groups)]\\n        if arr.empty:\\n            result = DataFrame(columns=columns, dtype=object)\\n        else:\\n            dtype = _result_dtype(arr)\\n            result = DataFrame(\\n                [groups_or_na(val) for val in arr],\\n                columns=columns,\\n                index=arr.index,\\n                dtype=dtype,\\n            )\\n    return result, name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _str_extract_noexpand(arr, pat, flags=0):\\n    from pandas import DataFrame\\n    regex = re.compile(pat, flags=flags)\\n    groups_or_na = _groups_or_na_fun(regex)\\n    if regex.groups == 1:\\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\\n        name = _get_single_group_name(regex)\\n    else:\\n        if isinstance(arr, ABCIndexClass):\\n            raise ValueError(\"only one regex group is supported with Index\")\\n        name = None\\n        names = dict(zip(regex.groupindex.values(), regex.groupindex.keys()))\\n        columns = [names.get(1 + i, i) for i in range(regex.groups)]\\n        if arr.empty:\\n            result = DataFrame(columns=columns, dtype=object)\\n        else:\\n            dtype = _result_dtype(arr)\\n            result = DataFrame(\\n                [groups_or_na(val) for val in arr],\\n                columns=columns,\\n                index=arr.index,\\n                dtype=dtype,\\n            )\\n    return result, name"
  },
  {
    "code": "def _execute_helper(self):\\n        self.executor.start()\\n        self.log.info(\"Resetting orphaned tasks for active dag runs\")\\n        self.reset_state_for_orphaned_tasks()\\n        self.processor_agent.start()\\n        execute_start_time = timezone.utcnow()\\n        last_self_heartbeat_time = timezone.utcnow()\\n        while True:\\n            self.log.debug(\"Starting Loop...\")\\n            loop_start_time = time.time()\\n            if self.using_sqlite:\\n                self.processor_agent.heartbeat()\\n                self.log.debug(\\n                    \"Waiting for processors to finish since we're using sqlite\")\\n                self.processor_agent.wait_until_finished()\\n            self.log.debug(\"Harvesting DAG parsing results\")\\n            simple_dags = self.processor_agent.harvest_simple_dags()\\n            self.log.debug(\"Harvested {} SimpleDAGs\".format(len(simple_dags)))\\n            simple_dag_bag = SimpleDagBag(simple_dags)\\n            if len(simple_dags) > 0:\\n                try:\\n                    simple_dag_bag = SimpleDagBag(simple_dags)\\n                    self._change_state_for_tis_without_dagrun(simple_dag_bag,\\n                                                              [State.UP_FOR_RETRY],\\n                                                              State.FAILED)\\n                    self._change_state_for_tis_without_dagrun(simple_dag_bag,\\n                                                              [State.QUEUED,\\n                                                               State.SCHEDULED,\\n                                                               State.UP_FOR_RESCHEDULE],\\n                                                              State.NONE)\\n                    self._execute_task_instances(simple_dag_bag,\\n                                                 (State.SCHEDULED,))\\n                except Exception as e:\\n                    self.log.error(\"Error queuing tasks\")\\n                    self.log.exception(e)\\n                    continue\\n            self.log.debug(\"Heartbeating the executor\")\\n            self.executor.heartbeat()\\n            self._change_state_for_tasks_failed_to_execute()\\n            self._process_executor_events(simple_dag_bag)\\n            time_since_last_heartbeat = (timezone.utcnow() -\\n                                         last_self_heartbeat_time).total_seconds()\\n            if time_since_last_heartbeat > self.heartrate:\\n                self.log.debug(\"Heartbeating the scheduler\")\\n                self.heartbeat()\\n                last_self_heartbeat_time = timezone.utcnow()\\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')\\n            loop_end_time = time.time()\\n            loop_duration = loop_end_time - loop_start_time\\n            self.log.debug(\\n                \"Ran scheduling loop in %.2f seconds\",\\n                loop_duration)\\n            if not is_unit_test:\\n                self.log.debug(\"Sleeping for %.2f seconds\", self._processor_poll_interval)\\n                time.sleep(self._processor_poll_interval)\\n            if self.processor_agent.done:\\n                self.log.info(\"Exiting scheduler loop as all files\"\\n                              \" have been processed {} times\".format(self.num_runs))\\n                break\\n            if loop_duration < 1 and not is_unit_test:\\n                sleep_length = 1 - loop_duration\\n                self.log.debug(\\n                    \"Sleeping for {0:.2f} seconds to prevent excessive logging\"\\n                    .format(sleep_length))\\n                sleep(sleep_length)\\n        self.processor_agent.terminate()\\n        if self.processor_agent.all_files_processed:\\n            self.log.info(\\n                \"Deactivating DAGs that haven't been touched since %s\",\\n                execute_start_time.isoformat()\\n            )\\n            models.DAG.deactivate_stale_dags(execute_start_time)\\n        self.executor.end()\\n        settings.Session.remove()\\n    @provide_session",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6175] Fixes bug when tasks get stuck in \"scheduled\" state (#6732)\\n\\nThere is a bug caused by scheduler_jobs refactor which leads to task failure\\nand scheduler locking.\\n\\nEssentially when a there is an overflow of tasks going into the scheduler, the\\ntasks are set back to scheduled, but are not removed from the executor's\\nqueued_tasks queue.\\n\\nThis means that the executor will attempt to run tasks that are in the scheduled\\nstate, but those tasks will fail dependency checks. Eventually the queue is\\nfilled with scheduled tasks, and the scheduler can no longer run.",
    "fixed_code": "def _execute_helper(self):\\n        self.executor.start()\\n        self.log.info(\"Resetting orphaned tasks for active dag runs\")\\n        self.reset_state_for_orphaned_tasks()\\n        self.processor_agent.start()\\n        execute_start_time = timezone.utcnow()\\n        last_self_heartbeat_time = timezone.utcnow()\\n        while True:\\n            self.log.debug(\"Starting Loop...\")\\n            loop_start_time = time.time()\\n            if self.using_sqlite:\\n                self.processor_agent.heartbeat()\\n                self.log.debug(\\n                    \"Waiting for processors to finish since we're using sqlite\")\\n                self.processor_agent.wait_until_finished()\\n            self.log.debug(\"Harvesting DAG parsing results\")\\n            simple_dags = self._get_simple_dags()\\n            self.log.debug(\"Harvested {} SimpleDAGs\".format(len(simple_dags)))\\n            simple_dag_bag = SimpleDagBag(simple_dags)\\n            if not self._validate_and_run_task_instances(simple_dag_bag=simple_dag_bag):\\n                continue\\n            time_since_last_heartbeat = (timezone.utcnow() -\\n                                         last_self_heartbeat_time).total_seconds()\\n            if time_since_last_heartbeat > self.heartrate:\\n                self.log.debug(\"Heartbeating the scheduler\")\\n                self.heartbeat()\\n                last_self_heartbeat_time = timezone.utcnow()\\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')\\n            loop_end_time = time.time()\\n            loop_duration = loop_end_time - loop_start_time\\n            self.log.debug(\\n                \"Ran scheduling loop in %.2f seconds\",\\n                loop_duration)\\n            if not is_unit_test:\\n                self.log.debug(\"Sleeping for %.2f seconds\", self._processor_poll_interval)\\n                time.sleep(self._processor_poll_interval)\\n            if self.processor_agent.done:\\n                self.log.info(\"Exiting scheduler loop as all files\"\\n                              \" have been processed {} times\".format(self.num_runs))\\n                break\\n            if loop_duration < 1 and not is_unit_test:\\n                sleep_length = 1 - loop_duration\\n                self.log.debug(\\n                    \"Sleeping for {0:.2f} seconds to prevent excessive logging\"\\n                    .format(sleep_length))\\n                sleep(sleep_length)\\n        self.processor_agent.terminate()\\n        if self.processor_agent.all_files_processed:\\n            self.log.info(\\n                \"Deactivating DAGs that haven't been touched since %s\",\\n                execute_start_time.isoformat()\\n            )\\n            models.DAG.deactivate_stale_dags(execute_start_time)\\n        self.executor.end()\\n        settings.Session.remove()"
  },
  {
    "code": "def _nan_array(index, columns, dtype=np.float64):\\n    if index is None:\\n        index = NULL_INDEX\\n    if columns is None:\\n        columns = NULL_INDEX\\n    values = np.empty((len(index), len(columns)), dtype=dtype)\\n    values.fill(nan)\\n    return values\\nimport unittest\\nclass TestBlockOperations(unittest.TestCase):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _nan_array(index, columns, dtype=np.float64):\\n    if index is None:\\n        index = NULL_INDEX\\n    if columns is None:\\n        columns = NULL_INDEX\\n    values = np.empty((len(index), len(columns)), dtype=dtype)\\n    values.fill(nan)\\n    return values\\nimport unittest\\nclass TestBlockOperations(unittest.TestCase):"
  },
  {
    "code": "def main(\\n    ctx: click.Context,\\n    code: Optional[str],\\n    line_length: int,\\n    target_version: List[TargetVersion],\\n    check: bool,\\n    diff: bool,\\n    color: bool,\\n    fast: bool,\\n    pyi: bool,\\n    ipynb: bool,\\n    python_cell_magics: Sequence[str],\\n    skip_string_normalization: bool,\\n    skip_magic_trailing_comma: bool,\\n    experimental_string_processing: bool,\\n    preview: bool,\\n    quiet: bool,\\n    verbose: bool,\\n    required_version: Optional[str],\\n    include: Pattern[str],\\n    exclude: Optional[Pattern[str]],\\n    extend_exclude: Optional[Pattern[str]],\\n    force_exclude: Optional[Pattern[str]],\\n    stdin_filename: Optional[str],\\n    workers: int,\\n    src: Tuple[str, ...],\\n    config: Optional[str],\\n) -> None:\\n    ctx.ensure_object(dict)\\n    if src and code is not None:\\n        out(\\n            main.get_usage(ctx)\\n            + \"\\n'SRC' and 'code' cannot be passed simultaneously.\"\\n        )\\n        ctx.exit(1)\\n    if not src and code is None:\\n        out(main.get_usage(ctx) + \"\\nOne of 'SRC' or 'code' is required.\")\\n        ctx.exit(1)\\n    root, method = find_project_root(src) if code is None else (None, None)\\n    ctx.obj[\"root\"] = root\\n    if verbose:\\n        if root:\\n            out(\\n                f\"Identified `{root}` as project root containing a {method}.\",\\n                fg=\"blue\",\\n            )\\n            normalized = [\\n                (normalize_path_maybe_ignore(Path(source), root), source)\\n                for source in src\\n            ]\\n            srcs_string = \", \".join(\\n                [\\n                    f'\"{_norm}\"'\\n                    if _norm\\n                    else f'\\033[31m\"{source} (skipping - invalid)\"\\033[34m'\\n                    for _norm, source in normalized\\n                ]\\n            )\\n            out(f\"Sources to be formatted: {srcs_string}\", fg=\"blue\")\\n        if config:\\n            config_source = ctx.get_parameter_source(\"config\")\\n            if config_source in (ParameterSource.DEFAULT, ParameterSource.DEFAULT_MAP):\\n                out(\"Using configuration from project root.\", fg=\"blue\")\\n            else:\\n                out(f\"Using configuration in '{config}'.\", fg=\"blue\")\\n    error_msg = \"Oh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\"\\n    if required_version and required_version != __version__:\\n        err(\\n            f\"{error_msg} The required version `{required_version}` does not match\"\\n            f\" the running version `{__version__}`!\"\\n        )\\n        ctx.exit(1)\\n    if ipynb and pyi:\\n        err(\"Cannot pass both `pyi` and `ipynb` flags!\")\\n        ctx.exit(1)\\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\\n    if target_version:\\n        versions = set(target_version)\\n    else:\\n        versions = set()\\n    mode = Mode(\\n        target_versions=versions,\\n        line_length=line_length,\\n        is_pyi=pyi,\\n        is_ipynb=ipynb,\\n        string_normalization=not skip_string_normalization,\\n        magic_trailing_comma=not skip_magic_trailing_comma,\\n        experimental_string_processing=experimental_string_processing,\\n        preview=preview,\\n        python_cell_magics=set(python_cell_magics),\\n    )\\n    if code is not None:\\n        quiet = True\\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\\n    if code is not None:\\n        reformat_code(\\n            content=code, fast=fast, write_back=write_back, mode=mode, report=report\\n        )\\n    else:\\n        try:\\n            sources = get_sources(\\n                ctx=ctx,\\n                src=src,\\n                quiet=quiet,\\n                verbose=verbose,\\n                include=include,\\n                exclude=exclude,\\n                extend_exclude=extend_exclude,\\n                force_exclude=force_exclude,\\n                report=report,\\n                stdin_filename=stdin_filename,\\n            )\\n        except GitWildMatchPatternError:\\n            ctx.exit(1)\\n        path_empty(\\n            sources,\\n            \"No Python files are present to be formatted. Nothing to do \ud83d\ude34\",\\n            quiet,\\n            verbose,\\n            ctx,\\n        )\\n        if len(sources) == 1:\\n            reformat_one(\\n                src=sources.pop(),\\n                fast=fast,\\n                write_back=write_back,\\n                mode=mode,\\n                report=report,\\n            )\\n        else:\\n            reformat_many(\\n                sources=sources,\\n                fast=fast,\\n                write_back=write_back,\\n                mode=mode,\\n                report=report,\\n                workers=workers,\\n            )\\n    if verbose or not quiet:\\n        if code is None and (verbose or report.change_count or report.failure_count):\\n            out()\\n        out(error_msg if report.return_code else \"All done! \u2728 \ud83c\udf70 \u2728\")\\n        if code is None:\\n            click.echo(str(report), err=True)\\n    ctx.exit(report.return_code)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main(\\n    ctx: click.Context,\\n    code: Optional[str],\\n    line_length: int,\\n    target_version: List[TargetVersion],\\n    check: bool,\\n    diff: bool,\\n    color: bool,\\n    fast: bool,\\n    pyi: bool,\\n    ipynb: bool,\\n    python_cell_magics: Sequence[str],\\n    skip_string_normalization: bool,\\n    skip_magic_trailing_comma: bool,\\n    experimental_string_processing: bool,\\n    preview: bool,\\n    quiet: bool,\\n    verbose: bool,\\n    required_version: Optional[str],\\n    include: Pattern[str],\\n    exclude: Optional[Pattern[str]],\\n    extend_exclude: Optional[Pattern[str]],\\n    force_exclude: Optional[Pattern[str]],\\n    stdin_filename: Optional[str],\\n    workers: int,\\n    src: Tuple[str, ...],\\n    config: Optional[str],\\n) -> None:\\n    ctx.ensure_object(dict)\\n    if src and code is not None:\\n        out(\\n            main.get_usage(ctx)\\n            + \"\\n'SRC' and 'code' cannot be passed simultaneously.\"\\n        )\\n        ctx.exit(1)\\n    if not src and code is None:\\n        out(main.get_usage(ctx) + \"\\nOne of 'SRC' or 'code' is required.\")\\n        ctx.exit(1)\\n    root, method = find_project_root(src) if code is None else (None, None)\\n    ctx.obj[\"root\"] = root\\n    if verbose:\\n        if root:\\n            out(\\n                f\"Identified `{root}` as project root containing a {method}.\",\\n                fg=\"blue\",\\n            )\\n            normalized = [\\n                (normalize_path_maybe_ignore(Path(source), root), source)\\n                for source in src\\n            ]\\n            srcs_string = \", \".join(\\n                [\\n                    f'\"{_norm}\"'\\n                    if _norm\\n                    else f'\\033[31m\"{source} (skipping - invalid)\"\\033[34m'\\n                    for _norm, source in normalized\\n                ]\\n            )\\n            out(f\"Sources to be formatted: {srcs_string}\", fg=\"blue\")\\n        if config:\\n            config_source = ctx.get_parameter_source(\"config\")\\n            if config_source in (ParameterSource.DEFAULT, ParameterSource.DEFAULT_MAP):\\n                out(\"Using configuration from project root.\", fg=\"blue\")\\n            else:\\n                out(f\"Using configuration in '{config}'.\", fg=\"blue\")\\n    error_msg = \"Oh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\"\\n    if required_version and required_version != __version__:\\n        err(\\n            f\"{error_msg} The required version `{required_version}` does not match\"\\n            f\" the running version `{__version__}`!\"\\n        )\\n        ctx.exit(1)\\n    if ipynb and pyi:\\n        err(\"Cannot pass both `pyi` and `ipynb` flags!\")\\n        ctx.exit(1)\\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\\n    if target_version:\\n        versions = set(target_version)\\n    else:\\n        versions = set()\\n    mode = Mode(\\n        target_versions=versions,\\n        line_length=line_length,\\n        is_pyi=pyi,\\n        is_ipynb=ipynb,\\n        string_normalization=not skip_string_normalization,\\n        magic_trailing_comma=not skip_magic_trailing_comma,\\n        experimental_string_processing=experimental_string_processing,\\n        preview=preview,\\n        python_cell_magics=set(python_cell_magics),\\n    )\\n    if code is not None:\\n        quiet = True\\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\\n    if code is not None:\\n        reformat_code(\\n            content=code, fast=fast, write_back=write_back, mode=mode, report=report\\n        )\\n    else:\\n        try:\\n            sources = get_sources(\\n                ctx=ctx,\\n                src=src,\\n                quiet=quiet,\\n                verbose=verbose,\\n                include=include,\\n                exclude=exclude,\\n                extend_exclude=extend_exclude,\\n                force_exclude=force_exclude,\\n                report=report,\\n                stdin_filename=stdin_filename,\\n            )\\n        except GitWildMatchPatternError:\\n            ctx.exit(1)\\n        path_empty(\\n            sources,\\n            \"No Python files are present to be formatted. Nothing to do \ud83d\ude34\",\\n            quiet,\\n            verbose,\\n            ctx,\\n        )\\n        if len(sources) == 1:\\n            reformat_one(\\n                src=sources.pop(),\\n                fast=fast,\\n                write_back=write_back,\\n                mode=mode,\\n                report=report,\\n            )\\n        else:\\n            reformat_many(\\n                sources=sources,\\n                fast=fast,\\n                write_back=write_back,\\n                mode=mode,\\n                report=report,\\n                workers=workers,\\n            )\\n    if verbose or not quiet:\\n        if code is None and (verbose or report.change_count or report.failure_count):\\n            out()\\n        out(error_msg if report.return_code else \"All done! \u2728 \ud83c\udf70 \u2728\")\\n        if code is None:\\n            click.echo(str(report), err=True)\\n    ctx.exit(report.return_code)"
  },
  {
    "code": "def verify(mhash, key, S):\\n\\tmodBits = Crypto.Util.number.size(key.n)\\n\\tk = ceil_div(modBits,8) \\n\\tif len(S) != k:\\n\\t\\treturn 0\\n\\tm = key.encrypt(S, 0)[0]\\n\\tem1 = '\\x00'*(k-len(m)) + m\\n\\ttry:\\n\\t\\tem2 = EMSA_PKCS1_V1_5_ENCODE(mhash, k)\\n\\texcept ValueError:\\n\\t\\treturn 0\\n\\treturn em1==em2",
    "label": 1,
    "bug_type": "security",
    "bug_description": "To simplify, no RNG needs to be provided with PKCS1 encryption: the one belonging to each RSA key is reused. Error detection is internally implemented in a simpler (and safer) way for PKCS1 OAEP decryption. General fixes to documentation for PKCS1.",
    "fixed_code": "def verify(mhash, key, S):\\n\\tmodBits = Crypto.Util.number.size(key.n)\\n\\tk = ceil_div(modBits,8) \\n\\tif len(S) != k:\\n\\t\\treturn 0\\n\\tm = key.encrypt(S, 0)[0]\\n\\tem1 = '\\x00'*(k-len(m)) + m\\n\\ttry:\\n\\t\\tem2 = EMSA_PKCS1_V1_5_ENCODE(mhash, k)\\n\\texcept ValueError:\\n\\t\\treturn 0\\n\\treturn em1==em2"
  },
  {
    "code": "def push(self, type: int, newdfa: DFAS, newstate: int, context: Context) -> None:\\n        if self.is_backtracking:\\n            dfa, state, _ = self.stack[-1]\\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\\n            self.stack.append((newdfa, 0, DUMMY_NODE))\\n        else:\\n            dfa, state, node = self.stack[-1]\\n            newnode: RawNode = (type, None, context, [])\\n            self.stack[-1] = (dfa, newstate, node)\\n            self.stack.append((newdfa, 0, newnode))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def push(self, type: int, newdfa: DFAS, newstate: int, context: Context) -> None:\\n        if self.is_backtracking:\\n            dfa, state, _ = self.stack[-1]\\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\\n            self.stack.append((newdfa, 0, DUMMY_NODE))\\n        else:\\n            dfa, state, node = self.stack[-1]\\n            newnode: RawNode = (type, None, context, [])\\n            self.stack[-1] = (dfa, newstate, node)\\n            self.stack.append((newdfa, 0, newnode))"
  },
  {
    "code": "def stream_exists_backend(request, user_profile, stream_id, autosubscribe):\\n\\ttry:\\n\\t\\tstream = get_and_validate_stream_by_id(stream_id, user_profile.realm)\\n\\texcept JsonableError:\\n\\t\\tstream = None\\n\\tresult = {\"exists\": bool(stream)}\\n\\tif stream is not None:\\n\\t\\trecipient = get_recipient(Recipient.STREAM, stream.id)\\n\\t\\tif not stream.invite_only and autosubscribe:\\n\\t\\t\\tbulk_add_subscriptions([stream], [user_profile])\\n\\t\\tresult[\"subscribed\"] = is_active_subscriber(\\n\\t\\t\\tuser_profile=user_profile,\\n\\t\\t\\trecipient=recipient)\\n\\t\\treturn json_success(result) \\n\\treturn json_response(data=result, status=404)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def stream_exists_backend(request, user_profile, stream_id, autosubscribe):\\n\\ttry:\\n\\t\\tstream = get_and_validate_stream_by_id(stream_id, user_profile.realm)\\n\\texcept JsonableError:\\n\\t\\tstream = None\\n\\tresult = {\"exists\": bool(stream)}\\n\\tif stream is not None:\\n\\t\\trecipient = get_recipient(Recipient.STREAM, stream.id)\\n\\t\\tif not stream.invite_only and autosubscribe:\\n\\t\\t\\tbulk_add_subscriptions([stream], [user_profile])\\n\\t\\tresult[\"subscribed\"] = is_active_subscriber(\\n\\t\\t\\tuser_profile=user_profile,\\n\\t\\t\\trecipient=recipient)\\n\\t\\treturn json_success(result) \\n\\treturn json_response(data=result, status=404)"
  },
  {
    "code": "def __contains__(self, key) -> bool:\\n\\t\\tif is_scalar(key) and isna(key):\\n\\t\\t\\treturn self.isna().any()\\n\\t\\treturn contains(self, key, container=self._codes)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: CategoricalIndex.__contains__ incorrect NaTs (#33947)",
    "fixed_code": "def __contains__(self, key) -> bool:\\n\\t\\tif is_valid_nat_for_dtype(key, self.categories.dtype):\\n\\t\\t\\treturn self.isna().any()\\n\\t\\treturn contains(self, key, container=self._codes)"
  },
  {
    "code": "def icol(self, i):\\n        label = self.columns[i]\\n        if isinstance(i, slice):\\n            lab_slice = slice(label[0], label[-1])\\n            return self.ix[:, lab_slice]\\n        else:\\n            return self[label]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def icol(self, i):\\n        label = self.columns[i]\\n        if isinstance(i, slice):\\n            lab_slice = slice(label[0], label[-1])\\n            return self.ix[:, lab_slice]\\n        else:\\n            return self[label]"
  },
  {
    "code": "def fmgr_get_task_status(fmgr, paramgram):\\n    if paramgram[\"task_id\"] is not None:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task/{task_id}'.format(task_id=paramgram[\"task_id\"])\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    else:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task'\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    return response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fmgr_get_task_status(fmgr, paramgram):\\n    if paramgram[\"task_id\"] is not None:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task/{task_id}'.format(task_id=paramgram[\"task_id\"])\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    else:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task'\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    return response"
  },
  {
    "code": "def literal_eval(node_or_string):\\n    if isinstance(node_or_string, str):\\n        node_or_string = parse(node_or_string, mode='eval')\\n    if isinstance(node_or_string, Expression):\\n        node_or_string = node_or_string.body",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-41887: omit leading spaces/tabs on ast.literal_eval (#22469)\\n\\nAlso document that eval() does this (the same way).",
    "fixed_code": "def literal_eval(node_or_string):\\n    if isinstance(node_or_string, str):\\n        node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\\n    if isinstance(node_or_string, Expression):\\n        node_or_string = node_or_string.body"
  },
  {
    "code": "def make_bytes(self, value):\\n        if self.has_header('Content-Encoding'):\\n            if isinstance(value, int):\\n                value = six.text_type(value)\\n            if isinstance(value, six.text_type):\\n                value = value.encode('ascii')\\n            return bytes(value)\\n        else:\\n            return force_bytes(value, self._charset)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def make_bytes(self, value):\\n        if self.has_header('Content-Encoding'):\\n            if isinstance(value, int):\\n                value = six.text_type(value)\\n            if isinstance(value, six.text_type):\\n                value = value.encode('ascii')\\n            return bytes(value)\\n        else:\\n            return force_bytes(value, self._charset)"
  },
  {
    "code": "def get_groups(fmg, paramgram):\\n    datagram = {\\n        \"method\": \"get\"\\n    }\\n    url = '/dvmdb/adom/{adom}/group'.format(adom=paramgram[\"adom\"])\\n    response = fmg.get(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Auto Commit for: fmgr_device_group (#52784)",
    "fixed_code": "def get_groups(fmgr, paramgram):\\n    datagram = {\\n        \"method\": \"get\"\\n    }\\n    url = '/dvmdb/adom/{adom}/group'.format(adom=paramgram[\"adom\"])\\n    response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    return response"
  },
  {
    "code": "def _fetch_lfw_pairs(index_file_path, data_folder_path, slice_=None,\\n                    color=False, resize=None):\\n    splitted_lines = [l.strip().split('\\t')\\n                      for l in open(index_file_path, 'rb').readlines()]\\n    pair_specs = [sl for sl in splitted_lines if len(sl) > 2]\\n    n_pairs = len(pair_specs)\\n    target = np.zeros(n_pairs, dtype=np.int)\\n    file_paths = list()\\n    for i, components in enumerate(pair_specs):\\n        if len(components) == 3:\\n            target[i] = 1\\n            pair = (\\n                (components[0], int(components[1]) - 1),\\n                (components[0], int(components[2]) - 1),\\n            )\\n        elif len(components) == 4:\\n            target[i] = 0\\n            pair = (\\n                (components[0], int(components[1]) - 1),\\n                (components[2], int(components[3]) - 1),\\n            )\\n        else:\\n            raise ValueError(\"invalid line %d: %r\" % (i + 1, components))\\n        for j, (name, idx) in enumerate(pair):\\n            person_folder = join(data_folder_path, name)\\n            filenames = list(sorted(listdir(person_folder)))\\n            file_path = join(person_folder, filenames[idx])\\n            file_paths.append(file_path)\\n    pairs = _load_imgs(file_paths, slice_, color, resize)\\n    shape = list(pairs.shape)\\n    n_faces = shape.pop(0)\\n    shape.insert(0, 2)\\n    shape.insert(0, n_faces // 2)\\n    pairs.shape = shape\\n    return pairs, target, np.array(['Different persons', 'Same person'])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "COSMIT use urlretrieve and \"with\" syntax in LFW module\\n\\nReverts c953f1c99b3df293d8810eaaacc272e04773ea40.",
    "fixed_code": "def _fetch_lfw_pairs(index_file_path, data_folder_path, slice_=None,\\n                    color=False, resize=None):\\n    with open(index_file_path, 'rb') as index_file:\\n        split_lines = [ln.strip().split('\\t') for ln in index_file]\\n    pair_specs = [sl for sl in split_lines if len(sl) > 2]\\n    n_pairs = len(pair_specs)\\n    target = np.zeros(n_pairs, dtype=np.int)\\n    file_paths = list()\\n    for i, components in enumerate(pair_specs):\\n        if len(components) == 3:\\n            target[i] = 1\\n            pair = (\\n                (components[0], int(components[1]) - 1),\\n                (components[0], int(components[2]) - 1),\\n            )\\n        elif len(components) == 4:\\n            target[i] = 0\\n            pair = (\\n                (components[0], int(components[1]) - 1),\\n                (components[2], int(components[3]) - 1),\\n            )\\n        else:\\n            raise ValueError(\"invalid line %d: %r\" % (i + 1, components))\\n        for j, (name, idx) in enumerate(pair):\\n            person_folder = join(data_folder_path, name)\\n            filenames = list(sorted(listdir(person_folder)))\\n            file_path = join(person_folder, filenames[idx])\\n            file_paths.append(file_path)\\n    pairs = _load_imgs(file_paths, slice_, color, resize)\\n    shape = list(pairs.shape)\\n    n_faces = shape.pop(0)\\n    shape.insert(0, 2)\\n    shape.insert(0, n_faces // 2)\\n    pairs.shape = shape\\n    return pairs, target, np.array(['Different persons', 'Same person'])"
  },
  {
    "code": "def _get_hist_google(sym, start, end, retry_count, pause):\\n    start, end = _sanitize_dates(start, end)\\n    url = \"%s%s\" % (_HISTORICAL_GOOGLE_URL,\\n                    urlencode({\"q\": sym,\\n                               \"startdate\": start.strftime('%b %d, ' '%Y'),\\n                               \"enddate\": end.strftime('%b %d, %Y'),\\n                               \"output\": \"csv\"}))\\n    return _retry_read_url(url, retry_count, pause, 'Google')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add interval kwarg to get_data_yahoo",
    "fixed_code": "def _get_hist_google(sym, start, end, interval, retry_count, pause):\\n    start, end = _sanitize_dates(start, end)\\n    url = \"%s%s\" % (_HISTORICAL_GOOGLE_URL,\\n                    urlencode({\"q\": sym,\\n                               \"startdate\": start.strftime('%b %d, ' '%Y'),\\n                               \"enddate\": end.strftime('%b %d, %Y'),\\n                               \"output\": \"csv\"}))\\n    return _retry_read_url(url, retry_count, pause, 'Google')"
  },
  {
    "code": "def __repr__(self):\\n        return \"<%s: %s>\" % (self.__class__.__name__, self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25875 -- Prevented UnicodeDecodeError for Q object repr\\n\\nThanks Ben Kraft for the report, and Simon Charette for the review.",
    "fixed_code": "def __repr__(self):\\n        return str(\"<%s: %s>\") % (self.__class__.__name__, self)"
  },
  {
    "code": "def call(self, inputs, states, constants=None, training=None, **kwargs):\\n\\tstate_size = (self.state_size[::-1]\\n\\t\\t\\t\\t  if self.reverse_state_order else self.state_size)\\n\\tnested_states = nest.pack_sequence_as(state_size, nest.flatten(states))\\n\\tnew_nested_states = []\\n\\tfor cell, states in zip(self.cells, nested_states):\\n\\t  states = states if nest.is_sequence(states) else [states]\\n\\t  is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None\\n\\t  states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\\n\\t  if generic_utils.has_arg(cell.call, 'training'):\\n\\t\\tkwargs['training'] = training\\n\\t  else:\\n\\t\\tkwargs.pop('training', None)\\n\\t  if generic_utils.has_arg(cell.call, 'constants'):\\n\\t\\tinputs, states = cell.call(inputs, states, constants=constants,\\n\\t\\t\\t\\t\\t\\t\\t\\t   **kwargs)\\n\\t  else:\\n\\t\\tinputs, states = cell.call(inputs, states, **kwargs)\\n\\t  new_nested_states.append(states)\\n\\treturn inputs, nest.pack_sequence_as(state_size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t nest.flatten(new_nested_states))\\n  @tf_utils.shape_type_conversion",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def call(self, inputs, states, constants=None, training=None, **kwargs):\\n\\tstate_size = (self.state_size[::-1]\\n\\t\\t\\t\\t  if self.reverse_state_order else self.state_size)\\n\\tnested_states = nest.pack_sequence_as(state_size, nest.flatten(states))\\n\\tnew_nested_states = []\\n\\tfor cell, states in zip(self.cells, nested_states):\\n\\t  states = states if nest.is_sequence(states) else [states]\\n\\t  is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None\\n\\t  states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\\n\\t  if generic_utils.has_arg(cell.call, 'training'):\\n\\t\\tkwargs['training'] = training\\n\\t  else:\\n\\t\\tkwargs.pop('training', None)\\n\\t  if generic_utils.has_arg(cell.call, 'constants'):\\n\\t\\tinputs, states = cell.call(inputs, states, constants=constants,\\n\\t\\t\\t\\t\\t\\t\\t\\t   **kwargs)\\n\\t  else:\\n\\t\\tinputs, states = cell.call(inputs, states, **kwargs)\\n\\t  new_nested_states.append(states)\\n\\treturn inputs, nest.pack_sequence_as(state_size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t nest.flatten(new_nested_states))\\n  @tf_utils.shape_type_conversion"
  },
  {
    "code": "def from_crawler(cls, crawler):\\n        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\\n            raise NotConfigured\\n        return cls(crawler.stats)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def from_crawler(cls, crawler):\\n        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\\n            raise NotConfigured\\n        return cls(crawler.stats)"
  },
  {
    "code": "def _shallow_copy(self):\\n        nc = Context(self.prec, self.rounding, self.traps, self.flags,\\n                         self._rounding_decision, self.Emin, self.Emax,\\n                         self.capitals, self._clamp, self._ignored_flags)\\n        return nc",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _shallow_copy(self):\\n        nc = Context(self.prec, self.rounding, self.traps,\\n                     self.flags, self.Emin, self.Emax,\\n                     self.capitals, self._clamp, self._ignored_flags)\\n        return nc"
  },
  {
    "code": "def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True,\\n             **eval_kwargs):\\n    _bool_arith_check(op_str, a, b)\\n    if use_numexpr:\\n        return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\\n                         **eval_kwargs)\\n    return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: let +, * and - pass thru but eval via Python\\n\\nBut give a warning suggesting a better way to do it",
    "fixed_code": "def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True,\\n             **eval_kwargs):\\n    use_numexpr = use_numexpr and _bool_arith_check(op_str, a, b)\\n    if use_numexpr:\\n        return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\\n                         **eval_kwargs)\\n    return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)"
  },
  {
    "code": "def __new__(cls, data, dtype=None, copy=False, name=None, **kwargs):\\n        from pandas.tseries.period import PeriodIndex\\n        if isinstance(data, np.ndarray):\\n            if issubclass(data.dtype.type, np.datetime64):\\n                from pandas.tseries.index import DatetimeIndex\\n                result = DatetimeIndex(data, copy=copy, name=name, **kwargs)\\n                if dtype is not None and _o_dtype == dtype:\\n                    return Index(result.to_pydatetime(), dtype=_o_dtype)\\n                else:\\n                    return result\\n            elif issubclass(data.dtype.type, np.timedelta64):\\n                return Int64Index(data, copy=copy, name=name)\\n            if dtype is not None:\\n                try:\\n                    data = np.array(data, dtype=dtype, copy=copy)\\n                except TypeError:\\n                    pass\\n            elif isinstance(data, PeriodIndex):\\n                return PeriodIndex(data, copy=copy, name=name, **kwargs)\\n            if issubclass(data.dtype.type, np.integer):\\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\\n            subarr = com._asarray_tuplesafe(data, dtype=object)\\n        elif np.isscalar(data):\\n            raise TypeError('Index(...) must be called with a collection '\\n                             'of some kind, %s was passed' % repr(data))\\n        else:\\n            subarr = com._asarray_tuplesafe(data, dtype=object)\\n        if dtype is None:\\n            inferred = lib.infer_dtype(subarr)\\n            if inferred == 'integer':\\n                return Int64Index(subarr.astype('i8'), name=name)\\n            elif inferred != 'string':\\n                if (inferred.startswith('datetime') or\\n                        tslib.is_timestamp_array(subarr)):\\n                    from pandas.tseries.index import DatetimeIndex\\n                    return DatetimeIndex(subarr, copy=copy, name=name, **kwargs)\\n                elif inferred == 'period':\\n                    return PeriodIndex(subarr, name=name, **kwargs)\\n        subarr = subarr.view(cls)\\n        subarr._set_names([name])\\n        return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Additional keyword arguments for Index.copy()\\n\\n  `dtype` on copy. MultiIndex can set `levels`, `labels`, and `names`.\\n  copies axes as well. Defaults to False.\\n  copy its index.",
    "fixed_code": "def __new__(cls, data, dtype=None, copy=False, name=None, **kwargs):\\n        from pandas.tseries.period import PeriodIndex\\n        if isinstance(data, np.ndarray):\\n            if issubclass(data.dtype.type, np.datetime64):\\n                from pandas.tseries.index import DatetimeIndex\\n                result = DatetimeIndex(data, copy=copy, name=name, **kwargs)\\n                if dtype is not None and _o_dtype == dtype:\\n                    return Index(result.to_pydatetime(), dtype=_o_dtype)\\n                else:\\n                    return result\\n            elif issubclass(data.dtype.type, np.timedelta64):\\n                return Int64Index(data, copy=copy, name=name)\\n            if dtype is not None:\\n                try:\\n                    data = np.array(data, dtype=dtype, copy=copy)\\n                except TypeError:\\n                    pass\\n            elif isinstance(data, PeriodIndex):\\n                return PeriodIndex(data, copy=copy, name=name, **kwargs)\\n            if issubclass(data.dtype.type, np.integer):\\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\\n            subarr = com._asarray_tuplesafe(data, dtype=object)\\n            if copy:\\n                subarr = subarr.copy()\\n        elif np.isscalar(data):\\n            raise TypeError('Index(...) must be called with a collection '\\n                             'of some kind, %s was passed' % repr(data))\\n        else:\\n            subarr = com._asarray_tuplesafe(data, dtype=object)\\n        if dtype is None:\\n            inferred = lib.infer_dtype(subarr)\\n            if inferred == 'integer':\\n                return Int64Index(subarr.astype('i8'), copy=copy, name=name)\\n            elif inferred != 'string':\\n                if (inferred.startswith('datetime') or\\n                        tslib.is_timestamp_array(subarr)):\\n                    from pandas.tseries.index import DatetimeIndex\\n                    return DatetimeIndex(subarr, copy=copy, name=name, **kwargs)\\n                elif inferred == 'period':\\n                    return PeriodIndex(subarr, name=name, **kwargs)\\n        subarr = subarr.view(cls)\\n        subarr._set_names([name])\\n        return subarr"
  },
  {
    "code": "def max(self):\\n        return self.values.max()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Index.min and max doesnt handle nan and NaT properly",
    "fixed_code": "def max(self):\\n        import pandas.core.nanops\\n        return pandas.core.nanops.nanmax(self.values)"
  },
  {
    "code": "def __and__(self, otherseq, *rest):\\n' % (self, (otherseq,) + rest))\\n        if type(otherseq) != type(self):\\n            otherseq = apply(bitvec, (otherseq, ) + rest)\\n        return BitVec(self._data & otherseq._data, \\\\n                  min(self._len, otherseq._len))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove apply()",
    "fixed_code": "def __and__(self, otherseq, *rest):\\n' % (self, (otherseq,) + rest))\\n        if type(otherseq) != type(self):\\n            otherseq = bitvec(otherseq, *rest)\\n        return BitVec(self._data & otherseq._data, \\\\n                  min(self._len, otherseq._len))"
  },
  {
    "code": "def mgr_to_mgr(mgr, typ: str):\\n    new_mgr: Manager\\n    if typ == \"block\":\\n        if isinstance(mgr, BlockManager):\\n            new_mgr = mgr\\n        else:\\n            if mgr.ndim == 2:\\n                new_mgr = arrays_to_mgr(\\n                    mgr.arrays, mgr.axes[0], mgr.axes[1], mgr.axes[0], typ=\"block\"\\n                )\\n            else:\\n                new_mgr = SingleBlockManager.from_array(mgr.arrays[0], mgr.index)\\n    elif typ == \"array\":\\n        if isinstance(mgr, ArrayManager):\\n            new_mgr = mgr\\n        else:\\n            if mgr.ndim == 2:\\n                arrays = [mgr.iget_values(i).copy() for i in range(len(mgr.axes[0]))]\\n                new_mgr = ArrayManager(arrays, [mgr.axes[1], mgr.axes[0]])\\n            else:\\n                new_mgr = SingleArrayManager([mgr.internal_values()], [mgr.index])\\n    else:\\n        raise ValueError(f\"'typ' needs to be one of {{'block', 'array'}}, got '{typ}'\")\\n    return new_mgr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrayManager] Enable read_parquet to not create 2D blocks when using ArrayManager (#40303)",
    "fixed_code": "def mgr_to_mgr(mgr, typ: str, copy: bool = True):\\n    new_mgr: Manager\\n    if typ == \"block\":\\n        if isinstance(mgr, BlockManager):\\n            new_mgr = mgr\\n        else:\\n            if mgr.ndim == 2:\\n                new_mgr = arrays_to_mgr(\\n                    mgr.arrays, mgr.axes[0], mgr.axes[1], mgr.axes[0], typ=\"block\"\\n                )\\n            else:\\n                new_mgr = SingleBlockManager.from_array(mgr.arrays[0], mgr.index)\\n    elif typ == \"array\":\\n        if isinstance(mgr, ArrayManager):\\n            new_mgr = mgr\\n        else:\\n            if mgr.ndim == 2:\\n                arrays = [mgr.iget_values(i) for i in range(len(mgr.axes[0]))]\\n                if copy:\\n                    arrays = [arr.copy() for arr in arrays]\\n                new_mgr = ArrayManager(arrays, [mgr.axes[1], mgr.axes[0]])\\n            else:\\n                array = mgr.internal_values()\\n                if copy:\\n                    array = array.copy()\\n                new_mgr = SingleArrayManager([array], [mgr.index])\\n    else:\\n        raise ValueError(f\"'typ' needs to be one of {{'block', 'array'}}, got '{typ}'\")\\n    return new_mgr"
  },
  {
    "code": "def group(self, name):\\n        resp = self.shortcmd('GROUP ' + name)\\n        if not resp.startswith(b'211'):\\n            raise NNTPReplyError(resp)\\n        words = resp.split()\\n        count = first = last = 0\\n        n = len(words)\\n        if n > 1:\\n            count = words[1]\\n            if n > 2:\\n                first = words[2]\\n                if n > 3:\\n                    last = words[3]\\n                    if n > 4:\\n                        name = words[4].lower()\\n        return resp, count, first, last, name",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def group(self, name):\\n        resp = self._shortcmd('GROUP ' + name)\\n        if not resp.startswith('211'):\\n            raise NNTPReplyError(resp)\\n        words = resp.split()\\n        count = first = last = 0\\n        n = len(words)\\n        if n > 1:\\n            count = words[1]\\n            if n > 2:\\n                first = words[2]\\n                if n > 3:\\n                    last = words[3]\\n                    if n > 4:\\n                        name = words[4].lower()\\n        return resp, int(count), int(first), int(last), name"
  },
  {
    "code": "def read_hdf(path_or_buf, key, **kwargs):\\n    f = lambda store: store.select(key, **kwargs)\\n    if isinstance(path_or_buf, basestring):\\n        with get_store(path_or_buf) as store:\\n            return f(store)\\n    f(path_or_buf)\\nclass HDFStore(object):\\n    _quiet = False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_hdf(path_or_buf, key, **kwargs):\\n    f = lambda store: store.select(key, **kwargs)\\n    if isinstance(path_or_buf, basestring):\\n        with get_store(path_or_buf) as store:\\n            return f(store)\\n    f(path_or_buf)\\nclass HDFStore(object):\\n    _quiet = False"
  },
  {
    "code": "def set_atom(self, block, existing_col, min_itemsize, nan_rep, info, encoding=None, **kwargs):\\n        self.values = list(block.items)\\n        dtype = block.dtype.name\\n        rvalues = block.values.ravel()\\n        inferred_type = lib.infer_dtype(rvalues)\\n        if inferred_type == 'datetime64':\\n            self.set_atom_datetime64(block)\\n        elif inferred_type == 'date':\\n            raise TypeError(\\n                \"[date] is not implemented as a table column\")\\n        elif inferred_type == 'datetime':\\n            if getattr(rvalues[0],'tzinfo',None) is not None:\\n                if len(set([r.tzinfo for r in rvalues])) != 1:\\n                    raise TypeError(\\n                        \"too many timezones in this block, create separate data columns\")\\n                index = DatetimeIndex(rvalues)\\n                tz = getattr(index,'tz',None)\\n                if tz is None:\\n                    raise TypeError(\\n                        \"invalid timezone specification\")\\n                values = index.tz_convert('UTC').values.view('i8')\\n                self.tz = tz\\n                self.update_info(info)\\n                self.set_atom_datetime64(block, values.reshape(block.values.shape))\\n            else:\\n                raise TypeError(\\n                    \"[datetime] is not implemented as a table column\")\\n        elif inferred_type == 'unicode':\\n            raise TypeError(\\n                \"[unicode] is not implemented as a table column\")\\n        elif inferred_type == 'string' or dtype == 'object':\\n            self.set_atom_string(block, existing_col, min_itemsize, nan_rep, encoding)\\n        else:\\n            self.set_atom_data(block)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_atom(self, block, existing_col, min_itemsize, nan_rep, info, encoding=None, **kwargs):\\n        self.values = list(block.items)\\n        dtype = block.dtype.name\\n        rvalues = block.values.ravel()\\n        inferred_type = lib.infer_dtype(rvalues)\\n        if inferred_type == 'datetime64':\\n            self.set_atom_datetime64(block)\\n        elif inferred_type == 'date':\\n            raise TypeError(\\n                \"[date] is not implemented as a table column\")\\n        elif inferred_type == 'datetime':\\n            if getattr(rvalues[0],'tzinfo',None) is not None:\\n                if len(set([r.tzinfo for r in rvalues])) != 1:\\n                    raise TypeError(\\n                        \"too many timezones in this block, create separate data columns\")\\n                index = DatetimeIndex(rvalues)\\n                tz = getattr(index,'tz',None)\\n                if tz is None:\\n                    raise TypeError(\\n                        \"invalid timezone specification\")\\n                values = index.tz_convert('UTC').values.view('i8')\\n                self.tz = tz\\n                self.update_info(info)\\n                self.set_atom_datetime64(block, values.reshape(block.values.shape))\\n            else:\\n                raise TypeError(\\n                    \"[datetime] is not implemented as a table column\")\\n        elif inferred_type == 'unicode':\\n            raise TypeError(\\n                \"[unicode] is not implemented as a table column\")\\n        elif inferred_type == 'string' or dtype == 'object':\\n            self.set_atom_string(block, existing_col, min_itemsize, nan_rep, encoding)\\n        else:\\n            self.set_atom_data(block)\\n        return self"
  },
  {
    "code": "def show_versions(as_json=False):\\n    import imp\\n    sys_info = get_sys_info()\\n    deps = [\\n        (\"pandas\", lambda mod: mod.__version__),\\n        (\"nose\", lambda mod: mod.__version__),\\n        (\"Cython\", lambda mod: mod.__version__),\\n        (\"numpy\", lambda mod: mod.version.version),\\n        (\"scipy\", lambda mod: mod.version.version),\\n        (\"statsmodels\", lambda mod: mod.__version__),\\n        (\"IPython\", lambda mod: mod.__version__),\\n        (\"sphinx\", lambda mod: mod.__version__),\\n        (\"patsy\", lambda mod: mod.__version__),\\n        (\"dateutil\", lambda mod: mod.__version__),\\n        (\"pytz\", lambda mod: mod.VERSION),\\n        (\"bottleneck\", lambda mod: mod.__version__),\\n        (\"tables\", lambda mod: mod.__version__),\\n        (\"numexpr\", lambda mod: mod.__version__),\\n        (\"matplotlib\", lambda mod: mod.__version__),\\n        (\"openpyxl\", lambda mod: mod.__version__),\\n        (\"xlrd\", lambda mod: mod.__VERSION__),\\n        (\"xlwt\", lambda mod: mod.__VERSION__),\\n        (\"xlsxwriter\", lambda mod: mod.__version__),\\n        (\"lxml\", lambda mod: mod.etree.__version__),\\n        (\"bs4\", lambda mod: mod.__version__),\\n        (\"html5lib\", lambda mod: mod.__version__),\\n        (\"httplib2\", lambda mod: mod.__version__),\\n        (\"apiclient\", lambda mod: mod.__version__),\\n        (\"sqlalchemy\", lambda mod: mod.__version__),\\n        (\"pymysql\", lambda mod: mod.__version__),\\n        (\"psycopg2\", lambda mod: mod.__version__),\\n    ]\\n    deps_blob = list()\\n    for (modname, ver_f) in deps:\\n        try:\\n            try:\\n                mod = imp.load_module(modname, *imp.find_module(modname))\\n            except (ImportError):\\n                import importlib\\n                mod = importlib.import_module(modname)\\n            ver = ver_f(mod)\\n            deps_blob.append((modname, ver))\\n        except:\\n            deps_blob.append((modname, None))\\n    if (as_json):\\n        try:\\n            import json\\n        except:\\n            import simplejson as json\\n        j = dict(system=dict(sys_info), dependencies=dict(deps_blob))\\n        if as_json == True:\\n            print(j)\\n        else:\\n            with codecs.open(as_json, \"wb\", encoding='utf8') as f:\\n                json.dump(j, f, indent=2)\\n    else:\\n        print(\"\\nINSTALLED VERSIONS\")\\n        print(\"------------------\")\\n        for k, stat in sys_info:\\n            print(\"%s: %s\" % (k, stat))\\n        print(\"\")\\n        for k, stat in deps_blob:\\n            print(\"%s: %s\" % (k, stat))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def show_versions(as_json=False):\\n    import imp\\n    sys_info = get_sys_info()\\n    deps = [\\n        (\"pandas\", lambda mod: mod.__version__),\\n        (\"nose\", lambda mod: mod.__version__),\\n        (\"Cython\", lambda mod: mod.__version__),\\n        (\"numpy\", lambda mod: mod.version.version),\\n        (\"scipy\", lambda mod: mod.version.version),\\n        (\"statsmodels\", lambda mod: mod.__version__),\\n        (\"IPython\", lambda mod: mod.__version__),\\n        (\"sphinx\", lambda mod: mod.__version__),\\n        (\"patsy\", lambda mod: mod.__version__),\\n        (\"dateutil\", lambda mod: mod.__version__),\\n        (\"pytz\", lambda mod: mod.VERSION),\\n        (\"bottleneck\", lambda mod: mod.__version__),\\n        (\"tables\", lambda mod: mod.__version__),\\n        (\"numexpr\", lambda mod: mod.__version__),\\n        (\"matplotlib\", lambda mod: mod.__version__),\\n        (\"openpyxl\", lambda mod: mod.__version__),\\n        (\"xlrd\", lambda mod: mod.__VERSION__),\\n        (\"xlwt\", lambda mod: mod.__VERSION__),\\n        (\"xlsxwriter\", lambda mod: mod.__version__),\\n        (\"lxml\", lambda mod: mod.etree.__version__),\\n        (\"bs4\", lambda mod: mod.__version__),\\n        (\"html5lib\", lambda mod: mod.__version__),\\n        (\"httplib2\", lambda mod: mod.__version__),\\n        (\"apiclient\", lambda mod: mod.__version__),\\n        (\"sqlalchemy\", lambda mod: mod.__version__),\\n        (\"pymysql\", lambda mod: mod.__version__),\\n        (\"psycopg2\", lambda mod: mod.__version__),\\n    ]\\n    deps_blob = list()\\n    for (modname, ver_f) in deps:\\n        try:\\n            try:\\n                mod = imp.load_module(modname, *imp.find_module(modname))\\n            except (ImportError):\\n                import importlib\\n                mod = importlib.import_module(modname)\\n            ver = ver_f(mod)\\n            deps_blob.append((modname, ver))\\n        except:\\n            deps_blob.append((modname, None))\\n    if (as_json):\\n        try:\\n            import json\\n        except:\\n            import simplejson as json\\n        j = dict(system=dict(sys_info), dependencies=dict(deps_blob))\\n        if as_json == True:\\n            print(j)\\n        else:\\n            with codecs.open(as_json, \"wb\", encoding='utf8') as f:\\n                json.dump(j, f, indent=2)\\n    else:\\n        print(\"\\nINSTALLED VERSIONS\")\\n        print(\"------------------\")\\n        for k, stat in sys_info:\\n            print(\"%s: %s\" % (k, stat))\\n        print(\"\")\\n        for k, stat in deps_blob:\\n            print(\"%s: %s\" % (k, stat))"
  },
  {
    "code": "def sparse_categorical_accuracy(y_true, y_pred):\\n\\treturn K.cast(K.equal(K.max(y_true, axis=-1),\\n\\t\\t\\t\\t\\t\\t  K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\\n\\t\\t\\t\\t  K.floatx())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix sparse categorical acc (#11100)",
    "fixed_code": "def sparse_categorical_accuracy(y_true, y_pred):\\n\\treturn K.cast(K.equal(K.flatten(y_true),\\n\\t\\t\\t\\t\\t\\t  K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\\n\\t\\t\\t\\t  K.floatx())"
  },
  {
    "code": "def filter_system_global_data(json):\\n    option_list = ['admin_concurrent', 'admin_console_timeout', 'admin_hsts_max_age',\\n                   'admin_https_pki_required', 'admin_https_redirect', 'admin_https_ssl_versions',\\n                   'admin_lockout_duration', 'admin_lockout_threshold', 'admin_login_max',\\n                   'admin_maintainer', 'admin_port', 'admin_restrict_local',\\n                   'admin_scp', 'admin_server_cert', 'admin_sport',\\n                   'admin_ssh_grace_time', 'admin_ssh_password', 'admin_ssh_port',\\n                   'admin_ssh_v1', 'admin_telnet_port', 'admintimeout',\\n                   'alias', 'allow_traffic_redirect', 'anti_replay',\\n                   'arp_max_entry', 'asymroute', 'auth_cert',\\n                   'auth_http_port', 'auth_https_port', 'auth_keepalive',\\n                   'auth_session_limit', 'auto_auth_extension_device', 'av_affinity',\\n                   'av_failopen', 'av_failopen_session', 'batch_cmdb',\\n                   'block_session_timer', 'br_fdb_max_entry', 'cert_chain_max',\\n                   'cfg_revert_timeout', 'cfg_save', 'check_protocol_header',\\n                   'check_reset_range', 'cli_audit_log', 'clt_cert_req',\\n                   'compliance_check', 'compliance_check_time', 'cpu_use_threshold',\\n                   'csr_ca_attribute', 'daily_restart', 'device_identification_active_scan_delay',\\n                   'device_idle_timeout', 'dh_params', 'dnsproxy_worker_count',\\n                   'dst', 'endpoint_control_fds_access', 'endpoint_control_portal_port',\\n                   'failtime', 'fds_statistics', 'fds_statistics_period',\\n                   'fgd_alert_subscription', 'fortiextender', 'fortiextender_data_port',\\n                   'fortiextender_vlan_mode', 'fortiservice_port', 'gui_certificates',\\n                   'gui_custom_language', 'gui_date_format', 'gui_device_latitude',\\n                   'gui_device_longitude', 'gui_display_hostname', 'gui_ipv6',\\n                   'gui_lines_per_page', 'gui_theme', 'gui_wireless_opensecurity',\\n                   'honor_df', 'hostname', 'igmp_state_limit',\\n                   'interval', 'ip_src_port_range', 'ips_affinity',\\n                   'ipsec_asic_offload', 'ipsec_hmac_offload', 'ipsec_soft_dec_async',\\n                   'ipv6_accept_dad', 'ipv6_allow_anycast_probe', 'language',\\n                   'ldapconntimeout', 'lldp_transmission', 'log_ssl_connection',\\n                   'log_uuid', 'login_timestamp', 'long_vdom_name',\\n                   'management_vdom', 'max_dlpstat_memory', 'max_route_cache_size',\\n                   'mc_ttl_notchange', 'memory_use_threshold_extreme', 'memory_use_threshold_green',\\n                   'memory_use_threshold_red', 'miglog_affinity', 'miglogd_children',\\n                   'multi_factor_authentication', 'multicast_forward', 'ndp_max_entry',\\n                   'per_user_bwl', 'policy_auth_concurrent', 'post_login_banner',\\n                   'pre_login_banner', 'private_data_encryption', 'proxy_auth_lifetime',\\n                   'proxy_auth_lifetime_timeout', 'proxy_auth_timeout', 'proxy_cipher_hardware_acceleration',\\n                   'proxy_kxp_hardware_acceleration', 'proxy_re_authentication_mode', 'proxy_worker_count',\\n                   'radius_port', 'reboot_upon_config_restore', 'refresh',\\n                   'remoteauthtimeout', 'reset_sessionless_tcp', 'restart_time',\\n                   'revision_backup_on_logout', 'revision_image_auto_backup', 'scanunit_count',\\n                   'security_rating_result_submission', 'security_rating_run_on_schedule', 'send_pmtu_icmp',\\n                   'snat_route_change', 'special_file_23_support', 'ssd_trim_date',\\n                   'ssd_trim_freq', 'ssd_trim_hour', 'ssd_trim_min',\\n                   'ssd_trim_weekday', 'ssh_cbc_cipher', 'ssh_hmac_md5',\\n                   'ssh_kex_sha1', 'ssl_min_proto_version', 'ssl_static_key_ciphers',\\n                   'sslvpn_cipher_hardware_acceleration', 'sslvpn_kxp_hardware_acceleration', 'sslvpn_max_worker_count',\\n                   'sslvpn_plugin_version_check', 'strict_dirty_session_check', 'strong_crypto',\\n                   'switch_controller', 'switch_controller_reserved_network', 'sys_perf_log_interval',\\n                   'tcp_halfclose_timer', 'tcp_halfopen_timer', 'tcp_option',\\n                   'tcp_timewait_timer', 'tftp', 'timezone',\\n                   'tp_mc_skip_policy', 'traffic_priority', 'traffic_priority_level',\\n                   'two_factor_email_expiry', 'two_factor_fac_expiry', 'two_factor_ftk_expiry',\\n                   'two_factor_ftm_expiry', 'two_factor_sms_expiry', 'udp_idle_timer',\\n                   'user_server_cert', 'vdom_admin', 'vip_arp_range',\\n                   'virtual_server_count', 'virtual_server_hardware_acceleration', 'wad_affinity',\\n                   'wad_csvc_cs_count', 'wad_csvc_db_count', 'wad_source_affinity',\\n                   'wad_worker_count', 'wifi_ca_certificate', 'wifi_certificate',\\n                   'wimax_4g_usb', 'wireless_controller', 'wireless_controller_port']\\n    dictionary = {}\\n    for attribute in option_list:\\n        if attribute in json and json[attribute] is not None:\\n            dictionary[attribute] = json[attribute]\\n    return dictionary",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def filter_system_global_data(json):\\n    option_list = ['admin_concurrent', 'admin_console_timeout', 'admin_hsts_max_age',\\n                   'admin_https_pki_required', 'admin_https_redirect', 'admin_https_ssl_versions',\\n                   'admin_lockout_duration', 'admin_lockout_threshold', 'admin_login_max',\\n                   'admin_maintainer', 'admin_port', 'admin_restrict_local',\\n                   'admin_scp', 'admin_server_cert', 'admin_sport',\\n                   'admin_ssh_grace_time', 'admin_ssh_password', 'admin_ssh_port',\\n                   'admin_ssh_v1', 'admin_telnet_port', 'admintimeout',\\n                   'alias', 'allow_traffic_redirect', 'anti_replay',\\n                   'arp_max_entry', 'asymroute', 'auth_cert',\\n                   'auth_http_port', 'auth_https_port', 'auth_keepalive',\\n                   'auth_session_limit', 'auto_auth_extension_device', 'av_affinity',\\n                   'av_failopen', 'av_failopen_session', 'batch_cmdb',\\n                   'block_session_timer', 'br_fdb_max_entry', 'cert_chain_max',\\n                   'cfg_revert_timeout', 'cfg_save', 'check_protocol_header',\\n                   'check_reset_range', 'cli_audit_log', 'clt_cert_req',\\n                   'compliance_check', 'compliance_check_time', 'cpu_use_threshold',\\n                   'csr_ca_attribute', 'daily_restart', 'device_identification_active_scan_delay',\\n                   'device_idle_timeout', 'dh_params', 'dnsproxy_worker_count',\\n                   'dst', 'endpoint_control_fds_access', 'endpoint_control_portal_port',\\n                   'failtime', 'fds_statistics', 'fds_statistics_period',\\n                   'fgd_alert_subscription', 'fortiextender', 'fortiextender_data_port',\\n                   'fortiextender_vlan_mode', 'fortiservice_port', 'gui_certificates',\\n                   'gui_custom_language', 'gui_date_format', 'gui_device_latitude',\\n                   'gui_device_longitude', 'gui_display_hostname', 'gui_ipv6',\\n                   'gui_lines_per_page', 'gui_theme', 'gui_wireless_opensecurity',\\n                   'honor_df', 'hostname', 'igmp_state_limit',\\n                   'interval', 'ip_src_port_range', 'ips_affinity',\\n                   'ipsec_asic_offload', 'ipsec_hmac_offload', 'ipsec_soft_dec_async',\\n                   'ipv6_accept_dad', 'ipv6_allow_anycast_probe', 'language',\\n                   'ldapconntimeout', 'lldp_transmission', 'log_ssl_connection',\\n                   'log_uuid', 'login_timestamp', 'long_vdom_name',\\n                   'management_vdom', 'max_dlpstat_memory', 'max_route_cache_size',\\n                   'mc_ttl_notchange', 'memory_use_threshold_extreme', 'memory_use_threshold_green',\\n                   'memory_use_threshold_red', 'miglog_affinity', 'miglogd_children',\\n                   'multi_factor_authentication', 'multicast_forward', 'ndp_max_entry',\\n                   'per_user_bwl', 'policy_auth_concurrent', 'post_login_banner',\\n                   'pre_login_banner', 'private_data_encryption', 'proxy_auth_lifetime',\\n                   'proxy_auth_lifetime_timeout', 'proxy_auth_timeout', 'proxy_cipher_hardware_acceleration',\\n                   'proxy_kxp_hardware_acceleration', 'proxy_re_authentication_mode', 'proxy_worker_count',\\n                   'radius_port', 'reboot_upon_config_restore', 'refresh',\\n                   'remoteauthtimeout', 'reset_sessionless_tcp', 'restart_time',\\n                   'revision_backup_on_logout', 'revision_image_auto_backup', 'scanunit_count',\\n                   'security_rating_result_submission', 'security_rating_run_on_schedule', 'send_pmtu_icmp',\\n                   'snat_route_change', 'special_file_23_support', 'ssd_trim_date',\\n                   'ssd_trim_freq', 'ssd_trim_hour', 'ssd_trim_min',\\n                   'ssd_trim_weekday', 'ssh_cbc_cipher', 'ssh_hmac_md5',\\n                   'ssh_kex_sha1', 'ssl_min_proto_version', 'ssl_static_key_ciphers',\\n                   'sslvpn_cipher_hardware_acceleration', 'sslvpn_kxp_hardware_acceleration', 'sslvpn_max_worker_count',\\n                   'sslvpn_plugin_version_check', 'strict_dirty_session_check', 'strong_crypto',\\n                   'switch_controller', 'switch_controller_reserved_network', 'sys_perf_log_interval',\\n                   'tcp_halfclose_timer', 'tcp_halfopen_timer', 'tcp_option',\\n                   'tcp_timewait_timer', 'tftp', 'timezone',\\n                   'tp_mc_skip_policy', 'traffic_priority', 'traffic_priority_level',\\n                   'two_factor_email_expiry', 'two_factor_fac_expiry', 'two_factor_ftk_expiry',\\n                   'two_factor_ftm_expiry', 'two_factor_sms_expiry', 'udp_idle_timer',\\n                   'user_server_cert', 'vdom_admin', 'vip_arp_range',\\n                   'virtual_server_count', 'virtual_server_hardware_acceleration', 'wad_affinity',\\n                   'wad_csvc_cs_count', 'wad_csvc_db_count', 'wad_source_affinity',\\n                   'wad_worker_count', 'wifi_ca_certificate', 'wifi_certificate',\\n                   'wimax_4g_usb', 'wireless_controller', 'wireless_controller_port']\\n    dictionary = {}\\n    for attribute in option_list:\\n        if attribute in json and json[attribute] is not None:\\n            dictionary[attribute] = json[attribute]\\n    return dictionary"
  },
  {
    "code": "def _bistochastic_preprocess(X, maxiter=1000, tol=1e-5):\\n    X = _make_nonnegative(X)\\n    X_scaled = X\\n    dist = None\\n    for _ in range(maxiter):\\n        X_new, _, _ = _scale_preprocess(X_scaled)\\n        dist = np.linalg.norm(X_scaled - X_new)\\n        X_scaled = X_new\\n        if dist is not None and dist < tol:\\n            break\\n    return X_scaled",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "now supports sparse data",
    "fixed_code": "def _bistochastic_preprocess(X, maxiter=1000, tol=1e-5):\\n    X = _make_nonnegative(X)\\n    X_scaled = X\\n    dist = None\\n    for _ in range(maxiter):\\n        X_new, _, _ = _scale_preprocess(X_scaled)\\n        if issparse(X):\\n            dist = np.linalg.norm(X_scaled.data - X.data)\\n        else:\\n            dist = np.linalg.norm(X_scaled - X_new)\\n        X_scaled = X_new\\n        if dist is not None and dist < tol:\\n            break\\n    return X_scaled"
  },
  {
    "code": "def tell(self) -> int:\\n        return self.seek(0, 1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def tell(self) -> int:\\n        return self.seek(0, 1)"
  },
  {
    "code": "def delete_many(self, keys, version=None):\\n        key_list = []\\n        for key in keys:\\n            key = self.make_key(key, version)\\n            self.validate_key(key)\\n            key_list.append(key)\\n        self._base_delete_many(key_list)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def delete_many(self, keys, version=None):\\n        key_list = []\\n        for key in keys:\\n            key = self.make_key(key, version)\\n            self.validate_key(key)\\n            key_list.append(key)\\n        self._base_delete_many(key_list)"
  },
  {
    "code": "def _expand_mapped_kwargs(self, context: Context, session: Session) -> Tuple[Mapping[str, Any], Set[int]]:\\n        assert self.expand_input is EXPAND_INPUT_EMPTY\\n        op_kwargs, resolved_oids = super()._expand_mapped_kwargs(context, session)\\n        return {\"op_kwargs\": op_kwargs}, resolved_oids",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _expand_mapped_kwargs(self, context: Context, session: Session) -> Tuple[Mapping[str, Any], Set[int]]:\\n        assert self.expand_input is EXPAND_INPUT_EMPTY\\n        op_kwargs, resolved_oids = super()._expand_mapped_kwargs(context, session)\\n        return {\"op_kwargs\": op_kwargs}, resolved_oids"
  },
  {
    "code": "def _validate_shuffle_split_init(test_size, train_size):\\n    if test_size == \"default\":\\n        if train_size is not None:\\n            warnings.warn(\"From version 0.21, test_size will always \"\\n                          \"complement train_size unless both \"\\n                          \"are specified.\",\\n                          FutureWarning)\\n        test_size = 0.1\\n    if test_size is None and train_size is None:\\n        raise ValueError('test_size and train_size can not both be None')\\n    if test_size is not None:\\n        if np.asarray(test_size).dtype.kind == 'f':\\n            if test_size >= 1.:\\n                raise ValueError(\\n                    'test_size=%f should be smaller '\\n                    'than 1.0 or be an integer' % test_size)\\n        elif np.asarray(test_size).dtype.kind != 'i':\\n            raise ValueError(\"Invalid value for test_size: %r\" % test_size)\\n    if train_size is not None:\\n        if np.asarray(train_size).dtype.kind == 'f':\\n            if train_size >= 1.:\\n                raise ValueError(\"train_size=%f should be smaller \"\\n                                 \"than 1.0 or be an integer\" % train_size)\\n            elif (np.asarray(test_size).dtype.kind == 'f' and\\n                    (train_size + test_size) > 1.):\\n                raise ValueError('The sum of test_size and train_size = %f, '\\n                                 'should be smaller than 1.0. Reduce '\\n                                 'test_size and/or train_size.' %\\n                                 (train_size + test_size))\\n        elif np.asarray(train_size).dtype.kind != 'i':\\n            raise ValueError(\"Invalid value for train_size: %r\" % train_size)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX Add more validation for size parameters in train_test_split (#12733)",
    "fixed_code": "def _validate_shuffle_split_init(test_size, train_size):\\n    if test_size == \"default\":\\n        if train_size is not None:\\n            warnings.warn(\"From version 0.21, test_size will always \"\\n                          \"complement train_size unless both \"\\n                          \"are specified.\",\\n                          FutureWarning)\\n        test_size = 0.1\\n    if test_size is None and train_size is None:\\n        raise ValueError('test_size and train_size can not both be None')\\n    if test_size is not None:\\n        if np.asarray(test_size).dtype.kind == 'f':\\n            if test_size >= 1. or test_size <= 0:\\n                raise ValueError(\\n                    'test_size=%f should be in the (0, 1) range '\\n                    'or be an integer' % test_size)\\n        elif np.asarray(test_size).dtype.kind != 'i':\\n            raise ValueError(\"Invalid value for test_size: %r\" % test_size)\\n    if train_size is not None:\\n        if np.asarray(train_size).dtype.kind == 'f':\\n            if train_size >= 1. or train_size <= 0:\\n                raise ValueError('train_size=%f should be in the (0, 1) range '\\n                                 'or be an integer' % train_size)\\n            elif (np.asarray(test_size).dtype.kind == 'f' and\\n                    (\\n                        (train_size + test_size) > 1. or\\n                        (train_size + test_size) < 0)):\\n                raise ValueError('The sum of test_size and train_size = %f, '\\n                                 'should be in the (0, 1) range. Reduce '\\n                                 'test_size and/or train_size.' %\\n                                 (train_size + test_size))\\n        elif np.asarray(train_size).dtype.kind != 'i':\\n            raise ValueError(\"Invalid value for train_size: %r\" % train_size)"
  },
  {
    "code": "def enet_path(X, y, *, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,\\n              precompute='auto', Xy=None, copy_X=True, coef_init=None,\\n              verbose=False, return_n_iter=False, positive=False,\\n              check_input=True, **params):\\n    if check_input:\\n        X = check_array(X, accept_sparse='csc', dtype=[np.float64, np.float32],\\n                        order='F', copy=copy_X)\\n        y = check_array(y, accept_sparse='csc', dtype=X.dtype.type,\\n                        order='F', copy=False, ensure_2d=False)\\n        if Xy is not None:\\n            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False,\\n                             ensure_2d=False)\\n    n_samples, n_features = X.shape\\n    multi_output = False\\n    if y.ndim != 1:\\n        multi_output = True\\n        _, n_outputs = y.shape\\n    if multi_output and positive:\\n        raise ValueError('positive=True is not allowed for multi-output'\\n                         ' (y.ndim != 1)')\\n    if not multi_output and sparse.isspmatrix(X):\\n        if 'X_offset' in params:\\n            X_sparse_scaling = params['X_offset'] / params['X_scale']\\n            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)\\n        else:\\n            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)\\n    if check_input:\\n        X, y, X_offset, y_offset, X_scale, precompute, Xy = \\\\n            _pre_fit(X, y, Xy, precompute, normalize=False,\\n                     fit_intercept=False, copy=False, check_input=check_input)\\n    if alphas is None:\\n        alphas = _alpha_grid(X, y, Xy=Xy, l1_ratio=l1_ratio,\\n                             fit_intercept=False, eps=eps, n_alphas=n_alphas,\\n                             normalize=False, copy_X=False)\\n    else:\\n        alphas = np.sort(alphas)[::-1]  \\n    n_alphas = len(alphas)\\n    tol = params.get('tol', 1e-4)\\n    max_iter = params.get('max_iter', 1000)\\n    dual_gaps = np.empty(n_alphas)\\n    n_iters = []\\n    rng = check_random_state(params.get('random_state', None))\\n    selection = params.get('selection', 'cyclic')\\n    if selection not in ['random', 'cyclic']:\\n        raise ValueError(\"selection should be either random or cyclic.\")\\n    random = (selection == 'random')\\n    if not multi_output:\\n        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)\\n    else:\\n        coefs = np.empty((n_outputs, n_features, n_alphas),\\n                         dtype=X.dtype)\\n    if coef_init is None:\\n        coef_ = np.zeros(coefs.shape[:-1], dtype=X.dtype, order='F')\\n    else:\\n        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)\\n    for i, alpha in enumerate(alphas):\\n        l1_reg = alpha * l1_ratio * n_samples\\n        l2_reg = alpha * (1.0 - l1_ratio) * n_samples\\n        if not multi_output and sparse.isspmatrix(X):\\n            model = cd_fast.sparse_enet_coordinate_descent(\\n                coef_, l1_reg, l2_reg, X.data, X.indices,\\n                X.indptr, y, X_sparse_scaling,\\n                max_iter, tol, rng, random, positive)\\n        elif multi_output:\\n            model = cd_fast.enet_coordinate_descent_multi_task(\\n                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random)\\n        elif isinstance(precompute, np.ndarray):\\n            if check_input:\\n                precompute = check_array(precompute, dtype=X.dtype.type,\\n                                         order='C')\\n            model = cd_fast.enet_coordinate_descent_gram(\\n                coef_, l1_reg, l2_reg, precompute, Xy, y, max_iter,\\n                tol, rng, random, positive)\\n        elif precompute is False:\\n            model = cd_fast.enet_coordinate_descent(\\n                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random,\\n                positive)\\n        else:\\n            raise ValueError(\"Precompute should be one of True, False, \"\\n                             \"'auto' or array-like. Got %r\" % precompute)\\n        coef_, dual_gap_, eps_, n_iter_ = model\\n        coefs[..., i] = coef_\\n        dual_gaps[i] = dual_gap_ / n_samples\\n        n_iters.append(n_iter_)\\n        if verbose:\\n            if verbose > 2:\\n                print(model)\\n            elif verbose > 1:\\n                print('Path: %03i out of %03i' % (i, n_alphas))\\n            else:\\n                sys.stderr.write('.')\\n    if return_n_iter:\\n        return alphas, coefs, dual_gaps, n_iters\\n    return alphas, coefs, dual_gaps",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def enet_path(X, y, *, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,\\n              precompute='auto', Xy=None, copy_X=True, coef_init=None,\\n              verbose=False, return_n_iter=False, positive=False,\\n              check_input=True, **params):\\n    if check_input:\\n        X = check_array(X, accept_sparse='csc', dtype=[np.float64, np.float32],\\n                        order='F', copy=copy_X)\\n        y = check_array(y, accept_sparse='csc', dtype=X.dtype.type,\\n                        order='F', copy=False, ensure_2d=False)\\n        if Xy is not None:\\n            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False,\\n                             ensure_2d=False)\\n    n_samples, n_features = X.shape\\n    multi_output = False\\n    if y.ndim != 1:\\n        multi_output = True\\n        _, n_outputs = y.shape\\n    if multi_output and positive:\\n        raise ValueError('positive=True is not allowed for multi-output'\\n                         ' (y.ndim != 1)')\\n    if not multi_output and sparse.isspmatrix(X):\\n        if 'X_offset' in params:\\n            X_sparse_scaling = params['X_offset'] / params['X_scale']\\n            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)\\n        else:\\n            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)\\n    if check_input:\\n        X, y, X_offset, y_offset, X_scale, precompute, Xy = \\\\n            _pre_fit(X, y, Xy, precompute, normalize=False,\\n                     fit_intercept=False, copy=False, check_input=check_input)\\n    if alphas is None:\\n        alphas = _alpha_grid(X, y, Xy=Xy, l1_ratio=l1_ratio,\\n                             fit_intercept=False, eps=eps, n_alphas=n_alphas,\\n                             normalize=False, copy_X=False)\\n    else:\\n        alphas = np.sort(alphas)[::-1]  \\n    n_alphas = len(alphas)\\n    tol = params.get('tol', 1e-4)\\n    max_iter = params.get('max_iter', 1000)\\n    dual_gaps = np.empty(n_alphas)\\n    n_iters = []\\n    rng = check_random_state(params.get('random_state', None))\\n    selection = params.get('selection', 'cyclic')\\n    if selection not in ['random', 'cyclic']:\\n        raise ValueError(\"selection should be either random or cyclic.\")\\n    random = (selection == 'random')\\n    if not multi_output:\\n        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)\\n    else:\\n        coefs = np.empty((n_outputs, n_features, n_alphas),\\n                         dtype=X.dtype)\\n    if coef_init is None:\\n        coef_ = np.zeros(coefs.shape[:-1], dtype=X.dtype, order='F')\\n    else:\\n        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)\\n    for i, alpha in enumerate(alphas):\\n        l1_reg = alpha * l1_ratio * n_samples\\n        l2_reg = alpha * (1.0 - l1_ratio) * n_samples\\n        if not multi_output and sparse.isspmatrix(X):\\n            model = cd_fast.sparse_enet_coordinate_descent(\\n                coef_, l1_reg, l2_reg, X.data, X.indices,\\n                X.indptr, y, X_sparse_scaling,\\n                max_iter, tol, rng, random, positive)\\n        elif multi_output:\\n            model = cd_fast.enet_coordinate_descent_multi_task(\\n                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random)\\n        elif isinstance(precompute, np.ndarray):\\n            if check_input:\\n                precompute = check_array(precompute, dtype=X.dtype.type,\\n                                         order='C')\\n            model = cd_fast.enet_coordinate_descent_gram(\\n                coef_, l1_reg, l2_reg, precompute, Xy, y, max_iter,\\n                tol, rng, random, positive)\\n        elif precompute is False:\\n            model = cd_fast.enet_coordinate_descent(\\n                coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random,\\n                positive)\\n        else:\\n            raise ValueError(\"Precompute should be one of True, False, \"\\n                             \"'auto' or array-like. Got %r\" % precompute)\\n        coef_, dual_gap_, eps_, n_iter_ = model\\n        coefs[..., i] = coef_\\n        dual_gaps[i] = dual_gap_ / n_samples\\n        n_iters.append(n_iter_)\\n        if verbose:\\n            if verbose > 2:\\n                print(model)\\n            elif verbose > 1:\\n                print('Path: %03i out of %03i' % (i, n_alphas))\\n            else:\\n                sys.stderr.write('.')\\n    if return_n_iter:\\n        return alphas, coefs, dual_gaps, n_iters\\n    return alphas, coefs, dual_gaps"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        object=dict(required=True, type=\"str\", choices=[\"device\", \"cluster_nodes\", \"task\", \"custom\"]),\\n        custom_endpoint=dict(required=False, type=\"str\"),\\n        custom_dict=dict(required=False, type=\"dict\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_unique_name=dict(required=False, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\"),\\n        nodes=dict(required=False, type=\"list\"),\\n        task_id=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"adom\": module.params[\"adom\"],\\n        \"object\": module.params[\"object\"],\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"nodes\": module.params[\"nodes\"],\\n        \"task_id\": module.params[\"task_id\"],\\n        \"custom_endpoint\": module.params[\"custom_endpoint\"],\\n        \"custom_dict\": module.params[\"custom_dict\"]\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"object\"] == \"device\" and any(v is not None for v in [paramgram[\"device_unique_name\"],\\n                                                                           paramgram[\"device_serial\"],\\n                                                                           paramgram[\"device_ip\"]]):\\n            results = fmgr_get_device(fmgr, paramgram)\\n            if results[0] not in [0]:\\n                module.fail_json(msg=\"Device query failed!\")\\n            elif len(results[1]) == 0:\\n                module.exit_json(msg=\"Device NOT FOUND!\")\\n            else:\\n                module.exit_json(msg=\"Device Found\", **results[1][0])\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"cluster_nodes\" and paramgram[\"nodes\"] is not None:\\n            results = fmgr_get_cluster_nodes(fmgr, paramgram)\\n            if results[\"cluster_status\"] == \"MISSING\":\\n                module.exit_json(msg=\"No cluster device found!\", **results)\\n            elif results[\"query_status\"] == \"good\":\\n                module.exit_json(msg=\"Cluster Found - Showing Nodes\", **results)\\n            elif results is None:\\n                module.fail_json(msg=\"Query FAILED -- Check module or playbook syntax\")\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"task\":\\n            results = fmgr_get_task_status(fmgr, paramgram)\\n            if results[0] != 0:\\n                module.fail_json(**results[1])\\n            if results[0] == 0:\\n                module.exit_json(**results[1])\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"custom\":\\n            results = fmgr_get_custom(fmgr, paramgram)\\n            if results[0] != 0:\\n                module.fail_json(msg=\"QUERY FAILED -- Please check syntax check JSON guide if needed.\")\\n            if results[0] == 0:\\n                results_len = len(results[1])\\n                if results_len > 0:\\n                    results_combine = dict()\\n                    if isinstance(results[1], dict):\\n                        results_combine[\"results\"] = results[1]\\n                    if isinstance(results[1], list):\\n                        results_combine[\"results\"] = results[1][0:results_len]\\n                    module.exit_json(msg=\"Custom Query Success\", **results_combine)\\n                else:\\n                    module.exit_json(msg=\"NO RESULTS\")\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        object=dict(required=True, type=\"str\", choices=[\"device\", \"cluster_nodes\", \"task\", \"custom\"]),\\n        custom_endpoint=dict(required=False, type=\"str\"),\\n        custom_dict=dict(required=False, type=\"dict\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_unique_name=dict(required=False, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\"),\\n        nodes=dict(required=False, type=\"list\"),\\n        task_id=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"adom\": module.params[\"adom\"],\\n        \"object\": module.params[\"object\"],\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"nodes\": module.params[\"nodes\"],\\n        \"task_id\": module.params[\"task_id\"],\\n        \"custom_endpoint\": module.params[\"custom_endpoint\"],\\n        \"custom_dict\": module.params[\"custom_dict\"]\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"object\"] == \"device\" and any(v is not None for v in [paramgram[\"device_unique_name\"],\\n                                                                           paramgram[\"device_serial\"],\\n                                                                           paramgram[\"device_ip\"]]):\\n            results = fmgr_get_device(fmgr, paramgram)\\n            if results[0] not in [0]:\\n                module.fail_json(msg=\"Device query failed!\")\\n            elif len(results[1]) == 0:\\n                module.exit_json(msg=\"Device NOT FOUND!\")\\n            else:\\n                module.exit_json(msg=\"Device Found\", **results[1][0])\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"cluster_nodes\" and paramgram[\"nodes\"] is not None:\\n            results = fmgr_get_cluster_nodes(fmgr, paramgram)\\n            if results[\"cluster_status\"] == \"MISSING\":\\n                module.exit_json(msg=\"No cluster device found!\", **results)\\n            elif results[\"query_status\"] == \"good\":\\n                module.exit_json(msg=\"Cluster Found - Showing Nodes\", **results)\\n            elif results is None:\\n                module.fail_json(msg=\"Query FAILED -- Check module or playbook syntax\")\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"task\":\\n            results = fmgr_get_task_status(fmgr, paramgram)\\n            if results[0] != 0:\\n                module.fail_json(**results[1])\\n            if results[0] == 0:\\n                module.exit_json(**results[1])\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"custom\":\\n            results = fmgr_get_custom(fmgr, paramgram)\\n            if results[0] != 0:\\n                module.fail_json(msg=\"QUERY FAILED -- Please check syntax check JSON guide if needed.\")\\n            if results[0] == 0:\\n                results_len = len(results[1])\\n                if results_len > 0:\\n                    results_combine = dict()\\n                    if isinstance(results[1], dict):\\n                        results_combine[\"results\"] = results[1]\\n                    if isinstance(results[1], list):\\n                        results_combine[\"results\"] = results[1][0:results_len]\\n                    module.exit_json(msg=\"Custom Query Success\", **results_combine)\\n                else:\\n                    module.exit_json(msg=\"NO RESULTS\")\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def _stringify(col):\\n    try:\\n        return unicode(col)\\n    except UnicodeError:\\n        return console_encode(col)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _stringify(col):\\n    try:\\n        return unicode(col)\\n    except UnicodeError:\\n        return console_encode(col)"
  },
  {
    "code": "def __setstate__(self, state):\\n        self.mgr_locs = libinternals.BlockPlacement(state[0])\\n        self.values = extract_array(state[1], extract_numpy=True)\\n        self.ndim = self.values.ndim",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __setstate__(self, state):\\n        self.mgr_locs = libinternals.BlockPlacement(state[0])\\n        self.values = extract_array(state[1], extract_numpy=True)\\n        self.ndim = self.values.ndim"
  },
  {
    "code": "def __nonzero__(self):\\n        return self._is_special or self._int != '0'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __nonzero__(self):\\n        return self._is_special or self._int != '0'"
  },
  {
    "code": "def set_autocommit(self, autocommit):\\n        self.validate_no_atomic_block()\\n        self.ensure_connection()\\n        self._set_autocommit(autocommit)\\n        self.autocommit = autocommit",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_autocommit(self, autocommit):\\n        self.validate_no_atomic_block()\\n        self.ensure_connection()\\n        self._set_autocommit(autocommit)\\n        self.autocommit = autocommit"
  },
  {
    "code": "def copy(self):\\n        vec_copy = self._vector.copy()\\n        return SparseSeries(vec_copy, index=self.index)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "refactored to remove SparseVector class",
    "fixed_code": "def copy(self):\\n        values = np.asarray(self).copy()\\n        return SparseSeries(values, index=self.index,\\n                            sparse_index=self.sparse_index)\\nclass SparseTimeSeries(SparseSeries, TimeSeries):\\n    pass\\nclass SparseDataFrame(DataFrame):\\n    _columns = None"
  },
  {
    "code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen,\\n                   match_args, kw_only, slots):\\n    fields = {}\\n    if cls.__module__ in sys.modules:\\n        globals = sys.modules[cls.__module__].__dict__\\n    else:\\n        globals = {}\\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n                                           unsafe_hash, frozen))\\n    any_frozen_base = False\\n    has_dataclass_bases = False\\n    for b in cls.__mro__[-1:0:-1]:\\n        base_fields = getattr(b, _FIELDS, None)\\n        if base_fields is not None:\\n            has_dataclass_bases = True\\n            for f in base_fields.values():\\n                fields[f.name] = f\\n            if getattr(b, _PARAMS).frozen:\\n                any_frozen_base = True\\n    cls_annotations = cls.__dict__.get('__annotations__', {})\\n    cls_fields = []\\n    KW_ONLY_seen = False\\n    dataclasses = sys.modules[__name__]\\n    for name, type in cls_annotations.items():\\n        if (_is_kw_only(type, dataclasses)\\n            or (isinstance(type, str)\\n                and _is_type(type, cls, dataclasses, dataclasses.KW_ONLY,\\n                             _is_kw_only))):\\n            if KW_ONLY_seen:\\n                raise TypeError(f'{name!r} is KW_ONLY, but KW_ONLY '\\n                                'has already been specified')\\n            KW_ONLY_seen = True\\n            kw_only = True\\n        else:\\n            cls_fields.append(_get_field(cls, name, type, kw_only))\\n    for f in cls_fields:\\n        fields[f.name] = f\\n        if isinstance(getattr(cls, f.name, None), Field):\\n            if f.default is MISSING:\\n                delattr(cls, f.name)\\n            else:\\n                setattr(cls, f.name, f.default)\\n    for name, value in cls.__dict__.items():\\n        if isinstance(value, Field) and not name in cls_annotations:\\n            raise TypeError(f'{name!r} is a field but has no type annotation')\\n    if has_dataclass_bases:\\n        if any_frozen_base and not frozen:\\n            raise TypeError('cannot inherit non-frozen dataclass from a '\\n                            'frozen one')\\n        if not any_frozen_base and frozen:\\n            raise TypeError('cannot inherit frozen dataclass from a '\\n                            'non-frozen one')\\n    setattr(cls, _FIELDS, fields)\\n    class_hash = cls.__dict__.get('__hash__', MISSING)\\n    has_explicit_hash = not (class_hash is MISSING or\\n                             (class_hash is None and '__eq__' in cls.__dict__))\\n    if order and not eq:\\n        raise ValueError('eq must be true if order is true')\\n    all_init_fields = [f for f in fields.values()\\n                       if f._field_type in (_FIELD, _FIELD_INITVAR)]\\n    (std_init_fields,\\n     kw_only_init_fields) = _fields_in_init_order(all_init_fields)\\n    if init:\\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\\n        _set_new_attribute(cls, '__init__',\\n                           _init_fn(all_init_fields,\\n                                    std_init_fields,\\n                                    kw_only_init_fields,\\n                                    frozen,\\n                                    has_post_init,\\n                                    '__dataclass_self__' if 'self' in fields\\n                                            else 'self',\\n                                    globals,\\n                          ))\\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\\n    if repr:\\n        flds = [f for f in field_list if f.repr]\\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n    if eq:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        _set_new_attribute(cls, '__eq__',\\n                           _cmp_fn('__eq__', '==',\\n                                   self_tuple, other_tuple,\\n                                   globals=globals))\\n    if order:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        for name, op in [('__lt__', '<'),\\n                         ('__le__', '<='),\\n                         ('__gt__', '>'),\\n                         ('__ge__', '>='),\\n                         ]:\\n            if _set_new_attribute(cls, name,\\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\\n                                          globals=globals)):\\n                raise TypeError(f'Cannot overwrite attribute {name} '\\n                                f'in class {cls.__name__}. Consider using '\\n                                'functools.total_ordering')\\n    if frozen:\\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\\n            if _set_new_attribute(cls, fn.__name__, fn):\\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n                                f'in class {cls.__name__}')\\n    hash_action = _hash_action[bool(unsafe_hash),\\n                               bool(eq),\\n                               bool(frozen),\\n                               has_explicit_hash]\\n    if hash_action:\\n        cls.__hash__ = hash_action(cls, field_list, globals)\\n    if not getattr(cls, '__doc__'):\\n        cls.__doc__ = (cls.__name__ +\\n                       str(inspect.signature(cls)).replace(' -> None', ''))\\n    if match_args:\\n        _set_new_attribute(cls, '__match_args__',\\n                           tuple(f.name for f in std_init_fields))\\n    if slots:\\n        cls = _add_slots(cls, frozen)\\n    abc.update_abstractmethods(cls)\\n    return cls",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen,\\n                   match_args, kw_only, slots):\\n    fields = {}\\n    if cls.__module__ in sys.modules:\\n        globals = sys.modules[cls.__module__].__dict__\\n    else:\\n        globals = {}\\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n                                           unsafe_hash, frozen))\\n    any_frozen_base = False\\n    has_dataclass_bases = False\\n    for b in cls.__mro__[-1:0:-1]:\\n        base_fields = getattr(b, _FIELDS, None)\\n        if base_fields is not None:\\n            has_dataclass_bases = True\\n            for f in base_fields.values():\\n                fields[f.name] = f\\n            if getattr(b, _PARAMS).frozen:\\n                any_frozen_base = True\\n    cls_annotations = cls.__dict__.get('__annotations__', {})\\n    cls_fields = []\\n    KW_ONLY_seen = False\\n    dataclasses = sys.modules[__name__]\\n    for name, type in cls_annotations.items():\\n        if (_is_kw_only(type, dataclasses)\\n            or (isinstance(type, str)\\n                and _is_type(type, cls, dataclasses, dataclasses.KW_ONLY,\\n                             _is_kw_only))):\\n            if KW_ONLY_seen:\\n                raise TypeError(f'{name!r} is KW_ONLY, but KW_ONLY '\\n                                'has already been specified')\\n            KW_ONLY_seen = True\\n            kw_only = True\\n        else:\\n            cls_fields.append(_get_field(cls, name, type, kw_only))\\n    for f in cls_fields:\\n        fields[f.name] = f\\n        if isinstance(getattr(cls, f.name, None), Field):\\n            if f.default is MISSING:\\n                delattr(cls, f.name)\\n            else:\\n                setattr(cls, f.name, f.default)\\n    for name, value in cls.__dict__.items():\\n        if isinstance(value, Field) and not name in cls_annotations:\\n            raise TypeError(f'{name!r} is a field but has no type annotation')\\n    if has_dataclass_bases:\\n        if any_frozen_base and not frozen:\\n            raise TypeError('cannot inherit non-frozen dataclass from a '\\n                            'frozen one')\\n        if not any_frozen_base and frozen:\\n            raise TypeError('cannot inherit frozen dataclass from a '\\n                            'non-frozen one')\\n    setattr(cls, _FIELDS, fields)\\n    class_hash = cls.__dict__.get('__hash__', MISSING)\\n    has_explicit_hash = not (class_hash is MISSING or\\n                             (class_hash is None and '__eq__' in cls.__dict__))\\n    if order and not eq:\\n        raise ValueError('eq must be true if order is true')\\n    all_init_fields = [f for f in fields.values()\\n                       if f._field_type in (_FIELD, _FIELD_INITVAR)]\\n    (std_init_fields,\\n     kw_only_init_fields) = _fields_in_init_order(all_init_fields)\\n    if init:\\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\\n        _set_new_attribute(cls, '__init__',\\n                           _init_fn(all_init_fields,\\n                                    std_init_fields,\\n                                    kw_only_init_fields,\\n                                    frozen,\\n                                    has_post_init,\\n                                    '__dataclass_self__' if 'self' in fields\\n                                            else 'self',\\n                                    globals,\\n                          ))\\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\\n    if repr:\\n        flds = [f for f in field_list if f.repr]\\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n    if eq:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        _set_new_attribute(cls, '__eq__',\\n                           _cmp_fn('__eq__', '==',\\n                                   self_tuple, other_tuple,\\n                                   globals=globals))\\n    if order:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        for name, op in [('__lt__', '<'),\\n                         ('__le__', '<='),\\n                         ('__gt__', '>'),\\n                         ('__ge__', '>='),\\n                         ]:\\n            if _set_new_attribute(cls, name,\\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\\n                                          globals=globals)):\\n                raise TypeError(f'Cannot overwrite attribute {name} '\\n                                f'in class {cls.__name__}. Consider using '\\n                                'functools.total_ordering')\\n    if frozen:\\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\\n            if _set_new_attribute(cls, fn.__name__, fn):\\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n                                f'in class {cls.__name__}')\\n    hash_action = _hash_action[bool(unsafe_hash),\\n                               bool(eq),\\n                               bool(frozen),\\n                               has_explicit_hash]\\n    if hash_action:\\n        cls.__hash__ = hash_action(cls, field_list, globals)\\n    if not getattr(cls, '__doc__'):\\n        cls.__doc__ = (cls.__name__ +\\n                       str(inspect.signature(cls)).replace(' -> None', ''))\\n    if match_args:\\n        _set_new_attribute(cls, '__match_args__',\\n                           tuple(f.name for f in std_init_fields))\\n    if slots:\\n        cls = _add_slots(cls, frozen)\\n    abc.update_abstractmethods(cls)\\n    return cls"
  },
  {
    "code": "def _solve_eigen_gram(self, alpha, y, sqrt_sw, X_mean, eigvals, Q, QT_y):\\n        w = 1. / (eigvals + alpha)\\n        if self.fit_intercept:\\n            normalized_sw = sqrt_sw / np.linalg.norm(sqrt_sw)\\n            intercept_dim = _find_smallest_angle(normalized_sw, Q)\\n            w[intercept_dim] = 0  \\n        c = np.dot(Q, self._diag_dot(w, QT_y))\\n        G_inverse_diag = self._decomp_diag(w, Q)\\n        if len(y.shape) != 1:\\n            G_inverse_diag = G_inverse_diag[:, np.newaxis]\\n        return G_inverse_diag, c",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _solve_eigen_gram(self, alpha, y, sqrt_sw, X_mean, eigvals, Q, QT_y):\\n        w = 1. / (eigvals + alpha)\\n        if self.fit_intercept:\\n            normalized_sw = sqrt_sw / np.linalg.norm(sqrt_sw)\\n            intercept_dim = _find_smallest_angle(normalized_sw, Q)\\n            w[intercept_dim] = 0  \\n        c = np.dot(Q, self._diag_dot(w, QT_y))\\n        G_inverse_diag = self._decomp_diag(w, Q)\\n        if len(y.shape) != 1:\\n            G_inverse_diag = G_inverse_diag[:, np.newaxis]\\n        return G_inverse_diag, c"
  },
  {
    "code": "def add_device(fmg, paramgram):\\n    datagram = {\\n        \"adom\": paramgram[\"adom\"],\\n        \"flags\": [\"create_task\", \"nonblocking\"],\\n        \"odd_request_form\": \"True\",\\n        \"device\": {\"adm_usr\": paramgram[\"device_username\"], \"adm_pass\": paramgram[\"device_password\"],\\n                   \"ip\": paramgram[\"device_ip\"], \"name\": paramgram[\"device_unique_name\"],\\n                   \"sn\": paramgram[\"device_serial\"], \"mgmt_mode\": \"fmgfaz\", \"flags\": 24}\\n    }\\n    url = '/dvm/cmd/add/device/'\\n    response = fmg.execute(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_device (#52767)",
    "fixed_code": "def add_device(fmgr, paramgram):\\n    datagram = {\\n        \"adom\": paramgram[\"adom\"],\\n        \"flags\": [\"create_task\", \"nonblocking\"],\\n        \"odd_request_form\": \"True\",\\n        \"device\": {\"adm_usr\": paramgram[\"device_username\"], \"adm_pass\": paramgram[\"device_password\"],\\n                   \"ip\": paramgram[\"device_ip\"], \"name\": paramgram[\"device_unique_name\"],\\n                   \"sn\": paramgram[\"device_serial\"], \"mgmt_mode\": \"fmgfaz\", \"flags\": 24}\\n    }\\n    url = '/dvm/cmd/add/device/'\\n    response = fmgr.process_request(url, datagram, FMGRMethods.EXEC)\\n    return response"
  },
  {
    "code": "def get_group_labels(self, group):\\n        inds = self.indices[group]\\n        return self.index.take(inds)\\n    _labels = None\\n    _ids = None\\n    _counts = None\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_group_labels(self, group):\\n        inds = self.indices[group]\\n        return self.index.take(inds)\\n    _labels = None\\n    _ids = None\\n    _counts = None\\n    @property"
  },
  {
    "code": "def _tp_tn_fp_fn(y_true, y_pred, labels=None):\\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\\n    if labels is None:\\n        labels = unique_labels(y_true, y_pred)\\n    else:\\n        labels = np.asarray(labels)\\n    n_labels = labels.size\\n    true_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_neg = np.zeros((n_labels,), dtype=np.int)\\n    if y_type == 'multilabel-indicator':\\n        true_pos = np.sum(np.logical_and(y_true == 1,\\n                                         y_pred == 1), axis=0)\\n        false_pos = np.sum(np.logical_and(y_true != 1,\\n                                          y_pred == 1), axis=0)\\n        false_neg = np.sum(np.logical_and(y_true == 1,\\n                                          y_pred != 1), axis=0)\\n    elif y_type == 'multilabel-sequences':\\n        idx_to_label = dict((label_i, i)\\n                            for i, label_i in enumerate(labels))\\n        for true, pred in zip(y_true, y_pred):\\n            true_set = np.array([idx_to_label[l] for l in set(true)],\\n                                dtype=np.int)\\n            pred_set = np.array([idx_to_label[l] for l in set(pred)],\\n                                dtype=np.int)\\n            true_pos[np.intersect1d(true_set, pred_set)] += 1\\n            false_pos[np.setdiff1d(pred_set, true_set)] += 1\\n            false_neg[np.setdiff1d(true_set, pred_set)] += 1\\n    else:\\n        y_true, y_pred = check_arrays(y_true, y_pred)\\n        for i, label_i in enumerate(labels):\\n            true_pos[i] = np.sum(y_pred[y_true == label_i] == label_i)\\n            false_pos[i] = np.sum(y_pred[y_true != label_i] == label_i)\\n            false_neg[i] = np.sum(y_pred[y_true == label_i] != label_i)\\n    n_samples = len(y_true)\\n    true_neg = n_samples - true_pos - false_pos - false_neg\\n    return true_pos, true_neg, false_pos, false_neg",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH handle properly row vector",
    "fixed_code": "def _tp_tn_fp_fn(y_true, y_pred, labels=None):\\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\\n    if labels is None:\\n        labels = unique_labels(y_true, y_pred)\\n    else:\\n        labels = np.asarray(labels)\\n    n_labels = labels.size\\n    true_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_pos = np.zeros((n_labels,), dtype=np.int)\\n    false_neg = np.zeros((n_labels,), dtype=np.int)\\n    if y_type == 'multilabel-indicator':\\n        true_pos = np.sum(np.logical_and(y_true == 1,\\n                                         y_pred == 1), axis=0)\\n        false_pos = np.sum(np.logical_and(y_true != 1,\\n                                          y_pred == 1), axis=0)\\n        false_neg = np.sum(np.logical_and(y_true == 1,\\n                                          y_pred != 1), axis=0)\\n    elif y_type == 'multilabel-sequences':\\n        idx_to_label = dict((label_i, i)\\n                            for i, label_i in enumerate(labels))\\n        for true, pred in zip(y_true, y_pred):\\n            true_set = np.array([idx_to_label[l] for l in set(true)],\\n                                dtype=np.int)\\n            pred_set = np.array([idx_to_label[l] for l in set(pred)],\\n                                dtype=np.int)\\n            true_pos[np.intersect1d(true_set, pred_set)] += 1\\n            false_pos[np.setdiff1d(pred_set, true_set)] += 1\\n            false_neg[np.setdiff1d(true_set, pred_set)] += 1\\n    else:\\n        for i, label_i in enumerate(labels):\\n            true_pos[i] = np.sum(y_pred[y_true == label_i] == label_i)\\n            false_pos[i] = np.sum(y_pred[y_true != label_i] == label_i)\\n            false_neg[i] = np.sum(y_pred[y_true == label_i] != label_i)\\n    n_samples = len(y_true)\\n    true_neg = n_samples - true_pos - false_pos - false_neg\\n    return true_pos, true_neg, false_pos, false_neg"
  },
  {
    "code": "def get_gce_driver(self):\\n        ''\\n        gce_ini_default_path = os.path.join(\\n            os.path.dirname(os.path.realpath(__file__)), \"gce.ini\")\\n        gce_ini_path = os.environ.get('GCE_INI_PATH', gce_ini_default_path)\\n        config = ConfigParser.SafeConfigParser()\\n        config.read(gce_ini_path)\\n        secrets_path = config.get('gce', 'libcloud_secrets')\\n        secrets_found = False\\n        try:\\n            import secrets\\n            args = getattr(secrets, 'GCE_PARAMS', ())\\n            kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n            secrets_found = True\\n        except:\\n            pass\\n        if not secrets_found and secrets_path:\\n            if not secrets_path.endswith('secrets.py'):\\n                err = \"Must specify libcloud secrets file as \"\\n                err += \"/absolute/path/to/secrets.py\"\\n                print(err)\\n                sys.exit(1)\\n            sys.path.append(os.path.dirname(secrets_path))\\n            try:\\n                import secrets\\n                args = getattr(secrets, 'GCE_PARAMS', ())\\n                kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n                secrets_found = True\\n            except:\\n                pass\\n        if not secrets_found:\\n            args = (\\n                config.get('gce','gce_service_account_email_address'),\\n                config.get('gce','gce_service_account_pem_file_path')\\n            )\\n            kwargs = {'project': config.get('gce','gce_project_id')}\\n        gce = get_driver(Provider.GCE)(*args, **kwargs)\\n        gce.connection.user_agent_append(\"%s/%s\" % (\\n                USER_AGENT_PRODUCT, USER_AGENT_VERSION))\\n        return gce",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_gce_driver(self):\\n        ''\\n        gce_ini_default_path = os.path.join(\\n            os.path.dirname(os.path.realpath(__file__)), \"gce.ini\")\\n        gce_ini_path = os.environ.get('GCE_INI_PATH', gce_ini_default_path)\\n        config = ConfigParser.SafeConfigParser()\\n        config.read(gce_ini_path)\\n        secrets_path = config.get('gce', 'libcloud_secrets')\\n        secrets_found = False\\n        try:\\n            import secrets\\n            args = getattr(secrets, 'GCE_PARAMS', ())\\n            kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n            secrets_found = True\\n        except:\\n            pass\\n        if not secrets_found and secrets_path:\\n            if not secrets_path.endswith('secrets.py'):\\n                err = \"Must specify libcloud secrets file as \"\\n                err += \"/absolute/path/to/secrets.py\"\\n                print(err)\\n                sys.exit(1)\\n            sys.path.append(os.path.dirname(secrets_path))\\n            try:\\n                import secrets\\n                args = getattr(secrets, 'GCE_PARAMS', ())\\n                kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n                secrets_found = True\\n            except:\\n                pass\\n        if not secrets_found:\\n            args = (\\n                config.get('gce','gce_service_account_email_address'),\\n                config.get('gce','gce_service_account_pem_file_path')\\n            )\\n            kwargs = {'project': config.get('gce','gce_project_id')}\\n        gce = get_driver(Provider.GCE)(*args, **kwargs)\\n        gce.connection.user_agent_append(\"%s/%s\" % (\\n                USER_AGENT_PRODUCT, USER_AGENT_VERSION))\\n        return gce"
  },
  {
    "code": "def dispatch_reduction_ufunc(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\\n    assert method == \"reduce\"\\n    if len(inputs) != 1 or inputs[0] is not self:\\n        return NotImplemented\\n    if ufunc.__name__ not in REDUCTION_ALIASES:\\n        return NotImplemented\\n    method_name = REDUCTION_ALIASES[ufunc.__name__]\\n    if not hasattr(self, method_name):\\n        return NotImplemented\\n    if self.ndim > 1:\\n        if isinstance(self, ABCNDFrame):\\n            kwargs[\"numeric_only\"] = False\\n        if \"axis\" not in kwargs:\\n            kwargs[\"axis\"] = None\\n    return getattr(self, method_name)(skipna=False, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR/ENH: support axis=None in min/max (#45072)",
    "fixed_code": "def dispatch_reduction_ufunc(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\\n    assert method == \"reduce\"\\n    if len(inputs) != 1 or inputs[0] is not self:\\n        return NotImplemented\\n    if ufunc.__name__ not in REDUCTION_ALIASES:\\n        return NotImplemented\\n    method_name = REDUCTION_ALIASES[ufunc.__name__]\\n    if not hasattr(self, method_name):\\n        return NotImplemented\\n    if self.ndim > 1:\\n        if isinstance(self, ABCNDFrame):\\n            kwargs[\"numeric_only\"] = False\\n        if \"axis\" not in kwargs:\\n            kwargs[\"axis\"] = 0\\n    return getattr(self, method_name)(skipna=False, **kwargs)"
  },
  {
    "code": "def _reset(self):\\n\\t\\tif self.mode not in ['auto', 'min', 'max']:\\n\\t\\t\\twarnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\\n\\t\\t\\t\\t\\t\\t  'fallback to auto mode.' % (self.mode),\\n\\t\\t\\t\\t\\t\\t  RuntimeWarning)\\n\\t\\t\\tself.mode = 'auto'\\n\\t\\tif (self.mode == 'min' or\\n\\t\\t   (self.mode == 'auto' and 'acc' not in self.monitor)):\\n\\t\\t\\tself.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\\n\\t\\t\\tself.best = np.Inf\\n\\t\\telse:\\n\\t\\t\\tself.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)\\n\\t\\t\\tself.best = -np.Inf\\n\\t\\tself.cooldown_counter = 0\\n\\t\\tself.wait = 0",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed inconsistencies regarding ReduceLROnPlateau (#9723)",
    "fixed_code": "def _reset(self):\\n\\t\\tif self.mode not in ['auto', 'min', 'max']:\\n\\t\\t\\twarnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\\n\\t\\t\\t\\t\\t\\t  'fallback to auto mode.' % (self.mode),\\n\\t\\t\\t\\t\\t\\t  RuntimeWarning)\\n\\t\\t\\tself.mode = 'auto'\\n\\t\\tif (self.mode == 'min' or\\n\\t\\t   (self.mode == 'auto' and 'acc' not in self.monitor)):\\n\\t\\t\\tself.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\\n\\t\\t\\tself.best = np.Inf\\n\\t\\telse:\\n\\t\\t\\tself.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\\n\\t\\t\\tself.best = -np.Inf\\n\\t\\tself.cooldown_counter = 0\\n\\t\\tself.wait = 0"
  },
  {
    "code": "def _ensure_data(values, dtype=None):\\n    if (needs_i8_conversion(values) or\\n            is_period_dtype(dtype) or\\n            is_datetime64_any_dtype(dtype) or\\n            is_timedelta64_dtype(dtype)):\\n        if is_period_dtype(values) or is_period_dtype(dtype):\\n            from pandas import PeriodIndex\\n            values = PeriodIndex(values)\\n            dtype = values.dtype\\n        elif is_timedelta64_dtype(values) or is_timedelta64_dtype(dtype):\\n            from pandas import TimedeltaIndex\\n            values = TimedeltaIndex(values)\\n            dtype = values.dtype\\n        else:\\n            from pandas import DatetimeIndex\\n            values = DatetimeIndex(values)\\n            dtype = values.dtype\\n        return values.asi8, dtype, 'int64'\\n    elif is_categorical_dtype(values) or is_categorical_dtype(dtype):\\n        values = getattr(values, 'values', values)\\n        values = values.codes\\n        dtype = 'category'\\n        values = _ensure_int64(values)\\n        return values, dtype, 'int64'\\n    values = np.asarray(values)\\n    try:\\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\\n            values = values.astype('uint64')\\n            dtype = 'bool'\\n            ndtype = 'uint64'\\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\\n            values = _ensure_int64(values)\\n            ndtype = dtype = 'int64'\\n        elif (is_unsigned_integer_dtype(values) or\\n              is_unsigned_integer_dtype(dtype)):\\n            values = _ensure_uint64(values)\\n            ndtype = dtype = 'uint64'\\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\\n            with catch_warnings(record=True):\\n                values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        elif is_float_dtype(values) or is_float_dtype(dtype):\\n            values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        else:\\n            values = _ensure_object(values)\\n            ndtype = dtype = 'object'\\n    except (TypeError, ValueError):\\n        values = _ensure_object(values)\\n        ndtype = dtype = 'object'\\n    return values, dtype, ndtype",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ensure_data(values, dtype=None):\\n    if (needs_i8_conversion(values) or\\n            is_period_dtype(dtype) or\\n            is_datetime64_any_dtype(dtype) or\\n            is_timedelta64_dtype(dtype)):\\n        if is_period_dtype(values) or is_period_dtype(dtype):\\n            from pandas import PeriodIndex\\n            values = PeriodIndex(values)\\n            dtype = values.dtype\\n        elif is_timedelta64_dtype(values) or is_timedelta64_dtype(dtype):\\n            from pandas import TimedeltaIndex\\n            values = TimedeltaIndex(values)\\n            dtype = values.dtype\\n        else:\\n            from pandas import DatetimeIndex\\n            values = DatetimeIndex(values)\\n            dtype = values.dtype\\n        return values.asi8, dtype, 'int64'\\n    elif is_categorical_dtype(values) or is_categorical_dtype(dtype):\\n        values = getattr(values, 'values', values)\\n        values = values.codes\\n        dtype = 'category'\\n        values = _ensure_int64(values)\\n        return values, dtype, 'int64'\\n    values = np.asarray(values)\\n    try:\\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\\n            values = values.astype('uint64')\\n            dtype = 'bool'\\n            ndtype = 'uint64'\\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\\n            values = _ensure_int64(values)\\n            ndtype = dtype = 'int64'\\n        elif (is_unsigned_integer_dtype(values) or\\n              is_unsigned_integer_dtype(dtype)):\\n            values = _ensure_uint64(values)\\n            ndtype = dtype = 'uint64'\\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\\n            with catch_warnings(record=True):\\n                values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        elif is_float_dtype(values) or is_float_dtype(dtype):\\n            values = _ensure_float64(values)\\n            ndtype = dtype = 'float64'\\n        else:\\n            values = _ensure_object(values)\\n            ndtype = dtype = 'object'\\n    except (TypeError, ValueError):\\n        values = _ensure_object(values)\\n        ndtype = dtype = 'object'\\n    return values, dtype, ndtype"
  },
  {
    "code": "def __getitem__(self, key):\\n        key = self._get_canonical_key(key)\\n        if key not in self:\\n            raise ValueError(\\n                '{key} is not a valid pandas plotting option'.format(key=key))\\n        return super(_Options, self).__getitem__(key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, key):\\n        key = self._get_canonical_key(key)\\n        if key not in self:\\n            raise ValueError(\\n                '{key} is not a valid pandas plotting option'.format(key=key))\\n        return super(_Options, self).__getitem__(key)"
  },
  {
    "code": "def _logical_method(self, other, op):\\n        assert op.__name__ in {\"or_\", \"ror_\", \"and_\", \"rand_\", \"xor\", \"rxor\"}\\n        other_is_scalar = lib.is_scalar(other)\\n        mask = None\\n        if isinstance(other, BooleanArray):\\n            other, mask = other._data, other._mask\\n        elif is_list_like(other):\\n            other = np.asarray(other, dtype=\"bool\")\\n            if other.ndim > 1:\\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\\n            other, mask = coerce_to_array(other, copy=False)\\n        elif isinstance(other, np.bool_):\\n            other = other.item()\\n        if other_is_scalar and other is not libmissing.NA and not lib.is_bool(other):\\n            raise TypeError(\\n                \"'other' should be pandas.NA or a bool. \"\\n                f\"Got {type(other).__name__} instead.\"\\n            )\\n        if not other_is_scalar and len(self) != len(other):\\n            raise ValueError(\"Lengths must match\")\\n        if op.__name__ in {\"or_\", \"ror_\"}:\\n            result, mask = ops.kleene_or(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"and_\", \"rand_\"}:\\n            result, mask = ops.kleene_and(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"xor\", \"rxor\"}:\\n            result, mask = ops.kleene_xor(self._data, other, self._mask, mask)\\n        return BooleanArray(result, mask)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _logical_method(self, other, op):\\n        assert op.__name__ in {\"or_\", \"ror_\", \"and_\", \"rand_\", \"xor\", \"rxor\"}\\n        other_is_scalar = lib.is_scalar(other)\\n        mask = None\\n        if isinstance(other, BooleanArray):\\n            other, mask = other._data, other._mask\\n        elif is_list_like(other):\\n            other = np.asarray(other, dtype=\"bool\")\\n            if other.ndim > 1:\\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\\n            other, mask = coerce_to_array(other, copy=False)\\n        elif isinstance(other, np.bool_):\\n            other = other.item()\\n        if other_is_scalar and other is not libmissing.NA and not lib.is_bool(other):\\n            raise TypeError(\\n                \"'other' should be pandas.NA or a bool. \"\\n                f\"Got {type(other).__name__} instead.\"\\n            )\\n        if not other_is_scalar and len(self) != len(other):\\n            raise ValueError(\"Lengths must match\")\\n        if op.__name__ in {\"or_\", \"ror_\"}:\\n            result, mask = ops.kleene_or(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"and_\", \"rand_\"}:\\n            result, mask = ops.kleene_and(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"xor\", \"rxor\"}:\\n            result, mask = ops.kleene_xor(self._data, other, self._mask, mask)\\n        return BooleanArray(result, mask)"
  },
  {
    "code": "def changes(self, graph, trim_to_apps=None, convert_apps=None):\\n        changes = self._detect_changes(convert_apps)\\n        changes = self.arrange_for_graph(changes, graph)\\n        if trim_to_apps:\\n            changes = self._trim_to_apps(changes, trim_to_apps)\\n        return changes",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def changes(self, graph, trim_to_apps=None, convert_apps=None):\\n        changes = self._detect_changes(convert_apps)\\n        changes = self.arrange_for_graph(changes, graph)\\n        if trim_to_apps:\\n            changes = self._trim_to_apps(changes, trim_to_apps)\\n        return changes"
  },
  {
    "code": "def _process_data(self, frame, type):\\n        frame.columns = ['Strike', 'Symbol', 'Last', 'Bid', 'Ask', 'Chg', 'PctChg', 'Vol', 'Open_Int', 'IV']\\n        frame[\"Rootexp\"] = frame.Symbol.str[0:-9]\\n        frame[\"Root\"] = frame.Rootexp.str[0:-6]\\n        frame[\"Expiry\"] = to_datetime(frame.Rootexp.str[-6:])\\n        frame[\"IsNonstandard\"] = frame['Root'] != self.symbol.replace('-', '')\\n        del frame[\"Rootexp\"]\\n        frame[\"Underlying\"] = self.symbol\\n        try:\\n            frame['Underlying_Price'] = self.underlying_price\\n            frame[\"Quote_Time\"] = self.quote_time\\n        except AttributeError:\\n            frame['Underlying_Price'] = np.nan\\n            frame[\"Quote_Time\"] = np.nan\\n        frame.rename(columns={'Open Int': 'Open_Int'}, inplace=True)\\n        frame['Type'] = type\\n        frame.set_index(['Strike', 'Expiry', 'Type', 'Symbol'], inplace=True)\\n        return frame",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_data(self, frame, type):\\n        frame.columns = ['Strike', 'Symbol', 'Last', 'Bid', 'Ask', 'Chg', 'PctChg', 'Vol', 'Open_Int', 'IV']\\n        frame[\"Rootexp\"] = frame.Symbol.str[0:-9]\\n        frame[\"Root\"] = frame.Rootexp.str[0:-6]\\n        frame[\"Expiry\"] = to_datetime(frame.Rootexp.str[-6:])\\n        frame[\"IsNonstandard\"] = frame['Root'] != self.symbol.replace('-', '')\\n        del frame[\"Rootexp\"]\\n        frame[\"Underlying\"] = self.symbol\\n        try:\\n            frame['Underlying_Price'] = self.underlying_price\\n            frame[\"Quote_Time\"] = self.quote_time\\n        except AttributeError:\\n            frame['Underlying_Price'] = np.nan\\n            frame[\"Quote_Time\"] = np.nan\\n        frame.rename(columns={'Open Int': 'Open_Int'}, inplace=True)\\n        frame['Type'] = type\\n        frame.set_index(['Strike', 'Expiry', 'Type', 'Symbol'], inplace=True)\\n        return frame"
  },
  {
    "code": "def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\\n        key = self.make_key(key, version=version)\\n        self.validate_key(key)\\n        pickled = pickle.dumps(value, pickle.HIGHEST_PROTOCOL)\\n        with self._lock.writer():\\n            self._set(key, pickled, timeout)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\\n        key = self.make_key(key, version=version)\\n        self.validate_key(key)\\n        pickled = pickle.dumps(value, pickle.HIGHEST_PROTOCOL)\\n        with self._lock.writer():\\n            self._set(key, pickled, timeout)"
  },
  {
    "code": "def ewmcov(seriesA, seriesB, com, minCom=0, correctBias=True):\\n    if correctBias:\\n        bias_adj = ( 1.0 + 2.0 * com ) / (2.0 * com)\\n    else:\\n        bias_adj = 1.0\\n    if not isinstance(seriesB, type(seriesA)):\\n        raise Exception('Input arrays must be of the same type!')\\n    if isinstance(seriesA, Series):\\n        if seriesA.index is not seriesB.index:\\n            commonIndex = seriesA.index.intersection(seriesB.index)\\n            seriesA = seriesA.reindex(commonIndex)\\n            seriesB = seriesB.reindex(commonIndex)\\n    okLocs = notnull(seriesA) & notnull(seriesB)\\n    cleanSeriesA = seriesA[okLocs]\\n    cleanSeriesB = seriesB.reindex(cleanSeriesA.index)\\n    XY = ewma(cleanSeriesA * cleanSeriesB, com=com, minCom=minCom)\\n    X  = ewma(cleanSeriesA, com=com, minCom=minCom)\\n    Y  = ewma(cleanSeriesB, com=com, minCom=minCom)\\n    return bias_adj * (XY - X * Y)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "refactoring EW moments to have uniform interface, unit test coverage",
    "fixed_code": "def ewmcov(arg1, arg2, com=None, span=None, min_periods=0, bias=False):\\n    X, Y = _prep_binary(arg1, arg2)\\n    mean = lambda x: ewma(x, com=com, span=span, min_periods=min_periods)\\n    result = (mean(X*Y) - mean(X) * mean(Y))\\n    if not bias:\\n        result *= (1.0 + 2.0 * com) / (2.0 * com)\\n    return result\\newmcov.__doc__ = _ewm_doc % (\"Moving exponentially-weighted moving covariance\",\\n                             _binary_arg, \"\")"
  },
  {
    "code": "def append(self, key, value):\\n        self._write_to_group(key, value, table=True, append=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def append(self, key, value):\\n        self._write_to_group(key, value, table=True, append=True)"
  },
  {
    "code": "def from_product(cls, iterables, sortorder=None, names=_no_default_names):\\n        from pandas.core.reshape.util import cartesian_product\\n        if not is_list_like(iterables):\\n            raise TypeError(\"Input must be a list / sequence of iterables.\")\\n        elif is_iterator(iterables):\\n            iterables = list(iterables)\\n        codes, levels = _factorize_from_iterables(iterables)\\n        if names is _no_default_names:\\n            names = [getattr(it, \"name\", None) for it in iterables]\\n        codes = cartesian_product(codes)\\n        return MultiIndex(levels, codes, sortorder=sortorder, names=names)\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def from_product(cls, iterables, sortorder=None, names=_no_default_names):\\n        from pandas.core.reshape.util import cartesian_product\\n        if not is_list_like(iterables):\\n            raise TypeError(\"Input must be a list / sequence of iterables.\")\\n        elif is_iterator(iterables):\\n            iterables = list(iterables)\\n        codes, levels = _factorize_from_iterables(iterables)\\n        if names is _no_default_names:\\n            names = [getattr(it, \"name\", None) for it in iterables]\\n        codes = cartesian_product(codes)\\n        return MultiIndex(levels, codes, sortorder=sortorder, names=names)\\n    @classmethod"
  },
  {
    "code": "def __iter__(self):\\n        return itertools.chain((DEFAULTSECT,), self._sections.keys())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "configparser: the name of the DEFAULT section is now customizable",
    "fixed_code": "def __iter__(self):\\n        return itertools.chain((self._default_section,), self._sections.keys())"
  },
  {
    "code": "def gateway_in_subnet_exists(client, subnet_id, allocation_id=None,\\n                             check_mode=False):\\n    allocation_id_exists = False\\n    gateways = []\\n    states = ['available', 'pending']\\n    gws_retrieved, err_msg, gws = (\\n        get_nat_gateways(\\n            client, subnet_id, states=states, check_mode=check_mode\\n        )\\n    )\\n    if not gws_retrieved:\\n        return gateways, allocation_id_exists\\n    for gw in gws:\\n        for address in gw['nat_gateway_addresses']:\\n            if allocation_id:\\n                if address.get('allocation_id') == allocation_id:\\n                    allocation_id_exists = True\\n                    gateways.append(gw)\\n            else:\\n                gateways.append(gw)\\n    return gateways, allocation_id_exists",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def gateway_in_subnet_exists(client, subnet_id, allocation_id=None,\\n                             check_mode=False):\\n    allocation_id_exists = False\\n    gateways = []\\n    states = ['available', 'pending']\\n    gws_retrieved, err_msg, gws = (\\n        get_nat_gateways(\\n            client, subnet_id, states=states, check_mode=check_mode\\n        )\\n    )\\n    if not gws_retrieved:\\n        return gateways, allocation_id_exists\\n    for gw in gws:\\n        for address in gw['nat_gateway_addresses']:\\n            if allocation_id:\\n                if address.get('allocation_id') == allocation_id:\\n                    allocation_id_exists = True\\n                    gateways.append(gw)\\n            else:\\n                gateways.append(gw)\\n    return gateways, allocation_id_exists"
  },
  {
    "code": "def _fix_polygon(self, poly):\\n        if poly.empty:\\n            return poly\\n        if not poly.exterior_ring.is_counterclockwise:\\n            poly.exterior_ring = list(reversed(poly.exterior_ring))\\n        for i in range(1, len(poly)):\\n            if poly[i].is_counterclockwise:\\n                poly[i] = list(reversed(poly[i]))\\n        return poly",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fix_polygon(self, poly):\\n        if poly.empty:\\n            return poly\\n        if not poly.exterior_ring.is_counterclockwise:\\n            poly.exterior_ring = list(reversed(poly.exterior_ring))\\n        for i in range(1, len(poly)):\\n            if poly[i].is_counterclockwise:\\n                poly[i] = list(reversed(poly[i]))\\n        return poly"
  },
  {
    "code": "def _binary_clf_curve(y_true, y_score, pos_label=None):\\n    y_true, y_score = check_arrays(y_true, y_score)\\n    if not (y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1)):\\n        raise ValueError(\"Bad y_true input shape\")\\n    if not (y_score.ndim == 1 or\\n            (y_score.ndim == 2 and y_score.shape[1] == 1)):\\n        raise ValueError(\"Bad y_score input shape\")\\n    y_true = np.squeeze(y_true)\\n    y_score = np.squeeze(y_score)\\n    classes = np.unique(y_true)\\n    if (pos_label is None and\\n        not (np.all(classes == [0, 1]) or\\n             np.all(classes == [-1, 1]) or\\n             np.all(classes == [0]) or\\n             np.all(classes == [-1]) or\\n             np.all(classes == [1]))):\\n        raise ValueError(\"Data is not binary and pos_label is not specified\")\\n    elif pos_label is None:\\n        pos_label = 1.\\n    y_true = (y_true == pos_label)\\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\\n    y_score = y_score[desc_score_indices]\\n    y_true = y_true[desc_score_indices]\\n    distinct_value_indices = np.where(np.diff(y_score))[0]\\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\\n    tps = y_true.cumsum()[threshold_idxs]\\n    fps = 1 + threshold_idxs - tps\\n    return fps, tps, y_score[threshold_idxs]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH handle properly row vector",
    "fixed_code": "def _binary_clf_curve(y_true, y_score, pos_label=None):\\n    y_true, y_score = check_arrays(y_true, y_score)\\n    if (not (y_true.ndim == 1 or\\n            (y_true.ndim == 2 and y_true.shape[1] == 1)) or\\n            not (y_score.ndim == 1 or\\n                (y_score.ndim == 2 and y_score.shape[1] == 1))):\\n        raise ValueError(\"Bad input shape\")\\n    y_true = np.squeeze(y_true)\\n    y_score = np.squeeze(y_score)\\n    classes = np.unique(y_true)\\n    if (pos_label is None and\\n        not (np.all(classes == [0, 1]) or\\n             np.all(classes == [-1, 1]) or\\n             np.all(classes == [0]) or\\n             np.all(classes == [-1]) or\\n             np.all(classes == [1]))):\\n        raise ValueError(\"Data is not binary and pos_label is not specified\")\\n    elif pos_label is None:\\n        pos_label = 1.\\n    y_true = (y_true == pos_label)\\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\\n    y_score = y_score[desc_score_indices]\\n    y_true = y_true[desc_score_indices]\\n    distinct_value_indices = np.where(np.diff(y_score))[0]\\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\\n    tps = y_true.cumsum()[threshold_idxs]\\n    fps = 1 + threshold_idxs - tps\\n    return fps, tps, y_score[threshold_idxs]"
  },
  {
    "code": "def _convert_index(index, encoding=None, format_type=None):\\n    index_name = getattr(index, 'name', None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif isinstance(index, TimedeltaIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'timedelta64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(\\n            index.values, 'integer', atom, freq=getattr(index, 'freq', None),\\n            index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise TypeError('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif inferred_type == 'timedelta64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'timedelta64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.asarray([(time.mktime(v.timetuple()) +\\n                                 v.microsecond / 1E6) for v in values],\\n                               dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.asarray([v.toordinal() for v in values],\\n                               dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(\\n            converted, 'string', _tables().StringCol(itemsize),\\n            itemsize=itemsize, index_name=index_name\\n        )\\n    elif inferred_type == 'unicode':\\n        if format_type == 'fixed':\\n            atom = _tables().ObjectAtom()\\n            return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                            index_name=index_name)\\n        raise TypeError(\\n            \"[unicode] is not supported as a in index type for [{0}] formats\"\\n            .format(format_type)\\n        )\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_index(index, encoding=None, format_type=None):\\n    index_name = getattr(index, 'name', None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif isinstance(index, TimedeltaIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'timedelta64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(\\n            index.values, 'integer', atom, freq=getattr(index, 'freq', None),\\n            index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise TypeError('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif inferred_type == 'timedelta64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'timedelta64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.asarray([(time.mktime(v.timetuple()) +\\n                                 v.microsecond / 1E6) for v in values],\\n                               dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.asarray([v.toordinal() for v in values],\\n                               dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(\\n            converted, 'string', _tables().StringCol(itemsize),\\n            itemsize=itemsize, index_name=index_name\\n        )\\n    elif inferred_type == 'unicode':\\n        if format_type == 'fixed':\\n            atom = _tables().ObjectAtom()\\n            return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                            index_name=index_name)\\n        raise TypeError(\\n            \"[unicode] is not supported as a in index type for [{0}] formats\"\\n            .format(format_type)\\n        )\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)"
  },
  {
    "code": "def mask_zero_div_zero(x, y, result):\\n\\tif not isinstance(result, np.ndarray):\\n\\t\\treturn result\\n\\tif is_scalar(y):\\n\\t\\ty = np.array(y)\\n\\tzmask = y == 0\\n\\tif isinstance(zmask, bool):\\n\\t\\treturn result\\n\\tif zmask.any():\\n\\t\\tshape = result.shape\\n\\t\\tzneg_mask = zmask & np.signbit(y)\\n\\t\\tzpos_mask = zmask & ~zneg_mask\\n\\t\\tnan_mask = (zmask & (x == 0)).ravel()\\n\\t\\twith np.errstate(invalid=\"ignore\"):\\n\\t\\t\\tneginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()\\n\\t\\t\\tposinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()\\n\\t\\tif nan_mask.any() or neginf_mask.any() or posinf_mask.any():\\n\\t\\t\\tresult = result.astype(\"float64\", copy=False).ravel()\\n\\t\\t\\tnp.putmask(result, nan_mask, np.nan)\\n\\t\\t\\tnp.putmask(result, posinf_mask, np.inf)\\n\\t\\t\\tnp.putmask(result, neginf_mask, -np.inf)\\n\\t\\t\\tresult = result.reshape(shape)\\n\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DataFrame.floordiv(ser, axis=0) not matching column-wise bheavior (#31271)",
    "fixed_code": "def mask_zero_div_zero(x, y, result):\\n\\tif not isinstance(result, np.ndarray):\\n\\t\\treturn result\\n\\tif is_scalar(y):\\n\\t\\ty = np.array(y)\\n\\tzmask = y == 0\\n\\tif isinstance(zmask, bool):\\n\\t\\treturn result\\n\\tif zmask.any():\\n\\t\\tzneg_mask = zmask & np.signbit(y)\\n\\t\\tzpos_mask = zmask & ~zneg_mask\\n\\t\\tnan_mask = zmask & (x == 0)\\n\\t\\twith np.errstate(invalid=\"ignore\"):\\n\\t\\t\\tneginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))\\n\\t\\t\\tposinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))\\n\\t\\tif nan_mask.any() or neginf_mask.any() or posinf_mask.any():\\n\\t\\t\\tresult = result.astype(\"float64\", copy=False)\\n\\t\\t\\tresult[nan_mask] = np.nan\\n\\t\\t\\tresult[posinf_mask] = np.inf\\n\\t\\t\\tresult[neginf_mask] = -np.inf\\n\\treturn result"
  },
  {
    "code": "def _is_role(self, path):\\n\\t\\t''\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tb_upath = to_bytes(unfrackpath(path, follow=False), errors='surrogate_or_strict')\\n\\t\\tfor b_finddir in (b'meta', b'tasks'):\\n\\t\\t\\tfor b_suffix in (b'.yml', b'.yaml', b''):\\n\\t\\t\\t\\tb_main = b'main%s' % (b_suffix)\\n\\t\\t\\t\\tb_tasked = os.path.join(b_finddir, b_main)\\n\\t\\t\\t\\tif (\\n\\t\\t\\t\\t\\tRE_TASKS.search(path) and\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_path, b_main)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_upath, b_tasked)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(os.path.dirname(b_path), b_tasked))\\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _is_role(self, path):\\n\\t\\t''\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tb_upath = to_bytes(unfrackpath(path, follow=False), errors='surrogate_or_strict')\\n\\t\\tfor b_finddir in (b'meta', b'tasks'):\\n\\t\\t\\tfor b_suffix in (b'.yml', b'.yaml', b''):\\n\\t\\t\\t\\tb_main = b'main%s' % (b_suffix)\\n\\t\\t\\t\\tb_tasked = os.path.join(b_finddir, b_main)\\n\\t\\t\\t\\tif (\\n\\t\\t\\t\\t\\tRE_TASKS.search(path) and\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_path, b_main)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_upath, b_tasked)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(os.path.dirname(b_path), b_tasked))\\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False"
  },
  {
    "code": "def decode(self, input, errors='strict'):\\n        raise NotImplementedError\\nclass IncrementalEncoder(object):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decode(self, input, errors='strict'):\\n        raise NotImplementedError\\nclass IncrementalEncoder(object):"
  },
  {
    "code": "def _get_converter(kind):\\n    if kind == 'datetime64':\\n        return lambda x: np.array(x, dtype='M8[ns]')\\n    if kind == 'datetime':\\n        return lib.convert_timestamps\\n    else:  \\n        raise ValueError('invalid kind %s' % kind)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: provide py3k string decoding and compat",
    "fixed_code": "def _get_converter(kind, encoding):\\n    if kind == 'datetime64':\\n        return lambda x: np.array(x, dtype='M8[ns]')\\n    elif kind == 'datetime':\\n        return lib.convert_timestamps\\n    elif kind == 'string':\\n        return lambda x: _unconvert_string_array(x,encoding=encoding)\\n    else:  \\n        raise ValueError('invalid kind %s' % kind)"
  },
  {
    "code": "def create_empty_table(self,\\n                           project_id,\\n                           dataset_id,\\n                           table_id,\\n                           schema_fields=None,\\n                           time_partitioning=None,\\n                           cluster_fields=None,\\n                           labels=None,\\n                           view=None,\\n                           num_retries=None):\\n        project_id = project_id if project_id is not None else self.project_id\\n        table_resource = {\\n            'tableReference': {\\n                'tableId': table_id\\n            }\\n        }\\n        if schema_fields:\\n            table_resource['schema'] = {'fields': schema_fields}\\n        if time_partitioning:\\n            table_resource['timePartitioning'] = time_partitioning\\n        if cluster_fields:\\n            table_resource['clustering'] = {\\n                'fields': cluster_fields\\n            }\\n        if labels:\\n            table_resource['labels'] = labels\\n        if view:\\n            table_resource['view'] = view\\n        num_retries = num_retries if num_retries else self.num_retries\\n        self.log.info('Creating Table %s:%s.%s',\\n                      project_id, dataset_id, table_id)\\n        try:\\n            self.service.tables().insert(\\n                projectId=project_id,\\n                datasetId=dataset_id,\\n                body=table_resource).execute(num_retries=num_retries)\\n            self.log.info('Table created successfully: %s:%s.%s',\\n                          project_id, dataset_id, table_id)\\n        except HttpError as err:\\n            raise AirflowException(\\n                'BigQuery job failed. Error was: {}'.format(err.content)\\n            )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_empty_table(self,\\n                           project_id,\\n                           dataset_id,\\n                           table_id,\\n                           schema_fields=None,\\n                           time_partitioning=None,\\n                           cluster_fields=None,\\n                           labels=None,\\n                           view=None,\\n                           num_retries=None):\\n        project_id = project_id if project_id is not None else self.project_id\\n        table_resource = {\\n            'tableReference': {\\n                'tableId': table_id\\n            }\\n        }\\n        if schema_fields:\\n            table_resource['schema'] = {'fields': schema_fields}\\n        if time_partitioning:\\n            table_resource['timePartitioning'] = time_partitioning\\n        if cluster_fields:\\n            table_resource['clustering'] = {\\n                'fields': cluster_fields\\n            }\\n        if labels:\\n            table_resource['labels'] = labels\\n        if view:\\n            table_resource['view'] = view\\n        num_retries = num_retries if num_retries else self.num_retries\\n        self.log.info('Creating Table %s:%s.%s',\\n                      project_id, dataset_id, table_id)\\n        try:\\n            self.service.tables().insert(\\n                projectId=project_id,\\n                datasetId=dataset_id,\\n                body=table_resource).execute(num_retries=num_retries)\\n            self.log.info('Table created successfully: %s:%s.%s',\\n                          project_id, dataset_id, table_id)\\n        except HttpError as err:\\n            raise AirflowException(\\n                'BigQuery job failed. Error was: {}'.format(err.content)\\n            )"
  },
  {
    "code": "def _get_vif_infos():\\n\\t\\t\\tvif_infos = []\\n\\t\\t\\tif network_info is None:\\n\\t\\t\\t\\treturn vif_infos\\n\\t\\t\\tfor vif in network_info:\\n\\t\\t\\t\\tmac_address = vif['address']\\n\\t\\t\\t\\tnetwork_name = vif['network']['bridge'] or \\\\n\\t\\t\\t\\t\\t\\t\\t   CONF.vmware.integration_bridge\\n\\t\\t\\t\\tif vif['network'].get_meta('should_create_vlan', False):\\n\\t\\t\\t\\t\\tnetwork_ref = vmwarevif.ensure_vlan_bridge(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself._session, vif,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself._cluster)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnetwork_ref = vmwarevif.ensure_vlan_bridge(\\n\\t\\t\\t\\t\\t\\tself._session, vif, self._cluster, create_vlan=False)\\n\\t\\t\\t\\tvif_infos.append({'network_name': network_name,\\n\\t\\t\\t\\t\\t\\t\\t\\t  'mac_address': mac_address,\\n\\t\\t\\t\\t\\t\\t\\t\\t  'network_ref': network_ref,\\n\\t\\t\\t\\t\\t\\t\\t\\t  'iface_id': vif['id'],\\n\\t\\t\\t\\t\\t\\t\\t\\t })\\n\\t\\t\\treturn vif_infos\\n\\t\\tvif_infos = _get_vif_infos()\\n\\t\\tconfig_spec = vm_util.get_vm_create_spec(\\n\\t\\t\\t\\t\\t\\t\\tclient_factory, instance,\\n\\t\\t\\t\\t\\t\\t\\tdata_store_name, vif_infos, os_type)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_vif_infos():\\n\\t\\t\\tvif_infos = []\\n\\t\\t\\tif network_info is None:\\n\\t\\t\\t\\treturn vif_infos\\n\\t\\t\\tfor vif in network_info:\\n\\t\\t\\t\\tmac_address = vif['address']\\n\\t\\t\\t\\tnetwork_name = vif['network']['bridge'] or \\\\n\\t\\t\\t\\t\\t\\t\\t   CONF.vmware.integration_bridge\\n\\t\\t\\t\\tif vif['network'].get_meta('should_create_vlan', False):\\n\\t\\t\\t\\t\\tnetwork_ref = vmwarevif.ensure_vlan_bridge(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself._session, vif,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself._cluster)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnetwork_ref = vmwarevif.ensure_vlan_bridge(\\n\\t\\t\\t\\t\\t\\tself._session, vif, self._cluster, create_vlan=False)\\n\\t\\t\\t\\tvif_infos.append({'network_name': network_name,\\n\\t\\t\\t\\t\\t\\t\\t\\t  'mac_address': mac_address,\\n\\t\\t\\t\\t\\t\\t\\t\\t  'network_ref': network_ref,\\n\\t\\t\\t\\t\\t\\t\\t\\t  'iface_id': vif['id'],\\n\\t\\t\\t\\t\\t\\t\\t\\t })\\n\\t\\t\\treturn vif_infos\\n\\t\\tvif_infos = _get_vif_infos()\\n\\t\\tconfig_spec = vm_util.get_vm_create_spec(\\n\\t\\t\\t\\t\\t\\t\\tclient_factory, instance,\\n\\t\\t\\t\\t\\t\\t\\tdata_store_name, vif_infos, os_type)"
  },
  {
    "code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            model_list = self._get_models_cache[cache_key]\\n            if self.available_apps is not None and only_installed:\\n                model_list = [\\n                    m for m in model_list\\n                    if self.app_configs[m._meta.app_label].name in self.available_apps\\n                ]\\n            return model_list\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        if self.available_apps is not None and only_installed:\\n            model_list = [\\n                m for m in model_list\\n                if self.app_configs[m._meta.app_label].name in self.available_apps\\n            ]\\n        return model_list\\n    def get_model(self, app_label, model_name, only_installed=True):\\n        if not self.master:\\n            only_installed = False\\n        self.populate_models()\\n        if only_installed:\\n            app_config = self.app_configs.get(app_label)\\n            if app_config is None:\\n                return None\\n            if (self.available_apps is not None\\n                    and app_config.name not in self.available_apps):\\n                raise UnavailableApp(\"App with label %s isn't available.\" % app_label)\\n        return self.all_models[app_label].get(model_name.lower())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactored INSTALLED_APPS overrides.\\n\\n  INSTALLED_APPS setting.\\n  with [un]set_installed_apps.\\n  this situation with public methods of the new app cache.",
    "fixed_code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            return self._get_models_cache[cache_key]\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        return model_list\\n    def get_model(self, app_label, model_name, only_installed=True):\\n        if not self.master:\\n            only_installed = False\\n        self.populate_models()\\n        if only_installed:\\n            app_config = self.app_configs.get(app_label)\\n            if app_config is None:\\n                return None\\n        return self.all_models[app_label].get(model_name.lower())"
  },
  {
    "code": "def find_hook(hook_name, hooks_dir='hooks'):\\n\\tlogger.debug('hooks_dir is %s', os.path.abspath(hooks_dir))\\n\\tif not os.path.isdir(hooks_dir):\\n\\t\\tlogger.debug('No hooks/dir in template_dir')\\n\\t\\treturn None\\n\\tscripts = []\\n\\tfor hook_file in os.listdir(hooks_dir):\\n\\t\\tif valid_hook(hook_file, hook_name):\\n\\t\\t\\tscripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\\n\\tif len(scripts) == 0:\\n\\t\\treturn None\\n\\treturn scripts",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def find_hook(hook_name, hooks_dir='hooks'):\\n\\tlogger.debug('hooks_dir is %s', os.path.abspath(hooks_dir))\\n\\tif not os.path.isdir(hooks_dir):\\n\\t\\tlogger.debug('No hooks/dir in template_dir')\\n\\t\\treturn None\\n\\tscripts = []\\n\\tfor hook_file in os.listdir(hooks_dir):\\n\\t\\tif valid_hook(hook_file, hook_name):\\n\\t\\t\\tscripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\\n\\tif len(scripts) == 0:\\n\\t\\treturn None\\n\\treturn scripts"
  },
  {
    "code": "def delete_many(self, keys, version=None):\\n        key_list = []\\n        for key in keys:\\n            self.validate_key(key)\\n            key_list.append(self.make_key(key, version))\\n        self._base_delete_many(key_list)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #33060 -- Ensured cache backends validate keys.\\n\\nThe validate_key() function should be called after make_key() to ensure\\nthat the validation is performed on the key that will actually be\\nstored in the cache.",
    "fixed_code": "def delete_many(self, keys, version=None):\\n        key_list = []\\n        for key in keys:\\n            key = self.make_key(key, version)\\n            self.validate_key(key)\\n            key_list.append(key)\\n        self._base_delete_many(key_list)"
  },
  {
    "code": "def _simple_blockify(dct, ref_columns, dtype):\\n    block_columns, values = _stack_dict(dct)\\n    if values.dtype != dtype:\\n        values = values.astype(dtype)\\n    locs, mask = ref_columns.get_indexer(block_columns)\\n    assert(mask.all())\\n    return make_block(values, locs, ref_columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def _simple_blockify(dct, ref_columns, dtype):\\n    block_columns, values = _stack_dict(dct)\\n    if values.dtype != dtype:\\n        values = values.astype(dtype)\\n    return make_block(values, block_columns, ref_columns)"
  },
  {
    "code": "def infer_dtype_from_array(arr, pandas_dtype: bool = False):\\n    if isinstance(arr, np.ndarray):\\n        return arr.dtype, arr\\n    if not is_list_like(arr):\\n        arr = [arr]\\n    if pandas_dtype and is_extension_array_dtype(arr):\\n        return arr.dtype, arr\\n    elif isinstance(arr, ABCSeries):\\n        return arr.dtype, np.asarray(arr)\\n    inferred = lib.infer_dtype(arr, skipna=False)\\n    if inferred in [\"string\", \"bytes\", \"unicode\", \"mixed\", \"mixed-integer\"]:\\n        return (np.object_, arr)\\n    arr = np.asarray(arr)\\n    return arr.dtype, arr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: remove checks for inferred_dtype==unicode (#31020)",
    "fixed_code": "def infer_dtype_from_array(arr, pandas_dtype: bool = False):\\n    if isinstance(arr, np.ndarray):\\n        return arr.dtype, arr\\n    if not is_list_like(arr):\\n        arr = [arr]\\n    if pandas_dtype and is_extension_array_dtype(arr):\\n        return arr.dtype, arr\\n    elif isinstance(arr, ABCSeries):\\n        return arr.dtype, np.asarray(arr)\\n    inferred = lib.infer_dtype(arr, skipna=False)\\n    if inferred in [\"string\", \"bytes\", \"mixed\", \"mixed-integer\"]:\\n        return (np.object_, arr)\\n    arr = np.asarray(arr)\\n    return arr.dtype, arr"
  },
  {
    "code": "def gettext(message):\\n    result = TECHNICAL_ID_MAP.get(message, message)\\n    if isinstance(message, SafeData):\\n        return mark_safe(result)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def gettext(message):\\n    result = TECHNICAL_ID_MAP.get(message, message)\\n    if isinstance(message, SafeData):\\n        return mark_safe(result)\\n    return result"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        feature=dict(type='str', required=True),\\n        state=dict(choices=['enabled', 'disabled'], default='enabled')\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\\n    warnings = list()\\n    results = dict(changed=False, warnings=warnings)\\n    feature = validate_feature(module)\\n    state = module.params['state'].lower()\\n    available_features = get_available_features(feature, module)\\n    if feature not in available_features:\\n        module.fail_json(\\n            msg='Invalid feature name.',\\n            features_currently_supported=available_features,\\n            invalid_feature=feature)\\n    else:\\n        existstate = available_features[feature]\\n        existing = dict(state=existstate)\\n        proposed = dict(state=state)\\n        results['changed'] = False\\n        cmds = get_commands(proposed, existing, state, module)\\n        if cmds:\\n            cmds.insert(0, 'terminal dont-ask')\\n            if not module.check_mode:\\n                load_config(module, cmds)\\n            results['changed'] = True\\n    results['commands'] = cmds\\n    module.exit_json(**results)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        feature=dict(type='str', required=True),\\n        state=dict(choices=['enabled', 'disabled'], default='enabled')\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\\n    warnings = list()\\n    results = dict(changed=False, warnings=warnings)\\n    feature = validate_feature(module)\\n    state = module.params['state'].lower()\\n    available_features = get_available_features(feature, module)\\n    if feature not in available_features:\\n        module.fail_json(\\n            msg='Invalid feature name.',\\n            features_currently_supported=available_features,\\n            invalid_feature=feature)\\n    else:\\n        existstate = available_features[feature]\\n        existing = dict(state=existstate)\\n        proposed = dict(state=state)\\n        results['changed'] = False\\n        cmds = get_commands(proposed, existing, state, module)\\n        if cmds:\\n            cmds.insert(0, 'terminal dont-ask')\\n            if not module.check_mode:\\n                load_config(module, cmds)\\n            results['changed'] = True\\n    results['commands'] = cmds\\n    module.exit_json(**results)"
  },
  {
    "code": "def inverse_transform(self, X):\\n        if sp.isspmatrix_coo(X):  \\n            X = X.tocsr()\\n        elif not sp.issparse(X):\\n            X = np.asmatrix(X)\\n        n_samples = X.shape[0]\\n        terms = np.array(self.vocabulary.keys())\\n        indices = np.array(self.vocabulary.values())\\n        inverse_vocabulary = terms[np.argsort(indices)]\\n        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\\n                for i in xrange(n_samples)]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Flatten the feature extraction API",
    "fixed_code": "def inverse_transform(self, X):\\n        if sp.isspmatrix_coo(X):  \\n            X = X.tocsr()\\n        elif not sp.issparse(X):\\n            X = np.asmatrix(X)\\n        n_samples = X.shape[0]\\n        terms = np.array(self.vocabulary_.keys())\\n        indices = np.array(self.vocabulary_.values())\\n        inverse_vocabulary = terms[np.argsort(indices)]\\n        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\\n                for i in xrange(n_samples)]"
  },
  {
    "code": "def fit(self, X, y):\\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\\n            raise NotImplementedError('Multilabel and multi-output'\\n                                      ' classification is not supported.')\\n        if self.voting not in ('soft', 'hard'):\\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\\n                             % self.voting)\\n        if self.weights and len(self.weights) != len(self.estimators):\\n            raise ValueError('Number of classifiers and weights must be equal'\\n                             '; got %d weights, %d estimators'\\n                             % (len(self.weights), len(self.estimators)))\\n        self.le_ = LabelEncoder()\\n        self.le_.fit(y)\\n        self.classes_ = self.le_.classes_\\n        self.estimators_ = []\\n        for name, clf in self.estimators:\\n            fitted_clf = clone(clf).fit(X, self.le_.transform(y))\\n            self.estimators_.append(fitted_clf)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, y):\\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\\n            raise NotImplementedError('Multilabel and multi-output'\\n                                      ' classification is not supported.')\\n        if self.voting not in ('soft', 'hard'):\\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\\n                             % self.voting)\\n        if self.weights and len(self.weights) != len(self.estimators):\\n            raise ValueError('Number of classifiers and weights must be equal'\\n                             '; got %d weights, %d estimators'\\n                             % (len(self.weights), len(self.estimators)))\\n        self.le_ = LabelEncoder()\\n        self.le_.fit(y)\\n        self.classes_ = self.le_.classes_\\n        self.estimators_ = []\\n        for name, clf in self.estimators:\\n            fitted_clf = clone(clf).fit(X, self.le_.transform(y))\\n            self.estimators_.append(fitted_clf)\\n        return self"
  },
  {
    "code": "def load(klass):\\n\\t\\tconfig = klass()\\n\\t\\tfor path in klass.CONF_PATHS:\\n\\t\\t\\tif os.path.exists(path):\\n\\t\\t\\t\\twith open(path, 'r') as conf:\\n\\t\\t\\t\\t\\tconfig.configure(yaml.load(conf))\\n\\t\\treturn config",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "use safe load instead of load",
    "fixed_code": "def load(klass):\\n\\t\\tconfig = klass()\\n\\t\\tfor path in klass.CONF_PATHS:\\n\\t\\t\\tif os.path.exists(path):\\n\\t\\t\\t\\twith open(path, 'r') as conf:\\n\\t\\t\\t\\t\\tconfig.configure(yaml.safe_load(conf))\\n\\t\\treturn config"
  },
  {
    "code": "def __rsub__(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        tmp = Decimal(self)\\n        tmp._sign = 1 - tmp._sign\\n        return other.__add__(tmp, context=context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def __rsub__(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        return other.__sub__(self, context=context)"
  },
  {
    "code": "def connect_to_api(module, disconnect_atexit=True, return_si=False):\\n    hostname = module.params['hostname']\\n    username = module.params['username']\\n    password = module.params['password']\\n    port = module.params.get('port', 443)\\n    validate_certs = module.params['validate_certs']\\n    if not hostname:\\n        module.fail_json(msg=\"Hostname parameter is missing.\"\\n                             \" Please specify this parameter in task or\"\\n                             \" export environment variable like 'export VMWARE_HOST=ESXI_HOSTNAME'\")\\n    if not username:\\n        module.fail_json(msg=\"Username parameter is missing.\"\\n                             \" Please specify this parameter in task or\"\\n                             \" export environment variable like 'export VMWARE_USER=ESXI_USERNAME'\")\\n    if not password:\\n        module.fail_json(msg=\"Password parameter is missing.\"\\n                             \" Please specify this parameter in task or\"\\n                             \" export environment variable like 'export VMWARE_PASSWORD=ESXI_PASSWORD'\")\\n    if validate_certs and not hasattr(ssl, 'SSLContext'):\\n        module.fail_json(msg='pyVim does not support changing verification mode with python < 2.7.9. Either update '\\n                             'python or use validate_certs=false.')\\n    elif validate_certs:\\n        ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\\n        ssl_context.verify_mode = ssl.CERT_REQUIRED\\n        ssl_context.check_hostname = True\\n        ssl_context.load_default_certs()\\n    elif hasattr(ssl, 'SSLContext'):\\n        ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\\n        ssl_context.verify_mode = ssl.CERT_NONE\\n        ssl_context.check_hostname = False\\n    else:  \\n        ssl_context = None\\n    service_instance = None\\n    proxy_host = module.params.get('proxy_host')\\n    proxy_port = module.params.get('proxy_port')\\n    connect_args = dict(\\n        host=hostname,\\n        port=port,\\n    )\\n    if ssl_context:\\n        connect_args.update(sslContext=ssl_context)\\n    msg_suffix = ''\\n    try:\\n        if proxy_host:\\n            msg_suffix = \" [proxy: %s:%d]\" % (proxy_host, proxy_port)\\n            connect_args.update(httpProxyHost=proxy_host, httpProxyPort=proxy_port)\\n            smart_stub = connect.SmartStubAdapter(**connect_args)\\n            session_stub = connect.VimSessionOrientedStub(smart_stub, connect.VimSessionOrientedStub.makeUserLoginMethod(username, password))\\n            service_instance = vim.ServiceInstance('ServiceInstance', session_stub)\\n        else:\\n            connect_args.update(user=username, pwd=password)\\n            service_instance = connect.SmartConnect(**connect_args)\\n    except vim.fault.InvalidLogin as invalid_login:\\n        msg = \"Unable to log on to vCenter or ESXi API at %s:%s \" % (hostname, port)\\n        module.fail_json(msg=\"%s as %s: %s\" % (msg, username, invalid_login.msg) + msg_suffix)\\n    except vim.fault.NoPermission as no_permission:\\n        module.fail_json(msg=\"User %s does not have required permission\"\\n                             \" to log on to vCenter or ESXi API at %s:%s : %s\" % (username, hostname, port, no_permission.msg))\\n    except (requests.ConnectionError, ssl.SSLError) as generic_req_exc:\\n        module.fail_json(msg=\"Unable to connect to vCenter or ESXi API at %s on TCP/%s: %s\" % (hostname, port, generic_req_exc))\\n    except vmodl.fault.InvalidRequest as invalid_request:\\n        msg = \"Failed to get a response from server %s:%s \" % (hostname, port)\\n        module.fail_json(msg=\"%s as request is malformed: %s\" % (msg, invalid_request.msg) + msg_suffix)\\n    except Exception as generic_exc:\\n        msg = \"Unknown error while connecting to vCenter or ESXi API at %s:%s\" % (hostname, port) + msg_suffix\\n        module.fail_json(msg=\"%s : %s\" % (msg, generic_exc))\\n    if service_instance is None:\\n        msg = \"Unknown error while connecting to vCenter or ESXi API at %s:%s\" % (hostname, port)\\n        module.fail_json(msg=msg + msg_suffix)\\n    if disconnect_atexit:\\n        atexit.register(connect.Disconnect, service_instance)\\n    if return_si:\\n        return service_instance, service_instance.RetrieveContent()\\n    return service_instance.RetrieveContent()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def connect_to_api(module, disconnect_atexit=True, return_si=False):\\n    hostname = module.params['hostname']\\n    username = module.params['username']\\n    password = module.params['password']\\n    port = module.params.get('port', 443)\\n    validate_certs = module.params['validate_certs']\\n    if not hostname:\\n        module.fail_json(msg=\"Hostname parameter is missing.\"\\n                             \" Please specify this parameter in task or\"\\n                             \" export environment variable like 'export VMWARE_HOST=ESXI_HOSTNAME'\")\\n    if not username:\\n        module.fail_json(msg=\"Username parameter is missing.\"\\n                             \" Please specify this parameter in task or\"\\n                             \" export environment variable like 'export VMWARE_USER=ESXI_USERNAME'\")\\n    if not password:\\n        module.fail_json(msg=\"Password parameter is missing.\"\\n                             \" Please specify this parameter in task or\"\\n                             \" export environment variable like 'export VMWARE_PASSWORD=ESXI_PASSWORD'\")\\n    if validate_certs and not hasattr(ssl, 'SSLContext'):\\n        module.fail_json(msg='pyVim does not support changing verification mode with python < 2.7.9. Either update '\\n                             'python or use validate_certs=false.')\\n    elif validate_certs:\\n        ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\\n        ssl_context.verify_mode = ssl.CERT_REQUIRED\\n        ssl_context.check_hostname = True\\n        ssl_context.load_default_certs()\\n    elif hasattr(ssl, 'SSLContext'):\\n        ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\\n        ssl_context.verify_mode = ssl.CERT_NONE\\n        ssl_context.check_hostname = False\\n    else:  \\n        ssl_context = None\\n    service_instance = None\\n    proxy_host = module.params.get('proxy_host')\\n    proxy_port = module.params.get('proxy_port')\\n    connect_args = dict(\\n        host=hostname,\\n        port=port,\\n    )\\n    if ssl_context:\\n        connect_args.update(sslContext=ssl_context)\\n    msg_suffix = ''\\n    try:\\n        if proxy_host:\\n            msg_suffix = \" [proxy: %s:%d]\" % (proxy_host, proxy_port)\\n            connect_args.update(httpProxyHost=proxy_host, httpProxyPort=proxy_port)\\n            smart_stub = connect.SmartStubAdapter(**connect_args)\\n            session_stub = connect.VimSessionOrientedStub(smart_stub, connect.VimSessionOrientedStub.makeUserLoginMethod(username, password))\\n            service_instance = vim.ServiceInstance('ServiceInstance', session_stub)\\n        else:\\n            connect_args.update(user=username, pwd=password)\\n            service_instance = connect.SmartConnect(**connect_args)\\n    except vim.fault.InvalidLogin as invalid_login:\\n        msg = \"Unable to log on to vCenter or ESXi API at %s:%s \" % (hostname, port)\\n        module.fail_json(msg=\"%s as %s: %s\" % (msg, username, invalid_login.msg) + msg_suffix)\\n    except vim.fault.NoPermission as no_permission:\\n        module.fail_json(msg=\"User %s does not have required permission\"\\n                             \" to log on to vCenter or ESXi API at %s:%s : %s\" % (username, hostname, port, no_permission.msg))\\n    except (requests.ConnectionError, ssl.SSLError) as generic_req_exc:\\n        module.fail_json(msg=\"Unable to connect to vCenter or ESXi API at %s on TCP/%s: %s\" % (hostname, port, generic_req_exc))\\n    except vmodl.fault.InvalidRequest as invalid_request:\\n        msg = \"Failed to get a response from server %s:%s \" % (hostname, port)\\n        module.fail_json(msg=\"%s as request is malformed: %s\" % (msg, invalid_request.msg) + msg_suffix)\\n    except Exception as generic_exc:\\n        msg = \"Unknown error while connecting to vCenter or ESXi API at %s:%s\" % (hostname, port) + msg_suffix\\n        module.fail_json(msg=\"%s : %s\" % (msg, generic_exc))\\n    if service_instance is None:\\n        msg = \"Unknown error while connecting to vCenter or ESXi API at %s:%s\" % (hostname, port)\\n        module.fail_json(msg=msg + msg_suffix)\\n    if disconnect_atexit:\\n        atexit.register(connect.Disconnect, service_instance)\\n    if return_si:\\n        return service_instance, service_instance.RetrieveContent()\\n    return service_instance.RetrieveContent()"
  },
  {
    "code": "def get_traceback_data(self):\\n        \"Return a Context instance containing traceback information.\"\\n        if self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\\n            from django.template.loader import template_source_loaders\\n            self.template_does_not_exist = True\\n            self.loader_debug_info = []\\n            for loader in template_source_loaders:\\n                try:\\n                    source_list_func = loader.get_template_sources\\n                    template_list = [{'name': t, 'exists': os.path.exists(t)} \\\\n                        for t in source_list_func(str(self.exc_value))]\\n                except AttributeError:\\n                    template_list = []\\n                loader_name = loader.__module__ + '.' + loader.__class__.__name__\\n                self.loader_debug_info.append({\\n                    'loader': loader_name,\\n                    'templates': template_list,\\n                })\\n        if (settings.TEMPLATE_DEBUG and\\n            hasattr(self.exc_value, 'django_template_source')):\\n            self.get_template_exception_info()\\n        frames = self.get_traceback_frames()\\n        for i, frame in enumerate(frames):\\n            if 'vars' in frame:\\n                frame['vars'] = [(k, force_escape(pprint(v))) for k, v in frame['vars']]\\n            frames[i] = frame\\n        unicode_hint = ''\\n        if self.exc_type and issubclass(self.exc_type, UnicodeError):\\n            start = getattr(self.exc_value, 'start', None)\\n            end = getattr(self.exc_value, 'end', None)\\n            if start is not None and end is not None:\\n                unicode_str = self.exc_value.args[1]\\n                unicode_hint = smart_text(unicode_str[max(start-5, 0):min(end+5, len(unicode_str))], 'ascii', errors='replace')\\n        from django import get_version\\n        c = {\\n            'is_email': self.is_email,\\n            'unicode_hint': unicode_hint,\\n            'frames': frames,\\n            'request': self.request,\\n            'filtered_POST': self.filter.get_post_parameters(self.request),\\n            'settings': get_safe_settings(),\\n            'sys_executable': sys.executable,\\n            'sys_version_info': '%d.%d.%d' % sys.version_info[0:3],\\n            'server_time': datetime.datetime.now(),\\n            'django_version_info': get_version(),\\n            'sys_path' : sys.path,\\n            'template_info': self.template_info,\\n            'template_does_not_exist': self.template_does_not_exist,\\n            'loader_debug_info': self.loader_debug_info,\\n        }\\n        if self.exc_type:\\n            c['exception_type'] = self.exc_type.__name__\\n        if self.exc_value:\\n            c['exception_value'] = smart_text(self.exc_value, errors='replace')\\n        if frames:\\n            c['lastframe'] = frames[-1]\\n        return c",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #6412 -- More details if a template file cannot be loaded\\n\\nReport more details about template files in loader postmortem.",
    "fixed_code": "def get_traceback_data(self):\\n        \"Return a Context instance containing traceback information.\"\\n        if self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\\n            from django.template.loader import template_source_loaders\\n            self.template_does_not_exist = True\\n            self.loader_debug_info = []\\n            for loader in template_source_loaders:\\n                try:\\n                    source_list_func = loader.get_template_sources\\n                    template_list = [{\\n                        'name': t,\\n                        'status': self.format_path_status(t),\\n                    } for t in source_list_func(str(self.exc_value))]\\n                except AttributeError:\\n                    template_list = []\\n                loader_name = loader.__module__ + '.' + loader.__class__.__name__\\n                self.loader_debug_info.append({\\n                    'loader': loader_name,\\n                    'templates': template_list,\\n                })\\n        if (settings.TEMPLATE_DEBUG and\\n            hasattr(self.exc_value, 'django_template_source')):\\n            self.get_template_exception_info()\\n        frames = self.get_traceback_frames()\\n        for i, frame in enumerate(frames):\\n            if 'vars' in frame:\\n                frame['vars'] = [(k, force_escape(pprint(v))) for k, v in frame['vars']]\\n            frames[i] = frame\\n        unicode_hint = ''\\n        if self.exc_type and issubclass(self.exc_type, UnicodeError):\\n            start = getattr(self.exc_value, 'start', None)\\n            end = getattr(self.exc_value, 'end', None)\\n            if start is not None and end is not None:\\n                unicode_str = self.exc_value.args[1]\\n                unicode_hint = smart_text(unicode_str[max(start-5, 0):min(end+5, len(unicode_str))], 'ascii', errors='replace')\\n        from django import get_version\\n        c = {\\n            'is_email': self.is_email,\\n            'unicode_hint': unicode_hint,\\n            'frames': frames,\\n            'request': self.request,\\n            'filtered_POST': self.filter.get_post_parameters(self.request),\\n            'settings': get_safe_settings(),\\n            'sys_executable': sys.executable,\\n            'sys_version_info': '%d.%d.%d' % sys.version_info[0:3],\\n            'server_time': datetime.datetime.now(),\\n            'django_version_info': get_version(),\\n            'sys_path' : sys.path,\\n            'template_info': self.template_info,\\n            'template_does_not_exist': self.template_does_not_exist,\\n            'loader_debug_info': self.loader_debug_info,\\n        }\\n        if self.exc_type:\\n            c['exception_type'] = self.exc_type.__name__\\n        if self.exc_value:\\n            c['exception_value'] = smart_text(self.exc_value, errors='replace')\\n        if frames:\\n            c['lastframe'] = frames[-1]\\n        return c"
  },
  {
    "code": "def join(\\n        self,\\n        other: DataFrame | Series,\\n        on: IndexLabel | None = None,\\n        how: str = \"left\",\\n        lsuffix: str = \"\",\\n        rsuffix: str = \"\",\\n        sort: bool = False,\\n        validate: str | None = None,\\n    ) -> DataFrame:\\n        return self._join_compat(\\n            other,\\n            on=on,\\n            how=how,\\n            lsuffix=lsuffix,\\n            rsuffix=rsuffix,\\n            sort=sort,\\n            validate=validate,\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Update join docs for other param (#46850)\\n\\n\\nUpdate join docs regarding using multiple Series",
    "fixed_code": "def join(\\n        self,\\n        other: DataFrame | Series | list[DataFrame | Series],\\n        on: IndexLabel | None = None,\\n        how: str = \"left\",\\n        lsuffix: str = \"\",\\n        rsuffix: str = \"\",\\n        sort: bool = False,\\n        validate: str | None = None,\\n    ) -> DataFrame:\\n        return self._join_compat(\\n            other,\\n            on=on,\\n            how=how,\\n            lsuffix=lsuffix,\\n            rsuffix=rsuffix,\\n            sort=sort,\\n            validate=validate,\\n        )"
  },
  {
    "code": "def codes_info(self) -> npt.NDArray[np.intp]:\\n        ids, _, _ = self.group_info\\n        if self.indexer is not None:\\n            sorter = np.lexsort((ids, self.indexer))\\n            ids = ids[sorter]\\n            ids = ensure_platform_int(ids)\\n        return ids\\n    @final",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def codes_info(self) -> npt.NDArray[np.intp]:\\n        ids, _, _ = self.group_info\\n        if self.indexer is not None:\\n            sorter = np.lexsort((ids, self.indexer))\\n            ids = ids[sorter]\\n            ids = ensure_platform_int(ids)\\n        return ids\\n    @final"
  },
  {
    "code": "def resolve(self, value: Any = NOTSET, suppress_exception: bool = False) -> Any:\\n        import jsonschema\\n        from jsonschema import FormatChecker\\n        from jsonschema.exceptions import ValidationError\\n        if value is not NOTSET:\\n            self._warn_if_not_json(value)\\n        final_val = value if value is not NOTSET else self.value\\n        if isinstance(final_val, ArgNotSet):\\n            if suppress_exception:\\n                return None\\n            raise ParamValidationError(\"No value passed and Param has no default value\")\\n        try:\\n            jsonschema.validate(final_val, self.schema, format_checker=FormatChecker())\\n        except ValidationError as err:\\n            if suppress_exception:\\n                return None\\n            raise ParamValidationError(err) from None\\n        self.value = final_val\\n        return final_val",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def resolve(self, value: Any = NOTSET, suppress_exception: bool = False) -> Any:\\n        import jsonschema\\n        from jsonschema import FormatChecker\\n        from jsonschema.exceptions import ValidationError\\n        if value is not NOTSET:\\n            self._warn_if_not_json(value)\\n        final_val = value if value is not NOTSET else self.value\\n        if isinstance(final_val, ArgNotSet):\\n            if suppress_exception:\\n                return None\\n            raise ParamValidationError(\"No value passed and Param has no default value\")\\n        try:\\n            jsonschema.validate(final_val, self.schema, format_checker=FormatChecker())\\n        except ValidationError as err:\\n            if suppress_exception:\\n                return None\\n            raise ParamValidationError(err) from None\\n        self.value = final_val\\n        return final_val"
  },
  {
    "code": "def _check_input(self, X, *, reset):\\n        if self.validate:\\n            return self._validate_data(X, accept_sparse=self.accept_sparse, reset=reset)\\n        return X",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_input(self, X, *, reset):\\n        if self.validate:\\n            return self._validate_data(X, accept_sparse=self.accept_sparse, reset=reset)\\n        return X"
  },
  {
    "code": "def JSONObject(s_and_end, strict, scan_once, object_hook, object_pairs_hook,\\n        _w=WHITESPACE.match, _ws=WHITESPACE_STR):\\n    s, end = s_and_end\\n    pairs = []\\n    pairs_append = pairs.append\\n    nextchar = s[end:end + 1]\\n    if nextchar != '\"':\\n        if nextchar in _ws:\\n            end = _w(s, end).end()\\n            nextchar = s[end:end + 1]\\n        if nextchar == '}':\\n            if object_pairs_hook is not None:\\n                result = object_pairs_hook(pairs)\\n                return result, end\\n            pairs = {}\\n            if object_hook is not None:\\n                pairs = object_hook(pairs)\\n            return pairs, end + 1\\n        elif nextchar != '\"':\\n            raise ValueError(errmsg(\"Expecting property name\", s, end))\\n    end += 1\\n    while True:\\n        key, end = scanstring(s, end, strict)\\n        if s[end:end + 1] != ':':\\n            end = _w(s, end).end()\\n            if s[end:end + 1] != ':':\\n                raise ValueError(errmsg(\"Expecting : delimiter\", s, end))\\n        end += 1\\n        try:\\n            if s[end] in _ws:\\n                end += 1\\n                if s[end] in _ws:\\n                    end = _w(s, end + 1).end()\\n        except IndexError:\\n            pass\\n        try:\\n            value, end = scan_once(s, end)\\n        except StopIteration:\\n            raise ValueError(errmsg(\"Expecting object\", s, end))\\n        pairs_append((key, value))\\n        try:\\n            nextchar = s[end]\\n            if nextchar in _ws:\\n                end = _w(s, end + 1).end()\\n                nextchar = s[end]\\n        except IndexError:\\n            nextchar = ''\\n        end += 1\\n        if nextchar == '}':\\n            break\\n        elif nextchar != ',':\\n            raise ValueError(errmsg(\"Expecting , delimiter\", s, end - 1))\\n        end = _w(s, end).end()\\n        nextchar = s[end:end + 1]\\n        end += 1\\n        if nextchar != '\"':\\n            raise ValueError(errmsg(\"Expecting property name\", s, end - 1))\\n    if object_pairs_hook is not None:\\n        result = object_pairs_hook(pairs)\\n        return result, end\\n    pairs = dict(pairs)\\n    if object_hook is not None:\\n        pairs = object_hook(pairs)\\n    return pairs, end",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def JSONObject(s_and_end, strict, scan_once, object_hook, object_pairs_hook,\\n        _w=WHITESPACE.match, _ws=WHITESPACE_STR):\\n    s, end = s_and_end\\n    pairs = []\\n    pairs_append = pairs.append\\n    nextchar = s[end:end + 1]\\n    if nextchar != '\"':\\n        if nextchar in _ws:\\n            end = _w(s, end).end()\\n            nextchar = s[end:end + 1]\\n        if nextchar == '}':\\n            if object_pairs_hook is not None:\\n                result = object_pairs_hook(pairs)\\n                return result, end\\n            pairs = {}\\n            if object_hook is not None:\\n                pairs = object_hook(pairs)\\n            return pairs, end + 1\\n        elif nextchar != '\"':\\n            raise ValueError(errmsg(\"Expecting property name\", s, end))\\n    end += 1\\n    while True:\\n        key, end = scanstring(s, end, strict)\\n        if s[end:end + 1] != ':':\\n            end = _w(s, end).end()\\n            if s[end:end + 1] != ':':\\n                raise ValueError(errmsg(\"Expecting : delimiter\", s, end))\\n        end += 1\\n        try:\\n            if s[end] in _ws:\\n                end += 1\\n                if s[end] in _ws:\\n                    end = _w(s, end + 1).end()\\n        except IndexError:\\n            pass\\n        try:\\n            value, end = scan_once(s, end)\\n        except StopIteration:\\n            raise ValueError(errmsg(\"Expecting object\", s, end))\\n        pairs_append((key, value))\\n        try:\\n            nextchar = s[end]\\n            if nextchar in _ws:\\n                end = _w(s, end + 1).end()\\n                nextchar = s[end]\\n        except IndexError:\\n            nextchar = ''\\n        end += 1\\n        if nextchar == '}':\\n            break\\n        elif nextchar != ',':\\n            raise ValueError(errmsg(\"Expecting , delimiter\", s, end - 1))\\n        end = _w(s, end).end()\\n        nextchar = s[end:end + 1]\\n        end += 1\\n        if nextchar != '\"':\\n            raise ValueError(errmsg(\"Expecting property name\", s, end - 1))\\n    if object_pairs_hook is not None:\\n        result = object_pairs_hook(pairs)\\n        return result, end\\n    pairs = dict(pairs)\\n    if object_hook is not None:\\n        pairs = object_hook(pairs)\\n    return pairs, end"
  },
  {
    "code": "def iter_format_modules(lang):\\n    if check_for_language(lang) or settings.USE_L10N:\\n        format_locations = ['django.conf.locale.%s']\\n        if settings.FORMAT_MODULE_PATH:\\n            format_locations.append(settings.FORMAT_MODULE_PATH + '.%s')\\n            format_locations.reverse()\\n        locale = to_locale(lang)\\n        locales = set((locale, locale.split('_')[0]))\\n        for location in format_locations:\\n            for loc in locales:\\n                try:\\n                    yield import_module('.formats', location % loc)\\n                except ImportError:\\n                    pass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #15024 -- Ensure that choice of L10N format module used is stable given a stable setup of format modules in ll/ and ll_CC/ dirs. Thanks David Reynolds for the report and suggestions leading to the solution.",
    "fixed_code": "def iter_format_modules(lang):\\n    if check_for_language(lang) or settings.USE_L10N:\\n        format_locations = ['django.conf.locale.%s']\\n        if settings.FORMAT_MODULE_PATH:\\n            format_locations.append(settings.FORMAT_MODULE_PATH + '.%s')\\n            format_locations.reverse()\\n        locale = to_locale(lang)\\n        locales = [locale]\\n        if '_' in locale:\\n            locales.append(locale.split('_')[0])\\n        for location in format_locations:\\n            for loc in locales:\\n                try:\\n                    yield import_module('.formats', location % loc)\\n                except ImportError:\\n                    pass"
  },
  {
    "code": "def configure_logging(logging_config, logging_settings):\\n    if not sys.warnoptions:\\n        logging.captureWarnings(True)\\n        warnings.simplefilter(\"default\", RemovedInNextVersionWarning)\\n    if logging_config:\\n        logging_config_func = import_string(logging_config)\\n        logging.config.dictConfig(DEFAULT_LOGGING)\\n        if logging_settings:\\n            logging_config_func(logging_settings)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def configure_logging(logging_config, logging_settings):\\n    if not sys.warnoptions:\\n        logging.captureWarnings(True)\\n        warnings.simplefilter(\"default\", RemovedInNextVersionWarning)\\n    if logging_config:\\n        logging_config_func = import_string(logging_config)\\n        logging.config.dictConfig(DEFAULT_LOGGING)\\n        if logging_settings:\\n            logging_config_func(logging_settings)"
  },
  {
    "code": "def wrap_future(fut, *, loop=None):\\n    if isinstance(fut, Future):\\n        return fut\\n    assert isinstance(fut, concurrent.futures.Future), \\\\n        'concurrent.futures.Future is expected, got {!r}'.format(fut)\\n    if loop is None:\\n        loop = events.get_event_loop()\\n    new_future = Future(loop=loop)\\n    fut.add_done_callback(\\n        lambda future: loop.call_soon_threadsafe(\\n            new_future._copy_state, fut))\\n    return new_future",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio: Pass cancellation from wrapping Future to wrapped Future. By Sa\u00fal Ibarra Corretg\u00e9 (mostly).",
    "fixed_code": "def wrap_future(fut, *, loop=None):\\n    if isinstance(fut, Future):\\n        return fut\\n    assert isinstance(fut, concurrent.futures.Future), \\\\n        'concurrent.futures.Future is expected, got {!r}'.format(fut)\\n    if loop is None:\\n        loop = events.get_event_loop()\\n    new_future = Future(loop=loop)"
  },
  {
    "code": "def _str_extract_noexpand(arr, pat, flags=0):\\n    from pandas import (\\n        DataFrame,\\n        array as pd_array,\\n    )\\n    regex = re.compile(pat, flags=flags)\\n    groups_or_na = _groups_or_na_fun(regex)\\n    result_dtype = _result_dtype(arr)\\n    if regex.groups == 1:\\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\\n        name = _get_single_group_name(regex)\\n        result = pd_array(result, dtype=result_dtype)\\n    else:\\n        name = None\\n        columns = _get_group_names(regex)\\n        if arr.size == 0:\\n            result = DataFrame(  \\n                columns=columns, dtype=result_dtype\\n            )\\n        else:\\n            result = DataFrame(  \\n                [groups_or_na(val) for val in arr],\\n                columns=columns,\\n                index=arr.index,\\n                dtype=result_dtype,\\n            )\\n    return result, name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _str_extract_noexpand(arr, pat, flags=0):\\n    from pandas import (\\n        DataFrame,\\n        array as pd_array,\\n    )\\n    regex = re.compile(pat, flags=flags)\\n    groups_or_na = _groups_or_na_fun(regex)\\n    result_dtype = _result_dtype(arr)\\n    if regex.groups == 1:\\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\\n        name = _get_single_group_name(regex)\\n        result = pd_array(result, dtype=result_dtype)\\n    else:\\n        name = None\\n        columns = _get_group_names(regex)\\n        if arr.size == 0:\\n            result = DataFrame(  \\n                columns=columns, dtype=result_dtype\\n            )\\n        else:\\n            result = DataFrame(  \\n                [groups_or_na(val) for val in arr],\\n                columns=columns,\\n                index=arr.index,\\n                dtype=result_dtype,\\n            )\\n    return result, name"
  },
  {
    "code": "def isreadable(object):\\n    return PrettyPrinter()._safe_repr(object, {}, None, 0)[1]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def isreadable(object):\\n    return PrettyPrinter()._safe_repr(object, {}, None, 0)[1]"
  },
  {
    "code": "def add_field(self, model, field):\\n        if not field.null or self.effective_default(field) is not None:\\n            self._remake_table(model, create_field=field)\\n        else:\\n            super().add_field(model, field)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #33408 -- Fixed adding nullable unique fields on SQLite.\\n\\nRegression in 2f73e5406d54cb8945e187eff302a3a3373350be.\\n\\nThanks Alan Crosswell for the report.",
    "fixed_code": "def add_field(self, model, field):\\n        if (\\n            field.primary_key or field.unique or\\n            not field.null or self.effective_default(field) is not None\\n        ):\\n            self._remake_table(model, create_field=field)\\n        else:\\n            super().add_field(model, field)"
  },
  {
    "code": "def value_counts_arraylike(values, dropna: bool):\\n    values = _ensure_arraylike(values)\\n    original = values\\n    values, _ = _ensure_data(values)\\n    ndtype = values.dtype.name\\n    if needs_i8_conversion(original.dtype):\\n        keys, counts = htable.value_count_int64(values, dropna)\\n        if dropna:\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n    else:\\n        f = getattr(htable, f\"value_count_{ndtype}\")\\n        keys, counts = f(values, dropna)\\n    keys = _reconstruct_data(keys, original.dtype, original)\\n    return keys, counts",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def value_counts_arraylike(values, dropna: bool):\\n    values = _ensure_arraylike(values)\\n    original = values\\n    values, _ = _ensure_data(values)\\n    ndtype = values.dtype.name\\n    if needs_i8_conversion(original.dtype):\\n        keys, counts = htable.value_count_int64(values, dropna)\\n        if dropna:\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n    else:\\n        f = getattr(htable, f\"value_count_{ndtype}\")\\n        keys, counts = f(values, dropna)\\n    keys = _reconstruct_data(keys, original.dtype, original)\\n    return keys, counts"
  },
  {
    "code": "def __array_wrap__(self, result, context=None):\\n        d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\\n        return self._constructor(result, **d).__finalize__(self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: remove ptp (#30458)",
    "fixed_code": "def __array_wrap__(self, result, context=None):\\n        result = lib.item_from_zerodim(result)\\n        if is_scalar(result):\\n            return result\\n        d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\\n        return self._constructor(result, **d).__finalize__(self)"
  },
  {
    "code": "def load_package(name, path):\\n    if os.path.isdir(path):\\n        extensions = _bootstrap._suffix_list(PY_SOURCE)\\n        extensions += _bootstrap._suffix_list(PY_COMPILED)\\n        for extension in extensions:\\n            path = os.path.join(path, '__init__'+extension)\\n            if os.path.exists(path):\\n                break\\n        else:\\n            raise ValueError('{!r} is not a package'.format(path))\\n    return _bootstrap.SourceFileLoader(name, path).load_module(name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_package(name, path):\\n    if os.path.isdir(path):\\n        extensions = _bootstrap._suffix_list(PY_SOURCE)\\n        extensions += _bootstrap._suffix_list(PY_COMPILED)\\n        for extension in extensions:\\n            path = os.path.join(path, '__init__'+extension)\\n            if os.path.exists(path):\\n                break\\n        else:\\n            raise ValueError('{!r} is not a package'.format(path))\\n    return _bootstrap.SourceFileLoader(name, path).load_module(name)"
  },
  {
    "code": "def patch_click() -> None:\\n    try:\\n        from click import core\\n        from click import _unicodefun\\n    except ModuleNotFoundError:\\n        return\\n    for module in (core, _unicodefun):\\n        if hasattr(module, \"_verify_python3_env\"):\\n            module._verify_python3_env = lambda: None  \\n        if hasattr(module, \"_verify_python_env\"):\\n            module._verify_python_env = lambda: None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix _unicodefun patch code for Click 8.1.0 (#2966)\\n\\nFixes #2964",
    "fixed_code": "def patch_click() -> None:\\n    modules: List[Any] = []\\n    try:\\n        from click import core\\n    except ImportError:\\n        pass\\n    else:\\n        modules.append(core)\\n    try:\\n        from click import _unicodefun\\n    except ImportError:\\n        pass\\n    else:\\n        modules.append(_unicodefun)\\n    for module in modules:\\n        if hasattr(module, \"_verify_python3_env\"):\\n            module._verify_python3_env = lambda: None  \\n        if hasattr(module, \"_verify_python_env\"):\\n            module._verify_python_env = lambda: None"
  },
  {
    "code": "def read_feather(path, use_threads=True):\\n    feather, pyarrow = _try_import()\\n    path = _stringify_path(path)\\n    if LooseVersion(pyarrow.__version__) < LooseVersion('0.11.0'):\\n        int_use_threads = int(use_threads)\\n        if int_use_threads < 1:\\n            int_use_threads = 1\\n        return feather.read_feather(path, nthreads=int_use_threads)\\n    return feather.read_feather(path, use_threads=bool(use_threads))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_feather(path, use_threads=True):\\n    feather, pyarrow = _try_import()\\n    path = _stringify_path(path)\\n    if LooseVersion(pyarrow.__version__) < LooseVersion('0.11.0'):\\n        int_use_threads = int(use_threads)\\n        if int_use_threads < 1:\\n            int_use_threads = 1\\n        return feather.read_feather(path, nthreads=int_use_threads)\\n    return feather.read_feather(path, use_threads=bool(use_threads))"
  },
  {
    "code": "def __internal_pivot_table(\\n    data: DataFrame,\\n    values,\\n    index,\\n    columns,\\n    aggfunc: AggFuncTypeBase | AggFuncTypeDict,\\n    fill_value,\\n    margins: bool,\\n    dropna: bool,\\n    margins_name: str,\\n    observed: bool,\\n    sort: bool,\\n) -> DataFrame:\\n    keys = index + columns\\n    values_passed = values is not None\\n    if values_passed:\\n        if is_list_like(values):\\n            values_multi = True\\n            values = list(values)\\n        else:\\n            values_multi = False\\n            values = [values]\\n        for i in values:\\n            if i not in data:\\n                raise KeyError(i)\\n        to_filter = []\\n        for x in keys + values:\\n            if isinstance(x, Grouper):\\n                x = x.key\\n            try:\\n                if x in data:\\n                    to_filter.append(x)\\n            except TypeError:\\n                pass\\n        if len(to_filter) < len(data.columns):\\n            data = data[to_filter]\\n    else:\\n        values = data.columns\\n        for key in keys:\\n            try:\\n                values = values.drop(key)\\n            except (TypeError, ValueError, KeyError):\\n                pass\\n        values = list(values)\\n    grouped = data.groupby(keys, observed=observed, sort=sort)\\n    agged = grouped.agg(aggfunc)\\n    if dropna and isinstance(agged, ABCDataFrame) and len(agged.columns):\\n        agged = agged.dropna(how=\"all\")\\n        for v in values:\\n            if (\\n                v in data\\n                and is_integer_dtype(data[v])\\n                and v in agged\\n                and not is_integer_dtype(agged[v])\\n            ):\\n                if not isinstance(agged[v], ABCDataFrame):\\n                    agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)\\n    table = agged\\n    if table.index.nlevels > 1 and index:\\n        index_names = agged.index.names[: len(index)]\\n        to_unstack = []\\n        for i in range(len(index), len(keys)):\\n            name = agged.index.names[i]\\n            if name is None or name in index_names:\\n                to_unstack.append(i)\\n            else:\\n                to_unstack.append(name)\\n        table = agged.unstack(to_unstack)\\n    if not dropna:\\n        if isinstance(table.index, MultiIndex):\\n            m = MultiIndex.from_arrays(\\n                cartesian_product(table.index.levels), names=table.index.names\\n            )\\n            table = table.reindex(m, axis=0)\\n        if isinstance(table.columns, MultiIndex):\\n            m = MultiIndex.from_arrays(\\n                cartesian_product(table.columns.levels), names=table.columns.names\\n            )\\n            table = table.reindex(m, axis=1)\\n    if isinstance(table, ABCDataFrame):\\n        table = table.sort_index(axis=1)\\n    if fill_value is not None:\\n        table = table.fillna(fill_value, downcast=\"infer\")\\n    if margins:\\n        if dropna:\\n            data = data[data.notna().all(axis=1)]\\n        table = _add_margins(\\n            table,\\n            data,\\n            values,\\n            rows=index,\\n            cols=columns,\\n            aggfunc=aggfunc,\\n            observed=dropna,\\n            margins_name=margins_name,\\n            fill_value=fill_value,\\n        )\\n    if values_passed and not values_multi and table.columns.nlevels > 1:\\n        table = table.droplevel(0, axis=1)\\n    if len(index) == 0 and len(columns) > 0:\\n        table = table.T\\n    if isinstance(table, ABCDataFrame) and dropna:\\n        table = table.dropna(how=\"all\", axis=1)\\n    return table",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: do not sort resulting columns when sort=False (#46994)",
    "fixed_code": "def __internal_pivot_table(\\n    data: DataFrame,\\n    values,\\n    index,\\n    columns,\\n    aggfunc: AggFuncTypeBase | AggFuncTypeDict,\\n    fill_value,\\n    margins: bool,\\n    dropna: bool,\\n    margins_name: str,\\n    observed: bool,\\n    sort: bool,\\n) -> DataFrame:\\n    keys = index + columns\\n    values_passed = values is not None\\n    if values_passed:\\n        if is_list_like(values):\\n            values_multi = True\\n            values = list(values)\\n        else:\\n            values_multi = False\\n            values = [values]\\n        for i in values:\\n            if i not in data:\\n                raise KeyError(i)\\n        to_filter = []\\n        for x in keys + values:\\n            if isinstance(x, Grouper):\\n                x = x.key\\n            try:\\n                if x in data:\\n                    to_filter.append(x)\\n            except TypeError:\\n                pass\\n        if len(to_filter) < len(data.columns):\\n            data = data[to_filter]\\n    else:\\n        values = data.columns\\n        for key in keys:\\n            try:\\n                values = values.drop(key)\\n            except (TypeError, ValueError, KeyError):\\n                pass\\n        values = list(values)\\n    grouped = data.groupby(keys, observed=observed, sort=sort)\\n    agged = grouped.agg(aggfunc)\\n    if dropna and isinstance(agged, ABCDataFrame) and len(agged.columns):\\n        agged = agged.dropna(how=\"all\")\\n        for v in values:\\n            if (\\n                v in data\\n                and is_integer_dtype(data[v])\\n                and v in agged\\n                and not is_integer_dtype(agged[v])\\n            ):\\n                if not isinstance(agged[v], ABCDataFrame):\\n                    agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)\\n    table = agged\\n    if table.index.nlevels > 1 and index:\\n        index_names = agged.index.names[: len(index)]\\n        to_unstack = []\\n        for i in range(len(index), len(keys)):\\n            name = agged.index.names[i]\\n            if name is None or name in index_names:\\n                to_unstack.append(i)\\n            else:\\n                to_unstack.append(name)\\n        table = agged.unstack(to_unstack)\\n    if not dropna:\\n        if isinstance(table.index, MultiIndex):\\n            m = MultiIndex.from_arrays(\\n                cartesian_product(table.index.levels), names=table.index.names\\n            )\\n            table = table.reindex(m, axis=0)\\n        if isinstance(table.columns, MultiIndex):\\n            m = MultiIndex.from_arrays(\\n                cartesian_product(table.columns.levels), names=table.columns.names\\n            )\\n            table = table.reindex(m, axis=1)\\n    if sort is True and isinstance(table, ABCDataFrame):\\n        table = table.sort_index(axis=1)\\n    if fill_value is not None:\\n        table = table.fillna(fill_value, downcast=\"infer\")\\n    if margins:\\n        if dropna:\\n            data = data[data.notna().all(axis=1)]\\n        table = _add_margins(\\n            table,\\n            data,\\n            values,\\n            rows=index,\\n            cols=columns,\\n            aggfunc=aggfunc,\\n            observed=dropna,\\n            margins_name=margins_name,\\n            fill_value=fill_value,\\n        )\\n    if values_passed and not values_multi and table.columns.nlevels > 1:\\n        table = table.droplevel(0, axis=1)\\n    if len(index) == 0 and len(columns) > 0:\\n        table = table.T\\n    if isinstance(table, ABCDataFrame) and dropna:\\n        table = table.dropna(how=\"all\", axis=1)\\n    return table"
  },
  {
    "code": "def sync(self):\\n        if hasattr(self.dict, 'sync'):\\n            self.dict.sync()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Patch #553171: Add writeback parameter. Also add protocol parameter.",
    "fixed_code": "def sync(self):\\n        if self.writeback and self.cache:\\n            self.writeback = False\\n            for key, entry in self.cache.iteritems():\\n                self[key] = entry\\n            self.writeback = True\\n            self.cache = {}\\n        if hasattr(self.dict, 'sync'):\\n            self.dict.sync()"
  },
  {
    "code": "def main():\\n  le_handler = get_le_handler()\\n  le_level = 20  \\n  ctx = zmq.Context().instance()\\n  sock = ctx.socket(zmq.PULL)\\n  sock.bind(\"ipc:///tmp/logmessage\")\\n  pub_sock = messaging.pub_sock('logMessage')\\n  while True:\\n    dat = b''.join(sock.recv_multipart())\\n    dat = dat.decode('utf8')\\n    levelnum = ord(dat[0])\\n    dat = dat[1:]\\n    if levelnum >= le_level:\\n      le_handler.emit_raw(dat)\\n    msg = messaging.new_message()\\n    msg.logMessage = dat\\n    pub_sock.send(msg.to_bytes())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n  le_handler = get_le_handler()\\n  le_level = 20  \\n  ctx = zmq.Context().instance()\\n  sock = ctx.socket(zmq.PULL)\\n  sock.bind(\"ipc:///tmp/logmessage\")\\n  pub_sock = messaging.pub_sock('logMessage')\\n  while True:\\n    dat = b''.join(sock.recv_multipart())\\n    dat = dat.decode('utf8')\\n    levelnum = ord(dat[0])\\n    dat = dat[1:]\\n    if levelnum >= le_level:\\n      le_handler.emit_raw(dat)\\n    msg = messaging.new_message()\\n    msg.logMessage = dat\\n    pub_sock.send(msg.to_bytes())"
  },
  {
    "code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options"
  },
  {
    "code": "def _hash_add(cls, fields):\\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\\n    return _hash_fn(flds)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34776: Fix dataclasses to support __future__ \"annotations\" mode (#9518)",
    "fixed_code": "def _hash_add(cls, fields, globals):\\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\\n    return _hash_fn(flds, globals)"
  },
  {
    "code": "def boxplot(data, column=None, by=None, ax=None, fontsize=None,\\n            rot=0, grid=True, figsize=None, layout=None, **kwds):\\n    from pandas import Series, DataFrame\\n    if isinstance(data, Series):\\n        data = DataFrame({'x': data})\\n        column = 'x'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def boxplot(data, column=None, by=None, ax=None, fontsize=None,\\n            rot=0, grid=True, figsize=None, layout=None, **kwds):\\n    from pandas import Series, DataFrame\\n    if isinstance(data, Series):\\n        data = DataFrame({'x': data})\\n        column = 'x'"
  },
  {
    "code": "def plot_frame(frame=None, x=None, y=None, subplots=False, sharex=True,\\n               sharey=False, use_index=True, figsize=None, grid=False,\\n               legend=True, rot=None, ax=None, style=None, title=None,\\n               xlim=None, ylim=None, logx=False, logy=False, xticks=None,\\n               yticks=None, kind='line', sort_columns=False, fontsize=None,\\n               secondary_y=False, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    if x is not None:\\n        if com.is_integer(x) and not frame.columns.holds_integer():\\n            x = frame.columns[x]\\n        frame = frame.set_index(x)\\n    if y is not None:\\n        if com.is_integer(y) and not frame.columns.holds_integer():\\n            y = frame.columns[y]\\n        label = x if x is not None else frame.index.name\\n        label = kwds.pop('label', label)\\n        ser = frame[y]\\n        ser.index.name = label\\n        return plot_series(ser, label=label, kind=kind,\\n                           use_index=use_index,\\n                           rot=rot, xticks=xticks, yticks=yticks,\\n                           xlim=xlim, ylim=ylim, ax=ax, style=style,\\n                           grid=grid, logx=logx, logy=logy,\\n                           secondary_y=secondary_y, title=title,\\n                           figsize=figsize, fontsize=fontsize, **kwds)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, style=style, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logx=logx,\\n                     logy=logy, sort_columns=sort_columns,\\n                     secondary_y=secondary_y, **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def plot_frame(frame=None, x=None, y=None, subplots=False, sharex=True,\\n               sharey=False, use_index=True, figsize=None, grid=False,\\n               legend=True, rot=None, ax=None, style=None, title=None,\\n               xlim=None, ylim=None, logx=False, logy=False, xticks=None,\\n               yticks=None, kind='line', sort_columns=False, fontsize=None,\\n               secondary_y=False, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    if x is not None:\\n        if com.is_integer(x) and not frame.columns.holds_integer():\\n            x = frame.columns[x]\\n        frame = frame.set_index(x)\\n    if y is not None:\\n        if com.is_integer(y) and not frame.columns.holds_integer():\\n            y = frame.columns[y]\\n        label = x if x is not None else frame.index.name\\n        label = kwds.pop('label', label)\\n        ser = frame[y]\\n        ser.index.name = label\\n        return plot_series(ser, label=label, kind=kind,\\n                           use_index=use_index,\\n                           rot=rot, xticks=xticks, yticks=yticks,\\n                           xlim=xlim, ylim=ylim, ax=ax, style=style,\\n                           grid=grid, logx=logx, logy=logy,\\n                           secondary_y=secondary_y, title=title,\\n                           figsize=figsize, fontsize=fontsize, **kwds)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, style=style, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logx=logx,\\n                     logy=logy, sort_columns=sort_columns,\\n                     secondary_y=secondary_y, **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]"
  },
  {
    "code": "def _isnull_new(obj):\\n    if lib.isscalar(obj):\\n        return lib.checknull(obj)\\n    if isinstance(obj, (ABCSeries, np.ndarray)):\\n        return _isnull_ndarraylike(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj.apply(isnull)\\n    elif isinstance(obj, list) or hasattr(obj, '__array__'):\\n        return _isnull_ndarraylike(np.asarray(obj))\\n    else:\\n        return obj is None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix segfault on isnull(MultiIndex)\\n\\nNow raises NotImplementedError b/c not yet clear what it should return.",
    "fixed_code": "def _isnull_new(obj):\\n    if lib.isscalar(obj):\\n        return lib.checknull(obj)\\n    elif isinstance(obj, pd.MultiIndex):\\n        raise NotImplementedError(\"isnull is not defined for MultiIndex\")\\n    elif isinstance(obj, (ABCSeries, np.ndarray)):\\n        return _isnull_ndarraylike(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj.apply(isnull)\\n    elif isinstance(obj, list) or hasattr(obj, '__array__'):\\n        return _isnull_ndarraylike(np.asarray(obj))\\n    else:\\n        return obj is None"
  },
  {
    "code": "def __init__(self, expression, **extra):\\n        super(Avg, self).__init__(expression, output_field=FloatField(), **extra)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24649 -- Allowed using Avg aggregate on non-numeric field types.",
    "fixed_code": "def __init__(self, expression, **extra):\\n        output_field = extra.pop('output_field', FloatField())\\n        super(Avg, self).__init__(expression, output_field=output_field, **extra)"
  },
  {
    "code": "def _read(filepath_or_buffer, kwds):\\n    encoding = kwds.get('encoding', None)\\n    if encoding is not None:\\n        encoding = re.sub('_', '-', encoding).lower()\\n        kwds['encoding'] = encoding\\n    compression = kwds.get('compression', 'infer')\\n    compression = _infer_compression(filepath_or_buffer, compression)\\n    filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\\n        filepath_or_buffer, encoding, compression)\\n    kwds['compression'] = compression\\n    if kwds.get('date_parser', None) is not None:\\n        if isinstance(kwds['parse_dates'], bool):\\n            kwds['parse_dates'] = True\\n    iterator = kwds.get('iterator', False)\\n    chunksize = _validate_integer('chunksize', kwds.get('chunksize', None), 1)\\n    nrows = kwds.get('nrows', None)\\n    _validate_names(kwds.get(\"names\", None))\\n    parser = TextFileReader(filepath_or_buffer, **kwds)\\n    if chunksize or iterator:\\n        return parser\\n    try:\\n        data = parser.read(nrows)\\n    finally:\\n        parser.close()\\n    if should_close:\\n        try:\\n            filepath_or_buffer.close()\\n        except ValueError:\\n            pass\\n    return data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read(filepath_or_buffer, kwds):\\n    encoding = kwds.get('encoding', None)\\n    if encoding is not None:\\n        encoding = re.sub('_', '-', encoding).lower()\\n        kwds['encoding'] = encoding\\n    compression = kwds.get('compression', 'infer')\\n    compression = _infer_compression(filepath_or_buffer, compression)\\n    filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\\n        filepath_or_buffer, encoding, compression)\\n    kwds['compression'] = compression\\n    if kwds.get('date_parser', None) is not None:\\n        if isinstance(kwds['parse_dates'], bool):\\n            kwds['parse_dates'] = True\\n    iterator = kwds.get('iterator', False)\\n    chunksize = _validate_integer('chunksize', kwds.get('chunksize', None), 1)\\n    nrows = kwds.get('nrows', None)\\n    _validate_names(kwds.get(\"names\", None))\\n    parser = TextFileReader(filepath_or_buffer, **kwds)\\n    if chunksize or iterator:\\n        return parser\\n    try:\\n        data = parser.read(nrows)\\n    finally:\\n        parser.close()\\n    if should_close:\\n        try:\\n            filepath_or_buffer.close()\\n        except ValueError:\\n            pass\\n    return data"
  },
  {
    "code": "def run_table_delete(self, deletion_dataset_table: str,\\n                         ignore_if_missing: bool = False) -> None:\\n        deletion_project, deletion_dataset, deletion_table = \\\\n            _split_tablename(table_input=deletion_dataset_table,\\n                             default_project_id=self.project_id)\\n        try:\\n            self.service.tables() \\\\n                .delete(projectId=deletion_project,\\n                        datasetId=deletion_dataset,\\n                        tableId=deletion_table) \\\\n                .execute(num_retries=self.num_retries)\\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\\n                          deletion_dataset, deletion_table)\\n        except HttpError as e:\\n            if e.resp.status == 404 and ignore_if_missing:\\n                self.log.info('Table does not exist. Skipping.')\\n            else:\\n                raise e\\n    @CloudBaseHook.catch_http_exception",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run_table_delete(self, deletion_dataset_table: str,\\n                         ignore_if_missing: bool = False) -> None:\\n        deletion_project, deletion_dataset, deletion_table = \\\\n            _split_tablename(table_input=deletion_dataset_table,\\n                             default_project_id=self.project_id)\\n        try:\\n            self.service.tables() \\\\n                .delete(projectId=deletion_project,\\n                        datasetId=deletion_dataset,\\n                        tableId=deletion_table) \\\\n                .execute(num_retries=self.num_retries)\\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\\n                          deletion_dataset, deletion_table)\\n        except HttpError as e:\\n            if e.resp.status == 404 and ignore_if_missing:\\n                self.log.info('Table does not exist. Skipping.')\\n            else:\\n                raise e\\n    @CloudBaseHook.catch_http_exception"
  },
  {
    "code": "def __eq__(self, other):\\n        return self.settings_dict == other.settings_dict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Corrected the way databases were compared.  This allows running the test suite with two in memory SQLite databases.",
    "fixed_code": "def __eq__(self, other):\\n        return self.alias == other.alias"
  },
  {
    "code": "def get_supported_language_variant(lang_code, strict=False):\\n    if lang_code and lang_code.lower() == settings.LANGUAGE_CODE.lower():\\n        return lang_code\\n    else:\\n        raise LookupError(lang_code)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_supported_language_variant(lang_code, strict=False):\\n    if lang_code and lang_code.lower() == settings.LANGUAGE_CODE.lower():\\n        return lang_code\\n    else:\\n        raise LookupError(lang_code)"
  },
  {
    "code": "def serializer_factory(value):\\n    if isinstance(value, Promise):\\n        value = str(value)\\n    elif isinstance(value, LazyObject):\\n        value = value.__reduce__()[1][0]\\n    if isinstance(value, models.Field):\\n        return ModelFieldSerializer(value)\\n    if isinstance(value, models.manager.BaseManager):\\n        return ModelManagerSerializer(value)\\n    if isinstance(value, Operation):\\n        return OperationSerializer(value)\\n    if isinstance(value, type):\\n        return TypeSerializer(value)\\n    if hasattr(value, 'deconstruct'):\\n        return DeconstructableSerializer(value)\\n    if isinstance(value, frozenset):\\n        return FrozensetSerializer(value)\\n    if isinstance(value, list):\\n        return SequenceSerializer(value)\\n    if isinstance(value, set):\\n        return SetSerializer(value)\\n    if isinstance(value, tuple):\\n        return TupleSerializer(value)\\n    if isinstance(value, dict):\\n        return DictionarySerializer(value)\\n    if isinstance(value, enum.Enum):\\n        return EnumSerializer(value)\\n    if isinstance(value, datetime.datetime):\\n        return DatetimeDatetimeSerializer(value)\\n    if isinstance(value, (datetime.date, datetime.timedelta, datetime.time)):\\n        return DateTimeSerializer(value)\\n    if isinstance(value, SettingsReference):\\n        return SettingsReferenceSerializer(value)\\n    if isinstance(value, float):\\n        return FloatSerializer(value)\\n    if isinstance(value, (bool, int, type(None), bytes, str)):\\n        return BaseSimpleSerializer(value)\\n    if isinstance(value, decimal.Decimal):\\n        return DecimalSerializer(value)\\n    if isinstance(value, (functools.partial, functools.partialmethod)):\\n        return FunctoolsPartialSerializer(value)\\n    if isinstance(value, (types.FunctionType, types.BuiltinFunctionType, types.MethodType)):\\n        return FunctionTypeSerializer(value)\\n    if isinstance(value, collections.abc.Iterable):\\n        return IterableSerializer(value)\\n    if isinstance(value, (COMPILED_REGEX_TYPE, RegexObject)):\\n        return RegexSerializer(value)\\n    if isinstance(value, uuid.UUID):\\n        return UUIDSerializer(value)\\n    raise ValueError(\\n        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\\n        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\\n        \"topics/migrations/\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #29738 -- Allowed registering serializers with MigrationWriter.",
    "fixed_code": "def serializer_factory(value):\\n    if isinstance(value, Promise):\\n        value = str(value)\\n    elif isinstance(value, LazyObject):\\n        value = value.__reduce__()[1][0]\\n    if isinstance(value, models.Field):\\n        return ModelFieldSerializer(value)\\n    if isinstance(value, models.manager.BaseManager):\\n        return ModelManagerSerializer(value)\\n    if isinstance(value, Operation):\\n        return OperationSerializer(value)\\n    if isinstance(value, type):\\n        return TypeSerializer(value)\\n    if hasattr(value, 'deconstruct'):\\n        return DeconstructableSerializer(value)\\n    for type_, serializer_cls in Serializer._registry.items():\\n        if isinstance(value, type_):\\n            return serializer_cls(value)\\n    raise ValueError(\\n        \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\\n        \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\\n        \"topics/migrations/\\n    )"
  },
  {
    "code": "def _sanitize_token(token):\\n\\tif re.search('[^a-zA-Z0-9]', token):\\n\\t\\treturn _get_new_csrf_token()\\n\\telif len(token) == CSRF_TOKEN_LENGTH:\\n\\t\\treturn token\\n\\telif len(token) == CSRF_SECRET_LENGTH:\\n\\t\\treturn _salt_cipher_secret(token)\\n\\treturn _get_new_csrf_token()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #31291 -- Renamed salt to mask for CSRF tokens.",
    "fixed_code": "def _sanitize_token(token):\\n\\tif re.search('[^a-zA-Z0-9]', token):\\n\\t\\treturn _get_new_csrf_token()\\n\\telif len(token) == CSRF_TOKEN_LENGTH:\\n\\t\\treturn token\\n\\telif len(token) == CSRF_SECRET_LENGTH:\\n\\t\\treturn _mask_cipher_secret(token)\\n\\treturn _get_new_csrf_token()"
  },
  {
    "code": "def context_diff(a, b, fromfile='', tofile='',\\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\\n    r\\n    started = False\\n    prefixmap = {'insert':'+ ', 'delete':'- ', 'replace':'! ', 'equal':'  '}\\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\\n        if not started:\\n            yield '*** %s %s%s' % (fromfile, fromfiledate, lineterm)\\n            yield '--- %s %s%s' % (tofile, tofiledate, lineterm)\\n            started = True\\n        yield '***************%s' % (lineterm,)\\n        if group[-1][2] - group[0][1] >= 2:\\n            yield '*** %d,%d ****%s' % (group[0][1]+1, group[-1][2], lineterm)\\n        else:\\n            yield '*** %d ****%s' % (group[-1][2], lineterm)\\n        visiblechanges = [e for e in group if e[0] in ('replace', 'delete')]\\n        if visiblechanges:\\n            for tag, i1, i2, _, _ in group:\\n                if tag != 'insert':\\n                    for line in a[i1:i2]:\\n                        yield prefixmap[tag] + line\\n        if group[-1][4] - group[0][3] >= 2:\\n            yield '--- %d,%d ----%s' % (group[0][3]+1, group[-1][4], lineterm)\\n        else:\\n            yield '--- %d ----%s' % (group[-1][4], lineterm)\\n        visiblechanges = [e for e in group if e[0] in ('replace', 'insert')]\\n        if visiblechanges:\\n            for tag, _, _, j1, j2 in group:\\n                if tag != 'delete':\\n                    for line in b[j1:j2]:\\n                        yield prefixmap[tag] + line",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue 11747: Fix output format for context diffs.",
    "fixed_code": "def context_diff(a, b, fromfile='', tofile='',\\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\\n    r\\n    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')\\n    started = False\\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\\n        if not started:\\n            started = True\\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\\n            yield '*** {}{}{}'.format(fromfile, fromdate, lineterm)\\n            yield '--- {}{}{}'.format(tofile, todate, lineterm)\\n        first, last = group[0], group[-1]\\n        yield '***************' + lineterm\\n        file1_range = _format_range_context(first[1], last[2])\\n        yield '*** {} ****{}'.format(file1_range, lineterm)\\n        if any(tag in {'replace', 'delete'} for tag, _, _, _, _ in group):\\n            for tag, i1, i2, _, _ in group:\\n                if tag != 'insert':\\n                    for line in a[i1:i2]:\\n                        yield prefix[tag] + line\\n        file2_range = _format_range_context(first[3], last[4])\\n        yield '--- {} ----{}'.format(file2_range, lineterm)\\n        if any(tag in {'replace', 'insert'} for tag, _, _, _, _ in group):\\n            for tag, _, _, j1, j2 in group:\\n                if tag != 'delete':\\n                    for line in b[j1:j2]:\\n                        yield prefix[tag] + line"
  },
  {
    "code": "def run(self, tmp=None, task_vars=None):\\n        changed = True\\n        socket_path = None\\n        play_context = copy.deepcopy(self._play_context)\\n        play_context.network_os = self._get_network_os(task_vars)\\n        result = super(ActionModule, self).run(task_vars=task_vars)\\n        if play_context.connection != 'network_cli':\\n            result['failed'] = True\\n            result['msg'] = ('please use network_cli connection type for net_put module')\\n            return result\\n        try:\\n            src = self._task.args.get('src')\\n        except KeyError as exc:\\n            return {'failed': True, 'msg': 'missing required argument: %s' % exc}\\n        src_file_path_name = src\\n        dest = self._task.args.get('dest')\\n        proto = self._task.args.get('protocol')\\n        if proto is None:\\n            proto = 'scp'\\n        mode = self._task.args.get('mode')\\n        if mode is None:\\n            mode = 'binary'\\n        if mode == 'text':\\n            try:\\n                self._handle_template(convert_data=False)\\n            except ValueError as exc:\\n                return dict(failed=True, msg=to_text(exc))\\n            src = self._task.args.get('src')\\n            filename = str(uuid.uuid4())\\n            cwd = self._loader.get_basedir()\\n            output_file = os.path.join(cwd, filename)\\n            try:\\n                with open(output_file, 'wb') as f:\\n                    f.write(to_bytes(src, encoding='utf-8'))\\n            except Exception:\\n                os.remove(output_file)\\n                raise\\n        else:\\n            try:\\n                output_file = self._get_binary_src_file(src)\\n            except ValueError as exc:\\n                return dict(failed=True, msg=to_text(exc))\\n        if socket_path is None:\\n            socket_path = self._connection.socket_path\\n        conn = Connection(socket_path)\\n        sock_timeout = conn.get_option('persistent_command_timeout')\\n        if dest is None:\\n            dest = src_file_path_name\\n        try:\\n            changed = self._handle_existing_file(conn, output_file, dest, proto, sock_timeout)\\n            if changed is False:\\n                result['changed'] = False\\n                result['destination'] = dest\\n                return result\\n        except Exception as exc:\\n            result['msg'] = ('Warning: Exc %s idempotency check failed. Check'\\n                             'dest' % exc)\\n        try:\\n            out = conn.copy_file(\\n                source=output_file, destination=dest,\\n                proto=proto, timeout=sock_timeout\\n            )\\n        except Exception as exc:\\n            if to_text(exc) == \"No response from server\":\\n                if play_context.network_os == 'iosxr':\\n                    result['msg'] = 'Warning: iosxr scp server pre close issue. Please check dest'\\n            else:\\n                result['failed'] = True\\n                result['msg'] = ('Exception received : %s' % exc)\\n        if mode == 'text':\\n            os.remove(output_file)\\n        result['changed'] = changed\\n        result['destination'] = dest\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Allow the use of _paramiko_conn even if the connection hasn't been started. (#61570)\\n\\n\\nI'm not sure what the benefit is of Noneing paramiko_conn on close, but will keep for now",
    "fixed_code": "def run(self, tmp=None, task_vars=None):\\n        socket_path = None\\n        play_context = copy.deepcopy(self._play_context)\\n        play_context.network_os = self._get_network_os(task_vars)\\n        result = super(ActionModule, self).run(task_vars=task_vars)\\n        if play_context.connection != 'network_cli':\\n            result['failed'] = True\\n            result['msg'] = ('please use network_cli connection type for net_put module')\\n            return result\\n        try:\\n            src = self._task.args['src']\\n        except KeyError as exc:\\n            return {'failed': True, 'msg': 'missing required argument: %s' % exc}\\n        src_file_path_name = src\\n        dest = self._task.args.get('dest')\\n        proto = self._task.args.get('protocol')\\n        if proto is None:\\n            proto = 'scp'\\n        mode = self._task.args.get('mode')\\n        if mode is None:\\n            mode = 'binary'\\n        if mode == 'text':\\n            try:\\n                self._handle_template(convert_data=False)\\n            except ValueError as exc:\\n                return dict(failed=True, msg=to_text(exc))\\n            src = self._task.args.get('src')\\n            filename = str(uuid.uuid4())\\n            cwd = self._loader.get_basedir()\\n            output_file = os.path.join(cwd, filename)\\n            try:\\n                with open(output_file, 'wb') as f:\\n                    f.write(to_bytes(src, encoding='utf-8'))\\n            except Exception:\\n                os.remove(output_file)\\n                raise\\n        else:\\n            try:\\n                output_file = self._get_binary_src_file(src)\\n            except ValueError as exc:\\n                return dict(failed=True, msg=to_text(exc))\\n        if socket_path is None:\\n            socket_path = self._connection.socket_path\\n        conn = Connection(socket_path)\\n        sock_timeout = conn.get_option('persistent_command_timeout')\\n        if dest is None:\\n            dest = src_file_path_name\\n        try:\\n            changed = self._handle_existing_file(conn, output_file, dest, proto, sock_timeout)\\n            if changed is False:\\n                result['changed'] = changed\\n                result['destination'] = dest\\n                return result\\n        except Exception as exc:\\n            result['msg'] = ('Warning: %s idempotency check failed. Check dest' % exc)\\n        try:\\n            conn.copy_file(\\n                source=output_file, destination=dest,\\n                proto=proto, timeout=sock_timeout\\n            )\\n        except Exception as exc:\\n            if to_text(exc) == \"No response from server\":\\n                if play_context.network_os == 'iosxr':\\n                    result['msg'] = 'Warning: iosxr scp server pre close issue. Please check dest'\\n            else:\\n                result['failed'] = True\\n                result['msg'] = 'Exception received: %s' % exc\\n        if mode == 'text':\\n            os.remove(output_file)\\n        result['changed'] = changed\\n        result['destination'] = dest\\n        return result"
  },
  {
    "code": "def _train_tree(X, y, i, r, t, random_state):\\n    X = np.asanyarray(X, dtype=np.float64, order='C')\\n    n_samples, n_features = X.shape\\n    y = np.asanyarray(y, dtype=np.float64, order='C')\\n    tree = copy.copy(t)\\n    if r <= 0 or r > 1 :\\n        raise ValueError(\"r must be in 0 <= r < 1.\\n\" +\\n                         \"r is %s\" % r)\\n    n = np.ceil(r * n_samples)\\n    sample_mask = np.zeros(n_samples, dtype=np.bool)\\n    permutation = random_state.permutation(n_samples)\\n    ind_in = permutation[-n:]\\n    sample_mask[ind_in] = True\\n    X_in = X[sample_mask]\\n    X_out = X[~sample_mask]\\n    y_in = y[sample_mask]\\n    y_out = y[~sample_mask]\\n    tree.fit(X_in, y_in)\\n    return tree",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "replaced ratio r with sampling with replacement",
    "fixed_code": "def _train_tree(X, y, i, t, random_state):\\n    X = np.asanyarray(X, dtype=DTYPE, order='F')\\n    n_samples, n_features = X.shape\\n    y = np.asanyarray(y, dtype=DTYPE, order='C')\\n    tree = copy.copy(t)\\n    in_indices = np.random.randint(0, n_samples, n_samples)\\n    out = np.bincount(in_indices) == 0\\n    X_in = X[in_indices]\\n    y_in = y[in_indices]\\n    X_out = X[out]\\n    y_out = y[out]\\n    tree.fit(X_in, y_in)\\n    return tree"
  },
  {
    "code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    def _parse_xls(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              chunksize=None):\\n        from datetime import MINYEAR, time, datetime\\n        from xlrd import xldate_as_tuple, XL_CELL_DATE\\n        datemode = self.book.datemode\\n        sheet = self.book.sheet_by_name(sheetname)\\n        data = []\\n        for i in range(sheet.nrows):\\n            row = []\\n            for value, typ in izip(sheet.row_values(i), sheet.row_types(i)):\\n                if typ == XL_CELL_DATE:\\n                    dt = xldate_as_tuple(value, datemode)\\n                    if dt[0] < MINYEAR: \\n                        value = time(*dt[3:])\\n                    else:\\n                        value = datetime(*dt)\\n                row.append(value)\\n            data.append(row)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: parsing numbers with commas in read_csv/table/clipboard/fwf #796 VB: read_csv with/without thousands separator parsing",
    "fixed_code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_dates=False, date_parser=None, na_values=None,\\n                    thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    def _parse_xls(self, sheetname, header=0, skiprows=None, index_col=None,\\n                   parse_dates=False, date_parser=None, na_values=None,\\n                   thousands=None, chunksize=None):\\n        from datetime import MINYEAR, time, datetime\\n        from xlrd import xldate_as_tuple, XL_CELL_DATE\\n        datemode = self.book.datemode\\n        sheet = self.book.sheet_by_name(sheetname)\\n        data = []\\n        for i in range(sheet.nrows):\\n            row = []\\n            for value, typ in izip(sheet.row_values(i), sheet.row_types(i)):\\n                if typ == XL_CELL_DATE:\\n                    dt = xldate_as_tuple(value, datemode)\\n                    if dt[0] < MINYEAR: \\n                        value = time(*dt[3:])\\n                    else:\\n                        value = datetime(*dt)\\n                row.append(value)\\n            data.append(row)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    @property"
  },
  {
    "code": "def __init__(self, n_comp=None, copy=True, do_fast_svd=False,\\n                 iterated_power=3):\\n        self.n_comp = n_comp\\n        self.copy = copy\\n        self.do_fast_svd = do_fast_svd\\n        self.iterated_power = iterated_power",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: make the PCA transformer perform variance scaling by default + update the face recognition accordingly",
    "fixed_code": "def __init__(self, n_comp=None, copy=True, do_fast_svd=False,\\n                 iterated_power=3, whiten=True):\\n        self.n_comp = n_comp\\n        self.copy = copy\\n        self.do_fast_svd = do_fast_svd\\n        self.iterated_power = iterated_power\\n        self.whiten = whiten"
  },
  {
    "code": "def astype_array_safe(\\n    values: ArrayLike, dtype, copy: bool = False, errors: IgnoreRaise = \"raise\"\\n) -> ArrayLike:\\n    errors_legal_values = (\"raise\", \"ignore\")\\n    if errors not in errors_legal_values:\\n        invalid_arg = (\\n            \"Expected value of kwarg 'errors' to be one of \"\\n            f\"{list(errors_legal_values)}. Supplied value is '{errors}'\"\\n        )\\n        raise ValueError(invalid_arg)\\n    if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\\n        msg = (\\n            f\"Expected an instance of {dtype.__name__}, \"\\n            \"but got the class instead. Try instantiating 'dtype'.\"\\n        )\\n        raise TypeError(msg)\\n    dtype = pandas_dtype(dtype)\\n    if isinstance(dtype, PandasDtype):\\n        dtype = dtype.numpy_dtype\\n    if (\\n        is_datetime64_dtype(values.dtype)\\n        and isinstance(dtype, np.dtype)\\n        and dtype.kind == \"M\"\\n        and not is_unitless(dtype)\\n        and not is_dtype_equal(dtype, values.dtype)\\n    ):\\n        return values.copy()\\n    try:\\n        new_values = astype_array(values, dtype, copy=copy)\\n    except (ValueError, TypeError):\\n        if errors == \"ignore\":\\n            new_values = values\\n        else:\\n            raise\\n    return new_values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: .astype to non-nano return the specified dtype (#48928)",
    "fixed_code": "def astype_array_safe(\\n    values: ArrayLike, dtype, copy: bool = False, errors: IgnoreRaise = \"raise\"\\n) -> ArrayLike:\\n    errors_legal_values = (\"raise\", \"ignore\")\\n    if errors not in errors_legal_values:\\n        invalid_arg = (\\n            \"Expected value of kwarg 'errors' to be one of \"\\n            f\"{list(errors_legal_values)}. Supplied value is '{errors}'\"\\n        )\\n        raise ValueError(invalid_arg)\\n    if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\\n        msg = (\\n            f\"Expected an instance of {dtype.__name__}, \"\\n            \"but got the class instead. Try instantiating 'dtype'.\"\\n        )\\n        raise TypeError(msg)\\n    dtype = pandas_dtype(dtype)\\n    if isinstance(dtype, PandasDtype):\\n        dtype = dtype.numpy_dtype\\n    if (\\n        is_datetime64_dtype(values.dtype)\\n        and isinstance(dtype, np.dtype)\\n        and dtype.kind == \"M\"\\n        and not is_unitless(dtype)\\n        and not is_dtype_equal(dtype, values.dtype)\\n        and not is_supported_unit(get_unit_from_dtype(dtype))\\n    ):\\n        return np.asarray(values).astype(dtype)\\n    try:\\n        new_values = astype_array(values, dtype, copy=copy)\\n    except (ValueError, TypeError):\\n        if errors == \"ignore\":\\n            new_values = values\\n        else:\\n            raise\\n    return new_values"
  },
  {
    "code": "def append(self, to_append, ignore_index=False, verify_integrity=False):\\n\\t\\tfrom pandas.core.reshape.concat import concat\\n\\t\\tif isinstance(to_append, (list, tuple)):\\n\\t\\t\\tto_concat = [self] + to_append\\n\\t\\telse:\\n\\t\\t\\tto_concat = [self, to_append]\\n\\t\\treturn concat(\\n\\t\\t\\tto_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\\n\\t\\t)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix Series.append raises TypeError with tuple of Series (#28412)",
    "fixed_code": "def append(self, to_append, ignore_index=False, verify_integrity=False):\\n\\t\\tfrom pandas.core.reshape.concat import concat\\n\\t\\tif isinstance(to_append, (list, tuple)):\\n\\t\\t\\tto_concat = [self]\\n\\t\\t\\tto_concat.extend(to_append)\\n\\t\\telse:\\n\\t\\t\\tto_concat = [self, to_append]\\n\\t\\treturn concat(\\n\\t\\t\\tto_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\\n\\t\\t)"
  },
  {
    "code": "def get_work(self, worker, host=None, assistant=False, **kwargs):\\n\\t\\tself.update(worker, {'host': host})\\n\\t\\tif assistant:\\n\\t\\t\\tself.add_worker(worker, [('assistant', assistant)])\\n\\t\\tbest_task = None\\n\\t\\tlocally_pending_tasks = 0\\n\\t\\trunning_tasks = []\\n\\t\\tused_resources = self._used_resources()\\n\\t\\tgreedy_resources = collections.defaultdict(int)\\n\\t\\tn_unique_pending = 0\\n\\t\\tgreedy_workers = dict((worker.id, worker.info.get('workers', 1))\\n\\t\\t\\t\\t\\t\\t\\t  for worker in self._state.get_active_workers())\\n\\t\\ttasks = list(self._state.get_pending_tasks())\\n\\t\\ttasks.sort(key=self._rank(), reverse=True)\\n\\t\\tfor task in tasks:\\n\\t\\t\\tin_workers = assistant or worker in task.workers\\n\\t\\t\\tif task.status == 'RUNNING' and in_workers:\\n\\t\\t\\t\\tother_worker = self._state.get_worker(task.worker_running)\\n\\t\\t\\t\\tmore_info = {'task_id': task.id, 'worker': str(other_worker)}\\n\\t\\t\\t\\tif other_worker is not None:\\n\\t\\t\\t\\t\\tmore_info.update(other_worker.info)\\n\\t\\t\\t\\t\\trunning_tasks.append(more_info)\\n\\t\\t\\tif task.status == PENDING and in_workers:\\n\\t\\t\\t\\tlocally_pending_tasks += 1\\n\\t\\t\\t\\tif len(task.workers) == 1 and not assistant:\\n\\t\\t\\t\\t\\tn_unique_pending += 1\\n\\t\\t\\tif task.status == RUNNING and (task.worker_running in greedy_workers):\\n\\t\\t\\t\\tgreedy_workers[task.worker_running] -= 1\\n\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\tif not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\\n\\t\\t\\t\\tif in_workers and self._has_resources(task.resources, used_resources):\\n\\t\\t\\t\\t\\tbest_task = task\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tworkers = itertools.chain(task.workers, [worker]) if assistant else task.workers\\n\\t\\t\\t\\t\\tfor task_worker in workers:\\n\\t\\t\\t\\t\\t\\tif greedy_workers.get(task_worker, 0) > 0:\\n\\t\\t\\t\\t\\t\\t\\tgreedy_workers[task_worker] -= 1\\n\\t\\t\\t\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treply = {'n_pending_tasks': locally_pending_tasks,\\n\\t\\t\\t\\t 'running_tasks': running_tasks,\\n\\t\\t\\t\\t 'task_id': None,\\n\\t\\t\\t\\t 'n_unique_pending': n_unique_pending}\\n\\t\\tif best_task:\\n\\t\\t\\tself._state.set_status(best_task, RUNNING, self._config)\\n\\t\\t\\tbest_task.worker_running = worker\\n\\t\\t\\tbest_task.time_running = time.time()\\n\\t\\t\\tself._update_task_history(best_task.id, RUNNING, host=host)\\n\\t\\t\\treply['task_id'] = best_task.id\\n\\t\\t\\treply['task_family'] = best_task.family\\n\\t\\t\\treply['task_module'] = getattr(best_task, 'module', None)\\n\\t\\t\\treply['task_params'] = best_task.params\\n\\t\\treturn reply",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Prevents non-runnable tasks from going to assistants\\n\\nI was seeing a lot of errors from assistants trying to run external tasks and\\nbeing unable to even find and load the class. It turns out assistants were\\nbeing treated as potential workers even by tasks with no workers. As a partial\\nfix, assistants can now only work on tasks that some worker is capable of.\\n\\nThis fix is great for workflows where external tasks only exist to check that\\nnon-luigi jobs are complete, but won't work for tasks that are external to the\\nassistant but not to all workers.",
    "fixed_code": "def get_work(self, worker, host=None, assistant=False, **kwargs):\\n\\t\\tself.update(worker, {'host': host})\\n\\t\\tif assistant:\\n\\t\\t\\tself.add_worker(worker, [('assistant', assistant)])\\n\\t\\tbest_task = None\\n\\t\\tlocally_pending_tasks = 0\\n\\t\\trunning_tasks = []\\n\\t\\tused_resources = self._used_resources()\\n\\t\\tgreedy_resources = collections.defaultdict(int)\\n\\t\\tn_unique_pending = 0\\n\\t\\tgreedy_workers = dict((worker.id, worker.info.get('workers', 1))\\n\\t\\t\\t\\t\\t\\t\\t  for worker in self._state.get_active_workers())\\n\\t\\ttasks = list(self._state.get_pending_tasks())\\n\\t\\ttasks.sort(key=self._rank(), reverse=True)\\n\\t\\tfor task in tasks:\\n\\t\\t\\tin_workers = (assistant and task.workers) or worker in task.workers\\n\\t\\t\\tif task.status == 'RUNNING' and in_workers:\\n\\t\\t\\t\\tother_worker = self._state.get_worker(task.worker_running)\\n\\t\\t\\t\\tmore_info = {'task_id': task.id, 'worker': str(other_worker)}\\n\\t\\t\\t\\tif other_worker is not None:\\n\\t\\t\\t\\t\\tmore_info.update(other_worker.info)\\n\\t\\t\\t\\t\\trunning_tasks.append(more_info)\\n\\t\\t\\tif task.status == PENDING and in_workers:\\n\\t\\t\\t\\tlocally_pending_tasks += 1\\n\\t\\t\\t\\tif len(task.workers) == 1 and not assistant:\\n\\t\\t\\t\\t\\tn_unique_pending += 1\\n\\t\\t\\tif task.status == RUNNING and (task.worker_running in greedy_workers):\\n\\t\\t\\t\\tgreedy_workers[task.worker_running] -= 1\\n\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\tif not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\\n\\t\\t\\t\\tif in_workers and self._has_resources(task.resources, used_resources):\\n\\t\\t\\t\\t\\tbest_task = task\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tworkers = itertools.chain(task.workers, [worker]) if assistant else task.workers\\n\\t\\t\\t\\t\\tfor task_worker in workers:\\n\\t\\t\\t\\t\\t\\tif greedy_workers.get(task_worker, 0) > 0:\\n\\t\\t\\t\\t\\t\\t\\tgreedy_workers[task_worker] -= 1\\n\\t\\t\\t\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treply = {'n_pending_tasks': locally_pending_tasks,\\n\\t\\t\\t\\t 'running_tasks': running_tasks,\\n\\t\\t\\t\\t 'task_id': None,\\n\\t\\t\\t\\t 'n_unique_pending': n_unique_pending}\\n\\t\\tif best_task:\\n\\t\\t\\tself._state.set_status(best_task, RUNNING, self._config)\\n\\t\\t\\tbest_task.worker_running = worker\\n\\t\\t\\tbest_task.time_running = time.time()\\n\\t\\t\\tself._update_task_history(best_task.id, RUNNING, host=host)\\n\\t\\t\\treply['task_id'] = best_task.id\\n\\t\\t\\treply['task_family'] = best_task.family\\n\\t\\t\\treply['task_module'] = getattr(best_task, 'module', None)\\n\\t\\t\\treply['task_params'] = best_task.params\\n\\t\\treturn reply"
  },
  {
    "code": "def values(self):\\n        lis = []\\n        for each in self.dict.values():\\n            if len( each ) == 1 :\\n                lis.append(each[0])\\n            else: lis.append(each)\\n        return lis",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def values(self):\\n        lis = []\\n        for each in self.dict.values():\\n            if len( each ) == 1 :\\n                lis.append(each[0])\\n            else: lis.append(each)\\n        return lis"
  },
  {
    "code": "def check_all_models(app_configs=None, **kwargs):\\n    db_table_models = defaultdict(list)\\n    indexes = defaultdict(list)\\n    constraints = defaultdict(list)\\n    errors = []\\n    if app_configs is None:\\n        models = apps.get_models()\\n    else:\\n        models = chain.from_iterable(app_config.get_models() for app_config in app_configs)\\n    for model in models:\\n        if model._meta.managed and not model._meta.proxy:\\n            db_table_models[model._meta.db_table].append(model._meta.label)\\n        if not inspect.ismethod(model.check):\\n            errors.append(\\n                Error(\\n                    \"The '%s.check()' class method is currently overridden by %r.\"\\n                    % (model.__name__, model.check),\\n                    obj=model,\\n                    id='models.E020'\\n                )\\n            )\\n        else:\\n            errors.extend(model.check(**kwargs))\\n        for model_index in model._meta.indexes:\\n            indexes[model_index.name].append(model._meta.label)\\n        for model_constraint in model._meta.constraints:\\n            constraints[model_constraint.name].append(model._meta.label)\\n    for db_table, model_labels in db_table_models.items():\\n        if len(model_labels) != 1:\\n            errors.append(\\n                Error(\\n                    \"db_table '%s' is used by multiple models: %s.\"\\n                    % (db_table, ', '.join(db_table_models[db_table])),\\n                    obj=db_table,\\n                    id='models.E028',\\n                )\\n            )\\n    for index_name, model_labels in indexes.items():\\n        if len(model_labels) > 1:\\n            model_labels = set(model_labels)\\n            errors.append(\\n                Error(\\n                    \"index name '%s' is not unique %s %s.\" % (\\n                        index_name,\\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\\n                        ', '.join(sorted(model_labels)),\\n                    ),\\n                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\\n                ),\\n            )\\n    for constraint_name, model_labels in constraints.items():\\n        if len(model_labels) > 1:\\n            model_labels = set(model_labels)\\n            errors.append(\\n                Error(\\n                    \"constraint name '%s' is not unique %s %s.\" % (\\n                        constraint_name,\\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\\n                        ', '.join(sorted(model_labels)),\\n                    ),\\n                    id='models.E031' if len(model_labels) == 1 else 'models.E032',\\n                ),\\n            )\\n    return errors",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #30673 -- Relaxed system check for db_table collision when database routers are installed by turning the error into a warning.",
    "fixed_code": "def check_all_models(app_configs=None, **kwargs):\\n    db_table_models = defaultdict(list)\\n    indexes = defaultdict(list)\\n    constraints = defaultdict(list)\\n    errors = []\\n    if app_configs is None:\\n        models = apps.get_models()\\n    else:\\n        models = chain.from_iterable(app_config.get_models() for app_config in app_configs)\\n    for model in models:\\n        if model._meta.managed and not model._meta.proxy:\\n            db_table_models[model._meta.db_table].append(model._meta.label)\\n        if not inspect.ismethod(model.check):\\n            errors.append(\\n                Error(\\n                    \"The '%s.check()' class method is currently overridden by %r.\"\\n                    % (model.__name__, model.check),\\n                    obj=model,\\n                    id='models.E020'\\n                )\\n            )\\n        else:\\n            errors.extend(model.check(**kwargs))\\n        for model_index in model._meta.indexes:\\n            indexes[model_index.name].append(model._meta.label)\\n        for model_constraint in model._meta.constraints:\\n            constraints[model_constraint.name].append(model._meta.label)\\n    if settings.DATABASE_ROUTERS:\\n        error_class, error_id = Warning, 'models.W035'\\n        error_hint = (\\n            'You have configured settings.DATABASE_ROUTERS. Verify that %s '\\n            'are correctly routed to separate databases.'\\n        )\\n    else:\\n        error_class, error_id = Error, 'models.E028'\\n        error_hint = None\\n    for db_table, model_labels in db_table_models.items():\\n        if len(model_labels) != 1:\\n            model_labels_str = ', '.join(model_labels)\\n            errors.append(\\n                error_class(\\n                    \"db_table '%s' is used by multiple models: %s.\"\\n                    % (db_table, model_labels_str),\\n                    obj=db_table,\\n                    hint=(error_hint % model_labels_str) if error_hint else None,\\n                    id=error_id,\\n                )\\n            )\\n    for index_name, model_labels in indexes.items():\\n        if len(model_labels) > 1:\\n            model_labels = set(model_labels)\\n            errors.append(\\n                Error(\\n                    \"index name '%s' is not unique %s %s.\" % (\\n                        index_name,\\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\\n                        ', '.join(sorted(model_labels)),\\n                    ),\\n                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\\n                ),\\n            )\\n    for constraint_name, model_labels in constraints.items():\\n        if len(model_labels) > 1:\\n            model_labels = set(model_labels)\\n            errors.append(\\n                Error(\\n                    \"constraint name '%s' is not unique %s %s.\" % (\\n                        constraint_name,\\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\\n                        ', '.join(sorted(model_labels)),\\n                    ),\\n                    id='models.E031' if len(model_labels) == 1 else 'models.E032',\\n                ),\\n            )\\n    return errors"
  },
  {
    "code": "def scale(X, axis=0, with_mean=True, with_std=True, copy=True):\\n    X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\\n                    warn_on_dtype=True, estimator='the scale function',\\n                    dtype=FLOAT_DTYPES)\\n    if sparse.issparse(X):\\n        if with_mean:\\n            raise ValueError(\\n                \"Cannot center sparse matrices: pass `with_mean=False` instead\"\\n                \" See docstring for motivation and alternatives.\")\\n        if axis != 0:\\n            raise ValueError(\"Can only scale sparse matrix on axis=0, \"\\n                             \" got axis=%d\" % axis)\\n        if with_std:\\n            _, var = mean_variance_axis(X, axis=0)\\n            var = _handle_zeros_in_scale(var, copy=False)\\n            inplace_column_scale(X, 1 / np.sqrt(var))\\n    else:\\n        X = np.asarray(X)\\n        if with_mean:\\n            mean_ = np.mean(X, axis)\\n        if with_std:\\n            scale_ = np.std(X, axis)\\n        Xr = np.rollaxis(X, axis)\\n        if with_mean:\\n            Xr -= mean_\\n            mean_1 = Xr.mean(axis=0)\\n            if not np.allclose(mean_1, 0):\\n                warnings.warn(\"Numerical issues were encountered \"\\n                              \"when centering the data \"\\n                              \"and might not be solved. Dataset may \"\\n                              \"contain too large values. You may need \"\\n                              \"to prescale your features.\")\\n                Xr -= mean_1\\n        if with_std:\\n            scale_ = _handle_zeros_in_scale(scale_, copy=False)\\n            Xr /= scale_\\n            if with_mean:\\n                mean_2 = Xr.mean(axis=0)\\n                if not np.allclose(mean_2, 0):\\n                    warnings.warn(\"Numerical issues were encountered \"\\n                                  \"when scaling the data \"\\n                                  \"and might not be solved. The standard \"\\n                                  \"deviation of the data is probably \"\\n                                  \"very close to 0. \")\\n                    Xr -= mean_2\\n    return X",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def scale(X, axis=0, with_mean=True, with_std=True, copy=True):\\n    X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\\n                    warn_on_dtype=True, estimator='the scale function',\\n                    dtype=FLOAT_DTYPES)\\n    if sparse.issparse(X):\\n        if with_mean:\\n            raise ValueError(\\n                \"Cannot center sparse matrices: pass `with_mean=False` instead\"\\n                \" See docstring for motivation and alternatives.\")\\n        if axis != 0:\\n            raise ValueError(\"Can only scale sparse matrix on axis=0, \"\\n                             \" got axis=%d\" % axis)\\n        if with_std:\\n            _, var = mean_variance_axis(X, axis=0)\\n            var = _handle_zeros_in_scale(var, copy=False)\\n            inplace_column_scale(X, 1 / np.sqrt(var))\\n    else:\\n        X = np.asarray(X)\\n        if with_mean:\\n            mean_ = np.mean(X, axis)\\n        if with_std:\\n            scale_ = np.std(X, axis)\\n        Xr = np.rollaxis(X, axis)\\n        if with_mean:\\n            Xr -= mean_\\n            mean_1 = Xr.mean(axis=0)\\n            if not np.allclose(mean_1, 0):\\n                warnings.warn(\"Numerical issues were encountered \"\\n                              \"when centering the data \"\\n                              \"and might not be solved. Dataset may \"\\n                              \"contain too large values. You may need \"\\n                              \"to prescale your features.\")\\n                Xr -= mean_1\\n        if with_std:\\n            scale_ = _handle_zeros_in_scale(scale_, copy=False)\\n            Xr /= scale_\\n            if with_mean:\\n                mean_2 = Xr.mean(axis=0)\\n                if not np.allclose(mean_2, 0):\\n                    warnings.warn(\"Numerical issues were encountered \"\\n                                  \"when scaling the data \"\\n                                  \"and might not be solved. The standard \"\\n                                  \"deviation of the data is probably \"\\n                                  \"very close to 0. \")\\n                    Xr -= mean_2\\n    return X"
  },
  {
    "code": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,\\n                     sample_weight=None, return_mean=False, check_input=True):\\n    if isinstance(sample_weight, numbers.Number):\\n        sample_weight = None\\n    if sample_weight is not None:\\n        sample_weight = np.asarray(sample_weight)\\n    if check_input:\\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\\n                        dtype=FLOAT_DTYPES)\\n    elif copy:\\n        if sp.issparse(X):\\n            X = X.copy()\\n        else:\\n            X = X.copy(order='K')\\n    y = np.asarray(y, dtype=X.dtype)\\n    if fit_intercept:\\n        if sp.issparse(X):\\n            X_offset, X_var = mean_variance_axis(\\n                X, axis=0, weights=sample_weight\\n            )\\n            if not return_mean:\\n                X_offset[:] = X.dtype.type(0)\\n        else:\\n            if normalize:\\n                X_offset, X_var, _ = _incremental_mean_and_var(\\n                    X, last_mean=0., last_variance=0., last_sample_count=0.,\\n                    sample_weight=sample_weight\\n                )\\n            else:\\n                X_offset = np.average(X, axis=0, weights=sample_weight)\\n            X_offset = X_offset.astype(X.dtype, copy=False)\\n            X -= X_offset\\n        if normalize:\\n            X_var = X_var.astype(X.dtype, copy=False)\\n            constant_mask = _is_constant_feature(X_var, X_offset, X.shape[0])\\n            X_var *= X.shape[0]\\n            X_scale = np.sqrt(X_var, out=X_var)\\n            X_scale[constant_mask] = 1.\\n            if sp.issparse(X):\\n                inplace_column_scale(X, 1. / X_scale)\\n            else:\\n                X /= X_scale\\n        else:\\n            X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        y_offset = np.average(y, axis=0, weights=sample_weight)\\n        y = y - y_offset\\n    else:\\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        if y.ndim == 1:\\n            y_offset = X.dtype.type(0)\\n        else:\\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\\n    return X, y, X_offset, y_offset, X_scale",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,\\n                     sample_weight=None, return_mean=False, check_input=True):\\n    if isinstance(sample_weight, numbers.Number):\\n        sample_weight = None\\n    if sample_weight is not None:\\n        sample_weight = np.asarray(sample_weight)\\n    if check_input:\\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\\n                        dtype=FLOAT_DTYPES)\\n    elif copy:\\n        if sp.issparse(X):\\n            X = X.copy()\\n        else:\\n            X = X.copy(order='K')\\n    y = np.asarray(y, dtype=X.dtype)\\n    if fit_intercept:\\n        if sp.issparse(X):\\n            X_offset, X_var = mean_variance_axis(\\n                X, axis=0, weights=sample_weight\\n            )\\n            if not return_mean:\\n                X_offset[:] = X.dtype.type(0)\\n        else:\\n            if normalize:\\n                X_offset, X_var, _ = _incremental_mean_and_var(\\n                    X, last_mean=0., last_variance=0., last_sample_count=0.,\\n                    sample_weight=sample_weight\\n                )\\n            else:\\n                X_offset = np.average(X, axis=0, weights=sample_weight)\\n            X_offset = X_offset.astype(X.dtype, copy=False)\\n            X -= X_offset\\n        if normalize:\\n            X_var = X_var.astype(X.dtype, copy=False)\\n            constant_mask = _is_constant_feature(X_var, X_offset, X.shape[0])\\n            X_var *= X.shape[0]\\n            X_scale = np.sqrt(X_var, out=X_var)\\n            X_scale[constant_mask] = 1.\\n            if sp.issparse(X):\\n                inplace_column_scale(X, 1. / X_scale)\\n            else:\\n                X /= X_scale\\n        else:\\n            X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        y_offset = np.average(y, axis=0, weights=sample_weight)\\n        y = y - y_offset\\n    else:\\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        if y.ndim == 1:\\n            y_offset = X.dtype.type(0)\\n        else:\\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\\n    return X, y, X_offset, y_offset, X_scale"
  },
  {
    "code": "def strip_tags(value):\\n    s = MLStripper()\\n    try:\\n        s.feed(value)\\n        s.close()\\n    except HTMLParseError:\\n        return value\\n    else:\\n        return s.get_data()\\nstrip_tags = allow_lazy(strip_tags)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Improved strip_tags and clarified documentation\\n\\nThe fact that strip_tags cannot guarantee to really strip all\\nnon-safe HTML content was not clear enough. Also see:",
    "fixed_code": "def strip_tags(value):\\n    while True:\\n        if not ('<' in value or '>' in value):\\n            return value\\n        new_value = _strip_once(value)\\n        if new_value == value:\\n            return value\\n        else:\\n            value = new_value\\nstrip_tags = allow_lazy(strip_tags)"
  },
  {
    "code": "def pivot_table(data, values=None, index=None, columns=None, aggfunc='mean',\\n                fill_value=None, margins=False, dropna=True,\\n                margins_name='All'):\\n    index = _convert_by(index)\\n    columns = _convert_by(columns)\\n    if isinstance(aggfunc, list):\\n        pieces = []\\n        keys = []\\n        for func in aggfunc:\\n            table = pivot_table(data, values=values, index=index,\\n                                columns=columns,\\n                                fill_value=fill_value, aggfunc=func,\\n                                margins=margins, margins_name=margins_name)\\n            pieces.append(table)\\n            keys.append(func.__name__)\\n        return concat(pieces, keys=keys, axis=1)\\n    keys = index + columns\\n    values_passed = values is not None\\n    if values_passed:\\n        if is_list_like(values):\\n            values_multi = True\\n            values = list(values)\\n        else:\\n            values_multi = False\\n            values = [values]\\n        for i in values:\\n            if i not in data:\\n                raise KeyError(i)\\n        to_filter = []\\n        for x in keys + values:\\n            if isinstance(x, Grouper):\\n                x = x.key\\n            try:\\n                if x in data:\\n                    to_filter.append(x)\\n            except TypeError:\\n                pass\\n        if len(to_filter) < len(data.columns):\\n            data = data[to_filter]\\n    else:\\n        values = data.columns\\n        for key in keys:\\n            try:\\n                values = values.drop(key)\\n            except (TypeError, ValueError):\\n                pass\\n        values = list(values)\\n    grouped = data.groupby(keys)\\n    agged = grouped.agg(aggfunc)\\n    table = agged\\n    if table.index.nlevels > 1:\\n        index_names = agged.index.names[:len(index)]\\n        to_unstack = []\\n        for i in range(len(index), len(keys)):\\n            name = agged.index.names[i]\\n            if name is None or name in index_names:\\n                to_unstack.append(i)\\n            else:\\n                to_unstack.append(name)\\n        table = agged.unstack(to_unstack)\\n    if not dropna:\\n        from pandas import MultiIndex\\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.index.levels),\\n                                       names=table.index.names)\\n            table = table.reindex(m, axis=0)\\n        except AttributeError:\\n            pass  \\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.columns.levels),\\n                                       names=table.columns.names)\\n            table = table.reindex(m, axis=1)\\n        except AttributeError:\\n            pass  \\n    if isinstance(table, ABCDataFrame):\\n        table = table.sort_index(axis=1)\\n    if fill_value is not None:\\n        table = table.fillna(value=fill_value, downcast='infer')\\n    if margins:\\n        if dropna:\\n            data = data[data.notna().all(axis=1)]\\n        table = _add_margins(table, data, values, rows=index,\\n                             cols=columns, aggfunc=aggfunc,\\n                             margins_name=margins_name, fill_value=fill_value)\\n    if values_passed and not values_multi and not table.empty and \\\\n       (table.columns.nlevels > 1):\\n        table = table[values[0]]\\n    if len(index) == 0 and len(columns) > 0:\\n        table = table.T\\n    if isinstance(table, ABCDataFrame) and dropna:\\n        table = table.dropna(how='all', axis=1)\\n    return table",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pivot_table strings as aggfunc (#18810)",
    "fixed_code": "def pivot_table(data, values=None, index=None, columns=None, aggfunc='mean',\\n                fill_value=None, margins=False, dropna=True,\\n                margins_name='All'):\\n    index = _convert_by(index)\\n    columns = _convert_by(columns)\\n    if isinstance(aggfunc, list):\\n        pieces = []\\n        keys = []\\n        for func in aggfunc:\\n            table = pivot_table(data, values=values, index=index,\\n                                columns=columns,\\n                                fill_value=fill_value, aggfunc=func,\\n                                margins=margins, margins_name=margins_name)\\n            pieces.append(table)\\n            keys.append(getattr(func, '__name__', func))\\n        return concat(pieces, keys=keys, axis=1)\\n    keys = index + columns\\n    values_passed = values is not None\\n    if values_passed:\\n        if is_list_like(values):\\n            values_multi = True\\n            values = list(values)\\n        else:\\n            values_multi = False\\n            values = [values]\\n        for i in values:\\n            if i not in data:\\n                raise KeyError(i)\\n        to_filter = []\\n        for x in keys + values:\\n            if isinstance(x, Grouper):\\n                x = x.key\\n            try:\\n                if x in data:\\n                    to_filter.append(x)\\n            except TypeError:\\n                pass\\n        if len(to_filter) < len(data.columns):\\n            data = data[to_filter]\\n    else:\\n        values = data.columns\\n        for key in keys:\\n            try:\\n                values = values.drop(key)\\n            except (TypeError, ValueError):\\n                pass\\n        values = list(values)\\n    grouped = data.groupby(keys)\\n    agged = grouped.agg(aggfunc)\\n    table = agged\\n    if table.index.nlevels > 1:\\n        index_names = agged.index.names[:len(index)]\\n        to_unstack = []\\n        for i in range(len(index), len(keys)):\\n            name = agged.index.names[i]\\n            if name is None or name in index_names:\\n                to_unstack.append(i)\\n            else:\\n                to_unstack.append(name)\\n        table = agged.unstack(to_unstack)\\n    if not dropna:\\n        from pandas import MultiIndex\\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.index.levels),\\n                                       names=table.index.names)\\n            table = table.reindex(m, axis=0)\\n        except AttributeError:\\n            pass  \\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.columns.levels),\\n                                       names=table.columns.names)\\n            table = table.reindex(m, axis=1)\\n        except AttributeError:\\n            pass  \\n    if isinstance(table, ABCDataFrame):\\n        table = table.sort_index(axis=1)\\n    if fill_value is not None:\\n        table = table.fillna(value=fill_value, downcast='infer')\\n    if margins:\\n        if dropna:\\n            data = data[data.notna().all(axis=1)]\\n        table = _add_margins(table, data, values, rows=index,\\n                             cols=columns, aggfunc=aggfunc,\\n                             margins_name=margins_name, fill_value=fill_value)\\n    if values_passed and not values_multi and not table.empty and \\\\n       (table.columns.nlevels > 1):\\n        table = table[values[0]]\\n    if len(index) == 0 and len(columns) > 0:\\n        table = table.T\\n    if isinstance(table, ABCDataFrame) and dropna:\\n        table = table.dropna(how='all', axis=1)\\n    return table"
  },
  {
    "code": "def _is_label_like(val):\\n    return (isinstance(val, compat.string_types) or\\n            (val is not None and is_scalar(val)))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: raise error for groupby() with invalid tuple key (#18826)\\n\\ncloses #18798",
    "fixed_code": "def _is_label_like(val):\\n    return (isinstance(val, (compat.string_types, tuple)) or\\n            (val is not None and is_scalar(val)))"
  },
  {
    "code": "def __init__(self, *, fit_intercept=True, normalize=False, copy_X=True,\\n                 n_jobs=None):\\n        self.fit_intercept = fit_intercept\\n        self.normalize = normalize\\n        self.copy_X = copy_X\\n        self.n_jobs = n_jobs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, *, fit_intercept=True, normalize=False, copy_X=True,\\n                 n_jobs=None):\\n        self.fit_intercept = fit_intercept\\n        self.normalize = normalize\\n        self.copy_X = copy_X\\n        self.n_jobs = n_jobs"
  },
  {
    "code": "def __array_finalize__(self, obj):\\n        if not self.ndim:  \\n            return self.item()\\n        self.freq = getattr(obj, 'freq', None)\\n        self.name = getattr(obj, 'name', None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __array_finalize__(self, obj):\\n        if not self.ndim:  \\n            return self.item()\\n        self.freq = getattr(obj, 'freq', None)\\n        self.name = getattr(obj, 'name', None)"
  },
  {
    "code": "def radius_neighbors(\\n        self, X=None, radius=None, return_distance=True, sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if X is not None:\\n            query_is_train = False\\n            if self.metric == \"precomputed\":\\n                X = _check_precomputed(X)\\n            else:\\n                X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\\n        else:\\n            query_is_train = True\\n            X = self._fit_X\\n        if radius is None:\\n            radius = self.radius\\n        if self._fit_method == \"brute\" and self.metric == \"precomputed\" and issparse(X):\\n            results = _radius_neighbors_from_graph(\\n                X, radius=radius, return_distance=return_distance\\n            )\\n        elif self._fit_method == \"brute\":\\n            if self.effective_metric_ == \"euclidean\":\\n                radius *= radius\\n                kwds = {\"squared\": True}\\n            else:\\n                kwds = self.effective_metric_params_\\n            reduce_func = partial(\\n                self._radius_neighbors_reduce_func,\\n                radius=radius,\\n                return_distance=return_distance,\\n            )\\n            chunked_results = pairwise_distances_chunked(\\n                X,\\n                self._fit_X,\\n                reduce_func=reduce_func,\\n                metric=self.effective_metric_,\\n                n_jobs=self.n_jobs,\\n                **kwds,\\n            )\\n            if return_distance:\\n                neigh_dist_chunks, neigh_ind_chunks = zip(*chunked_results)\\n                neigh_dist_list = sum(neigh_dist_chunks, [])\\n                neigh_ind_list = sum(neigh_ind_chunks, [])\\n                neigh_dist = _to_object_array(neigh_dist_list)\\n                neigh_ind = _to_object_array(neigh_ind_list)\\n                results = neigh_dist, neigh_ind\\n            else:\\n                neigh_ind_list = sum(chunked_results, [])\\n                results = _to_object_array(neigh_ind_list)\\n            if sort_results:\\n                if not return_distance:\\n                    raise ValueError(\\n                        \"return_distance must be True if sort_results is True.\"\\n                    )\\n                for ii in range(len(neigh_dist)):\\n                    order = np.argsort(neigh_dist[ii], kind=\"mergesort\")\\n                    neigh_ind[ii] = neigh_ind[ii][order]\\n                    neigh_dist[ii] = neigh_dist[ii][order]\\n                results = neigh_dist, neigh_ind\\n        elif self._fit_method in [\"ball_tree\", \"kd_tree\"]:\\n            if issparse(X):\\n                raise ValueError(\\n                    \"%s does not work with sparse matrices. Densify the data, \"\\n                    \"or set algorithm='brute'\"\\n                    % self._fit_method\\n                )\\n            n_jobs = effective_n_jobs(self.n_jobs)\\n            delayed_query = delayed(_tree_query_radius_parallel_helper)\\n            if parse_version(joblib.__version__) < parse_version(\"0.12\"):\\n                parallel_kwargs = {\"backend\": \"threading\"}\\n            else:\\n                parallel_kwargs = {\"prefer\": \"threads\"}\\n            chunked_results = Parallel(n_jobs, **parallel_kwargs)(\\n                delayed_query(\\n                    self._tree, X[s], radius, return_distance, sort_results=sort_results\\n                )\\n                for s in gen_even_slices(X.shape[0], n_jobs)\\n            )\\n            if return_distance:\\n                neigh_ind, neigh_dist = tuple(zip(*chunked_results))\\n                results = np.hstack(neigh_dist), np.hstack(neigh_ind)\\n            else:\\n                results = np.hstack(chunked_results)\\n        else:\\n            raise ValueError(\"internal: _fit_method not recognized\")\\n        if not query_is_train:\\n            return results\\n        else:\\n            if return_distance:\\n                neigh_dist, neigh_ind = results\\n            else:\\n                neigh_ind = results\\n            for ind, ind_neighbor in enumerate(neigh_ind):\\n                mask = ind_neighbor != ind\\n                neigh_ind[ind] = ind_neighbor[mask]\\n                if return_distance:\\n                    neigh_dist[ind] = neigh_dist[ind][mask]\\n            if return_distance:\\n                return neigh_dist, neigh_ind\\n            return neigh_ind\\n    def radius_neighbors_graph(\\n        self, X=None, radius=None, mode=\"connectivity\", sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if radius is None:\\n            radius = self.radius\\n        if mode == \"connectivity\":\\n            A_ind = self.radius_neighbors(X, radius, return_distance=False)\\n            A_data = None\\n        elif mode == \"distance\":\\n            dist, A_ind = self.radius_neighbors(\\n                X, radius, return_distance=True, sort_results=sort_results\\n            )\\n            A_data = np.concatenate(list(dist))\\n        else:\\n            raise ValueError(\\n                'Unsupported mode, must be one of \"connectivity\", '\\n                'or \"distance\" but got %s instead' % mode\\n            )\\n        n_queries = A_ind.shape[0]\\n        n_samples_fit = self.n_samples_fit_\\n        n_neighbors = np.array([len(a) for a in A_ind])\\n        A_ind = np.concatenate(list(A_ind))\\n        if A_data is None:\\n            A_data = np.ones(len(A_ind))\\n        A_indptr = np.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))\\n        return csr_matrix((A_data, A_ind, A_indptr), shape=(n_queries, n_samples_fit))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def radius_neighbors(\\n        self, X=None, radius=None, return_distance=True, sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if X is not None:\\n            query_is_train = False\\n            if self.metric == \"precomputed\":\\n                X = _check_precomputed(X)\\n            else:\\n                X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\\n        else:\\n            query_is_train = True\\n            X = self._fit_X\\n        if radius is None:\\n            radius = self.radius\\n        if self._fit_method == \"brute\" and self.metric == \"precomputed\" and issparse(X):\\n            results = _radius_neighbors_from_graph(\\n                X, radius=radius, return_distance=return_distance\\n            )\\n        elif self._fit_method == \"brute\":\\n            if self.effective_metric_ == \"euclidean\":\\n                radius *= radius\\n                kwds = {\"squared\": True}\\n            else:\\n                kwds = self.effective_metric_params_\\n            reduce_func = partial(\\n                self._radius_neighbors_reduce_func,\\n                radius=radius,\\n                return_distance=return_distance,\\n            )\\n            chunked_results = pairwise_distances_chunked(\\n                X,\\n                self._fit_X,\\n                reduce_func=reduce_func,\\n                metric=self.effective_metric_,\\n                n_jobs=self.n_jobs,\\n                **kwds,\\n            )\\n            if return_distance:\\n                neigh_dist_chunks, neigh_ind_chunks = zip(*chunked_results)\\n                neigh_dist_list = sum(neigh_dist_chunks, [])\\n                neigh_ind_list = sum(neigh_ind_chunks, [])\\n                neigh_dist = _to_object_array(neigh_dist_list)\\n                neigh_ind = _to_object_array(neigh_ind_list)\\n                results = neigh_dist, neigh_ind\\n            else:\\n                neigh_ind_list = sum(chunked_results, [])\\n                results = _to_object_array(neigh_ind_list)\\n            if sort_results:\\n                if not return_distance:\\n                    raise ValueError(\\n                        \"return_distance must be True if sort_results is True.\"\\n                    )\\n                for ii in range(len(neigh_dist)):\\n                    order = np.argsort(neigh_dist[ii], kind=\"mergesort\")\\n                    neigh_ind[ii] = neigh_ind[ii][order]\\n                    neigh_dist[ii] = neigh_dist[ii][order]\\n                results = neigh_dist, neigh_ind\\n        elif self._fit_method in [\"ball_tree\", \"kd_tree\"]:\\n            if issparse(X):\\n                raise ValueError(\\n                    \"%s does not work with sparse matrices. Densify the data, \"\\n                    \"or set algorithm='brute'\"\\n                    % self._fit_method\\n                )\\n            n_jobs = effective_n_jobs(self.n_jobs)\\n            delayed_query = delayed(_tree_query_radius_parallel_helper)\\n            if parse_version(joblib.__version__) < parse_version(\"0.12\"):\\n                parallel_kwargs = {\"backend\": \"threading\"}\\n            else:\\n                parallel_kwargs = {\"prefer\": \"threads\"}\\n            chunked_results = Parallel(n_jobs, **parallel_kwargs)(\\n                delayed_query(\\n                    self._tree, X[s], radius, return_distance, sort_results=sort_results\\n                )\\n                for s in gen_even_slices(X.shape[0], n_jobs)\\n            )\\n            if return_distance:\\n                neigh_ind, neigh_dist = tuple(zip(*chunked_results))\\n                results = np.hstack(neigh_dist), np.hstack(neigh_ind)\\n            else:\\n                results = np.hstack(chunked_results)\\n        else:\\n            raise ValueError(\"internal: _fit_method not recognized\")\\n        if not query_is_train:\\n            return results\\n        else:\\n            if return_distance:\\n                neigh_dist, neigh_ind = results\\n            else:\\n                neigh_ind = results\\n            for ind, ind_neighbor in enumerate(neigh_ind):\\n                mask = ind_neighbor != ind\\n                neigh_ind[ind] = ind_neighbor[mask]\\n                if return_distance:\\n                    neigh_dist[ind] = neigh_dist[ind][mask]\\n            if return_distance:\\n                return neigh_dist, neigh_ind\\n            return neigh_ind\\n    def radius_neighbors_graph(\\n        self, X=None, radius=None, mode=\"connectivity\", sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if radius is None:\\n            radius = self.radius\\n        if mode == \"connectivity\":\\n            A_ind = self.radius_neighbors(X, radius, return_distance=False)\\n            A_data = None\\n        elif mode == \"distance\":\\n            dist, A_ind = self.radius_neighbors(\\n                X, radius, return_distance=True, sort_results=sort_results\\n            )\\n            A_data = np.concatenate(list(dist))\\n        else:\\n            raise ValueError(\\n                'Unsupported mode, must be one of \"connectivity\", '\\n                'or \"distance\" but got %s instead' % mode\\n            )\\n        n_queries = A_ind.shape[0]\\n        n_samples_fit = self.n_samples_fit_\\n        n_neighbors = np.array([len(a) for a in A_ind])\\n        A_ind = np.concatenate(list(A_ind))\\n        if A_data is None:\\n            A_data = np.ones(len(A_ind))\\n        A_indptr = np.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))\\n        return csr_matrix((A_data, A_ind, A_indptr), shape=(n_queries, n_samples_fit))"
  },
  {
    "code": "def serve_file(\\n\\trequest: HttpRequest,\\n\\tmaybe_user_profile: Union[UserProfile, AnonymousUser],\\n\\trealm_id_str: str,\\n\\tfilename: str,\\n\\turl_only: bool = False,\\n\\tdownload: bool = False,\\n) -> HttpResponseBase:\\n\\tpath_id = f\"{realm_id_str}/{filename}\"\\n\\trealm = get_valid_realm_from_request(request)\\n\\tis_authorized = validate_attachment_request(maybe_user_profile, path_id, realm)\\n\\tif is_authorized is None:\\n\\t\\treturn HttpResponseNotFound(_(\"<p>File not found.</p>\"))\\n\\tif not is_authorized:\\n\\t\\treturn HttpResponseForbidden(_(\"<p>You are not authorized to view this file.</p>\"))\\n\\tif url_only:\\n\\t\\turl = generate_unauthed_file_access_url(path_id)\\n\\t\\treturn json_success(request, data=dict(url=url))\\n\\tmimetype, encoding = guess_type(path_id)\\n\\tdownload = download or mimetype not in INLINE_MIME_TYPES\\n\\tif settings.LOCAL_UPLOADS_DIR is not None:\\n\\t\\treturn serve_local(request, path_id, download=download)\\n\\telse:\\n\\t\\treturn serve_s3(request, path_id, download=download)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def serve_file(\\n\\trequest: HttpRequest,\\n\\tmaybe_user_profile: Union[UserProfile, AnonymousUser],\\n\\trealm_id_str: str,\\n\\tfilename: str,\\n\\turl_only: bool = False,\\n\\tdownload: bool = False,\\n) -> HttpResponseBase:\\n\\tpath_id = f\"{realm_id_str}/{filename}\"\\n\\trealm = get_valid_realm_from_request(request)\\n\\tis_authorized = validate_attachment_request(maybe_user_profile, path_id, realm)\\n\\tif is_authorized is None:\\n\\t\\treturn HttpResponseNotFound(_(\"<p>File not found.</p>\"))\\n\\tif not is_authorized:\\n\\t\\treturn HttpResponseForbidden(_(\"<p>You are not authorized to view this file.</p>\"))\\n\\tif url_only:\\n\\t\\turl = generate_unauthed_file_access_url(path_id)\\n\\t\\treturn json_success(request, data=dict(url=url))\\n\\tmimetype, encoding = guess_type(path_id)\\n\\tdownload = download or mimetype not in INLINE_MIME_TYPES\\n\\tif settings.LOCAL_UPLOADS_DIR is not None:\\n\\t\\treturn serve_local(request, path_id, download=download)\\n\\telse:\\n\\t\\treturn serve_s3(request, path_id, download=download)"
  },
  {
    "code": "def split_line(\\n    line: Line, line_length: int, inner: bool = False, py36: bool = False\\n) -> Iterator[Line]:\\n    line_str = str(line).strip('\\n')\\n    if len(line_str) <= line_length and '\\n' not in line_str:\\n        yield line\\n        return\\n    if line.is_def:\\n        split_funcs = [left_hand_split]\\n    elif line.inside_brackets:\\n        split_funcs = [delimiter_split]\\n        if '\\n' not in line_str:\\n            split_funcs.append(right_hand_split)\\n    else:\\n        split_funcs = [right_hand_split]\\n    for split_func in split_funcs:\\n        result: List[Line] = []\\n        try:\\n            for l in split_func(line, py36=py36):\\n                if str(l).strip('\\n') == line_str:\\n                    raise CannotSplit(\"Split function returned an unchanged result\")\\n                result.extend(\\n                    split_line(l, line_length=line_length, inner=True, py36=py36)\\n                )\\n        except CannotSplit as cs:\\n            continue\\n        else:\\n            yield from result\\n            break\\n    else:\\n        yield line",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Implement `# fmt: off` and `# fmt: on`\\n\\nFixes #5",
    "fixed_code": "def split_line(\\n    line: Line, line_length: int, inner: bool = False, py36: bool = False\\n) -> Iterator[Line]:\\n    if isinstance(line, UnformattedLines):\\n        yield line\\n        return\\n    line_str = str(line).strip('\\n')\\n    if len(line_str) <= line_length and '\\n' not in line_str:\\n        yield line\\n        return\\n    if line.is_def:\\n        split_funcs = [left_hand_split]\\n    elif line.inside_brackets:\\n        split_funcs = [delimiter_split]\\n        if '\\n' not in line_str:\\n            split_funcs.append(right_hand_split)\\n    else:\\n        split_funcs = [right_hand_split]\\n    for split_func in split_funcs:\\n        result: List[Line] = []\\n        try:\\n            for l in split_func(line, py36=py36):\\n                if str(l).strip('\\n') == line_str:\\n                    raise CannotSplit(\"Split function returned an unchanged result\")\\n                result.extend(\\n                    split_line(l, line_length=line_length, inner=True, py36=py36)\\n                )\\n        except CannotSplit as cs:\\n            continue\\n        else:\\n            yield from result\\n            break\\n    else:\\n        yield line"
  },
  {
    "code": "def predict_proba(self, X):\\n        if isinstance(self.loss_function, Log) and \\\\n               self.classes.shape[0] == 2:\\n            return 1.0 / (1.0 + np.exp(-self.predict_margin(X)))\\n        else:\\n            raise NotImplementedError('%s loss does not provide \"\\\\n            \"this functionality' % self.loss)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def predict_proba(self, X):\\n        if isinstance(self.loss_function, Log) and \\\\n               self.classes.shape[0] == 2:\\n            return 1.0 / (1.0 + np.exp(-self.predict_margin(X)))\\n        else:\\n            raise NotImplementedError('%s loss does not provide \"\\\\n            \"this functionality' % self.loss)"
  },
  {
    "code": "def __init__(self, stream_or_string, **options):\\n        super(Deserializer, self).__init__(stream_or_string, **options)\\n        self.event_stream = pulldom.parse(self.stream, self._make_parser())\\n        self.db = options.pop('using', DEFAULT_DB_ALIAS)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, stream_or_string, **options):\\n        super(Deserializer, self).__init__(stream_or_string, **options)\\n        self.event_stream = pulldom.parse(self.stream, self._make_parser())\\n        self.db = options.pop('using', DEFAULT_DB_ALIAS)"
  },
  {
    "code": "def clean_interp_method(method, **kwargs):\\n    order = kwargs.get('order')\\n    valid = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\\n             'quadratic', 'cubic', 'barycentric', 'polynomial', 'krogh',\\n             'piecewise_polynomial', 'pchip', 'akima', 'spline']\\n    if method in ('spline', 'polynomial') and order is None:\\n        raise ValueError(\"You must specify the order of the spline or \"\\n                         \"polynomial.\")\\n    if method not in valid:\\n        raise ValueError(\"method must be one of {0}.\"\\n                         \"Got '{1}' instead.\".format(valid, method))\\n    return method",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: COMPAT:0.18 added scipy version check\\n\\ncloses #12887\\ncloses #13007",
    "fixed_code": "def clean_interp_method(method, **kwargs):\\n    order = kwargs.get('order')\\n    valid = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\\n             'quadratic', 'cubic', 'barycentric', 'polynomial', 'krogh',\\n             'piecewise_polynomial', 'pchip', 'akima', 'spline',\\n             'from_derivatives']\\n    if method in ('spline', 'polynomial') and order is None:\\n        raise ValueError(\"You must specify the order of the spline or \"\\n                         \"polynomial.\")\\n    if method not in valid:\\n        raise ValueError(\"method must be one of {0}.\"\\n                         \"Got '{1}' instead.\".format(valid, method))\\n    return method"
  },
  {
    "code": "def deprecate_kwarg(old_arg_name, new_arg_name, mapping=None, stacklevel=2):\\n    \"\"\"\\n    Decorator to deprecate a keyword argument of a function.\\n    Parameters\\n    ----------\\n    old_arg_name : str\\n        Name of argument in function to deprecate\\n    new_arg_name : str or None\\n        Name of preferred argument in function. Use None to raise warning that\\n        ``old_arg_name`` keyword is deprecated.\\n    mapping : dict or callable\\n        If mapping is present, use it to translate old arguments to\\n        new arguments. A callable must do its own value checking;\\n        values not found in a dict will be forwarded unchanged.\\n    Examples\\n    --------\\n    The following deprecates 'cols', using 'columns' instead\\n    >>> @deprecate_kwarg(old_arg_name='cols', new_arg_name='columns')\\n    ... def f(columns=''):\\n    ...     print(columns)\\n    ...\\n    >>> f(columns='should work ok')\\n    should work ok\\n    >>> f(cols='should raise warning')\\n    FutureWarning: cols is deprecated, use columns instead\\n      warnings.warn(msg, FutureWarning)\\n    should raise warning\\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")\\n    TypeError: Can only specify 'cols' or 'columns', not both\\n    >>> @deprecate_kwarg('old', 'new', {'yes': True, 'no': False})\\n    ... def f(new=False):\\n    ...     print('yes!' if new else 'no!')\\n    ...\\n    >>> f(old='yes')\\n    FutureWarning: old='yes' is deprecated, use new=True instead\\n      warnings.warn(msg, FutureWarning)\\n    yes!",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def deprecate_kwarg(old_arg_name, new_arg_name, mapping=None, stacklevel=2):\\n    \"\"\"\\n    Decorator to deprecate a keyword argument of a function.\\n    Parameters\\n    ----------\\n    old_arg_name : str\\n        Name of argument in function to deprecate\\n    new_arg_name : str or None\\n        Name of preferred argument in function. Use None to raise warning that\\n        ``old_arg_name`` keyword is deprecated.\\n    mapping : dict or callable\\n        If mapping is present, use it to translate old arguments to\\n        new arguments. A callable must do its own value checking;\\n        values not found in a dict will be forwarded unchanged.\\n    Examples\\n    --------\\n    The following deprecates 'cols', using 'columns' instead\\n    >>> @deprecate_kwarg(old_arg_name='cols', new_arg_name='columns')\\n    ... def f(columns=''):\\n    ...     print(columns)\\n    ...\\n    >>> f(columns='should work ok')\\n    should work ok\\n    >>> f(cols='should raise warning')\\n    FutureWarning: cols is deprecated, use columns instead\\n      warnings.warn(msg, FutureWarning)\\n    should raise warning\\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")\\n    TypeError: Can only specify 'cols' or 'columns', not both\\n    >>> @deprecate_kwarg('old', 'new', {'yes': True, 'no': False})\\n    ... def f(new=False):\\n    ...     print('yes!' if new else 'no!')\\n    ...\\n    >>> f(old='yes')\\n    FutureWarning: old='yes' is deprecated, use new=True instead\\n      warnings.warn(msg, FutureWarning)\\n    yes!"
  },
  {
    "code": "def _add_methods(cls, new_methods):\\n    for name, method in new_methods.items():\\n        force = not (issubclass(cls, ABCSparseArray) and name.startswith(\"__i\"))\\n        if force or name not in cls.__dict__:\\n            setattr(cls, name, method)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: remove ABCSparseArray (#33957)",
    "fixed_code": "def _add_methods(cls, new_methods):\\n    for name, method in new_methods.items():\\n        setattr(cls, name, method)"
  },
  {
    "code": "def url(parser, token):\\n    warnings.warn(\\n        \"Loading the `url` tag from the `future` library is deprecated and \"\\n        \"will be removed in Django 1.9. Use the default `url` tag instead.\",\\n        PendingDeprecationWarning)\\n    return defaulttags.url(parser, token)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def url(parser, token):\\n    warnings.warn(\\n        \"Loading the `url` tag from the `future` library is deprecated and \"\\n        \"will be removed in Django 1.9. Use the default `url` tag instead.\",\\n        PendingDeprecationWarning)\\n    return defaulttags.url(parser, token)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        gather_subset=dict(default=[\"!config\"], type='list')\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)\\n    gather_subset = module.params['gather_subset']\\n    runable_subsets = set()\\n    exclude_subsets = set()\\n    for subset in gather_subset:\\n        if subset == 'all':\\n            runable_subsets.update(VALID_SUBSETS)\\n            continue\\n        if subset.startswith('!'):\\n            subset = subset[1:]\\n            if subset == 'all':\\n                exclude_subsets.update(VALID_SUBSETS)\\n                continue\\n            exclude = True\\n        else:\\n            exclude = False\\n        if subset not in VALID_SUBSETS:\\n            module.fail_json(msg='Bad subset')\\n        if exclude:\\n            exclude_subsets.add(subset)\\n        else:\\n            runable_subsets.add(subset)\\n    if not runable_subsets:\\n        runable_subsets.update(VALID_SUBSETS)\\n    runable_subsets.difference_update(exclude_subsets)\\n    runable_subsets.add('default')\\n    facts = dict()\\n    facts['gather_subset'] = list(runable_subsets)\\n    instances = list()\\n    for key in runable_subsets:\\n        instances.append(FACT_SUBSETS[key](module))\\n    for inst in instances:\\n        inst.populate()\\n        facts.update(inst.facts)\\n    ansible_facts = dict()\\n    for key, value in iteritems(facts):\\n        key = 'ansible_net_%s' % key\\n        ansible_facts[key] = value\\n    warnings = list()\\n    module.exit_json(ansible_facts=ansible_facts, warnings=warnings)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        gather_subset=dict(default=[\"!config\"], type='list')\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)\\n    gather_subset = module.params['gather_subset']\\n    runable_subsets = set()\\n    exclude_subsets = set()\\n    for subset in gather_subset:\\n        if subset == 'all':\\n            runable_subsets.update(VALID_SUBSETS)\\n            continue\\n        if subset.startswith('!'):\\n            subset = subset[1:]\\n            if subset == 'all':\\n                exclude_subsets.update(VALID_SUBSETS)\\n                continue\\n            exclude = True\\n        else:\\n            exclude = False\\n        if subset not in VALID_SUBSETS:\\n            module.fail_json(msg='Bad subset')\\n        if exclude:\\n            exclude_subsets.add(subset)\\n        else:\\n            runable_subsets.add(subset)\\n    if not runable_subsets:\\n        runable_subsets.update(VALID_SUBSETS)\\n    runable_subsets.difference_update(exclude_subsets)\\n    runable_subsets.add('default')\\n    facts = dict()\\n    facts['gather_subset'] = list(runable_subsets)\\n    instances = list()\\n    for key in runable_subsets:\\n        instances.append(FACT_SUBSETS[key](module))\\n    for inst in instances:\\n        inst.populate()\\n        facts.update(inst.facts)\\n    ansible_facts = dict()\\n    for key, value in iteritems(facts):\\n        key = 'ansible_net_%s' % key\\n        ansible_facts[key] = value\\n    warnings = list()\\n    module.exit_json(ansible_facts=ansible_facts, warnings=warnings)"
  },
  {
    "code": "def check_estimators_unfitted(name, Estimator):\\n    X, y = _boston_subset()\\n    with warnings.catch_warnings(record=True):\\n        est = Estimator()\\n    msg = \"fit\"\\n    if hasattr(est, 'predict'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.predict, X)\\n    if hasattr(est, 'decision_function'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.decision_function, X)\\n    if hasattr(est, 'predict_proba'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.predict_proba, X)\\n    if hasattr(est, 'predict_log_proba'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.predict_log_proba, X)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_estimators_unfitted(name, Estimator):\\n    X, y = _boston_subset()\\n    est = Estimator()\\n    msg = \"fit\"\\n    if hasattr(est, 'predict'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.predict, X)\\n    if hasattr(est, 'decision_function'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.decision_function, X)\\n    if hasattr(est, 'predict_proba'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.predict_proba, X)\\n    if hasattr(est, 'predict_log_proba'):\\n        assert_raise_message((AttributeError, ValueError), msg,\\n                             est.predict_log_proba, X)"
  },
  {
    "code": "class OVSQuantumPlugin(QuantumPluginBase):\\n\\tdef __init__(self, configfile=None):\\n\\t\\tconfig = ConfigParser.ConfigParser()\\n\\t\\tif configfile is None:\\n\\t\\t\\tif os.path.exists(CONF_FILE):\\n\\t\\t\\t\\tconfigfile = CONF_FILE\\n\\t\\t\\telse:\\n\\t\\t\\t\\tconfigfile = find_config(os.path.abspath(\\n\\t\\t\\t\\t\\t\\tos.path.dirname(__file__)))\\n\\t\\tif configfile is None:\\n\\t\\t\\traise Exception(\"Configuration file \\\"%s\\\" doesn't exist\" %\\n\\t\\t\\t  (configfile))\\n\\t\\tLOG.debug(\"Using configuration file: %s\" % configfile)\\n\\t\\tconfig.read(configfile)\\n\\t\\tLOG.debug(\"Config: %s\" % config)\\n\\t\\toptions = {\"sql_connection\": config.get(\"DATABASE\", \"sql_connection\")}\\n\\t\\tdb.configure_db(options)\\n\\t\\tself.vmap = VlanMap()\\n\\t\\tvlans = ovs_db.get_vlans()\\n\\t\\tfor x in vlans:\\n\\t\\t\\tvlan_id, network_id = x\\n\\t\\t\\tLOG.debug(\"Adding already populated vlan %s -> %s\"\\n\\t\\t\\t\\t\\t  % (vlan_id, network_id))\\n\\t\\t\\tself.vmap.already_used(vlan_id, network_id)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class OVSQuantumPlugin(QuantumPluginBase):\\n\\tdef __init__(self, configfile=None):\\n\\t\\tconfig = ConfigParser.ConfigParser()\\n\\t\\tif configfile is None:\\n\\t\\t\\tif os.path.exists(CONF_FILE):\\n\\t\\t\\t\\tconfigfile = CONF_FILE\\n\\t\\t\\telse:\\n\\t\\t\\t\\tconfigfile = find_config(os.path.abspath(\\n\\t\\t\\t\\t\\t\\tos.path.dirname(__file__)))\\n\\t\\tif configfile is None:\\n\\t\\t\\traise Exception(\"Configuration file \\\"%s\\\" doesn't exist\" %\\n\\t\\t\\t  (configfile))\\n\\t\\tLOG.debug(\"Using configuration file: %s\" % configfile)\\n\\t\\tconfig.read(configfile)\\n\\t\\tLOG.debug(\"Config: %s\" % config)\\n\\t\\toptions = {\"sql_connection\": config.get(\"DATABASE\", \"sql_connection\")}\\n\\t\\tdb.configure_db(options)\\n\\t\\tself.vmap = VlanMap()\\n\\t\\tvlans = ovs_db.get_vlans()\\n\\t\\tfor x in vlans:\\n\\t\\t\\tvlan_id, network_id = x\\n\\t\\t\\tLOG.debug(\"Adding already populated vlan %s -> %s\"\\n\\t\\t\\t\\t\\t  % (vlan_id, network_id))\\n\\t\\t\\tself.vmap.already_used(vlan_id, network_id)"
  },
  {
    "code": "def __init__(self, n_atoms, transform_algorithm='omp', split_sign=False):\\n        self.n_atoms = n_atoms\\n        self.transform_algorithm = transform_algorithm\\n        self.split_sign = split_sign",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, n_atoms, transform_algorithm='omp', split_sign=False):\\n        self.n_atoms = n_atoms\\n        self.transform_algorithm = transform_algorithm\\n        self.split_sign = split_sign"
  },
  {
    "code": "def _process_inputs(self, inputs, initial_state, constants):\\n\\tif (isinstance(inputs, collections.Sequence)\\n\\t\\tand not isinstance(inputs, tuple)):\\n\\t  if not self._num_constants:\\n\\t\\tinitial_state = inputs[1:]\\n\\t  else:\\n\\t\\tinitial_state = inputs[1:-self._num_constants]\\n\\t\\tconstants = inputs[-self._num_constants:]\\n\\t  if len(initial_state) == 0:\\n\\t\\tinitial_state = None\\n\\t  inputs = inputs[0]\\n\\tif self.stateful:\\n\\t  if initial_state is not None:\\n\\t\\tnon_zero_count = math_ops.add_n([math_ops.count_nonzero_v2(s)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t for s in nest.flatten(self.states)])\\n\\t\\tinitial_state = control_flow_ops.cond(non_zero_count > 0,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  true_fn=lambda: self.states,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  false_fn=lambda: initial_state,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=True)\\n\\t  else:\\n\\t\\tinitial_state = self.states\\n\\telif initial_state is None:\\n\\t  initial_state = self.get_initial_state(inputs)\\n\\tif len(initial_state) != len(self.states):\\n\\t  raise ValueError('Layer has ' + str(len(self.states)) +\\n\\t\\t\\t\\t\\t   ' states but was passed ' + str(len(initial_state)) +\\n\\t\\t\\t\\t\\t   ' initial states.')\\n\\treturn inputs, initial_state, constants",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_inputs(self, inputs, initial_state, constants):\\n\\tif (isinstance(inputs, collections.Sequence)\\n\\t\\tand not isinstance(inputs, tuple)):\\n\\t  if not self._num_constants:\\n\\t\\tinitial_state = inputs[1:]\\n\\t  else:\\n\\t\\tinitial_state = inputs[1:-self._num_constants]\\n\\t\\tconstants = inputs[-self._num_constants:]\\n\\t  if len(initial_state) == 0:\\n\\t\\tinitial_state = None\\n\\t  inputs = inputs[0]\\n\\tif self.stateful:\\n\\t  if initial_state is not None:\\n\\t\\tnon_zero_count = math_ops.add_n([math_ops.count_nonzero_v2(s)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t for s in nest.flatten(self.states)])\\n\\t\\tinitial_state = control_flow_ops.cond(non_zero_count > 0,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  true_fn=lambda: self.states,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  false_fn=lambda: initial_state,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  strict=True)\\n\\t  else:\\n\\t\\tinitial_state = self.states\\n\\telif initial_state is None:\\n\\t  initial_state = self.get_initial_state(inputs)\\n\\tif len(initial_state) != len(self.states):\\n\\t  raise ValueError('Layer has ' + str(len(self.states)) +\\n\\t\\t\\t\\t\\t   ' states but was passed ' + str(len(initial_state)) +\\n\\t\\t\\t\\t\\t   ' initial states.')\\n\\treturn inputs, initial_state, constants"
  },
  {
    "code": "def post_comment(request, next=None):\\n    if request.method != 'POST':\\n        return http.HttpResponseNotAllowed([\"POST\"])\\n    data = request.POST.copy()\\n    if request.user.is_authenticated():\\n        if not data.get('name', ''):\\n            data[\"name\"] = request.user.get_full_name()\\n        if not data.get('email', ''):\\n            data[\"email\"] = request.user.email\\n    ctype = data.get(\"content_type\")\\n    object_pk = data.get(\"object_pk\")\\n    if ctype is None or object_pk is None:\\n        return CommentPostBadRequest(\"Missing content_type or object_pk field.\")\\n    try:\\n        model = models.get_model(*ctype.split(\".\", 1))\\n        target = model._default_manager.get(pk=object_pk)\\n    except TypeError:\\n        return CommentPostBadRequest(\\n            \"Invalid content_type value: %r\" % escape(ctype))\\n    except AttributeError:\\n        return CommentPostBadRequest(\\n            \"The given content-type %r does not resolve to a valid model.\" % \\\\n                escape(ctype))\\n    except ObjectDoesNotExist:\\n        return CommentPostBadRequest(\\n            \"No object matching content-type %r and object PK %r exists.\" % \\\\n                (escape(ctype), escape(object_pk)))\\n    preview = data.get(\"submit\", \"\").lower() == \"preview\" or \\\\n              data.get(\"preview\", None) is not None\\n    form = comments.get_form()(target, data=data)\\n    if form.security_errors():\\n        return CommentPostBadRequest(\\n            \"The comment form failed security verification: %s\" % \\\\n                escape(str(form.security_errors())))\\n    if form.errors or preview:\\n        template_list = [\\n            \"comments/%s_%s_preview.html\" % tuple(str(model._meta).split(\".\")),\\n            \"comments/%s_preview.html\" % model._meta.app_label,\\n            \"comments/preview.html\",\\n        ]\\n        return render_to_response(\\n            template_list, {\\n                \"comment\" : form.data.get(\"comment\", \"\"),\\n                \"form\" : form,\\n            },\\n            RequestContext(request, {})\\n        )\\n    comment = form.get_comment_object()\\n    comment.ip_address = request.META.get(\"REMOTE_ADDR\", None)\\n    if request.user.is_authenticated():\\n        comment.user = request.user\\n    responses = signals.comment_will_be_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    for (receiver, response) in responses:\\n        if response == False:\\n            return CommentPostBadRequest(\\n                \"comment_will_be_posted receiver %r killed the comment\" % receiver.__name__)\\n    comment.save()\\n    signals.comment_was_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    return next_redirect(data, next, comment_done, c=comment._get_pk_val())\\ncomment_done = confirmation_view(\\n    template = \"comments/posted.html\",\\n    doc = \\n)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #8803 -- Allow authenticated users without first_name/last_name values set to post comments.",
    "fixed_code": "def post_comment(request, next=None):\\n    if request.method != 'POST':\\n        return http.HttpResponseNotAllowed([\"POST\"])\\n    data = request.POST.copy()\\n    if request.user.is_authenticated():\\n        if not data.get('name', ''):\\n            data[\"name\"] = request.user.get_full_name() or request.user.username\\n        if not data.get('email', ''):\\n            data[\"email\"] = request.user.email\\n    ctype = data.get(\"content_type\")\\n    object_pk = data.get(\"object_pk\")\\n    if ctype is None or object_pk is None:\\n        return CommentPostBadRequest(\"Missing content_type or object_pk field.\")\\n    try:\\n        model = models.get_model(*ctype.split(\".\", 1))\\n        target = model._default_manager.get(pk=object_pk)\\n    except TypeError:\\n        return CommentPostBadRequest(\\n            \"Invalid content_type value: %r\" % escape(ctype))\\n    except AttributeError:\\n        return CommentPostBadRequest(\\n            \"The given content-type %r does not resolve to a valid model.\" % \\\\n                escape(ctype))\\n    except ObjectDoesNotExist:\\n        return CommentPostBadRequest(\\n            \"No object matching content-type %r and object PK %r exists.\" % \\\\n                (escape(ctype), escape(object_pk)))\\n    preview = data.get(\"submit\", \"\").lower() == \"preview\" or \\\\n              data.get(\"preview\", None) is not None\\n    form = comments.get_form()(target, data=data)\\n    if form.security_errors():\\n        return CommentPostBadRequest(\\n            \"The comment form failed security verification: %s\" % \\\\n                escape(str(form.security_errors())))\\n    if form.errors or preview:\\n        template_list = [\\n            \"comments/%s_%s_preview.html\" % tuple(str(model._meta).split(\".\")),\\n            \"comments/%s_preview.html\" % model._meta.app_label,\\n            \"comments/preview.html\",\\n        ]\\n        return render_to_response(\\n            template_list, {\\n                \"comment\" : form.data.get(\"comment\", \"\"),\\n                \"form\" : form,\\n            },\\n            RequestContext(request, {})\\n        )\\n    comment = form.get_comment_object()\\n    comment.ip_address = request.META.get(\"REMOTE_ADDR\", None)\\n    if request.user.is_authenticated():\\n        comment.user = request.user\\n    responses = signals.comment_will_be_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    for (receiver, response) in responses:\\n        if response == False:\\n            return CommentPostBadRequest(\\n                \"comment_will_be_posted receiver %r killed the comment\" % receiver.__name__)\\n    comment.save()\\n    signals.comment_was_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    return next_redirect(data, next, comment_done, c=comment._get_pk_val())\\ncomment_done = confirmation_view(\\n    template = \"comments/posted.html\",\\n    doc = \\n)"
  },
  {
    "code": "def fix(filename):\\n' % (filename,))\\n    if filename == '-':\\n        f = sys.stdin\\n        g = sys.stdout\\n    else:\\n        try:\\n            f = open(filename, 'r')\\n        except IOError as msg:\\n            err(filename + ': cannot open: ' + str(msg) + '\\n')\\n            return 1\\n        head, tail = os.path.split(filename)\\n        tempname = os.path.join(head, '@' + tail)\\n        g = None\\n    lineno = 0\\n    initfixline()\\n    while 1:\\n        line = f.readline()\\n        if not line: break\\n        lineno = lineno + 1\\n        while line[-2:] == '\\\\\\n':\\n            nextline = f.readline()\\n            if not nextline: break\\n            line = line + nextline\\n            lineno = lineno + 1\\n        newline = fixline(line)\\n        if newline != line:\\n            if g is None:\\n                try:\\n                    g = open(tempname, 'w')\\n                except IOError as msg:\\n                    f.close()\\n                    err(tempname+': cannot create: '+\\n                        str(msg)+'\\n')\\n                    return 1\\n                f.seek(0)\\n                lineno = 0\\n                initfixline()\\n                rep(filename + ':\\n')\\n                continue \\n            rep(repr(lineno) + '\\n')\\n            rep('< ' + line)\\n            rep('> ' + newline)\\n        if g is not None:\\n            g.write(newline)\\n    if filename == '-': return 0 \\n    f.close()\\n    if not g: return 0 \\n    g.close()\\n    try:\\n        statbuf = os.stat(filename)\\n        os.chmod(tempname, statbuf[ST_MODE] & 0o7777)\\n    except OSError as msg:\\n        err(tempname + ': warning: chmod failed (' + str(msg) + ')\\n')\\n    try:\\n        os.rename(filename, filename + '~')\\n    except OSError as msg:\\n        err(filename + ': warning: backup failed (' + str(msg) + ')\\n')\\n    try:\\n        os.rename(tempname, filename)\\n    except OSError as msg:\\n        err(filename + ': rename failed (' + str(msg) + ')\\n')\\n        return 1\\n    return 0\\nIdentifier = '(struct )?[a-zA-Z_][a-zA-Z0-9_]+'\\nString = r'\"([^\\n\\\\\"]|\\\\.)*\"'\\nChar = r\"'([^\\n\\\\']|\\\\.)*'\"\\nCommentStart = r'/\\*'\\nCommentEnd = r'\\*/'\\nHexnumber = '0[xX][0-9a-fA-F]*[uUlL]*'\\nOctnumber = '0[0-7]*[uUlL]*'\\nDecnumber = '[1-9][0-9]*[uUlL]*'\\nIntnumber = Hexnumber + '|' + Octnumber + '|' + Decnumber\\nExponent = '[eE][-+]?[0-9]+'\\nPointfloat = r'([0-9]+\\.[0-9]*|\\.[0-9]+)(' + Exponent + r')?'\\nExpfloat = '[0-9]+' + Exponent\\nFloatnumber = Pointfloat + '|' + Expfloat\\nNumber = Floatnumber + '|' + Intnumber\\nOutsideComment = (Identifier, Number, String, Char, CommentStart)\\nOutsideCommentPattern = '(' + '|'.join(OutsideComment) + ')'\\nOutsideCommentProgram = re.compile(OutsideCommentPattern)\\nInsideComment = (Identifier, Number, CommentEnd)\\nInsideCommentPattern = '(' + '|'.join(InsideComment) + ')'\\nInsideCommentProgram = re.compile(InsideCommentPattern)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fix(filename):\\n' % (filename,))\\n    if filename == '-':\\n        f = sys.stdin\\n        g = sys.stdout\\n    else:\\n        try:\\n            f = open(filename, 'r')\\n        except IOError as msg:\\n            err(filename + ': cannot open: ' + str(msg) + '\\n')\\n            return 1\\n        head, tail = os.path.split(filename)\\n        tempname = os.path.join(head, '@' + tail)\\n        g = None\\n    lineno = 0\\n    initfixline()\\n    while 1:\\n        line = f.readline()\\n        if not line: break\\n        lineno = lineno + 1\\n        while line[-2:] == '\\\\\\n':\\n            nextline = f.readline()\\n            if not nextline: break\\n            line = line + nextline\\n            lineno = lineno + 1\\n        newline = fixline(line)\\n        if newline != line:\\n            if g is None:\\n                try:\\n                    g = open(tempname, 'w')\\n                except IOError as msg:\\n                    f.close()\\n                    err(tempname+': cannot create: '+\\n                        str(msg)+'\\n')\\n                    return 1\\n                f.seek(0)\\n                lineno = 0\\n                initfixline()\\n                rep(filename + ':\\n')\\n                continue \\n            rep(repr(lineno) + '\\n')\\n            rep('< ' + line)\\n            rep('> ' + newline)\\n        if g is not None:\\n            g.write(newline)\\n    if filename == '-': return 0 \\n    f.close()\\n    if not g: return 0 \\n    g.close()\\n    try:\\n        statbuf = os.stat(filename)\\n        os.chmod(tempname, statbuf[ST_MODE] & 0o7777)\\n    except OSError as msg:\\n        err(tempname + ': warning: chmod failed (' + str(msg) + ')\\n')\\n    try:\\n        os.rename(filename, filename + '~')\\n    except OSError as msg:\\n        err(filename + ': warning: backup failed (' + str(msg) + ')\\n')\\n    try:\\n        os.rename(tempname, filename)\\n    except OSError as msg:\\n        err(filename + ': rename failed (' + str(msg) + ')\\n')\\n        return 1\\n    return 0\\nIdentifier = '(struct )?[a-zA-Z_][a-zA-Z0-9_]+'\\nString = r'\"([^\\n\\\\\"]|\\\\.)*\"'\\nChar = r\"'([^\\n\\\\']|\\\\.)*'\"\\nCommentStart = r'/\\*'\\nCommentEnd = r'\\*/'\\nHexnumber = '0[xX][0-9a-fA-F]*[uUlL]*'\\nOctnumber = '0[0-7]*[uUlL]*'\\nDecnumber = '[1-9][0-9]*[uUlL]*'\\nIntnumber = Hexnumber + '|' + Octnumber + '|' + Decnumber\\nExponent = '[eE][-+]?[0-9]+'\\nPointfloat = r'([0-9]+\\.[0-9]*|\\.[0-9]+)(' + Exponent + r')?'\\nExpfloat = '[0-9]+' + Exponent\\nFloatnumber = Pointfloat + '|' + Expfloat\\nNumber = Floatnumber + '|' + Intnumber\\nOutsideComment = (Identifier, Number, String, Char, CommentStart)\\nOutsideCommentPattern = '(' + '|'.join(OutsideComment) + ')'\\nOutsideCommentProgram = re.compile(OutsideCommentPattern)\\nInsideComment = (Identifier, Number, CommentEnd)\\nInsideCommentPattern = '(' + '|'.join(InsideComment) + ')'\\nInsideCommentProgram = re.compile(InsideCommentPattern)"
  },
  {
    "code": "def signature(obj):\\n    ''\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if _signature_is_builtin(obj):\\n        return Signature.from_builtin(obj)\\n    if isinstance(obj, types.MethodType):\\n        sig = signature(obj.__func__)\\n        return _signature_bound_method(sig)\\n    obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        wrapped_sig = signature(partialmethod.func)\\n        sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n        first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n        new_params = (first_wrapped_param,) + tuple(sig.parameters.values())\\n        return sig.replace(parameters=new_params)\\n    if isinstance(obj, types.FunctionType):\\n        return Signature.from_function(obj)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = signature(obj.func)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n        else:\\n            new = _get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = signature(new)\\n            else:\\n                init = _get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = signature(init)\\n        if sig is None:\\n            if type in obj.__mro__:\\n                return signature(type)\\n            else:\\n                return signature(object)\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n    if sig is not None:\\n        return _signature_bound_method(sig)\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "inspect.signature: Add support for decorated (wrapped) builtins #20425",
    "fixed_code": "def signature(obj):\\n    ''\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = signature(obj.__func__)\\n        return _signature_bound_method(sig)\\n    obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        wrapped_sig = signature(partialmethod.func)\\n        sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n        first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n        new_params = (first_wrapped_param,) + tuple(sig.parameters.values())\\n        return sig.replace(parameters=new_params)\\n    if _signature_is_builtin(obj):\\n        return Signature.from_builtin(obj)\\n    if isinstance(obj, types.FunctionType):\\n        return Signature.from_function(obj)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = signature(obj.func)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n        else:\\n            new = _get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = signature(new)\\n            else:\\n                init = _get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = signature(init)\\n        if sig is None:\\n            if type in obj.__mro__:\\n                return signature(type)\\n            else:\\n                return signature(object)\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n    if sig is not None:\\n        return _signature_bound_method(sig)\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))"
  },
  {
    "code": "def to_yaml(self, **kwargs):\\n\\tif yaml is None:\\n\\t  raise ImportError(\\n\\t\\t  'Requires yaml module installed (`pip install pyyaml`).')\\n\\treturn yaml.dump(self._updated_config(), **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use the safer `safe_load` function instead of `unsafe_load` when possible\\n\\nThere is no need to open ourselves up to arbitrary code execution, especially since this is not in a performance critical loop, so we can take the slowdown due to safety.",
    "fixed_code": "def to_yaml(self, **kwargs):\\n\\traise RuntimeError(\\n\\t\\t'Method `model.to_yaml()` has been removed due to security risk of '\\n\\t\\t'arbitrary code execution. Please use `model.to_json()` instead.'\\n\\t)"
  },
  {
    "code": "def _findLib_gcc(name):\\n        expr = r'[^\\(\\)\\s]*lib%s\\.[^\\(\\)\\s]*' % re.escape(name)\\n        fdout, ccout = tempfile.mkstemp()\\n        os.close(fdout)\\n        cmd = 'if type gcc >/dev/null 2>&1; then CC=gcc; elif type cc >/dev/null 2>&1; then CC=cc;else exit 10; fi;' \\\\n              'LANG=C LC_ALL=C $CC -Wl,-t -o ' + ccout + ' 2>&1 -l' + name\\n        try:\\n            f = os.popen(cmd)\\n            try:\\n                trace = f.read()\\n            finally:\\n                rv = f.close()\\n        finally:\\n            try:\\n                os.unlink(ccout)\\n            except FileNotFoundError:\\n                pass\\n        if rv == 10:\\n            raise OSError('gcc or cc command not found')\\n        res = re.search(expr, trace)\\n        if not res:\\n            return None\\n        return res.group(0)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #22636: avoid using a shell in the ctypes.util module\\n\\nReplace os.popen() with subprocess.Popen. Based on patch by Victor Stinner.\\n\\nIf the \"gcc\", \"cc\" or \"objdump\" command is not available, the code was\\nsupposed to raise an OSError exception. But there was a bug in the code. The\\nshell code returns the exit code 10 if the required command is missing, and the\\ncode tries to check for the status 10. The problem is that os.popen() doesn't\\nreturn the exit code directly, but a status which should be processed by\\nos.WIFEXITED() and os.WEXITSTATUS(). In practice, the exception was never\\nraised. The OSError exception was not documented and ctypes.util.find_library()\\nis expected to return None if the library is not found.",
    "fixed_code": "def _findLib_gcc(name):\\n        expr = os.fsencode(r'[^\\(\\)\\s]*lib%s\\.[^\\(\\)\\s]*' % re.escape(name))\\n        c_compiler = shutil.which('gcc')\\n        if not c_compiler:\\n            c_compiler = shutil.which('cc')\\n        if not c_compiler:\\n            return None\\n        temp = tempfile.NamedTemporaryFile()\\n        try:\\n            args = [c_compiler, '-Wl,-t', '-o', temp.name, '-l' + name]\\n            env = dict(os.environ)\\n            env['LC_ALL'] = 'C'\\n            env['LANG'] = 'C'\\n            proc = subprocess.Popen(args,\\n                                    stdout=subprocess.PIPE,\\n                                    stderr=subprocess.STDOUT,\\n                                    env=env)\\n            with proc:\\n                trace = proc.stdout.read()\\n        finally:\\n            try:\\n                temp.close()\\n            except FileNotFoundError:\\n                pass\\n        res = re.search(expr, trace)\\n        if not res:\\n            return None\\n        return os.fsdecode(res.group(0))"
  },
  {
    "code": "def sumTrailer(self, name):\\n        self.emit(\"\", 0)\\n        self.emit(\"tmp = PyObject_Repr(obj);\", 1)\\n        self.emit(\"if (tmp == NULL) goto failed;\", 1)\\n        error = \"expected some sort of %s, but got %%.400s\" % name\\n        format = \"PyErr_Format(PyExc_TypeError, \\\"%s\\\", PyBytes_AS_STRING(tmp));\"\\n        self.emit(format % error, 1, reflow=False)\\n        self.emit(\"failed:\", 0)\\n        self.emit(\"Py_XDECREF(tmp);\", 1)\\n        self.emit(\"return 1;\", 1)\\n        self.emit(\"}\", 0)\\n        self.emit(\"\", 0)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "use %R format code; fixes invalid dereferencing #10391",
    "fixed_code": "def sumTrailer(self, name, add_label=False):\\n        self.emit(\"\", 0)\\n        error = \"expected some sort of %s, but got %%R\" % name\\n        format = \"PyErr_Format(PyExc_TypeError, \\\"%s\\\", obj);\"\\n        self.emit(format % error, 1, reflow=False)\\n        if add_label:\\n            self.emit(\"failed:\", 1)\\n        self.emit(\"return 1;\", 1)\\n        self.emit(\"}\", 0)\\n        self.emit(\"\", 0)"
  },
  {
    "code": "def _convert_tznames_to_sql(self, tzname):\\n\\t\\tif tzname and settings.USE_TZ:\\n\\t\\t\\treturn \"'%s'\" % tzname, \"'%s'\" % self.connection.timezone_name\\n\\t\\treturn \"NULL\", \"NULL\"",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Refs CVE-2022-34265 -- Properly escaped Extract() and Trunc() parameters.",
    "fixed_code": "def _convert_tznames_to_sql(self, tzname):\\n\\t\\tif tzname and settings.USE_TZ:\\n\\t\\t\\treturn tzname, self.connection.timezone_name\\n\\t\\treturn None, None"
  },
  {
    "code": "def IsExpandable(self):\\n        \"Return True if Python (.py) file.\"\\n        return os.path.normcase(self.file[-3:]) == \".py\"",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-95411: IDLE - Enable using the module browser with .pyw files (#95397)",
    "fixed_code": "def IsExpandable(self):\\n        \"Return True if Python file.\"\\n        return is_browseable_extension(self.file)"
  },
  {
    "code": "def _fit_transform(self, X):\\n        self.nbrs_ = NearestNeighbors(n_neighbors=self.n_neighbors,\\n                                      algorithm=self.neighbors_algorithm,\\n                                      n_jobs=self.n_jobs)\\n        random_state = check_random_state(self.random_state)\\n        X = self._validate_data(X, dtype=float)\\n        self.nbrs_.fit(X)\\n        self.embedding_, self.reconstruction_error_ = \\\\n            locally_linear_embedding(\\n                self.nbrs_, self.n_neighbors, self.n_components,\\n                eigen_solver=self.eigen_solver, tol=self.tol,\\n                max_iter=self.max_iter, method=self.method,\\n                hessian_tol=self.hessian_tol, modified_tol=self.modified_tol,\\n                random_state=random_state, reg=self.reg, n_jobs=self.n_jobs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fit_transform(self, X):\\n        self.nbrs_ = NearestNeighbors(n_neighbors=self.n_neighbors,\\n                                      algorithm=self.neighbors_algorithm,\\n                                      n_jobs=self.n_jobs)\\n        random_state = check_random_state(self.random_state)\\n        X = self._validate_data(X, dtype=float)\\n        self.nbrs_.fit(X)\\n        self.embedding_, self.reconstruction_error_ = \\\\n            locally_linear_embedding(\\n                self.nbrs_, self.n_neighbors, self.n_components,\\n                eigen_solver=self.eigen_solver, tol=self.tol,\\n                max_iter=self.max_iter, method=self.method,\\n                hessian_tol=self.hessian_tol, modified_tol=self.modified_tol,\\n                random_state=random_state, reg=self.reg, n_jobs=self.n_jobs)"
  },
  {
    "code": "def convert_tuple_type(self, values: Tuple[Any]) -> Dict[str, Any]:\\n        names = ['field_' + str(i) for i in range(len(values))]\\n        return self.generate_data_dict(names, values)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def convert_tuple_type(self, values: Tuple[Any]) -> Dict[str, Any]:\\n        names = ['field_' + str(i) for i in range(len(values))]\\n        return self.generate_data_dict(names, values)"
  },
  {
    "code": "def _can_use_numexpr(op, op_str, a, b, dtype_check):\\n\\tif op_str is not None:\\n\\t\\tif np.prod(a.shape) > _MIN_ELEMENTS:\\n\\t\\t\\tdtypes = set()\\n\\t\\t\\tfor o in [a, b]:\\n\\t\\t\\t\\tif hasattr(o, \"dtypes\") and o.ndim > 1:\\n\\t\\t\\t\\t\\ts = o.dtypes.value_counts()\\n\\t\\t\\t\\t\\tif len(s) > 1:\\n\\t\\t\\t\\t\\t\\treturn False\\n\\t\\t\\t\\t\\tdtypes |= set(s.index.astype(str))\\n\\t\\t\\t\\telif hasattr(o, \"dtype\"):\\n\\t\\t\\t\\t\\tdtypes |= {o.dtype.name}\\n\\t\\t\\tif not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes:\\n\\t\\t\\t\\treturn True\\n\\treturn False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _can_use_numexpr(op, op_str, a, b, dtype_check):\\n\\tif op_str is not None:\\n\\t\\tif np.prod(a.shape) > _MIN_ELEMENTS:\\n\\t\\t\\tdtypes = set()\\n\\t\\t\\tfor o in [a, b]:\\n\\t\\t\\t\\tif hasattr(o, \"dtypes\") and o.ndim > 1:\\n\\t\\t\\t\\t\\ts = o.dtypes.value_counts()\\n\\t\\t\\t\\t\\tif len(s) > 1:\\n\\t\\t\\t\\t\\t\\treturn False\\n\\t\\t\\t\\t\\tdtypes |= set(s.index.astype(str))\\n\\t\\t\\t\\telif hasattr(o, \"dtype\"):\\n\\t\\t\\t\\t\\tdtypes |= {o.dtype.name}\\n\\t\\t\\tif not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes:\\n\\t\\t\\t\\treturn True\\n\\treturn False"
  },
  {
    "code": "def _apply(\\n        self,\\n        func: Callable,\\n        center: bool,\\n        require_min_periods: int = 0,\\n        floor: int = 1,\\n        is_weighted: bool = False,\\n        name: Optional[str] = None,\\n        use_numba_cache: bool = False,\\n        **kwargs,\\n    ):\\n        kwargs.pop(\"floor\", None)\\n        kwargs.pop(\"original_func\", None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: Remove rolling window fixed algorithms (#36567)",
    "fixed_code": "def _apply(\\n        self,\\n        func: Callable,\\n        require_min_periods: int = 0,\\n        floor: int = 1,\\n        is_weighted: bool = False,\\n        name: Optional[str] = None,\\n        use_numba_cache: bool = False,\\n        **kwargs,\\n    ):\\n        kwargs.pop(\"floor\", None)\\n        kwargs.pop(\"original_func\", None)"
  },
  {
    "code": "def _copy(master_fd, master_read=_read, stdin_read=_read):\\n    fds = [master_fd, STDIN_FILENO]\\n    while True:\\n        rfds, wfds, xfds = select(fds, [], [])\\n        if master_fd in rfds:\\n            data = master_read(master_fd)\\n            if not data:  \\n                fds.remove(master_fd)\\n            else:\\n                os.write(STDOUT_FILENO, data)\\n        if STDIN_FILENO in rfds:\\n            data = stdin_read(STDIN_FILENO)\\n            if not data:\\n                fds.remove(STDIN_FILENO)\\n            else:\\n                _writen(master_fd, data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _copy(master_fd, master_read=_read, stdin_read=_read):\\n    fds = [master_fd, STDIN_FILENO]\\n    while True:\\n        rfds, wfds, xfds = select(fds, [], [])\\n        if master_fd in rfds:\\n            data = master_read(master_fd)\\n            if not data:  \\n                fds.remove(master_fd)\\n            else:\\n                os.write(STDOUT_FILENO, data)\\n        if STDIN_FILENO in rfds:\\n            data = stdin_read(STDIN_FILENO)\\n            if not data:\\n                fds.remove(STDIN_FILENO)\\n            else:\\n                _writen(master_fd, data)"
  },
  {
    "code": "def _length_check(others):\\n    n = None\\n    for x in others:\\n        try:\\n            if n is None:\\n                n = len(x)\\n            elif len(x) != n:\\n                raise ValueError('All arrays must be same length')\\n        except TypeError:\\n            raise ValueError(\"Did you mean to supply a `sep` keyword?\")\\n    return n",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _length_check(others):\\n    n = None\\n    for x in others:\\n        try:\\n            if n is None:\\n                n = len(x)\\n            elif len(x) != n:\\n                raise ValueError('All arrays must be same length')\\n        except TypeError:\\n            raise ValueError(\"Did you mean to supply a `sep` keyword?\")\\n    return n"
  },
  {
    "code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tself._options = input_dataset.options()\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "[tf.data] Solve the stack overflow problem of getting all options on Python side.",
    "fixed_code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)\\n\\tif self._options_attr:\\n\\t  self._options_attr = self._options_attr.merge(options)\\n\\telse:\\n\\t  self._options_attr = options"
  },
  {
    "code": "def app_index(self, request, app_label, extra_context=None):\\n\\t\\tuser = request.user\\n\\t\\thas_module_perms = user.has_module_perms(app_label)\\n\\t\\tif not has_module_perms:\\n\\t\\t\\traise PermissionDenied\\n\\t\\tapp_dict = {}\\n\\t\\tfor model, model_admin in self._registry.items():\\n\\t\\t\\tif app_label == model._meta.app_label:\\n\\t\\t\\t\\tperms = model_admin.get_model_perms(request)\\n\\t\\t\\t\\tif True in perms.values():\\n\\t\\t\\t\\t\\tinfo = (app_label, model._meta.model_name)\\n\\t\\t\\t\\t\\tmodel_dict = {\\n\\t\\t\\t\\t\\t\\t'name': capfirst(model._meta.verbose_name_plural),\\n\\t\\t\\t\\t\\t\\t'object_name': model._meta.object_name,\\n\\t\\t\\t\\t\\t\\t'perms': perms,\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tif perms.get('change'):\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tmodel_dict['admin_url'] = reverse('admin:%s_%s_changelist' % info, current_app=self.name)\\n\\t\\t\\t\\t\\t\\texcept NoReverseMatch:\\n\\t\\t\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\t\\tif perms.get('add'):\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tmodel_dict['add_url'] = reverse('admin:%s_%s_add' % info, current_app=self.name)\\n\\t\\t\\t\\t\\t\\texcept NoReverseMatch:\\n\\t\\t\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\t\\tif app_dict:\\n\\t\\t\\t\\t\\t\\tapp_dict['models'].append(model_dict),\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tapp_dict = {\\n\\t\\t\\t\\t\\t\\t\\t'name': app_label.title(),\\n\\t\\t\\t\\t\\t\\t\\t'app_label': app_label,\\n\\t\\t\\t\\t\\t\\t\\t'app_url': '',\\n\\t\\t\\t\\t\\t\\t\\t'has_module_perms': has_module_perms,\\n\\t\\t\\t\\t\\t\\t\\t'models': [model_dict],\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\tif not app_dict:\\n\\t\\t\\traise Http404('The requested admin page does not exist.')\\n\\t\\tapp_dict['models'].sort(key=lambda x: x['name'])\\n\\t\\tcontext = dict(self.each_context(),\\n\\t\\t\\ttitle=_('%s administration') % capfirst(app_label),\\n\\t\\t\\tapp_list=[app_dict],\\n\\t\\t\\tapp_label=app_label,\\n\\t\\t)\\n\\t\\tcontext.update(extra_context or {})\\n\\t\\treturn TemplateResponse(request, self.app_index_template or [\\n\\t\\t\\t'admin/%s/app_index.html' % app_label,\\n\\t\\t\\t'admin/app_index.html'\\n\\t\\t], context, current_app=self.name)\\nsite = AdminSite()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def app_index(self, request, app_label, extra_context=None):\\n\\t\\tuser = request.user\\n\\t\\thas_module_perms = user.has_module_perms(app_label)\\n\\t\\tif not has_module_perms:\\n\\t\\t\\traise PermissionDenied\\n\\t\\tapp_dict = {}\\n\\t\\tfor model, model_admin in self._registry.items():\\n\\t\\t\\tif app_label == model._meta.app_label:\\n\\t\\t\\t\\tperms = model_admin.get_model_perms(request)\\n\\t\\t\\t\\tif True in perms.values():\\n\\t\\t\\t\\t\\tinfo = (app_label, model._meta.model_name)\\n\\t\\t\\t\\t\\tmodel_dict = {\\n\\t\\t\\t\\t\\t\\t'name': capfirst(model._meta.verbose_name_plural),\\n\\t\\t\\t\\t\\t\\t'object_name': model._meta.object_name,\\n\\t\\t\\t\\t\\t\\t'perms': perms,\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tif perms.get('change'):\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tmodel_dict['admin_url'] = reverse('admin:%s_%s_changelist' % info, current_app=self.name)\\n\\t\\t\\t\\t\\t\\texcept NoReverseMatch:\\n\\t\\t\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\t\\tif perms.get('add'):\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tmodel_dict['add_url'] = reverse('admin:%s_%s_add' % info, current_app=self.name)\\n\\t\\t\\t\\t\\t\\texcept NoReverseMatch:\\n\\t\\t\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\t\\tif app_dict:\\n\\t\\t\\t\\t\\t\\tapp_dict['models'].append(model_dict),\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tapp_dict = {\\n\\t\\t\\t\\t\\t\\t\\t'name': app_label.title(),\\n\\t\\t\\t\\t\\t\\t\\t'app_label': app_label,\\n\\t\\t\\t\\t\\t\\t\\t'app_url': '',\\n\\t\\t\\t\\t\\t\\t\\t'has_module_perms': has_module_perms,\\n\\t\\t\\t\\t\\t\\t\\t'models': [model_dict],\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\tif not app_dict:\\n\\t\\t\\traise Http404('The requested admin page does not exist.')\\n\\t\\tapp_dict['models'].sort(key=lambda x: x['name'])\\n\\t\\tcontext = dict(self.each_context(),\\n\\t\\t\\ttitle=_('%s administration') % capfirst(app_label),\\n\\t\\t\\tapp_list=[app_dict],\\n\\t\\t\\tapp_label=app_label,\\n\\t\\t)\\n\\t\\tcontext.update(extra_context or {})\\n\\t\\treturn TemplateResponse(request, self.app_index_template or [\\n\\t\\t\\t'admin/%s/app_index.html' % app_label,\\n\\t\\t\\t'admin/app_index.html'\\n\\t\\t], context, current_app=self.name)\\nsite = AdminSite()"
  },
  {
    "code": "def get_system_username():\\n    try:\\n        result = getpass.getuser()\\n    except (ImportError, KeyError):\\n        return ''\\n    if not six.PY3:\\n        try:\\n            result = result.decode(DEFAULT_LOCALE_ENCODING)\\n        except UnicodeDecodeError:\\n            return ''\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Replaced \"not PY3\" by \"PY2\", new in six 1.4.0.",
    "fixed_code": "def get_system_username():\\n    try:\\n        result = getpass.getuser()\\n    except (ImportError, KeyError):\\n        return ''\\n    if six.PY2:\\n        try:\\n            result = result.decode(DEFAULT_LOCALE_ENCODING)\\n        except UnicodeDecodeError:\\n            return ''\\n    return result"
  },
  {
    "code": "def execute(self, context):\\n        self.ftp_hook = FTPHook(ftp_conn_id=self.ftp_conn_id)\\n        self.s3_hook = S3Hook(self.aws_conn_id)\\n        if self.ftp_filenames:\\n            if isinstance(self.ftp_filenames, str):\\n                self.log.info(f'Getting files in {self.ftp_path}')\\n                list_dir = self.ftp_hook.list_directory(\\n                    path=self.ftp_path,\\n                )\\n                if self.ftp_filenames == '*':\\n                    files = list_dir\\n                else:\\n                    files = list(filter(lambda file: self.ftp_filenames in file, list_dir))\\n                for file in files:\\n                    self.log.info(f'Moving file {file}')\\n                    if self.s3_filenames:\\n                        filename = file.replace(self.ftp_filenames, self.s3_filenames)\\n                    else:\\n                        filename = file\\n                    s3_file_key = f'{self.s3_key}{filename}'\\n                    self.__upload_to_s3_from_ftp(file, s3_file_key)\\n            else:\\n                if self.s3_filenames:\\n                    for ftp_file, s3_file in zip(self.ftp_filenames, self.s3_filenames):\\n                        self.__upload_to_s3_from_ftp(self.ftp_path + ftp_file, self.s3_key + s3_file)\\n                else:\\n                    for ftp_file in self.ftp_filenames:\\n                        self.__upload_to_s3_from_ftp(self.ftp_path + ftp_file, self.s3_key + ftp_file)\\n        else:\\n            self.__upload_to_s3_from_ftp(self.ftp_path, self.s3_key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(self, context):\\n        self.ftp_hook = FTPHook(ftp_conn_id=self.ftp_conn_id)\\n        self.s3_hook = S3Hook(self.aws_conn_id)\\n        if self.ftp_filenames:\\n            if isinstance(self.ftp_filenames, str):\\n                self.log.info(f'Getting files in {self.ftp_path}')\\n                list_dir = self.ftp_hook.list_directory(\\n                    path=self.ftp_path,\\n                )\\n                if self.ftp_filenames == '*':\\n                    files = list_dir\\n                else:\\n                    files = list(filter(lambda file: self.ftp_filenames in file, list_dir))\\n                for file in files:\\n                    self.log.info(f'Moving file {file}')\\n                    if self.s3_filenames:\\n                        filename = file.replace(self.ftp_filenames, self.s3_filenames)\\n                    else:\\n                        filename = file\\n                    s3_file_key = f'{self.s3_key}{filename}'\\n                    self.__upload_to_s3_from_ftp(file, s3_file_key)\\n            else:\\n                if self.s3_filenames:\\n                    for ftp_file, s3_file in zip(self.ftp_filenames, self.s3_filenames):\\n                        self.__upload_to_s3_from_ftp(self.ftp_path + ftp_file, self.s3_key + s3_file)\\n                else:\\n                    for ftp_file in self.ftp_filenames:\\n                        self.__upload_to_s3_from_ftp(self.ftp_path + ftp_file, self.s3_key + ftp_file)\\n        else:\\n            self.__upload_to_s3_from_ftp(self.ftp_path, self.s3_key)"
  },
  {
    "code": "def args_check(name, func, provided):\\n        provided = list(provided)\\n        plen = len(provided)\\n        func = getattr(func, '_decorated_function', func)\\n        args, varargs, varkw, defaults = getargspec(func)\\n        args.pop(0)\\n        if defaults:\\n            nondefs = args[:-len(defaults)]\\n        else:\\n            nondefs = args\\n        try:\\n            for arg in nondefs:\\n                provided.pop(0)\\n        except IndexError:\\n            raise TemplateSyntaxError, \"%s requires %d arguments, %d provided\" % (name, len(nondefs), plen)\\n        defaults = defaults and list(defaults) or []\\n        try:\\n            for parg in provided:\\n                defaults.pop(0)\\n        except IndexError:\\n            raise TemplateSyntaxError, \"%s requires %d arguments, %d provided\" % (name, len(nondefs), plen)\\n        return True\\n    args_check = staticmethod(args_check)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def args_check(name, func, provided):\\n        provided = list(provided)\\n        plen = len(provided)\\n        func = getattr(func, '_decorated_function', func)\\n        args, varargs, varkw, defaults = getargspec(func)\\n        args.pop(0)\\n        if defaults:\\n            nondefs = args[:-len(defaults)]\\n        else:\\n            nondefs = args\\n        try:\\n            for arg in nondefs:\\n                provided.pop(0)\\n        except IndexError:\\n            raise TemplateSyntaxError, \"%s requires %d arguments, %d provided\" % (name, len(nondefs), plen)\\n        defaults = defaults and list(defaults) or []\\n        try:\\n            for parg in provided:\\n                defaults.pop(0)\\n        except IndexError:\\n            raise TemplateSyntaxError, \"%s requires %d arguments, %d provided\" % (name, len(nondefs), plen)\\n        return True\\n    args_check = staticmethod(args_check)"
  },
  {
    "code": "def dag_link(attr):\\n    dag_id = attr.get('dag_id')\\n    execution_date = attr.get('execution_date')\\n    url = url_for('Airflow.graph', dag_id=dag_id, execution_date=execution_date)\\n    return Markup('<a href=\"{}\">{}</a>').format(url, dag_id)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Audit Log records View should not contain link if dag_id is None (#13619)",
    "fixed_code": "def dag_link(attr):\\n    dag_id = attr.get('dag_id')\\n    execution_date = attr.get('execution_date')\\n    url = url_for('Airflow.graph', dag_id=dag_id, execution_date=execution_date)\\n    return Markup('<a href=\"{}\">{}</a>').format(url, dag_id) if dag_id else Markup('None')"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        whitelist=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        use_ssl_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        untrusted_caname=dict(required=False, type=\"str\"),\\n        ssl_exemptions_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_anomalies_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        server_cert_mode=dict(required=False, type=\"str\", choices=[\"re-sign\", \"replace\"]),\\n        server_cert=dict(required=False, type=\"str\"),\\n        rpc_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        name=dict(required=False, type=\"str\"),\\n        mapi_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        caname=dict(required=False, type=\"str\"),\\n        ftps=dict(required=False, type=\"list\"),\\n        ftps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_ports=dict(required=False, type=\"str\"),\\n        ftps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ftps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        https=dict(required=False, type=\"list\"),\\n        https_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_ports=dict(required=False, type=\"str\"),\\n        https_status=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\", \"deep-inspection\"]),\\n        https_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        imaps=dict(required=False, type=\"list\"),\\n        imaps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_ports=dict(required=False, type=\"str\"),\\n        imaps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        imaps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        pop3s=dict(required=False, type=\"list\"),\\n        pop3s_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_ports=dict(required=False, type=\"str\"),\\n        pop3s_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        pop3s_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        smtps=dict(required=False, type=\"list\"),\\n        smtps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_ports=dict(required=False, type=\"str\"),\\n        smtps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        smtps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssh=dict(required=False, type=\"list\"),\\n        ssh_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_ports=dict(required=False, type=\"str\"),\\n        ssh_ssh_algorithm=dict(required=False, type=\"str\", choices=[\"compatible\", \"high-encryption\"]),\\n        ssh_ssh_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_ssh_tun_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_unsupported_version=dict(required=False, type=\"str\", choices=[\"block\", \"bypass\"]),\\n        ssl=dict(required=False, type=\"list\"),\\n        ssl_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\",\\n                                                                  \"deep-inspection\"]),\\n        ssl_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssl_exempt=dict(required=False, type=\"list\"),\\n        ssl_exempt_address=dict(required=False, type=\"str\"),\\n        ssl_exempt_address6=dict(required=False, type=\"str\"),\\n        ssl_exempt_fortiguard_category=dict(required=False, type=\"str\"),\\n        ssl_exempt_regex=dict(required=False, type=\"str\"),\\n        ssl_exempt_type=dict(required=False, type=\"str\", choices=[\"fortiguard-category\", \"address\", \"address6\",\\n                                                                  \"wildcard-fqdn\", \"regex\"]),\\n        ssl_exempt_wildcard_fqdn=dict(required=False, type=\"str\"),\\n        ssl_server=dict(required=False, type=\"list\"),\\n        ssl_server_ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ip=dict(required=False, type=\"str\"),\\n        ssl_server_pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ssl_other_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\",\\n                                                                                           \"block\"]),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"whitelist\": module.params[\"whitelist\"],\\n        \"use-ssl-server\": module.params[\"use_ssl_server\"],\\n        \"untrusted-caname\": module.params[\"untrusted_caname\"],\\n        \"ssl-exemptions-log\": module.params[\"ssl_exemptions_log\"],\\n        \"ssl-anomalies-log\": module.params[\"ssl_anomalies_log\"],\\n        \"server-cert-mode\": module.params[\"server_cert_mode\"],\\n        \"server-cert\": module.params[\"server_cert\"],\\n        \"rpc-over-https\": module.params[\"rpc_over_https\"],\\n        \"name\": module.params[\"name\"],\\n        \"mapi-over-https\": module.params[\"mapi_over_https\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"caname\": module.params[\"caname\"],\\n        \"ftps\": {\\n            \"allow-invalid-server-cert\": module.params[\"ftps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ftps_client_cert_request\"],\\n            \"ports\": module.params[\"ftps_ports\"],\\n            \"status\": module.params[\"ftps_status\"],\\n            \"unsupported-ssl\": module.params[\"ftps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ftps_untrusted_cert\"],\\n        },\\n        \"https\": {\\n            \"allow-invalid-server-cert\": module.params[\"https_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"https_client_cert_request\"],\\n            \"ports\": module.params[\"https_ports\"],\\n            \"status\": module.params[\"https_status\"],\\n            \"unsupported-ssl\": module.params[\"https_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"https_untrusted_cert\"],\\n        },\\n        \"imaps\": {\\n            \"allow-invalid-server-cert\": module.params[\"imaps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"imaps_client_cert_request\"],\\n            \"ports\": module.params[\"imaps_ports\"],\\n            \"status\": module.params[\"imaps_status\"],\\n            \"unsupported-ssl\": module.params[\"imaps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"imaps_untrusted_cert\"],\\n        },\\n        \"pop3s\": {\\n            \"allow-invalid-server-cert\": module.params[\"pop3s_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"pop3s_client_cert_request\"],\\n            \"ports\": module.params[\"pop3s_ports\"],\\n            \"status\": module.params[\"pop3s_status\"],\\n            \"unsupported-ssl\": module.params[\"pop3s_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"pop3s_untrusted_cert\"],\\n        },\\n        \"smtps\": {\\n            \"allow-invalid-server-cert\": module.params[\"smtps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"smtps_client_cert_request\"],\\n            \"ports\": module.params[\"smtps_ports\"],\\n            \"status\": module.params[\"smtps_status\"],\\n            \"unsupported-ssl\": module.params[\"smtps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"smtps_untrusted_cert\"],\\n        },\\n        \"ssh\": {\\n            \"inspect-all\": module.params[\"ssh_inspect_all\"],\\n            \"ports\": module.params[\"ssh_ports\"],\\n            \"ssh-algorithm\": module.params[\"ssh_ssh_algorithm\"],\\n            \"ssh-policy-check\": module.params[\"ssh_ssh_policy_check\"],\\n            \"ssh-tun-policy-check\": module.params[\"ssh_ssh_tun_policy_check\"],\\n            \"status\": module.params[\"ssh_status\"],\\n            \"unsupported-version\": module.params[\"ssh_unsupported_version\"],\\n        },\\n        \"ssl\": {\\n            \"allow-invalid-server-cert\": module.params[\"ssl_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ssl_client_cert_request\"],\\n            \"inspect-all\": module.params[\"ssl_inspect_all\"],\\n            \"unsupported-ssl\": module.params[\"ssl_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ssl_untrusted_cert\"],\\n        },\\n        \"ssl-exempt\": {\\n            \"address\": module.params[\"ssl_exempt_address\"],\\n            \"address6\": module.params[\"ssl_exempt_address6\"],\\n            \"fortiguard-category\": module.params[\"ssl_exempt_fortiguard_category\"],\\n            \"regex\": module.params[\"ssl_exempt_regex\"],\\n            \"type\": module.params[\"ssl_exempt_type\"],\\n            \"wildcard-fqdn\": module.params[\"ssl_exempt_wildcard_fqdn\"],\\n        },\\n        \"ssl-server\": {\\n            \"ftps-client-cert-request\": module.params[\"ssl_server_ftps_client_cert_request\"],\\n            \"https-client-cert-request\": module.params[\"ssl_server_https_client_cert_request\"],\\n            \"imaps-client-cert-request\": module.params[\"ssl_server_imaps_client_cert_request\"],\\n            \"ip\": module.params[\"ssl_server_ip\"],\\n            \"pop3s-client-cert-request\": module.params[\"ssl_server_pop3s_client_cert_request\"],\\n            \"smtps-client-cert-request\": module.params[\"ssl_server_smtps_client_cert_request\"],\\n            \"ssl-other-client-cert-request\": module.params[\"ssl_server_ssl_other_client_cert_request\"],\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['ftps', 'https', 'imaps', 'pop3s', 'smtps', 'ssh', 'ssl', 'ssl-exempt', 'ssl-server']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_firewall_ssl_ssh_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        whitelist=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        use_ssl_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        untrusted_caname=dict(required=False, type=\"str\"),\\n        ssl_exemptions_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_anomalies_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        server_cert_mode=dict(required=False, type=\"str\", choices=[\"re-sign\", \"replace\"]),\\n        server_cert=dict(required=False, type=\"str\"),\\n        rpc_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        name=dict(required=False, type=\"str\"),\\n        mapi_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        caname=dict(required=False, type=\"str\"),\\n        ftps=dict(required=False, type=\"list\"),\\n        ftps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_ports=dict(required=False, type=\"str\"),\\n        ftps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ftps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        https=dict(required=False, type=\"list\"),\\n        https_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_ports=dict(required=False, type=\"str\"),\\n        https_status=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\", \"deep-inspection\"]),\\n        https_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        imaps=dict(required=False, type=\"list\"),\\n        imaps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_ports=dict(required=False, type=\"str\"),\\n        imaps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        imaps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        pop3s=dict(required=False, type=\"list\"),\\n        pop3s_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_ports=dict(required=False, type=\"str\"),\\n        pop3s_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        pop3s_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        smtps=dict(required=False, type=\"list\"),\\n        smtps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_ports=dict(required=False, type=\"str\"),\\n        smtps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        smtps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssh=dict(required=False, type=\"list\"),\\n        ssh_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_ports=dict(required=False, type=\"str\"),\\n        ssh_ssh_algorithm=dict(required=False, type=\"str\", choices=[\"compatible\", \"high-encryption\"]),\\n        ssh_ssh_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_ssh_tun_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_unsupported_version=dict(required=False, type=\"str\", choices=[\"block\", \"bypass\"]),\\n        ssl=dict(required=False, type=\"list\"),\\n        ssl_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\",\\n                                                                  \"deep-inspection\"]),\\n        ssl_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssl_exempt=dict(required=False, type=\"list\"),\\n        ssl_exempt_address=dict(required=False, type=\"str\"),\\n        ssl_exempt_address6=dict(required=False, type=\"str\"),\\n        ssl_exempt_fortiguard_category=dict(required=False, type=\"str\"),\\n        ssl_exempt_regex=dict(required=False, type=\"str\"),\\n        ssl_exempt_type=dict(required=False, type=\"str\", choices=[\"fortiguard-category\", \"address\", \"address6\",\\n                                                                  \"wildcard-fqdn\", \"regex\"]),\\n        ssl_exempt_wildcard_fqdn=dict(required=False, type=\"str\"),\\n        ssl_server=dict(required=False, type=\"list\"),\\n        ssl_server_ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ip=dict(required=False, type=\"str\"),\\n        ssl_server_pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ssl_other_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\",\\n                                                                                           \"block\"]),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"whitelist\": module.params[\"whitelist\"],\\n        \"use-ssl-server\": module.params[\"use_ssl_server\"],\\n        \"untrusted-caname\": module.params[\"untrusted_caname\"],\\n        \"ssl-exemptions-log\": module.params[\"ssl_exemptions_log\"],\\n        \"ssl-anomalies-log\": module.params[\"ssl_anomalies_log\"],\\n        \"server-cert-mode\": module.params[\"server_cert_mode\"],\\n        \"server-cert\": module.params[\"server_cert\"],\\n        \"rpc-over-https\": module.params[\"rpc_over_https\"],\\n        \"name\": module.params[\"name\"],\\n        \"mapi-over-https\": module.params[\"mapi_over_https\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"caname\": module.params[\"caname\"],\\n        \"ftps\": {\\n            \"allow-invalid-server-cert\": module.params[\"ftps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ftps_client_cert_request\"],\\n            \"ports\": module.params[\"ftps_ports\"],\\n            \"status\": module.params[\"ftps_status\"],\\n            \"unsupported-ssl\": module.params[\"ftps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ftps_untrusted_cert\"],\\n        },\\n        \"https\": {\\n            \"allow-invalid-server-cert\": module.params[\"https_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"https_client_cert_request\"],\\n            \"ports\": module.params[\"https_ports\"],\\n            \"status\": module.params[\"https_status\"],\\n            \"unsupported-ssl\": module.params[\"https_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"https_untrusted_cert\"],\\n        },\\n        \"imaps\": {\\n            \"allow-invalid-server-cert\": module.params[\"imaps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"imaps_client_cert_request\"],\\n            \"ports\": module.params[\"imaps_ports\"],\\n            \"status\": module.params[\"imaps_status\"],\\n            \"unsupported-ssl\": module.params[\"imaps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"imaps_untrusted_cert\"],\\n        },\\n        \"pop3s\": {\\n            \"allow-invalid-server-cert\": module.params[\"pop3s_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"pop3s_client_cert_request\"],\\n            \"ports\": module.params[\"pop3s_ports\"],\\n            \"status\": module.params[\"pop3s_status\"],\\n            \"unsupported-ssl\": module.params[\"pop3s_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"pop3s_untrusted_cert\"],\\n        },\\n        \"smtps\": {\\n            \"allow-invalid-server-cert\": module.params[\"smtps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"smtps_client_cert_request\"],\\n            \"ports\": module.params[\"smtps_ports\"],\\n            \"status\": module.params[\"smtps_status\"],\\n            \"unsupported-ssl\": module.params[\"smtps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"smtps_untrusted_cert\"],\\n        },\\n        \"ssh\": {\\n            \"inspect-all\": module.params[\"ssh_inspect_all\"],\\n            \"ports\": module.params[\"ssh_ports\"],\\n            \"ssh-algorithm\": module.params[\"ssh_ssh_algorithm\"],\\n            \"ssh-policy-check\": module.params[\"ssh_ssh_policy_check\"],\\n            \"ssh-tun-policy-check\": module.params[\"ssh_ssh_tun_policy_check\"],\\n            \"status\": module.params[\"ssh_status\"],\\n            \"unsupported-version\": module.params[\"ssh_unsupported_version\"],\\n        },\\n        \"ssl\": {\\n            \"allow-invalid-server-cert\": module.params[\"ssl_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ssl_client_cert_request\"],\\n            \"inspect-all\": module.params[\"ssl_inspect_all\"],\\n            \"unsupported-ssl\": module.params[\"ssl_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ssl_untrusted_cert\"],\\n        },\\n        \"ssl-exempt\": {\\n            \"address\": module.params[\"ssl_exempt_address\"],\\n            \"address6\": module.params[\"ssl_exempt_address6\"],\\n            \"fortiguard-category\": module.params[\"ssl_exempt_fortiguard_category\"],\\n            \"regex\": module.params[\"ssl_exempt_regex\"],\\n            \"type\": module.params[\"ssl_exempt_type\"],\\n            \"wildcard-fqdn\": module.params[\"ssl_exempt_wildcard_fqdn\"],\\n        },\\n        \"ssl-server\": {\\n            \"ftps-client-cert-request\": module.params[\"ssl_server_ftps_client_cert_request\"],\\n            \"https-client-cert-request\": module.params[\"ssl_server_https_client_cert_request\"],\\n            \"imaps-client-cert-request\": module.params[\"ssl_server_imaps_client_cert_request\"],\\n            \"ip\": module.params[\"ssl_server_ip\"],\\n            \"pop3s-client-cert-request\": module.params[\"ssl_server_pop3s_client_cert_request\"],\\n            \"smtps-client-cert-request\": module.params[\"ssl_server_smtps_client_cert_request\"],\\n            \"ssl-other-client-cert-request\": module.params[\"ssl_server_ssl_other_client_cert_request\"],\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['ftps', 'https', 'imaps', 'pop3s', 'smtps', 'ssh', 'ssl', 'ssl-exempt', 'ssl-server']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_firewall_ssl_ssh_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def  __init__(self, shared_name, init_op_fn, name=None, v2=False):\\n\\tself._resource = gen_summary_ops.summary_writer(\\n\\t\\tshared_name=shared_name, name=name)\\n\\tself._init_op_fn = init_op_fn\\n\\tself._init_op = init_op_fn(self._resource)\\n\\tself._v2 = v2\\n\\tself._closed = False\\n\\tif context.executing_eagerly():\\n\\t  self._resource_deleter = resource_variable_ops.EagerResourceDeleter(\\n\\t\\t  handle=self._resource, handle_device=\"cpu:0\")\\n\\telse:\\n\\t  ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME, self._init_op)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def  __init__(self, shared_name, init_op_fn, name=None, v2=False):\\n\\tself._resource = gen_summary_ops.summary_writer(\\n\\t\\tshared_name=shared_name, name=name)\\n\\tself._init_op_fn = init_op_fn\\n\\tself._init_op = init_op_fn(self._resource)\\n\\tself._v2 = v2\\n\\tself._closed = False\\n\\tif context.executing_eagerly():\\n\\t  self._resource_deleter = resource_variable_ops.EagerResourceDeleter(\\n\\t\\t  handle=self._resource, handle_device=\"cpu:0\")\\n\\telse:\\n\\t  ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME, self._init_op)"
  },
  {
    "code": "def set_autocommit(self, autocommit=True):\\n        self.validate_no_atomic_block()\\n        self.ensure_connection()\\n        self._set_autocommit(autocommit)\\n        self.autocommit = autocommit",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Improved the API of set_autocommit.",
    "fixed_code": "def set_autocommit(self, autocommit):\\n        self.validate_no_atomic_block()\\n        self.ensure_connection()\\n        self._set_autocommit(autocommit)\\n        self.autocommit = autocommit"
  },
  {
    "code": "def state_present(module, existing, proposed, candidate):\\n    fixed_proposed, commands = fix_proposed(module, proposed, existing)\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, fixed_proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if key == 'address-family':\\n            addr_family_command = \"address-family {0} {1}\".format(\\n                module.params['afi'], module.params['safi'])\\n            if addr_family_command not in commands:\\n                commands.append(addr_family_command)\\n        elif key.startswith('table-map'):\\n            table_map_commands = get_table_map_command(module, existing, key, value)\\n            if table_map_commands:\\n                commands.extend(table_map_commands)\\n        elif value is True:\\n            commands.append(key)\\n        elif value is False:\\n            commands.append('no {0}'.format(key))\\n        elif value == 'default':\\n            if key in PARAM_TO_DEFAULT_KEYMAP:\\n                commands.append('{0} {1}'.format(key, PARAM_TO_DEFAULT_KEYMAP[key]))\\n            elif existing_commands.get(key):\\n                if key == 'table-map-filter':\\n                    default_tmf_command = get_default_table_map_filter(existing)\\n                    if default_tmf_command:\\n                        commands.extend(default_tmf_command)\\n                else:\\n                    existing_value = existing_commands.get(key)\\n                    default_command = default_existing(existing_value, key, value)\\n                    if default_command:\\n                        commands.extend(default_command)\\n        else:\\n            if key == 'network':\\n                network_commands = get_network_command(existing, key, value)\\n                if network_commands:\\n                    commands.extend(network_commands)\\n            elif key == 'inject-map':\\n                inject_map_commands = get_inject_map_command(existing, key, value)\\n                if inject_map_commands:\\n                    commands.extend(inject_map_commands)\\n            elif key == 'redistribute':\\n                redistribute_commands = get_redistribute_command(existing, key, value)\\n                if redistribute_commands:\\n                    commands.extend(redistribute_commands)\\n            else:\\n                command = '{0} {1}'.format(key, value)\\n                commands.append(command)\\n    if commands:\\n        parents = [\"router bgp {0}\".format(module.params['asn'])]\\n        if module.params['vrf'] != 'default':\\n            parents.append('vrf {0}'.format(module.params['vrf']))\\n        addr_family_command = \"address-family {0} {1}\".format(module.params['afi'],\\n                                                              module.params['safi'])\\n        parents.append(addr_family_command)\\n        if addr_family_command in commands:\\n            commands.remove(addr_family_command)\\n        candidate.add(commands, parents=parents)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def state_present(module, existing, proposed, candidate):\\n    fixed_proposed, commands = fix_proposed(module, proposed, existing)\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, fixed_proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if key == 'address-family':\\n            addr_family_command = \"address-family {0} {1}\".format(\\n                module.params['afi'], module.params['safi'])\\n            if addr_family_command not in commands:\\n                commands.append(addr_family_command)\\n        elif key.startswith('table-map'):\\n            table_map_commands = get_table_map_command(module, existing, key, value)\\n            if table_map_commands:\\n                commands.extend(table_map_commands)\\n        elif value is True:\\n            commands.append(key)\\n        elif value is False:\\n            commands.append('no {0}'.format(key))\\n        elif value == 'default':\\n            if key in PARAM_TO_DEFAULT_KEYMAP:\\n                commands.append('{0} {1}'.format(key, PARAM_TO_DEFAULT_KEYMAP[key]))\\n            elif existing_commands.get(key):\\n                if key == 'table-map-filter':\\n                    default_tmf_command = get_default_table_map_filter(existing)\\n                    if default_tmf_command:\\n                        commands.extend(default_tmf_command)\\n                else:\\n                    existing_value = existing_commands.get(key)\\n                    default_command = default_existing(existing_value, key, value)\\n                    if default_command:\\n                        commands.extend(default_command)\\n        else:\\n            if key == 'network':\\n                network_commands = get_network_command(existing, key, value)\\n                if network_commands:\\n                    commands.extend(network_commands)\\n            elif key == 'inject-map':\\n                inject_map_commands = get_inject_map_command(existing, key, value)\\n                if inject_map_commands:\\n                    commands.extend(inject_map_commands)\\n            elif key == 'redistribute':\\n                redistribute_commands = get_redistribute_command(existing, key, value)\\n                if redistribute_commands:\\n                    commands.extend(redistribute_commands)\\n            else:\\n                command = '{0} {1}'.format(key, value)\\n                commands.append(command)\\n    if commands:\\n        parents = [\"router bgp {0}\".format(module.params['asn'])]\\n        if module.params['vrf'] != 'default':\\n            parents.append('vrf {0}'.format(module.params['vrf']))\\n        addr_family_command = \"address-family {0} {1}\".format(module.params['afi'],\\n                                                              module.params['safi'])\\n        parents.append(addr_family_command)\\n        if addr_family_command in commands:\\n            commands.remove(addr_family_command)\\n        candidate.add(commands, parents=parents)"
  },
  {
    "code": "def toString(self, to_stdout=True, verbose=False, colSpace=15, nanRep=None):\\n        series = self._series\\n        skeys = sorted(series.keys())\\n        if len(skeys) == 0 or len(self.index) == 0:\\n            output = 'Empty DataFrame\\n'\\n            output += self.index.__repr__()\\n        else:\\n            idxSpace = max([len(str(idx)) for idx in self.index]) + 4\\n            head = _pfixed('', idxSpace)\\n            if verbose:\\n                colSpace = max([len(c) for c in self.columns]) + 4\\n            for h in skeys:\\n                head += _pfixed(h, colSpace)\\n            output = head + '\\n'\\n            for idx in self.index:\\n                ot = _pfixed(idx, idxSpace)\\n                for k in skeys:\\n                    ot += _pfixed(series[k][idx], colSpace, nanRep=nanRep)\\n                output += ot + '\\n'\\n        if to_stdout:\\n            print output\\n        else:\\n            return output",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "not weighting untransformed data in PanelOLS. changed DataFrame and DataMatrix.toString to take a buffer instead of a to_stdout flag",
    "fixed_code": "def toString(self, buffer=sys.stdout, verbose=False,\\n                 colSpace=15, nanRep=None, formatters=None,\\n                 float_format=None):\\n        series = self._series\\n        columns = sorted(series.keys())\\n        formatters = formatters or {}"
  },
  {
    "code": "def fit(self, X, y=None):\\n        X = self._validate_data(X, ensure_min_samples=2, estimator=self)\\n        memory = check_memory(self.memory)\\n        if self.n_clusters is not None and self.n_clusters <= 0:\\n            raise ValueError(\\n                \"n_clusters should be an integer greater than 0. %s was provided.\"\\n                % str(self.n_clusters)\\n            )\\n        if not ((self.n_clusters is None) ^ (self.distance_threshold is None)):\\n            raise ValueError(\\n                \"Exactly one of n_clusters and \"\\n                \"distance_threshold has to be set, and the other \"\\n                \"needs to be None.\"\\n            )\\n        if self.distance_threshold is not None and not self.compute_full_tree:\\n            raise ValueError(\\n                \"compute_full_tree must be True if distance_threshold is set.\"\\n            )\\n        if self.linkage == \"ward\" and self.affinity != \"euclidean\":\\n            raise ValueError(\\n                \"%s was provided as affinity. Ward can only \"\\n                \"work with euclidean distances.\" % (self.affinity,)\\n            )\\n        if self.linkage not in _TREE_BUILDERS:\\n            raise ValueError(\\n                \"Unknown linkage type %s. Valid options are %s\"\\n                % (self.linkage, _TREE_BUILDERS.keys())\\n            )\\n        tree_builder = _TREE_BUILDERS[self.linkage]\\n        connectivity = self.connectivity\\n        if self.connectivity is not None:\\n            if callable(self.connectivity):\\n                connectivity = self.connectivity(X)\\n            connectivity = check_array(\\n                connectivity, accept_sparse=[\"csr\", \"coo\", \"lil\"]\\n            )\\n        n_samples = len(X)\\n        compute_full_tree = self.compute_full_tree\\n        if self.connectivity is None:\\n            compute_full_tree = True\\n        if compute_full_tree == \"auto\":\\n            if self.distance_threshold is not None:\\n                compute_full_tree = True\\n            else:\\n                compute_full_tree = self.n_clusters < max(100, 0.02 * n_samples)\\n        n_clusters = self.n_clusters\\n        if compute_full_tree:\\n            n_clusters = None\\n        kwargs = {}\\n        if self.linkage != \"ward\":\\n            kwargs[\"linkage\"] = self.linkage\\n            kwargs[\"affinity\"] = self.affinity\\n        distance_threshold = self.distance_threshold\\n        return_distance = (distance_threshold is not None) or self.compute_distances\\n        out = memory.cache(tree_builder)(\\n            X,\\n            connectivity=connectivity,\\n            n_clusters=n_clusters,\\n            return_distance=return_distance,\\n            **kwargs,\\n        )\\n        (self.children_, self.n_connected_components_, self.n_leaves_, parents) = out[\\n            :4\\n        ]\\n        if return_distance:\\n            self.distances_ = out[-1]\\n        if self.distance_threshold is not None:  \\n            self.n_clusters_ = (\\n                np.count_nonzero(self.distances_ >= distance_threshold) + 1\\n            )\\n        else:  \\n            self.n_clusters_ = self.n_clusters\\n        if compute_full_tree:\\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_, self.n_leaves_)\\n        else:\\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\\n            labels = np.copy(labels[:n_samples])\\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\\n        return self\\n    def fit_predict(self, X, y=None):\\n        return super().fit_predict(X, y)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, y=None):\\n        X = self._validate_data(X, ensure_min_samples=2, estimator=self)\\n        memory = check_memory(self.memory)\\n        if self.n_clusters is not None and self.n_clusters <= 0:\\n            raise ValueError(\\n                \"n_clusters should be an integer greater than 0. %s was provided.\"\\n                % str(self.n_clusters)\\n            )\\n        if not ((self.n_clusters is None) ^ (self.distance_threshold is None)):\\n            raise ValueError(\\n                \"Exactly one of n_clusters and \"\\n                \"distance_threshold has to be set, and the other \"\\n                \"needs to be None.\"\\n            )\\n        if self.distance_threshold is not None and not self.compute_full_tree:\\n            raise ValueError(\\n                \"compute_full_tree must be True if distance_threshold is set.\"\\n            )\\n        if self.linkage == \"ward\" and self.affinity != \"euclidean\":\\n            raise ValueError(\\n                \"%s was provided as affinity. Ward can only \"\\n                \"work with euclidean distances.\" % (self.affinity,)\\n            )\\n        if self.linkage not in _TREE_BUILDERS:\\n            raise ValueError(\\n                \"Unknown linkage type %s. Valid options are %s\"\\n                % (self.linkage, _TREE_BUILDERS.keys())\\n            )\\n        tree_builder = _TREE_BUILDERS[self.linkage]\\n        connectivity = self.connectivity\\n        if self.connectivity is not None:\\n            if callable(self.connectivity):\\n                connectivity = self.connectivity(X)\\n            connectivity = check_array(\\n                connectivity, accept_sparse=[\"csr\", \"coo\", \"lil\"]\\n            )\\n        n_samples = len(X)\\n        compute_full_tree = self.compute_full_tree\\n        if self.connectivity is None:\\n            compute_full_tree = True\\n        if compute_full_tree == \"auto\":\\n            if self.distance_threshold is not None:\\n                compute_full_tree = True\\n            else:\\n                compute_full_tree = self.n_clusters < max(100, 0.02 * n_samples)\\n        n_clusters = self.n_clusters\\n        if compute_full_tree:\\n            n_clusters = None\\n        kwargs = {}\\n        if self.linkage != \"ward\":\\n            kwargs[\"linkage\"] = self.linkage\\n            kwargs[\"affinity\"] = self.affinity\\n        distance_threshold = self.distance_threshold\\n        return_distance = (distance_threshold is not None) or self.compute_distances\\n        out = memory.cache(tree_builder)(\\n            X,\\n            connectivity=connectivity,\\n            n_clusters=n_clusters,\\n            return_distance=return_distance,\\n            **kwargs,\\n        )\\n        (self.children_, self.n_connected_components_, self.n_leaves_, parents) = out[\\n            :4\\n        ]\\n        if return_distance:\\n            self.distances_ = out[-1]\\n        if self.distance_threshold is not None:  \\n            self.n_clusters_ = (\\n                np.count_nonzero(self.distances_ >= distance_threshold) + 1\\n            )\\n        else:  \\n            self.n_clusters_ = self.n_clusters\\n        if compute_full_tree:\\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_, self.n_leaves_)\\n        else:\\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\\n            labels = np.copy(labels[:n_samples])\\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\\n        return self\\n    def fit_predict(self, X, y=None):\\n        return super().fit_predict(X, y)"
  },
  {
    "code": "def generate_put_template(template, use_ints = True, use_floats = True):\\n    floats_list = [\\n        ('float64', 'float64_t', 'float64_t', 'np.float64'),\\n        ('float32', 'float32_t', 'float32_t', 'np.float32'),\\n        ]\\n    ints_list = [\\n        ('int8',  'int8_t',  'float32_t', 'np.float32'),\\n        ('int16', 'int16_t', 'float32_t', 'np.float32'),\\n        ('int32', 'int32_t', 'float64_t', 'np.float64'),\\n        ('int64', 'int64_t', 'float64_t', 'np.float64'),\\n        ]\\n    function_list = []\\n    if use_floats:\\n        function_list.extend(floats_list)\\n    if use_ints:\\n        function_list.extend(ints_list)\\n    output = StringIO()\\n    for name, c_type, dest_type, dest_dtype in function_list:\\n        func = template % {'name' : name,\\n                           'c_type' : c_type,\\n                           'dest_type' : dest_type.replace('_t', ''),\\n                           'dest_type2' : dest_type,\\n                           'dest_dtype' : dest_dtype}\\n        output.write(func)\\n    return output.getvalue()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: numeric_only must be False for count",
    "fixed_code": "def generate_put_template(template, use_ints = True, use_floats = True,\\n                          use_objects=False):\\n    floats_list = [\\n        ('float64', 'float64_t', 'float64_t', 'np.float64'),\\n        ('float32', 'float32_t', 'float32_t', 'np.float32'),\\n        ]\\n    ints_list = [\\n        ('int8',  'int8_t',  'float32_t', 'np.float32'),\\n        ('int16', 'int16_t', 'float32_t', 'np.float32'),\\n        ('int32', 'int32_t', 'float64_t', 'np.float64'),\\n        ('int64', 'int64_t', 'float64_t', 'np.float64'),\\n        ]\\n    object_list = [('object', 'object', 'float64_t', 'np.float64')]\\n    function_list = []\\n    if use_floats:\\n        function_list.extend(floats_list)\\n    if use_ints:\\n        function_list.extend(ints_list)\\n    if use_objects:\\n        function_list.extend(object_list)\\n    output = StringIO()\\n    for name, c_type, dest_type, dest_dtype in function_list:\\n        func = template % {'name' : name,\\n                           'c_type' : c_type,\\n                           'dest_type' : dest_type.replace('_t', ''),\\n                           'dest_type2' : dest_type,\\n                           'dest_dtype' : dest_dtype}\\n        output.write(func)\\n    return output.getvalue()"
  },
  {
    "code": "def parse_from_args(self, param_name, task_name, args, params):\\n\\t\\tdest = self.parser_dest(param_name, task_name, glob=False)\\n\\t\\tif dest is not None:\\n\\t\\t\\tvalue = getattr(args, dest, None)\\n\\t\\t\\tparams[param_name] = self.parse_from_input(param_name, value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix bug where command line Tasks skip config\\n\\nBefore this patch, when you run a task from the command line, it'll\\nignore the defaults you've specified in the config. By config I mean the\\n`[MyClass] my_param: myvalue` syntax in `/etc/luigi/client.cfg`.\\n\\nAfter this patch, the config is honored.\\n\\nI also added another small and related fix, the default shown when doing\\n`--help` will now also honor the config and not only the `default=` value.",
    "fixed_code": "def parse_from_args(self, param_name, task_name, args, params):\\n\\t\\tdest = self.parser_dest(param_name, task_name, glob=False)\\n\\t\\tif dest is not None:\\n\\t\\t\\tvalue = getattr(args, dest, None)\\n\\t\\t\\tparams[param_name] = self.parse_from_input(param_name, value, task_name=task_name)"
  },
  {
    "code": "def infer_compression(\\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str]\\n) -> Optional[str]:\\n    if compression is None:\\n        return None\\n    if compression == \"infer\":\\n        filepath_or_buffer = stringify_path(filepath_or_buffer)\\n        if not isinstance(filepath_or_buffer, str):\\n            return None\\n        for compression, extension in _compression_to_extension.items():\\n            if filepath_or_buffer.lower().endswith(extension):\\n                return compression\\n        return None\\n    if compression in _compression_to_extension:\\n        return compression\\n    msg = f\"Unrecognized compression type: {compression}\"\\n    valid = [\"infer\", None] + sorted(_compression_to_extension)\\n    msg += f\"\\nValid compression types are {valid}\"\\n    raise ValueError(msg)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def infer_compression(\\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str]\\n) -> Optional[str]:\\n    if compression is None:\\n        return None\\n    if compression == \"infer\":\\n        filepath_or_buffer = stringify_path(filepath_or_buffer)\\n        if not isinstance(filepath_or_buffer, str):\\n            return None\\n        for compression, extension in _compression_to_extension.items():\\n            if filepath_or_buffer.lower().endswith(extension):\\n                return compression\\n        return None\\n    if compression in _compression_to_extension:\\n        return compression\\n    msg = f\"Unrecognized compression type: {compression}\"\\n    valid = [\"infer\", None] + sorted(_compression_to_extension)\\n    msg += f\"\\nValid compression types are {valid}\"\\n    raise ValueError(msg)"
  },
  {
    "code": "def field_choices(self, field, request, model_admin):\\n        ordering = self.field_admin_ordering(field, request, model_admin)\\n        return field.get_choices(include_blank=False, ordering=ordering)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def field_choices(self, field, request, model_admin):\\n        ordering = self.field_admin_ordering(field, request, model_admin)\\n        return field.get_choices(include_blank=False, ordering=ordering)"
  },
  {
    "code": "def discover_device(fmg, paramgram):\\n    datagram = {\\n        \"odd_request_form\": \"True\",\\n        \"device\": {\"adm_usr\": paramgram[\"device_username\"],\\n                   \"adm_pass\": paramgram[\"device_password\"],\\n                   \"ip\": paramgram[\"device_ip\"]}\\n    }\\n    url = '/dvm/cmd/discover/device/'\\n    response = fmg.execute(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_device (#52767)",
    "fixed_code": "def discover_device(fmgr, paramgram):\\n    datagram = {\\n        \"odd_request_form\": \"True\",\\n        \"device\": {\"adm_usr\": paramgram[\"device_username\"],\\n                   \"adm_pass\": paramgram[\"device_password\"],\\n                   \"ip\": paramgram[\"device_ip\"]}\\n    }\\n    url = '/dvm/cmd/discover/device/'\\n    response = fmgr.process_request(url, datagram, FMGRMethods.EXEC)\\n    return response"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        name=dict(type='str', required=True),\\n        image=dict(type='str'),\\n        state=dict(type='str', default='present', choices=['present', 'absent']),\\n        mounts=dict(type='list', elements='dict', options=dict(\\n            source=dict(type='str', required=True),\\n            target=dict(type='str', required=True),\\n            type=dict(\\n                type='str',\\n                default='bind',\\n                choices=['bind', 'volume', 'tmpfs']\\n            ),\\n            readonly=dict(type='bool', default=False),\\n        )),\\n        configs=dict(type='list', elements='dict', options=dict(\\n            config_id=dict(type='str', required=True),\\n            config_name=dict(type='str', required=True),\\n            filename=dict(type='str'),\\n            uid=dict(type='int', default=0),\\n            gid=dict(type='int', default=0),\\n            mode=dict(type='int', default=0o444),\\n        )),\\n        secrets=dict(type='list', elements='dict', options=dict(\\n            secret_id=dict(type='str', required=True),\\n            secret_name=dict(type='str', required=True),\\n            filename=dict(type='str'),\\n            uid=dict(type='int', default=0),\\n            gid=dict(type='int', default=0),\\n            mode=dict(type='int', default=0o444),\\n        )),\\n        networks=dict(type='list', elements='str'),\\n        command=dict(type='raw'),\\n        args=dict(type='list', elements='str'),\\n        env=dict(type='raw'),\\n        env_files=dict(type='list', elements='path'),\\n        force_update=dict(type='bool', default=False),\\n        groups=dict(type='list', elements='str'),\\n        log_driver=dict(type='str'),\\n        log_driver_options=dict(type='dict'),\\n        publish=dict(type='list', elements='dict', options=dict(\\n            published_port=dict(type='int', required=True),\\n            target_port=dict(type='int', required=True),\\n            protocol=dict(type='str', default='tcp', choices=('tcp', 'udp')),\\n            mode=dict(type='str', choices=('ingress', 'host')),\\n        )),\\n        constraints=dict(type='list'),\\n        placement_preferences=dict(type='list'),\\n        tty=dict(type='bool'),\\n        dns=dict(type='list'),\\n        dns_search=dict(type='list'),\\n        dns_options=dict(type='list'),\\n        healthcheck=dict(type='dict', options=dict(\\n            test=dict(type='raw'),\\n            interval=dict(type='str'),\\n            timeout=dict(type='str'),\\n            start_period=dict(type='str'),\\n            retries=dict(type='int'),\\n        )),\\n        hostname=dict(type='str'),\\n        labels=dict(type='dict'),\\n        container_labels=dict(type='dict'),\\n        mode=dict(type='str', default='replicated'),\\n        replicas=dict(type='int', default=-1),\\n        endpoint_mode=dict(type='str', choices=['vip', 'dnsrr']),\\n        stop_signal=dict(type='str'),\\n        limit_cpu=dict(type='float'),\\n        limit_memory=dict(type='str'),\\n        reserve_cpu=dict(type='float'),\\n        reserve_memory=dict(type='str'),\\n        resolve_image=dict(type='bool', default=True),\\n        restart_policy=dict(type='str', choices=['none', 'on-failure', 'any']),\\n        restart_policy_delay=dict(type='raw'),\\n        restart_policy_attempts=dict(type='int'),\\n        restart_policy_window=dict(type='raw'),\\n        update_delay=dict(type='raw'),\\n        update_parallelism=dict(type='int'),\\n        update_failure_action=dict(type='str', choices=['continue', 'pause']),\\n        update_monitor=dict(type='raw'),\\n        update_max_failure_ratio=dict(type='float'),\\n        update_order=dict(type='str'),\\n        user=dict(type='str'),\\n        working_dir=dict(type='str'),\\n    )\\n    option_minimal_versions = dict(\\n        dns=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        dns_options=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        dns_search=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        endpoint_mode=dict(docker_py_version='3.0.0', docker_api_version='1.25'),\\n        force_update=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        healthcheck=dict(docker_py_version='2.0.0', docker_api_version='1.25'),\\n        hostname=dict(docker_py_version='2.2.0', docker_api_version='1.25'),\\n        groups=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        tty=dict(docker_py_version='2.4.0', docker_api_version='1.25'),\\n        secrets=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        configs=dict(docker_py_version='2.6.0', docker_api_version='1.30'),\\n        update_max_failure_ratio=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        update_monitor=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        update_order=dict(docker_py_version='2.7.0', docker_api_version='1.29'),\\n        stop_signal=dict(docker_py_version='2.6.0', docker_api_version='1.28'),\\n        placement_preferences=dict(docker_py_version='2.4.0', docker_api_version='1.27'),\\n        publish=dict(docker_py_version='3.0.0', docker_api_version='1.25'),\\n        publish_mode=dict(\\n            docker_py_version='3.0.0',\\n            docker_api_version='1.25',\\n            detect_usage=_detect_publish_mode_usage,\\n            usage_msg='set publish.mode'\\n        ),\\n        healthcheck_start_period=dict(\\n            docker_py_version='2.4.0',\\n            docker_api_version='1.25',\\n            detect_usage=_detect_healthcheck_start_period,\\n            usage_msg='set healthcheck.start_period'\\n        )\\n    )\\n    required_if = [\\n        ('state', 'present', ['image'])\\n    ]\\n    client = AnsibleDockerClient(\\n        argument_spec=argument_spec,\\n        required_if=required_if,\\n        supports_check_mode=True,\\n        min_docker_version='2.0.0',\\n        min_docker_api_version='1.24',\\n        option_minimal_versions=option_minimal_versions,\\n    )\\n    dsm = DockerServiceManager(client)\\n    msg, changed, rebuilt, changes, facts = dsm.run_safe()\\n    results = dict(\\n        msg=msg,\\n        changed=changed,\\n        rebuilt=rebuilt,\\n        changes=changes,\\n        ansible_docker_service=facts,\\n    )\\n    if client.module._diff:\\n        before, after = dsm.diff_tracker.get_before_after()\\n        results['diff'] = dict(before=before, after=after)\\n    client.module.exit_json(**results)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        name=dict(type='str', required=True),\\n        image=dict(type='str'),\\n        state=dict(type='str', default='present', choices=['present', 'absent']),\\n        mounts=dict(type='list', elements='dict', options=dict(\\n            source=dict(type='str', required=True),\\n            target=dict(type='str', required=True),\\n            type=dict(\\n                type='str',\\n                default='bind',\\n                choices=['bind', 'volume', 'tmpfs']\\n            ),\\n            readonly=dict(type='bool', default=False),\\n        )),\\n        configs=dict(type='list', elements='dict', options=dict(\\n            config_id=dict(type='str', required=True),\\n            config_name=dict(type='str', required=True),\\n            filename=dict(type='str'),\\n            uid=dict(type='int', default=0),\\n            gid=dict(type='int', default=0),\\n            mode=dict(type='int', default=0o444),\\n        )),\\n        secrets=dict(type='list', elements='dict', options=dict(\\n            secret_id=dict(type='str', required=True),\\n            secret_name=dict(type='str', required=True),\\n            filename=dict(type='str'),\\n            uid=dict(type='int', default=0),\\n            gid=dict(type='int', default=0),\\n            mode=dict(type='int', default=0o444),\\n        )),\\n        networks=dict(type='list', elements='str'),\\n        command=dict(type='raw'),\\n        args=dict(type='list', elements='str'),\\n        env=dict(type='raw'),\\n        env_files=dict(type='list', elements='path'),\\n        force_update=dict(type='bool', default=False),\\n        groups=dict(type='list', elements='str'),\\n        log_driver=dict(type='str'),\\n        log_driver_options=dict(type='dict'),\\n        publish=dict(type='list', elements='dict', options=dict(\\n            published_port=dict(type='int', required=True),\\n            target_port=dict(type='int', required=True),\\n            protocol=dict(type='str', default='tcp', choices=('tcp', 'udp')),\\n            mode=dict(type='str', choices=('ingress', 'host')),\\n        )),\\n        constraints=dict(type='list'),\\n        placement_preferences=dict(type='list'),\\n        tty=dict(type='bool'),\\n        dns=dict(type='list'),\\n        dns_search=dict(type='list'),\\n        dns_options=dict(type='list'),\\n        healthcheck=dict(type='dict', options=dict(\\n            test=dict(type='raw'),\\n            interval=dict(type='str'),\\n            timeout=dict(type='str'),\\n            start_period=dict(type='str'),\\n            retries=dict(type='int'),\\n        )),\\n        hostname=dict(type='str'),\\n        labels=dict(type='dict'),\\n        container_labels=dict(type='dict'),\\n        mode=dict(type='str', default='replicated'),\\n        replicas=dict(type='int', default=-1),\\n        endpoint_mode=dict(type='str', choices=['vip', 'dnsrr']),\\n        stop_signal=dict(type='str'),\\n        limit_cpu=dict(type='float'),\\n        limit_memory=dict(type='str'),\\n        reserve_cpu=dict(type='float'),\\n        reserve_memory=dict(type='str'),\\n        resolve_image=dict(type='bool', default=True),\\n        restart_policy=dict(type='str', choices=['none', 'on-failure', 'any']),\\n        restart_policy_delay=dict(type='raw'),\\n        restart_policy_attempts=dict(type='int'),\\n        restart_policy_window=dict(type='raw'),\\n        update_delay=dict(type='raw'),\\n        update_parallelism=dict(type='int'),\\n        update_failure_action=dict(type='str', choices=['continue', 'pause']),\\n        update_monitor=dict(type='raw'),\\n        update_max_failure_ratio=dict(type='float'),\\n        update_order=dict(type='str'),\\n        user=dict(type='str'),\\n        working_dir=dict(type='str'),\\n    )\\n    option_minimal_versions = dict(\\n        dns=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        dns_options=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        dns_search=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        endpoint_mode=dict(docker_py_version='3.0.0', docker_api_version='1.25'),\\n        force_update=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        healthcheck=dict(docker_py_version='2.0.0', docker_api_version='1.25'),\\n        hostname=dict(docker_py_version='2.2.0', docker_api_version='1.25'),\\n        groups=dict(docker_py_version='2.6.0', docker_api_version='1.25'),\\n        tty=dict(docker_py_version='2.4.0', docker_api_version='1.25'),\\n        secrets=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        configs=dict(docker_py_version='2.6.0', docker_api_version='1.30'),\\n        update_max_failure_ratio=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        update_monitor=dict(docker_py_version='2.1.0', docker_api_version='1.25'),\\n        update_order=dict(docker_py_version='2.7.0', docker_api_version='1.29'),\\n        stop_signal=dict(docker_py_version='2.6.0', docker_api_version='1.28'),\\n        placement_preferences=dict(docker_py_version='2.4.0', docker_api_version='1.27'),\\n        publish=dict(docker_py_version='3.0.0', docker_api_version='1.25'),\\n        publish_mode=dict(\\n            docker_py_version='3.0.0',\\n            docker_api_version='1.25',\\n            detect_usage=_detect_publish_mode_usage,\\n            usage_msg='set publish.mode'\\n        ),\\n        healthcheck_start_period=dict(\\n            docker_py_version='2.4.0',\\n            docker_api_version='1.25',\\n            detect_usage=_detect_healthcheck_start_period,\\n            usage_msg='set healthcheck.start_period'\\n        )\\n    )\\n    required_if = [\\n        ('state', 'present', ['image'])\\n    ]\\n    client = AnsibleDockerClient(\\n        argument_spec=argument_spec,\\n        required_if=required_if,\\n        supports_check_mode=True,\\n        min_docker_version='2.0.0',\\n        min_docker_api_version='1.24',\\n        option_minimal_versions=option_minimal_versions,\\n    )\\n    dsm = DockerServiceManager(client)\\n    msg, changed, rebuilt, changes, facts = dsm.run_safe()\\n    results = dict(\\n        msg=msg,\\n        changed=changed,\\n        rebuilt=rebuilt,\\n        changes=changes,\\n        ansible_docker_service=facts,\\n    )\\n    if client.module._diff:\\n        before, after = dsm.diff_tracker.get_before_after()\\n        results['diff'] = dict(before=before, after=after)\\n    client.module.exit_json(**results)"
  },
  {
    "code": "def get_export_symbols(self, ext):\\n        initfunc_name = \"init\" + ext.name.split('.')[-1]\\n        if initfunc_name not in ext.export_symbols:\\n            ext.export_symbols.append(initfunc_name)\\n        return ext.export_symbols",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fixed #6459: distutils.command.build_ext.get_export_symbols now uses 'PyInit'",
    "fixed_code": "def get_export_symbols(self, ext):\\n        initfunc_name = \"PyInit_\" + ext.name.split('.')[-1]\\n        if initfunc_name not in ext.export_symbols:\\n            ext.export_symbols.append(initfunc_name)\\n        return ext.export_symbols"
  },
  {
    "code": "def _validate_argument_values(argument_spec, parameters, options_context=None, errors=None):\\n    if errors is None:\\n        errors = AnsibleValidationErrorMultiple()\\n    for param, spec in argument_spec.items():\\n        choices = spec.get('choices')\\n        if choices is None:\\n            continue\\n        if isinstance(choices, (frozenset, KeysView, Sequence)) and not isinstance(choices, (binary_type, text_type)):\\n            if param in parameters:\\n                if isinstance(parameters[param], list):\\n                    diff_list = \", \".join([item for item in parameters[param] if item not in choices])\\n                    if diff_list:\\n                        choices_str = \", \".join([to_native(c) for c in choices])\\n                        msg = \"value of %s must be one or more of: %s. Got no match for: %s\" % (param, choices_str, diff_list)\\n                        if options_context:\\n                            msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\\n                        errors.append(ArgumentValueError(msg))\\n                elif parameters[param] not in choices:\\n                    if parameters[param] == 'False':\\n                        overlap = BOOLEANS_FALSE.intersection(choices)\\n                        if len(overlap) == 1:\\n                            (parameters[param],) = overlap\\n                    if parameters[param] == 'True':\\n                        overlap = BOOLEANS_TRUE.intersection(choices)\\n                        if len(overlap) == 1:\\n                            (parameters[param],) = overlap\\n                    if parameters[param] not in choices:\\n                        choices_str = \", \".join([to_native(c) for c in choices])\\n                        msg = \"value of %s must be one of: %s, got: %s\" % (param, choices_str, parameters[param])\\n                        if options_context:\\n                            msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\\n                        errors.append(ArgumentValueError(msg))\\n        else:\\n            msg = \"internal error: choices for argument %s are not iterable: %s\" % (param, choices)\\n            if options_context:\\n                msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\\n            errors.append(ArgumentTypeError(msg))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "parameters: handle blank values when argument is a list (#77119)\\n\\nFixes: #77108",
    "fixed_code": "def _validate_argument_values(argument_spec, parameters, options_context=None, errors=None):\\n    if errors is None:\\n        errors = AnsibleValidationErrorMultiple()\\n    for param, spec in argument_spec.items():\\n        choices = spec.get('choices')\\n        if choices is None:\\n            continue\\n        if isinstance(choices, (frozenset, KeysView, Sequence)) and not isinstance(choices, (binary_type, text_type)):\\n            if param in parameters:\\n                if isinstance(parameters[param], list):\\n                    diff_list = [item for item in parameters[param] if item not in choices]\\n                    if diff_list:\\n                        choices_str = \", \".join([to_native(c) for c in choices])\\n                        diff_str = \", \".join(diff_list)\\n                        msg = \"value of %s must be one or more of: %s. Got no match for: %s\" % (param, choices_str, diff_str)\\n                        if options_context:\\n                            msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\\n                        errors.append(ArgumentValueError(msg))\\n                elif parameters[param] not in choices:\\n                    if parameters[param] == 'False':\\n                        overlap = BOOLEANS_FALSE.intersection(choices)\\n                        if len(overlap) == 1:\\n                            (parameters[param],) = overlap\\n                    if parameters[param] == 'True':\\n                        overlap = BOOLEANS_TRUE.intersection(choices)\\n                        if len(overlap) == 1:\\n                            (parameters[param],) = overlap\\n                    if parameters[param] not in choices:\\n                        choices_str = \", \".join([to_native(c) for c in choices])\\n                        msg = \"value of %s must be one of: %s, got: %s\" % (param, choices_str, parameters[param])\\n                        if options_context:\\n                            msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\\n                        errors.append(ArgumentValueError(msg))\\n        else:\\n            msg = \"internal error: choices for argument %s are not iterable: %s\" % (param, choices)\\n            if options_context:\\n                msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\\n            errors.append(ArgumentTypeError(msg))"
  },
  {
    "code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    err = err1 = err2 = None\\n    code = code1 = code2 = None\\n    try:\\n        code = compiler(source, filename, symbol)\\n    except SyntaxError, err:\\n        pass\\n    try:\\n        code1 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err1:\\n        pass\\n    try:\\n        code2 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err2:\\n        pass\\n    if code:\\n        return code\\n    try:\\n        e1 = err1.__dict__\\n    except AttributeError:\\n        e1 = err1\\n    try:\\n        e2 = err2.__dict__\\n    except AttributeError:\\n        e2 = err2\\n    if not code1 and e1 == e2:\\n        raise SyntaxError, err1",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Conversion of exceptions over from faked-up classes to new-style C types.",
    "fixed_code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    err = err1 = err2 = None\\n    code = code1 = code2 = None\\n    try:\\n        code = compiler(source, filename, symbol)\\n    except SyntaxError, err:\\n        pass\\n    try:\\n        code1 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err1:\\n        pass\\n    try:\\n        code2 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err2:\\n        pass\\n    if code:\\n        return code\\n    if not code1 and repr(err1) == repr(err2):\\n        raise SyntaxError, err1"
  },
  {
    "code": "def get_conn(self) -> CloudRedisClient:\\n        if not self._client:\\n            self._client = CloudRedisClient(credentials=self.get_credentials())\\n        return self._client\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_conn(self) -> CloudRedisClient:\\n        if not self._client:\\n            self._client = CloudRedisClient(credentials=self.get_credentials())\\n        return self._client\\n    @staticmethod"
  },
  {
    "code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    else:\\n        return left_ax, None, right_indexer",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    else:\\n        return left_ax, None, right_indexer"
  },
  {
    "code": "def available_member_count(self):\\n        if 'availableMemberCnt' in self._values['stats']:\\n            return int(self._values['stats']['availableMemberCnt'])\\n        return None\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def available_member_count(self):\\n        if 'availableMemberCnt' in self._values['stats']:\\n            return int(self._values['stats']['availableMemberCnt'])\\n        return None\\n    @property"
  },
  {
    "code": "def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\\n\\t\\ttry:\\n\\t\\t\\tself._created_files.remove(path)\\n\\t\\texcept KeyError:\\n\\t\\t\\tpass\\n\\t\\tif mode is None:\\n\\t\\t\\treturn changed\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tif expand:\\n\\t\\t\\tb_path = os.path.expanduser(os.path.expandvars(b_path))\\n\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\tif self.check_file_absent_if_check_mode(b_path):\\n\\t\\t\\treturn True\\n\\t\\tif not isinstance(mode, int):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tmode = int(mode, 8)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tmode = self._symbolic_mode_to_octal(path_stat, mode)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path,\\n\\t\\t\\t\\t\\t\\t\\t\\t   msg=\"mode must be in octal or symbolic form\",\\n\\t\\t\\t\\t\\t\\t\\t\\t   details=to_native(e))\\n\\t\\t\\t\\tif mode != stat.S_IMODE(mode):\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\\n\\t\\tprev_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\tif prev_mode != mode:\\n\\t\\t\\tif diff is not None:\\n\\t\\t\\t\\tif 'before' not in diff:\\n\\t\\t\\t\\t\\tdiff['before'] = {}\\n\\t\\t\\t\\tdiff['before']['mode'] = '0%03o' % prev_mode\\n\\t\\t\\t\\tif 'after' not in diff:\\n\\t\\t\\t\\t\\tdiff['after'] = {}\\n\\t\\t\\t\\tdiff['after']['mode'] = '0%03o' % mode\\n\\t\\t\\tif self.check_mode:\\n\\t\\t\\t\\treturn True\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif hasattr(os, 'lchmod'):\\n\\t\\t\\t\\t\\tos.lchmod(b_path, mode)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not os.path.islink(b_path):\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tunderlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\t\\tnew_underlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tif underlying_stat.st_mode != new_underlying_stat.st_mode:\\n\\t\\t\\t\\t\\t\\t\\tos.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\\n\\t\\t\\texcept OSError as e:\\n\\t\\t\\t\\tif os.path.islink(b_path) and e.errno in (\\n\\t\\t\\t\\t\\terrno.EACCES,  \\n\\t\\t\\t\\t\\terrno.EPERM,  \\n\\t\\t\\t\\t\\terrno.EROFS,  \\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telif e.errno in (errno.ENOENT, errno.ELOOP):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\tself.fail_json(path=path, msg='chmod failed', details=to_native(e),\\n\\t\\t\\t\\t\\t\\t\\t   exception=traceback.format_exc())\\n\\t\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\t\\tnew_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\t\\tif new_mode != prev_mode:\\n\\t\\t\\t\\tchanged = True\\n\\t\\treturn changed",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\\n\\t\\ttry:\\n\\t\\t\\tself._created_files.remove(path)\\n\\t\\texcept KeyError:\\n\\t\\t\\tpass\\n\\t\\tif mode is None:\\n\\t\\t\\treturn changed\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tif expand:\\n\\t\\t\\tb_path = os.path.expanduser(os.path.expandvars(b_path))\\n\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\tif self.check_file_absent_if_check_mode(b_path):\\n\\t\\t\\treturn True\\n\\t\\tif not isinstance(mode, int):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tmode = int(mode, 8)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tmode = self._symbolic_mode_to_octal(path_stat, mode)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path,\\n\\t\\t\\t\\t\\t\\t\\t\\t   msg=\"mode must be in octal or symbolic form\",\\n\\t\\t\\t\\t\\t\\t\\t\\t   details=to_native(e))\\n\\t\\t\\t\\tif mode != stat.S_IMODE(mode):\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\\n\\t\\tprev_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\tif prev_mode != mode:\\n\\t\\t\\tif diff is not None:\\n\\t\\t\\t\\tif 'before' not in diff:\\n\\t\\t\\t\\t\\tdiff['before'] = {}\\n\\t\\t\\t\\tdiff['before']['mode'] = '0%03o' % prev_mode\\n\\t\\t\\t\\tif 'after' not in diff:\\n\\t\\t\\t\\t\\tdiff['after'] = {}\\n\\t\\t\\t\\tdiff['after']['mode'] = '0%03o' % mode\\n\\t\\t\\tif self.check_mode:\\n\\t\\t\\t\\treturn True\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif hasattr(os, 'lchmod'):\\n\\t\\t\\t\\t\\tos.lchmod(b_path, mode)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not os.path.islink(b_path):\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tunderlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\t\\tnew_underlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tif underlying_stat.st_mode != new_underlying_stat.st_mode:\\n\\t\\t\\t\\t\\t\\t\\tos.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\\n\\t\\t\\texcept OSError as e:\\n\\t\\t\\t\\tif os.path.islink(b_path) and e.errno in (\\n\\t\\t\\t\\t\\terrno.EACCES,  \\n\\t\\t\\t\\t\\terrno.EPERM,  \\n\\t\\t\\t\\t\\terrno.EROFS,  \\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telif e.errno in (errno.ENOENT, errno.ELOOP):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\tself.fail_json(path=path, msg='chmod failed', details=to_native(e),\\n\\t\\t\\t\\t\\t\\t\\t   exception=traceback.format_exc())\\n\\t\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\t\\tnew_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\t\\tif new_mode != prev_mode:\\n\\t\\t\\t\\tchanged = True\\n\\t\\treturn changed"
  },
  {
    "code": "def post_execute(self, context, result=None):\\n        execution_date = context['execution_date']\\n        dag_run = self._get_dagrun(execution_date=execution_date)\\n        self.log.info(\"Execution finished. State is %s\", dag_run.state)\\n        if dag_run.state != State.SUCCESS:\\n            raise AirflowException(f\"Expected state: SUCCESS. Actual state: {dag_run.state}\")\\n        if self.propagate_skipped_state and self._check_skipped_states(context):\\n            self._skip_downstream_tasks(context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add pre/post execution hooks (#17576)\\n\\nAdds overrideable pre-/ post- execution hooks. With this change you can override pre-/post- hooks at the time of DAG creation, without the need of creating your own derived operators. This means that you can - for example - skip /fail any task by raising appropriate exception in a method passed as the pre- execution hook based on some criteria (for example you can make a number of tasks always skipped in a development environment). You can also plug-in post-execution behaviour this way that will be always executed at the same worker as the task run, sequentially to the task (as opposed to callbacks, which can be executed elsewhere and asynchronously)",
    "fixed_code": "def post_execute(self, context, result=None):\\n        super().post_execute(context)\\n        execution_date = context['execution_date']\\n        dag_run = self._get_dagrun(execution_date=execution_date)\\n        self.log.info(\"Execution finished. State is %s\", dag_run.state)\\n        if dag_run.state != State.SUCCESS:\\n            raise AirflowException(f\"Expected state: SUCCESS. Actual state: {dag_run.state}\")\\n        if self.propagate_skipped_state and self._check_skipped_states(context):\\n            self._skip_downstream_tasks(context)"
  },
  {
    "code": "def _monitor_pod(self, pod, get_logs):\\n        if get_logs:\\n            logs = self._client.read_namespaced_pod_log(\\n                name=pod.name,\\n                namespace=pod.namespace,\\n                follow=True,\\n                tail_lines=10,\\n                _preload_content=False)\\n            for line in logs:\\n                self.log.info(line)\\n        else:\\n            while self.pod_is_running(pod):\\n                self.log.info(\"Pod {} has state {}\".format(pod.name, State.RUNNING))\\n                time.sleep(2)\\n        return self._task_status(self.read_pod(pod))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _monitor_pod(self, pod, get_logs):\\n        if get_logs:\\n            logs = self._client.read_namespaced_pod_log(\\n                name=pod.name,\\n                namespace=pod.namespace,\\n                follow=True,\\n                tail_lines=10,\\n                _preload_content=False)\\n            for line in logs:\\n                self.log.info(line)\\n        else:\\n            while self.pod_is_running(pod):\\n                self.log.info(\"Pod {} has state {}\".format(pod.name, State.RUNNING))\\n                time.sleep(2)\\n        return self._task_status(self.read_pod(pod))"
  },
  {
    "code": "def floatformat(text, arg=-1):\\n    force_grouping = False\\n    use_l10n = True\\n    if isinstance(arg, str):\\n        last_char = arg[-1]\\n        if arg[-2:] in {\"gu\", \"ug\"}:\\n            force_grouping = True\\n            use_l10n = False\\n            arg = arg[:-2] or -1\\n        elif last_char == \"g\":\\n            force_grouping = True\\n            arg = arg[:-1] or -1\\n        elif last_char == \"u\":\\n            use_l10n = False\\n            arg = arg[:-1] or -1\\n    try:\\n        input_val = str(text)\\n        d = Decimal(input_val)\\n    except InvalidOperation:\\n        try:\\n            d = Decimal(str(float(text)))\\n        except (ValueError, InvalidOperation, TypeError):\\n            return \"\"\\n    try:\\n        p = int(arg)\\n    except ValueError:\\n        return input_val\\n    try:\\n        m = int(d) - d\\n    except (ValueError, OverflowError, InvalidOperation):\\n        return input_val\\n    if not m and p < 0:\\n        return mark_safe(\\n            formats.number_format(\\n                \"%d\" % (int(d)),\\n                0,\\n                use_l10n=use_l10n,\\n                force_grouping=force_grouping,\\n            )\\n        )\\n    exp = Decimal(1).scaleb(-abs(p))\\n    tupl = d.as_tuple()\\n    units = len(tupl[1])\\n    units += -tupl[2] if m else tupl[2]\\n    prec = abs(p) + units + 1\\n    rounded_d = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec))\\n    sign, digits, exponent = rounded_d.as_tuple()\\n    digits = [str(digit) for digit in reversed(digits)]\\n    while len(digits) <= abs(exponent):\\n        digits.append(\"0\")\\n    digits.insert(-exponent, \".\")\\n    if sign and rounded_d:\\n        digits.append(\"-\")\\n    number = \"\".join(reversed(digits))\\n    return mark_safe(\\n        formats.number_format(\\n            number,\\n            abs(p),\\n            use_l10n=use_l10n,\\n            force_grouping=force_grouping,\\n        )\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def floatformat(text, arg=-1):\\n    force_grouping = False\\n    use_l10n = True\\n    if isinstance(arg, str):\\n        last_char = arg[-1]\\n        if arg[-2:] in {\"gu\", \"ug\"}:\\n            force_grouping = True\\n            use_l10n = False\\n            arg = arg[:-2] or -1\\n        elif last_char == \"g\":\\n            force_grouping = True\\n            arg = arg[:-1] or -1\\n        elif last_char == \"u\":\\n            use_l10n = False\\n            arg = arg[:-1] or -1\\n    try:\\n        input_val = str(text)\\n        d = Decimal(input_val)\\n    except InvalidOperation:\\n        try:\\n            d = Decimal(str(float(text)))\\n        except (ValueError, InvalidOperation, TypeError):\\n            return \"\"\\n    try:\\n        p = int(arg)\\n    except ValueError:\\n        return input_val\\n    try:\\n        m = int(d) - d\\n    except (ValueError, OverflowError, InvalidOperation):\\n        return input_val\\n    if not m and p < 0:\\n        return mark_safe(\\n            formats.number_format(\\n                \"%d\" % (int(d)),\\n                0,\\n                use_l10n=use_l10n,\\n                force_grouping=force_grouping,\\n            )\\n        )\\n    exp = Decimal(1).scaleb(-abs(p))\\n    tupl = d.as_tuple()\\n    units = len(tupl[1])\\n    units += -tupl[2] if m else tupl[2]\\n    prec = abs(p) + units + 1\\n    rounded_d = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec))\\n    sign, digits, exponent = rounded_d.as_tuple()\\n    digits = [str(digit) for digit in reversed(digits)]\\n    while len(digits) <= abs(exponent):\\n        digits.append(\"0\")\\n    digits.insert(-exponent, \".\")\\n    if sign and rounded_d:\\n        digits.append(\"-\")\\n    number = \"\".join(reversed(digits))\\n    return mark_safe(\\n        formats.number_format(\\n            number,\\n            abs(p),\\n            use_l10n=use_l10n,\\n            force_grouping=force_grouping,\\n        )\\n    )"
  },
  {
    "code": "def fit(self, X, y=None):\\n        X = self._validate_data(X, dtype='numeric')\\n        supported_dtype = (np.float64, np.float32)\\n        if self.dtype in supported_dtype:\\n            output_dtype = self.dtype\\n        elif self.dtype is None:\\n            output_dtype = X.dtype\\n        else:\\n            raise ValueError(\\n                f\"Valid options for 'dtype' are \"\\n                f\"{supported_dtype + (None,)}. Got dtype={self.dtype} \"\\n                f\" instead.\"\\n            )\\n        valid_encode = ('onehot', 'onehot-dense', 'ordinal')\\n        if self.encode not in valid_encode:\\n            raise ValueError(\"Valid options for 'encode' are {}. \"\\n                             \"Got encode={!r} instead.\"\\n                             .format(valid_encode, self.encode))\\n        valid_strategy = ('uniform', 'quantile', 'kmeans')\\n        if self.strategy not in valid_strategy:\\n            raise ValueError(\"Valid options for 'strategy' are {}. \"\\n                             \"Got strategy={!r} instead.\"\\n                             .format(valid_strategy, self.strategy))\\n        n_features = X.shape[1]\\n        n_bins = self._validate_n_bins(n_features)\\n        bin_edges = np.zeros(n_features, dtype=object)\\n        for jj in range(n_features):\\n            column = X[:, jj]\\n            col_min, col_max = column.min(), column.max()\\n            if col_min == col_max:\\n                warnings.warn(\"Feature %d is constant and will be \"\\n                              \"replaced with 0.\" % jj)\\n                n_bins[jj] = 1\\n                bin_edges[jj] = np.array([-np.inf, np.inf])\\n                continue\\n            if self.strategy == 'uniform':\\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n            elif self.strategy == 'quantile':\\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\\n                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\\n            elif self.strategy == 'kmeans':\\n                from ..cluster import KMeans  \\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\\n                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\\n                centers.sort()\\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\\n            if self.strategy in ('quantile', 'kmeans'):\\n                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8\\n                bin_edges[jj] = bin_edges[jj][mask]\\n                if len(bin_edges[jj]) - 1 != n_bins[jj]:\\n                    warnings.warn('Bins whose width are too small (i.e., <= '\\n                                  '1e-8) in feature %d are removed. Consider '\\n                                  'decreasing the number of bins.' % jj)\\n                    n_bins[jj] = len(bin_edges[jj]) - 1\\n        self.bin_edges_ = bin_edges\\n        self.n_bins_ = n_bins\\n        if 'onehot' in self.encode:\\n            self._encoder = OneHotEncoder(\\n                categories=[np.arange(i) for i in self.n_bins_],\\n                sparse=self.encode == 'onehot',\\n                dtype=output_dtype)\\n            self._encoder.fit(np.zeros((1, len(self.n_bins_))))\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, y=None):\\n        X = self._validate_data(X, dtype='numeric')\\n        supported_dtype = (np.float64, np.float32)\\n        if self.dtype in supported_dtype:\\n            output_dtype = self.dtype\\n        elif self.dtype is None:\\n            output_dtype = X.dtype\\n        else:\\n            raise ValueError(\\n                f\"Valid options for 'dtype' are \"\\n                f\"{supported_dtype + (None,)}. Got dtype={self.dtype} \"\\n                f\" instead.\"\\n            )\\n        valid_encode = ('onehot', 'onehot-dense', 'ordinal')\\n        if self.encode not in valid_encode:\\n            raise ValueError(\"Valid options for 'encode' are {}. \"\\n                             \"Got encode={!r} instead.\"\\n                             .format(valid_encode, self.encode))\\n        valid_strategy = ('uniform', 'quantile', 'kmeans')\\n        if self.strategy not in valid_strategy:\\n            raise ValueError(\"Valid options for 'strategy' are {}. \"\\n                             \"Got strategy={!r} instead.\"\\n                             .format(valid_strategy, self.strategy))\\n        n_features = X.shape[1]\\n        n_bins = self._validate_n_bins(n_features)\\n        bin_edges = np.zeros(n_features, dtype=object)\\n        for jj in range(n_features):\\n            column = X[:, jj]\\n            col_min, col_max = column.min(), column.max()\\n            if col_min == col_max:\\n                warnings.warn(\"Feature %d is constant and will be \"\\n                              \"replaced with 0.\" % jj)\\n                n_bins[jj] = 1\\n                bin_edges[jj] = np.array([-np.inf, np.inf])\\n                continue\\n            if self.strategy == 'uniform':\\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n            elif self.strategy == 'quantile':\\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\\n                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\\n            elif self.strategy == 'kmeans':\\n                from ..cluster import KMeans  \\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\\n                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\\n                centers.sort()\\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\\n            if self.strategy in ('quantile', 'kmeans'):\\n                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8\\n                bin_edges[jj] = bin_edges[jj][mask]\\n                if len(bin_edges[jj]) - 1 != n_bins[jj]:\\n                    warnings.warn('Bins whose width are too small (i.e., <= '\\n                                  '1e-8) in feature %d are removed. Consider '\\n                                  'decreasing the number of bins.' % jj)\\n                    n_bins[jj] = len(bin_edges[jj]) - 1\\n        self.bin_edges_ = bin_edges\\n        self.n_bins_ = n_bins\\n        if 'onehot' in self.encode:\\n            self._encoder = OneHotEncoder(\\n                categories=[np.arange(i) for i in self.n_bins_],\\n                sparse=self.encode == 'onehot',\\n                dtype=output_dtype)\\n            self._encoder.fit(np.zeros((1, len(self.n_bins_))))\\n        return self"
  },
  {
    "code": "def make_node_repr(self, node):\\n        if node.is_leaf:\\n            return \"error = %s \\\\n samples = %s \\\\n v = %s\" \\\\n                % (node.error, node.samples, node.value)\\n        else:\\n            return \"x[%s] < %s \\\\n error = %s \\\\n samples = %s \\\\n v = %s\" \\\\n                   % (node.feature, node.threshold,\\\\n                      node.error, node.samples, node.value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def make_node_repr(self, node):\\n        if node.is_leaf:\\n            return \"error = %s \\\\n samples = %s \\\\n v = %s\" \\\\n                % (node.error, node.samples, node.value)\\n        else:\\n            return \"x[%s] < %s \\\\n error = %s \\\\n samples = %s \\\\n v = %s\" \\\\n                   % (node.feature, node.threshold,\\\\n                      node.error, node.samples, node.value)"
  },
  {
    "code": "def __len__(self):\\n\\t\\treturn len(self.iterable)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "SI unit boundary bugfix, iterator len bugfix",
    "fixed_code": "def __len__(self):\\n\\t\\treturn len(self.iterable) if self.iterable else self.total"
  },
  {
    "code": "def _new_DatetimeIndex(cls, d):\\n    if \"data\" in d and not isinstance(d[\"data\"], DatetimeIndex):\\n        data = d.pop(\"data\")\\n        result = cls._simple_new(data, **d)\\n    else:\\n        with warnings.catch_warnings():\\n            warnings.simplefilter(\"ignore\")\\n            result = cls.__new__(cls, verify_integrity=False, **d)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: DTI/TDI/PI constructor arguments (#29930)",
    "fixed_code": "def _new_DatetimeIndex(cls, d):\\n    if \"data\" in d and not isinstance(d[\"data\"], DatetimeIndex):\\n        data = d.pop(\"data\")\\n        result = cls._simple_new(data, **d)\\n    else:\\n        with warnings.catch_warnings():\\n            warnings.simplefilter(\"ignore\")\\n            result = cls.__new__(cls, **d)\\n    return result"
  },
  {
    "code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    from pandas.tseries.period import PeriodIndex\\n    is_period = com.is_period_arraylike(values)\\n    values = Series(values).values\\n    is_category = com.is_categorical_dtype(values.dtype)\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    elif is_category:\\n        bins = values.levels\\n        cat = values\\n        values = cat.codes\\n    dtype = values.dtype\\n    if issubclass(values.dtype.type, (np.datetime64, np.timedelta64)) or is_period:\\n        if is_period:\\n            values = PeriodIndex(values)\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        if dropna:\\n            from pandas.tslib import iNaT\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n        keys = keys.astype(dtype)\\n    elif com.is_integer_dtype(dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    else:\\n        values = com._ensure_object(values)\\n        mask = com.isnull(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n        if not dropna:\\n            keys = np.insert(keys, 0, np.NaN)\\n            counts = np.insert(counts, 0, mask.sum())\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.levels)), fill_value=0)\\n        if not is_category:\\n            result.index = bins[:-1]\\n        else:\\n            result.index = cat.levels\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    from pandas.tseries.period import PeriodIndex\\n    is_period = com.is_period_arraylike(values)\\n    values = Series(values).values\\n    is_category = com.is_categorical_dtype(values.dtype)\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    elif is_category:\\n        bins = values.levels\\n        cat = values\\n        values = cat.codes\\n    dtype = values.dtype\\n    if issubclass(values.dtype.type, (np.datetime64, np.timedelta64)) or is_period:\\n        if is_period:\\n            values = PeriodIndex(values)\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        if dropna:\\n            from pandas.tslib import iNaT\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n        keys = keys.astype(dtype)\\n    elif com.is_integer_dtype(dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    else:\\n        values = com._ensure_object(values)\\n        mask = com.isnull(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n        if not dropna:\\n            keys = np.insert(keys, 0, np.NaN)\\n            counts = np.insert(counts, 0, mask.sum())\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.levels)), fill_value=0)\\n        if not is_category:\\n            result.index = bins[:-1]\\n        else:\\n            result.index = cat.levels\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result"
  },
  {
    "code": "def init_appbuilder_views(app):\\n    appbuilder = app.appbuilder\\n    from airflow.www import views\\n    appbuilder.session.remove()\\n    appbuilder.add_view_no_menu(views.Airflow())\\n    appbuilder.add_view_no_menu(views.DagModelView())\\n    appbuilder.add_view(views.DagRunModelView, \"DAG Runs\", category=\"Browse\", category_icon=\"fa-globe\")\\n    appbuilder.add_view(views.JobModelView, \"Jobs\", category=\"Browse\")\\n    appbuilder.add_view(views.LogModelView, \"Logs\", category=\"Browse\")\\n    appbuilder.add_view(views.SlaMissModelView, \"SLA Misses\", category=\"Browse\")\\n    appbuilder.add_view(views.TaskInstanceModelView, \"Task Instances\", category=\"Browse\")\\n    appbuilder.add_view(views.TaskRescheduleModelView, \"Task Reschedules\", category=\"Browse\")\\n    appbuilder.add_view(views.ConfigurationView, \"Configurations\", category=\"Admin\", category_icon=\"fa-user\")\\n    appbuilder.add_view(views.ConnectionModelView, \"Connections\", category=\"Admin\")\\n    appbuilder.add_view(views.PoolModelView, \"Pools\", category=\"Admin\")\\n    appbuilder.add_view(views.VariableModelView, \"Variables\", category=\"Admin\")\\n    appbuilder.add_view(views.XComModelView, \"XComs\", category=\"Admin\")\\n    appbuilder.add_view(views.VersionView, 'Version', category='About', category_icon='fa-th')\\n    appbuilder.add_view_no_menu(views.RedocView)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def init_appbuilder_views(app):\\n    appbuilder = app.appbuilder\\n    from airflow.www import views\\n    appbuilder.session.remove()\\n    appbuilder.add_view_no_menu(views.Airflow())\\n    appbuilder.add_view_no_menu(views.DagModelView())\\n    appbuilder.add_view(views.DagRunModelView, \"DAG Runs\", category=\"Browse\", category_icon=\"fa-globe\")\\n    appbuilder.add_view(views.JobModelView, \"Jobs\", category=\"Browse\")\\n    appbuilder.add_view(views.LogModelView, \"Logs\", category=\"Browse\")\\n    appbuilder.add_view(views.SlaMissModelView, \"SLA Misses\", category=\"Browse\")\\n    appbuilder.add_view(views.TaskInstanceModelView, \"Task Instances\", category=\"Browse\")\\n    appbuilder.add_view(views.TaskRescheduleModelView, \"Task Reschedules\", category=\"Browse\")\\n    appbuilder.add_view(views.ConfigurationView, \"Configurations\", category=\"Admin\", category_icon=\"fa-user\")\\n    appbuilder.add_view(views.ConnectionModelView, \"Connections\", category=\"Admin\")\\n    appbuilder.add_view(views.PoolModelView, \"Pools\", category=\"Admin\")\\n    appbuilder.add_view(views.VariableModelView, \"Variables\", category=\"Admin\")\\n    appbuilder.add_view(views.XComModelView, \"XComs\", category=\"Admin\")\\n    appbuilder.add_view(views.VersionView, 'Version', category='About', category_icon='fa-th')\\n    appbuilder.add_view_no_menu(views.RedocView)"
  },
  {
    "code": "def _str_extract_noexpand(arr, pat, flags=0):\\n    from pandas import (\\n        DataFrame,\\n        array as pd_array,\\n    )\\n    regex = re.compile(pat, flags=flags)\\n    groups_or_na = _groups_or_na_fun(regex)\\n    result_dtype = _result_dtype(arr)\\n    if regex.groups == 1:\\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\\n        name = _get_single_group_name(regex)\\n        result = pd_array(result, dtype=result_dtype)\\n    else:\\n        name = None\\n        columns = _get_group_names(regex)\\n        if arr.size == 0:\\n            result = DataFrame(  \\n                columns=columns, dtype=object\\n            )\\n        else:\\n            dtype = _result_dtype(arr)\\n            result = DataFrame(  \\n                [groups_or_na(val) for val in arr],\\n                columns=columns,\\n                index=arr.index,\\n                dtype=dtype,\\n            )\\n    return result, name",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Series.str.extract with StringArray returning object dtype (#41441)",
    "fixed_code": "def _str_extract_noexpand(arr, pat, flags=0):\\n    from pandas import (\\n        DataFrame,\\n        array as pd_array,\\n    )\\n    regex = re.compile(pat, flags=flags)\\n    groups_or_na = _groups_or_na_fun(regex)\\n    result_dtype = _result_dtype(arr)\\n    if regex.groups == 1:\\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\\n        name = _get_single_group_name(regex)\\n        result = pd_array(result, dtype=result_dtype)\\n    else:\\n        name = None\\n        columns = _get_group_names(regex)\\n        if arr.size == 0:\\n            result = DataFrame(  \\n                columns=columns, dtype=result_dtype\\n            )\\n        else:\\n            result = DataFrame(  \\n                [groups_or_na(val) for val in arr],\\n                columns=columns,\\n                index=arr.index,\\n                dtype=result_dtype,\\n            )\\n    return result, name"
  },
  {
    "code": "def post_comment(request, next=None, using=None):\\n    data = request.POST.copy()\\n    if request.user.is_authenticated():\\n        if not data.get('name', ''):\\n            data[\"name\"] = request.user.get_full_name() or request.user.username\\n        if not data.get('email', ''):\\n            data[\"email\"] = request.user.email\\n    next = data.get(\"next\", next)\\n    ctype = data.get(\"content_type\")\\n    object_pk = data.get(\"object_pk\")\\n    if ctype is None or object_pk is None:\\n        return CommentPostBadRequest(\"Missing content_type or object_pk field.\")\\n    try:\\n        model = models.get_model(*ctype.split(\".\", 1))\\n        target = model._default_manager.using(using).get(pk=object_pk)\\n    except TypeError:\\n        return CommentPostBadRequest(\\n            \"Invalid content_type value: %r\" % escape(ctype))\\n    except AttributeError:\\n        return CommentPostBadRequest(\\n            \"The given content-type %r does not resolve to a valid model.\" % \\\\n                escape(ctype))\\n    except ObjectDoesNotExist:\\n        return CommentPostBadRequest(\\n            \"No object matching content-type %r and object PK %r exists.\" % \\\\n                (escape(ctype), escape(object_pk)))\\n    except (ValueError, ValidationError), e:\\n        return CommentPostBadRequest(\\n            \"Attempting go get content-type %r and object PK %r exists raised %s\" % \\\\n                (escape(ctype), escape(object_pk), e.__class__.__name__))\\n    preview = \"preview\" in data\\n    form = comments.get_form()(target, data=data)\\n    if form.security_errors():\\n        return CommentPostBadRequest(\\n            \"The comment form failed security verification: %s\" % \\\\n                escape(str(form.security_errors())))\\n    if form.errors or preview:\\n        template_list = [\\n            \"comments/%s_%s_preview.html\" % (model._meta.app_label, model._meta.module_name),\\n            \"comments/%s_preview.html\" % model._meta.app_label,\\n            \"comments/%s/%s/preview.html\" % (model._meta.app_label, model._meta.module_name),\\n            \"comments/%s/preview.html\" % model._meta.app_label,\\n            \"comments/preview.html\",\\n        ]\\n        return render_to_response(\\n            template_list, {\\n                \"comment\" : form.data.get(\"comment\", \"\"),\\n                \"form\" : form,\\n                \"next\": next,\\n            },\\n            RequestContext(request, {})\\n        )\\n    comment = form.get_comment_object()\\n    comment.ip_address = request.META.get(\"REMOTE_ADDR\", None)\\n    if request.user.is_authenticated():\\n        comment.user = request.user\\n    responses = signals.comment_will_be_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    for (receiver, response) in responses:\\n        if response == False:\\n            return CommentPostBadRequest(\\n                \"comment_will_be_posted receiver %r killed the comment\" % receiver.__name__)\\n    comment.save()\\n    signals.comment_was_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    return next_redirect(data, next, comment_done, c=comment._get_pk_val())\\ncomment_done = confirmation_view(\\n    template = \"comments/posted.html\",\\n    doc = \\n)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def post_comment(request, next=None, using=None):\\n    data = request.POST.copy()\\n    if request.user.is_authenticated():\\n        if not data.get('name', ''):\\n            data[\"name\"] = request.user.get_full_name() or request.user.username\\n        if not data.get('email', ''):\\n            data[\"email\"] = request.user.email\\n    next = data.get(\"next\", next)\\n    ctype = data.get(\"content_type\")\\n    object_pk = data.get(\"object_pk\")\\n    if ctype is None or object_pk is None:\\n        return CommentPostBadRequest(\"Missing content_type or object_pk field.\")\\n    try:\\n        model = models.get_model(*ctype.split(\".\", 1))\\n        target = model._default_manager.using(using).get(pk=object_pk)\\n    except TypeError:\\n        return CommentPostBadRequest(\\n            \"Invalid content_type value: %r\" % escape(ctype))\\n    except AttributeError:\\n        return CommentPostBadRequest(\\n            \"The given content-type %r does not resolve to a valid model.\" % \\\\n                escape(ctype))\\n    except ObjectDoesNotExist:\\n        return CommentPostBadRequest(\\n            \"No object matching content-type %r and object PK %r exists.\" % \\\\n                (escape(ctype), escape(object_pk)))\\n    except (ValueError, ValidationError), e:\\n        return CommentPostBadRequest(\\n            \"Attempting go get content-type %r and object PK %r exists raised %s\" % \\\\n                (escape(ctype), escape(object_pk), e.__class__.__name__))\\n    preview = \"preview\" in data\\n    form = comments.get_form()(target, data=data)\\n    if form.security_errors():\\n        return CommentPostBadRequest(\\n            \"The comment form failed security verification: %s\" % \\\\n                escape(str(form.security_errors())))\\n    if form.errors or preview:\\n        template_list = [\\n            \"comments/%s_%s_preview.html\" % (model._meta.app_label, model._meta.module_name),\\n            \"comments/%s_preview.html\" % model._meta.app_label,\\n            \"comments/%s/%s/preview.html\" % (model._meta.app_label, model._meta.module_name),\\n            \"comments/%s/preview.html\" % model._meta.app_label,\\n            \"comments/preview.html\",\\n        ]\\n        return render_to_response(\\n            template_list, {\\n                \"comment\" : form.data.get(\"comment\", \"\"),\\n                \"form\" : form,\\n                \"next\": next,\\n            },\\n            RequestContext(request, {})\\n        )\\n    comment = form.get_comment_object()\\n    comment.ip_address = request.META.get(\"REMOTE_ADDR\", None)\\n    if request.user.is_authenticated():\\n        comment.user = request.user\\n    responses = signals.comment_will_be_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    for (receiver, response) in responses:\\n        if response == False:\\n            return CommentPostBadRequest(\\n                \"comment_will_be_posted receiver %r killed the comment\" % receiver.__name__)\\n    comment.save()\\n    signals.comment_was_posted.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        request = request\\n    )\\n    return next_redirect(data, next, comment_done, c=comment._get_pk_val())\\ncomment_done = confirmation_view(\\n    template = \"comments/posted.html\",\\n    doc = \\n)"
  },
  {
    "code": "def __exit__(self, exc_type, exc_val, exc_tb):\\n        if exc_type is futures.CancelledError and self._cancelled:\\n            self._cancel_handler = None\\n            self._task = None\\n            raise futures.TimeoutError\\n        self._cancel_handler.cancel()\\n        self._cancel_handler = None\\n        self._task = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio: allow None as wait timeout\\n\\nFix GH#325: Allow to pass None as a timeout value to disable timeout logic.\\n\\nChange written by Andrew Svetlov and merged by Guido van Rossum.",
    "fixed_code": "def __exit__(self, exc_type, exc_val, exc_tb):\\n        if exc_type is futures.CancelledError and self._cancelled:\\n            self._cancel_handler = None\\n            self._task = None\\n            raise futures.TimeoutError\\n        if self._timeout is not None:\\n            self._cancel_handler.cancel()\\n            self._cancel_handler = None\\n        self._task = None"
  },
  {
    "code": "def save_manifest(self):\\n        payload = {'paths': self.hashed_files, 'version': self.manifest_version}\\n        if self.manifest_storage.exists(self.manifest_name):\\n            self.manifest_storage.delete(self.manifest_name)\\n        contents = json.dumps(payload).encode()\\n        self.manifest_storage._save(self.manifest_name, ContentFile(contents))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def save_manifest(self):\\n        payload = {'paths': self.hashed_files, 'version': self.manifest_version}\\n        if self.manifest_storage.exists(self.manifest_name):\\n            self.manifest_storage.delete(self.manifest_name)\\n        contents = json.dumps(payload).encode()\\n        self.manifest_storage._save(self.manifest_name, ContentFile(contents))"
  },
  {
    "code": "def open(filename, flag='c', protocol=None, writeback=False, binary=None):\\n    return DbfilenameShelf(filename, flag, binary, writeback)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def open(filename, flag='c', protocol=None, writeback=False, binary=None):\\n    return DbfilenameShelf(filename, flag, binary, writeback)"
  },
  {
    "code": "def logout(request, next_page=None,\\n           template_name='registration/logged_out.html',\\n           redirect_field_name=REDIRECT_FIELD_NAME,\\n           extra_context=None):\\n    auth_logout(request)\\n    if next_page is not None:\\n        next_page = resolve_url(next_page)\\n    if (redirect_field_name in request.POST or\\n            redirect_field_name in request.GET):\\n        next_page = request.POST.get(redirect_field_name,\\n                                     request.GET.get(redirect_field_name))\\n        if not is_safe_url(url=next_page, host=request.get_host()):\\n            next_page = request.path\\n    if next_page:\\n        return HttpResponseRedirect(next_page)\\n    current_site = get_current_site(request)\\n    context = {\\n        'site': current_site,\\n        'site_name': current_site.name,\\n        'title': _('Logged out')\\n    }\\n    if extra_context is not None:\\n        context.update(extra_context)\\n    return TemplateResponse(request, template_name, context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #12405 -- Added LOGOUT_REDIRECT_URL setting.\\n\\nAfter a user logs out via auth.views.logout(), they're redirected\\nto LOGOUT_REDIRECT_URL if no `next_page` argument is provided.",
    "fixed_code": "def logout(request, next_page=None,\\n           template_name='registration/logged_out.html',\\n           redirect_field_name=REDIRECT_FIELD_NAME,\\n           extra_context=None):\\n    auth_logout(request)\\n    if next_page is not None:\\n        next_page = resolve_url(next_page)\\n    elif settings.LOGOUT_REDIRECT_URL:\\n        next_page = resolve_url(settings.LOGOUT_REDIRECT_URL)\\n    if (redirect_field_name in request.POST or\\n            redirect_field_name in request.GET):\\n        next_page = request.POST.get(redirect_field_name,\\n                                     request.GET.get(redirect_field_name))\\n        if not is_safe_url(url=next_page, host=request.get_host()):\\n            next_page = request.path\\n    if next_page:\\n        return HttpResponseRedirect(next_page)\\n    current_site = get_current_site(request)\\n    context = {\\n        'site': current_site,\\n        'site_name': current_site.name,\\n        'title': _('Logged out')\\n    }\\n    if extra_context is not None:\\n        context.update(extra_context)\\n    return TemplateResponse(request, template_name, context)"
  },
  {
    "code": "def main():\\n    import sys\\n    import getopt\\n    try:\\n        opts, args = getopt.getopt(sys.argv[1:], 'td')\\n    except getopt.error as msg:\\n        sys.stdout = sys.stderr\\n        print(msg)\\n        print(\"usage: quopri [-t | -d] [file] ...\")\\n        print(\"-t: quote tabs\")\\n        print(\"-d: decode; default encode\")\\n        sys.exit(2)\\n    deco = 0\\n    tabs = 0\\n    for o, a in opts:\\n        if o == '-t': tabs = 1\\n        if o == '-d': deco = 1\\n    if tabs and deco:\\n        sys.stdout = sys.stderr\\n        print(\"-t and -d are mutually exclusive\")\\n        sys.exit(2)\\n    if not args: args = ['-']\\n    sts = 0\\n    for file in args:\\n        if file == '-':\\n            fp = sys.stdin.buffer\\n        else:\\n            try:\\n                fp = open(file, \"rb\")\\n            except IOError as msg:\\n                sys.stderr.write(\"%s: can't open (%s)\\n\" % (file, msg))\\n                sts = 1\\n                continue\\n        if deco:\\n            decode(fp, sys.stdout.buffer)\\n        else:\\n            encode(fp, sys.stdout.buffer, tabs)\\n        if fp is not sys.stdin:\\n            fp.close()\\n    if sts:\\n        sys.exit(sts)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    import sys\\n    import getopt\\n    try:\\n        opts, args = getopt.getopt(sys.argv[1:], 'td')\\n    except getopt.error as msg:\\n        sys.stdout = sys.stderr\\n        print(msg)\\n        print(\"usage: quopri [-t | -d] [file] ...\")\\n        print(\"-t: quote tabs\")\\n        print(\"-d: decode; default encode\")\\n        sys.exit(2)\\n    deco = 0\\n    tabs = 0\\n    for o, a in opts:\\n        if o == '-t': tabs = 1\\n        if o == '-d': deco = 1\\n    if tabs and deco:\\n        sys.stdout = sys.stderr\\n        print(\"-t and -d are mutually exclusive\")\\n        sys.exit(2)\\n    if not args: args = ['-']\\n    sts = 0\\n    for file in args:\\n        if file == '-':\\n            fp = sys.stdin.buffer\\n        else:\\n            try:\\n                fp = open(file, \"rb\")\\n            except IOError as msg:\\n                sys.stderr.write(\"%s: can't open (%s)\\n\" % (file, msg))\\n                sts = 1\\n                continue\\n        if deco:\\n            decode(fp, sys.stdout.buffer)\\n        else:\\n            encode(fp, sys.stdout.buffer, tabs)\\n        if fp is not sys.stdin:\\n            fp.close()\\n    if sts:\\n        sys.exit(sts)"
  },
  {
    "code": "def to_datetime(arg, errors='ignore', dayfirst=False):\\n    from pandas.core.series import Series\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, datetime):\\n        return arg\\n    elif isinstance(arg, Series):\\n        values = lib.string_to_datetime(com._ensure_object(arg.values),\\n                                        raise_=errors == 'raise',\\n                                        dayfirst=dayfirst)\\n        return Series(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (np.ndarray, list)):\\n        if isinstance(arg, list):\\n            arg = np.array(arg, dtype='O')\\n        return lib.string_to_datetime(com._ensure_object(arg),\\n                                      raise_=errors == 'raise',\\n                                      dayfirst=dayfirst)\\n    try:\\n        if not arg:\\n            return arg\\n        return parser.parse(arg, dayfirst=dayfirst)\\n    except Exception:\\n        if errors == 'raise':\\n            raise\\n        return arg",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_datetime(arg, errors='ignore', dayfirst=False):\\n    from pandas.core.series import Series\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, datetime):\\n        return arg\\n    elif isinstance(arg, Series):\\n        values = lib.string_to_datetime(com._ensure_object(arg.values),\\n                                        raise_=errors == 'raise',\\n                                        dayfirst=dayfirst)\\n        return Series(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (np.ndarray, list)):\\n        if isinstance(arg, list):\\n            arg = np.array(arg, dtype='O')\\n        return lib.string_to_datetime(com._ensure_object(arg),\\n                                      raise_=errors == 'raise',\\n                                      dayfirst=dayfirst)\\n    try:\\n        if not arg:\\n            return arg\\n        return parser.parse(arg, dayfirst=dayfirst)\\n    except Exception:\\n        if errors == 'raise':\\n            raise\\n        return arg"
  },
  {
    "code": "def get(self, block=1):\\n\\t\\tif block:\\n\\t\\t\\tself.esema.acquire()\\n\\t\\telif not self.esema.acquire(0):\\n\\t\\t\\traise Empty\\n\\t\\tself.mutex.acquire()\\n\\t\\trelease_esema = True\\n\\t\\ttry:\\n\\t\\t\\twas_full = self._full()\\n\\t\\t\\titem = self._get()\\n\\t\\t\\tif was_full:\\n\\t\\t\\t\\tself.fsema.release()\\n\\t\\t\\trelease_esema = not self._empty()\\n\\t\\tfinally:\\n\\t\\t\\tif release_esema:\\n\\t\\t\\t\\tself.esema.release()\\n\\t\\t\\tself.mutex.release()\\n\\t\\treturn item",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get(self, block=1):\\n\\t\\tif block:\\n\\t\\t\\tself.esema.acquire()\\n\\t\\telif not self.esema.acquire(0):\\n\\t\\t\\traise Empty\\n\\t\\tself.mutex.acquire()\\n\\t\\trelease_esema = True\\n\\t\\ttry:\\n\\t\\t\\twas_full = self._full()\\n\\t\\t\\titem = self._get()\\n\\t\\t\\tif was_full:\\n\\t\\t\\t\\tself.fsema.release()\\n\\t\\t\\trelease_esema = not self._empty()\\n\\t\\tfinally:\\n\\t\\t\\tif release_esema:\\n\\t\\t\\t\\tself.esema.release()\\n\\t\\t\\tself.mutex.release()\\n\\t\\treturn item"
  },
  {
    "code": "def trigger_dag(dag_id):\\n    data = request.get_json(force=True)\\n    run_id = None\\n    if 'run_id' in data:\\n        run_id = data['run_id']\\n    conf = None\\n    if 'conf' in data:\\n        conf = data['conf']\\n    execution_date = None\\n    if 'execution_date' in data and data['execution_date'] is not None:\\n        execution_date = data['execution_date']\\n        try:\\n            execution_date = timezone.parse(execution_date)\\n        except ValueError:\\n            error_message = (\\n                'Given execution date, {}, could not be identified '\\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\\n                .format(execution_date))\\n            log.info(error_message)\\n            response = jsonify({'error': error_message})\\n            response.status_code = 400\\n            return response\\n    replace_microseconds = (execution_date is None)\\n    if 'replace_microseconds' in data:\\n        replace_microseconds = to_boolean(data['replace_microseconds'])\\n    try:\\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date, replace_microseconds)\\n    except AirflowException as err:\\n        log.error(err)\\n        response = jsonify(error=\"{}\".format(err))\\n        response.status_code = err.status_code\\n        return response\\n    if getattr(g, 'user', None):\\n        log.info(\"User %s created %s\", g.user, dr)\\n    response = jsonify(message=\"Created {}\".format(dr), execution_date=dr.execution_date.isoformat())\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5590] Add run_id to trigger DAG run API response (#6256)",
    "fixed_code": "def trigger_dag(dag_id):\\n    data = request.get_json(force=True)\\n    run_id = None\\n    if 'run_id' in data:\\n        run_id = data['run_id']\\n    conf = None\\n    if 'conf' in data:\\n        conf = data['conf']\\n    execution_date = None\\n    if 'execution_date' in data and data['execution_date'] is not None:\\n        execution_date = data['execution_date']\\n        try:\\n            execution_date = timezone.parse(execution_date)\\n        except ValueError:\\n            error_message = (\\n                'Given execution date, {}, could not be identified '\\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\\n                .format(execution_date))\\n            log.info(error_message)\\n            response = jsonify({'error': error_message})\\n            response.status_code = 400\\n            return response\\n    replace_microseconds = (execution_date is None)\\n    if 'replace_microseconds' in data:\\n        replace_microseconds = to_boolean(data['replace_microseconds'])\\n    try:\\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date, replace_microseconds)\\n    except AirflowException as err:\\n        log.error(err)\\n        response = jsonify(error=\"{}\".format(err))\\n        response.status_code = err.status_code\\n        return response\\n    if getattr(g, 'user', None):\\n        log.info(\"User %s created %s\", g.user, dr)\\n    response = jsonify(\\n        message=\"Created {}\".format(dr),\\n        execution_date=dr.execution_date.isoformat(),\\n        run_id=dr.run_id\\n    )\\n    return response"
  },
  {
    "code": "def _create_mime_attachment(self, content, mimetype):\\n        basetype, subtype = mimetype.split('/', 1)\\n        if basetype == 'text':\\n            encoding = self.encoding or settings.DEFAULT_CHARSET\\n            attachment = SafeMIMEText(content, subtype, encoding)\\n        elif basetype == 'message' and subtype == 'rfc822':\\n            if isinstance(content, EmailMessage):\\n                content = content.message()\\n            elif not isinstance(content, Message):\\n                content = message_from_string(content)\\n            attachment = SafeMIMEMessage(content, subtype)\\n        else:\\n            attachment = MIMEBase(basetype, subtype)\\n            attachment.set_payload(content)\\n            Encoders.encode_base64(attachment)\\n        return attachment",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_mime_attachment(self, content, mimetype):\\n        basetype, subtype = mimetype.split('/', 1)\\n        if basetype == 'text':\\n            encoding = self.encoding or settings.DEFAULT_CHARSET\\n            attachment = SafeMIMEText(content, subtype, encoding)\\n        elif basetype == 'message' and subtype == 'rfc822':\\n            if isinstance(content, EmailMessage):\\n                content = content.message()\\n            elif not isinstance(content, Message):\\n                content = message_from_string(content)\\n            attachment = SafeMIMEMessage(content, subtype)\\n        else:\\n            attachment = MIMEBase(basetype, subtype)\\n            attachment.set_payload(content)\\n            Encoders.encode_base64(attachment)\\n        return attachment"
  },
  {
    "code": "def generate_tokens(readline):\\n\\tlnum = parenlev = continued = 0\\n\\tnumchars = '0123456789'\\n\\tcontstr, needcont = '', 0\\n\\tcontline = None\\n\\tindents = [0]\\n\\tstashed = None\\n\\tasync_def = False\\n\\tasync_def_indent = 0\\n\\tasync_def_nl = False\\n\\twhile 1:\\t\\t\\t\\t\\t\\t\\t\\t   \\n\\t\\ttry:\\n\\t\\t\\tline = readline()\\n\\t\\texcept StopIteration:\\n\\t\\t\\tline = ''\\n\\t\\tlnum = lnum + 1\\n\\t\\tpos, max = 0, len(line)\\n\\t\\tif contstr:\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\tif not line:\\n\\t\\t\\t\\traise TokenError(\"EOF in multi-line string\", strstart)\\n\\t\\t\\tendmatch = endprog.match(line)\\n\\t\\t\\tif endmatch:\\n\\t\\t\\t\\tpos = end = endmatch.end(0)\\n\\t\\t\\t\\tyield (STRING, contstr + line[:end],\\n\\t\\t\\t\\t\\t   strstart, (lnum, end), contline + line)\\n\\t\\t\\t\\tcontstr, needcont = '', 0\\n\\t\\t\\t\\tcontline = None\\n\\t\\t\\telif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\\n\\t\\t\\t\\tyield (ERRORTOKEN, contstr + line,\\n\\t\\t\\t\\t\\t\\t   strstart, (lnum, len(line)), contline)\\n\\t\\t\\t\\tcontstr = ''\\n\\t\\t\\t\\tcontline = None\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontstr = contstr + line\\n\\t\\t\\t\\tcontline = contline + line\\n\\t\\t\\t\\tcontinue\\n\\t\\telif parenlev == 0 and not continued:  \\n\\t\\t\\tif not line: break\\n\\t\\t\\tcolumn = 0\\n\\t\\t\\twhile pos < max:\\t\\t\\t\\t   \\n\\t\\t\\t\\tif line[pos] == ' ': column = column + 1\\n\\t\\t\\t\\telif line[pos] == '\\t': column = (column//tabsize + 1)*tabsize\\n\\t\\t\\t\\telif line[pos] == '\\f': column = 0\\n\\t\\t\\t\\telse: break\\n\\t\\t\\t\\tpos = pos + 1\\n\\t\\t\\tif pos == max: break\\n\\t\\t\\tif stashed:\\n\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\tstashed = None\\n\\t\\t\\tif line[pos] in '\\r\\n':\\t\\t\\t\\n\\t\\t\\t\\tyield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif line[pos] == '\\n\\t\\t\\t\\tcomment_token = line[pos:].rstrip('\\r\\n')\\n\\t\\t\\t\\tnl_pos = pos + len(comment_token)\\n\\t\\t\\t\\tyield (COMMENT, comment_token,\\n\\t\\t\\t\\t\\t\\t(lnum, pos), (lnum, pos + len(comment_token)), line)\\n\\t\\t\\t\\tyield (NL, line[nl_pos:],\\n\\t\\t\\t\\t\\t\\t(lnum, nl_pos), (lnum, len(line)), line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif column > indents[-1]:\\t\\t   \\n\\t\\t\\t\\tindents.append(column)\\n\\t\\t\\t\\tyield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\\n\\t\\t\\twhile column < indents[-1]:\\t\\t\\n\\t\\t\\t\\tif column not in indents:\\n\\t\\t\\t\\t\\traise IndentationError(\\n\\t\\t\\t\\t\\t\\t\"unindent does not match any outer indentation level\",\\n\\t\\t\\t\\t\\t\\t(\"<tokenize>\", lnum, pos, line))\\n\\t\\t\\t\\tindents = indents[:-1]\\n\\t\\t\\t\\tif async_def and async_def_indent >= indents[-1]:\\n\\t\\t\\t\\t\\tasync_def = False\\n\\t\\t\\t\\t\\tasync_def_nl = False\\n\\t\\t\\t\\t\\tasync_def_indent = 0\\n\\t\\t\\t\\tyield (DEDENT, '', (lnum, pos), (lnum, pos), line)\\n\\t\\t\\tif async_def and async_def_nl and async_def_indent >= indents[-1]:\\n\\t\\t\\t\\tasync_def = False\\n\\t\\t\\t\\tasync_def_nl = False\\n\\t\\t\\t\\tasync_def_indent = 0\\n\\t\\telse:\\t\\t\\t\\t\\t\\t\\t\\t  \\n\\t\\t\\tif not line:\\n\\t\\t\\t\\traise TokenError(\"EOF in multi-line statement\", (lnum, 0))\\n\\t\\t\\tcontinued = 0\\n\\t\\twhile pos < max:\\n\\t\\t\\tpseudomatch = pseudoprog.match(line, pos)\\n\\t\\t\\tif pseudomatch:\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\tstart, end = pseudomatch.span(1)\\n\\t\\t\\t\\tspos, epos, pos = (lnum, start), (lnum, end), end\\n\\t\\t\\t\\ttoken, initial = line[start:end], line[start]\\n\\t\\t\\t\\tif initial in numchars or \\\\n\\t\\t\\t\\t   (initial == '.' and token != '.'):\\t  \\n\\t\\t\\t\\t\\tyield (NUMBER, token, spos, epos, line)\\n\\t\\t\\t\\telif initial in '\\r\\n':\\n\\t\\t\\t\\t\\tnewline = NEWLINE\\n\\t\\t\\t\\t\\tif parenlev > 0:\\n\\t\\t\\t\\t\\t\\tnewline = NL\\n\\t\\t\\t\\t\\telif async_def:\\n\\t\\t\\t\\t\\t\\tasync_def_nl = True\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (newline, token, spos, epos, line)\\n\\t\\t\\t\\telif initial == '\\n\\t\\t\\t\\t\\tassert not token.endswith(\"\\n\")\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (COMMENT, token, spos, epos, line)\\n\\t\\t\\t\\telif token in triple_quoted:\\n\\t\\t\\t\\t\\tendprog = endprogs[token]\\n\\t\\t\\t\\t\\tendmatch = endprog.match(line, pos)\\n\\t\\t\\t\\t\\tif endmatch:\\t\\t\\t\\t\\t\\t   \\n\\t\\t\\t\\t\\t\\tpos = endmatch.end(0)\\n\\t\\t\\t\\t\\t\\ttoken = line[start:pos]\\n\\t\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\t\\tyield (STRING, token, spos, (lnum, pos), line)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tstrstart = (lnum, start)\\t\\t   \\n\\t\\t\\t\\t\\t\\tcontstr = line[start:]\\n\\t\\t\\t\\t\\t\\tcontline = line\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\telif initial in single_quoted or \\\\n\\t\\t\\t\\t\\ttoken[:2] in single_quoted or \\\\n\\t\\t\\t\\t\\ttoken[:3] in single_quoted:\\n\\t\\t\\t\\t\\tif token[-1] == '\\n':\\t\\t\\t\\t  \\n\\t\\t\\t\\t\\t\\tstrstart = (lnum, start)\\n\\t\\t\\t\\t\\t\\tendprog = (endprogs[initial] or endprogs[token[1]] or\\n\\t\\t\\t\\t\\t\\t\\t\\t   endprogs[token[2]])\\n\\t\\t\\t\\t\\t\\tcontstr, needcont = line[start:], 1\\n\\t\\t\\t\\t\\t\\tcontline = line\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\telse:\\t\\t\\t\\t\\t\\t\\t\\t  \\n\\t\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\t\\tyield (STRING, token, spos, epos, line)\\n\\t\\t\\t\\telif initial.isidentifier():\\t\\t\\t   \\n\\t\\t\\t\\t\\tif token in ('async', 'await'):\\n\\t\\t\\t\\t\\t\\tif async_def:\\n\\t\\t\\t\\t\\t\\t\\tyield (ASYNC if token == 'async' else AWAIT,\\n\\t\\t\\t\\t\\t\\t\\t\\t   token, spos, epos, line)\\n\\t\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\ttok = (NAME, token, spos, epos, line)\\n\\t\\t\\t\\t\\tif token == 'async' and not stashed:\\n\\t\\t\\t\\t\\t\\tstashed = tok\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\tif token == 'def':\\n\\t\\t\\t\\t\\t\\tif (stashed\\n\\t\\t\\t\\t\\t\\t\\t\\tand stashed[0] == NAME\\n\\t\\t\\t\\t\\t\\t\\t\\tand stashed[1] == 'async'):\\n\\t\\t\\t\\t\\t\\t\\tasync_def = True\\n\\t\\t\\t\\t\\t\\t\\tasync_def_indent = indents[-1]\\n\\t\\t\\t\\t\\t\\t\\tyield (ASYNC, stashed[1],\\n\\t\\t\\t\\t\\t\\t\\t\\t   stashed[2], stashed[3],\\n\\t\\t\\t\\t\\t\\t\\t\\t   stashed[4])\\n\\t\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield tok\\n\\t\\t\\t\\telif initial == '\\\\':\\t\\t\\t\\t\\t  \\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (NL, token, spos, (lnum, pos), line)\\n\\t\\t\\t\\t\\tcontinued = 1\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif initial in '([{': parenlev = parenlev + 1\\n\\t\\t\\t\\t\\telif initial in ')]}': parenlev = parenlev - 1\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (OP, token, spos, epos, line)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tyield (ERRORTOKEN, line[pos],\\n\\t\\t\\t\\t\\t\\t   (lnum, pos), (lnum, pos+1), line)\\n\\t\\t\\t\\tpos = pos + 1\\n\\tif stashed:\\n\\t\\tyield stashed\\n\\t\\tstashed = None\\n\\tfor indent in indents[1:]:\\t\\t\\t\\t \\n\\t\\tyield (DEDENT, '', (lnum, 0), (lnum, 0), '')\\n\\tyield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\\nif __name__ == '__main__':\\t\\t\\t\\t\\t \\n\\timport sys\\n\\tif len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)\\n\\telse: tokenize(sys.stdin.readline)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Support parsing of async generators in non-async functions (#165)\\n\\nThis is a new syntax added in python3.7, so black can't verify that reformatting will not change the ast unless black itself is run with 3.7. We'll need to change the error message black gives in this case. @ambv any ideas?\\n\\nFixes #125.",
    "fixed_code": "def generate_tokens(readline):\\n\\tlnum = parenlev = continued = 0\\n\\tnumchars = '0123456789'\\n\\tcontstr, needcont = '', 0\\n\\tcontline = None\\n\\tindents = [0]\\n\\tstashed = None\\n\\tasync_def = False\\n\\tasync_def_indent = 0\\n\\tasync_def_nl = False\\n\\twhile 1:\\t\\t\\t\\t\\t\\t\\t\\t   \\n\\t\\ttry:\\n\\t\\t\\tline = readline()\\n\\t\\texcept StopIteration:\\n\\t\\t\\tline = ''\\n\\t\\tlnum = lnum + 1\\n\\t\\tpos, max = 0, len(line)\\n\\t\\tif contstr:\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\tif not line:\\n\\t\\t\\t\\traise TokenError(\"EOF in multi-line string\", strstart)\\n\\t\\t\\tendmatch = endprog.match(line)\\n\\t\\t\\tif endmatch:\\n\\t\\t\\t\\tpos = end = endmatch.end(0)\\n\\t\\t\\t\\tyield (STRING, contstr + line[:end],\\n\\t\\t\\t\\t\\t   strstart, (lnum, end), contline + line)\\n\\t\\t\\t\\tcontstr, needcont = '', 0\\n\\t\\t\\t\\tcontline = None\\n\\t\\t\\telif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\\n\\t\\t\\t\\tyield (ERRORTOKEN, contstr + line,\\n\\t\\t\\t\\t\\t\\t   strstart, (lnum, len(line)), contline)\\n\\t\\t\\t\\tcontstr = ''\\n\\t\\t\\t\\tcontline = None\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontstr = contstr + line\\n\\t\\t\\t\\tcontline = contline + line\\n\\t\\t\\t\\tcontinue\\n\\t\\telif parenlev == 0 and not continued:  \\n\\t\\t\\tif not line: break\\n\\t\\t\\tcolumn = 0\\n\\t\\t\\twhile pos < max:\\t\\t\\t\\t   \\n\\t\\t\\t\\tif line[pos] == ' ': column = column + 1\\n\\t\\t\\t\\telif line[pos] == '\\t': column = (column//tabsize + 1)*tabsize\\n\\t\\t\\t\\telif line[pos] == '\\f': column = 0\\n\\t\\t\\t\\telse: break\\n\\t\\t\\t\\tpos = pos + 1\\n\\t\\t\\tif pos == max: break\\n\\t\\t\\tif stashed:\\n\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\tstashed = None\\n\\t\\t\\tif line[pos] in '\\r\\n':\\t\\t\\t\\n\\t\\t\\t\\tyield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif line[pos] == '\\n\\t\\t\\t\\tcomment_token = line[pos:].rstrip('\\r\\n')\\n\\t\\t\\t\\tnl_pos = pos + len(comment_token)\\n\\t\\t\\t\\tyield (COMMENT, comment_token,\\n\\t\\t\\t\\t\\t\\t(lnum, pos), (lnum, pos + len(comment_token)), line)\\n\\t\\t\\t\\tyield (NL, line[nl_pos:],\\n\\t\\t\\t\\t\\t\\t(lnum, nl_pos), (lnum, len(line)), line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif column > indents[-1]:\\t\\t   \\n\\t\\t\\t\\tindents.append(column)\\n\\t\\t\\t\\tyield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\\n\\t\\t\\twhile column < indents[-1]:\\t\\t\\n\\t\\t\\t\\tif column not in indents:\\n\\t\\t\\t\\t\\traise IndentationError(\\n\\t\\t\\t\\t\\t\\t\"unindent does not match any outer indentation level\",\\n\\t\\t\\t\\t\\t\\t(\"<tokenize>\", lnum, pos, line))\\n\\t\\t\\t\\tindents = indents[:-1]\\n\\t\\t\\t\\tif async_def and async_def_indent >= indents[-1]:\\n\\t\\t\\t\\t\\tasync_def = False\\n\\t\\t\\t\\t\\tasync_def_nl = False\\n\\t\\t\\t\\t\\tasync_def_indent = 0\\n\\t\\t\\t\\tyield (DEDENT, '', (lnum, pos), (lnum, pos), line)\\n\\t\\t\\tif async_def and async_def_nl and async_def_indent >= indents[-1]:\\n\\t\\t\\t\\tasync_def = False\\n\\t\\t\\t\\tasync_def_nl = False\\n\\t\\t\\t\\tasync_def_indent = 0\\n\\t\\telse:\\t\\t\\t\\t\\t\\t\\t\\t  \\n\\t\\t\\tif not line:\\n\\t\\t\\t\\traise TokenError(\"EOF in multi-line statement\", (lnum, 0))\\n\\t\\t\\tcontinued = 0\\n\\t\\twhile pos < max:\\n\\t\\t\\tpseudomatch = pseudoprog.match(line, pos)\\n\\t\\t\\tif pseudomatch:\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\tstart, end = pseudomatch.span(1)\\n\\t\\t\\t\\tspos, epos, pos = (lnum, start), (lnum, end), end\\n\\t\\t\\t\\ttoken, initial = line[start:end], line[start]\\n\\t\\t\\t\\tif initial in numchars or \\\\n\\t\\t\\t\\t   (initial == '.' and token != '.'):\\t  \\n\\t\\t\\t\\t\\tyield (NUMBER, token, spos, epos, line)\\n\\t\\t\\t\\telif initial in '\\r\\n':\\n\\t\\t\\t\\t\\tnewline = NEWLINE\\n\\t\\t\\t\\t\\tif parenlev > 0:\\n\\t\\t\\t\\t\\t\\tnewline = NL\\n\\t\\t\\t\\t\\telif async_def:\\n\\t\\t\\t\\t\\t\\tasync_def_nl = True\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (newline, token, spos, epos, line)\\n\\t\\t\\t\\telif initial == '\\n\\t\\t\\t\\t\\tassert not token.endswith(\"\\n\")\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (COMMENT, token, spos, epos, line)\\n\\t\\t\\t\\telif token in triple_quoted:\\n\\t\\t\\t\\t\\tendprog = endprogs[token]\\n\\t\\t\\t\\t\\tendmatch = endprog.match(line, pos)\\n\\t\\t\\t\\t\\tif endmatch:\\t\\t\\t\\t\\t\\t   \\n\\t\\t\\t\\t\\t\\tpos = endmatch.end(0)\\n\\t\\t\\t\\t\\t\\ttoken = line[start:pos]\\n\\t\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\t\\tyield (STRING, token, spos, (lnum, pos), line)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tstrstart = (lnum, start)\\t\\t   \\n\\t\\t\\t\\t\\t\\tcontstr = line[start:]\\n\\t\\t\\t\\t\\t\\tcontline = line\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\telif initial in single_quoted or \\\\n\\t\\t\\t\\t\\ttoken[:2] in single_quoted or \\\\n\\t\\t\\t\\t\\ttoken[:3] in single_quoted:\\n\\t\\t\\t\\t\\tif token[-1] == '\\n':\\t\\t\\t\\t  \\n\\t\\t\\t\\t\\t\\tstrstart = (lnum, start)\\n\\t\\t\\t\\t\\t\\tendprog = (endprogs[initial] or endprogs[token[1]] or\\n\\t\\t\\t\\t\\t\\t\\t\\t   endprogs[token[2]])\\n\\t\\t\\t\\t\\t\\tcontstr, needcont = line[start:], 1\\n\\t\\t\\t\\t\\t\\tcontline = line\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\telse:\\t\\t\\t\\t\\t\\t\\t\\t  \\n\\t\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\t\\tyield (STRING, token, spos, epos, line)\\n\\t\\t\\t\\telif initial.isidentifier():\\t\\t\\t   \\n\\t\\t\\t\\t\\tif token in ('async', 'await'):\\n\\t\\t\\t\\t\\t\\tif async_def:\\n\\t\\t\\t\\t\\t\\t\\tyield (ASYNC if token == 'async' else AWAIT,\\n\\t\\t\\t\\t\\t\\t\\t\\t   token, spos, epos, line)\\n\\t\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\ttok = (NAME, token, spos, epos, line)\\n\\t\\t\\t\\t\\tif token == 'async' and not stashed:\\n\\t\\t\\t\\t\\t\\tstashed = tok\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\tif token in ('def', 'for'):\\n\\t\\t\\t\\t\\t\\tif (stashed\\n\\t\\t\\t\\t\\t\\t\\t\\tand stashed[0] == NAME\\n\\t\\t\\t\\t\\t\\t\\t\\tand stashed[1] == 'async'):\\n\\t\\t\\t\\t\\t\\t\\tif token == 'def':\\n\\t\\t\\t\\t\\t\\t\\t\\tasync_def = True\\n\\t\\t\\t\\t\\t\\t\\t\\tasync_def_indent = indents[-1]\\n\\t\\t\\t\\t\\t\\t\\tyield (ASYNC, stashed[1],\\n\\t\\t\\t\\t\\t\\t\\t\\t   stashed[2], stashed[3],\\n\\t\\t\\t\\t\\t\\t\\t\\t   stashed[4])\\n\\t\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield tok\\n\\t\\t\\t\\telif initial == '\\\\':\\t\\t\\t\\t\\t  \\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (NL, token, spos, (lnum, pos), line)\\n\\t\\t\\t\\t\\tcontinued = 1\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif initial in '([{': parenlev = parenlev + 1\\n\\t\\t\\t\\t\\telif initial in ')]}': parenlev = parenlev - 1\\n\\t\\t\\t\\t\\tif stashed:\\n\\t\\t\\t\\t\\t\\tyield stashed\\n\\t\\t\\t\\t\\t\\tstashed = None\\n\\t\\t\\t\\t\\tyield (OP, token, spos, epos, line)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tyield (ERRORTOKEN, line[pos],\\n\\t\\t\\t\\t\\t\\t   (lnum, pos), (lnum, pos+1), line)\\n\\t\\t\\t\\tpos = pos + 1\\n\\tif stashed:\\n\\t\\tyield stashed\\n\\t\\tstashed = None\\n\\tfor indent in indents[1:]:\\t\\t\\t\\t \\n\\t\\tyield (DEDENT, '', (lnum, 0), (lnum, 0), '')\\n\\tyield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\\nif __name__ == '__main__':\\t\\t\\t\\t\\t \\n\\timport sys\\n\\tif len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)\\n\\telse: tokenize(sys.stdin.readline)"
  },
  {
    "code": "def _deprecated(self):\\n        warnings.warn(\".resample() is now a deferred operation\\n\"\\n                      \"use .resample(...).mean() instead of .resample(...)\",\\n                      FutureWarning, stacklevel=3)\\n        return self.mean()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: resample warnings\\n\\ncloses #13618\\ncloses #13520\\n\\nAuthor: Chris <cbartak@gmail.com>\\n\\nCloses #13675 from chris-b1/resample-warning and squashes the following commits:\\n\\n2185c1f [Chris] whatsnew note\\nc58c70c [Chris] DOC: resample warnings",
    "fixed_code": "def _deprecated(self, op):\\n        warnings.warn((\"\\n.resample() is now a deferred operation\\n\"\\n                       \"You called {op}(...) on this deferred object \"\\n                       \"which materialized it into a {klass}\\nby implicitly \"\\n                       \"taking the mean.  Use .resample(...).mean() \"\\n                       \"instead\").format(op=op, klass=self._typ),\\n                      FutureWarning, stacklevel=3)\\n        return self.mean()"
  },
  {
    "code": "def load(data, play=None, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\\n        b = Block(play=play, parent_block=parent_block, role=role, task_include=task_include, use_handlers=use_handlers)\\n        return b.load_data(data, variable_manager=variable_manager, loader=loader)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load(data, play=None, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\\n        b = Block(play=play, parent_block=parent_block, role=role, task_include=task_include, use_handlers=use_handlers)\\n        return b.load_data(data, variable_manager=variable_manager, loader=loader)"
  },
  {
    "code": "def _create_methods(cls, arith_method, comp_method, bool_method, special):\\n    have_divmod = issubclass(cls, ABCSeries)\\n    new_methods = dict(\\n        add=arith_method(cls, operator.add, special),\\n        radd=arith_method(cls, radd, special),\\n        sub=arith_method(cls, operator.sub, special),\\n        mul=arith_method(cls, operator.mul, special),\\n        truediv=arith_method(cls, operator.truediv, special),\\n        floordiv=arith_method(cls, operator.floordiv, special),\\n        mod=arith_method(cls, operator.mod, special),\\n        pow=arith_method(cls, operator.pow, special),\\n        rmul=arith_method(cls, rmul, special),\\n        rsub=arith_method(cls, rsub, special),\\n        rtruediv=arith_method(cls, rtruediv, special),\\n        rfloordiv=arith_method(cls, rfloordiv, special),\\n        rpow=arith_method(cls, rpow, special),\\n        rmod=arith_method(cls, rmod, special))\\n    new_methods['div'] = new_methods['truediv']\\n    new_methods['rdiv'] = new_methods['rtruediv']\\n    if have_divmod:\\n        new_methods['divmod'] = arith_method(cls, divmod, special)\\n    new_methods.update(dict(\\n        eq=comp_method(cls, operator.eq, special),\\n        ne=comp_method(cls, operator.ne, special),\\n        lt=comp_method(cls, operator.lt, special),\\n        gt=comp_method(cls, operator.gt, special),\\n        le=comp_method(cls, operator.le, special),\\n        ge=comp_method(cls, operator.ge, special)))\\n    if bool_method:\\n        new_methods.update(\\n            dict(and_=bool_method(cls, operator.and_, special),\\n                 or_=bool_method(cls, operator.or_, special),\\n                 xor=bool_method(cls, operator.xor, special),\\n                 rand_=bool_method(cls, rand_, special),\\n                 ror_=bool_method(cls, ror_, special),\\n                 rxor=bool_method(cls, rxor, special)))\\n    if special:\\n        dunderize = lambda x: '__{name}__'.format(name=x.strip('_'))\\n    else:\\n        dunderize = lambda x: x\\n    new_methods = {dunderize(k): v for k, v in new_methods.items()}\\n    return new_methods",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Implement Series.__rdivmod__, un-xfail tests (#23271)",
    "fixed_code": "def _create_methods(cls, arith_method, comp_method, bool_method, special):\\n    have_divmod = issubclass(cls, ABCSeries)\\n    new_methods = dict(\\n        add=arith_method(cls, operator.add, special),\\n        radd=arith_method(cls, radd, special),\\n        sub=arith_method(cls, operator.sub, special),\\n        mul=arith_method(cls, operator.mul, special),\\n        truediv=arith_method(cls, operator.truediv, special),\\n        floordiv=arith_method(cls, operator.floordiv, special),\\n        mod=arith_method(cls, operator.mod, special),\\n        pow=arith_method(cls, operator.pow, special),\\n        rmul=arith_method(cls, rmul, special),\\n        rsub=arith_method(cls, rsub, special),\\n        rtruediv=arith_method(cls, rtruediv, special),\\n        rfloordiv=arith_method(cls, rfloordiv, special),\\n        rpow=arith_method(cls, rpow, special),\\n        rmod=arith_method(cls, rmod, special))\\n    new_methods['div'] = new_methods['truediv']\\n    new_methods['rdiv'] = new_methods['rtruediv']\\n    if have_divmod:\\n        new_methods['divmod'] = arith_method(cls, divmod, special)\\n        new_methods['rdivmod'] = arith_method(cls, rdivmod, special)\\n    new_methods.update(dict(\\n        eq=comp_method(cls, operator.eq, special),\\n        ne=comp_method(cls, operator.ne, special),\\n        lt=comp_method(cls, operator.lt, special),\\n        gt=comp_method(cls, operator.gt, special),\\n        le=comp_method(cls, operator.le, special),\\n        ge=comp_method(cls, operator.ge, special)))\\n    if bool_method:\\n        new_methods.update(\\n            dict(and_=bool_method(cls, operator.and_, special),\\n                 or_=bool_method(cls, operator.or_, special),\\n                 xor=bool_method(cls, operator.xor, special),\\n                 rand_=bool_method(cls, rand_, special),\\n                 ror_=bool_method(cls, ror_, special),\\n                 rxor=bool_method(cls, rxor, special)))\\n    if special:\\n        dunderize = lambda x: '__{name}__'.format(name=x.strip('_'))\\n    else:\\n        dunderize = lambda x: x\\n    new_methods = {dunderize(k): v for k, v in new_methods.items()}\\n    return new_methods"
  },
  {
    "code": "def _init_matrix(self, values, index, columns, dtype=None):\\n        from pandas.core.internals import make_block\\n        values = _prep_ndarray(values)\\n        if values.ndim == 1:\\n            N = values.shape[0]\\n            if N == 0:\\n                values = values.reshape((values.shape[0], 0))\\n            else:\\n                values = values.reshape((values.shape[0], 1))\\n        if dtype is not None:\\n            try:\\n                values = values.astype(dtype)\\n            except Exception:\\n                pass\\n        N, K = values.shape\\n        if index is None:\\n            index = _default_index(N)\\n        if columns is None:\\n            columns = _default_index(K)\\n        columns = _ensure_index(columns)\\n        block = make_block(values, columns)\\n        return BlockManager([block], index, columns)\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _init_matrix(self, values, index, columns, dtype=None):\\n        from pandas.core.internals import make_block\\n        values = _prep_ndarray(values)\\n        if values.ndim == 1:\\n            N = values.shape[0]\\n            if N == 0:\\n                values = values.reshape((values.shape[0], 0))\\n            else:\\n                values = values.reshape((values.shape[0], 1))\\n        if dtype is not None:\\n            try:\\n                values = values.astype(dtype)\\n            except Exception:\\n                pass\\n        N, K = values.shape\\n        if index is None:\\n            index = _default_index(N)\\n        if columns is None:\\n            columns = _default_index(K)\\n        columns = _ensure_index(columns)\\n        block = make_block(values, columns)\\n        return BlockManager([block], index, columns)\\n    @property"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        replace_src=dict(),\\n        lines=dict(aliases=['commands'], type='list'),\\n        parents=dict(type='list'),\\n        before=dict(type='list'),\\n        after=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'strict', 'exact', 'none']),\\n        replace=dict(default='line', choices=['line', 'block', 'config']),\\n        running_config=dict(aliases=['config']),\\n        intended_config=dict(),\\n        defaults=dict(type='bool', default=False),\\n        backup=dict(type='bool', default=False),\\n        save_when=dict(choices=['always', 'never', 'modified', 'changed'], default='never'),\\n        diff_against=dict(choices=['running', 'startup', 'intended']),\\n        diff_ignore_lines=dict(type='list'),\\n        save=dict(default=False, type='bool', removed_in_version='2.8'),\\n        force=dict(default=False, type='bool', removed_in_version='2.6')\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    mutually_exclusive = [('lines', 'src', 'replace_src'),\\n                          ('parents', 'src'),\\n                          ('save', 'save_when')]\\n    required_if = [('match', 'strict', ['lines']),\\n                   ('match', 'exact', ['lines']),\\n                   ('replace', 'block', ['lines']),\\n                   ('replace', 'config', ['replace_src']),\\n                   ('diff_against', 'intended', ['intended_config'])]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           required_if=required_if,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    nxos_check_args(module, warnings)\\n    result = {'changed': False, 'warnings': warnings}\\n    config = None\\n    diff_ignore_lines = module.params['diff_ignore_lines']\\n    path = module.params['parents']\\n    connection = get_connection(module)\\n    contents = None\\n    replace_src = module.params['replace_src']\\n    if replace_src:\\n        if module.params['replace'] != 'config':\\n            module.fail_json(msg='replace: config is required with replace_src')\\n    if module.params['backup'] or (module._diff and module.params['diff_against'] == 'running'):\\n        contents = get_config(module)\\n        config = NetworkConfig(indent=2, contents=contents)\\n        if module.params['backup']:\\n            result['__backup__'] = contents\\n    if any((module.params['src'], module.params['lines'], replace_src)):\\n        match = module.params['match']\\n        replace = module.params['replace']\\n        commit = not module.check_mode\\n        candidate = get_candidate(module)\\n        running = get_running_config(module, contents)\\n        if replace_src:\\n            commands = candidate.split('\\n')\\n            result['commands'] = result['updates'] = commands\\n            if commit:\\n                load_config(module, commands, replace=replace_src)\\n            result['changed'] = True\\n        else:\\n            try:\\n                response = connection.get_diff(candidate=candidate, running=running, diff_match=match, diff_ignore_lines=diff_ignore_lines, path=path,\\n                                               diff_replace=replace)\\n            except ConnectionError as exc:\\n                module.fail_json(msg=to_text(exc, errors='surrogate_then_replace'))\\n            config_diff = response['config_diff']\\n            if config_diff:\\n                commands = config_diff.split('\\n')\\n                if module.params['before']:\\n                    commands[:0] = module.params['before']\\n                if module.params['after']:\\n                    commands.extend(module.params['after'])\\n                result['commands'] = commands\\n                result['updates'] = commands\\n                if commit:\\n                    load_config(module, commands, replace=replace_src)\\n                result['changed'] = True\\n    running_config = module.params['running_config']\\n    startup_config = None\\n    if module.params['save_when'] == 'always' or module.params['save']:\\n        save_config(module, result)\\n    elif module.params['save_when'] == 'modified':\\n        output = execute_show_commands(module, ['show running-config', 'show startup-config'])\\n        running_config = NetworkConfig(indent=2, contents=output[0], ignore_lines=diff_ignore_lines)\\n        startup_config = NetworkConfig(indent=2, contents=output[1], ignore_lines=diff_ignore_lines)\\n        if running_config.sha1 != startup_config.sha1:\\n            save_config(module, result)\\n    elif module.params['save_when'] == 'changed' and result['changed']:\\n        save_config(module, result)\\n    if module._diff:\\n        if not running_config:\\n            output = execute_show_commands(module, 'show running-config')\\n            contents = output[0]\\n        else:\\n            contents = running_config\\n        running_config = NetworkConfig(indent=2, contents=contents, ignore_lines=diff_ignore_lines)\\n        if module.params['diff_against'] == 'running':\\n            if module.check_mode:\\n                module.warn(\"unable to perform diff against running-config due to check mode\")\\n                contents = None\\n            else:\\n                contents = config.config_text\\n        elif module.params['diff_against'] == 'startup':\\n            if not startup_config:\\n                output = execute_show_commands(module, 'show startup-config')\\n                contents = output[0]\\n            else:\\n                contents = startup_config.config_text\\n        elif module.params['diff_against'] == 'intended':\\n            contents = module.params['intended_config']\\n        if contents is not None:\\n            base_config = NetworkConfig(indent=2, contents=contents, ignore_lines=diff_ignore_lines)\\n            if running_config.sha1 != base_config.sha1:\\n                if module.params['diff_against'] == 'intended':\\n                    before = running_config\\n                    after = base_config\\n                elif module.params['diff_against'] in ('startup', 'running'):\\n                    before = base_config\\n                    after = running_config\\n                result.update({\\n                    'changed': True,\\n                    'diff': {'before': str(before), 'after': str(after)}\\n                })\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        replace_src=dict(),\\n        lines=dict(aliases=['commands'], type='list'),\\n        parents=dict(type='list'),\\n        before=dict(type='list'),\\n        after=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'strict', 'exact', 'none']),\\n        replace=dict(default='line', choices=['line', 'block', 'config']),\\n        running_config=dict(aliases=['config']),\\n        intended_config=dict(),\\n        defaults=dict(type='bool', default=False),\\n        backup=dict(type='bool', default=False),\\n        save_when=dict(choices=['always', 'never', 'modified', 'changed'], default='never'),\\n        diff_against=dict(choices=['running', 'startup', 'intended']),\\n        diff_ignore_lines=dict(type='list'),\\n        save=dict(default=False, type='bool', removed_in_version='2.8'),\\n        force=dict(default=False, type='bool', removed_in_version='2.6')\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    mutually_exclusive = [('lines', 'src', 'replace_src'),\\n                          ('parents', 'src'),\\n                          ('save', 'save_when')]\\n    required_if = [('match', 'strict', ['lines']),\\n                   ('match', 'exact', ['lines']),\\n                   ('replace', 'block', ['lines']),\\n                   ('replace', 'config', ['replace_src']),\\n                   ('diff_against', 'intended', ['intended_config'])]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           required_if=required_if,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    nxos_check_args(module, warnings)\\n    result = {'changed': False, 'warnings': warnings}\\n    config = None\\n    diff_ignore_lines = module.params['diff_ignore_lines']\\n    path = module.params['parents']\\n    connection = get_connection(module)\\n    contents = None\\n    replace_src = module.params['replace_src']\\n    if replace_src:\\n        if module.params['replace'] != 'config':\\n            module.fail_json(msg='replace: config is required with replace_src')\\n    if module.params['backup'] or (module._diff and module.params['diff_against'] == 'running'):\\n        contents = get_config(module)\\n        config = NetworkConfig(indent=2, contents=contents)\\n        if module.params['backup']:\\n            result['__backup__'] = contents\\n    if any((module.params['src'], module.params['lines'], replace_src)):\\n        match = module.params['match']\\n        replace = module.params['replace']\\n        commit = not module.check_mode\\n        candidate = get_candidate(module)\\n        running = get_running_config(module, contents)\\n        if replace_src:\\n            commands = candidate.split('\\n')\\n            result['commands'] = result['updates'] = commands\\n            if commit:\\n                load_config(module, commands, replace=replace_src)\\n            result['changed'] = True\\n        else:\\n            try:\\n                response = connection.get_diff(candidate=candidate, running=running, diff_match=match, diff_ignore_lines=diff_ignore_lines, path=path,\\n                                               diff_replace=replace)\\n            except ConnectionError as exc:\\n                module.fail_json(msg=to_text(exc, errors='surrogate_then_replace'))\\n            config_diff = response['config_diff']\\n            if config_diff:\\n                commands = config_diff.split('\\n')\\n                if module.params['before']:\\n                    commands[:0] = module.params['before']\\n                if module.params['after']:\\n                    commands.extend(module.params['after'])\\n                result['commands'] = commands\\n                result['updates'] = commands\\n                if commit:\\n                    load_config(module, commands, replace=replace_src)\\n                result['changed'] = True\\n    running_config = module.params['running_config']\\n    startup_config = None\\n    if module.params['save_when'] == 'always' or module.params['save']:\\n        save_config(module, result)\\n    elif module.params['save_when'] == 'modified':\\n        output = execute_show_commands(module, ['show running-config', 'show startup-config'])\\n        running_config = NetworkConfig(indent=2, contents=output[0], ignore_lines=diff_ignore_lines)\\n        startup_config = NetworkConfig(indent=2, contents=output[1], ignore_lines=diff_ignore_lines)\\n        if running_config.sha1 != startup_config.sha1:\\n            save_config(module, result)\\n    elif module.params['save_when'] == 'changed' and result['changed']:\\n        save_config(module, result)\\n    if module._diff:\\n        if not running_config:\\n            output = execute_show_commands(module, 'show running-config')\\n            contents = output[0]\\n        else:\\n            contents = running_config\\n        running_config = NetworkConfig(indent=2, contents=contents, ignore_lines=diff_ignore_lines)\\n        if module.params['diff_against'] == 'running':\\n            if module.check_mode:\\n                module.warn(\"unable to perform diff against running-config due to check mode\")\\n                contents = None\\n            else:\\n                contents = config.config_text\\n        elif module.params['diff_against'] == 'startup':\\n            if not startup_config:\\n                output = execute_show_commands(module, 'show startup-config')\\n                contents = output[0]\\n            else:\\n                contents = startup_config.config_text\\n        elif module.params['diff_against'] == 'intended':\\n            contents = module.params['intended_config']\\n        if contents is not None:\\n            base_config = NetworkConfig(indent=2, contents=contents, ignore_lines=diff_ignore_lines)\\n            if running_config.sha1 != base_config.sha1:\\n                if module.params['diff_against'] == 'intended':\\n                    before = running_config\\n                    after = base_config\\n                elif module.params['diff_against'] in ('startup', 'running'):\\n                    before = base_config\\n                    after = running_config\\n                result.update({\\n                    'changed': True,\\n                    'diff': {'before': str(before), 'after': str(after)}\\n                })\\n    module.exit_json(**result)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        lines=dict(aliases=['commands'], type='list'),\\n        parents=dict(type='list'),\\n        before=dict(type='list'),\\n        after=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'strict', 'exact', 'none']),\\n        replace=dict(default='line', choices=['line', 'block', 'config']),\\n        defaults=dict(type='bool', default=False),\\n        backup=dict(type='bool', default=False),\\n        save_when=dict(choices=['always', 'never', 'modified', 'changed'], default='never'),\\n        diff_against=dict(choices=['startup', 'session', 'intended', 'running'], default='session'),\\n        diff_ignore_lines=dict(type='list'),\\n        running_config=dict(aliases=['config']),\\n        intended_config=dict(),\\n        save=dict(default=False, type='bool', removed_in_version='2.8'),\\n        force=dict(default=False, type='bool', removed_in_version='2.6')\\n    )\\n    argument_spec.update(eos_argument_spec)\\n    mutually_exclusive = [('lines', 'src'),\\n                          ('parents', 'src'),\\n                          ('save', 'save_when')]\\n    required_if = [('match', 'strict', ['lines']),\\n                   ('match', 'exact', ['lines']),\\n                   ('replace', 'block', ['lines']),\\n                   ('replace', 'config', ['src']),\\n                   ('diff_against', 'intended', ['intended_config'])]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           required_if=required_if,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    config = None\\n    if module.params['backup'] or (module._diff and module.params['diff_against'] == 'running'):\\n        contents = get_config(module)\\n        config = NetworkConfig(indent=3, contents=contents)\\n        if module.params['backup']:\\n            result['__backup__'] = contents\\n    if any((module.params['src'], module.params['lines'])):\\n        match = module.params['match']\\n        replace = module.params['replace']\\n        candidate = get_candidate(module)\\n        if match != 'none' and replace != 'config':\\n            config_text = get_running_config(module)\\n            config = NetworkConfig(indent=3, contents=config_text)\\n            path = module.params['parents']\\n            configobjs = candidate.difference(config, match=match, replace=replace, path=path)\\n        else:\\n            configobjs = candidate.items\\n        if configobjs:\\n            commands = dumps(configobjs, 'commands').split('\\n')\\n            if module.params['before']:\\n                commands[:0] = module.params['before']\\n            if module.params['after']:\\n                commands.extend(module.params['after'])\\n            result['commands'] = commands\\n            result['updates'] = commands\\n            replace = module.params['replace'] == 'config'\\n            commit = not module.check_mode\\n            response = load_config(module, commands, replace=replace, commit=commit)\\n            if 'diff' in response and module.params['diff_against'] == 'session':\\n                result['diff'] = {'prepared': response['diff']}\\n            if 'session' in response:\\n                result['session'] = response['session']\\n            result['changed'] = True\\n    running_config = None\\n    startup_config = None\\n    diff_ignore_lines = module.params['diff_ignore_lines']\\n    if module.params['save_when'] == 'always' or module.params['save']:\\n        save_config(module, result)\\n    elif module.params['save_when'] == 'modified':\\n        output = run_commands(module, [{'command': 'show running-config', 'output': 'text'},\\n                                       {'command': 'show startup-config', 'output': 'text'}])\\n        running_config = NetworkConfig(indent=1, contents=output[0], ignore_lines=diff_ignore_lines)\\n        startup_config = NetworkConfig(indent=1, contents=output[1], ignore_lines=diff_ignore_lines)\\n        if running_config.sha1 != startup_config.sha1:\\n            save_config(module, result)\\n    elif module.params['save_when'] == 'changed' and result['changed']:\\n        save_config(module, result)\\n    if module._diff:\\n        if not running_config:\\n            output = run_commands(module, {'command': 'show running-config', 'output': 'text'})\\n            contents = output[0]\\n        else:\\n            contents = running_config.config_text\\n        running_config = NetworkConfig(indent=1, contents=contents, ignore_lines=diff_ignore_lines)\\n        if module.params['diff_against'] == 'running':\\n            if module.check_mode:\\n                module.warn(\"unable to perform diff against running-config due to check mode\")\\n                contents = None\\n            else:\\n                contents = config.config_text\\n        elif module.params['diff_against'] == 'startup':\\n            if not startup_config:\\n                output = run_commands(module, {'command': 'show startup-config', 'output': 'text'})\\n                contents = output[0]\\n            else:\\n                contents = startup_config.config_text\\n        elif module.params['diff_against'] == 'intended':\\n            contents = module.params['intended_config']\\n        if contents is not None:\\n            base_config = NetworkConfig(indent=1, contents=contents, ignore_lines=diff_ignore_lines)\\n            if running_config.sha1 != base_config.sha1:\\n                if module.params['diff_against'] == 'intended':\\n                    before = running_config\\n                    after = base_config\\n                elif module.params['diff_against'] in ('startup', 'running'):\\n                    before = base_config\\n                    after = running_config\\n                result.update({\\n                    'changed': True,\\n                    'diff': {'before': str(before), 'after': str(after)}\\n                })\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        lines=dict(aliases=['commands'], type='list'),\\n        parents=dict(type='list'),\\n        before=dict(type='list'),\\n        after=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'strict', 'exact', 'none']),\\n        replace=dict(default='line', choices=['line', 'block', 'config']),\\n        defaults=dict(type='bool', default=False),\\n        backup=dict(type='bool', default=False),\\n        save_when=dict(choices=['always', 'never', 'modified', 'changed'], default='never'),\\n        diff_against=dict(choices=['startup', 'session', 'intended', 'running'], default='session'),\\n        diff_ignore_lines=dict(type='list'),\\n        running_config=dict(aliases=['config']),\\n        intended_config=dict(),\\n        save=dict(default=False, type='bool', removed_in_version='2.8'),\\n        force=dict(default=False, type='bool', removed_in_version='2.6')\\n    )\\n    argument_spec.update(eos_argument_spec)\\n    mutually_exclusive = [('lines', 'src'),\\n                          ('parents', 'src'),\\n                          ('save', 'save_when')]\\n    required_if = [('match', 'strict', ['lines']),\\n                   ('match', 'exact', ['lines']),\\n                   ('replace', 'block', ['lines']),\\n                   ('replace', 'config', ['src']),\\n                   ('diff_against', 'intended', ['intended_config'])]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           required_if=required_if,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    config = None\\n    if module.params['backup'] or (module._diff and module.params['diff_against'] == 'running'):\\n        contents = get_config(module)\\n        config = NetworkConfig(indent=3, contents=contents)\\n        if module.params['backup']:\\n            result['__backup__'] = contents\\n    if any((module.params['src'], module.params['lines'])):\\n        match = module.params['match']\\n        replace = module.params['replace']\\n        candidate = get_candidate(module)\\n        if match != 'none' and replace != 'config':\\n            config_text = get_running_config(module)\\n            config = NetworkConfig(indent=3, contents=config_text)\\n            path = module.params['parents']\\n            configobjs = candidate.difference(config, match=match, replace=replace, path=path)\\n        else:\\n            configobjs = candidate.items\\n        if configobjs:\\n            commands = dumps(configobjs, 'commands').split('\\n')\\n            if module.params['before']:\\n                commands[:0] = module.params['before']\\n            if module.params['after']:\\n                commands.extend(module.params['after'])\\n            result['commands'] = commands\\n            result['updates'] = commands\\n            replace = module.params['replace'] == 'config'\\n            commit = not module.check_mode\\n            response = load_config(module, commands, replace=replace, commit=commit)\\n            if 'diff' in response and module.params['diff_against'] == 'session':\\n                result['diff'] = {'prepared': response['diff']}\\n            if 'session' in response:\\n                result['session'] = response['session']\\n            result['changed'] = True\\n    running_config = None\\n    startup_config = None\\n    diff_ignore_lines = module.params['diff_ignore_lines']\\n    if module.params['save_when'] == 'always' or module.params['save']:\\n        save_config(module, result)\\n    elif module.params['save_when'] == 'modified':\\n        output = run_commands(module, [{'command': 'show running-config', 'output': 'text'},\\n                                       {'command': 'show startup-config', 'output': 'text'}])\\n        running_config = NetworkConfig(indent=1, contents=output[0], ignore_lines=diff_ignore_lines)\\n        startup_config = NetworkConfig(indent=1, contents=output[1], ignore_lines=diff_ignore_lines)\\n        if running_config.sha1 != startup_config.sha1:\\n            save_config(module, result)\\n    elif module.params['save_when'] == 'changed' and result['changed']:\\n        save_config(module, result)\\n    if module._diff:\\n        if not running_config:\\n            output = run_commands(module, {'command': 'show running-config', 'output': 'text'})\\n            contents = output[0]\\n        else:\\n            contents = running_config.config_text\\n        running_config = NetworkConfig(indent=1, contents=contents, ignore_lines=diff_ignore_lines)\\n        if module.params['diff_against'] == 'running':\\n            if module.check_mode:\\n                module.warn(\"unable to perform diff against running-config due to check mode\")\\n                contents = None\\n            else:\\n                contents = config.config_text\\n        elif module.params['diff_against'] == 'startup':\\n            if not startup_config:\\n                output = run_commands(module, {'command': 'show startup-config', 'output': 'text'})\\n                contents = output[0]\\n            else:\\n                contents = startup_config.config_text\\n        elif module.params['diff_against'] == 'intended':\\n            contents = module.params['intended_config']\\n        if contents is not None:\\n            base_config = NetworkConfig(indent=1, contents=contents, ignore_lines=diff_ignore_lines)\\n            if running_config.sha1 != base_config.sha1:\\n                if module.params['diff_against'] == 'intended':\\n                    before = running_config\\n                    after = base_config\\n                elif module.params['diff_against'] in ('startup', 'running'):\\n                    before = base_config\\n                    after = running_config\\n                result.update({\\n                    'changed': True,\\n                    'diff': {'before': str(before), 'after': str(after)}\\n                })\\n    module.exit_json(**result)"
  },
  {
    "code": "def __init__(\\n        self,\\n        dag_folder: Union[str, \"pathlib.Path\", None] = None,\\n        include_examples: bool = conf.getboolean('core', 'LOAD_EXAMPLES'),\\n        include_smart_sensor: bool = conf.getboolean('smart_sensor', 'USE_SMART_SENSOR'),\\n        safe_mode: bool = conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE'),\\n        read_dags_from_db: bool = False,\\n        store_serialized_dags: Optional[bool] = None,\\n        load_op_links: bool = True,\\n    ):\\n        from airflow.models.dag import DAG\\n        super().__init__()\\n        if store_serialized_dags:\\n            warnings.warn(\\n                \"The store_serialized_dags parameter has been deprecated. \"\\n                \"You should pass the read_dags_from_db parameter.\",\\n                DeprecationWarning,\\n                stacklevel=2,\\n            )\\n            read_dags_from_db = store_serialized_dags\\n        dag_folder = dag_folder or settings.DAGS_FOLDER\\n        self.dag_folder = dag_folder\\n        self.dags: Dict[str, DAG] = {}\\n        self.file_last_changed: Dict[str, datetime] = {}\\n        self.import_errors: Dict[str, str] = {}\\n        self.has_logged = False\\n        self.read_dags_from_db = read_dags_from_db\\n        self.dags_last_fetched: Dict[str, datetime] = {}\\n        self.dags_hash: Dict[str, str] = {}\\n        self.dagbag_import_error_tracebacks = conf.getboolean('core', 'dagbag_import_error_tracebacks')\\n        self.dagbag_import_error_traceback_depth = conf.getint('core', 'dagbag_import_error_traceback_depth')\\n        self.collect_dags(\\n            dag_folder=dag_folder,\\n            include_examples=include_examples,\\n            include_smart_sensor=include_smart_sensor,\\n            safe_mode=safe_mode,\\n        )\\n        self.load_op_links = load_op_links",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        dag_folder: Union[str, \"pathlib.Path\", None] = None,\\n        include_examples: bool = conf.getboolean('core', 'LOAD_EXAMPLES'),\\n        include_smart_sensor: bool = conf.getboolean('smart_sensor', 'USE_SMART_SENSOR'),\\n        safe_mode: bool = conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE'),\\n        read_dags_from_db: bool = False,\\n        store_serialized_dags: Optional[bool] = None,\\n        load_op_links: bool = True,\\n    ):\\n        from airflow.models.dag import DAG\\n        super().__init__()\\n        if store_serialized_dags:\\n            warnings.warn(\\n                \"The store_serialized_dags parameter has been deprecated. \"\\n                \"You should pass the read_dags_from_db parameter.\",\\n                DeprecationWarning,\\n                stacklevel=2,\\n            )\\n            read_dags_from_db = store_serialized_dags\\n        dag_folder = dag_folder or settings.DAGS_FOLDER\\n        self.dag_folder = dag_folder\\n        self.dags: Dict[str, DAG] = {}\\n        self.file_last_changed: Dict[str, datetime] = {}\\n        self.import_errors: Dict[str, str] = {}\\n        self.has_logged = False\\n        self.read_dags_from_db = read_dags_from_db\\n        self.dags_last_fetched: Dict[str, datetime] = {}\\n        self.dags_hash: Dict[str, str] = {}\\n        self.dagbag_import_error_tracebacks = conf.getboolean('core', 'dagbag_import_error_tracebacks')\\n        self.dagbag_import_error_traceback_depth = conf.getint('core', 'dagbag_import_error_traceback_depth')\\n        self.collect_dags(\\n            dag_folder=dag_folder,\\n            include_examples=include_examples,\\n            include_smart_sensor=include_smart_sensor,\\n            safe_mode=safe_mode,\\n        )\\n        self.load_op_links = load_op_links"
  },
  {
    "code": "def convert_value(self, value, expression, connection):\\n        if value is None and self.default is None:\\n            return []\\n        return value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #10929 -- Deprecated forced empty result value for PostgreSQL aggregates.\\n\\nThis deprecates forcing a return value for ArrayAgg, JSONBAgg, and\\nStringAgg when there are no rows in the query. Now that we have a\\n``default`` argument for aggregates, we want to revert to returning the\\ndefault of ``None`` which most aggregate functions return and leave it\\nup to the user to decide what they want to be returned by default.",
    "fixed_code": "def convert_value(self, value, expression, connection):\\n        if value is None and not self._default_provided:\\n            warnings.warn(self.deprecation_msg, category=RemovedInDjango50Warning)\\n            return self.deprecation_value\\n        return value"
  },
  {
    "code": "def edit_columns(self) -> list[str]:\\n\\t\\tif self._edit_columns is type(self)._edit_columns and has_request_context():\\n\\t\\t\\tself._edit_columns = [*self._edit_columns, *(k for k, _ in self._iter_extra_field_names())]\\n\\t\\treturn self._edit_columns\\n\\t@action(\"muldelete\", \"Delete\", \"Are you sure you want to delete selected records?\", single=False)\\n\\t@auth.has_access(\\n\\t\\t[\\n\\t\\t\\t(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),\\n\\t\\t]\\n\\t)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Hide sensitive values from extra in connection edit form (#32309)\\n\\nThe fields that are sensitive (i.e password field is used\\nto show them in Connection edit view) should also be hidden\\nwhen they are stored as \"extra\" in the form extra field.\\n\\nThis PR handles both - replacing such values with a\\nplaceholder as well as not updating the value if the\\nplaceholder has not been modified.",
    "fixed_code": "def edit_columns(self) -> list[str]:\\n\\t\\tif self._edit_columns is type(self)._edit_columns and has_request_context():\\n\\t\\t\\tself._edit_columns = [\\n\\t\\t\\t\\t*self._edit_columns,\\n\\t\\t\\t\\t*(k for k, _, _ in self._iter_extra_field_names_and_sensitivity()),\\n\\t\\t\\t]\\n\\t\\treturn self._edit_columns\\n\\t@action(\"muldelete\", \"Delete\", \"Are you sure you want to delete selected records?\", single=False)\\n\\t@auth.has_access(\\n\\t\\t[\\n\\t\\t\\t(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),\\n\\t\\t]\\n\\t)"
  },
  {
    "code": "def rollback(self, transaction):\\n        self.connection.projects().rollback(\\n            projectId=self.project_id, body={'transaction': transaction})\\\\n            .execute()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4014] Change DatastoreHook and add tests (#4842)\\n\\n- update default used version for connecting to the Admin API from v1beta1 to v1\\n- move the establishment of the connection to the function calls instead of the hook init\\n- change get_conn signature to be able to pass an is_admin arg to set an admin connection\\n- rename GoogleCloudBaseHook._authorize function to GoogleCloudBaseHook.authorize\\n- rename the `partialKeys` argument of function `allocate_ids` to `partial_keys`.\\n- add tests\\n- update docs\\n- refactor code\\n\\nMove version attribute from get_conn to __init__\\n\\n- revert renaming of authorize function\\n- improve docs\\n- refactor code",
    "fixed_code": "def rollback(self, transaction):\\n        conn = self.get_conn()\\n        conn.projects().rollback(projectId=self.project_id, body={'transaction': transaction}).execute()"
  },
  {
    "code": "def open(self, name, mode='rb', mixin=None):\\n        file = self._open(name, mode)\\n        if mixin:\\n            file.__class__ = type(mixin.__name__, (mixin, file.__class__), {})\\n        return file",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #16833 -- Removed undocumented `mixin` parameter from the `Storage.open()` method as this was an undocumented and obscure feature. Thanks to Marty and Russell for sanity-checking.",
    "fixed_code": "def open(self, name, mode='rb'):\\n        return self._open(name, mode)"
  },
  {
    "code": "def _get_unmap_kwargs(self, mapped_kwargs: Mapping[str, Any], *, strict: bool) -> Dict[str, Any]:\\n        partial_op_kwargs = self.partial_kwargs[\"op_kwargs\"]\\n        mapped_op_kwargs = mapped_kwargs[\"op_kwargs\"]\\n        if strict:\\n            prevent_duplicates(partial_op_kwargs, mapped_op_kwargs, fail_reason=\"mapping already partial\")\\n        kwargs = {\\n            \"multiple_outputs\": self.multiple_outputs,\\n            \"python_callable\": self.python_callable,\\n            \"op_kwargs\": {**partial_op_kwargs, **mapped_op_kwargs},\\n        }\\n        return super()._get_unmap_kwargs(kwargs, strict=False)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_unmap_kwargs(self, mapped_kwargs: Mapping[str, Any], *, strict: bool) -> Dict[str, Any]:\\n        partial_op_kwargs = self.partial_kwargs[\"op_kwargs\"]\\n        mapped_op_kwargs = mapped_kwargs[\"op_kwargs\"]\\n        if strict:\\n            prevent_duplicates(partial_op_kwargs, mapped_op_kwargs, fail_reason=\"mapping already partial\")\\n        kwargs = {\\n            \"multiple_outputs\": self.multiple_outputs,\\n            \"python_callable\": self.python_callable,\\n            \"op_kwargs\": {**partial_op_kwargs, **mapped_op_kwargs},\\n        }\\n        return super()._get_unmap_kwargs(kwargs, strict=False)"
  },
  {
    "code": "def is_valid_ipv6_address(ip_str):\\n    from django.core.validators import validate_ipv4_address\\n    symbols_re = re.compile(r'^[0-9a-fA-F:.]+$')\\n    if not symbols_re.match(ip_str):\\n        return False\\n    if ':' not in ip_str:\\n        return False\\n    if ip_str.count('::') > 1:\\n        return False\\n    if ':::' in ip_str:\\n        return False\\n    if ((ip_str.startswith(':') and not ip_str.startswith('::')) or\\n            (ip_str.endswith(':') and not ip_str.endswith('::'))):\\n        return False\\n    if ip_str.count(':') > 7:\\n        return False\\n    if '::' not in ip_str and ip_str.count(':') != 7:\\n        if ip_str.count('.') != 3:\\n            return False\\n    ip_str = _explode_shorthand_ip_string(ip_str)\\n    for hextet in ip_str.split(':'):\\n        if hextet.count('.') == 3:\\n            if not ip_str.split(':')[-1] == hextet:\\n                return False\\n            try:\\n                validate_ipv4_address(hextet)\\n            except ValidationError:\\n                return False\\n        else:\\n            try:\\n                if int(hextet, 16) < 0x0 or int(hextet, 16) > 0xFFFF:\\n                    return False\\n            except ValueError:\\n                return False\\n    return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_valid_ipv6_address(ip_str):\\n    from django.core.validators import validate_ipv4_address\\n    symbols_re = re.compile(r'^[0-9a-fA-F:.]+$')\\n    if not symbols_re.match(ip_str):\\n        return False\\n    if ':' not in ip_str:\\n        return False\\n    if ip_str.count('::') > 1:\\n        return False\\n    if ':::' in ip_str:\\n        return False\\n    if ((ip_str.startswith(':') and not ip_str.startswith('::')) or\\n            (ip_str.endswith(':') and not ip_str.endswith('::'))):\\n        return False\\n    if ip_str.count(':') > 7:\\n        return False\\n    if '::' not in ip_str and ip_str.count(':') != 7:\\n        if ip_str.count('.') != 3:\\n            return False\\n    ip_str = _explode_shorthand_ip_string(ip_str)\\n    for hextet in ip_str.split(':'):\\n        if hextet.count('.') == 3:\\n            if not ip_str.split(':')[-1] == hextet:\\n                return False\\n            try:\\n                validate_ipv4_address(hextet)\\n            except ValidationError:\\n                return False\\n        else:\\n            try:\\n                if int(hextet, 16) < 0x0 or int(hextet, 16) > 0xFFFF:\\n                    return False\\n            except ValueError:\\n                return False\\n    return True"
  },
  {
    "code": "def to_excel(self, excel_writer, sheet_name='sheet1', na_rep='',\\n                 cols=None, header=True, index=True, index_label=None):\\n        from pandas.io.parsers import ExcelWriter\\n        need_save = False\\n        if isinstance(excel_writer, str):\\n            excel_writer = ExcelWriter(excel_writer)\\n            need_save = True\\n        excel_writer.cur_sheet = sheet_name\\n        self._helper_csvexcel(excel_writer, na_rep=na_rep, cols=cols,\\n                              header=header, index=index,\\n                              index_label=index_label, encoding=None)\\n        if need_save:\\n            excel_writer.save()\\n    @Appender(fmt.docstring_to_string, indents=1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_excel(self, excel_writer, sheet_name='sheet1', na_rep='',\\n                 cols=None, header=True, index=True, index_label=None):\\n        from pandas.io.parsers import ExcelWriter\\n        need_save = False\\n        if isinstance(excel_writer, str):\\n            excel_writer = ExcelWriter(excel_writer)\\n            need_save = True\\n        excel_writer.cur_sheet = sheet_name\\n        self._helper_csvexcel(excel_writer, na_rep=na_rep, cols=cols,\\n                              header=header, index=index,\\n                              index_label=index_label, encoding=None)\\n        if need_save:\\n            excel_writer.save()\\n    @Appender(fmt.docstring_to_string, indents=1)"
  },
  {
    "code": "def handle(self, addrport='', *args, **options):\\n        self.use_ipv6 = options.get('use_ipv6')\\n        if self.use_ipv6 and not socket.has_ipv6:\\n            raise CommandError('Your Python does not support IPv6.')\\n        if args:\\n            raise CommandError('Usage is runserver %s' % self.args)\\n        self._raw_ipv6 = False\\n        if not addrport:\\n            self.addr = ''\\n            self.port = DEFAULT_PORT\\n        else:\\n            m = re.match(naiveip_re, addrport)\\n            if m is None:\\n                raise CommandError('\"%s\" is not a valid port number '\\n                                   'or address:port pair.' % addrport)\\n            self.addr, _ipv4, _ipv6, _fqdn, self.port = m.groups()\\n            if not self.port.isdigit():\\n                raise CommandError(\"%r is not a valid port number.\" % self.port)\\n            if self.addr:\\n                if _ipv6:\\n                    self.addr = self.addr[1:-1]\\n                    self.use_ipv6 = True\\n                    self._raw_ipv6 = True\\n                elif self.use_ipv6 and not _fqdn:\\n                    raise CommandError('\"%s\" is not a valid IPv6 address.' % self.addr)\\n        if not self.addr:\\n            self.addr = '::1' if self.use_ipv6 else '127.0.0.1'\\n            self._raw_ipv6 = bool(self.use_ipv6)\\n        self.run(*args, **options)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added runserver validation to detect if DEBUG=False and ALLOWED_HOSTS is empty.\\n\\nRefs #19875.",
    "fixed_code": "def handle(self, addrport='', *args, **options):\\n        from django.conf import settings\\n        if not settings.DEBUG and not settings.ALLOWED_HOSTS:\\n            raise CommandError('You must set settings.ALLOWED_HOSTS if DEBUG is False.')\\n        self.use_ipv6 = options.get('use_ipv6')\\n        if self.use_ipv6 and not socket.has_ipv6:\\n            raise CommandError('Your Python does not support IPv6.')\\n        if args:\\n            raise CommandError('Usage is runserver %s' % self.args)\\n        self._raw_ipv6 = False\\n        if not addrport:\\n            self.addr = ''\\n            self.port = DEFAULT_PORT\\n        else:\\n            m = re.match(naiveip_re, addrport)\\n            if m is None:\\n                raise CommandError('\"%s\" is not a valid port number '\\n                                   'or address:port pair.' % addrport)\\n            self.addr, _ipv4, _ipv6, _fqdn, self.port = m.groups()\\n            if not self.port.isdigit():\\n                raise CommandError(\"%r is not a valid port number.\" % self.port)\\n            if self.addr:\\n                if _ipv6:\\n                    self.addr = self.addr[1:-1]\\n                    self.use_ipv6 = True\\n                    self._raw_ipv6 = True\\n                elif self.use_ipv6 and not _fqdn:\\n                    raise CommandError('\"%s\" is not a valid IPv6 address.' % self.addr)\\n        if not self.addr:\\n            self.addr = '::1' if self.use_ipv6 else '127.0.0.1'\\n            self._raw_ipv6 = bool(self.use_ipv6)\\n        self.run(*args, **options)"
  },
  {
    "code": "def _convert_index(index):\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return index.values, 'integer', atom\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                            v.microsecond / 1E6) for v in values],\\n                            dtype=np.float64)\\n        return converted, 'datetime', _tables().Time64Col()\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                            dtype=np.int32)\\n        return converted, 'date', _tables().Time32Col()\\n    elif inferred_type =='string':\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return converted, 'string', _tables().StringCol(itemsize)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return np.asarray(values, dtype=np.int64), 'integer', atom\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return np.asarray(values, dtype=np.float64), 'float', atom\\n    else: \\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: prevent segfault since MultiIndex not supported in HDF5 table format. close #1848",
    "fixed_code": "def _convert_index(index):\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return index.values, 'integer', atom\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                            v.microsecond / 1E6) for v in values],\\n                            dtype=np.float64)\\n        return converted, 'datetime', _tables().Time64Col()\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                            dtype=np.int32)\\n        return converted, 'date', _tables().Time32Col()\\n    elif inferred_type =='string':\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return converted, 'string', _tables().StringCol(itemsize)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return np.asarray(values, dtype=np.int64), 'integer', atom\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return np.asarray(values, dtype=np.float64), 'float', atom\\n    else: \\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom"
  },
  {
    "code": "def getreader(encoding):\\n    return lookup(encoding)[2]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Patch #1436130: codecs.lookup() now returns a CodecInfo object (a subclass of tuple) that provides incremental decoders and encoders (a way to use stateful codecs without the stream API). Functions codecs.getincrementaldecoder() and codecs.getincrementalencoder() have been added.",
    "fixed_code": "def getreader(encoding):\\n    return lookup(encoding).streamreader"
  },
  {
    "code": "def _process_dags(self, dags: List[DAG], tis_out: List[TaskInstanceKeyType]):\\n        check_slas = conf.getboolean('core', 'CHECK_SLAS', fallback=True)\\n        for dag in dags:\\n            if dag.is_paused:\\n                self.log.info(\"Not processing DAG %s since it's paused\", dag.dag_id)\\n                continue\\n            self.log.info(\"Processing %s\", dag.dag_id)\\n            if not dag.is_subdag:\\n                dag_run = self.create_dag_run(dag)\\n                if dag_run:\\n                    expected_start_date = dag.following_schedule(dag_run.execution_date)\\n                    if expected_start_date:\\n                        schedule_delay = dag_run.start_date - expected_start_date\\n                        Stats.timing(\\n                            'dagrun.schedule_delay.{dag_id}'.format(dag_id=dag.dag_id),\\n                            schedule_delay)\\n                self.log.info(\"Created %s\", dag_run)\\n            self._process_task_instances(dag, tis_out)\\n            if check_slas:\\n                self.manage_slas(dag)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_dags(self, dags: List[DAG], tis_out: List[TaskInstanceKeyType]):\\n        check_slas = conf.getboolean('core', 'CHECK_SLAS', fallback=True)\\n        for dag in dags:\\n            if dag.is_paused:\\n                self.log.info(\"Not processing DAG %s since it's paused\", dag.dag_id)\\n                continue\\n            self.log.info(\"Processing %s\", dag.dag_id)\\n            if not dag.is_subdag:\\n                dag_run = self.create_dag_run(dag)\\n                if dag_run:\\n                    expected_start_date = dag.following_schedule(dag_run.execution_date)\\n                    if expected_start_date:\\n                        schedule_delay = dag_run.start_date - expected_start_date\\n                        Stats.timing(\\n                            'dagrun.schedule_delay.{dag_id}'.format(dag_id=dag.dag_id),\\n                            schedule_delay)\\n                self.log.info(\"Created %s\", dag_run)\\n            self._process_task_instances(dag, tis_out)\\n            if check_slas:\\n                self.manage_slas(dag)"
  },
  {
    "code": "def generate_commands(vlan_id, to_set, to_remove):\\n\\tcommands = []\\n\\tif \"vlan_id\" in to_remove:\\n\\t\\treturn [\"no vlan {0}\".format(vlan_id)]\\n\\tfor key in to_remove:\\n\\t\\tif key in to_set.keys():\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"no {0}\".format(key))\\n\\tfor key, value in to_set.items():\\n\\t\\tif key == \"vlan_id\" or value is None:\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"{0} {1}\".format(key, value))\\n\\tif commands:\\n\\t\\tcommands.insert(0, \"vlan {0}\".format(vlan_id))\\n\\treturn commands",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_commands(vlan_id, to_set, to_remove):\\n\\tcommands = []\\n\\tif \"vlan_id\" in to_remove:\\n\\t\\treturn [\"no vlan {0}\".format(vlan_id)]\\n\\tfor key in to_remove:\\n\\t\\tif key in to_set.keys():\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"no {0}\".format(key))\\n\\tfor key, value in to_set.items():\\n\\t\\tif key == \"vlan_id\" or value is None:\\n\\t\\t\\tcontinue\\n\\t\\tcommands.append(\"{0} {1}\".format(key, value))\\n\\tif commands:\\n\\t\\tcommands.insert(0, \"vlan {0}\".format(vlan_id))\\n\\treturn commands"
  },
  {
    "code": "def RawArray(typecode_or_type, size_or_initializer):\\n    ''\\n    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)\\n    if isinstance(size_or_initializer, int):\\n        type_ = type_ * size_or_initializer\\n        return _new_value(type_)\\n    else:\\n        type_ = type_ * len(size_or_initializer)\\n        result = _new_value(type_)\\n        result.__init__(*size_or_initializer)\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #11673: Fix multiprocessing.[Raw]Array constructor to accept a size of type long.  Thanks Robert Kern.",
    "fixed_code": "def RawArray(typecode_or_type, size_or_initializer):\\n    ''\\n    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)\\n    if isinstance(size_or_initializer, (int, long)):\\n        type_ = type_ * size_or_initializer\\n        return _new_value(type_)\\n    else:\\n        type_ = type_ * len(size_or_initializer)\\n        result = _new_value(type_)\\n        result.__init__(*size_or_initializer)\\n        return result"
  },
  {
    "code": "def _kmeans_single_lloyd(X, sample_weight, centers_init, max_iter=300,\\n                         verbose=False, x_squared_norms=None, tol=1e-4,\\n                         n_threads=1):\\n    n_clusters = centers_init.shape[0]\\n    centers = centers_init\\n    centers_new = np.zeros_like(centers)\\n    labels = np.full(X.shape[0], -1, dtype=np.int32)\\n    labels_old = labels.copy()\\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\\n    if sp.issparse(X):\\n        lloyd_iter = lloyd_iter_chunked_sparse\\n        _inertia = _inertia_sparse\\n    else:\\n        lloyd_iter = lloyd_iter_chunked_dense\\n        _inertia = _inertia_dense\\n    strict_convergence = False\\n    with threadpool_limits(limits=1, user_api=\"blas\"):\\n        for i in range(max_iter):\\n            lloyd_iter(X, sample_weight, x_squared_norms, centers, centers_new,\\n                       weight_in_clusters, labels, center_shift, n_threads)\\n            if verbose:\\n                inertia = _inertia(X, sample_weight, centers, labels)\\n                print(f\"Iteration {i}, inertia {inertia}.\")\\n            centers, centers_new = centers_new, centers\\n            if np.array_equal(labels, labels_old):\\n                if verbose:\\n                    print(f\"Converged at iteration {i}: strict convergence.\")\\n                strict_convergence = True\\n                break\\n            else:\\n                center_shift_tot = (center_shift**2).sum()\\n                if center_shift_tot <= tol:\\n                    if verbose:\\n                        print(f\"Converged at iteration {i}: center shift \"\\n                              f\"{center_shift_tot} within tolerance {tol}.\")\\n                    break\\n            labels_old[:] = labels\\n        if not strict_convergence:\\n            lloyd_iter(X, sample_weight, x_squared_norms, centers, centers,\\n                       weight_in_clusters, labels, center_shift, n_threads,\\n                       update_centers=False)\\n    inertia = _inertia(X, sample_weight, centers, labels)\\n    return labels, inertia, centers, i + 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _kmeans_single_lloyd(X, sample_weight, centers_init, max_iter=300,\\n                         verbose=False, x_squared_norms=None, tol=1e-4,\\n                         n_threads=1):\\n    n_clusters = centers_init.shape[0]\\n    centers = centers_init\\n    centers_new = np.zeros_like(centers)\\n    labels = np.full(X.shape[0], -1, dtype=np.int32)\\n    labels_old = labels.copy()\\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\\n    if sp.issparse(X):\\n        lloyd_iter = lloyd_iter_chunked_sparse\\n        _inertia = _inertia_sparse\\n    else:\\n        lloyd_iter = lloyd_iter_chunked_dense\\n        _inertia = _inertia_dense\\n    strict_convergence = False\\n    with threadpool_limits(limits=1, user_api=\"blas\"):\\n        for i in range(max_iter):\\n            lloyd_iter(X, sample_weight, x_squared_norms, centers, centers_new,\\n                       weight_in_clusters, labels, center_shift, n_threads)\\n            if verbose:\\n                inertia = _inertia(X, sample_weight, centers, labels)\\n                print(f\"Iteration {i}, inertia {inertia}.\")\\n            centers, centers_new = centers_new, centers\\n            if np.array_equal(labels, labels_old):\\n                if verbose:\\n                    print(f\"Converged at iteration {i}: strict convergence.\")\\n                strict_convergence = True\\n                break\\n            else:\\n                center_shift_tot = (center_shift**2).sum()\\n                if center_shift_tot <= tol:\\n                    if verbose:\\n                        print(f\"Converged at iteration {i}: center shift \"\\n                              f\"{center_shift_tot} within tolerance {tol}.\")\\n                    break\\n            labels_old[:] = labels\\n        if not strict_convergence:\\n            lloyd_iter(X, sample_weight, x_squared_norms, centers, centers,\\n                       weight_in_clusters, labels, center_shift, n_threads,\\n                       update_centers=False)\\n    inertia = _inertia(X, sample_weight, centers, labels)\\n    return labels, inertia, centers, i + 1"
  },
  {
    "code": "def stop_trigger(\\n        self,\\n        trigger_name: str,\\n        resource_group_name: Optional[str] = None,\\n        factory_name: Optional[str] = None,\\n        **config: Any,\\n    ) -> AzureOperationPoller:\\n        return self.get_conn().triggers.stop(resource_group_name, factory_name, trigger_name, **config)\\n    @provide_targeted_factory",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix AzureDataFactoryHook failing to instantiate its connection (#14565)\\n\\ncloses #14557",
    "fixed_code": "def stop_trigger(\\n        self,\\n        trigger_name: str,\\n        resource_group_name: Optional[str] = None,\\n        factory_name: Optional[str] = None,\\n        **config: Any,\\n    ) -> LROPoller:\\n        return self.get_conn().triggers.begin_stop(resource_group_name, factory_name, trigger_name, **config)\\n    @provide_targeted_factory"
  },
  {
    "code": "class StringAgg(Aggregate):\\n\\tfunction = 'STRING_AGG'\\n\\ttemplate = '%(function)s(%(distinct)s%(expressions)s)'\\n\\tdef __init__(self, expression, delimiter, distinct=False, **extra):\\n\\t\\tdistinct = 'DISTINCT ' if distinct else ''\\n\\t\\tdelimiter_expr = Value(str(delimiter))\\n\\t\\tsuper(StringAgg, self).__init__(expression, delimiter_expr, distinct=distinct, **extra)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class StringAgg(Aggregate):\\n\\tfunction = 'STRING_AGG'\\n\\ttemplate = '%(function)s(%(distinct)s%(expressions)s)'\\n\\tdef __init__(self, expression, delimiter, distinct=False, **extra):\\n\\t\\tdistinct = 'DISTINCT ' if distinct else ''\\n\\t\\tdelimiter_expr = Value(str(delimiter))\\n\\t\\tsuper(StringAgg, self).__init__(expression, delimiter_expr, distinct=distinct, **extra)"
  },
  {
    "code": "def time_strip(self):\\n        self.s.str.strip(\"A\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_strip(self, dtype):\\n        self.s.str.strip(\"A\")"
  },
  {
    "code": "def executemany(self, query, params=None):\\n        if params is not None and not isinstance(params, (list, tuple)):\\n            params = list(params)\\n        try:\\n            args = [(':arg%d' % i) for i in range(len(params[0]))]\\n        except (IndexError, TypeError):\\n            return None\\n        if query.endswith(';') or query.endswith('/'):\\n            query = query[:-1]\\n        query = convert_unicode(query % tuple(args), self.charset)\\n        formatted = [self._format_params(i) for i in params]\\n        self._guess_input_sizes(formatted)\\n        try:\\n            return self.cursor.executemany(query,\\n                                [self._param_generator(p) for p in formatted])\\n        except Database.IntegrityError, e:\\n            raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n        except Database.DatabaseError, e:\\n            if hasattr(e.args[0], 'code') and e.args[0].code == 1400 and not isinstance(e, IntegrityError):\\n                raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n            raise utils.DatabaseError, utils.DatabaseError(*tuple(e)), sys.exc_info()[2]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def executemany(self, query, params=None):\\n        if params is not None and not isinstance(params, (list, tuple)):\\n            params = list(params)\\n        try:\\n            args = [(':arg%d' % i) for i in range(len(params[0]))]\\n        except (IndexError, TypeError):\\n            return None\\n        if query.endswith(';') or query.endswith('/'):\\n            query = query[:-1]\\n        query = convert_unicode(query % tuple(args), self.charset)\\n        formatted = [self._format_params(i) for i in params]\\n        self._guess_input_sizes(formatted)\\n        try:\\n            return self.cursor.executemany(query,\\n                                [self._param_generator(p) for p in formatted])\\n        except Database.IntegrityError, e:\\n            raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n        except Database.DatabaseError, e:\\n            if hasattr(e.args[0], 'code') and e.args[0].code == 1400 and not isinstance(e, IntegrityError):\\n                raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n            raise utils.DatabaseError, utils.DatabaseError(*tuple(e)), sys.exc_info()[2]"
  },
  {
    "code": "def time_replace(self, dtype):\\n        self.s.str.replace(\"A\", \"\\x01\\x01\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_replace(self, dtype):\\n        self.s.str.replace(\"A\", \"\\x01\\x01\")"
  },
  {
    "code": "def untokenize(self, iterable):\\n        for t in iterable:\\n            if len(t) == 2:\\n                self.compat(t, iterable)\\n                break\\n            tok_type, token, start, end, line = t\\n            if tok_type == ENCODING:\\n                self.encoding = token\\n                continue\\n            self.add_whitespace(start)\\n            self.tokens.append(token)\\n            self.prev_row, self.prev_col = end\\n            if tok_type in (NEWLINE, NL):\\n                self.prev_row += 1\\n                self.prev_col = 0\\n        return \"\".join(self.tokens)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #8478: Untokenizer.compat now processes first token from iterator input. Patch based on lines from Georg Brandl, Eric Snow, and Gareth Rees.",
    "fixed_code": "def untokenize(self, iterable):\\n        it = iter(iterable)\\n        for t in it:\\n            if len(t) == 2:\\n                self.compat(t, it)\\n                break\\n            tok_type, token, start, end, line = t\\n            if tok_type == ENCODING:\\n                self.encoding = token\\n                continue\\n            self.add_whitespace(start)\\n            self.tokens.append(token)\\n            self.prev_row, self.prev_col = end\\n            if tok_type in (NEWLINE, NL):\\n                self.prev_row += 1\\n                self.prev_col = 0\\n        return \"\".join(self.tokens)"
  },
  {
    "code": "def _get_index_name(self):\\n        columns = self.columns\\n        try:\\n            line = self._next_line()\\n        except StopIteration:\\n            line = None\\n        index_name = None\\n        implicit_first_col = (line is not None and\\n                              len(line) == len(columns) + 1)\\n        passed_names = self.names is not None\\n        if implicit_first_col:\\n            if self.index_col is None:\\n                self.index_col = 0\\n            index_name = None\\n        elif np.isscalar(self.index_col):\\n            if passed_names:\\n                index_name = None\\n            else:\\n                index_name = columns.pop(self.index_col)\\n        elif self.index_col is not None:\\n            if not passed_names:\\n                cp_cols = list(columns)\\n                index_name = []\\n                for i in self.index_col:\\n                    name = cp_cols[i]\\n                    columns.remove(name)\\n                    index_name.append(name)\\n            else:\\n                index_name=None\\n        return index_name",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: csv reader bugfixes and test coverage",
    "fixed_code": "def _get_index_name(self):\\n        columns = self.columns\\n        passed_names = self.names is not None\\n        try:\\n            line = self._next_line()\\n        except StopIteration:\\n            line = None\\n        implicit_first_col = (line is not None and\\n                              len(line) == len(columns) + 1)\\n        index_name = None\\n        if implicit_first_col:\\n            if self.index_col is None:\\n                self.index_col = 0\\n            index_name = None\\n        elif np.isscalar(self.index_col):\\n            index_name = columns.pop(self.index_col)\\n        elif self.index_col is not None:\\n            cp_cols = list(columns)\\n            index_name = []\\n            for i in self.index_col:\\n                name = cp_cols[i]\\n                columns.remove(name)\\n                index_name.append(name)\\n        return index_name"
  },
  {
    "code": "def download_file(\\n        self, local_path, remote_path, nthreads=64, overwrite=True, buffersize=4194304, blocksize=4194304\\n    ):\\n        multithread.ADLDownloader(\\n            self.get_conn(),\\n            lpath=local_path,\\n            rpath=remote_path,\\n            nthreads=nthreads,\\n            overwrite=overwrite,\\n            buffersize=buffersize,\\n            blocksize=blocksize,\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add LocalToAzureDataLakeStorageOperator (#10814)",
    "fixed_code": "def download_file(\\n        self,\\n        local_path: str,\\n        remote_path: str,\\n        nthreads: int = 64,\\n        overwrite: bool = True,\\n        buffersize: int = 4194304,\\n        blocksize: int = 4194304,\\n        **kwargs,\\n    ) -> Any:\\n        multithread.ADLDownloader(\\n            self.get_conn(),\\n            lpath=local_path,\\n            rpath=remote_path,\\n            nthreads=nthreads,\\n            overwrite=overwrite,\\n            buffersize=buffersize,\\n            blocksize=blocksize,\\n            **kwargs,\\n        )"
  },
  {
    "code": "def _build_unsafe_keys(d):\\n\\t\\tunsafe_keys = set()\\n\\t\\tfor key, value in six.iteritems(d):\\n\\t\\t\\tif isinstance(value, six.text_type):\\n\\t\\t\\t\\tif '@' in value:\\n\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tfor unsafe_key in unsafe_keywords:\\n\\t\\t\\t\\t\\t\\tif unsafe_key in key:\\n\\t\\t\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treturn unsafe_keys\\n\\tunsafe_keywords = set(unsafe_keywords or\\n\\t\\t\\t\\t\\t\\t  getattr(settings, 'DJBLETS_PII_UNSAFE_URL_KEYWORDS',\\n\\t\\t\\t\\t\\t\\t\\t\\t  DEFAULT_PII_UNSAFE_URL_KEYWORDS))\\n\\tcur_unsafe_url_keys = _build_unsafe_keys(url_kwargs or {})\\n\\tcur_unsafe_qs_keys = _build_unsafe_keys(query_dict or {})\\n\\tif cur_unsafe_url_keys:\\n\\t\\tnew_url = re.sub(\\n\\t\\t\\t'(%s)' % '|'.join(\\n\\t\\t\\t\\tre.escape(url_kwargs[key])\\n\\t\\t\\t\\tfor key in cur_unsafe_url_keys\\n\\t\\t\\t),\\n\\t\\t\\t'<REDACTED>',\\n\\t\\t\\turl)\\n\\telse:\\n\\t\\tnew_url = url\\n\\tif cur_unsafe_qs_keys:\\n\\t\\tnew_query = query_dict.copy()\\n\\t\\tfor key in cur_unsafe_qs_keys:\\n\\t\\t\\tnew_query[key] = '<REDACTED>'\\n\\telse:\\n\\t\\tnew_query = query_dict\\n\\tif new_query:\\n\\t\\tnew_url = '%s?%s' % (new_url, new_query.urlencode(safe='/<>'))\\n\\treturn new_url",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_unsafe_keys(d):\\n\\t\\tunsafe_keys = set()\\n\\t\\tfor key, value in six.iteritems(d):\\n\\t\\t\\tif isinstance(value, six.text_type):\\n\\t\\t\\t\\tif '@' in value:\\n\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tfor unsafe_key in unsafe_keywords:\\n\\t\\t\\t\\t\\t\\tif unsafe_key in key:\\n\\t\\t\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treturn unsafe_keys\\n\\tunsafe_keywords = set(unsafe_keywords or\\n\\t\\t\\t\\t\\t\\t  getattr(settings, 'DJBLETS_PII_UNSAFE_URL_KEYWORDS',\\n\\t\\t\\t\\t\\t\\t\\t\\t  DEFAULT_PII_UNSAFE_URL_KEYWORDS))\\n\\tcur_unsafe_url_keys = _build_unsafe_keys(url_kwargs or {})\\n\\tcur_unsafe_qs_keys = _build_unsafe_keys(query_dict or {})\\n\\tif cur_unsafe_url_keys:\\n\\t\\tnew_url = re.sub(\\n\\t\\t\\t'(%s)' % '|'.join(\\n\\t\\t\\t\\tre.escape(url_kwargs[key])\\n\\t\\t\\t\\tfor key in cur_unsafe_url_keys\\n\\t\\t\\t),\\n\\t\\t\\t'<REDACTED>',\\n\\t\\t\\turl)\\n\\telse:\\n\\t\\tnew_url = url\\n\\tif cur_unsafe_qs_keys:\\n\\t\\tnew_query = query_dict.copy()\\n\\t\\tfor key in cur_unsafe_qs_keys:\\n\\t\\t\\tnew_query[key] = '<REDACTED>'\\n\\telse:\\n\\t\\tnew_query = query_dict\\n\\tif new_query:\\n\\t\\tnew_url = '%s?%s' % (new_url, new_query.urlencode(safe='/<>'))\\n\\treturn new_url"
  },
  {
    "code": "def __nonzero__(self):\\n        raise ValueError(\"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\")\\n    __bool__ = __nonzero__",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __nonzero__(self):\\n        raise ValueError(\"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\")\\n    __bool__ = __nonzero__"
  },
  {
    "code": "def check_required_arguments(argument_spec, module_parameters):\\n    missing = []\\n    if argument_spec is None:\\n        return missing\\n    for (k, v) in argument_spec.items():\\n        required = v.get('required', False)\\n        if required and k not in module_parameters:\\n            missing.append(k)\\n    if missing:\\n        msg = \"missing required arguments: %s\" % \", \".join(missing)\\n        raise TypeError(to_native(msg))\\n    return missing",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix and add tests for some module_utils.common.validation (#67771)\\n\\n\\nAsserting inside of the `with` context of `pytest.raises`\\ndoesn't actually have any effect. So we move the assert\\nout, using the exception that gets placed into the scope\\nafter we leave the context, and ensure that it actually gets\\nchecked.\\n\\nThis is also what the pytest documentation says to do:",
    "fixed_code": "def check_required_arguments(argument_spec, module_parameters):\\n    missing = []\\n    if argument_spec is None:\\n        return missing\\n    for (k, v) in argument_spec.items():\\n        required = v.get('required', False)\\n        if required and k not in module_parameters:\\n            missing.append(k)\\n    if missing:\\n        msg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\\n        raise TypeError(to_native(msg))\\n    return missing"
  },
  {
    "code": "def merge(self, right, how='inner', on=None, left_on=None, right_on=None,\\n              left_index=False, right_index=False, sort=False,\\n              suffixes=('_x', '_y'), copy=True, indicator=False):\\n        from pandas.tools.merge import merge\\n        return merge(self, right, how=how, on=on,\\n                     left_on=left_on, right_on=right_on,\\n                     left_index=left_index, right_index=right_index, sort=sort,\\n                     suffixes=suffixes, copy=copy, indicator=indicator)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def merge(self, right, how='inner', on=None, left_on=None, right_on=None,\\n              left_index=False, right_index=False, sort=False,\\n              suffixes=('_x', '_y'), copy=True, indicator=False):\\n        from pandas.tools.merge import merge\\n        return merge(self, right, how=how, on=on,\\n                     left_on=left_on, right_on=right_on,\\n                     left_index=left_index, right_index=right_index, sort=sort,\\n                     suffixes=suffixes, copy=copy, indicator=indicator)"
  },
  {
    "code": "def _check_psd_eigenvalues(lambdas, enable_warnings=False):\\n    lambdas = np.array(lambdas)\\n    is_double_precision = lambdas.dtype == np.float64\\n    significant_imag_ratio = 1e-5\\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\\n    small_pos_ratio = 1e-12 if is_double_precision else 1e-7\\n    if not np.isreal(lambdas).all():\\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\\n        max_real_abs = np.abs(np.real(lambdas)).max()\\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\\n            raise ValueError(\\n                \"There are significant imaginary parts in eigenvalues (%g \"\\n                \"of the maximum real part). Either the matrix is not PSD, or \"\\n                \"there was an issue while computing the eigendecomposition \"\\n                \"of the matrix.\"\\n                % (max_imag_abs / max_real_abs))\\n        if enable_warnings:\\n            warnings.warn(\"There are imaginary parts in eigenvalues (%g \"\\n                          \"of the maximum real part). Either the matrix is not\"\\n                          \" PSD, or there was an issue while computing the \"\\n                          \"eigendecomposition of the matrix. Only the real \"\\n                          \"parts will be kept.\"\\n                          % (max_imag_abs / max_real_abs),\\n                          PositiveSpectrumWarning)\\n    lambdas = np.real(lambdas)\\n    max_eig = lambdas.max()\\n    if max_eig < 0:\\n        raise ValueError(\"All eigenvalues are negative (maximum is %g). \"\\n                         \"Either the matrix is not PSD, or there was an \"\\n                         \"issue while computing the eigendecomposition of \"\\n                         \"the matrix.\" % max_eig)\\n    else:\\n        min_eig = lambdas.min()\\n        if (min_eig < -significant_neg_ratio * max_eig\\n                and min_eig < -significant_neg_value):\\n            raise ValueError(\"There are significant negative eigenvalues (%g\"\\n                             \" of the maximum positive). Either the matrix is \"\\n                             \"not PSD, or there was an issue while computing \"\\n                             \"the eigendecomposition of the matrix.\"\\n                             % (-min_eig / max_eig))\\n        elif min_eig < 0:\\n            if enable_warnings:\\n                warnings.warn(\"There are negative eigenvalues (%g of the \"\\n                              \"maximum positive). Either the matrix is not \"\\n                              \"PSD, or there was an issue while computing the\"\\n                              \" eigendecomposition of the matrix. Negative \"\\n                              \"eigenvalues will be replaced with 0.\"\\n                              % (-min_eig / max_eig),\\n                              PositiveSpectrumWarning)\\n            lambdas[lambdas < 0] = 0\\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\\n    if too_small_lambdas.any():\\n        if enable_warnings:\\n            warnings.warn(\"Badly conditioned PSD matrix spectrum: the largest \"\\n                          \"eigenvalue is more than %g times the smallest. \"\\n                          \"Small eigenvalues will be replaced with 0.\"\\n                          \"\" % (1 / small_pos_ratio),\\n                          PositiveSpectrumWarning)\\n        lambdas[too_small_lambdas] = 0\\n    return lambdas",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_psd_eigenvalues(lambdas, enable_warnings=False):\\n    lambdas = np.array(lambdas)\\n    is_double_precision = lambdas.dtype == np.float64\\n    significant_imag_ratio = 1e-5\\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\\n    small_pos_ratio = 1e-12 if is_double_precision else 1e-7\\n    if not np.isreal(lambdas).all():\\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\\n        max_real_abs = np.abs(np.real(lambdas)).max()\\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\\n            raise ValueError(\\n                \"There are significant imaginary parts in eigenvalues (%g \"\\n                \"of the maximum real part). Either the matrix is not PSD, or \"\\n                \"there was an issue while computing the eigendecomposition \"\\n                \"of the matrix.\"\\n                % (max_imag_abs / max_real_abs))\\n        if enable_warnings:\\n            warnings.warn(\"There are imaginary parts in eigenvalues (%g \"\\n                          \"of the maximum real part). Either the matrix is not\"\\n                          \" PSD, or there was an issue while computing the \"\\n                          \"eigendecomposition of the matrix. Only the real \"\\n                          \"parts will be kept.\"\\n                          % (max_imag_abs / max_real_abs),\\n                          PositiveSpectrumWarning)\\n    lambdas = np.real(lambdas)\\n    max_eig = lambdas.max()\\n    if max_eig < 0:\\n        raise ValueError(\"All eigenvalues are negative (maximum is %g). \"\\n                         \"Either the matrix is not PSD, or there was an \"\\n                         \"issue while computing the eigendecomposition of \"\\n                         \"the matrix.\" % max_eig)\\n    else:\\n        min_eig = lambdas.min()\\n        if (min_eig < -significant_neg_ratio * max_eig\\n                and min_eig < -significant_neg_value):\\n            raise ValueError(\"There are significant negative eigenvalues (%g\"\\n                             \" of the maximum positive). Either the matrix is \"\\n                             \"not PSD, or there was an issue while computing \"\\n                             \"the eigendecomposition of the matrix.\"\\n                             % (-min_eig / max_eig))\\n        elif min_eig < 0:\\n            if enable_warnings:\\n                warnings.warn(\"There are negative eigenvalues (%g of the \"\\n                              \"maximum positive). Either the matrix is not \"\\n                              \"PSD, or there was an issue while computing the\"\\n                              \" eigendecomposition of the matrix. Negative \"\\n                              \"eigenvalues will be replaced with 0.\"\\n                              % (-min_eig / max_eig),\\n                              PositiveSpectrumWarning)\\n            lambdas[lambdas < 0] = 0\\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\\n    if too_small_lambdas.any():\\n        if enable_warnings:\\n            warnings.warn(\"Badly conditioned PSD matrix spectrum: the largest \"\\n                          \"eigenvalue is more than %g times the smallest. \"\\n                          \"Small eigenvalues will be replaced with 0.\"\\n                          \"\" % (1 / small_pos_ratio),\\n                          PositiveSpectrumWarning)\\n        lambdas[too_small_lambdas] = 0\\n    return lambdas"
  },
  {
    "code": "def time_rfind(self, dtype):\\n        self.s.str.rfind(\"[A-Z]+\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_rfind(self, dtype):\\n        self.s.str.rfind(\"[A-Z]+\")"
  },
  {
    "code": "def _key_to_file(self, session_key=None):\\n        if session_key is None:\\n            session_key = self.session_key\\n        if not set(session_key).issubset(self.VALID_KEY_CHARS):\\n            raise SuspiciousOperation(\\n                \"Invalid characters in session key\")\\n        return os.path.join(self.storage_path, self.file_prefix + session_key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _key_to_file(self, session_key=None):\\n        if session_key is None:\\n            session_key = self.session_key\\n        if not set(session_key).issubset(self.VALID_KEY_CHARS):\\n            raise SuspiciousOperation(\\n                \"Invalid characters in session key\")\\n        return os.path.join(self.storage_path, self.file_prefix + session_key)"
  },
  {
    "code": "def lookup_field(name, obj, model_admin=None):\\n    opts = obj._meta\\n    try:\\n        f = _get_non_gfk_field(opts, name)\\n    except FieldDoesNotExist:\\n        if callable(name):\\n            attr = name\\n            value = attr(obj)\\n        elif (model_admin is not None and\\n                hasattr(model_admin, name) and\\n                not name == '__str__' and\\n                not name == '__unicode__'):\\n            attr = getattr(model_admin, name)\\n            value = attr(obj)\\n        else:\\n            attr = getattr(obj, name)\\n            if callable(attr):\\n                value = attr()\\n            else:\\n                value = attr\\n        f = None\\n    else:\\n        attr = None\\n        value = getattr(obj, name)\\n    return f, attr, value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixing #26524 -- Made a foreign key id reference in ModelAdmin.list_display display the id.",
    "fixed_code": "def lookup_field(name, obj, model_admin=None):\\n    opts = obj._meta\\n    try:\\n        f = _get_non_gfk_field(opts, name)\\n    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):\\n        if callable(name):\\n            attr = name\\n            value = attr(obj)\\n        elif (model_admin is not None and\\n                hasattr(model_admin, name) and\\n                not name == '__str__' and\\n                not name == '__unicode__'):\\n            attr = getattr(model_admin, name)\\n            value = attr(obj)\\n        else:\\n            attr = getattr(obj, name)\\n            if callable(attr):\\n                value = attr()\\n            else:\\n                value = attr\\n        f = None\\n    else:\\n        attr = None\\n        value = getattr(obj, name)\\n    return f, attr, value"
  },
  {
    "code": "def __init__(self, filename, mode, perm):\\n        import dbm\\n        self.db = dbm.open(filename, mode, perm)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Create the dbm package from PEP 3108. #2881.",
    "fixed_code": "def __init__(self, filename, mode, perm):\\n        import dbm.ndbm\\n        self.db = dbm.ndbm.open(filename, mode, perm)"
  },
  {
    "code": "def get_all_permissions(self, user_obj, obj=None):\\n        if not user_obj.is_active or obj is not None:\\n            return set()\\n        if not hasattr(user_obj, '_perm_cache'):\\n            user_obj._perm_cache = self.get_user_permissions(user_obj)\\n            user_obj._perm_cache.update(self.get_group_permissions(user_obj))\\n        return user_obj._perm_cache",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_all_permissions(self, user_obj, obj=None):\\n        if not user_obj.is_active or obj is not None:\\n            return set()\\n        if not hasattr(user_obj, '_perm_cache'):\\n            user_obj._perm_cache = self.get_user_permissions(user_obj)\\n            user_obj._perm_cache.update(self.get_group_permissions(user_obj))\\n        return user_obj._perm_cache"
  },
  {
    "code": "def _make_sqlite_account_info(self, env=None, last_upgrade_to_run=None):\\n\\t\\twith mock.patch('os.environ', env or {'HOME': self.home}):\\n\\t\\t\\treturn SqliteAccountInfo(\\n\\t\\t\\t\\tfile_name=self.db_path if not env else None,\\n\\t\\t\\t\\tlast_upgrade_to_run=last_upgrade_to_run,\\n\\t\\t\\t)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Merge pull request from GHSA-p867-fxfr-ph2w\\n\\n\\nThanks to Jan Schejbal for responsible disclosure!\\n\\nThere used to be a brief moment between creation of the sqlite\\ndatabase and applying chmod, now there is no such delay.",
    "fixed_code": "def _make_sqlite_account_info(self, env=None, last_upgrade_to_run=None):\\n\\t\\twith mock.patch('os.environ', env or {'HOME': self.test_home}):\\n\\t\\t\\treturn SqliteAccountInfo(\\n\\t\\t\\t\\tfile_name=self.db_path if not env else None,\\n\\t\\t\\t\\tlast_upgrade_to_run=last_upgrade_to_run,\\n\\t\\t\\t)"
  },
  {
    "code": "def __getitem__(self, key):\\n        if key not in self.obj:\\n            raise KeyError('column %s not found' % key)\\n        return SeriesGroupBy(self.obj[key], groupings=self.groupings,\\n                             name=key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, key):\\n        if key not in self.obj:\\n            raise KeyError('column %s not found' % key)\\n        return SeriesGroupBy(self.obj[key], groupings=self.groupings,\\n                             name=key)"
  },
  {
    "code": "def init_ndarray(values, index, columns, dtype=None, copy=False):\\n\\tif isinstance(values, ABCSeries):\\n\\t\\tif columns is None:\\n\\t\\t\\tif values.name is not None:\\n\\t\\t\\t\\tcolumns = [values.name]\\n\\t\\tif index is None:\\n\\t\\t\\tindex = values.index\\n\\t\\telse:\\n\\t\\t\\tvalues = values.reindex(index)\\n\\t\\tif not len(values) and columns is not None and len(columns):\\n\\t\\t\\tvalues = np.empty((0, 1), dtype=object)\\n\\tif is_categorical_dtype(getattr(values, \"dtype\", None)) or is_categorical_dtype(\\n\\t\\tdtype\\n\\t):\\n\\t\\tif not hasattr(values, \"dtype\"):\\n\\t\\t\\tvalues = prep_ndarray(values, copy=copy)\\n\\t\\t\\tvalues = values.ravel()\\n\\t\\telif copy:\\n\\t\\t\\tvalues = values.copy()\\n\\t\\tindex, columns = _get_axes(len(values), 1, index, columns)\\n\\t\\treturn arrays_to_mgr([values], columns, index, columns, dtype=dtype)\\n\\telif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\\n\\t\\tif columns is None:\\n\\t\\t\\tcolumns = [0]\\n\\t\\treturn arrays_to_mgr([values], columns, index, columns, dtype=dtype)\\n\\tvalues = prep_ndarray(values, copy=copy)\\n\\tif dtype is not None:\\n\\t\\tif not is_dtype_equal(values.dtype, dtype):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tvalues = values.astype(dtype)\\n\\t\\t\\texcept Exception as orig:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\tf\"failed to cast to '{dtype}' (Exception was: {orig})\"\\n\\t\\t\\t\\t) from orig\\n\\tindex, columns = _get_axes(*values.shape, index=index, columns=columns)\\n\\tvalues = values.T\\n\\tif dtype is None and is_object_dtype(values):\\n\\t\\tif values.ndim == 2 and values.shape[0] != 1:\\n\\t\\t\\tdvals_list = [maybe_infer_to_datetimelike(row) for row in values]\\n\\t\\t\\tfor n in range(len(dvals_list)):\\n\\t\\t\\t\\tif isinstance(dvals_list[n], np.ndarray):\\n\\t\\t\\t\\t\\tdvals_list[n] = dvals_list[n].reshape(1, -1)\\n\\t\\t\\tfrom pandas.core.internals.blocks import make_block\\n\\t\\t\\tblock_values = [\\n\\t\\t\\t\\tmake_block(dvals_list[n], placement=[n]) for n in range(len(dvals_list))\\n\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tdatelike_vals = maybe_infer_to_datetimelike(values)\\n\\t\\t\\tblock_values = [datelike_vals]\\n\\telse:\\n\\t\\tblock_values = [values]\\n\\treturn create_block_manager_from_blocks(block_values, [columns, index])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pass 2D ndarray and EA-dtype to DataFrame, closes #12513 (#30507)",
    "fixed_code": "def init_ndarray(values, index, columns, dtype=None, copy=False):\\n\\tif isinstance(values, ABCSeries):\\n\\t\\tif columns is None:\\n\\t\\t\\tif values.name is not None:\\n\\t\\t\\t\\tcolumns = [values.name]\\n\\t\\tif index is None:\\n\\t\\t\\tindex = values.index\\n\\t\\telse:\\n\\t\\t\\tvalues = values.reindex(index)\\n\\t\\tif not len(values) and columns is not None and len(columns):\\n\\t\\t\\tvalues = np.empty((0, 1), dtype=object)\\n\\tif is_categorical_dtype(getattr(values, \"dtype\", None)) or is_categorical_dtype(\\n\\t\\tdtype\\n\\t):\\n\\t\\tif not hasattr(values, \"dtype\"):\\n\\t\\t\\tvalues = prep_ndarray(values, copy=copy)\\n\\t\\t\\tvalues = values.ravel()\\n\\t\\telif copy:\\n\\t\\t\\tvalues = values.copy()\\n\\t\\tindex, columns = _get_axes(len(values), 1, index, columns)\\n\\t\\treturn arrays_to_mgr([values], columns, index, columns, dtype=dtype)\\n\\telif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\\n\\t\\tif isinstance(values, np.ndarray) and values.ndim > 1:\\n\\t\\t\\tvalues = [values[:, n] for n in range(values.shape[1])]\\n\\t\\telse:\\n\\t\\t\\tvalues = [values]\\n\\t\\tif columns is None:\\n\\t\\t\\tcolumns = list(range(len(values)))\\n\\t\\treturn arrays_to_mgr(values, columns, index, columns, dtype=dtype)\\n\\tvalues = prep_ndarray(values, copy=copy)\\n\\tif dtype is not None:\\n\\t\\tif not is_dtype_equal(values.dtype, dtype):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tvalues = values.astype(dtype)\\n\\t\\t\\texcept Exception as orig:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\tf\"failed to cast to '{dtype}' (Exception was: {orig})\"\\n\\t\\t\\t\\t) from orig\\n\\tindex, columns = _get_axes(*values.shape, index=index, columns=columns)\\n\\tvalues = values.T\\n\\tif dtype is None and is_object_dtype(values):\\n\\t\\tif values.ndim == 2 and values.shape[0] != 1:\\n\\t\\t\\tdvals_list = [maybe_infer_to_datetimelike(row) for row in values]\\n\\t\\t\\tfor n in range(len(dvals_list)):\\n\\t\\t\\t\\tif isinstance(dvals_list[n], np.ndarray):\\n\\t\\t\\t\\t\\tdvals_list[n] = dvals_list[n].reshape(1, -1)\\n\\t\\t\\tfrom pandas.core.internals.blocks import make_block\\n\\t\\t\\tblock_values = [\\n\\t\\t\\t\\tmake_block(dvals_list[n], placement=[n]) for n in range(len(dvals_list))\\n\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tdatelike_vals = maybe_infer_to_datetimelike(values)\\n\\t\\t\\tblock_values = [datelike_vals]\\n\\telse:\\n\\t\\tblock_values = [values]\\n\\treturn create_block_manager_from_blocks(block_values, [columns, index])"
  },
  {
    "code": "def _maybe_cache(\\n    arg: ArrayConvertible,\\n    format: str | None,\\n    cache: bool,\\n    convert_listlike: Callable,\\n) -> Series:\\n    from pandas import Series\\n    cache_array = Series(dtype=object)\\n    if cache:\\n        if not should_cache(arg):\\n            return cache_array\\n        unique_dates = unique(arg)\\n        if len(unique_dates) < len(arg):\\n            cache_dates = convert_listlike(unique_dates, format)\\n            cache_array = Series(cache_dates, index=unique_dates)\\n            if not cache_array.index.is_unique:\\n                cache_array = cache_array[~cache_array.index.duplicated()]\\n    return cache_array",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_cache(\\n    arg: ArrayConvertible,\\n    format: str | None,\\n    cache: bool,\\n    convert_listlike: Callable,\\n) -> Series:\\n    from pandas import Series\\n    cache_array = Series(dtype=object)\\n    if cache:\\n        if not should_cache(arg):\\n            return cache_array\\n        unique_dates = unique(arg)\\n        if len(unique_dates) < len(arg):\\n            cache_dates = convert_listlike(unique_dates, format)\\n            cache_array = Series(cache_dates, index=unique_dates)\\n            if not cache_array.index.is_unique:\\n                cache_array = cache_array[~cache_array.index.duplicated()]\\n    return cache_array"
  },
  {
    "code": "def _is_role(self, path):\\n\\t\\t''\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tb_upath = to_bytes(unfrackpath(path, follow=False), errors='surrogate_or_strict')\\n\\t\\tfor finddir in (b'meta', b'tasks'):\\n\\t\\t\\tfor suffix in (b'.yml', b'.yaml', b''):\\n\\t\\t\\t\\tb_main = b'main%s' % (suffix)\\n\\t\\t\\t\\tb_tasked = b'%s/%s' % (finddir, b_main)\\n\\t\\t\\t\\tif (\\n\\t\\t\\t\\t\\tRE_TASKS.search(path) and\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_path, b_main)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_upath, b_tasked)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(os.path.dirname(b_path), b_tasked))\\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes #23680 bug with py3.x due to binary string handling (#23688)\\n\\nfunction and make sure it doesn't break in any Python version.",
    "fixed_code": "def _is_role(self, path):\\n\\t\\t''\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tb_upath = to_bytes(unfrackpath(path, follow=False), errors='surrogate_or_strict')\\n\\t\\tfor b_finddir in (b'meta', b'tasks'):\\n\\t\\t\\tfor b_suffix in (b'.yml', b'.yaml', b''):\\n\\t\\t\\t\\tb_main = b'main%s' % (b_suffix)\\n\\t\\t\\t\\tb_tasked = os.path.join(b_finddir, b_main)\\n\\t\\t\\t\\tif (\\n\\t\\t\\t\\t\\tRE_TASKS.search(path) and\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_path, b_main)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(b_upath, b_tasked)) or\\n\\t\\t\\t\\t\\tos.path.exists(os.path.join(os.path.dirname(b_path), b_tasked))\\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        host=dict(required=True, type=\"str\"),\\n        password=dict(fallback=(env_fallback, [\"ANSIBLE_NET_PASSWORD\"]), no_log=True, required=True),\\n        username=dict(fallback=(env_fallback, [\"ANSIBLE_NET_USERNAME\"]), no_log=True, required=True),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        whitelist=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        use_ssl_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        untrusted_caname=dict(required=False, type=\"str\"),\\n        ssl_exemptions_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_anomalies_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        server_cert_mode=dict(required=False, type=\"str\", choices=[\"re-sign\", \"replace\"]),\\n        server_cert=dict(required=False, type=\"str\"),\\n        rpc_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        name=dict(required=False, type=\"str\"),\\n        mapi_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        caname=dict(required=False, type=\"str\"),\\n        ftps=dict(required=False, type=\"list\"),\\n        ftps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_ports=dict(required=False, type=\"str\"),\\n        ftps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ftps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        https=dict(required=False, type=\"list\"),\\n        https_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_ports=dict(required=False, type=\"str\"),\\n        https_status=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\", \"deep-inspection\"]),\\n        https_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        imaps=dict(required=False, type=\"list\"),\\n        imaps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_ports=dict(required=False, type=\"str\"),\\n        imaps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        imaps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        pop3s=dict(required=False, type=\"list\"),\\n        pop3s_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_ports=dict(required=False, type=\"str\"),\\n        pop3s_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        pop3s_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        smtps=dict(required=False, type=\"list\"),\\n        smtps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_ports=dict(required=False, type=\"str\"),\\n        smtps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        smtps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssh=dict(required=False, type=\"list\"),\\n        ssh_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_ports=dict(required=False, type=\"str\"),\\n        ssh_ssh_algorithm=dict(required=False, type=\"str\", choices=[\"compatible\", \"high-encryption\"]),\\n        ssh_ssh_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_ssh_tun_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_unsupported_version=dict(required=False, type=\"str\", choices=[\"block\", \"bypass\"]),\\n        ssl=dict(required=False, type=\"list\"),\\n        ssl_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\",\\n                                                                  \"deep-inspection\"]),\\n        ssl_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssl_exempt=dict(required=False, type=\"list\"),\\n        ssl_exempt_address=dict(required=False, type=\"str\"),\\n        ssl_exempt_address6=dict(required=False, type=\"str\"),\\n        ssl_exempt_fortiguard_category=dict(required=False, type=\"str\"),\\n        ssl_exempt_regex=dict(required=False, type=\"str\"),\\n        ssl_exempt_type=dict(required=False, type=\"str\", choices=[\"fortiguard-category\", \"address\", \"address6\",\\n                                                                  \"wildcard-fqdn\", \"regex\"]),\\n        ssl_exempt_wildcard_fqdn=dict(required=False, type=\"str\"),\\n        ssl_server=dict(required=False, type=\"list\"),\\n        ssl_server_ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ip=dict(required=False, type=\"str\"),\\n        ssl_server_pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ssl_other_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\",\\n                                                                                           \"block\"]),\\n    )\\n    module = AnsibleModule(argument_spec, supports_check_mode=False)\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"whitelist\": module.params[\"whitelist\"],\\n        \"use-ssl-server\": module.params[\"use_ssl_server\"],\\n        \"untrusted-caname\": module.params[\"untrusted_caname\"],\\n        \"ssl-exemptions-log\": module.params[\"ssl_exemptions_log\"],\\n        \"ssl-anomalies-log\": module.params[\"ssl_anomalies_log\"],\\n        \"server-cert-mode\": module.params[\"server_cert_mode\"],\\n        \"server-cert\": module.params[\"server_cert\"],\\n        \"rpc-over-https\": module.params[\"rpc_over_https\"],\\n        \"name\": module.params[\"name\"],\\n        \"mapi-over-https\": module.params[\"mapi_over_https\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"caname\": module.params[\"caname\"],\\n        \"ftps\": {\\n            \"allow-invalid-server-cert\": module.params[\"ftps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ftps_client_cert_request\"],\\n            \"ports\": module.params[\"ftps_ports\"],\\n            \"status\": module.params[\"ftps_status\"],\\n            \"unsupported-ssl\": module.params[\"ftps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ftps_untrusted_cert\"],\\n        },\\n        \"https\": {\\n            \"allow-invalid-server-cert\": module.params[\"https_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"https_client_cert_request\"],\\n            \"ports\": module.params[\"https_ports\"],\\n            \"status\": module.params[\"https_status\"],\\n            \"unsupported-ssl\": module.params[\"https_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"https_untrusted_cert\"],\\n        },\\n        \"imaps\": {\\n            \"allow-invalid-server-cert\": module.params[\"imaps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"imaps_client_cert_request\"],\\n            \"ports\": module.params[\"imaps_ports\"],\\n            \"status\": module.params[\"imaps_status\"],\\n            \"unsupported-ssl\": module.params[\"imaps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"imaps_untrusted_cert\"],\\n        },\\n        \"pop3s\": {\\n            \"allow-invalid-server-cert\": module.params[\"pop3s_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"pop3s_client_cert_request\"],\\n            \"ports\": module.params[\"pop3s_ports\"],\\n            \"status\": module.params[\"pop3s_status\"],\\n            \"unsupported-ssl\": module.params[\"pop3s_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"pop3s_untrusted_cert\"],\\n        },\\n        \"smtps\": {\\n            \"allow-invalid-server-cert\": module.params[\"smtps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"smtps_client_cert_request\"],\\n            \"ports\": module.params[\"smtps_ports\"],\\n            \"status\": module.params[\"smtps_status\"],\\n            \"unsupported-ssl\": module.params[\"smtps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"smtps_untrusted_cert\"],\\n        },\\n        \"ssh\": {\\n            \"inspect-all\": module.params[\"ssh_inspect_all\"],\\n            \"ports\": module.params[\"ssh_ports\"],\\n            \"ssh-algorithm\": module.params[\"ssh_ssh_algorithm\"],\\n            \"ssh-policy-check\": module.params[\"ssh_ssh_policy_check\"],\\n            \"ssh-tun-policy-check\": module.params[\"ssh_ssh_tun_policy_check\"],\\n            \"status\": module.params[\"ssh_status\"],\\n            \"unsupported-version\": module.params[\"ssh_unsupported_version\"],\\n        },\\n        \"ssl\": {\\n            \"allow-invalid-server-cert\": module.params[\"ssl_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ssl_client_cert_request\"],\\n            \"inspect-all\": module.params[\"ssl_inspect_all\"],\\n            \"unsupported-ssl\": module.params[\"ssl_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ssl_untrusted_cert\"],\\n        },\\n        \"ssl-exempt\": {\\n            \"address\": module.params[\"ssl_exempt_address\"],\\n            \"address6\": module.params[\"ssl_exempt_address6\"],\\n            \"fortiguard-category\": module.params[\"ssl_exempt_fortiguard_category\"],\\n            \"regex\": module.params[\"ssl_exempt_regex\"],\\n            \"type\": module.params[\"ssl_exempt_type\"],\\n            \"wildcard-fqdn\": module.params[\"ssl_exempt_wildcard_fqdn\"],\\n        },\\n        \"ssl-server\": {\\n            \"ftps-client-cert-request\": module.params[\"ssl_server_ftps_client_cert_request\"],\\n            \"https-client-cert-request\": module.params[\"ssl_server_https_client_cert_request\"],\\n            \"imaps-client-cert-request\": module.params[\"ssl_server_imaps_client_cert_request\"],\\n            \"ip\": module.params[\"ssl_server_ip\"],\\n            \"pop3s-client-cert-request\": module.params[\"ssl_server_pop3s_client_cert_request\"],\\n            \"smtps-client-cert-request\": module.params[\"ssl_server_smtps_client_cert_request\"],\\n            \"ssl-other-client-cert-request\": module.params[\"ssl_server_ssl_other_client_cert_request\"],\\n        }\\n    }\\n    list_overrides = ['ftps', 'https', 'imaps', 'pop3s', 'smtps', 'ssh', 'ssl', 'ssl-exempt', 'ssl-server']\\n    for list_variable in list_overrides:\\n        override_data = list()\\n        try:\\n            override_data = module.params[list_variable]\\n        except Exception:\\n            pass\\n        try:\\n            if override_data:\\n                del paramgram[list_variable]\\n                paramgram[list_variable] = override_data\\n        except Exception:\\n            pass\\n    host = module.params[\"host\"]\\n    password = module.params[\"password\"]\\n    username = module.params[\"username\"]\\n    if host is None or username is None or password is None:\\n        module.fail_json(msg=\"Host and username and password are required\")\\n    fmg = AnsibleFortiManager(module, module.params[\"host\"], module.params[\"username\"], module.params[\"password\"])\\n    response = fmg.login()\\n    if response[1]['status']['code'] != 0:\\n        module.fail_json(msg=\"Connection to FortiManager Failed\")\\n    results = fmgr_firewall_ssl_ssh_profile_addsetdelete(fmg, paramgram)\\n    if results[0] != 0:\\n        fmgr_logout(fmg, module, results=results, good_codes=[0])\\n    fmg.logout()\\n    if results is not None:\\n        return module.exit_json(**results[1])\\n    else:\\n        return module.exit_json(msg=\"No results were returned from the API call.\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_secprof_ssl_ssh (#52787)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        whitelist=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        use_ssl_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        untrusted_caname=dict(required=False, type=\"str\"),\\n        ssl_exemptions_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_anomalies_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        server_cert_mode=dict(required=False, type=\"str\", choices=[\"re-sign\", \"replace\"]),\\n        server_cert=dict(required=False, type=\"str\"),\\n        rpc_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        name=dict(required=False, type=\"str\"),\\n        mapi_over_https=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        caname=dict(required=False, type=\"str\"),\\n        ftps=dict(required=False, type=\"list\"),\\n        ftps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_ports=dict(required=False, type=\"str\"),\\n        ftps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ftps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ftps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        https=dict(required=False, type=\"list\"),\\n        https_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_ports=dict(required=False, type=\"str\"),\\n        https_status=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\", \"deep-inspection\"]),\\n        https_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        https_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        imaps=dict(required=False, type=\"list\"),\\n        imaps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_ports=dict(required=False, type=\"str\"),\\n        imaps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        imaps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        imaps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        pop3s=dict(required=False, type=\"list\"),\\n        pop3s_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_ports=dict(required=False, type=\"str\"),\\n        pop3s_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        pop3s_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        pop3s_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        smtps=dict(required=False, type=\"list\"),\\n        smtps_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_ports=dict(required=False, type=\"str\"),\\n        smtps_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        smtps_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        smtps_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssh=dict(required=False, type=\"list\"),\\n        ssh_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_ports=dict(required=False, type=\"str\"),\\n        ssh_ssh_algorithm=dict(required=False, type=\"str\", choices=[\"compatible\", \"high-encryption\"]),\\n        ssh_ssh_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_ssh_tun_policy_check=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssh_status=dict(required=False, type=\"str\", choices=[\"disable\", \"deep-inspection\"]),\\n        ssh_unsupported_version=dict(required=False, type=\"str\", choices=[\"block\", \"bypass\"]),\\n        ssl=dict(required=False, type=\"list\"),\\n        ssl_allow_invalid_server_cert=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_inspect_all=dict(required=False, type=\"str\", choices=[\"disable\", \"certificate-inspection\",\\n                                                                  \"deep-inspection\"]),\\n        ssl_unsupported_ssl=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_untrusted_cert=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"ignore\"]),\\n        ssl_exempt=dict(required=False, type=\"list\"),\\n        ssl_exempt_address=dict(required=False, type=\"str\"),\\n        ssl_exempt_address6=dict(required=False, type=\"str\"),\\n        ssl_exempt_fortiguard_category=dict(required=False, type=\"str\"),\\n        ssl_exempt_regex=dict(required=False, type=\"str\"),\\n        ssl_exempt_type=dict(required=False, type=\"str\", choices=[\"fortiguard-category\", \"address\", \"address6\",\\n                                                                  \"wildcard-fqdn\", \"regex\"]),\\n        ssl_exempt_wildcard_fqdn=dict(required=False, type=\"str\"),\\n        ssl_server=dict(required=False, type=\"list\"),\\n        ssl_server_ftps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_https_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_imaps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ip=dict(required=False, type=\"str\"),\\n        ssl_server_pop3s_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_smtps_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\", \"block\"]),\\n        ssl_server_ssl_other_client_cert_request=dict(required=False, type=\"str\", choices=[\"bypass\", \"inspect\",\\n                                                                                           \"block\"]),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"whitelist\": module.params[\"whitelist\"],\\n        \"use-ssl-server\": module.params[\"use_ssl_server\"],\\n        \"untrusted-caname\": module.params[\"untrusted_caname\"],\\n        \"ssl-exemptions-log\": module.params[\"ssl_exemptions_log\"],\\n        \"ssl-anomalies-log\": module.params[\"ssl_anomalies_log\"],\\n        \"server-cert-mode\": module.params[\"server_cert_mode\"],\\n        \"server-cert\": module.params[\"server_cert\"],\\n        \"rpc-over-https\": module.params[\"rpc_over_https\"],\\n        \"name\": module.params[\"name\"],\\n        \"mapi-over-https\": module.params[\"mapi_over_https\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"caname\": module.params[\"caname\"],\\n        \"ftps\": {\\n            \"allow-invalid-server-cert\": module.params[\"ftps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ftps_client_cert_request\"],\\n            \"ports\": module.params[\"ftps_ports\"],\\n            \"status\": module.params[\"ftps_status\"],\\n            \"unsupported-ssl\": module.params[\"ftps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ftps_untrusted_cert\"],\\n        },\\n        \"https\": {\\n            \"allow-invalid-server-cert\": module.params[\"https_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"https_client_cert_request\"],\\n            \"ports\": module.params[\"https_ports\"],\\n            \"status\": module.params[\"https_status\"],\\n            \"unsupported-ssl\": module.params[\"https_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"https_untrusted_cert\"],\\n        },\\n        \"imaps\": {\\n            \"allow-invalid-server-cert\": module.params[\"imaps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"imaps_client_cert_request\"],\\n            \"ports\": module.params[\"imaps_ports\"],\\n            \"status\": module.params[\"imaps_status\"],\\n            \"unsupported-ssl\": module.params[\"imaps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"imaps_untrusted_cert\"],\\n        },\\n        \"pop3s\": {\\n            \"allow-invalid-server-cert\": module.params[\"pop3s_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"pop3s_client_cert_request\"],\\n            \"ports\": module.params[\"pop3s_ports\"],\\n            \"status\": module.params[\"pop3s_status\"],\\n            \"unsupported-ssl\": module.params[\"pop3s_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"pop3s_untrusted_cert\"],\\n        },\\n        \"smtps\": {\\n            \"allow-invalid-server-cert\": module.params[\"smtps_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"smtps_client_cert_request\"],\\n            \"ports\": module.params[\"smtps_ports\"],\\n            \"status\": module.params[\"smtps_status\"],\\n            \"unsupported-ssl\": module.params[\"smtps_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"smtps_untrusted_cert\"],\\n        },\\n        \"ssh\": {\\n            \"inspect-all\": module.params[\"ssh_inspect_all\"],\\n            \"ports\": module.params[\"ssh_ports\"],\\n            \"ssh-algorithm\": module.params[\"ssh_ssh_algorithm\"],\\n            \"ssh-policy-check\": module.params[\"ssh_ssh_policy_check\"],\\n            \"ssh-tun-policy-check\": module.params[\"ssh_ssh_tun_policy_check\"],\\n            \"status\": module.params[\"ssh_status\"],\\n            \"unsupported-version\": module.params[\"ssh_unsupported_version\"],\\n        },\\n        \"ssl\": {\\n            \"allow-invalid-server-cert\": module.params[\"ssl_allow_invalid_server_cert\"],\\n            \"client-cert-request\": module.params[\"ssl_client_cert_request\"],\\n            \"inspect-all\": module.params[\"ssl_inspect_all\"],\\n            \"unsupported-ssl\": module.params[\"ssl_unsupported_ssl\"],\\n            \"untrusted-cert\": module.params[\"ssl_untrusted_cert\"],\\n        },\\n        \"ssl-exempt\": {\\n            \"address\": module.params[\"ssl_exempt_address\"],\\n            \"address6\": module.params[\"ssl_exempt_address6\"],\\n            \"fortiguard-category\": module.params[\"ssl_exempt_fortiguard_category\"],\\n            \"regex\": module.params[\"ssl_exempt_regex\"],\\n            \"type\": module.params[\"ssl_exempt_type\"],\\n            \"wildcard-fqdn\": module.params[\"ssl_exempt_wildcard_fqdn\"],\\n        },\\n        \"ssl-server\": {\\n            \"ftps-client-cert-request\": module.params[\"ssl_server_ftps_client_cert_request\"],\\n            \"https-client-cert-request\": module.params[\"ssl_server_https_client_cert_request\"],\\n            \"imaps-client-cert-request\": module.params[\"ssl_server_imaps_client_cert_request\"],\\n            \"ip\": module.params[\"ssl_server_ip\"],\\n            \"pop3s-client-cert-request\": module.params[\"ssl_server_pop3s_client_cert_request\"],\\n            \"smtps-client-cert-request\": module.params[\"ssl_server_smtps_client_cert_request\"],\\n            \"ssl-other-client-cert-request\": module.params[\"ssl_server_ssl_other_client_cert_request\"],\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['ftps', 'https', 'imaps', 'pop3s', 'smtps', 'ssh', 'ssl', 'ssl-exempt', 'ssl-server']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_firewall_ssl_ssh_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan,\\n            mask_info=None, allow_fill=True):\\n    if is_categorical(arr):\\n        return arr.take_nd(indexer, fill_value=fill_value,\\n                           allow_fill=allow_fill)\\n    if indexer is None:\\n        indexer = np.arange(arr.shape[axis], dtype=np.int64)\\n        dtype, fill_value = arr.dtype, arr.dtype.type()\\n    else:\\n        indexer = _ensure_int64(indexer)\\n        if not allow_fill:\\n            dtype, fill_value = arr.dtype, arr.dtype.type()\\n            mask_info = None, False\\n        else:\\n            dtype, fill_value = _maybe_promote(arr.dtype, fill_value)\\n            if dtype != arr.dtype and (out is None or out.dtype != dtype):\\n                if mask_info is not None:\\n                    mask, needs_masking = mask_info\\n                else:\\n                    mask = indexer == -1\\n                    needs_masking = mask.any()\\n                    mask_info = mask, needs_masking\\n                if needs_masking:\\n                    if out is not None and out.dtype != dtype:\\n                        raise TypeError('Incompatible type for fill_value')\\n                else:\\n                    dtype, fill_value = arr.dtype, arr.dtype.type()\\n    flip_order = False\\n    if arr.ndim == 2:\\n        if arr.flags.f_contiguous:\\n            flip_order = True\\n    if flip_order:\\n        arr = arr.T\\n        axis = arr.ndim - axis - 1\\n        if out is not None:\\n            out = out.T\\n    if out is None:\\n        out_shape = list(arr.shape)\\n        out_shape[axis] = len(indexer)\\n        out_shape = tuple(out_shape)\\n        if arr.flags.f_contiguous and axis == arr.ndim - 1:\\n            out = np.empty(out_shape, dtype=dtype, order='F')\\n        else:\\n            out = np.empty(out_shape, dtype=dtype)\\n    func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype,\\n                                 axis=axis, mask_info=mask_info)\\n    indexer = _ensure_int64(indexer)\\n    func(arr, indexer, out, fill_value)\\n    if flip_order:\\n        out = out.T\\n    return out",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan,\\n            mask_info=None, allow_fill=True):\\n    if is_categorical(arr):\\n        return arr.take_nd(indexer, fill_value=fill_value,\\n                           allow_fill=allow_fill)\\n    if indexer is None:\\n        indexer = np.arange(arr.shape[axis], dtype=np.int64)\\n        dtype, fill_value = arr.dtype, arr.dtype.type()\\n    else:\\n        indexer = _ensure_int64(indexer)\\n        if not allow_fill:\\n            dtype, fill_value = arr.dtype, arr.dtype.type()\\n            mask_info = None, False\\n        else:\\n            dtype, fill_value = _maybe_promote(arr.dtype, fill_value)\\n            if dtype != arr.dtype and (out is None or out.dtype != dtype):\\n                if mask_info is not None:\\n                    mask, needs_masking = mask_info\\n                else:\\n                    mask = indexer == -1\\n                    needs_masking = mask.any()\\n                    mask_info = mask, needs_masking\\n                if needs_masking:\\n                    if out is not None and out.dtype != dtype:\\n                        raise TypeError('Incompatible type for fill_value')\\n                else:\\n                    dtype, fill_value = arr.dtype, arr.dtype.type()\\n    flip_order = False\\n    if arr.ndim == 2:\\n        if arr.flags.f_contiguous:\\n            flip_order = True\\n    if flip_order:\\n        arr = arr.T\\n        axis = arr.ndim - axis - 1\\n        if out is not None:\\n            out = out.T\\n    if out is None:\\n        out_shape = list(arr.shape)\\n        out_shape[axis] = len(indexer)\\n        out_shape = tuple(out_shape)\\n        if arr.flags.f_contiguous and axis == arr.ndim - 1:\\n            out = np.empty(out_shape, dtype=dtype, order='F')\\n        else:\\n            out = np.empty(out_shape, dtype=dtype)\\n    func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype,\\n                                 axis=axis, mask_info=mask_info)\\n    indexer = _ensure_int64(indexer)\\n    func(arr, indexer, out, fill_value)\\n    if flip_order:\\n        out = out.T\\n    return out"
  },
  {
    "code": "def _normalize_galaxy_yml_manifest(\\n        galaxy_yml,  \\n        b_galaxy_yml_path,  \\n        require_build_metadata=True,  \\n):\\n    galaxy_yml_schema = (\\n        get_collections_galaxy_meta_info()\\n    )  \\n    mandatory_keys = set()\\n    string_keys = set()  \\n    list_keys = set()  \\n    dict_keys = set()  \\n    sentinel_keys = set()  \\n    for info in galaxy_yml_schema:\\n        if info.get('required', False):\\n            mandatory_keys.add(info['key'])\\n        key_list_type = {\\n            'str': string_keys,\\n            'list': list_keys,\\n            'dict': dict_keys,\\n            'sentinel': sentinel_keys,\\n        }[info.get('type', 'str')]\\n        key_list_type.add(info['key'])\\n    all_keys = frozenset(mandatory_keys | string_keys | list_keys | dict_keys | sentinel_keys)\\n    set_keys = set(galaxy_yml.keys())\\n    missing_keys = mandatory_keys.difference(set_keys)\\n    if missing_keys:\\n        msg = (\\n            \"The collection galaxy.yml at '%s' is missing the following mandatory keys: %s\"\\n            % (to_native(b_galaxy_yml_path), \", \".join(sorted(missing_keys)))\\n        )\\n        if require_build_metadata:\\n            raise AnsibleError(msg)\\n        display.warning(msg)\\n        raise ValueError(msg)\\n    extra_keys = set_keys.difference(all_keys)\\n    if len(extra_keys) > 0:\\n        display.warning(\"Found unknown keys in collection galaxy.yml at '%s': %s\"\\n                        % (to_text(b_galaxy_yml_path), \", \".join(extra_keys)))\\n    for optional_string in string_keys:\\n        if optional_string not in galaxy_yml:\\n            galaxy_yml[optional_string] = None\\n    for optional_list in list_keys:\\n        list_val = galaxy_yml.get(optional_list, None)\\n        if list_val is None:\\n            galaxy_yml[optional_list] = []\\n        elif not isinstance(list_val, list):\\n            galaxy_yml[optional_list] = [list_val]  \\n    for optional_dict in dict_keys:\\n        if optional_dict not in galaxy_yml:\\n            galaxy_yml[optional_dict] = {}\\n    for optional_sentinel in sentinel_keys:\\n        if optional_sentinel not in galaxy_yml:\\n            galaxy_yml[optional_sentinel] = Sentinel\\n    if not galaxy_yml.get('version'):\\n        galaxy_yml['version'] = '*'\\n    return galaxy_yml",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _normalize_galaxy_yml_manifest(\\n        galaxy_yml,  \\n        b_galaxy_yml_path,  \\n        require_build_metadata=True,  \\n):\\n    galaxy_yml_schema = (\\n        get_collections_galaxy_meta_info()\\n    )  \\n    mandatory_keys = set()\\n    string_keys = set()  \\n    list_keys = set()  \\n    dict_keys = set()  \\n    sentinel_keys = set()  \\n    for info in galaxy_yml_schema:\\n        if info.get('required', False):\\n            mandatory_keys.add(info['key'])\\n        key_list_type = {\\n            'str': string_keys,\\n            'list': list_keys,\\n            'dict': dict_keys,\\n            'sentinel': sentinel_keys,\\n        }[info.get('type', 'str')]\\n        key_list_type.add(info['key'])\\n    all_keys = frozenset(mandatory_keys | string_keys | list_keys | dict_keys | sentinel_keys)\\n    set_keys = set(galaxy_yml.keys())\\n    missing_keys = mandatory_keys.difference(set_keys)\\n    if missing_keys:\\n        msg = (\\n            \"The collection galaxy.yml at '%s' is missing the following mandatory keys: %s\"\\n            % (to_native(b_galaxy_yml_path), \", \".join(sorted(missing_keys)))\\n        )\\n        if require_build_metadata:\\n            raise AnsibleError(msg)\\n        display.warning(msg)\\n        raise ValueError(msg)\\n    extra_keys = set_keys.difference(all_keys)\\n    if len(extra_keys) > 0:\\n        display.warning(\"Found unknown keys in collection galaxy.yml at '%s': %s\"\\n                        % (to_text(b_galaxy_yml_path), \", \".join(extra_keys)))\\n    for optional_string in string_keys:\\n        if optional_string not in galaxy_yml:\\n            galaxy_yml[optional_string] = None\\n    for optional_list in list_keys:\\n        list_val = galaxy_yml.get(optional_list, None)\\n        if list_val is None:\\n            galaxy_yml[optional_list] = []\\n        elif not isinstance(list_val, list):\\n            galaxy_yml[optional_list] = [list_val]  \\n    for optional_dict in dict_keys:\\n        if optional_dict not in galaxy_yml:\\n            galaxy_yml[optional_dict] = {}\\n    for optional_sentinel in sentinel_keys:\\n        if optional_sentinel not in galaxy_yml:\\n            galaxy_yml[optional_sentinel] = Sentinel\\n    if not galaxy_yml.get('version'):\\n        galaxy_yml['version'] = '*'\\n    return galaxy_yml"
  },
  {
    "code": "def rename(self, name, inplace=False):\\n        return self.set_names([name], inplace=inplace)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: Update docstring of Index.set_names (#22363)",
    "fixed_code": "def rename(self, name, inplace=False):\\n        return self.set_names([name], inplace=inplace)\\n    @property"
  },
  {
    "code": "def copytree(src, dst, symlinks=False, ignore=None):\\n    names = os.listdir(src)\\n    if ignore is not None:\\n        ignored_names = ignore(src, names)\\n    else:\\n        ignored_names = set()\\n    os.makedirs(dst)\\n    errors = []\\n    for name in names:\\n        if name in ignored_names:\\n            continue\\n        srcname = os.path.join(src, name)\\n        dstname = os.path.join(dst, name)\\n        try:\\n            if symlinks and os.path.islink(srcname):\\n                linkto = os.readlink(srcname)\\n                os.symlink(linkto, dstname)\\n            elif os.path.isdir(srcname):\\n                copytree(srcname, dstname, symlinks, ignore)\\n            else:\\n                copy2(srcname, dstname)\\n        except (IOError, os.error), why:\\n            errors.append((srcname, dstname, str(why)))\\n        except Error, err:\\n            errors.extend(err.args[0])\\n    try:\\n        copystat(src, dst)\\n    except OSError, why:\\n        if WindowsError is not None and isinstance(why, WindowsError):\\n            pass\\n        else:\\n            errors.extend((src, dst, str(why)))\\n    if errors:\\n        raise Error, errors",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #3002: `shutil.copyfile()` and `shutil.copytree()` now raise an error when a named pipe is encountered, rather than blocking infinitely.",
    "fixed_code": "def copytree(src, dst, symlinks=False, ignore=None):\\n    names = os.listdir(src)\\n    if ignore is not None:\\n        ignored_names = ignore(src, names)\\n    else:\\n        ignored_names = set()\\n    os.makedirs(dst)\\n    errors = []\\n    for name in names:\\n        if name in ignored_names:\\n            continue\\n        srcname = os.path.join(src, name)\\n        dstname = os.path.join(dst, name)\\n        try:\\n            if symlinks and os.path.islink(srcname):\\n                linkto = os.readlink(srcname)\\n                os.symlink(linkto, dstname)\\n            elif os.path.isdir(srcname):\\n                copytree(srcname, dstname, symlinks, ignore)\\n            else:\\n                copy2(srcname, dstname)\\n        except Error, err:\\n            errors.extend(err.args[0])\\n        except EnvironmentError, why:\\n            errors.append((srcname, dstname, str(why)))\\n    try:\\n        copystat(src, dst)\\n    except OSError, why:\\n        if WindowsError is not None and isinstance(why, WindowsError):\\n            pass\\n        else:\\n            errors.extend((src, dst, str(why)))\\n    if errors:\\n        raise Error, errors"
  },
  {
    "code": "def do_unixy_check(manifest, args):\\n    okay = True\\n    present_macros = gcc_get_limited_api_macros(['Include/Python.h'])\\n    feature_macros = set(m.name for m in manifest.select({'feature_macro'}))\\n    feature_macros &= present_macros\\n    expected_macros = set(\\n        item.name for item in manifest.select({'macro'})\\n    )\\n    missing_macros = expected_macros - present_macros\\n    okay &= _report_unexpected_items(\\n        missing_macros,\\n        'Some macros from are not defined from \"Include/Python.h\"'\\n        + 'with Py_LIMITED_API:')\\n    expected_symbols = set(item.name for item in manifest.select(\\n        {'function', 'data'}, include_abi_only=True, ifdef=feature_macros,\\n    ))\\n    LIBRARY = sysconfig.get_config_var(\"LIBRARY\")\\n    if not LIBRARY:\\n        raise Exception(\"failed to get LIBRARY variable from sysconfig\")\\n    if os.path.exists(LIBRARY):\\n        okay &= binutils_check_library(\\n            manifest, LIBRARY, expected_symbols, dynamic=False)\\n    LDLIBRARY = sysconfig.get_config_var(\"LDLIBRARY\")\\n    if not LDLIBRARY:\\n        raise Exception(\"failed to get LDLIBRARY variable from sysconfig\")\\n    okay &= binutils_check_library(\\n            manifest, LDLIBRARY, expected_symbols, dynamic=False)\\n    expected_defs = set(item.name for item in manifest.select(\\n        {'function', 'data'}, include_abi_only=False, ifdef=feature_macros,\\n    ))\\n    found_defs = gcc_get_limited_api_definitions(['Include/Python.h'])\\n    missing_defs = expected_defs - found_defs\\n    okay &= _report_unexpected_items(\\n        missing_defs,\\n        'Some expected declarations were not declared in '\\n        + '\"Include/Python.h\" with Py_LIMITED_API:')\\n    private_symbols = {n for n in expected_symbols if n.startswith('_')}\\n    extra_defs = found_defs - expected_defs - private_symbols\\n    okay &= _report_unexpected_items(\\n        extra_defs,\\n        'Some extra declarations were found in \"Include/Python.h\" '\\n        + 'with Py_LIMITED_API:')\\n    return okay",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_unixy_check(manifest, args):\\n    okay = True\\n    present_macros = gcc_get_limited_api_macros(['Include/Python.h'])\\n    feature_macros = set(m.name for m in manifest.select({'feature_macro'}))\\n    feature_macros &= present_macros\\n    expected_macros = set(\\n        item.name for item in manifest.select({'macro'})\\n    )\\n    missing_macros = expected_macros - present_macros\\n    okay &= _report_unexpected_items(\\n        missing_macros,\\n        'Some macros from are not defined from \"Include/Python.h\"'\\n        + 'with Py_LIMITED_API:')\\n    expected_symbols = set(item.name for item in manifest.select(\\n        {'function', 'data'}, include_abi_only=True, ifdef=feature_macros,\\n    ))\\n    LIBRARY = sysconfig.get_config_var(\"LIBRARY\")\\n    if not LIBRARY:\\n        raise Exception(\"failed to get LIBRARY variable from sysconfig\")\\n    if os.path.exists(LIBRARY):\\n        okay &= binutils_check_library(\\n            manifest, LIBRARY, expected_symbols, dynamic=False)\\n    LDLIBRARY = sysconfig.get_config_var(\"LDLIBRARY\")\\n    if not LDLIBRARY:\\n        raise Exception(\"failed to get LDLIBRARY variable from sysconfig\")\\n    okay &= binutils_check_library(\\n            manifest, LDLIBRARY, expected_symbols, dynamic=False)\\n    expected_defs = set(item.name for item in manifest.select(\\n        {'function', 'data'}, include_abi_only=False, ifdef=feature_macros,\\n    ))\\n    found_defs = gcc_get_limited_api_definitions(['Include/Python.h'])\\n    missing_defs = expected_defs - found_defs\\n    okay &= _report_unexpected_items(\\n        missing_defs,\\n        'Some expected declarations were not declared in '\\n        + '\"Include/Python.h\" with Py_LIMITED_API:')\\n    private_symbols = {n for n in expected_symbols if n.startswith('_')}\\n    extra_defs = found_defs - expected_defs - private_symbols\\n    okay &= _report_unexpected_items(\\n        extra_defs,\\n        'Some extra declarations were found in \"Include/Python.h\" '\\n        + 'with Py_LIMITED_API:')\\n    return okay"
  },
  {
    "code": "def maybe_cast_to_datetime(\\n    value: Union[ExtensionArray, np.ndarray, list], dtype: Optional[DtypeObj]\\n) -> Union[ExtensionArray, np.ndarray, list]:\\n    from pandas.core.tools.datetimes import to_datetime\\n    from pandas.core.tools.timedeltas import to_timedelta\\n    if not is_list_like(value):\\n        raise TypeError(\"value must be listlike\")\\n    if dtype is not None:\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_datetime64tz = is_datetime64tz_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_datetime64tz or is_timedelta64:\\n            msg = (\\n                f\"The '{dtype.name}' dtype has no unit. \"\\n                f\"Please pass in '{dtype.name}[ns]' instead.\"\\n            )\\n            if is_datetime64:\\n                dtype = getattr(dtype, \"subtype\", dtype)\\n                if not is_dtype_equal(dtype, DT64NS_DTYPE):\\n                    if dtype <= np.dtype(\"M8[ns]\"):\\n                        if dtype.name == \"datetime64\":\\n                            raise ValueError(msg)\\n                        dtype = DT64NS_DTYPE\\n                    else:\\n                        raise TypeError(\\n                            f\"cannot convert datetimelike to dtype [{dtype}]\"\\n                        )\\n            elif is_timedelta64 and not is_dtype_equal(dtype, TD64NS_DTYPE):\\n                if dtype <= np.dtype(\"m8[ns]\"):\\n                    if dtype.name == \"timedelta64\":\\n                        raise ValueError(msg)\\n                    dtype = TD64NS_DTYPE\\n                else:\\n                    raise TypeError(f\"cannot convert timedeltalike to dtype [{dtype}]\")\\n            if not is_sparse(value):\\n                value = np.array(value, copy=False)\\n                if value.ndim == 0:\\n                    value = iNaT\\n                elif value.size or not is_dtype_equal(value.dtype, dtype):\\n                    _disallow_mismatched_datetimelike(value, dtype)\\n                    try:\\n                        if is_datetime64:\\n                            dti = to_datetime(value, errors=\"raise\")\\n                            if dti.tz is not None:\\n                                dti = dti.tz_localize(None)\\n                            value = dti._values\\n                        elif is_datetime64tz:\\n                            is_dt_string = is_string_dtype(value.dtype)\\n                            dta = to_datetime(value, errors=\"raise\").array\\n                            if dta.tz is not None:\\n                                value = dta.astype(dtype, copy=False)\\n                            elif is_dt_string:\\n                                value = dta.tz_localize(dtype.tz)\\n                            else:\\n                                value = dta.tz_localize(\"UTC\").tz_convert(dtype.tz)\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, errors=\"raise\")._values\\n                    except OutOfBoundsDatetime:\\n                        raise\\n                    except ParserError:\\n                        pass\\n                    except ValueError as err:\\n                        if \"mixed datetimes and integers in passed array\" in str(err):\\n                            pass\\n                        else:\\n                            raise\\n        elif is_datetime64_dtype(\\n            getattr(value, \"dtype\", None)\\n        ) and not is_datetime64_dtype(dtype):\\n            if is_object_dtype(dtype):\\n                value = cast(np.ndarray, value)\\n                if value.dtype != DT64NS_DTYPE:\\n                    value = value.astype(DT64NS_DTYPE)\\n                ints = np.asarray(value).view(\"i8\")\\n                return ints_to_pydatetime(ints)\\n            raise TypeError(f\"Cannot cast datetime64 to {dtype}\")\\n    elif isinstance(value, np.ndarray):\\n        if value.dtype.kind in [\"M\", \"m\"]:\\n            value = sanitize_to_nanoseconds(value)\\n        elif value.dtype == object:\\n            value = maybe_infer_to_datetimelike(value)\\n    else:\\n        value = maybe_infer_to_datetimelike(value)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_cast_to_datetime(\\n    value: Union[ExtensionArray, np.ndarray, list], dtype: Optional[DtypeObj]\\n) -> Union[ExtensionArray, np.ndarray, list]:\\n    from pandas.core.tools.datetimes import to_datetime\\n    from pandas.core.tools.timedeltas import to_timedelta\\n    if not is_list_like(value):\\n        raise TypeError(\"value must be listlike\")\\n    if dtype is not None:\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_datetime64tz = is_datetime64tz_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_datetime64tz or is_timedelta64:\\n            msg = (\\n                f\"The '{dtype.name}' dtype has no unit. \"\\n                f\"Please pass in '{dtype.name}[ns]' instead.\"\\n            )\\n            if is_datetime64:\\n                dtype = getattr(dtype, \"subtype\", dtype)\\n                if not is_dtype_equal(dtype, DT64NS_DTYPE):\\n                    if dtype <= np.dtype(\"M8[ns]\"):\\n                        if dtype.name == \"datetime64\":\\n                            raise ValueError(msg)\\n                        dtype = DT64NS_DTYPE\\n                    else:\\n                        raise TypeError(\\n                            f\"cannot convert datetimelike to dtype [{dtype}]\"\\n                        )\\n            elif is_timedelta64 and not is_dtype_equal(dtype, TD64NS_DTYPE):\\n                if dtype <= np.dtype(\"m8[ns]\"):\\n                    if dtype.name == \"timedelta64\":\\n                        raise ValueError(msg)\\n                    dtype = TD64NS_DTYPE\\n                else:\\n                    raise TypeError(f\"cannot convert timedeltalike to dtype [{dtype}]\")\\n            if not is_sparse(value):\\n                value = np.array(value, copy=False)\\n                if value.ndim == 0:\\n                    value = iNaT\\n                elif value.size or not is_dtype_equal(value.dtype, dtype):\\n                    _disallow_mismatched_datetimelike(value, dtype)\\n                    try:\\n                        if is_datetime64:\\n                            dti = to_datetime(value, errors=\"raise\")\\n                            if dti.tz is not None:\\n                                dti = dti.tz_localize(None)\\n                            value = dti._values\\n                        elif is_datetime64tz:\\n                            is_dt_string = is_string_dtype(value.dtype)\\n                            dta = to_datetime(value, errors=\"raise\").array\\n                            if dta.tz is not None:\\n                                value = dta.astype(dtype, copy=False)\\n                            elif is_dt_string:\\n                                value = dta.tz_localize(dtype.tz)\\n                            else:\\n                                value = dta.tz_localize(\"UTC\").tz_convert(dtype.tz)\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, errors=\"raise\")._values\\n                    except OutOfBoundsDatetime:\\n                        raise\\n                    except ParserError:\\n                        pass\\n                    except ValueError as err:\\n                        if \"mixed datetimes and integers in passed array\" in str(err):\\n                            pass\\n                        else:\\n                            raise\\n        elif is_datetime64_dtype(\\n            getattr(value, \"dtype\", None)\\n        ) and not is_datetime64_dtype(dtype):\\n            if is_object_dtype(dtype):\\n                value = cast(np.ndarray, value)\\n                if value.dtype != DT64NS_DTYPE:\\n                    value = value.astype(DT64NS_DTYPE)\\n                ints = np.asarray(value).view(\"i8\")\\n                return ints_to_pydatetime(ints)\\n            raise TypeError(f\"Cannot cast datetime64 to {dtype}\")\\n    elif isinstance(value, np.ndarray):\\n        if value.dtype.kind in [\"M\", \"m\"]:\\n            value = sanitize_to_nanoseconds(value)\\n        elif value.dtype == object:\\n            value = maybe_infer_to_datetimelike(value)\\n    else:\\n        value = maybe_infer_to_datetimelike(value)\\n    return value"
  },
  {
    "code": "def groups(self):\\n        return {ep.group for ep in self}\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def groups(self):\\n        return {ep.group for ep in self}\\n    @classmethod"
  },
  {
    "code": "def __init__(self, required=True, widget=None, label=None):\\n        self.required, self.label = required, label\\n        widget = widget or self.widget\\n        if isinstance(widget, type):\\n            widget = widget()\\n        extra_attrs = self.widget_attrs(widget)\\n        if extra_attrs:\\n            widget.attrs.update(extra_attrs)\\n        self.widget = widget\\n        self.creation_counter = Field.creation_counter\\n        Field.creation_counter += 1",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #3153 -- newforms 'label' argument now can contain wacky characters. Thanks, dswistowski",
    "fixed_code": "def __init__(self, required=True, widget=None, label=None):\\n        if label is not None:\\n            label = smart_unicode(label)\\n        self.required, self.label = required, label\\n        widget = widget or self.widget\\n        if isinstance(widget, type):\\n            widget = widget()\\n        extra_attrs = self.widget_attrs(widget)\\n        if extra_attrs:\\n            widget.attrs.update(extra_attrs)\\n        self.widget = widget\\n        self.creation_counter = Field.creation_counter\\n        Field.creation_counter += 1"
  },
  {
    "code": "def handle_value(k):\\n\\t\\t\\tis_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\\n\\t\\t\\tif isinstance(k, str):\\n\\t\\t\\t\\treturn k\\n\\t\\t\\telif (\\n\\t\\t\\t\\tisinstance(k, collections.abc.Iterable)\\n\\t\\t\\t\\tand not is_zero_dim_ndarray\\n\\t\\t\\t):\\n\\t\\t\\t\\treturn f\"\\\"[{', '.join(map(str, k))}]\\\"\"\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn k\\n\\t\\tif self.keys is None:\\n\\t\\t\\tself.keys = sorted(logs.keys())\\n\\t\\tif self.model.stop_training:\\n\\t\\t\\tlogs = dict(\\n\\t\\t\\t\\t(k, logs[k]) if k in logs else (k, \"NA\") for k in self.keys\\n\\t\\t\\t)\\n\\t\\tif not self.writer:\\n\\t\\t\\tclass CustomDialect(csv.excel):\\n\\t\\t\\t\\tdelimiter = self.sep\\n\\t\\t\\tfieldnames = [\"epoch\"] + self.keys\\n\\t\\t\\tself.writer = csv.DictWriter(\\n\\t\\t\\t\\tself.csv_file, fieldnames=fieldnames, dialect=CustomDialect\\n\\t\\t\\t)\\n\\t\\t\\tif self.append_header:\\n\\t\\t\\t\\tself.writer.writeheader()\\n\\t\\trow_dict = collections.OrderedDict({\"epoch\": epoch})\\n\\t\\trow_dict.update((key, handle_value(logs[key])) for key in self.keys)\\n\\t\\tself.writer.writerow(row_dict)\\n\\t\\tself.csv_file.flush()",
    "label": 1,
    "bug_type": "perf",
    "bug_description": "Fixes GitHub #14049 on missing `val` metrics in CSVLogger, when validation_freq > 1.",
    "fixed_code": "def handle_value(k):\\n\\t\\t\\tis_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\\n\\t\\t\\tif isinstance(k, str):\\n\\t\\t\\t\\treturn k\\n\\t\\t\\telif (\\n\\t\\t\\t\\tisinstance(k, collections.abc.Iterable)\\n\\t\\t\\t\\tand not is_zero_dim_ndarray\\n\\t\\t\\t):\\n\\t\\t\\t\\treturn f\"\\\"[{', '.join(map(str, k))}]\\\"\"\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn k\\n\\t\\tif self.keys is None:\\n\\t\\t\\tself.keys = sorted(logs.keys())\\n\\t\\t\\tval_keys_found = False\\n\\t\\t\\tfor key in self.keys:\\n\\t\\t\\t\\tif key.startswith(\"val_\"):\\n\\t\\t\\t\\t\\tval_keys_found = True\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tif not val_keys_found:\\n\\t\\t\\t\\tself.keys.extend([\"val_\" + k for k in self.keys])\\n\\t\\tif not self.writer:\\n\\t\\t\\tclass CustomDialect(csv.excel):\\n\\t\\t\\t\\tdelimiter = self.sep\\n\\t\\t\\tfieldnames = [\"epoch\"] + self.keys\\n\\t\\t\\tself.writer = csv.DictWriter(\\n\\t\\t\\t\\tself.csv_file, fieldnames=fieldnames, dialect=CustomDialect\\n\\t\\t\\t)\\n\\t\\t\\tif self.append_header:\\n\\t\\t\\t\\tself.writer.writeheader()\\n\\t\\trow_dict = collections.OrderedDict({\"epoch\": epoch})\\n\\t\\trow_dict.update(\\n\\t\\t\\t(key, handle_value(logs.get(key, \"NA\"))) for key in self.keys\\n\\t\\t)\\n\\t\\tself.writer.writerow(row_dict)\\n\\t\\tself.csv_file.flush()"
  },
  {
    "code": "def configuration(parent_package='',top_path=None):\\n    from numpy.distutils.misc_util import Configuration\\n    from numpy.distutils.system_info import get_info, get_standard_file, BlasNotFoundError\\n    config = Configuration('learn',parent_package,top_path)\\n    site_cfg  = ConfigParser()\\n    site_cfg.read(get_standard_file('site.cfg'))\\n    config.add_subpackage('datasets')\\n    config.add_subpackage('features')\\n    config.add_subpackage('feature_selection')\\n    config.add_subpackage('utils')\\n    libsvm_includes = [numpy.get_include()]\\n    libsvm_libraries = []\\n    libsvm_library_dirs = []\\n    libsvm_sources = [join('src', 'libsvm', '_libsvm.c')]\\n    if site_cfg.has_section('libsvm'):\\n        libsvm_includes.append(site_cfg.get('libsvm', 'include_dirs'))\\n        libsvm_libraries.append(site_cfg.get('libsvm', 'libraries'))\\n        libsvm_library_dirs.append(site_cfg.get('libsvm', 'library_dirs'))\\n    else:\\n        libsvm_sources.append(join('src', 'libsvm', 'svm.cpp'))\\n    config.add_extension('_libsvm',\\n                         sources=libsvm_sources,\\n                         include_dirs=libsvm_includes,\\n                         libraries=libsvm_libraries,\\n                         library_dirs=libsvm_library_dirs,\\n                         depends=[join('src', 'libsvm', 'svm.h'),\\n                                  join('src', 'libsvm', 'libsvm_helper.c')]\\n                                  )\\n    blas_sources = [join('src', 'blas', 'daxpy.c'),\\n                    join('src', 'blas', 'ddot.c'),\\n                    join('src', 'blas', 'dnrm2.c'),\\n                    join('src', 'blas', 'dscal.c')]\\n    liblinear_sources = [join('src', 'linear.cpp'),\\n                         join('src', '_liblinear.c'),\\n                         join('src', 'tron.cpp')]\\n    blas_info = get_info('blas_opt', 0)\\n    if not blas_info:\\n        config.add_library('blas', blas_sources)\\n        warnings.warn(BlasNotFoundError.__doc__)\\n    config.add_extension('_liblinear',\\n                         sources=liblinear_sources,\\n                         libraries = blas_info.pop('libraries', ['blas']),\\n                         include_dirs=['src',\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         depends=[join('src', 'linear.h'),\\n                                  join('src', 'tron.h'),\\n                                  join('src', 'blas', 'blas.h'),\\n                                  join('src', 'blas', 'blasp.h')],\\n                         **blas_info)\\n    blas_info = get_info('blas_opt', 0)\\n    if (not blas_info) or (\\n        ('NO_ATLAS_INFO', 1) in blas_info.get('define_macros', [])) :\\n        config.add_library('cblas',\\n                           sources=[\\n                               join('src', 'cblas', '*.c'),\\n                               ]\\n                           )\\n        cblas_libs = ['cblas']\\n        blas_info.pop('libraries', None)\\n    else:\\n        cblas_libs = blas_info.pop('libraries', [])\\n    minilearn_sources = [\\n        join('src', 'minilearn', 'lars.c'),\\n        join('src', 'minilearn', '_minilearn.c')]\\n    config.add_extension('_minilearn',\\n                         sources = minilearn_sources,\\n                         libraries = cblas_libs,\\n                         include_dirs=[join('src', 'minilearn'),\\n                                       join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )\\n    config.add_extension('ball_tree',\\n                         sources=[join('src', 'BallTree.cpp')],\\n                         include_dirs=[numpy.get_include()]\\n                         )\\n    config.add_extension('cd_fast',\\n                         sources=[join('src', 'cd_fast.c')],\\n                         libraries=cblas_libs,\\n                         include_dirs=[join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def configuration(parent_package='',top_path=None):\\n    from numpy.distutils.misc_util import Configuration\\n    from numpy.distutils.system_info import get_info, get_standard_file, BlasNotFoundError\\n    config = Configuration('learn',parent_package,top_path)\\n    site_cfg  = ConfigParser()\\n    site_cfg.read(get_standard_file('site.cfg'))\\n    config.add_subpackage('datasets')\\n    config.add_subpackage('features')\\n    config.add_subpackage('feature_selection')\\n    config.add_subpackage('utils')\\n    libsvm_includes = [numpy.get_include()]\\n    libsvm_libraries = []\\n    libsvm_library_dirs = []\\n    libsvm_sources = [join('src', 'libsvm', '_libsvm.c')]\\n    if site_cfg.has_section('libsvm'):\\n        libsvm_includes.append(site_cfg.get('libsvm', 'include_dirs'))\\n        libsvm_libraries.append(site_cfg.get('libsvm', 'libraries'))\\n        libsvm_library_dirs.append(site_cfg.get('libsvm', 'library_dirs'))\\n    else:\\n        libsvm_sources.append(join('src', 'libsvm', 'svm.cpp'))\\n    config.add_extension('_libsvm',\\n                         sources=libsvm_sources,\\n                         include_dirs=libsvm_includes,\\n                         libraries=libsvm_libraries,\\n                         library_dirs=libsvm_library_dirs,\\n                         depends=[join('src', 'libsvm', 'svm.h'),\\n                                  join('src', 'libsvm', 'libsvm_helper.c')]\\n                                  )\\n    blas_sources = [join('src', 'blas', 'daxpy.c'),\\n                    join('src', 'blas', 'ddot.c'),\\n                    join('src', 'blas', 'dnrm2.c'),\\n                    join('src', 'blas', 'dscal.c')]\\n    liblinear_sources = [join('src', 'linear.cpp'),\\n                         join('src', '_liblinear.c'),\\n                         join('src', 'tron.cpp')]\\n    blas_info = get_info('blas_opt', 0)\\n    if not blas_info:\\n        config.add_library('blas', blas_sources)\\n        warnings.warn(BlasNotFoundError.__doc__)\\n    config.add_extension('_liblinear',\\n                         sources=liblinear_sources,\\n                         libraries = blas_info.pop('libraries', ['blas']),\\n                         include_dirs=['src',\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         depends=[join('src', 'linear.h'),\\n                                  join('src', 'tron.h'),\\n                                  join('src', 'blas', 'blas.h'),\\n                                  join('src', 'blas', 'blasp.h')],\\n                         **blas_info)\\n    blas_info = get_info('blas_opt', 0)\\n    if (not blas_info) or (\\n        ('NO_ATLAS_INFO', 1) in blas_info.get('define_macros', [])) :\\n        config.add_library('cblas',\\n                           sources=[\\n                               join('src', 'cblas', '*.c'),\\n                               ]\\n                           )\\n        cblas_libs = ['cblas']\\n        blas_info.pop('libraries', None)\\n    else:\\n        cblas_libs = blas_info.pop('libraries', [])\\n    minilearn_sources = [\\n        join('src', 'minilearn', 'lars.c'),\\n        join('src', 'minilearn', '_minilearn.c')]\\n    config.add_extension('_minilearn',\\n                         sources = minilearn_sources,\\n                         libraries = cblas_libs,\\n                         include_dirs=[join('src', 'minilearn'),\\n                                       join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )\\n    config.add_extension('ball_tree',\\n                         sources=[join('src', 'BallTree.cpp')],\\n                         include_dirs=[numpy.get_include()]\\n                         )\\n    config.add_extension('cd_fast',\\n                         sources=[join('src', 'cd_fast.c')],\\n                         libraries=cblas_libs,\\n                         include_dirs=[join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )"
  },
  {
    "code": "def _convert_listlike_datetimes(arg, box, format, name=None, tz=None,\\n                                unit=None, errors=None,\\n                                infer_datetime_format=None, dayfirst=None,\\n                                yearfirst=None, exact=None):\\n    from pandas import DatetimeIndex\\n    if isinstance(arg, (list, tuple)):\\n        arg = np.array(arg, dtype='O')\\n    if is_datetime64tz_dtype(arg):\\n        if not isinstance(arg, DatetimeIndex):\\n            return DatetimeIndex(arg, tz=tz, name=name)\\n        if tz == 'utc':\\n            arg = arg.tz_convert(None).tz_localize(tz)\\n        return arg\\n    elif is_datetime64_ns_dtype(arg):\\n        if box and not isinstance(arg, DatetimeIndex):\\n            try:\\n                return DatetimeIndex(arg, tz=tz, name=name)\\n            except ValueError:\\n                pass\\n        return arg\\n    elif unit is not None:\\n        if format is not None:\\n            raise ValueError(\"cannot specify both format and unit\")\\n        arg = getattr(arg, 'values', arg)\\n        result = tslib.array_with_unit_to_datetime(arg, unit,\\n                                                   errors=errors)\\n        if box:\\n            if errors == 'ignore':\\n                from pandas import Index\\n                return Index(result)\\n            return DatetimeIndex(result, tz=tz, name=name)\\n        return result\\n    elif getattr(arg, 'ndim', 1) > 1:\\n        raise TypeError('arg must be a string, datetime, list, tuple, '\\n                        '1-d array, or Series')\\n    arg = ensure_object(arg)\\n    require_iso8601 = False\\n    if infer_datetime_format and format is None:\\n        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)\\n    if format is not None:\\n        format_is_iso8601 = _format_is_iso(format)\\n        if format_is_iso8601:\\n            require_iso8601 = not infer_datetime_format\\n            format = None\\n    try:\\n        result = None\\n        if format is not None:\\n            if format == '%Y%m%d':\\n                try:\\n                    result = _attempt_YYYYMMDD(arg, errors=errors)\\n                except:\\n                    raise ValueError(\"cannot convert the input to \"\\n                                     \"'%Y%m%d' date format\")\\n            if result is None:\\n                try:\\n                    result, timezones = array_strptime(\\n                        arg, format, exact=exact, errors=errors)\\n                    if '%Z' in format or '%z' in format:\\n                        return _return_parsed_timezone_results(\\n                            result, timezones, box, tz)\\n                except tslibs.OutOfBoundsDatetime:\\n                    if errors == 'raise':\\n                        raise\\n                    result = arg\\n                except ValueError:\\n                    if not infer_datetime_format:\\n                        if errors == 'raise':\\n                            raise\\n                        result = arg\\n        if result is None and (format is None or infer_datetime_format):\\n            result, tz_parsed = tslib.array_to_datetime(\\n                arg,\\n                errors=errors,\\n                utc=tz == 'utc',\\n                dayfirst=dayfirst,\\n                yearfirst=yearfirst,\\n                require_iso8601=require_iso8601\\n            )\\n            if tz_parsed is not None and box:\\n                return DatetimeIndex._simple_new(result, name=name,\\n                                                 tz=tz_parsed)\\n        if box:\\n            if is_datetime64_dtype(result):\\n                return DatetimeIndex(result, tz=tz, name=name)\\n            elif is_object_dtype(result):\\n                from pandas import Index\\n                return Index(result, name=name)\\n        return result\\n    except ValueError as e:\\n        try:\\n            values, tz = conversion.datetime_to_datetime64(arg)\\n            return DatetimeIndex._simple_new(values, name=name, tz=tz)\\n        except (ValueError, TypeError):\\n            raise e",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Retain timezone information in to_datetime if box=False (#22457)",
    "fixed_code": "def _convert_listlike_datetimes(arg, box, format, name=None, tz=None,\\n                                unit=None, errors=None,\\n                                infer_datetime_format=None, dayfirst=None,\\n                                yearfirst=None, exact=None):\\n    from pandas import DatetimeIndex\\n    if isinstance(arg, (list, tuple)):\\n        arg = np.array(arg, dtype='O')\\n    if is_datetime64tz_dtype(arg):\\n        if not isinstance(arg, DatetimeIndex):\\n            return DatetimeIndex(arg, tz=tz, name=name)\\n        if tz == 'utc':\\n            arg = arg.tz_convert(None).tz_localize(tz)\\n        return arg\\n    elif is_datetime64_ns_dtype(arg):\\n        if box and not isinstance(arg, DatetimeIndex):\\n            try:\\n                return DatetimeIndex(arg, tz=tz, name=name)\\n            except ValueError:\\n                pass\\n        return arg\\n    elif unit is not None:\\n        if format is not None:\\n            raise ValueError(\"cannot specify both format and unit\")\\n        arg = getattr(arg, 'values', arg)\\n        result = tslib.array_with_unit_to_datetime(arg, unit,\\n                                                   errors=errors)\\n        if box:\\n            if errors == 'ignore':\\n                from pandas import Index\\n                return Index(result)\\n            return DatetimeIndex(result, tz=tz, name=name)\\n        return result\\n    elif getattr(arg, 'ndim', 1) > 1:\\n        raise TypeError('arg must be a string, datetime, list, tuple, '\\n                        '1-d array, or Series')\\n    arg = ensure_object(arg)\\n    require_iso8601 = False\\n    if infer_datetime_format and format is None:\\n        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)\\n    if format is not None:\\n        format_is_iso8601 = _format_is_iso(format)\\n        if format_is_iso8601:\\n            require_iso8601 = not infer_datetime_format\\n            format = None\\n    try:\\n        result = None\\n        if format is not None:\\n            if format == '%Y%m%d':\\n                try:\\n                    result = _attempt_YYYYMMDD(arg, errors=errors)\\n                except:\\n                    raise ValueError(\"cannot convert the input to \"\\n                                     \"'%Y%m%d' date format\")\\n            if result is None:\\n                try:\\n                    result, timezones = array_strptime(\\n                        arg, format, exact=exact, errors=errors)\\n                    if '%Z' in format or '%z' in format:\\n                        return _return_parsed_timezone_results(\\n                            result, timezones, box, tz)\\n                except tslibs.OutOfBoundsDatetime:\\n                    if errors == 'raise':\\n                        raise\\n                    result = arg\\n                except ValueError:\\n                    if not infer_datetime_format:\\n                        if errors == 'raise':\\n                            raise\\n                        result = arg\\n        if result is None and (format is None or infer_datetime_format):\\n            result, tz_parsed = tslib.array_to_datetime(\\n                arg,\\n                errors=errors,\\n                utc=tz == 'utc',\\n                dayfirst=dayfirst,\\n                yearfirst=yearfirst,\\n                require_iso8601=require_iso8601\\n            )\\n            if tz_parsed is not None:\\n                if box:\\n                    return DatetimeIndex._simple_new(result, name=name,\\n                                                     tz=tz_parsed)\\n                else:\\n                    result = [Timestamp(ts, tz=tz_parsed).to_pydatetime()\\n                              for ts in result]\\n                    return np.array(result, dtype=object)\\n        if box:\\n            if is_datetime64_dtype(result):\\n                return DatetimeIndex(result, tz=tz, name=name)\\n            elif is_object_dtype(result):\\n                from pandas import Index\\n                return Index(result, name=name)\\n        return result\\n    except ValueError as e:\\n        try:\\n            values, tz = conversion.datetime_to_datetime64(arg)\\n            return DatetimeIndex._simple_new(values, name=name, tz=tz)\\n        except (ValueError, TypeError):\\n            raise e"
  },
  {
    "code": "def add_group_member(fmg, paramgram):\\n    response = (-100000, {\"msg\": \"Illegal or malformed paramgram discovered. System Exception\"})\\n    url = \"\"\\n    device_member_list = paramgram[\"grp_members\"].replace(' ', '')\\n    device_member_list = device_member_list.split(',')\\n    for dev_name in device_member_list:\\n        datagram = {'name': dev_name, 'vdom': paramgram[\"vdom\"]}\\n        url = '/dvmdb/adom/{adom}/group/{grp_name}/object member'.format(adom=paramgram[\"adom\"],\\n                                                                         grp_name=paramgram[\"grp_name\"])\\n        response = fmg.add(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Auto Commit for: fmgr_device_group (#52784)",
    "fixed_code": "def add_group_member(fmgr, paramgram):\\n    response = DEFAULT_RESULT_OBJ\\n    url = \"\"\\n    device_member_list = paramgram[\"grp_members\"].replace(' ', '')\\n    device_member_list = device_member_list.split(',')\\n    for dev_name in device_member_list:\\n        datagram = {'name': dev_name, 'vdom': paramgram[\"vdom\"]}\\n        url = '/dvmdb/adom/{adom}/group/{grp_name}/object member'.format(adom=paramgram[\"adom\"],\\n                                                                         grp_name=paramgram[\"grp_name\"])\\n        response = fmgr.process_request(url, datagram, FMGRMethods.ADD)\\n    return response"
  },
  {
    "code": "def _write_wide_table(self, group, panel, append=False, comp=None, **kwargs):\\n        t = create_table(self, group, typ = 'appendable_panel')\\n        t.write(axes_to_index=[1,2], obj=panel,\\n                append=append, compression=comp, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: ndim tables in HDFStore      changes axes_to_index keyword in table creation to axes      allow passing of numeric or named axes (e.g. 0 or 'minor_axis') in axes      create_axes now checks for current table scheme; raises if this indexing scheme is violated      added many p4d tests for appending/selection/partial selection/and axis permuation      added addition Term tests to include p4d      add __eq__ operators to IndexCol/DataCol/Table to comparisons      updated docs with Panel4D saving & issues relating to threading      supporting non-regular indexables:        e.g. can index a Panel4D on say [labels,major_axis,minor_axis], rather             than the default of [items,major_axis,minor_axis]      support column oriented DataFrames (e.g. queryable by the columns)",
    "fixed_code": "def _write_wide_table(self, group, panel, append=False, comp=None, axes=None, **kwargs):\\n        if axes is None:\\n            axes = [1,2]\\n        t = create_table(self, group, typ = 'appendable_panel')\\n        t.write(axes=axes, obj=panel,\\n                append=append, compression=comp, **kwargs)\\n    _read_wide_table = _read_ndim_table"
  },
  {
    "code": "def get_value(arg, config, module):\\n    command = PARAM_TO_COMMAND_KEYMAP[arg]\\n    has_command = re.search(r'\\s+{0}\\s*$'.format(command), config, re.M)\\n    has_command_val = re.search(r'(?:{0}\\s)(?P<value>.*)$'.format(command), config, re.M)\\n    if command == 'ip router ospf':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'ospf':\\n                value = value_list[0]\\n            elif arg == 'area':\\n                value = value_list[2]\\n                value = normalize_area(value, module)\\n    elif command == 'ip ospf message-digest-key':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'message_digest_key_id':\\n                value = value_list[0]\\n            elif arg == 'message_digest_algorithm_type':\\n                value = value_list[1]\\n            elif arg == 'message_digest_encryption_type':\\n                value = value_list[2]\\n                if value == '3':\\n                    value = '3des'\\n                elif value == '7':\\n                    value = 'cisco_type_7'\\n            elif arg == 'message_digest_password':\\n                value = value_list[3]\\n    elif arg == 'passive_interface':\\n        has_no_command = re.search(r'\\s+no\\s+{0}\\s*$'.format(command), config, re.M)\\n        value = False\\n        if has_command and not has_no_command:\\n            value = True\\n    elif arg in BOOL_PARAMS:\\n        value = bool(has_command)\\n    else:\\n        value = ''\\n        if has_command_val:\\n            value = has_command_val.group('value')\\n    return value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_interfaces_ospf: fix passive-interface states & check_mode (#54260)\\n\\n\\nThis fix addresses issues #41704 and #45343.\\n\\nThe crux of the problem is that `passive-interface` should have been treated as a tri-state value instead of a boolean.\\n\\nThe `no` form of the command disables the passive state on an interface (allows it to form adjacencies and send routing updates).  It's essentially an override for `passive-interface default` which enables passive state on all OSPF interfaces.\\*\\nThis `no` config will be present in `running-config`.\\n\\n   \\**See `router ospf` configuration.*\\n\\nSince both enable and disable states are explicit configs, the proper way to remove either of these is with the `default` syntax.\\n\\nPassive-interface config syntax:\\n```\\n  ip ospf passive-interface              # enable  (nvgens)\\n  no ip ospf passive-interface           # disable (nvgens)\\n  default ip ospf passive-interface      # default (removes config, does not nvgen)\\n```\\n\\nCode changes:\\n\\n\\n\\n\\nSanity verified on: N9K,N7K,N3K,N6K",
    "fixed_code": "def get_value(arg, config, module):\\n    command = PARAM_TO_COMMAND_KEYMAP[arg]\\n    has_command = re.search(r'\\s+{0}\\s*$'.format(command), config, re.M)\\n    has_command_val = re.search(r'(?:{0}\\s)(?P<value>.*)$'.format(command), config, re.M)\\n    if command == 'ip router ospf':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'ospf':\\n                value = value_list[0]\\n            elif arg == 'area':\\n                value = value_list[2]\\n                value = normalize_area(value, module)\\n    elif command == 'ip ospf message-digest-key':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'message_digest_key_id':\\n                value = value_list[0]\\n            elif arg == 'message_digest_algorithm_type':\\n                value = value_list[1]\\n            elif arg == 'message_digest_encryption_type':\\n                value = value_list[2]\\n                if value == '3':\\n                    value = '3des'\\n                elif value == '7':\\n                    value = 'cisco_type_7'\\n            elif arg == 'message_digest_password':\\n                value = value_list[3]\\n    elif arg == 'passive_interface':\\n        has_no_command = re.search(r'\\s+no\\s+{0}\\s*$'.format(command), config, re.M)\\n        if has_no_command:\\n            value = False\\n        elif has_command:\\n            value = True\\n        else:\\n            value = None\\n    elif arg in BOOL_PARAMS:\\n        value = bool(has_command)\\n    else:\\n        value = ''\\n        if has_command_val:\\n            value = has_command_val.group('value')\\n    return value"
  },
  {
    "code": "def __invert__(self):\\n        members, uncovered = _decompose(self.__class__, self._value_)\\n        inverted_members = [\\n                m for m in self.__class__\\n                if m not in members and not m._value_ & self._value_\\n                ]\\n        inverted = reduce(_or_, inverted_members, self.__class__(0))\\n        return self.__class__(inverted)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __invert__(self):\\n        members, uncovered = _decompose(self.__class__, self._value_)\\n        inverted_members = [\\n                m for m in self.__class__\\n                if m not in members and not m._value_ & self._value_\\n                ]\\n        inverted = reduce(_or_, inverted_members, self.__class__(0))\\n        return self.__class__(inverted)"
  },
  {
    "code": "def time_count(self, dtype):\\n        self.s.str.count(\"A\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_count(self, dtype):\\n        self.s.str.count(\"A\")"
  },
  {
    "code": "def _maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = tslib.iNaT\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, (np.datetime64, np.timedelta64)):\\n        if isnull(fill_value):\\n            fill_value = tslib.iNaT\\n        else:\\n            if issubclass(dtype.type, np.datetime64):\\n                try:\\n                    fill_value = lib.Timestamp(fill_value).value\\n                except:\\n                    fill_value = tslib.iNaT\\n            else:\\n                fill_value = tslib.iNaT\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    else:\\n        dtype = np.object_\\n    if is_categorical_dtype(dtype):\\n        dtype = dtype\\n    elif issubclass(np.dtype(dtype).type, compat.string_types):\\n        dtype = np.object_\\n    return dtype, fill_value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = tslib.iNaT\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, (np.datetime64, np.timedelta64)):\\n        if isnull(fill_value):\\n            fill_value = tslib.iNaT\\n        else:\\n            if issubclass(dtype.type, np.datetime64):\\n                try:\\n                    fill_value = lib.Timestamp(fill_value).value\\n                except:\\n                    fill_value = tslib.iNaT\\n            else:\\n                fill_value = tslib.iNaT\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    else:\\n        dtype = np.object_\\n    if is_categorical_dtype(dtype):\\n        dtype = dtype\\n    elif issubclass(np.dtype(dtype).type, compat.string_types):\\n        dtype = np.object_\\n    return dtype, fill_value"
  },
  {
    "code": "def rset(self):\\n        return self.docmd(\"rset\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#22027: Add RFC6531 support to smtplib.\\n\\nInitial patch by Milan Oberkirch.",
    "fixed_code": "def rset(self):\\n        self.command_encoding = 'ascii'\\n        return self.docmd(\"rset\")"
  },
  {
    "code": "def _get_multiindex_indexer(join_keys, index, sort):\\n    from functools import partial\\n    fkeys = partial(_factorize_keys, sort=sort)\\n    rlab, llab, shape = map(list, zip( * map(fkeys, index.levels, join_keys)))\\n    if sort:\\n        rlab = list(map(np.take, rlab, index.labels))\\n    else:\\n        i8copy = lambda a: a.astype('i8', subok=False, copy=True)\\n        rlab = list(map(i8copy, index.labels))\\n    for i in range(len(join_keys)):\\n        mask = index.labels[i] == -1\\n        if mask.any():\\n            a = join_keys[i][llab[i] == shape[i] - 1]\\n            if a.size == 0 or not a[0] != a[0]:\\n                shape[i] += 1\\n            rlab[i][mask] = shape[i] - 1\\n    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\\n    lkey, rkey, count = fkeys(lkey, rkey)\\n    return algos.left_outer_join(lkey, rkey, count, sort=sort)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_multiindex_indexer(join_keys, index, sort):\\n    from functools import partial\\n    fkeys = partial(_factorize_keys, sort=sort)\\n    rlab, llab, shape = map(list, zip( * map(fkeys, index.levels, join_keys)))\\n    if sort:\\n        rlab = list(map(np.take, rlab, index.labels))\\n    else:\\n        i8copy = lambda a: a.astype('i8', subok=False, copy=True)\\n        rlab = list(map(i8copy, index.labels))\\n    for i in range(len(join_keys)):\\n        mask = index.labels[i] == -1\\n        if mask.any():\\n            a = join_keys[i][llab[i] == shape[i] - 1]\\n            if a.size == 0 or not a[0] != a[0]:\\n                shape[i] += 1\\n            rlab[i][mask] = shape[i] - 1\\n    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\\n    lkey, rkey, count = fkeys(lkey, rkey)\\n    return algos.left_outer_join(lkey, rkey, count, sort=sort)"
  },
  {
    "code": "def arith_method_FRAME(cls: Type[\"DataFrame\"], op, special: bool):\\n    op_name = _get_op_name(op, special)\\n    default_axis = None if special else \"columns\"\\n    na_op = get_array_op(op)\\n    if op_name in _op_descriptions:\\n        doc = _make_flex_doc(op_name, \"dataframe\")\\n    else:\\n        doc = _arith_doc_FRAME % op_name\\n    @Appender(doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def arith_method_FRAME(cls: Type[\"DataFrame\"], op, special: bool):\\n    op_name = _get_op_name(op, special)\\n    default_axis = None if special else \"columns\"\\n    na_op = get_array_op(op)\\n    if op_name in _op_descriptions:\\n        doc = _make_flex_doc(op_name, \"dataframe\")\\n    else:\\n        doc = _arith_doc_FRAME % op_name\\n    @Appender(doc)"
  },
  {
    "code": "def compile(self, format):\\n        return re_compile(self.pattern(format), IGNORECASE | ASCII)\\n_cache_lock = _thread_allocate_lock()\\n_TimeRE_cache = TimeRE()\\n_CACHE_MAX_SIZE = 5 \\n_regex_cache = {}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make sure time.strptime only accepts strings (and document the fact like strftime). Already didn't accept bytes but make the check earlier. This also lifts the limitation of requiring ASCII.\\n\\nCloses issue #5236. Thanks Tennessee Leeuwenburg.",
    "fixed_code": "def compile(self, format):\\n        return re_compile(self.pattern(format), IGNORECASE)\\n_cache_lock = _thread_allocate_lock()\\n_TimeRE_cache = TimeRE()\\n_CACHE_MAX_SIZE = 5 \\n_regex_cache = {}"
  },
  {
    "code": "def call(\\n        self,\\n        path: str,\\n        query: Optional[dict] = None,\\n        get_all_pages: bool = True,\\n        side_loading: bool = False,\\n    ) -> dict:\\n        query_params = query or {}\\n        zendesk = self.get_conn()\\n        first_request_successful = False\\n        while not first_request_successful:\\n            try:\\n                results = zendesk.call(path, query_params)\\n                first_request_successful = True\\n            except RateLimitError as rle:\\n                self.__handle_rate_limit_exception(rle)\\n        keys = [path.split(\"/\")[-1].split(\".json\")[0]]\\n        next_page = results['next_page']\\n        if side_loading:\\n            keys += query_params['include'].split(',')\\n        results = {key: results[key] for key in keys}\\n        if get_all_pages:\\n            while next_page is not None:\\n                try:\\n                    next_url = next_page.split(self.__url)[1]\\n                    self.log.info(\"Calling %s\", next_url)\\n                    more_res = zendesk.call(next_url)\\n                    for key in results:\\n                        results[key].extend(more_res[key])\\n                    if next_page == more_res['next_page']:\\n                        break\\n                    next_page = more_res['next_page']\\n                except RateLimitError as rle:\\n                    self.__handle_rate_limit_exception(rle)\\n                except ZendeskError as zde:\\n                    if b\"Use a start_time older than 5 minutes\" in zde.msg:\\n                        break\\n                    raise zde\\n        return results",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def call(\\n        self,\\n        path: str,\\n        query: Optional[dict] = None,\\n        get_all_pages: bool = True,\\n        side_loading: bool = False,\\n    ) -> dict:\\n        query_params = query or {}\\n        zendesk = self.get_conn()\\n        first_request_successful = False\\n        while not first_request_successful:\\n            try:\\n                results = zendesk.call(path, query_params)\\n                first_request_successful = True\\n            except RateLimitError as rle:\\n                self.__handle_rate_limit_exception(rle)\\n        keys = [path.split(\"/\")[-1].split(\".json\")[0]]\\n        next_page = results['next_page']\\n        if side_loading:\\n            keys += query_params['include'].split(',')\\n        results = {key: results[key] for key in keys}\\n        if get_all_pages:\\n            while next_page is not None:\\n                try:\\n                    next_url = next_page.split(self.__url)[1]\\n                    self.log.info(\"Calling %s\", next_url)\\n                    more_res = zendesk.call(next_url)\\n                    for key in results:\\n                        results[key].extend(more_res[key])\\n                    if next_page == more_res['next_page']:\\n                        break\\n                    next_page = more_res['next_page']\\n                except RateLimitError as rle:\\n                    self.__handle_rate_limit_exception(rle)\\n                except ZendeskError as zde:\\n                    if b\"Use a start_time older than 5 minutes\" in zde.msg:\\n                        break\\n                    raise zde\\n        return results"
  },
  {
    "code": "def _create_fn(name, args, body, *, globals=None, locals=None,\\n               return_type=MISSING):\\n    if locals is None:\\n        locals = {}\\n    if globals is not None and '__builtins__' not in globals:\\n        globals['__builtins__'] = builtins\\n    return_annotation = ''\\n    if return_type is not MISSING:\\n        locals['_return_type'] = return_type\\n        return_annotation = '->_return_type'\\n    args = ','.join(args)\\n    body = '\\n'.join(f' {b}' for b in body)\\n    txt = f'def {name}({args}){return_annotation}:\\n{body}'\\n    exec(txt, globals, locals)\\n    return locals[name]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34776: Fix dataclasses to support __future__ \"annotations\" mode (#9518)",
    "fixed_code": "def _create_fn(name, args, body, *, globals=None, locals=None,\\n               return_type=MISSING):\\n    if locals is None:\\n        locals = {}\\n    if 'BUILTINS' not in locals:\\n        locals['BUILTINS'] = builtins\\n    return_annotation = ''\\n    if return_type is not MISSING:\\n        locals['_return_type'] = return_type\\n        return_annotation = '->_return_type'\\n    args = ','.join(args)\\n    body = '\\n'.join(f'  {b}' for b in body)\\n    txt = f' def {name}({args}){return_annotation}:\\n{body}'\\n    local_vars = ', '.join(locals.keys())\\n    txt = f\"def __create_fn__({local_vars}):\\n{txt}\\n return {name}\"\\n    ns = {}\\n    exec(txt, globals, ns)\\n    return ns['__create_fn__'](**locals)"
  },
  {
    "code": "def finalize_other (self):          \\n        if self.prefix is None:\\n            self.prefix = os.path.normpath(sys.prefix)\\n        self.install_base = self.install_platbase = self.prefix\\n        try:\\n            self.select_scheme(os.name)\\n        except KeyError:\\n            raise DistutilsPlatformError, \\\\n                  \"I don't know how to install stuff on '%s'\" % os.name",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make distutils \"install --home\" support all platforms.",
    "fixed_code": "def finalize_other (self):          \\n        if self.home is not None:\\n            self.install_base = self.install_platbase = self.home\\n            self.select_scheme(\"unix_home\")\\n        else:\\n            if self.prefix is None:\\n                self.prefix = os.path.normpath(sys.prefix)\\n            self.install_base = self.install_platbase = self.prefix\\n            try:\\n                self.select_scheme(os.name)\\n            except KeyError:\\n                raise DistutilsPlatformError, \\\\n                      \"I don't know how to install stuff on '%s'\" % os.name"
  },
  {
    "code": "def opt_jinja_env_helper(opts, optname):\\n\\t\\tfor k, v in opts.items():\\n\\t\\t\\tk = k.lower()\\n\\t\\t\\tif hasattr(jinja2.defaults, k.upper()):\\n\\t\\t\\t\\tlog.debug(\"Jinja2 environment %s was set to %s by %s\", k, v, optname)\\n\\t\\t\\t\\tenv_args[k] = v\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlog.warning(\"Jinja2 environment %s is not recognized\", k)\\n\\tif \"sls\" in context and context[\"sls\"] != \"\":\\n\\t\\topt_jinja_env_helper(opt_jinja_sls_env, \"jinja_sls_env\")\\n\\telse:\\n\\t\\topt_jinja_env_helper(opt_jinja_env, \"jinja_env\")\\n\\tif opts.get(\"allow_undefined\", False):\\n\\t\\tjinja_env = jinja2.Environment(**env_args)\\n\\telse:\\n\\t\\tjinja_env = jinja2.Environment(undefined=jinja2.StrictUndefined, **env_args)\\n\\ttojson_filter = jinja_env.filters.get(\"tojson\")\\n\\tindent_filter = jinja_env.filters.get(\"indent\")\\n\\tjinja_env.tests.update(JinjaTest.salt_jinja_tests)\\n\\tjinja_env.filters.update(JinjaFilter.salt_jinja_filters)\\n\\tif tojson_filter is not None:\\n\\t\\tjinja_env.filters[\"tojson\"] = tojson_filter\\n\\tif salt.utils.jinja.JINJA_VERSION >= LooseVersion(\"2.11\"):\\n\\t\\tjinja_env.filters[\"indent\"] = indent_filter\\n\\tjinja_env.globals.update(JinjaGlobal.salt_jinja_globals)\\n\\tjinja_env.globals[\"odict\"] = OrderedDict\\n\\tjinja_env.globals[\"show_full_context\"] = salt.utils.jinja.show_full_context\\n\\tjinja_env.tests[\"list\"] = salt.utils.data.is_list\\n\\tdecoded_context = {}\\n\\tfor key, value in context.items():\\n\\t\\tif not isinstance(value, str):\\n\\t\\t\\tdecoded_context[key] = value\\n\\t\\t\\tcontinue\\n\\t\\ttry:\\n\\t\\t\\tdecoded_context[key] = salt.utils.stringutils.to_unicode(\\n\\t\\t\\t\\tvalue, encoding=SLS_ENCODING\\n\\t\\t\\t)\\n\\t\\texcept UnicodeDecodeError as ex:\\n\\t\\t\\tlog.debug(\\n\\t\\t\\t\\t\"Failed to decode using default encoding (%s), trying system encoding\",\\n\\t\\t\\t\\tSLS_ENCODING,\\n\\t\\t\\t)\\n\\t\\t\\tdecoded_context[key] = salt.utils.data.decode(value)\\n\\ttry:\\n\\t\\ttemplate = jinja_env.from_string(tmplstr)\\n\\t\\ttemplate.globals.update(decoded_context)\\n\\t\\toutput = template.render(**decoded_context)\\n\\texcept jinja2.exceptions.UndefinedError as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tout = _get_jinja_error(trace, context=decoded_context)[1]\\n\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\"Jinja variable {}{}\".format(exc, out), buf=tmplstr)\\n\\texcept (\\n\\t\\tjinja2.exceptions.TemplateRuntimeError,\\n\\t\\tjinja2.exceptions.TemplateSyntaxError,\\n\\t) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja syntax error: {}{}\".format(exc, out), line, tmplstr\\n\\t\\t)\\n\\texcept (SaltInvocationError, CommandExecutionError) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Problem running salt function in Jinja template: {}{}\".format(exc, out),\\n\\t\\t\\tline,\\n\\t\\t\\ttmplstr,\\n\\t\\t)\\n\\texcept Exception as exc:  \\n\\t\\ttracestr = traceback.format_exc()\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\telse:\\n\\t\\t\\ttmplstr += \"\\n{}\".format(tracestr)\\n\\t\\tlog.debug(\"Jinja Error\")\\n\\t\\tlog.debug(\"Exception:\", exc_info=True)\\n\\t\\tlog.debug(\"Out: %s\", out)\\n\\t\\tlog.debug(\"Line: %s\", line)\\n\\t\\tlog.debug(\"TmplStr: %s\", tmplstr)\\n\\t\\tlog.debug(\"TraceStr: %s\", tracestr)\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja error: {}{}\".format(exc, out), line, tmplstr, trace=tracestr\\n\\t\\t)\\n\\tif newline:\\n\\t\\toutput += newline\\n\\treturn output",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Tests and fix for CVE-2021-25283",
    "fixed_code": "def opt_jinja_env_helper(opts, optname):\\n\\t\\tfor k, v in opts.items():\\n\\t\\t\\tk = k.lower()\\n\\t\\t\\tif hasattr(jinja2.defaults, k.upper()):\\n\\t\\t\\t\\tlog.debug(\"Jinja2 environment %s was set to %s by %s\", k, v, optname)\\n\\t\\t\\t\\tenv_args[k] = v\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlog.warning(\"Jinja2 environment %s is not recognized\", k)\\n\\tif \"sls\" in context and context[\"sls\"] != \"\":\\n\\t\\topt_jinja_env_helper(opt_jinja_sls_env, \"jinja_sls_env\")\\n\\telse:\\n\\t\\topt_jinja_env_helper(opt_jinja_env, \"jinja_env\")\\n\\tif opts.get(\"allow_undefined\", False):\\n\\t\\tjinja_env = jinja2.sandbox.SandboxedEnvironment(**env_args)\\n\\telse:\\n\\t\\tjinja_env = jinja2.sandbox.SandboxedEnvironment(\\n\\t\\t\\tundefined=jinja2.StrictUndefined, **env_args\\n\\t\\t)\\n\\ttojson_filter = jinja_env.filters.get(\"tojson\")\\n\\tindent_filter = jinja_env.filters.get(\"indent\")\\n\\tjinja_env.tests.update(JinjaTest.salt_jinja_tests)\\n\\tjinja_env.filters.update(JinjaFilter.salt_jinja_filters)\\n\\tif tojson_filter is not None:\\n\\t\\tjinja_env.filters[\"tojson\"] = tojson_filter\\n\\tif salt.utils.jinja.JINJA_VERSION >= LooseVersion(\"2.11\"):\\n\\t\\tjinja_env.filters[\"indent\"] = indent_filter\\n\\tjinja_env.globals.update(JinjaGlobal.salt_jinja_globals)\\n\\tjinja_env.globals[\"odict\"] = OrderedDict\\n\\tjinja_env.globals[\"show_full_context\"] = salt.utils.jinja.show_full_context\\n\\tjinja_env.tests[\"list\"] = salt.utils.data.is_list\\n\\tdecoded_context = {}\\n\\tfor key, value in context.items():\\n\\t\\tif not isinstance(value, str):\\n\\t\\t\\tdecoded_context[key] = value\\n\\t\\t\\tcontinue\\n\\t\\ttry:\\n\\t\\t\\tdecoded_context[key] = salt.utils.stringutils.to_unicode(\\n\\t\\t\\t\\tvalue, encoding=SLS_ENCODING\\n\\t\\t\\t)\\n\\t\\texcept UnicodeDecodeError as ex:\\n\\t\\t\\tlog.debug(\\n\\t\\t\\t\\t\"Failed to decode using default encoding (%s), trying system encoding\",\\n\\t\\t\\t\\tSLS_ENCODING,\\n\\t\\t\\t)\\n\\t\\t\\tdecoded_context[key] = salt.utils.data.decode(value)\\n\\ttry:\\n\\t\\ttemplate = jinja_env.from_string(tmplstr)\\n\\t\\ttemplate.globals.update(decoded_context)\\n\\t\\toutput = template.render(**decoded_context)\\n\\texcept jinja2.exceptions.UndefinedError as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tout = _get_jinja_error(trace, context=decoded_context)[1]\\n\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\"Jinja variable {}{}\".format(exc, out), buf=tmplstr)\\n\\texcept (\\n\\t\\tjinja2.exceptions.TemplateRuntimeError,\\n\\t\\tjinja2.exceptions.TemplateSyntaxError,\\n\\t\\tjinja2.exceptions.SecurityError,\\n\\t) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja syntax error: {}{}\".format(exc, out), line, tmplstr\\n\\t\\t)\\n\\texcept (SaltInvocationError, CommandExecutionError) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Problem running salt function in Jinja template: {}{}\".format(exc, out),\\n\\t\\t\\tline,\\n\\t\\t\\ttmplstr,\\n\\t\\t)\\n\\texcept Exception as exc:  \\n\\t\\ttracestr = traceback.format_exc()\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\telse:\\n\\t\\t\\ttmplstr += \"\\n{}\".format(tracestr)\\n\\t\\tlog.debug(\"Jinja Error\")\\n\\t\\tlog.debug(\"Exception:\", exc_info=True)\\n\\t\\tlog.debug(\"Out: %s\", out)\\n\\t\\tlog.debug(\"Line: %s\", line)\\n\\t\\tlog.debug(\"TmplStr: %s\", tmplstr)\\n\\t\\tlog.debug(\"TraceStr: %s\", tracestr)\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja error: {}{}\".format(exc, out), line, tmplstr, trace=tracestr\\n\\t\\t)\\n\\tif newline:\\n\\t\\toutput += newline\\n\\treturn output"
  },
  {
    "code": "def __init__(self, loader=DataLoader):\\n        self._role_path = None\\n        self._parents   = []\\n        super(Role, self).__init__(loader=loader)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, loader=DataLoader):\\n        self._role_path = None\\n        self._parents   = []\\n        super(Role, self).__init__(loader=loader)"
  },
  {
    "code": "def radviz(frame, class_column, ax=None, color=None, colormap=None, **kwds):\\n    import matplotlib.pyplot as plt\\n    import matplotlib.patches as patches",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def radviz(frame, class_column, ax=None, color=None, colormap=None, **kwds):\\n    import matplotlib.pyplot as plt\\n    import matplotlib.patches as patches"
  },
  {
    "code": "def compute_output_shape(self, input_shape):\\n\\t\\tif isinstance(input_shape, list):\\n\\t\\t\\tinput_shape = input_shape[0]\\n\\t\\tif hasattr(self.cell.state_size, '__len__'):\\n\\t\\t\\tstate_size = self.cell.state_size\\n\\t\\telse:\\n\\t\\t\\tstate_size = [self.cell.state_size]\\n\\t\\toutput_dim = state_size[0]\\n\\t\\tif self.return_sequences:\\n\\t\\t\\toutput_shape = (input_shape[0], input_shape[1], output_dim)\\n\\t\\telse:\\n\\t\\t\\toutput_shape = (input_shape[0], output_dim)\\n\\t\\tif self.return_state:\\n\\t\\t\\tstate_shape = [(input_shape[0], dim) for dim in state_size]\\n\\t\\t\\treturn [output_shape] + state_shape\\n\\t\\telse:\\n\\t\\t\\treturn output_shape",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def compute_output_shape(self, input_shape):\\n\\t\\tif isinstance(input_shape, list):\\n\\t\\t\\tinput_shape = input_shape[0]\\n\\t\\tif hasattr(self.cell.state_size, '__len__'):\\n\\t\\t\\tstate_size = self.cell.state_size\\n\\t\\telse:\\n\\t\\t\\tstate_size = [self.cell.state_size]\\n\\t\\toutput_dim = state_size[0]\\n\\t\\tif self.return_sequences:\\n\\t\\t\\toutput_shape = (input_shape[0], input_shape[1], output_dim)\\n\\t\\telse:\\n\\t\\t\\toutput_shape = (input_shape[0], output_dim)\\n\\t\\tif self.return_state:\\n\\t\\t\\tstate_shape = [(input_shape[0], dim) for dim in state_size]\\n\\t\\t\\treturn [output_shape] + state_shape\\n\\t\\telse:\\n\\t\\t\\treturn output_shape"
  },
  {
    "code": "def _term_count_dicts_to_matrix(self, term_count_dicts):\\n        i_indices = []\\n        j_indices = []\\n        values = []\\n        if self.fixed_vocabulary is not None:\\n            vocabulary = self.fixed_vocabulary\\n        else:\\n            vocabulary = self.vocabulary_\\n        for i, term_count_dict in enumerate(term_count_dicts):\\n            for term, count in term_count_dict.iteritems():\\n                j = vocabulary.get(term)\\n                if j is not None:\\n                    i_indices.append(i)\\n                    j_indices.append(j)\\n                    values.append(count)\\n            term_count_dict.clear()\\n        shape = (len(term_count_dicts), max(vocabulary.itervalues()) + 1)\\n        spmatrix = sp.coo_matrix((values, (i_indices, j_indices)),\\n                                 shape=shape, dtype=self.dtype)\\n        if self.binary:\\n            spmatrix.data[:] = 1\\n        return spmatrix",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _term_count_dicts_to_matrix(self, term_count_dicts):\\n        i_indices = []\\n        j_indices = []\\n        values = []\\n        if self.fixed_vocabulary is not None:\\n            vocabulary = self.fixed_vocabulary\\n        else:\\n            vocabulary = self.vocabulary_\\n        for i, term_count_dict in enumerate(term_count_dicts):\\n            for term, count in term_count_dict.iteritems():\\n                j = vocabulary.get(term)\\n                if j is not None:\\n                    i_indices.append(i)\\n                    j_indices.append(j)\\n                    values.append(count)\\n            term_count_dict.clear()\\n        shape = (len(term_count_dicts), max(vocabulary.itervalues()) + 1)\\n        spmatrix = sp.coo_matrix((values, (i_indices, j_indices)),\\n                                 shape=shape, dtype=self.dtype)\\n        if self.binary:\\n            spmatrix.data[:] = 1\\n        return spmatrix"
  },
  {
    "code": "def confusion_matrix(y_true, y_pred, labels=None):\\n\\tif labels is None:\\n\\t\\tlabels = unique_labels(y_true, y_pred)\\n\\telse:\\n\\t\\tlabels = np.asarray(labels, dtype=np.int)\\n\\tn_labels = labels.size\\n\\tlabel_to_ind = dict((y, x) for x, y in enumerate(labels))\\n\\ty_pred = np.array([label_to_ind.get(x,n_labels+1) for x in y_pred])\\n\\ty_true = np.array([label_to_ind.get(x,n_labels+1) for x in y_true])\\n\\tind = np.logical_and(y_pred < n_labels, y_true < n_labels)\\n\\ty_pred = y_pred[ind]\\n\\ty_true = y_true[ind]\\n\\tCM = np.asarray(coo_matrix((np.ones(y_true.shape[0]),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t(y_true, y_pred)),\\n\\t\\t\\t\\t\\t\\t\\t   shape=(n_labels, n_labels),\\n\\t\\t\\t\\t\\t\\t\\t   dtype=np.int).todense())\\n\\treturn CM",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def confusion_matrix(y_true, y_pred, labels=None):\\n\\tif labels is None:\\n\\t\\tlabels = unique_labels(y_true, y_pred)\\n\\telse:\\n\\t\\tlabels = np.asarray(labels, dtype=np.int)\\n\\tn_labels = labels.size\\n\\tlabel_to_ind = dict((y, x) for x, y in enumerate(labels))\\n\\ty_pred = np.array([label_to_ind.get(x,n_labels+1) for x in y_pred])\\n\\ty_true = np.array([label_to_ind.get(x,n_labels+1) for x in y_true])\\n\\tind = np.logical_and(y_pred < n_labels, y_true < n_labels)\\n\\ty_pred = y_pred[ind]\\n\\ty_true = y_true[ind]\\n\\tCM = np.asarray(coo_matrix((np.ones(y_true.shape[0]),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t(y_true, y_pred)),\\n\\t\\t\\t\\t\\t\\t\\t   shape=(n_labels, n_labels),\\n\\t\\t\\t\\t\\t\\t\\t   dtype=np.int).todense())\\n\\treturn CM"
  },
  {
    "code": "def _tidy_repr(self, max_vals=20):\\n        num = max_vals // 2\\n        head = self[:num]._get_repr(print_header=True, length=False,\\n                                    name=False)\\n        tail = self[-(max_vals - num):]._get_repr(print_header=False,\\n                                                  length=False,\\n                                                  name=False)\\n        result = head + '\\n...\\n' + tail\\n        namestr = \"Name: %s, \" % self.name if self.name else \"\"\\n        result = '%s\\n%sLength: %d' % (result, namestr, len(self))\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _tidy_repr(self, max_vals=20):\\n        num = max_vals // 2\\n        head = self[:num]._get_repr(print_header=True, length=False,\\n                                    name=False)\\n        tail = self[-(max_vals - num):]._get_repr(print_header=False,\\n                                                  length=False,\\n                                                  name=False)\\n        result = head + '\\n...\\n' + tail\\n        namestr = \"Name: %s, \" % self.name if self.name else \"\"\\n        result = '%s\\n%sLength: %d' % (result, namestr, len(self))\\n        return result"
  },
  {
    "code": "def _parse_raw_tfoot(self, table):\\n        tfoot = self._parse_tfoot(table)\\n        res = []\\n        if tfoot:\\n            res = lmap(self._text_getter, self._parse_td(tfoot[0]))\\n        return np.array(res).squeeze() if res and len(res) == 1 else res",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix for read_html with bs4 failing on table with header and one column\\n\\ncloses #12975\\ncloses #9178",
    "fixed_code": "def _parse_raw_tfoot(self, table):\\n        tfoot = self._parse_tfoot(table)\\n        res = []\\n        if tfoot:\\n            res = lmap(self._text_getter, self._parse_td(tfoot[0]))\\n        return np.atleast_1d(\\n            np.array(res).squeeze()) if res and len(res) == 1 else res"
  },
  {
    "code": "def create_snapshot_policy(self):\\n        self.validate_parameters()\\n        options = {'policy': self.parameters['name'],\\n                   'enabled': str(self.parameters['enabled']),\\n                   }\\n        positions = [str(i) for i in range(1, len(self.parameters['schedule']) + 1)]\\n        for schedule, count, position in zip(self.parameters['schedule'], self.parameters['count'], positions):\\n            options['count' + position] = str(count)\\n            options['schedule' + position] = schedule\\n        snapshot_obj = netapp_utils.zapi.NaElement.create_node_with_children('snapshot-policy-create', **options)\\n        if self.parameters.get('comment'):\\n            snapshot_obj.add_new_child(\"comment\", self.parameters['comment'])\\n        try:\\n            self.server.invoke_successfully(snapshot_obj, True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error creating snapshot policy %s: %s' %\\n                                  (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_snapshot_policy(self):\\n        self.validate_parameters()\\n        options = {'policy': self.parameters['name'],\\n                   'enabled': str(self.parameters['enabled']),\\n                   }\\n        positions = [str(i) for i in range(1, len(self.parameters['schedule']) + 1)]\\n        for schedule, count, position in zip(self.parameters['schedule'], self.parameters['count'], positions):\\n            options['count' + position] = str(count)\\n            options['schedule' + position] = schedule\\n        snapshot_obj = netapp_utils.zapi.NaElement.create_node_with_children('snapshot-policy-create', **options)\\n        if self.parameters.get('comment'):\\n            snapshot_obj.add_new_child(\"comment\", self.parameters['comment'])\\n        try:\\n            self.server.invoke_successfully(snapshot_obj, True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error creating snapshot policy %s: %s' %\\n                                  (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def users_manage_role(args, remove=False):\\n    if not args.username and not args.email:\\n        raise SystemExit('Missing args: must supply one of --username or --email')\\n    if args.username and args.email:\\n        raise SystemExit('Conflicting args: must supply either --username'\\n                         ' or --email, but not both')\\n    appbuilder = cached_appbuilder()\\n    user = (appbuilder.sm.find_user(username=args.username) or\\n            appbuilder.sm.find_user(email=args.email))\\n    if not user:\\n        raise SystemExit('User \"{}\" does not exist'.format(\\n            args.username or args.email))\\n    role = appbuilder.sm.find_role(args.role)\\n    if not role:\\n        valid_roles = appbuilder.sm.get_all_roles()\\n        raise SystemExit('{} is not a valid role. Valid roles are: {}'.format(args.role, valid_roles))\\n    if remove:\\n        if role in user.roles:\\n            user.roles = [r for r in user.roles if r != role]\\n            appbuilder.sm.update_user(user)\\n            print('User \"{}\" removed from role \"{}\".'.format(\\n                user,\\n                args.role))\\n        else:\\n            raise SystemExit('User \"{}\" is not a member of role \"{}\".'.format(\\n                user,\\n                args.role))\\n    else:\\n        if role in user.roles:\\n            raise SystemExit('User \"{}\" is already a member of role \"{}\".'.format(\\n                user,\\n                args.role))\\n        else:\\n            user.roles.append(role)\\n            appbuilder.sm.update_user(user)\\n            print('User \"{}\" added to role \"{}\".'.format(\\n                user,\\n                args.role))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def users_manage_role(args, remove=False):\\n    if not args.username and not args.email:\\n        raise SystemExit('Missing args: must supply one of --username or --email')\\n    if args.username and args.email:\\n        raise SystemExit('Conflicting args: must supply either --username'\\n                         ' or --email, but not both')\\n    appbuilder = cached_appbuilder()\\n    user = (appbuilder.sm.find_user(username=args.username) or\\n            appbuilder.sm.find_user(email=args.email))\\n    if not user:\\n        raise SystemExit('User \"{}\" does not exist'.format(\\n            args.username or args.email))\\n    role = appbuilder.sm.find_role(args.role)\\n    if not role:\\n        valid_roles = appbuilder.sm.get_all_roles()\\n        raise SystemExit('{} is not a valid role. Valid roles are: {}'.format(args.role, valid_roles))\\n    if remove:\\n        if role in user.roles:\\n            user.roles = [r for r in user.roles if r != role]\\n            appbuilder.sm.update_user(user)\\n            print('User \"{}\" removed from role \"{}\".'.format(\\n                user,\\n                args.role))\\n        else:\\n            raise SystemExit('User \"{}\" is not a member of role \"{}\".'.format(\\n                user,\\n                args.role))\\n    else:\\n        if role in user.roles:\\n            raise SystemExit('User \"{}\" is already a member of role \"{}\".'.format(\\n                user,\\n                args.role))\\n        else:\\n            user.roles.append(role)\\n            appbuilder.sm.update_user(user)\\n            print('User \"{}\" added to role \"{}\".'.format(\\n                user,\\n                args.role))"
  },
  {
    "code": "def __len__(self):\\n\\t\\treturn len(self.iterable) if self.iterable else self.total",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __len__(self):\\n\\t\\treturn len(self.iterable) if self.iterable else self.total"
  },
  {
    "code": "def __init__(self, data, kind=None, by=None, subplots=False, sharex=True,\\n                 sharey=False, use_index=True,\\n                 figsize=None, grid=True, legend=True, rot=None,\\n                 ax=None, fig=None, title=None, xlim=None, ylim=None,\\n                 xticks=None, yticks=None,\\n                 sort_columns=False, fontsize=None, **kwds):\\n        self.data = data\\n        self.by = by\\n        self.kind = kind\\n        self.sort_columns = sort_columns\\n        self.subplots = subplots\\n        self.sharex = sharex\\n        self.sharey = sharey\\n        self.figsize = figsize\\n        self.xticks = xticks\\n        self.yticks = yticks\\n        self.xlim = xlim\\n        self.ylim = ylim\\n        self.title = title\\n        self.use_index = use_index\\n        self.fontsize = fontsize\\n        self.rot = rot\\n        self.grid = grid\\n        self.legend = legend\\n        for attr in self._pop_attributes:\\n            value = kwds.pop(attr, self._attr_defaults.get(attr, None))\\n            setattr(self, attr, value)\\n        self.ax = ax\\n        self.fig = fig\\n        self.axes = None\\n        self.secondary_y = kwds.pop('secondary_y', False)\\n        self.kwds = kwds",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, data, kind=None, by=None, subplots=False, sharex=True,\\n                 sharey=False, use_index=True,\\n                 figsize=None, grid=True, legend=True, rot=None,\\n                 ax=None, fig=None, title=None, xlim=None, ylim=None,\\n                 xticks=None, yticks=None,\\n                 sort_columns=False, fontsize=None, **kwds):\\n        self.data = data\\n        self.by = by\\n        self.kind = kind\\n        self.sort_columns = sort_columns\\n        self.subplots = subplots\\n        self.sharex = sharex\\n        self.sharey = sharey\\n        self.figsize = figsize\\n        self.xticks = xticks\\n        self.yticks = yticks\\n        self.xlim = xlim\\n        self.ylim = ylim\\n        self.title = title\\n        self.use_index = use_index\\n        self.fontsize = fontsize\\n        self.rot = rot\\n        self.grid = grid\\n        self.legend = legend\\n        for attr in self._pop_attributes:\\n            value = kwds.pop(attr, self._attr_defaults.get(attr, None))\\n            setattr(self, attr, value)\\n        self.ax = ax\\n        self.fig = fig\\n        self.axes = None\\n        self.secondary_y = kwds.pop('secondary_y', False)\\n        self.kwds = kwds"
  },
  {
    "code": "def set_atom_string(self, block, existing_col, min_itemsize, nan_rep, encoding):\\n        block = block.fillna(nan_rep)\\n        data  = block.values\\n        inferred_type = lib.infer_dtype(data.ravel())\\n        if inferred_type != 'string':\\n            for item in block.items:\\n                col = block.get(item)\\n                inferred_type = lib.infer_dtype(col.ravel())\\n                if inferred_type != 'string':\\n                    raise TypeError(\"Cannot serialize the column [%s] because\\n\"\\n                                    \"its data contents are [%s] object dtype\" %\\n                                    (item,inferred_type))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_atom_string(self, block, existing_col, min_itemsize, nan_rep, encoding):\\n        block = block.fillna(nan_rep)\\n        data  = block.values\\n        inferred_type = lib.infer_dtype(data.ravel())\\n        if inferred_type != 'string':\\n            for item in block.items:\\n                col = block.get(item)\\n                inferred_type = lib.infer_dtype(col.ravel())\\n                if inferred_type != 'string':\\n                    raise TypeError(\"Cannot serialize the column [%s] because\\n\"\\n                                    \"its data contents are [%s] object dtype\" %\\n                                    (item,inferred_type))"
  },
  {
    "code": "def emit(self, record):\\n        try:\\n            request = record.request\\n            subject = '%s (%s IP): %s' % (\\n                record.levelname,\\n                ('internal' if request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS\\n                 else 'EXTERNAL'),\\n                record.getMessage()\\n            )\\n        except Exception:\\n            subject = '%s: %s' % (\\n                record.levelname,\\n                record.getMessage()\\n            )\\n            request = None\\n        subject = self.format_subject(subject)\\n        no_exc_record = copy(record)\\n        no_exc_record.exc_info = None\\n        no_exc_record.exc_text = None\\n        if record.exc_info:\\n            exc_info = record.exc_info\\n        else:\\n            exc_info = (None, record.getMessage(), None)\\n        reporter = self.reporter_class(request, is_email=True, *exc_info)\\n        message = \"%s\\n%s\" % (self.format(no_exc_record), reporter.get_traceback_text())\\n        html_message = reporter.get_traceback_html() if self.include_html else None\\n        self.send_mail(subject, message, fail_silently=True, html_message=html_message)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def emit(self, record):\\n        try:\\n            request = record.request\\n            subject = '%s (%s IP): %s' % (\\n                record.levelname,\\n                ('internal' if request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS\\n                 else 'EXTERNAL'),\\n                record.getMessage()\\n            )\\n        except Exception:\\n            subject = '%s: %s' % (\\n                record.levelname,\\n                record.getMessage()\\n            )\\n            request = None\\n        subject = self.format_subject(subject)\\n        no_exc_record = copy(record)\\n        no_exc_record.exc_info = None\\n        no_exc_record.exc_text = None\\n        if record.exc_info:\\n            exc_info = record.exc_info\\n        else:\\n            exc_info = (None, record.getMessage(), None)\\n        reporter = self.reporter_class(request, is_email=True, *exc_info)\\n        message = \"%s\\n%s\" % (self.format(no_exc_record), reporter.get_traceback_text())\\n        html_message = reporter.get_traceback_html() if self.include_html else None\\n        self.send_mail(subject, message, fail_silently=True, html_message=html_message)"
  },
  {
    "code": "def __setitem__(self, key, value):\\n        raise Exception(str(self.__class__) + ' object is immutable')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Make core/index exceptions more descriptive\\n\\n  produce copies that are not the same object. (uses `assert_almost_equal`\\n  under the hood).\\n  after iterable check)",
    "fixed_code": "def __setitem__(self, key, value):\\n        raise TypeError(str(self.__class__) + ' does not support item assignment')"
  },
  {
    "code": "def forbid_multi_line_headers(name, val):\\n    val = force_unicode(val)\\n    if '\\n' in val or '\\r' in val:\\n        raise BadHeaderError(\"Header values can't contain newlines (got %r for header %r)\" % (val, name))\\n    try:\\n        val = val.encode('ascii')\\n    except UnicodeEncodeError:\\n        if name.lower() in ('to', 'from', 'cc'):\\n            result = []\\n            for item in val.split(', '):\\n                nm, addr = parseaddr(item)\\n                nm = str(Header(nm, settings.DEFAULT_CHARSET))\\n                result.append(formataddr((nm, str(addr))))\\n            val = ', '.join(result)\\n        else:\\n            val = Header(val, settings.DEFAULT_CHARSET)\\n    return name, val\\nclass SafeMIMEText(MIMEText):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #7747: Altered EmailMessage such that messages with long subject lines don't use tabs in their continutation sequence. Tabs in subjects cause problems with Outlook and Thunderbird. Thanks to Mark Allison <mark.allison@maplecroft.com> for the report and fix.",
    "fixed_code": "def forbid_multi_line_headers(name, val):\\n    val = force_unicode(val)\\n    if '\\n' in val or '\\r' in val:\\n        raise BadHeaderError(\"Header values can't contain newlines (got %r for header %r)\" % (val, name))\\n    try:\\n        val = val.encode('ascii')\\n    except UnicodeEncodeError:\\n        if name.lower() in ('to', 'from', 'cc'):\\n            result = []\\n            for item in val.split(', '):\\n                nm, addr = parseaddr(item)\\n                nm = str(Header(nm, settings.DEFAULT_CHARSET))\\n                result.append(formataddr((nm, str(addr))))\\n            val = ', '.join(result)\\n        else:\\n            val = Header(val, settings.DEFAULT_CHARSET)\\n    else:\\n        if name.lower() == 'subject':\\n            val = Header(val)\\n    return name, val\\nclass SafeMIMEText(MIMEText):"
  },
  {
    "code": "def _possibly_cast_to_datetime(value, dtype, coerce=False):\\n    if dtype is not None:\\n        if isinstance(dtype, compat.string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if is_datetime64 and dtype != _NS_DTYPE:\\n                if dtype.name == 'datetime64[ns]':\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert datetimelike to dtype [%s]\" % dtype)\\n            elif is_timedelta64 and dtype != _TD_DTYPE:\\n                if dtype.name == 'timedelta64[ns]':\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert timedeltalike to dtype [%s]\" % dtype)\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            from pandas.tseries.tools import to_datetime\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            from pandas.tseries.timedeltas import \\\\n                                _possibly_cast_to_timedelta\\n                            value = _possibly_cast_to_timedelta(value, coerce=True)\\n                    except:\\n                        pass\\n    else:\\n        if (isinstance(value, np.ndarray) and not\\n                (issubclass(value.dtype.type, np.integer) or\\n                 value.dtype == np.object_)):\\n            pass\\n        else:\\n            v = value\\n            if not is_list_like(v):\\n                v = [v]\\n            if len(v):\\n                inferred_type = lib.infer_dtype(v)\\n                if inferred_type in ['datetime', 'datetime64']:\\n                    try:\\n                        value = tslib.array_to_datetime(np.array(v))\\n                    except:\\n                        pass\\n                elif inferred_type in ['timedelta', 'timedelta64']:\\n                    from pandas.tseries.timedeltas import \\\\n                        _possibly_cast_to_timedelta\\n                    value = _possibly_cast_to_timedelta(value)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _possibly_cast_to_datetime(value, dtype, coerce=False):\\n    if dtype is not None:\\n        if isinstance(dtype, compat.string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if is_datetime64 and dtype != _NS_DTYPE:\\n                if dtype.name == 'datetime64[ns]':\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert datetimelike to dtype [%s]\" % dtype)\\n            elif is_timedelta64 and dtype != _TD_DTYPE:\\n                if dtype.name == 'timedelta64[ns]':\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\\n                        \"cannot convert timedeltalike to dtype [%s]\" % dtype)\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            from pandas.tseries.tools import to_datetime\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            from pandas.tseries.timedeltas import \\\\n                                _possibly_cast_to_timedelta\\n                            value = _possibly_cast_to_timedelta(value, coerce=True)\\n                    except:\\n                        pass\\n    else:\\n        if (isinstance(value, np.ndarray) and not\\n                (issubclass(value.dtype.type, np.integer) or\\n                 value.dtype == np.object_)):\\n            pass\\n        else:\\n            v = value\\n            if not is_list_like(v):\\n                v = [v]\\n            if len(v):\\n                inferred_type = lib.infer_dtype(v)\\n                if inferred_type in ['datetime', 'datetime64']:\\n                    try:\\n                        value = tslib.array_to_datetime(np.array(v))\\n                    except:\\n                        pass\\n                elif inferred_type in ['timedelta', 'timedelta64']:\\n                    from pandas.tseries.timedeltas import \\\\n                        _possibly_cast_to_timedelta\\n                    value = _possibly_cast_to_timedelta(value)\\n    return value"
  },
  {
    "code": "def unstack(self, level=-1):\\n\\t\\tfrom pandas.core.reshape import unstack\\n\\t\\tif isinstance(level, (tuple, list)):\\n\\t\\t\\tresult = self\\n\\t\\t\\tfor lev in level:\\n\\t\\t\\t\\tresult = unstack(result, lev)\\n\\t\\t\\treturn result\\n\\t\\telse:\\n\\t\\t\\treturn unstack(self, level)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: unstack multiple levels bug described in #451",
    "fixed_code": "def unstack(self, level=-1):\\n\\t\\tfrom pandas.core.reshape import unstack\\n\\t\\tif isinstance(level, (tuple, list)):\\n\\t\\t\\tresult = self\\n\\t\\t\\tto_unstack = level\\n\\t\\t\\twhile to_unstack:\\n\\t\\t\\t\\tlev = to_unstack[0]\\n\\t\\t\\t\\tresult = unstack(result, lev)\\n\\t\\t\\t\\tto_unstack = [other - 1 if other > lev else other\\n\\t\\t\\t\\t\\t\\t\\t  for other in to_unstack[1:]]\\n\\t\\t\\treturn result\\n\\t\\telse:\\n\\t\\t\\treturn unstack(self, level)"
  },
  {
    "code": "def get_expiry_date(self):\\n        expiry = self.get('_session_expiry')\\n        if isinstance(expiry, datetime):\\n            return expiry\\n        if not expiry:   \\n            expiry = settings.SESSION_COOKIE_AGE\\n        return timezone.now() + timedelta(seconds=expiry)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added optional kwargs to get_expiry_age/date.\\n\\nThis change allows for cleaner tests: we can test the exact output.\\n\\nRefs #18194: this change makes it possible to compute session expiry\\ndates at times other than when the session is saved.\\n\\nFixed #18458: the existence of the `modification` kwarg implies that you\\nmust pass it to get_expiry_age/date if you call these functions outside\\nof a short request - response cycle (the intended use case).",
    "fixed_code": "def get_expiry_date(self, **kwargs):\\n        try:\\n            modification = kwargs['modification']\\n        except KeyError:\\n            modification = timezone.now()\\n        try:\\n            expiry = kwargs['expiry']\\n        except KeyError:\\n            expiry = self.get('_session_expiry')\\n        if isinstance(expiry, datetime):\\n            return expiry\\n        if not expiry:   \\n            expiry = settings.SESSION_COOKIE_AGE\\n        return modification + timedelta(seconds=expiry)"
  },
  {
    "code": "def add_suffix(self, suffix):\\n        f = ('%s' + str(suffix)).__mod__\\n        return self.rename_axis(f, axis=0)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Support strings containing '%' in add_prefix/add_suffix (#17151) (#17162)",
    "fixed_code": "def add_suffix(self, suffix):\\n        f = partial('{}{suffix}'.format, suffix=suffix)\\n        return self.rename_axis(f, axis=0)\\n    @property"
  },
  {
    "code": "def list_keys(self, bucket_name=None, prefix='', delimiter='',\\n                  page_size=None, max_items=None):\\n        config = {\\n            'PageSize': page_size,\\n            'MaxItems': max_items,\\n        }\\n        paginator = self.get_conn().get_paginator('list_objects_v2')\\n        response = paginator.paginate(Bucket=bucket_name,\\n                                      Prefix=prefix,\\n                                      Delimiter=delimiter,\\n                                      PaginationConfig=config)\\n        has_results = False\\n        keys = []\\n        for page in response:\\n            if 'Contents' in page:\\n                has_results = True\\n                for k in page['Contents']:\\n                    keys.append(k['Key'])\\n        if has_results:\\n            return keys\\n        return None\\n    @provide_bucket_name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def list_keys(self, bucket_name=None, prefix='', delimiter='',\\n                  page_size=None, max_items=None):\\n        config = {\\n            'PageSize': page_size,\\n            'MaxItems': max_items,\\n        }\\n        paginator = self.get_conn().get_paginator('list_objects_v2')\\n        response = paginator.paginate(Bucket=bucket_name,\\n                                      Prefix=prefix,\\n                                      Delimiter=delimiter,\\n                                      PaginationConfig=config)\\n        has_results = False\\n        keys = []\\n        for page in response:\\n            if 'Contents' in page:\\n                has_results = True\\n                for k in page['Contents']:\\n                    keys.append(k['Key'])\\n        if has_results:\\n            return keys\\n        return None\\n    @provide_bucket_name"
  },
  {
    "code": "def __eq__(self, other):\\n        return self._key() == other._key()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __eq__(self, other):\\n        return self._key() == other._key()"
  },
  {
    "code": "def _get_addsub_freq(self, other) -> Optional[DateOffset]:\\n\\t\\tif is_period_dtype(self.dtype):\\n\\t\\t\\treturn self.freq\\n\\t\\telif self.freq is None:\\n\\t\\t\\treturn None\\n\\t\\telif lib.is_scalar(other) and isna(other):\\n\\t\\t\\treturn None\\n\\t\\telif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n\\t\\t\\tnew_freq = None\\n\\t\\t\\tif isinstance(self.freq, Tick):\\n\\t\\t\\t\\tnew_freq = self.freq\\n\\t\\t\\treturn new_freq\\n\\t\\telif isinstance(other, DateOffset):\\n\\t\\t\\treturn None  \\n\\t\\telif isinstance(other, (datetime, np.datetime64)):\\n\\t\\t\\treturn self.freq\\n\\t\\telif is_timedelta64_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_object_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_datetime64_any_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telse:\\n\\t\\t\\traise NotImplementedError\\n\\t__add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\\n\\t__sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\\n\\t__radd__ = make_wrapped_arith_op(\"__radd__\")\\n\\t__rsub__ = make_wrapped_arith_op(\"__rsub__\")\\n\\t__pow__ = make_wrapped_arith_op(\"__pow__\")\\n\\t__rpow__ = make_wrapped_arith_op(\"__rpow__\")\\n\\t__mul__ = make_wrapped_arith_op(\"__mul__\")\\n\\t__rmul__ = make_wrapped_arith_op(\"__rmul__\")\\n\\t__floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\\n\\t__rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\\n\\t__mod__ = make_wrapped_arith_op(\"__mod__\")\\n\\t__rmod__ = make_wrapped_arith_op(\"__rmod__\")\\n\\t__divmod__ = make_wrapped_arith_op(\"__divmod__\")\\n\\t__rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\\n\\t__truediv__ = make_wrapped_arith_op(\"__truediv__\")\\n\\t__rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: incorrect freq in PeriodIndex-Period (#33801)",
    "fixed_code": "def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\\n\\t\\tif is_period_dtype(self.dtype):\\n\\t\\t\\tif is_period_dtype(result.dtype):\\n\\t\\t\\t\\treturn self.freq\\n\\t\\t\\treturn None\\n\\t\\telif self.freq is None:\\n\\t\\t\\treturn None\\n\\t\\telif lib.is_scalar(other) and isna(other):\\n\\t\\t\\treturn None\\n\\t\\telif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n\\t\\t\\tnew_freq = None\\n\\t\\t\\tif isinstance(self.freq, Tick):\\n\\t\\t\\t\\tnew_freq = self.freq\\n\\t\\t\\treturn new_freq\\n\\t\\telif isinstance(other, DateOffset):\\n\\t\\t\\treturn None  \\n\\t\\telif isinstance(other, (datetime, np.datetime64)):\\n\\t\\t\\treturn self.freq\\n\\t\\telif is_timedelta64_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_object_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_datetime64_any_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telse:\\n\\t\\t\\traise NotImplementedError\\n\\t__add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\\n\\t__sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\\n\\t__radd__ = make_wrapped_arith_op(\"__radd__\")\\n\\t__rsub__ = make_wrapped_arith_op(\"__rsub__\")\\n\\t__pow__ = make_wrapped_arith_op(\"__pow__\")\\n\\t__rpow__ = make_wrapped_arith_op(\"__rpow__\")\\n\\t__mul__ = make_wrapped_arith_op(\"__mul__\")\\n\\t__rmul__ = make_wrapped_arith_op(\"__rmul__\")\\n\\t__floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\\n\\t__rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\\n\\t__mod__ = make_wrapped_arith_op(\"__mod__\")\\n\\t__rmod__ = make_wrapped_arith_op(\"__rmod__\")\\n\\t__divmod__ = make_wrapped_arith_op(\"__divmod__\")\\n\\t__rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\\n\\t__truediv__ = make_wrapped_arith_op(\"__truediv__\")\\n\\t__rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")"
  },
  {
    "code": "def get_diagnostics(self, context, instance_uuid=None, instance=None):\\n\\t\\tif not instance:\\n\\t\\t\\tinstance = self.db.instance_get_by_uuid(context, instance_uuid)\\n\\t\\tcurrent_power_state = self._get_power_state(context, instance)\\n\\t\\tif current_power_state == power_state.RUNNING:\\n\\t\\t\\tLOG.audit(_(\"Retrieving diagnostics\"), context=context,\\n\\t\\t\\t\\t\\t  instance=instance)\\n\\t\\t\\treturn self.driver.get_diagnostics(instance)\\n\\t@exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())\\n\\t@checks_instance_lock\\n\\t@wrap_instance_fault",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix get_diagnostics RPC arg ordering.\\n\\nUpdates the ordering of the compute managers get_diagnostics\\ncall so it works correctly with the current wrap_instance_fault and\\nchecks_instance_lock decorators. To work correctly 'instance' needs\\nto be the first argument after the context.\\n\\nFixes LP Bug #1031788.",
    "fixed_code": "def get_diagnostics(self, context, instance=None, instance_uuid=None):\\n\\t\\tif not instance:\\n\\t\\t\\tinstance = self.db.instance_get_by_uuid(context, instance_uuid)\\n\\t\\tcurrent_power_state = self._get_power_state(context, instance)\\n\\t\\tif current_power_state == power_state.RUNNING:\\n\\t\\t\\tLOG.audit(_(\"Retrieving diagnostics\"), context=context,\\n\\t\\t\\t\\t\\t  instance=instance)\\n\\t\\t\\treturn self.driver.get_diagnostics(instance)\\n\\t@exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())\\n\\t@checks_instance_lock\\n\\t@wrap_instance_fault"
  },
  {
    "code": "def extract_index(data, index):\\n    if len(data) == 0:\\n        if index is None:\\n            index = NULL_INDEX\\n    elif len(data) > 0 and index is None:\\n        need_labels = False\\n        msg = ('Cannot mix Series / dict objects'\\n               ' with ndarray / sequence input')\\n        for v in data.values():\\n            if isinstance(v, Series):\\n                if index is None:\\n                    index = v.index\\n                elif need_labels:\\n                    raise Exception(msg)\\n                elif not index.equals(v.index):\\n                    index = index + v.index\\n            elif isinstance(v, dict):\\n                if index is None:\\n                    index = Index(try_sort(v))\\n                elif need_labels:\\n                    raise Exception(msg)\\n                else:\\n                    index = index + Index(v.keys())\\n            else: \\n                if index is not None and not need_labels:\\n                    raise Exception(msg)\\n                need_labels = True\\n                index = Index(np.arange(len(v)))\\n    if len(index) == 0 or index is None:\\n        index = NULL_INDEX\\n    if not isinstance(index, Index):\\n        index = Index(index)\\n    return index",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "working on SparseWidePanel. reduced some code dup in core classes",
    "fixed_code": "def extract_index(data, index):\\n    if len(data) == 0:\\n        if index is None:\\n            index = NULL_INDEX\\n    elif len(data) > 0 and index is None:\\n        need_labels = False\\n        msg = ('Cannot mix Series / dict objects'\\n               ' with ndarray / sequence input')\\n        for v in data.values():\\n            if isinstance(v, Series):\\n                if index is None:\\n                    index = v.index\\n                elif need_labels:\\n                    raise Exception(msg)\\n                elif not index.equals(v.index):\\n                    index = index + v.index\\n            elif isinstance(v, dict):\\n                if index is None:\\n                    index = Index(try_sort(v))\\n                elif need_labels:\\n                    raise Exception(msg)\\n                else:\\n                    index = index + Index(v.keys())\\n            else: \\n                if index is not None and not need_labels:\\n                    raise Exception(msg)\\n                need_labels = True\\n                index = Index(np.arange(len(v)))\\n    if len(index) == 0 or index is None:\\n        index = NULL_INDEX\\n    return _ensure_index(index)"
  },
  {
    "code": "def login(request, user, backend=None):\\n    session_auth_hash = ''\\n    if user is None:\\n        user = request.user\\n    if hasattr(user, 'get_session_auth_hash'):\\n        session_auth_hash = user.get_session_auth_hash()\\n    if SESSION_KEY in request.session:\\n        if _get_user_session_key(request) != user.pk or (\\n                session_auth_hash and\\n                not constant_time_compare(request.session.get(HASH_SESSION_KEY, ''), session_auth_hash)):\\n            request.session.flush()\\n    else:\\n        request.session.cycle_key()\\n    try:\\n        backend = backend or user.backend\\n    except AttributeError:\\n        backends = _get_backends(return_tuples=True)\\n        if len(backends) == 1:\\n            _, backend = backends[0]\\n        else:\\n            raise ValueError(\\n                'You have multiple authentication backends configured and '\\n                'therefore must provide the `backend` argument or set the '\\n                '`backend` attribute on the user.'\\n            )\\n    else:\\n        if not isinstance(backend, str):\\n            raise TypeError('backend must be a dotted import path string (got %r).' % backend)\\n    request.session[SESSION_KEY] = user._meta.pk.value_to_string(user)\\n    request.session[BACKEND_SESSION_KEY] = backend\\n    request.session[HASH_SESSION_KEY] = session_auth_hash\\n    if hasattr(request, 'user'):\\n        request.user = user\\n    rotate_token(request)\\n    user_logged_in.send(sender=user.__class__, request=request, user=user)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def login(request, user, backend=None):\\n    session_auth_hash = ''\\n    if user is None:\\n        user = request.user\\n    if hasattr(user, 'get_session_auth_hash'):\\n        session_auth_hash = user.get_session_auth_hash()\\n    if SESSION_KEY in request.session:\\n        if _get_user_session_key(request) != user.pk or (\\n                session_auth_hash and\\n                not constant_time_compare(request.session.get(HASH_SESSION_KEY, ''), session_auth_hash)):\\n            request.session.flush()\\n    else:\\n        request.session.cycle_key()\\n    try:\\n        backend = backend or user.backend\\n    except AttributeError:\\n        backends = _get_backends(return_tuples=True)\\n        if len(backends) == 1:\\n            _, backend = backends[0]\\n        else:\\n            raise ValueError(\\n                'You have multiple authentication backends configured and '\\n                'therefore must provide the `backend` argument or set the '\\n                '`backend` attribute on the user.'\\n            )\\n    else:\\n        if not isinstance(backend, str):\\n            raise TypeError('backend must be a dotted import path string (got %r).' % backend)\\n    request.session[SESSION_KEY] = user._meta.pk.value_to_string(user)\\n    request.session[BACKEND_SESSION_KEY] = backend\\n    request.session[HASH_SESSION_KEY] = session_auth_hash\\n    if hasattr(request, 'user'):\\n        request.user = user\\n    rotate_token(request)\\n    user_logged_in.send(sender=user.__class__, request=request, user=user)"
  },
  {
    "code": "def __dir__():\\n            return list(globals().keys()) + ['Tester', 'testing']\\n    else:\\n        from .testing import Tester\\n    from numpy._pytesttester import PytestTester\\n    test = PytestTester(__name__)\\n    del PytestTester",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT: dir(numpy) returned duplicate \"testing\" (gh-15425)\\n\\nModified __dir__() to remove duplicate \"Tester/Testing\" attribute. Also added a test to verify this.\\n\\nCloses gh-15383",
    "fixed_code": "def __dir__():\\n            return list(globals().keys() | {'Tester', 'testing'})\\n    else:\\n        from .testing import Tester\\n    from numpy._pytesttester import PytestTester\\n    test = PytestTester(__name__)\\n    del PytestTester"
  },
  {
    "code": "def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\\n    placemarks = []\\n    try:\\n        klass = apps.get_model(label, model)\\n    except LookupError:\\n        raise Http404('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model))\\n    if field_name:\\n        try:\\n            field = klass._meta.get_field(field_name)\\n            if not isinstance(field, GeometryField):\\n                raise FieldDoesNotExist\\n        except FieldDoesNotExist:\\n            raise Http404('Invalid geometry field.')\\n    connection = connections[using]\\n    if connection.features.has_AsKML_function:\\n        placemarks = klass._default_manager.using(using).annotate(kml=AsKML(field_name))\\n    else:\\n        placemarks = []\\n        if connection.features.has_Transform_function:\\n            qs = klass._default_manager.using(using).annotate(\\n                **{'%s_4326' % field_name: Transform(field_name, 4326)})\\n            field_name += '_4326'\\n        else:\\n            qs = klass._default_manager.using(using).all()\\n        for mod in qs:\\n            mod.kml = getattr(mod, field_name).kml\\n            placemarks.append(mod)\\n    if compress:\\n        render = render_to_kmz\\n    else:\\n        render = render_to_kml\\n    return render('gis/kml/placemarks.kml', {'places': placemarks})",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Removed unneeded GeoManagers in tests.",
    "fixed_code": "def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\\n    placemarks = []\\n    try:\\n        klass = apps.get_model(label, model)\\n    except LookupError:\\n        raise Http404('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model))\\n    if field_name:\\n        try:\\n            field = klass._meta.get_field(field_name)\\n            if not isinstance(field, GeometryField):\\n                raise FieldDoesNotExist\\n        except FieldDoesNotExist:\\n            raise Http404('Invalid geometry field.')\\n    connection = connections[using]\\n    if connection.features.has_AsKML_function:\\n        placemarks = klass._default_manager.using(using).annotate(kml=AsKML(field_name))\\n    else:\\n        placemarks = []\\n        if connection.features.has_Transform_function:\\n            qs = klass._default_manager.using(using).annotate(\\n                **{'%s_4326' % field_name: Transform(field_name, 4326)})\\n            field_name += '_4326'\\n        else:\\n            qs = klass._default_manager.using(using).all()\\n        for mod in qs:\\n            mod.kml = getattr(mod, field_name).kml\\n            placemarks.append(mod)\\n    if compress:\\n        render = render_to_kmz\\n    else:\\n        render = render_to_kml\\n    return render('gis/kml/placemarks.kml', {'places': placemarks})"
  },
  {
    "code": "def _normalise_json(\\n    data: Any,\\n    key_string: str,\\n    normalized_dict: dict[str, Any],\\n    separator: str,\\n) -> dict[str, Any]:\\n    if isinstance(data, dict):\\n        for key, value in data.items():\\n            new_key = f\"{key_string}{separator}{key}\"\\n            _normalise_json(\\n                data=value,\\n                key_string=new_key\\n                if new_key[len(separator) - 1] != separator\\n                else new_key[len(separator) :],\\n                normalized_dict=normalized_dict,\\n                separator=separator,\\n            )\\n    else:\\n        normalized_dict[key_string] = data\\n    return normalized_dict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: json_normalize prefixed multi-char sep to all keys (#43851)",
    "fixed_code": "def _normalise_json(\\n    data: Any,\\n    key_string: str,\\n    normalized_dict: dict[str, Any],\\n    separator: str,\\n) -> dict[str, Any]:\\n    if isinstance(data, dict):\\n        for key, value in data.items():\\n            new_key = f\"{key_string}{separator}{key}\"\\n            _normalise_json(\\n                data=value,\\n                key_string=new_key\\n                if new_key[: len(separator)] != separator\\n                else new_key[len(separator) :],\\n                normalized_dict=normalized_dict,\\n                separator=separator,\\n            )\\n    else:\\n        normalized_dict[key_string] = data\\n    return normalized_dict"
  },
  {
    "code": "def _sqlalchemy_type(self, arr_or_dtype):\\n        from sqlalchemy.types import Integer, Float, Text, Boolean, DateTime, Date, Interval\\n        if arr_or_dtype is date:\\n            return Date\\n        if com.is_datetime64_dtype(arr_or_dtype):\\n            try:\\n                tz = arr_or_dtype.tzinfo\\n                return DateTime(timezone=True)\\n            except:\\n                return DateTime\\n        if com.is_timedelta64_dtype(arr_or_dtype):\\n            warnings.warn(\"the 'timedelta' type is not supported, and will be \"\\n                          \"written as integer values (ns frequency) to the \"\\n                          \"database.\", UserWarning)\\n            return Integer\\n        elif com.is_float_dtype(arr_or_dtype):\\n            return Float\\n        elif com.is_integer_dtype(arr_or_dtype):\\n            return Integer\\n        elif com.is_bool(arr_or_dtype):\\n            return Boolean\\n        return Text",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: don't create 32 bit int type for int64 column (GH7433)",
    "fixed_code": "def _sqlalchemy_type(self, arr_or_dtype):\\n        from sqlalchemy.types import (BigInteger, Float, Text, Boolean,\\n            DateTime, Date, Interval)\\n        if arr_or_dtype is date:\\n            return Date\\n        if com.is_datetime64_dtype(arr_or_dtype):\\n            try:\\n                tz = arr_or_dtype.tzinfo\\n                return DateTime(timezone=True)\\n            except:\\n                return DateTime\\n        if com.is_timedelta64_dtype(arr_or_dtype):\\n            warnings.warn(\"the 'timedelta' type is not supported, and will be \"\\n                          \"written as integer values (ns frequency) to the \"\\n                          \"database.\", UserWarning)\\n            return BigInteger\\n        elif com.is_float_dtype(arr_or_dtype):\\n            return Float\\n        elif com.is_integer_dtype(arr_or_dtype):\\n            return BigInteger\\n        elif com.is_bool(arr_or_dtype):\\n            return Boolean\\n        return Text"
  },
  {
    "code": "def run_luks_remove_key(self, device, keyfile, force_remove_last_key=False):\\n        ''\\n        if not force_remove_last_key:\\n            result = self._run_command([self._cryptsetup_bin, 'luksDump', device])\\n            if result[RETURN_CODE] != 0:\\n                raise ValueError('Error while dumping LUKS header from %s'\\n                                 % (device, ))\\n            keyslot_count = 0\\n            keyslot_area = False\\n            keyslot_re = re.compile(r'^Key Slot [0-9]+: ENABLED')\\n            for line in result[STDOUT].splitlines():\\n                if line.startswith('Keyslots:'):\\n                    keyslot_area = True\\n                elif line.startswith('  '):\\n                    if keyslot_area and line[2] in '0123456789':\\n                        keyslot_count += 1\\n                elif line.startswith('\\t'):\\n                    pass\\n                elif keyslot_re.match(line):\\n                    keyslot_count += 1\\n                else:\\n                    keyslot_area = False\\n            if keyslot_count < 2:\\n                self._module.fail_json(msg=\"LUKS device %s has less than two active keyslots. \"\\n                                           \"To be able to remove a key, please set \"\\n                                           \"`force_remove_last_key` to `yes`.\" % device)\\n        result = self._run_command([self._cryptsetup_bin, 'luksRemoveKey', device,\\n                                    '-q', '--key-file', keyfile])\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while removing LUKS key from %s: %s'\\n                             % (device, result[STDERR]))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add passphrase support for luks_device  (#65050)\\n\\n\\nSeveral tests in `key-management.yml` don't `become` before executing,\\ndespite needing elevated privileges.  This commit fixes that.\\n\\n\\nPreviously, the luks_device module only worked with keyfiles.  The\\nimplication was that the key had to be written to disk before the module\\ncould be used.\\n\\nThis commit implements support for opening, adding and removing\\npassphrases supplied as strings to the module.\\n\\nCloses #52408",
    "fixed_code": "def run_luks_remove_key(self, device, keyfile, passphrase,\\n                            force_remove_last_key=False):\\n        ''\\n        if not force_remove_last_key:\\n            result = self._run_command([self._cryptsetup_bin, 'luksDump', device])\\n            if result[RETURN_CODE] != 0:\\n                raise ValueError('Error while dumping LUKS header from %s'\\n                                 % (device, ))\\n            keyslot_count = 0\\n            keyslot_area = False\\n            keyslot_re = re.compile(r'^Key Slot [0-9]+: ENABLED')\\n            for line in result[STDOUT].splitlines():\\n                if line.startswith('Keyslots:'):\\n                    keyslot_area = True\\n                elif line.startswith('  '):\\n                    if keyslot_area and line[2] in '0123456789':\\n                        keyslot_count += 1\\n                elif line.startswith('\\t'):\\n                    pass\\n                elif keyslot_re.match(line):\\n                    keyslot_count += 1\\n                else:\\n                    keyslot_area = False\\n            if keyslot_count < 2:\\n                self._module.fail_json(msg=\"LUKS device %s has less than two active keyslots. \"\\n                                           \"To be able to remove a key, please set \"\\n                                           \"`force_remove_last_key` to `yes`.\" % device)\\n        args = [self._cryptsetup_bin, 'luksRemoveKey', device, '-q']\\n        if keyfile:\\n            args.extend(['--key-file', keyfile])\\n        result = self._run_command(args, data=passphrase)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while removing LUKS key from %s: %s'\\n                             % (device, result[STDERR]))"
  },
  {
    "code": "def get_variable(self, key: str) -> Optional[str]:\\n        if self.variables_prefix is None:\\n            return None\\n        return self._get_secret(self.variables_prefix, key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_variable(self, key: str) -> Optional[str]:\\n        if self.variables_prefix is None:\\n            return None\\n        return self._get_secret(self.variables_prefix, key)"
  },
  {
    "code": "def __getitem__(self, key):\\n        arr_idx = self.view(np.ndarray)\\n        if np.isscalar(key):\\n            val = arr_idx[key]\\n            return Timestamp(val, offset=self.offset, tz=self.tz)\\n        else:\\n            if com._is_bool_indexer(key):\\n                key = np.asarray(key)\\n                key = lib.maybe_booleans_to_slice(key.view(np.uint8))\\n            new_offset = None\\n            if isinstance(key, slice):\\n                if self.offset is not None and key.step is not None:\\n                    new_offset = key.step * self.offset\\n                else:\\n                    new_offset = self.offset\\n            result = arr_idx[key]\\n            if result.ndim > 1:\\n                return result\\n            return self._simple_new(result, self.name, new_offset, self.tz)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, key):\\n        arr_idx = self.view(np.ndarray)\\n        if np.isscalar(key):\\n            val = arr_idx[key]\\n            return Timestamp(val, offset=self.offset, tz=self.tz)\\n        else:\\n            if com._is_bool_indexer(key):\\n                key = np.asarray(key)\\n                key = lib.maybe_booleans_to_slice(key.view(np.uint8))\\n            new_offset = None\\n            if isinstance(key, slice):\\n                if self.offset is not None and key.step is not None:\\n                    new_offset = key.step * self.offset\\n                else:\\n                    new_offset = self.offset\\n            result = arr_idx[key]\\n            if result.ndim > 1:\\n                return result\\n            return self._simple_new(result, self.name, new_offset, self.tz)"
  },
  {
    "code": "def execute(self, operation, parameters=None):\\n        sql = _bind_parameters(operation,\\n                               parameters) if parameters else operation\\n        self.job_id = self.run_query(sql)\\n    def executemany(self, operation, seq_of_parameters):\\n        for parameters in seq_of_parameters:\\n            self.execute(operation, parameters)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(self, operation, parameters=None):\\n        sql = _bind_parameters(operation,\\n                               parameters) if parameters else operation\\n        self.job_id = self.run_query(sql)\\n    def executemany(self, operation, seq_of_parameters):\\n        for parameters in seq_of_parameters:\\n            self.execute(operation, parameters)"
  },
  {
    "code": "def data(self, convert_dates=True, convert_categoricals=True, index=None,\\n             convert_missing=False, preserve_dtypes=True):\\n        self._missing_values = convert_missing\\n        if self._data_read:\\n            raise Exception(\"Data has already been read.\")\\n        self._data_read = True\\n        if self.format_version >= 117:\\n            self._read_strls()\\n        count = self.nobs\\n        dtype = []  \\n        for i, typ in enumerate(self.typlist):\\n            if typ in self.NUMPY_TYPE_MAP:\\n                dtype.append(('s' + str(i), self.NUMPY_TYPE_MAP[typ]))\\n            else:\\n                dtype.append(('s' + str(i), 'S' + str(typ)))\\n        dtype = np.dtype(dtype)\\n        read_len = count * dtype.itemsize\\n        self.path_or_buf.seek(self.data_location)\\n        data = np.frombuffer(self.path_or_buf.read(read_len),dtype=dtype,count=count)\\n        self._data_read = True\\n        if convert_categoricals:\\n            self._read_value_labels()\\n        if len(data)==0:\\n            data = DataFrame(columns=self.varlist, index=index)\\n        else:\\n            data = DataFrame.from_records(data, index=index)\\n            data.columns = self.varlist\\n        for col, typ in zip(data, self.typlist):\\n            if type(typ) is int:\\n                data[col] = data[col].apply(self._null_terminate, convert_dtype=True,)\\n        cols_ = np.where(self.dtyplist)[0]\\n        index = data.index\\n        requires_type_conversion = False\\n        data_formatted = []\\n        for i in cols_:\\n            if self.dtyplist[i] is not None:\\n                col = data.columns[i]\\n                dtype = data[col].dtype\\n                if (dtype != np.dtype(object)) and (dtype != self.dtyplist[i]):\\n                    requires_type_conversion = True\\n                    data_formatted.append((col, Series(data[col], index, self.dtyplist[i])))\\n                else:\\n                    data_formatted.append((col, data[col]))\\n        if requires_type_conversion:\\n            data = DataFrame.from_items(data_formatted)\\n        del data_formatted\\n        for i, colname in enumerate(data):\\n            fmt = self.typlist[i]\\n            if fmt not in self.VALID_RANGE:\\n                continue\\n            nmin, nmax = self.VALID_RANGE[fmt]\\n            series = data[colname]\\n            missing = np.logical_or(series < nmin, series > nmax)\\n            if not missing.any():\\n                continue\\n            if self._missing_values:  \\n                missing_loc = np.argwhere(missing)\\n                umissing, umissing_loc = np.unique(series[missing],\\n                                                   return_inverse=True)\\n                replacement = Series(series, dtype=np.object)\\n                for i, um in enumerate(umissing):\\n                    missing_value = StataMissingValue(um)\\n                    loc = missing_loc[umissing_loc == i]\\n                    replacement.iloc[loc] = missing_value\\n            else:  \\n                dtype = series.dtype\\n                if dtype not in (np.float32, np.float64):\\n                    dtype = np.float64\\n                replacement = Series(series, dtype=dtype)\\n                replacement[missing] = np.nan\\n            data[colname] = replacement\\n        if convert_dates:\\n            cols = np.where(lmap(lambda x: x in _date_formats,\\n                                 self.fmtlist))[0]\\n            for i in cols:\\n                col = data.columns[i]\\n                data[col] = _stata_elapsed_date_to_datetime_vec(data[col], self.fmtlist[i])\\n        if convert_categoricals:\\n            cols = np.where(\\n                lmap(lambda x: x in compat.iterkeys(self.value_label_dict),\\n                     self.lbllist)\\n            )[0]\\n            for i in cols:\\n                col = data.columns[i]\\n                labeled_data = np.copy(data[col])\\n                labeled_data = labeled_data.astype(object)\\n                for k, v in compat.iteritems(\\n                        self.value_label_dict[self.lbllist[i]]):\\n                    labeled_data[(data[col] == k).values] = v\\n                data[col] = Categorical.from_array(labeled_data)\\n        if not preserve_dtypes:\\n            retyped_data = []\\n            convert = False\\n            for col in data:\\n                dtype = data[col].dtype\\n                if dtype in (np.float16, np.float32):\\n                    dtype = np.float64\\n                    convert = True\\n                elif dtype in (np.int8, np.int16, np.int32):\\n                    dtype = np.int64\\n                    convert = True\\n                retyped_data.append((col, data[col].astype(dtype)))\\n            if convert:\\n                data = DataFrame.from_items(retyped_data)\\n        return data\\n    def data_label(self):\\n        return self.data_label",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def data(self, convert_dates=True, convert_categoricals=True, index=None,\\n             convert_missing=False, preserve_dtypes=True):\\n        self._missing_values = convert_missing\\n        if self._data_read:\\n            raise Exception(\"Data has already been read.\")\\n        self._data_read = True\\n        if self.format_version >= 117:\\n            self._read_strls()\\n        count = self.nobs\\n        dtype = []  \\n        for i, typ in enumerate(self.typlist):\\n            if typ in self.NUMPY_TYPE_MAP:\\n                dtype.append(('s' + str(i), self.NUMPY_TYPE_MAP[typ]))\\n            else:\\n                dtype.append(('s' + str(i), 'S' + str(typ)))\\n        dtype = np.dtype(dtype)\\n        read_len = count * dtype.itemsize\\n        self.path_or_buf.seek(self.data_location)\\n        data = np.frombuffer(self.path_or_buf.read(read_len),dtype=dtype,count=count)\\n        self._data_read = True\\n        if convert_categoricals:\\n            self._read_value_labels()\\n        if len(data)==0:\\n            data = DataFrame(columns=self.varlist, index=index)\\n        else:\\n            data = DataFrame.from_records(data, index=index)\\n            data.columns = self.varlist\\n        for col, typ in zip(data, self.typlist):\\n            if type(typ) is int:\\n                data[col] = data[col].apply(self._null_terminate, convert_dtype=True,)\\n        cols_ = np.where(self.dtyplist)[0]\\n        index = data.index\\n        requires_type_conversion = False\\n        data_formatted = []\\n        for i in cols_:\\n            if self.dtyplist[i] is not None:\\n                col = data.columns[i]\\n                dtype = data[col].dtype\\n                if (dtype != np.dtype(object)) and (dtype != self.dtyplist[i]):\\n                    requires_type_conversion = True\\n                    data_formatted.append((col, Series(data[col], index, self.dtyplist[i])))\\n                else:\\n                    data_formatted.append((col, data[col]))\\n        if requires_type_conversion:\\n            data = DataFrame.from_items(data_formatted)\\n        del data_formatted\\n        for i, colname in enumerate(data):\\n            fmt = self.typlist[i]\\n            if fmt not in self.VALID_RANGE:\\n                continue\\n            nmin, nmax = self.VALID_RANGE[fmt]\\n            series = data[colname]\\n            missing = np.logical_or(series < nmin, series > nmax)\\n            if not missing.any():\\n                continue\\n            if self._missing_values:  \\n                missing_loc = np.argwhere(missing)\\n                umissing, umissing_loc = np.unique(series[missing],\\n                                                   return_inverse=True)\\n                replacement = Series(series, dtype=np.object)\\n                for i, um in enumerate(umissing):\\n                    missing_value = StataMissingValue(um)\\n                    loc = missing_loc[umissing_loc == i]\\n                    replacement.iloc[loc] = missing_value\\n            else:  \\n                dtype = series.dtype\\n                if dtype not in (np.float32, np.float64):\\n                    dtype = np.float64\\n                replacement = Series(series, dtype=dtype)\\n                replacement[missing] = np.nan\\n            data[colname] = replacement\\n        if convert_dates:\\n            cols = np.where(lmap(lambda x: x in _date_formats,\\n                                 self.fmtlist))[0]\\n            for i in cols:\\n                col = data.columns[i]\\n                data[col] = _stata_elapsed_date_to_datetime_vec(data[col], self.fmtlist[i])\\n        if convert_categoricals:\\n            cols = np.where(\\n                lmap(lambda x: x in compat.iterkeys(self.value_label_dict),\\n                     self.lbllist)\\n            )[0]\\n            for i in cols:\\n                col = data.columns[i]\\n                labeled_data = np.copy(data[col])\\n                labeled_data = labeled_data.astype(object)\\n                for k, v in compat.iteritems(\\n                        self.value_label_dict[self.lbllist[i]]):\\n                    labeled_data[(data[col] == k).values] = v\\n                data[col] = Categorical.from_array(labeled_data)\\n        if not preserve_dtypes:\\n            retyped_data = []\\n            convert = False\\n            for col in data:\\n                dtype = data[col].dtype\\n                if dtype in (np.float16, np.float32):\\n                    dtype = np.float64\\n                    convert = True\\n                elif dtype in (np.int8, np.int16, np.int32):\\n                    dtype = np.int64\\n                    convert = True\\n                retyped_data.append((col, data[col].astype(dtype)))\\n            if convert:\\n                data = DataFrame.from_items(retyped_data)\\n        return data\\n    def data_label(self):\\n        return self.data_label"
  },
  {
    "code": "def _round_up(self, prec, expdiff, context):\\n        tmp = Decimal( (self._sign, self._int[:prec], self._exp - expdiff) )\\n        for digit in self._int[prec:]:\\n            if digit != 0:\\n                tmp = tmp._increment(round=1, context=context)\\n                if len(tmp._int) > prec:\\n                    return Decimal( (tmp._sign, tmp._int[:-1], tmp._exp + 1))\\n                else:\\n                    return tmp\\n        return tmp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _round_up(self, prec):\\n        return -self._round_down(prec)"
  },
  {
    "code": "def find_entry(self, entryid):\\n        results = []\\n        for name in self.conn.listNetworks():\\n            entry = self.conn.networkLookupByName(name)\\n            results.append(entry)\\n        for name in self.conn.listDefinedNetworks():\\n            entry = self.conn.networkLookupByName(name)\\n            results.append(entry)\\n        if entryid == -1:\\n            return results\\n        for entry in results:\\n            if entry.name() == entryid:\\n                return entry\\n        raise EntryNotFound(\"network %s not found\" % entryid)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "virt_net: idempotency of create/stop actions (#53276)\\n\\nCurrently, if we try to stop or start a network two time in a row, the\\nsecond call will fail. With this patch:\\n\\n- we don't recreate a network, if it exists\\n- we only stop a network if it's active, and so we avoid an exception\\n  saying the network is not active",
    "fixed_code": "def find_entry(self, entryid):\\n        if entryid == -1:  \\n            names = self.conn.listNetworks() + self.conn.listDefinedNetworks()\\n            return [self.conn.networkLookupByName(n) for n in names]\\n        try:\\n            return self.conn.networkLookupByName(entryid)\\n        except libvirt.libvirtError as e:\\n            if e.get_error_code() == libvirt.VIR_ERR_NO_NETWORK:\\n                raise EntryNotFound(\"network %s not found\" % entryid)\\n            raise"
  },
  {
    "code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    join_index = left_ax\\n    left_indexer = None\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_tmp, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax,\\n                                    sort=sort)\\n        if sort:\\n            left_indexer = left_tmp\\n            join_index = left_ax.take(left_indexer)\\n    else:\\n        jkey = join_keys[0]\\n        if sort:\\n            left_indexer, right_indexer = \\\\n                _get_single_indexer(jkey, right_ax, sort=sort)\\n            join_index = left_ax.take(left_indexer)\\n        else:\\n            right_indexer = right_ax.get_indexer(jkey)\\n    return join_index, left_indexer, right_indexer",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: left join on index with multiple matches (GH5391)",
    "fixed_code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    else:\\n        return left_ax, None, right_indexer"
  },
  {
    "code": "def join(value, arg, autoescape=None):\\n\\tvalue = map(force_text, value)\\n\\tif autoescape:\\n\\t\\tvalue = [conditional_escape(v) for v in value]\\n\\ttry:\\n\\t\\tdata = conditional_escape(arg).join(value)\\n\\texcept AttributeError:  \\n\\t\\treturn value\\n\\treturn mark_safe(data)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #24464 -- Made built-in HTML template filter functions escape their input by default.\\n\\nThis may cause some backwards compatibility issues, but may also\\nresolve security issues in third party projects that fail to heed warnings\\nin our documentation.\\n\\nThanks Markus Holtermann for help with tests and docs.",
    "fixed_code": "def join(value, arg, autoescape=True):\\n\\tvalue = map(force_text, value)\\n\\tif autoescape:\\n\\t\\tvalue = [conditional_escape(v) for v in value]\\n\\ttry:\\n\\t\\tdata = conditional_escape(arg).join(value)\\n\\texcept AttributeError:  \\n\\t\\treturn value\\n\\treturn mark_safe(data)"
  },
  {
    "code": "def _dual_gap(emp_cov, precision_, alpha):\\n    gap = np.sum(emp_cov * precision_)\\n    gap -= precision_.shape[0]\\n    gap += alpha*(np.abs(precision_).sum()\\n                        - np.abs(np.diag(precision_)).sum())\\n    return gap",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _dual_gap(emp_cov, precision_, alpha):\\n    gap = np.sum(emp_cov * precision_)\\n    gap -= precision_.shape[0]\\n    gap += alpha*(np.abs(precision_).sum()\\n                        - np.abs(np.diag(precision_)).sum())\\n    return gap"
  },
  {
    "code": "def _get_delegate(self):\\n        finder = ImpImporter(self.filename)\\n        spec = _get_spec(finder, '__init__')\\n        return spec.loader",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_delegate(self):\\n        finder = ImpImporter(self.filename)\\n        spec = _get_spec(finder, '__init__')\\n        return spec.loader"
  },
  {
    "code": "def _construct_instance(Estimator):\\n    required_parameters = getattr(Estimator, \"_required_parameters\", [])\\n    if len(required_parameters):\\n        if required_parameters in ([\"estimator\"], [\"base_estimator\"]):\\n            if issubclass(Estimator, RegressorMixin):\\n                estimator = Estimator(Ridge())\\n            else:\\n                estimator = Estimator(LinearDiscriminantAnalysis())\\n        else:\\n            raise SkipTest(\"Can't instantiate estimator {} which requires \"\\n                           \"parameters {}\".format(Estimator.__name__,\\n                                                  required_parameters))\\n    else:\\n        estimator = Estimator()\\n    return estimator",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST enable to run common test on stacking and voting estimators (#18045)",
    "fixed_code": "def _construct_instance(Estimator):\\n    required_parameters = getattr(Estimator, \"_required_parameters\", [])\\n    if len(required_parameters):\\n        if required_parameters in ([\"estimator\"], [\"base_estimator\"]):\\n            if issubclass(Estimator, RegressorMixin):\\n                estimator = Estimator(Ridge())\\n            else:\\n                estimator = Estimator(LinearDiscriminantAnalysis())\\n        elif required_parameters in (['estimators'],):\\n            if issubclass(Estimator, RegressorMixin):\\n                estimator = Estimator(estimators=[\\n                    (\"est1\", Ridge(alpha=0.1)),\\n                    (\"est2\", Ridge(alpha=1))\\n                ])\\n            else:\\n                estimator = Estimator(estimators=[\\n                    (\"est1\", LogisticRegression(C=0.1)),\\n                    (\"est2\", LogisticRegression(C=1))\\n                ])\\n        else:\\n            msg = (f\"Can't instantiate estimator {Estimator.__name__} \"\\n                   f\"parameters {required_parameters}\")\\n            warnings.warn(msg, SkipTestWarning)\\n            raise SkipTest(msg)\\n    else:\\n        estimator = Estimator()\\n    return estimator"
  },
  {
    "code": "def find_package_modules(self, package, package_dir):\\n        self.check_package(package, package_dir)\\n        module_files = glob.glob(os.path.join(glob.escape(package_dir), \"*.py\"))\\n        modules = []\\n        setup_script = os.path.abspath(self.distribution.script_name)\\n        for f in module_files:\\n            abs_f = os.path.abspath(f)\\n            if abs_f != setup_script:\\n                module = os.path.splitext(os.path.basename(f))[0]\\n                modules.append((package, module, f))\\n            else:\\n                self.debug_print(\"excluding %s\" % setup_script)\\n        return modules",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def find_package_modules(self, package, package_dir):\\n        self.check_package(package, package_dir)\\n        module_files = glob.glob(os.path.join(glob.escape(package_dir), \"*.py\"))\\n        modules = []\\n        setup_script = os.path.abspath(self.distribution.script_name)\\n        for f in module_files:\\n            abs_f = os.path.abspath(f)\\n            if abs_f != setup_script:\\n                module = os.path.splitext(os.path.basename(f))[0]\\n                modules.append((package, module, f))\\n            else:\\n                self.debug_print(\"excluding %s\" % setup_script)\\n        return modules"
  },
  {
    "code": "def replace_metacharacters(pattern):\\n    return re.sub(\\n        r'((?:^|(?<!\\\\))(?:\\\\\\\\)*)(\\\\?)([?^$])',\\n        lambda m: m[1] + m[3] if m[2] else m[1],\\n        pattern,\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #32499 -- Escaped additional metacharacters in simplify_regex().",
    "fixed_code": "def replace_metacharacters(pattern):\\n    return re.sub(\\n        r'((?:^|(?<!\\\\))(?:\\\\\\\\)*)(\\\\?)([?*+^$]|\\\\[bBAZ])',\\n        lambda m: m[1] + m[3] if m[2] else m[1],\\n        pattern,\\n    )"
  },
  {
    "code": "def delete_group_member(fmg, paramgram):\\n    response = (-100000, {\"msg\": \"Illegal or malformed paramgram discovered. System Exception\"})\\n    url = \"\"\\n    device_member_list = paramgram[\"grp_members\"].replace(' ', '')\\n    device_member_list = device_member_list.split(',')\\n    for dev_name in device_member_list:\\n        datagram = {'name': dev_name, 'vdom': paramgram[\"vdom\"]}\\n        url = '/dvmdb/adom/{adom}/group/{grp_name}/object member'.format(adom=paramgram[\"adom\"],\\n                                                                         grp_name=paramgram[\"grp_name\"])\\n        response = fmg.delete(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Auto Commit for: fmgr_device_group (#52784)",
    "fixed_code": "def delete_group_member(fmgr, paramgram):\\n    response = DEFAULT_RESULT_OBJ\\n    url = \"\"\\n    device_member_list = paramgram[\"grp_members\"].replace(' ', '')\\n    device_member_list = device_member_list.split(',')\\n    for dev_name in device_member_list:\\n        datagram = {'name': dev_name, 'vdom': paramgram[\"vdom\"]}\\n        url = '/dvmdb/adom/{adom}/group/{grp_name}/object member'.format(adom=paramgram[\"adom\"],\\n                                                                         grp_name=paramgram[\"grp_name\"])\\n        response = fmgr.process_request(url, datagram, FMGRMethods.DELETE)\\n    return response"
  },
  {
    "code": "def __init__(self, job_name, job_definition, job_queue, overrides, max_retries=4200,\\n                 aws_conn_id=None, region_name=None, **kwargs):\\n        super().__init__(**kwargs)\\n        self.job_name = job_name\\n        self.aws_conn_id = aws_conn_id\\n        self.region_name = region_name\\n        self.job_definition = job_definition\\n        self.job_queue = job_queue\\n        self.overrides = overrides\\n        self.max_retries = max_retries\\n        self.jobId = None  \\n        self.jobName = None  \\n        self.hook = self.get_hook()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3388] Add support to Array Jobs for AWS Batch Operator (#6153)",
    "fixed_code": "def __init__(self, job_name, job_definition, job_queue, overrides, array_properties=None,\\n                 max_retries=4200, aws_conn_id=None, region_name=None, **kwargs):\\n        super().__init__(**kwargs)\\n        self.job_name = job_name\\n        self.aws_conn_id = aws_conn_id\\n        self.region_name = region_name\\n        self.job_definition = job_definition\\n        self.job_queue = job_queue\\n        self.overrides = overrides\\n        self.array_properties = array_properties\\n        self.max_retries = max_retries\\n        self.jobId = None  \\n        self.jobName = None  \\n        self.hook = self.get_hook()"
  },
  {
    "code": "def readframes(self, nframes):\\n        if self._data_seek_needed:\\n            self._data_chunk.seek(0, 0)\\n            pos = self._soundpos * self._framesize\\n            if pos:\\n                self._data_chunk.seek(pos, 0)\\n            self._data_seek_needed = 0\\n        if nframes == 0:\\n            return ''\\n        if self._sampwidth > 1 and big_endian:\\n            import array\\n            chunk = self._data_chunk\\n            data = array.array(_array_fmts[self._sampwidth])\\n            nitems = nframes * self._nchannels\\n            if nitems * self._sampwidth > chunk.chunksize - chunk.size_read:\\n                nitems = (chunk.chunksize - chunk.size_read) / self._sampwidth\\n            data.fromfile(chunk.file.file, nitems)\\n            chunk.size_read = chunk.size_read + nitems * self._sampwidth\\n            chunk = chunk.file\\n            chunk.size_read = chunk.size_read + nitems * self._sampwidth\\n            data.byteswap()\\n            data = data.tostring()\\n        else:\\n            data = self._data_chunk.read(nframes * self._framesize)\\n        if self._convert and data:\\n            data = self._convert(data)\\n        self._soundpos = self._soundpos + len(data) / (self._nchannels * self._sampwidth)\\n        return data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "The first batch of changes recommended by the fixdiv tool.  These are mostly changes of / operators into //.  Once or twice I did more or less than recommended.",
    "fixed_code": "def readframes(self, nframes):\\n        if self._data_seek_needed:\\n            self._data_chunk.seek(0, 0)\\n            pos = self._soundpos * self._framesize\\n            if pos:\\n                self._data_chunk.seek(pos, 0)\\n            self._data_seek_needed = 0\\n        if nframes == 0:\\n            return ''\\n        if self._sampwidth > 1 and big_endian:\\n            import array\\n            chunk = self._data_chunk\\n            data = array.array(_array_fmts[self._sampwidth])\\n            nitems = nframes * self._nchannels\\n            if nitems * self._sampwidth > chunk.chunksize - chunk.size_read:\\n                nitems = (chunk.chunksize - chunk.size_read) / self._sampwidth\\n            data.fromfile(chunk.file.file, nitems)\\n            chunk.size_read = chunk.size_read + nitems * self._sampwidth\\n            chunk = chunk.file\\n            chunk.size_read = chunk.size_read + nitems * self._sampwidth\\n            data.byteswap()\\n            data = data.tostring()\\n        else:\\n            data = self._data_chunk.read(nframes * self._framesize)\\n        if self._convert and data:\\n            data = self._convert(data)\\n        self._soundpos = self._soundpos + len(data) // (self._nchannels * self._sampwidth)\\n        return data"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        name=dict(required=False, type=\"str\"),\\n        external=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        extended_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        address_list=dict(required=False, type=\"list\"),\\n        address_list_blocked_address=dict(required=False, type=\"str\"),\\n        address_list_blocked_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        address_list_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        address_list_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        address_list_trusted_address=dict(required=False, type=\"str\"),\\n        constraint=dict(required=False, type=\"list\"),\\n        constraint_content_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_content_length_length=dict(required=False, type=\"int\"),\\n        constraint_content_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_content_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_content_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_address=dict(required=False, type=\"str\"),\\n        constraint_exception_content_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_header_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_hostname=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_line_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_malformed=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_cookie=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_header_line=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_range_segment=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_url_param=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_method=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_param_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_pattern=dict(required=False, type=\"str\"),\\n        constraint_exception_regex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_url_param_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_version=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_header_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_header_length_length=dict(required=False, type=\"int\"),\\n        constraint_header_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_header_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_header_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_hostname_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_hostname_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_hostname_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_hostname_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_line_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_line_length_length=dict(required=False, type=\"int\"),\\n        constraint_line_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_line_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_line_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_malformed_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_malformed_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_malformed_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_malformed_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_cookie_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_cookie_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_cookie_max_cookie=dict(required=False, type=\"int\"),\\n        constraint_max_cookie_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_cookie_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_header_line_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_header_line_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_header_line_max_header_line=dict(required=False, type=\"int\"),\\n        constraint_max_header_line_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_header_line_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_range_segment_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_range_segment_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_range_segment_max_range_segment=dict(required=False, type=\"int\"),\\n        constraint_max_range_segment_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_range_segment_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_url_param_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_url_param_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_url_param_max_url_param=dict(required=False, type=\"int\"),\\n        constraint_max_url_param_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_url_param_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_method_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_method_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_method_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_method_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_param_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_param_length_length=dict(required=False, type=\"int\"),\\n        constraint_param_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_param_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_param_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_url_param_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_url_param_length_length=dict(required=False, type=\"int\"),\\n        constraint_url_param_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_url_param_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_url_param_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_version_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_version_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_version_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_version_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        method=dict(required=False, type=\"list\"),\\n        method_default_allowed_methods=dict(required=False, type=\"str\", choices=[\"delete\",\\n                                                                                 \"get\",\\n                                                                                 \"head\",\\n                                                                                 \"options\",\\n                                                                                 \"post\",\\n                                                                                 \"put\",\\n                                                                                 \"trace\",\\n                                                                                 \"others\",\\n                                                                                 \"connect\"]),\\n        method_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        method_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        method_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        method_method_policy_address=dict(required=False, type=\"str\"),\\n        method_method_policy_allowed_methods=dict(required=False, type=\"str\", choices=[\"delete\",\\n                                                                                       \"get\",\\n                                                                                       \"head\",\\n                                                                                       \"options\",\\n                                                                                       \"post\",\\n                                                                                       \"put\",\\n                                                                                       \"trace\",\\n                                                                                       \"others\",\\n                                                                                       \"connect\"]),\\n        method_method_policy_pattern=dict(required=False, type=\"str\"),\\n        method_method_policy_regex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature=dict(required=False, type=\"list\"),\\n        signature_credit_card_detection_threshold=dict(required=False, type=\"int\"),\\n        signature_disabled_signature=dict(required=False, type=\"str\"),\\n        signature_disabled_sub_class=dict(required=False, type=\"str\"),\\n        signature_custom_signature_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"erase\"]),\\n        signature_custom_signature_case_sensitivity=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_custom_signature_direction=dict(required=False, type=\"str\", choices=[\"request\", \"response\"]),\\n        signature_custom_signature_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_custom_signature_name=dict(required=False, type=\"str\"),\\n        signature_custom_signature_pattern=dict(required=False, type=\"str\"),\\n        signature_custom_signature_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        signature_custom_signature_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_custom_signature_target=dict(required=False, type=\"str\", choices=[\"arg\",\\n                                                                                    \"arg-name\",\\n                                                                                    \"req-body\",\\n                                                                                    \"req-cookie\",\\n                                                                                    \"req-cookie-name\",\\n                                                                                    \"req-filename\",\\n                                                                                    \"req-header\",\\n                                                                                    \"req-header-name\",\\n                                                                                    \"req-raw-uri\",\\n                                                                                    \"req-uri\",\\n                                                                                    \"resp-body\",\\n                                                                                    \"resp-hdr\",\\n                                                                                    \"resp-status\"]),\\n        signature_main_class_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"erase\"]),\\n        signature_main_class_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_main_class_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        signature_main_class_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access=dict(required=False, type=\"list\"),\\n        url_access_action=dict(required=False, type=\"str\", choices=[\"bypass\", \"permit\", \"block\"]),\\n        url_access_address=dict(required=False, type=\"str\"),\\n        url_access_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        url_access_access_pattern_negate=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access_access_pattern_pattern=dict(required=False, type=\"str\"),\\n        url_access_access_pattern_regex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access_access_pattern_srcaddr=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"name\": module.params[\"name\"],\\n        \"external\": module.params[\"external\"],\\n        \"extended-log\": module.params[\"extended_log\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"address-list\": {\\n            \"blocked-address\": module.params[\"address_list_blocked_address\"],\\n            \"blocked-log\": module.params[\"address_list_blocked_log\"],\\n            \"severity\": module.params[\"address_list_severity\"],\\n            \"status\": module.params[\"address_list_status\"],\\n            \"trusted-address\": module.params[\"address_list_trusted_address\"],\\n        },\\n        \"constraint\": {\\n            \"content-length\": {\\n                \"action\": module.params[\"constraint_content_length_action\"],\\n                \"length\": module.params[\"constraint_content_length_length\"],\\n                \"log\": module.params[\"constraint_content_length_log\"],\\n                \"severity\": module.params[\"constraint_content_length_severity\"],\\n                \"status\": module.params[\"constraint_content_length_status\"],\\n            },\\n            \"exception\": {\\n                \"address\": module.params[\"constraint_exception_address\"],\\n                \"content-length\": module.params[\"constraint_exception_content_length\"],\\n                \"header-length\": module.params[\"constraint_exception_header_length\"],\\n                \"hostname\": module.params[\"constraint_exception_hostname\"],\\n                \"line-length\": module.params[\"constraint_exception_line_length\"],\\n                \"malformed\": module.params[\"constraint_exception_malformed\"],\\n                \"max-cookie\": module.params[\"constraint_exception_max_cookie\"],\\n                \"max-header-line\": module.params[\"constraint_exception_max_header_line\"],\\n                \"max-range-segment\": module.params[\"constraint_exception_max_range_segment\"],\\n                \"max-url-param\": module.params[\"constraint_exception_max_url_param\"],\\n                \"method\": module.params[\"constraint_exception_method\"],\\n                \"param-length\": module.params[\"constraint_exception_param_length\"],\\n                \"pattern\": module.params[\"constraint_exception_pattern\"],\\n                \"regex\": module.params[\"constraint_exception_regex\"],\\n                \"url-param-length\": module.params[\"constraint_exception_url_param_length\"],\\n                \"version\": module.params[\"constraint_exception_version\"],\\n            },\\n            \"header-length\": {\\n                \"action\": module.params[\"constraint_header_length_action\"],\\n                \"length\": module.params[\"constraint_header_length_length\"],\\n                \"log\": module.params[\"constraint_header_length_log\"],\\n                \"severity\": module.params[\"constraint_header_length_severity\"],\\n                \"status\": module.params[\"constraint_header_length_status\"],\\n            },\\n            \"hostname\": {\\n                \"action\": module.params[\"constraint_hostname_action\"],\\n                \"log\": module.params[\"constraint_hostname_log\"],\\n                \"severity\": module.params[\"constraint_hostname_severity\"],\\n                \"status\": module.params[\"constraint_hostname_status\"],\\n            },\\n            \"line-length\": {\\n                \"action\": module.params[\"constraint_line_length_action\"],\\n                \"length\": module.params[\"constraint_line_length_length\"],\\n                \"log\": module.params[\"constraint_line_length_log\"],\\n                \"severity\": module.params[\"constraint_line_length_severity\"],\\n                \"status\": module.params[\"constraint_line_length_status\"],\\n            },\\n            \"malformed\": {\\n                \"action\": module.params[\"constraint_malformed_action\"],\\n                \"log\": module.params[\"constraint_malformed_log\"],\\n                \"severity\": module.params[\"constraint_malformed_severity\"],\\n                \"status\": module.params[\"constraint_malformed_status\"],\\n            },\\n            \"max-cookie\": {\\n                \"action\": module.params[\"constraint_max_cookie_action\"],\\n                \"log\": module.params[\"constraint_max_cookie_log\"],\\n                \"max-cookie\": module.params[\"constraint_max_cookie_max_cookie\"],\\n                \"severity\": module.params[\"constraint_max_cookie_severity\"],\\n                \"status\": module.params[\"constraint_max_cookie_status\"],\\n            },\\n            \"max-header-line\": {\\n                \"action\": module.params[\"constraint_max_header_line_action\"],\\n                \"log\": module.params[\"constraint_max_header_line_log\"],\\n                \"max-header-line\": module.params[\"constraint_max_header_line_max_header_line\"],\\n                \"severity\": module.params[\"constraint_max_header_line_severity\"],\\n                \"status\": module.params[\"constraint_max_header_line_status\"],\\n            },\\n            \"max-range-segment\": {\\n                \"action\": module.params[\"constraint_max_range_segment_action\"],\\n                \"log\": module.params[\"constraint_max_range_segment_log\"],\\n                \"max-range-segment\": module.params[\"constraint_max_range_segment_max_range_segment\"],\\n                \"severity\": module.params[\"constraint_max_range_segment_severity\"],\\n                \"status\": module.params[\"constraint_max_range_segment_status\"],\\n            },\\n            \"max-url-param\": {\\n                \"action\": module.params[\"constraint_max_url_param_action\"],\\n                \"log\": module.params[\"constraint_max_url_param_log\"],\\n                \"max-url-param\": module.params[\"constraint_max_url_param_max_url_param\"],\\n                \"severity\": module.params[\"constraint_max_url_param_severity\"],\\n                \"status\": module.params[\"constraint_max_url_param_status\"],\\n            },\\n            \"method\": {\\n                \"action\": module.params[\"constraint_method_action\"],\\n                \"log\": module.params[\"constraint_method_log\"],\\n                \"severity\": module.params[\"constraint_method_severity\"],\\n                \"status\": module.params[\"constraint_method_status\"],\\n            },\\n            \"param-length\": {\\n                \"action\": module.params[\"constraint_param_length_action\"],\\n                \"length\": module.params[\"constraint_param_length_length\"],\\n                \"log\": module.params[\"constraint_param_length_log\"],\\n                \"severity\": module.params[\"constraint_param_length_severity\"],\\n                \"status\": module.params[\"constraint_param_length_status\"],\\n            },\\n            \"url-param-length\": {\\n                \"action\": module.params[\"constraint_url_param_length_action\"],\\n                \"length\": module.params[\"constraint_url_param_length_length\"],\\n                \"log\": module.params[\"constraint_url_param_length_log\"],\\n                \"severity\": module.params[\"constraint_url_param_length_severity\"],\\n                \"status\": module.params[\"constraint_url_param_length_status\"],\\n            },\\n            \"version\": {\\n                \"action\": module.params[\"constraint_version_action\"],\\n                \"log\": module.params[\"constraint_version_log\"],\\n                \"severity\": module.params[\"constraint_version_severity\"],\\n                \"status\": module.params[\"constraint_version_status\"],\\n            },\\n        },\\n        \"method\": {\\n            \"default-allowed-methods\": module.params[\"method_default_allowed_methods\"],\\n            \"log\": module.params[\"method_log\"],\\n            \"severity\": module.params[\"method_severity\"],\\n            \"status\": module.params[\"method_status\"],\\n            \"method-policy\": {\\n                \"address\": module.params[\"method_method_policy_address\"],\\n                \"allowed-methods\": module.params[\"method_method_policy_allowed_methods\"],\\n                \"pattern\": module.params[\"method_method_policy_pattern\"],\\n                \"regex\": module.params[\"method_method_policy_regex\"],\\n            },\\n        },\\n        \"signature\": {\\n            \"credit-card-detection-threshold\": module.params[\"signature_credit_card_detection_threshold\"],\\n            \"disabled-signature\": module.params[\"signature_disabled_signature\"],\\n            \"disabled-sub-class\": module.params[\"signature_disabled_sub_class\"],\\n            \"custom-signature\": {\\n                \"action\": module.params[\"signature_custom_signature_action\"],\\n                \"case-sensitivity\": module.params[\"signature_custom_signature_case_sensitivity\"],\\n                \"direction\": module.params[\"signature_custom_signature_direction\"],\\n                \"log\": module.params[\"signature_custom_signature_log\"],\\n                \"name\": module.params[\"signature_custom_signature_name\"],\\n                \"pattern\": module.params[\"signature_custom_signature_pattern\"],\\n                \"severity\": module.params[\"signature_custom_signature_severity\"],\\n                \"status\": module.params[\"signature_custom_signature_status\"],\\n                \"target\": module.params[\"signature_custom_signature_target\"],\\n            },\\n            \"main-class\": {\\n                \"action\": module.params[\"signature_main_class_action\"],\\n                \"log\": module.params[\"signature_main_class_log\"],\\n                \"severity\": module.params[\"signature_main_class_severity\"],\\n                \"status\": module.params[\"signature_main_class_status\"],\\n            },\\n        },\\n        \"url-access\": {\\n            \"action\": module.params[\"url_access_action\"],\\n            \"address\": module.params[\"url_access_address\"],\\n            \"log\": module.params[\"url_access_log\"],\\n            \"severity\": module.params[\"url_access_severity\"],\\n            \"access-pattern\": {\\n                \"negate\": module.params[\"url_access_access_pattern_negate\"],\\n                \"pattern\": module.params[\"url_access_access_pattern_pattern\"],\\n                \"regex\": module.params[\"url_access_access_pattern_regex\"],\\n                \"srcaddr\": module.params[\"url_access_access_pattern_srcaddr\"],\\n            }\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['address-list', 'constraint', 'method', 'signature', 'url-access']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_waf_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        name=dict(required=False, type=\"str\"),\\n        external=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        extended_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        comment=dict(required=False, type=\"str\"),\\n        address_list=dict(required=False, type=\"list\"),\\n        address_list_blocked_address=dict(required=False, type=\"str\"),\\n        address_list_blocked_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        address_list_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        address_list_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        address_list_trusted_address=dict(required=False, type=\"str\"),\\n        constraint=dict(required=False, type=\"list\"),\\n        constraint_content_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_content_length_length=dict(required=False, type=\"int\"),\\n        constraint_content_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_content_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_content_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_address=dict(required=False, type=\"str\"),\\n        constraint_exception_content_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_header_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_hostname=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_line_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_malformed=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_cookie=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_header_line=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_range_segment=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_max_url_param=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_method=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_param_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_pattern=dict(required=False, type=\"str\"),\\n        constraint_exception_regex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_url_param_length=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_exception_version=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_header_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_header_length_length=dict(required=False, type=\"int\"),\\n        constraint_header_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_header_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_header_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_hostname_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_hostname_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_hostname_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_hostname_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_line_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_line_length_length=dict(required=False, type=\"int\"),\\n        constraint_line_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_line_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_line_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_malformed_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_malformed_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_malformed_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_malformed_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_cookie_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_cookie_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_cookie_max_cookie=dict(required=False, type=\"int\"),\\n        constraint_max_cookie_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_cookie_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_header_line_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_header_line_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_header_line_max_header_line=dict(required=False, type=\"int\"),\\n        constraint_max_header_line_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_header_line_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_range_segment_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_range_segment_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_range_segment_max_range_segment=dict(required=False, type=\"int\"),\\n        constraint_max_range_segment_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_range_segment_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_url_param_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_max_url_param_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_max_url_param_max_url_param=dict(required=False, type=\"int\"),\\n        constraint_max_url_param_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_max_url_param_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_method_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_method_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_method_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_method_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_param_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_param_length_length=dict(required=False, type=\"int\"),\\n        constraint_param_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_param_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_param_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_url_param_length_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_url_param_length_length=dict(required=False, type=\"int\"),\\n        constraint_url_param_length_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_url_param_length_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_url_param_length_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_version_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\"]),\\n        constraint_version_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        constraint_version_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        constraint_version_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        method=dict(required=False, type=\"list\"),\\n        method_default_allowed_methods=dict(required=False, type=\"str\", choices=[\"delete\",\\n                                                                                 \"get\",\\n                                                                                 \"head\",\\n                                                                                 \"options\",\\n                                                                                 \"post\",\\n                                                                                 \"put\",\\n                                                                                 \"trace\",\\n                                                                                 \"others\",\\n                                                                                 \"connect\"]),\\n        method_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        method_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        method_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        method_method_policy_address=dict(required=False, type=\"str\"),\\n        method_method_policy_allowed_methods=dict(required=False, type=\"str\", choices=[\"delete\",\\n                                                                                       \"get\",\\n                                                                                       \"head\",\\n                                                                                       \"options\",\\n                                                                                       \"post\",\\n                                                                                       \"put\",\\n                                                                                       \"trace\",\\n                                                                                       \"others\",\\n                                                                                       \"connect\"]),\\n        method_method_policy_pattern=dict(required=False, type=\"str\"),\\n        method_method_policy_regex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature=dict(required=False, type=\"list\"),\\n        signature_credit_card_detection_threshold=dict(required=False, type=\"int\"),\\n        signature_disabled_signature=dict(required=False, type=\"str\"),\\n        signature_disabled_sub_class=dict(required=False, type=\"str\"),\\n        signature_custom_signature_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"erase\"]),\\n        signature_custom_signature_case_sensitivity=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_custom_signature_direction=dict(required=False, type=\"str\", choices=[\"request\", \"response\"]),\\n        signature_custom_signature_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_custom_signature_name=dict(required=False, type=\"str\"),\\n        signature_custom_signature_pattern=dict(required=False, type=\"str\"),\\n        signature_custom_signature_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        signature_custom_signature_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_custom_signature_target=dict(required=False, type=\"str\", choices=[\"arg\",\\n                                                                                    \"arg-name\",\\n                                                                                    \"req-body\",\\n                                                                                    \"req-cookie\",\\n                                                                                    \"req-cookie-name\",\\n                                                                                    \"req-filename\",\\n                                                                                    \"req-header\",\\n                                                                                    \"req-header-name\",\\n                                                                                    \"req-raw-uri\",\\n                                                                                    \"req-uri\",\\n                                                                                    \"resp-body\",\\n                                                                                    \"resp-hdr\",\\n                                                                                    \"resp-status\"]),\\n        signature_main_class_action=dict(required=False, type=\"str\", choices=[\"allow\", \"block\", \"erase\"]),\\n        signature_main_class_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        signature_main_class_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        signature_main_class_status=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access=dict(required=False, type=\"list\"),\\n        url_access_action=dict(required=False, type=\"str\", choices=[\"bypass\", \"permit\", \"block\"]),\\n        url_access_address=dict(required=False, type=\"str\"),\\n        url_access_log=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access_severity=dict(required=False, type=\"str\", choices=[\"low\", \"medium\", \"high\"]),\\n        url_access_access_pattern_negate=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access_access_pattern_pattern=dict(required=False, type=\"str\"),\\n        url_access_access_pattern_regex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        url_access_access_pattern_srcaddr=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"mode\": module.params[\"mode\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"name\": module.params[\"name\"],\\n        \"external\": module.params[\"external\"],\\n        \"extended-log\": module.params[\"extended_log\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"address-list\": {\\n            \"blocked-address\": module.params[\"address_list_blocked_address\"],\\n            \"blocked-log\": module.params[\"address_list_blocked_log\"],\\n            \"severity\": module.params[\"address_list_severity\"],\\n            \"status\": module.params[\"address_list_status\"],\\n            \"trusted-address\": module.params[\"address_list_trusted_address\"],\\n        },\\n        \"constraint\": {\\n            \"content-length\": {\\n                \"action\": module.params[\"constraint_content_length_action\"],\\n                \"length\": module.params[\"constraint_content_length_length\"],\\n                \"log\": module.params[\"constraint_content_length_log\"],\\n                \"severity\": module.params[\"constraint_content_length_severity\"],\\n                \"status\": module.params[\"constraint_content_length_status\"],\\n            },\\n            \"exception\": {\\n                \"address\": module.params[\"constraint_exception_address\"],\\n                \"content-length\": module.params[\"constraint_exception_content_length\"],\\n                \"header-length\": module.params[\"constraint_exception_header_length\"],\\n                \"hostname\": module.params[\"constraint_exception_hostname\"],\\n                \"line-length\": module.params[\"constraint_exception_line_length\"],\\n                \"malformed\": module.params[\"constraint_exception_malformed\"],\\n                \"max-cookie\": module.params[\"constraint_exception_max_cookie\"],\\n                \"max-header-line\": module.params[\"constraint_exception_max_header_line\"],\\n                \"max-range-segment\": module.params[\"constraint_exception_max_range_segment\"],\\n                \"max-url-param\": module.params[\"constraint_exception_max_url_param\"],\\n                \"method\": module.params[\"constraint_exception_method\"],\\n                \"param-length\": module.params[\"constraint_exception_param_length\"],\\n                \"pattern\": module.params[\"constraint_exception_pattern\"],\\n                \"regex\": module.params[\"constraint_exception_regex\"],\\n                \"url-param-length\": module.params[\"constraint_exception_url_param_length\"],\\n                \"version\": module.params[\"constraint_exception_version\"],\\n            },\\n            \"header-length\": {\\n                \"action\": module.params[\"constraint_header_length_action\"],\\n                \"length\": module.params[\"constraint_header_length_length\"],\\n                \"log\": module.params[\"constraint_header_length_log\"],\\n                \"severity\": module.params[\"constraint_header_length_severity\"],\\n                \"status\": module.params[\"constraint_header_length_status\"],\\n            },\\n            \"hostname\": {\\n                \"action\": module.params[\"constraint_hostname_action\"],\\n                \"log\": module.params[\"constraint_hostname_log\"],\\n                \"severity\": module.params[\"constraint_hostname_severity\"],\\n                \"status\": module.params[\"constraint_hostname_status\"],\\n            },\\n            \"line-length\": {\\n                \"action\": module.params[\"constraint_line_length_action\"],\\n                \"length\": module.params[\"constraint_line_length_length\"],\\n                \"log\": module.params[\"constraint_line_length_log\"],\\n                \"severity\": module.params[\"constraint_line_length_severity\"],\\n                \"status\": module.params[\"constraint_line_length_status\"],\\n            },\\n            \"malformed\": {\\n                \"action\": module.params[\"constraint_malformed_action\"],\\n                \"log\": module.params[\"constraint_malformed_log\"],\\n                \"severity\": module.params[\"constraint_malformed_severity\"],\\n                \"status\": module.params[\"constraint_malformed_status\"],\\n            },\\n            \"max-cookie\": {\\n                \"action\": module.params[\"constraint_max_cookie_action\"],\\n                \"log\": module.params[\"constraint_max_cookie_log\"],\\n                \"max-cookie\": module.params[\"constraint_max_cookie_max_cookie\"],\\n                \"severity\": module.params[\"constraint_max_cookie_severity\"],\\n                \"status\": module.params[\"constraint_max_cookie_status\"],\\n            },\\n            \"max-header-line\": {\\n                \"action\": module.params[\"constraint_max_header_line_action\"],\\n                \"log\": module.params[\"constraint_max_header_line_log\"],\\n                \"max-header-line\": module.params[\"constraint_max_header_line_max_header_line\"],\\n                \"severity\": module.params[\"constraint_max_header_line_severity\"],\\n                \"status\": module.params[\"constraint_max_header_line_status\"],\\n            },\\n            \"max-range-segment\": {\\n                \"action\": module.params[\"constraint_max_range_segment_action\"],\\n                \"log\": module.params[\"constraint_max_range_segment_log\"],\\n                \"max-range-segment\": module.params[\"constraint_max_range_segment_max_range_segment\"],\\n                \"severity\": module.params[\"constraint_max_range_segment_severity\"],\\n                \"status\": module.params[\"constraint_max_range_segment_status\"],\\n            },\\n            \"max-url-param\": {\\n                \"action\": module.params[\"constraint_max_url_param_action\"],\\n                \"log\": module.params[\"constraint_max_url_param_log\"],\\n                \"max-url-param\": module.params[\"constraint_max_url_param_max_url_param\"],\\n                \"severity\": module.params[\"constraint_max_url_param_severity\"],\\n                \"status\": module.params[\"constraint_max_url_param_status\"],\\n            },\\n            \"method\": {\\n                \"action\": module.params[\"constraint_method_action\"],\\n                \"log\": module.params[\"constraint_method_log\"],\\n                \"severity\": module.params[\"constraint_method_severity\"],\\n                \"status\": module.params[\"constraint_method_status\"],\\n            },\\n            \"param-length\": {\\n                \"action\": module.params[\"constraint_param_length_action\"],\\n                \"length\": module.params[\"constraint_param_length_length\"],\\n                \"log\": module.params[\"constraint_param_length_log\"],\\n                \"severity\": module.params[\"constraint_param_length_severity\"],\\n                \"status\": module.params[\"constraint_param_length_status\"],\\n            },\\n            \"url-param-length\": {\\n                \"action\": module.params[\"constraint_url_param_length_action\"],\\n                \"length\": module.params[\"constraint_url_param_length_length\"],\\n                \"log\": module.params[\"constraint_url_param_length_log\"],\\n                \"severity\": module.params[\"constraint_url_param_length_severity\"],\\n                \"status\": module.params[\"constraint_url_param_length_status\"],\\n            },\\n            \"version\": {\\n                \"action\": module.params[\"constraint_version_action\"],\\n                \"log\": module.params[\"constraint_version_log\"],\\n                \"severity\": module.params[\"constraint_version_severity\"],\\n                \"status\": module.params[\"constraint_version_status\"],\\n            },\\n        },\\n        \"method\": {\\n            \"default-allowed-methods\": module.params[\"method_default_allowed_methods\"],\\n            \"log\": module.params[\"method_log\"],\\n            \"severity\": module.params[\"method_severity\"],\\n            \"status\": module.params[\"method_status\"],\\n            \"method-policy\": {\\n                \"address\": module.params[\"method_method_policy_address\"],\\n                \"allowed-methods\": module.params[\"method_method_policy_allowed_methods\"],\\n                \"pattern\": module.params[\"method_method_policy_pattern\"],\\n                \"regex\": module.params[\"method_method_policy_regex\"],\\n            },\\n        },\\n        \"signature\": {\\n            \"credit-card-detection-threshold\": module.params[\"signature_credit_card_detection_threshold\"],\\n            \"disabled-signature\": module.params[\"signature_disabled_signature\"],\\n            \"disabled-sub-class\": module.params[\"signature_disabled_sub_class\"],\\n            \"custom-signature\": {\\n                \"action\": module.params[\"signature_custom_signature_action\"],\\n                \"case-sensitivity\": module.params[\"signature_custom_signature_case_sensitivity\"],\\n                \"direction\": module.params[\"signature_custom_signature_direction\"],\\n                \"log\": module.params[\"signature_custom_signature_log\"],\\n                \"name\": module.params[\"signature_custom_signature_name\"],\\n                \"pattern\": module.params[\"signature_custom_signature_pattern\"],\\n                \"severity\": module.params[\"signature_custom_signature_severity\"],\\n                \"status\": module.params[\"signature_custom_signature_status\"],\\n                \"target\": module.params[\"signature_custom_signature_target\"],\\n            },\\n            \"main-class\": {\\n                \"action\": module.params[\"signature_main_class_action\"],\\n                \"log\": module.params[\"signature_main_class_log\"],\\n                \"severity\": module.params[\"signature_main_class_severity\"],\\n                \"status\": module.params[\"signature_main_class_status\"],\\n            },\\n        },\\n        \"url-access\": {\\n            \"action\": module.params[\"url_access_action\"],\\n            \"address\": module.params[\"url_access_address\"],\\n            \"log\": module.params[\"url_access_log\"],\\n            \"severity\": module.params[\"url_access_severity\"],\\n            \"access-pattern\": {\\n                \"negate\": module.params[\"url_access_access_pattern_negate\"],\\n                \"pattern\": module.params[\"url_access_access_pattern_pattern\"],\\n                \"regex\": module.params[\"url_access_access_pattern_regex\"],\\n                \"srcaddr\": module.params[\"url_access_access_pattern_srcaddr\"],\\n            }\\n        }\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    list_overrides = ['address-list', 'constraint', 'method', 'signature', 'url-access']\\n    paramgram = fmgr.tools.paramgram_child_list_override(list_overrides=list_overrides,\\n                                                         paramgram=paramgram, module=module)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        results = fmgr_waf_profile_modify(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def pivot_table(data, values=None, index=None, columns=None, aggfunc='mean',\\n                fill_value=None, margins=False, dropna=True,\\n                margins_name='All'):\\n    index = _convert_by(index)\\n    columns = _convert_by(columns)\\n    if isinstance(aggfunc, list):\\n        pieces = []\\n        keys = []\\n        for func in aggfunc:\\n            table = pivot_table(data, values=values, index=index,\\n                                columns=columns,\\n                                fill_value=fill_value, aggfunc=func,\\n                                margins=margins, margins_name=margins_name)\\n            pieces.append(table)\\n            keys.append(getattr(func, '__name__', func))\\n        return concat(pieces, keys=keys, axis=1)\\n    keys = index + columns\\n    values_passed = values is not None\\n    if values_passed:\\n        if is_list_like(values):\\n            values_multi = True\\n            values = list(values)\\n        else:\\n            values_multi = False\\n            values = [values]\\n        for i in values:\\n            if i not in data:\\n                raise KeyError(i)\\n        to_filter = []\\n        for x in keys + values:\\n            if isinstance(x, Grouper):\\n                x = x.key\\n            try:\\n                if x in data:\\n                    to_filter.append(x)\\n            except TypeError:\\n                pass\\n        if len(to_filter) < len(data.columns):\\n            data = data[to_filter]\\n    else:\\n        values = data.columns\\n        for key in keys:\\n            try:\\n                values = values.drop(key)\\n            except (TypeError, ValueError):\\n                pass\\n        values = list(values)\\n    grouped = data.groupby(keys)\\n    agged = grouped.agg(aggfunc)\\n    table = agged\\n    if table.index.nlevels > 1:\\n        index_names = agged.index.names[:len(index)]\\n        to_unstack = []\\n        for i in range(len(index), len(keys)):\\n            name = agged.index.names[i]\\n            if name is None or name in index_names:\\n                to_unstack.append(i)\\n            else:\\n                to_unstack.append(name)\\n        table = agged.unstack(to_unstack)\\n    if not dropna:\\n        from pandas import MultiIndex\\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.index.levels),\\n                                       names=table.index.names)\\n            table = table.reindex(m, axis=0)\\n        except AttributeError:\\n            pass  \\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.columns.levels),\\n                                       names=table.columns.names)\\n            table = table.reindex(m, axis=1)\\n        except AttributeError:\\n            pass  \\n    if isinstance(table, ABCDataFrame):\\n        table = table.sort_index(axis=1)\\n    if fill_value is not None:\\n        table = table.fillna(value=fill_value, downcast='infer')\\n    if margins:\\n        if dropna:\\n            data = data[data.notna().all(axis=1)]\\n        table = _add_margins(table, data, values, rows=index,\\n                             cols=columns, aggfunc=aggfunc,\\n                             margins_name=margins_name, fill_value=fill_value)\\n    if values_passed and not values_multi and not table.empty and \\\\n       (table.columns.nlevels > 1):\\n        table = table[values[0]]\\n    if len(index) == 0 and len(columns) > 0:\\n        table = table.T\\n    if isinstance(table, ABCDataFrame) and dropna:\\n        table = table.dropna(how='all', axis=1)\\n    return table",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pivot_table(data, values=None, index=None, columns=None, aggfunc='mean',\\n                fill_value=None, margins=False, dropna=True,\\n                margins_name='All'):\\n    index = _convert_by(index)\\n    columns = _convert_by(columns)\\n    if isinstance(aggfunc, list):\\n        pieces = []\\n        keys = []\\n        for func in aggfunc:\\n            table = pivot_table(data, values=values, index=index,\\n                                columns=columns,\\n                                fill_value=fill_value, aggfunc=func,\\n                                margins=margins, margins_name=margins_name)\\n            pieces.append(table)\\n            keys.append(getattr(func, '__name__', func))\\n        return concat(pieces, keys=keys, axis=1)\\n    keys = index + columns\\n    values_passed = values is not None\\n    if values_passed:\\n        if is_list_like(values):\\n            values_multi = True\\n            values = list(values)\\n        else:\\n            values_multi = False\\n            values = [values]\\n        for i in values:\\n            if i not in data:\\n                raise KeyError(i)\\n        to_filter = []\\n        for x in keys + values:\\n            if isinstance(x, Grouper):\\n                x = x.key\\n            try:\\n                if x in data:\\n                    to_filter.append(x)\\n            except TypeError:\\n                pass\\n        if len(to_filter) < len(data.columns):\\n            data = data[to_filter]\\n    else:\\n        values = data.columns\\n        for key in keys:\\n            try:\\n                values = values.drop(key)\\n            except (TypeError, ValueError):\\n                pass\\n        values = list(values)\\n    grouped = data.groupby(keys)\\n    agged = grouped.agg(aggfunc)\\n    table = agged\\n    if table.index.nlevels > 1:\\n        index_names = agged.index.names[:len(index)]\\n        to_unstack = []\\n        for i in range(len(index), len(keys)):\\n            name = agged.index.names[i]\\n            if name is None or name in index_names:\\n                to_unstack.append(i)\\n            else:\\n                to_unstack.append(name)\\n        table = agged.unstack(to_unstack)\\n    if not dropna:\\n        from pandas import MultiIndex\\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.index.levels),\\n                                       names=table.index.names)\\n            table = table.reindex(m, axis=0)\\n        except AttributeError:\\n            pass  \\n        try:\\n            m = MultiIndex.from_arrays(cartesian_product(table.columns.levels),\\n                                       names=table.columns.names)\\n            table = table.reindex(m, axis=1)\\n        except AttributeError:\\n            pass  \\n    if isinstance(table, ABCDataFrame):\\n        table = table.sort_index(axis=1)\\n    if fill_value is not None:\\n        table = table.fillna(value=fill_value, downcast='infer')\\n    if margins:\\n        if dropna:\\n            data = data[data.notna().all(axis=1)]\\n        table = _add_margins(table, data, values, rows=index,\\n                             cols=columns, aggfunc=aggfunc,\\n                             margins_name=margins_name, fill_value=fill_value)\\n    if values_passed and not values_multi and not table.empty and \\\\n       (table.columns.nlevels > 1):\\n        table = table[values[0]]\\n    if len(index) == 0 and len(columns) > 0:\\n        table = table.T\\n    if isinstance(table, ABCDataFrame) and dropna:\\n        table = table.dropna(how='all', axis=1)\\n    return table"
  },
  {
    "code": "def _EluGradGrad(op, grad):\\n  elu_x = op.inputs[1]\\n  return (gen_nn_ops._elu_grad(grad, op.outputs[0]),\\n\\t\\t  array_ops.where(elu_x < 0,\\n\\t\\t\\t\\t\\t\\t  grad * op.inputs[0],\\n\\t\\t\\t\\t\\t\\t  array_ops.zeros(shape=array_ops.shape(elu_x),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  dtype=elu_x.dtype)))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _EluGradGrad(op, grad):\\n  elu_x = op.inputs[1]\\n  return (gen_nn_ops._elu_grad(grad, op.outputs[0]),\\n\\t\\t  array_ops.where(elu_x < 0,\\n\\t\\t\\t\\t\\t\\t  grad * op.inputs[0],\\n\\t\\t\\t\\t\\t\\t  array_ops.zeros(shape=array_ops.shape(elu_x),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  dtype=elu_x.dtype)))"
  },
  {
    "code": "def markdown(value, arg=''):\\n    try:\\n        import markdown\\n    except ImportError:\\n        if settings.DEBUG:\\n            raise template.TemplateSyntaxError(\"Error in 'markdown' filter: The Python markdown library isn't installed.\")\\n        return force_unicode(value)\\n    else:\\n        if hasattr(markdown, 'version'):\\n            extensions = [e for e in arg.split(\",\") if e]\\n            if len(extensions) > 0 and extensions[0] == \"safe\":\\n                extensions = extensions[1:]\\n                safe_mode = True\\n            else:\\n                safe_mode = False\\n            python_markdown_deprecation = \"The use of Python-Markdown \"\\n            \"< 2.1 in Django is deprecated; please update to the current version\"\\n            markdown_vers = getattr(markdown, \"version_info\", None)\\n            if markdown_vers < (1,7):\\n                warnings.warn(python_markdown_deprecation, DeprecationWarning)\\n                return mark_safe(force_unicode(markdown.markdown(smart_str(value), extensions, safe_mode=safe_mode)))\\n            else:\\n                if markdown_vers >= (2,1):\\n                    if safe_mode:\\n                        return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode, enable_attributes=False))\\n                    else:\\n                        return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode))\\n                else:\\n                    warnings.warn(python_markdown_deprecation, DeprecationWarning)\\n                    return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode))\\n        else:\\n            warnings.warn(python_markdown_deprecation, DeprecationWarning)\\n            return mark_safe(force_unicode(markdown.markdown(smart_str(value))))\\n@register.filter(is_safe=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def markdown(value, arg=''):\\n    try:\\n        import markdown\\n    except ImportError:\\n        if settings.DEBUG:\\n            raise template.TemplateSyntaxError(\"Error in 'markdown' filter: The Python markdown library isn't installed.\")\\n        return force_unicode(value)\\n    else:\\n        if hasattr(markdown, 'version'):\\n            extensions = [e for e in arg.split(\",\") if e]\\n            if len(extensions) > 0 and extensions[0] == \"safe\":\\n                extensions = extensions[1:]\\n                safe_mode = True\\n            else:\\n                safe_mode = False\\n            python_markdown_deprecation = \"The use of Python-Markdown \"\\n            \"< 2.1 in Django is deprecated; please update to the current version\"\\n            markdown_vers = getattr(markdown, \"version_info\", None)\\n            if markdown_vers < (1,7):\\n                warnings.warn(python_markdown_deprecation, DeprecationWarning)\\n                return mark_safe(force_unicode(markdown.markdown(smart_str(value), extensions, safe_mode=safe_mode)))\\n            else:\\n                if markdown_vers >= (2,1):\\n                    if safe_mode:\\n                        return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode, enable_attributes=False))\\n                    else:\\n                        return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode))\\n                else:\\n                    warnings.warn(python_markdown_deprecation, DeprecationWarning)\\n                    return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode))\\n        else:\\n            warnings.warn(python_markdown_deprecation, DeprecationWarning)\\n            return mark_safe(force_unicode(markdown.markdown(smart_str(value))))\\n@register.filter(is_safe=True)"
  },
  {
    "code": "def compile(p, flags=0):\\n    if isstring(p):\\n        pattern = p\\n        p = sre_parse.parse(p, flags)\\n    else:\\n        pattern = None\\n    code = _code(p, flags)\\n    groupindex = p.pattern.groupdict\\n    indexgroup = [None] * p.pattern.groups\\n    for k, i in groupindex.items():\\n        indexgroup[i] = k\\n    return _sre.compile(\\n        pattern, flags | p.pattern.flags, code,\\n        p.pattern.groups-1,\\n        groupindex, tuple(indexgroup)\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def compile(p, flags=0):\\n    if isstring(p):\\n        pattern = p\\n        p = sre_parse.parse(p, flags)\\n    else:\\n        pattern = None\\n    code = _code(p, flags)\\n    groupindex = p.pattern.groupdict\\n    indexgroup = [None] * p.pattern.groups\\n    for k, i in groupindex.items():\\n        indexgroup[i] = k\\n    return _sre.compile(\\n        pattern, flags | p.pattern.flags, code,\\n        p.pattern.groups-1,\\n        groupindex, tuple(indexgroup)\\n        )"
  },
  {
    "code": "def copytree(src, dst, symlinks=False, ignore=None, copy_function=copy2,\\n             ignore_dangling_symlinks=False, dirs_exist_ok=False):\\n    sys.audit(\"shutil.copytree\", src, dst)\\n    with os.scandir(src) as itr:\\n        entries = list(itr)\\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\\n                     ignore=ignore, copy_function=copy_function,\\n                     ignore_dangling_symlinks=ignore_dangling_symlinks,\\n                     dirs_exist_ok=dirs_exist_ok)\\nif hasattr(os.stat_result, 'st_file_attributes'):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copytree(src, dst, symlinks=False, ignore=None, copy_function=copy2,\\n             ignore_dangling_symlinks=False, dirs_exist_ok=False):\\n    sys.audit(\"shutil.copytree\", src, dst)\\n    with os.scandir(src) as itr:\\n        entries = list(itr)\\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\\n                     ignore=ignore, copy_function=copy_function,\\n                     ignore_dangling_symlinks=ignore_dangling_symlinks,\\n                     dirs_exist_ok=dirs_exist_ok)\\nif hasattr(os.stat_result, 'st_file_attributes'):"
  },
  {
    "code": "def _infer_multiple_outputs(self):\\n        return_type = self.function_signature.return_annotation\\n        if return_type == dict:\\n            ttype = return_type\\n        elif sys.version_info < (3, 7):\\n            ttype = getattr(return_type, \"__extra__\", None)\\n        else:\\n            ttype = getattr(return_type, \"__origin__\", None)\\n        return return_type is not inspect.Signature.empty and ttype in (dict, Dict)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Better multiple_outputs inferral for @task.python (#20800)",
    "fixed_code": "def _infer_multiple_outputs(self):\\n        try:\\n            return_type = typing_extensions.get_type_hints(self.function).get(\"return\", Any)\\n        except Exception:  \\n            return False\\n        if sys.version_info < (3, 7):\\n            ttype = getattr(return_type, \"__extra__\", return_type)\\n        else:\\n            ttype = getattr(return_type, \"__origin__\", return_type)\\n        return ttype == dict or ttype == Dict"
  },
  {
    "code": "def _ensure_index(index_like):\\n    if isinstance(index_like, Index):\\n        return index_like\\n    if hasattr(index_like, 'name'):\\n        return Index(index_like, name=index_like.name)\\n    if isinstance(index_like, list):\\n        if type(index_like) != list:\\n            index_like = list(index_like)\\n        converted, all_arrays = lib.clean_index_list(index_like)\\n        if len(converted) > 0 and all_arrays:\\n            return MultiIndex.from_arrays(converted)\\n        else:\\n            index_like = converted\\n    return Index(index_like)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Additional keyword arguments for Index.copy()\\n\\n  `dtype` on copy. MultiIndex can set `levels`, `labels`, and `names`.\\n  copies axes as well. Defaults to False.\\n  copy its index.",
    "fixed_code": "def _ensure_index(index_like, copy=False):\\n    if isinstance(index_like, Index):\\n        if copy:\\n            index_like = index_like.copy()\\n        return index_like\\n    if hasattr(index_like, 'name'):\\n        return Index(index_like, name=index_like.name, copy=copy)\\n    if isinstance(index_like, list):\\n        if type(index_like) != list:\\n            index_like = list(index_like)\\n        converted, all_arrays = lib.clean_index_list(index_like)\\n        if len(converted) > 0 and all_arrays:\\n            return MultiIndex.from_arrays(converted)\\n        else:\\n            index_like = converted\\n    else:\\n        if copy:\\n            from copy import copy\\n            index_like = copy(index_like)\\n    return Index(index_like)"
  },
  {
    "code": "def main():\\n    prog = 'python -m json.tool'\\n    description = ('A simple command line interface for json module '\\n                   'to validate and pretty-print JSON objects.')\\n    parser = argparse.ArgumentParser(prog=prog, description=description)\\n    parser.add_argument('infile', nargs='?',\\n                        type=argparse.FileType(encoding=\"utf-8\"),\\n                        help='a JSON file to be validated or pretty-printed',\\n                        default=sys.stdin)\\n    parser.add_argument('outfile', nargs='?',\\n                        type=Path,\\n                        help='write the output of infile to outfile',\\n                        default=None)\\n    parser.add_argument('--sort-keys', action='store_true', default=False,\\n                        help='sort the output of dictionaries alphabetically by key')\\n    parser.add_argument('--no-ensure-ascii', dest='ensure_ascii', action='store_false',\\n                        help='disable escaping of non-ASCII characters')\\n    parser.add_argument('--json-lines', action='store_true', default=False,\\n                        help='parse input using the JSON Lines format. '\\n                        'Use with --no-indent or --compact to produce valid JSON Lines output.')\\n    group = parser.add_mutually_exclusive_group()\\n    group.add_argument('--indent', default=4, type=int,\\n                       help='separate items with newlines and use this number '\\n                       'of spaces for indentation')\\n    group.add_argument('--tab', action='store_const', dest='indent',\\n                       const='\\t', help='separate items with newlines and use '\\n                       'tabs for indentation')\\n    group.add_argument('--no-indent', action='store_const', dest='indent',\\n                       const=None,\\n                       help='separate items with spaces rather than newlines')\\n    group.add_argument('--compact', action='store_true',\\n                       help='suppress all whitespace separation (most compact)')\\n    options = parser.parse_args()\\n    dump_args = {\\n        'sort_keys': options.sort_keys,\\n        'indent': options.indent,\\n        'ensure_ascii': options.ensure_ascii,\\n    }\\n    if options.compact:\\n        dump_args['indent'] = None\\n        dump_args['separators'] = ',', ':'\\n    with options.infile as infile:\\n        try:\\n            if options.json_lines:\\n                objs = (json.loads(line) for line in infile)\\n            else:\\n                objs = (json.load(infile),)\\n            if options.outfile is None:\\n                out = sys.stdout\\n            else:\\n                out = options.outfile.open('w', encoding='utf-8')\\n            with out as outfile:\\n                for obj in objs:\\n                    json.dump(obj, outfile, **dump_args)\\n                    outfile.write('\\n')\\n        except ValueError as e:\\n            raise SystemExit(e)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    prog = 'python -m json.tool'\\n    description = ('A simple command line interface for json module '\\n                   'to validate and pretty-print JSON objects.')\\n    parser = argparse.ArgumentParser(prog=prog, description=description)\\n    parser.add_argument('infile', nargs='?',\\n                        type=argparse.FileType(encoding=\"utf-8\"),\\n                        help='a JSON file to be validated or pretty-printed',\\n                        default=sys.stdin)\\n    parser.add_argument('outfile', nargs='?',\\n                        type=Path,\\n                        help='write the output of infile to outfile',\\n                        default=None)\\n    parser.add_argument('--sort-keys', action='store_true', default=False,\\n                        help='sort the output of dictionaries alphabetically by key')\\n    parser.add_argument('--no-ensure-ascii', dest='ensure_ascii', action='store_false',\\n                        help='disable escaping of non-ASCII characters')\\n    parser.add_argument('--json-lines', action='store_true', default=False,\\n                        help='parse input using the JSON Lines format. '\\n                        'Use with --no-indent or --compact to produce valid JSON Lines output.')\\n    group = parser.add_mutually_exclusive_group()\\n    group.add_argument('--indent', default=4, type=int,\\n                       help='separate items with newlines and use this number '\\n                       'of spaces for indentation')\\n    group.add_argument('--tab', action='store_const', dest='indent',\\n                       const='\\t', help='separate items with newlines and use '\\n                       'tabs for indentation')\\n    group.add_argument('--no-indent', action='store_const', dest='indent',\\n                       const=None,\\n                       help='separate items with spaces rather than newlines')\\n    group.add_argument('--compact', action='store_true',\\n                       help='suppress all whitespace separation (most compact)')\\n    options = parser.parse_args()\\n    dump_args = {\\n        'sort_keys': options.sort_keys,\\n        'indent': options.indent,\\n        'ensure_ascii': options.ensure_ascii,\\n    }\\n    if options.compact:\\n        dump_args['indent'] = None\\n        dump_args['separators'] = ',', ':'\\n    with options.infile as infile:\\n        try:\\n            if options.json_lines:\\n                objs = (json.loads(line) for line in infile)\\n            else:\\n                objs = (json.load(infile),)\\n            if options.outfile is None:\\n                out = sys.stdout\\n            else:\\n                out = options.outfile.open('w', encoding='utf-8')\\n            with out as outfile:\\n                for obj in objs:\\n                    json.dump(obj, outfile, **dump_args)\\n                    outfile.write('\\n')\\n        except ValueError as e:\\n            raise SystemExit(e)"
  },
  {
    "code": "def read_fwf(filepath_or_buffer,\\n             colspecs=None,\\n             widths=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             delimiter=None,\\n             verbose=False,\\n             encoding=None):\\n    kwds = locals()\\n    colspecs = kwds.get('colspecs', None)\\n    widths = kwds.pop('widths', None)\\n    if bool(colspecs is None) == bool(widths is None):\\n        raise ValueError(\"You must specify only one of 'widths' and \"\\n                         \"'colspecs'\")\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append( (col, col+w) )\\n            col += w\\n        kwds['colspecs'] = colspecs\\n    kwds['thousands'] = thousands\\n    return _read(FixedWidthFieldParser, filepath_or_buffer, kwds)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: option to keep component date columns. #1252",
    "fixed_code": "def read_fwf(filepath_or_buffer,\\n             colspecs=None,\\n             widths=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             delimiter=None,\\n             verbose=False,\\n             encoding=None):\\n    kwds = locals()\\n    colspecs = kwds.get('colspecs', None)\\n    widths = kwds.pop('widths', None)\\n    if bool(colspecs is None) == bool(widths is None):\\n        raise ValueError(\"You must specify only one of 'widths' and \"\\n                         \"'colspecs'\")\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append( (col, col+w) )\\n            col += w\\n        kwds['colspecs'] = colspecs\\n    kwds['thousands'] = thousands\\n    return _read(FixedWidthFieldParser, filepath_or_buffer, kwds)"
  },
  {
    "code": "def _assume_role_with_saml(\\n        self, sts_client: boto3.client, role_arn: str, assume_role_kwargs: Dict[str, Any]\\n    ) -> Dict[str, Any]:\\n        saml_config = self.extra_config['assume_role_with_saml']\\n        principal_arn = saml_config['principal_arn']\\n        idp_auth_method = saml_config['idp_auth_method']\\n        if idp_auth_method == 'http_spegno_auth':\\n            saml_assertion = self._fetch_saml_assertion_using_http_spegno_auth(saml_config)\\n        else:\\n            raise NotImplementedError(\\n                f'idp_auth_method={idp_auth_method} in Connection {self.conn.conn_id} Extra.'\\n                'Currently only \"http_spegno_auth\" is supported, and must be specified.'\\n            )\\n        self.log.info(\"Doing sts_client.assume_role_with_saml to role_arn=%s\", role_arn)\\n        return sts_client.assume_role_with_saml(\\n            RoleArn=role_arn, PrincipalArn=principal_arn, SAMLAssertion=saml_assertion, **assume_role_kwargs\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Update AWS Base hook to use refreshable credentials (#16770) (#16771)",
    "fixed_code": "def _assume_role_with_saml(self, sts_client: boto3.client) -> Dict[str, Any]:\\n        saml_config = self.extra_config['assume_role_with_saml']\\n        principal_arn = saml_config['principal_arn']\\n        idp_auth_method = saml_config['idp_auth_method']\\n        if idp_auth_method == 'http_spegno_auth':\\n            saml_assertion = self._fetch_saml_assertion_using_http_spegno_auth(saml_config)\\n        else:\\n            raise NotImplementedError(\\n                f'idp_auth_method={idp_auth_method} in Connection {self.conn.conn_id} Extra.'\\n                'Currently only \"http_spegno_auth\" is supported, and must be specified.'\\n            )\\n        self.log.info(\"Doing sts_client.assume_role_with_saml to role_arn=%s\", self.role_arn)\\n        assume_role_kwargs = self.extra_config.get(\"assume_role_kwargs\", {})\\n        return sts_client.assume_role_with_saml(\\n            RoleArn=self.role_arn,\\n            PrincipalArn=principal_arn,\\n            SAMLAssertion=saml_assertion,\\n            **assume_role_kwargs,\\n        )"
  },
  {
    "code": "def _rename_columns_inplace(self, mapper):\\n        self._data = self._data.rename_items(mapper, copydata=False)\\n        self._clear_item_cache()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _rename_columns_inplace(self, mapper):\\n        self._data = self._data.rename_items(mapper, copydata=False)\\n        self._clear_item_cache()"
  },
  {
    "code": "def _partial_tup_index(self, tup, side='left'):\\n        n = len(tup)\\n        start, end = 0, len(self)\\n        zipped = izip(tup, self.levels, self.labels)\\n        for k, (lab, lev, labs) in enumerate(zipped):\\n            section = labs[start:end]\\n            if lab not in lev:\\n                loc = lev.searchsorted(lab, side=side)\\n                if side == 'right' and loc > 0:\\n                    loc -= 1\\n                return start + section.searchsorted(loc, side=side)\\n            idx = lev.get_loc(lab)\\n            if k < n - 1:\\n                end = start + section.searchsorted(idx, side='right')\\n                start = start + section.searchsorted(idx, side='left')\\n            else:\\n                return start + section.searchsorted(idx, side=side)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: implemented lexsort_depth and is_lexsorted",
    "fixed_code": "def _partial_tup_index(self, tup, side='left'):\\n        if len(tup) > self.lexsort_depth:\\n            raise Exception('MultiIndex lexsort depth %d, key was %d long' %\\n                            (self.lexsort_depth, len(tup)))\\n        n = len(tup)\\n        start, end = 0, len(self)\\n        zipped = izip(tup, self.levels, self.labels)\\n        for k, (lab, lev, labs) in enumerate(zipped):\\n            section = labs[start:end]\\n            if lab not in lev:\\n                loc = lev.searchsorted(lab, side=side)\\n                if side == 'right' and loc > 0:\\n                    loc -= 1\\n                return start + section.searchsorted(loc, side=side)\\n            idx = lev.get_loc(lab)\\n            if k < n - 1:\\n                end = start + section.searchsorted(idx, side='right')\\n                start = start + section.searchsorted(idx, side='left')\\n            else:\\n                return start + section.searchsorted(idx, side=side)"
  },
  {
    "code": "def delete_selected(modeladmin, request, queryset):\\n    opts = modeladmin.model._meta\\n    app_label = opts.app_label\\n    if not modeladmin.has_delete_permission(request):\\n        raise PermissionDenied\\n    using = router.db_for_write(modeladmin.model)\\n    deletable_objects, model_count, perms_needed, protected = get_deleted_objects(\\n        queryset, opts, request.user, modeladmin.admin_site, using)\\n    if request.POST.get('post'):\\n        if perms_needed:\\n            raise PermissionDenied\\n        n = queryset.count()\\n        if n:\\n            for obj in queryset:\\n                obj_display = force_text(obj)\\n                modeladmin.log_deletion(request, obj, obj_display)\\n            queryset.delete()\\n            modeladmin.message_user(request, _(\"Successfully deleted %(count)d %(items)s.\") % {\\n                \"count\": n, \"items\": model_ngettext(modeladmin.opts, n)\\n            }, messages.SUCCESS)\\n        return None\\n    if len(queryset) == 1:\\n        objects_name = force_text(opts.verbose_name)\\n    else:\\n        objects_name = force_text(opts.verbose_name_plural)\\n    if perms_needed or protected:\\n        title = _(\"Cannot delete %(name)s\") % {\"name\": objects_name}\\n    else:\\n        title = _(\"Are you sure?\")\\n    context = dict(\\n        modeladmin.admin_site.each_context(),\\n        title=title,\\n        objects_name=objects_name,\\n        deletable_objects=[deletable_objects],\\n        model_count=dict(model_count),\\n        queryset=queryset,\\n        perms_lacking=perms_needed,\\n        protected=protected,\\n        opts=opts,\\n        action_checkbox_name=helpers.ACTION_CHECKBOX_NAME,\\n    )\\n    return TemplateResponse(request, modeladmin.delete_selected_confirmation_template or [\\n        \"admin/%s/%s/delete_selected_confirmation.html\" % (app_label, opts.model_name),\\n        \"admin/%s/delete_selected_confirmation.html\" % app_label,\\n        \"admin/delete_selected_confirmation.html\"\\n    ], context, current_app=modeladmin.admin_site.name)\\ndelete_selected.short_description = ugettext_lazy(\"Delete selected %(verbose_name_plural)s\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def delete_selected(modeladmin, request, queryset):\\n    opts = modeladmin.model._meta\\n    app_label = opts.app_label\\n    if not modeladmin.has_delete_permission(request):\\n        raise PermissionDenied\\n    using = router.db_for_write(modeladmin.model)\\n    deletable_objects, model_count, perms_needed, protected = get_deleted_objects(\\n        queryset, opts, request.user, modeladmin.admin_site, using)\\n    if request.POST.get('post'):\\n        if perms_needed:\\n            raise PermissionDenied\\n        n = queryset.count()\\n        if n:\\n            for obj in queryset:\\n                obj_display = force_text(obj)\\n                modeladmin.log_deletion(request, obj, obj_display)\\n            queryset.delete()\\n            modeladmin.message_user(request, _(\"Successfully deleted %(count)d %(items)s.\") % {\\n                \"count\": n, \"items\": model_ngettext(modeladmin.opts, n)\\n            }, messages.SUCCESS)\\n        return None\\n    if len(queryset) == 1:\\n        objects_name = force_text(opts.verbose_name)\\n    else:\\n        objects_name = force_text(opts.verbose_name_plural)\\n    if perms_needed or protected:\\n        title = _(\"Cannot delete %(name)s\") % {\"name\": objects_name}\\n    else:\\n        title = _(\"Are you sure?\")\\n    context = dict(\\n        modeladmin.admin_site.each_context(),\\n        title=title,\\n        objects_name=objects_name,\\n        deletable_objects=[deletable_objects],\\n        model_count=dict(model_count),\\n        queryset=queryset,\\n        perms_lacking=perms_needed,\\n        protected=protected,\\n        opts=opts,\\n        action_checkbox_name=helpers.ACTION_CHECKBOX_NAME,\\n    )\\n    return TemplateResponse(request, modeladmin.delete_selected_confirmation_template or [\\n        \"admin/%s/%s/delete_selected_confirmation.html\" % (app_label, opts.model_name),\\n        \"admin/%s/delete_selected_confirmation.html\" % app_label,\\n        \"admin/delete_selected_confirmation.html\"\\n    ], context, current_app=modeladmin.admin_site.name)\\ndelete_selected.short_description = ugettext_lazy(\"Delete selected %(verbose_name_plural)s\")"
  },
  {
    "code": "def bdate_range(\\n    start=None,\\n    end=None,\\n    periods: int | None = None,\\n    freq=\"B\",\\n    tz=None,\\n    normalize: bool = True,\\n    name: Hashable = None,\\n    weekmask=None,\\n    holidays=None,\\n    closed: IntervalLeftRight | lib.NoDefault | None = lib.no_default,\\n    inclusive: IntervalClosedType | None = None,\\n    **kwargs,\\n) -> DatetimeIndex:\\n    if freq is None:\\n        msg = \"freq must be specified for bdate_range; use date_range instead\"\\n        raise TypeError(msg)\\n    if isinstance(freq, str) and freq.startswith(\"C\"):\\n        try:\\n            weekmask = weekmask or \"Mon Tue Wed Thu Fri\"\\n            freq = prefix_mapping[freq](holidays=holidays, weekmask=weekmask)\\n        except (KeyError, TypeError) as err:\\n            msg = f\"invalid custom frequency string: {freq}\"\\n            raise ValueError(msg) from err\\n    elif holidays or weekmask:\\n        msg = (\\n            \"a custom frequency string is required when holidays or \"\\n            f\"weekmask are passed, got frequency {freq}\"\\n        )\\n        raise ValueError(msg)\\n    return date_range(\\n        start=start,\\n        end=end,\\n        periods=periods,\\n        freq=freq,\\n        tz=tz,\\n        normalize=normalize,\\n        name=name,\\n        closed=closed,\\n        inclusive=inclusive,\\n        **kwargs,\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: Update documentation for date_range(), bdate_range(), and interval_range() to include timedelta as a possible data type for the freq parameter (#48631)",
    "fixed_code": "def bdate_range(\\n    start=None,\\n    end=None,\\n    periods: int | None = None,\\n    freq=\"B\",\\n    tz=None,\\n    normalize: bool = True,\\n    name: Hashable = None,\\n    weekmask=None,\\n    holidays=None,\\n    closed: IntervalLeftRight | lib.NoDefault | None = lib.no_default,\\n    inclusive: IntervalClosedType | None = None,\\n    **kwargs,\\n) -> DatetimeIndex:\\n    if freq is None:\\n        msg = \"freq must be specified for bdate_range; use date_range instead\"\\n        raise TypeError(msg)\\n    if isinstance(freq, str) and freq.startswith(\"C\"):\\n        try:\\n            weekmask = weekmask or \"Mon Tue Wed Thu Fri\"\\n            freq = prefix_mapping[freq](holidays=holidays, weekmask=weekmask)\\n        except (KeyError, TypeError) as err:\\n            msg = f\"invalid custom frequency string: {freq}\"\\n            raise ValueError(msg) from err\\n    elif holidays or weekmask:\\n        msg = (\\n            \"a custom frequency string is required when holidays or \"\\n            f\"weekmask are passed, got frequency {freq}\"\\n        )\\n        raise ValueError(msg)\\n    return date_range(\\n        start=start,\\n        end=end,\\n        periods=periods,\\n        freq=freq,\\n        tz=tz,\\n        normalize=normalize,\\n        name=name,\\n        closed=closed,\\n        inclusive=inclusive,\\n        **kwargs,\\n    )"
  },
  {
    "code": "def maketables(trace=0):\\n    print \"--- Reading\", UNICODE_DATA, \"...\"\\n    unicode = UnicodeData(UNICODE_DATA, COMPOSITION_EXCLUSIONS,\\n                          EASTASIAN_WIDTH)\\n    print len(filter(None, unicode.table)), \"characters\"\\n    makeunicodename(unicode, trace)\\n    makeunicodedata(unicode, trace)\\n    makeunicodetype(unicode, trace)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maketables(trace=0):\\n    print \"--- Reading\", UNICODE_DATA, \"...\"\\n    unicode = UnicodeData(UNICODE_DATA, COMPOSITION_EXCLUSIONS,\\n                          EASTASIAN_WIDTH)\\n    print len(filter(None, unicode.table)), \"characters\"\\n    makeunicodename(unicode, trace)\\n    makeunicodedata(unicode, trace)\\n    makeunicodetype(unicode, trace)"
  },
  {
    "code": "def __rsub__(self, other):\\n\\t\\tif is_datetime64_dtype(other) and is_timedelta64_dtype(self):\\n\\t\\t\\tif not isinstance(other, DatetimeLikeArrayMixin):\\n\\t\\t\\t\\tfrom pandas.core.arrays import DatetimeArray\\n\\t\\t\\t\\tother = DatetimeArray(other)\\n\\t\\t\\treturn other - self\\n\\t\\telif (\\n\\t\\t\\tis_datetime64_any_dtype(self)\\n\\t\\t\\tand hasattr(other, \"dtype\")\\n\\t\\t\\tand not is_datetime64_any_dtype(other)\\n\\t\\t):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"cannot subtract {cls} from {typ}\".format(\\n\\t\\t\\t\\t\\tcls=type(self).__name__, typ=type(other).__name__\\n\\t\\t\\t\\t)\\n\\t\\t\\t)\\n\\t\\telif is_period_dtype(self) and is_timedelta64_dtype(other):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"cannot subtract {cls} from {dtype}\".format(\\n\\t\\t\\t\\t\\tcls=type(self).__name__, dtype=other.dtype\\n\\t\\t\\t\\t)\\n\\t\\t\\t)\\n\\t\\treturn -(self - other)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix+test DTA/TDA/PA add/sub Index (#27726)",
    "fixed_code": "def __rsub__(self, other):\\n\\t\\tif is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\\n\\t\\t\\tif not isinstance(other, DatetimeLikeArrayMixin):\\n\\t\\t\\t\\tfrom pandas.core.arrays import DatetimeArray\\n\\t\\t\\t\\tother = DatetimeArray(other)\\n\\t\\t\\treturn other - self\\n\\t\\telif (\\n\\t\\t\\tis_datetime64_any_dtype(self)\\n\\t\\t\\tand hasattr(other, \"dtype\")\\n\\t\\t\\tand not is_datetime64_any_dtype(other)\\n\\t\\t):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"cannot subtract {cls} from {typ}\".format(\\n\\t\\t\\t\\t\\tcls=type(self).__name__, typ=type(other).__name__\\n\\t\\t\\t\\t)\\n\\t\\t\\t)\\n\\t\\telif is_period_dtype(self) and is_timedelta64_dtype(other):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"cannot subtract {cls} from {dtype}\".format(\\n\\t\\t\\t\\t\\tcls=type(self).__name__, dtype=other.dtype\\n\\t\\t\\t\\t)\\n\\t\\t\\t)\\n\\t\\treturn -(self - other)"
  },
  {
    "code": "def ihave(self, message_id, data):\\n        return self._post('IHAVE {0}'.format(message_id), data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ihave(self, message_id, data):\\n        return self._post('IHAVE {0}'.format(message_id), data)"
  },
  {
    "code": "def _config_to_json(config: Config) -> str:\\n    return json.dumps(config_schema.dump(config).data, indent=4)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Update FlaskAppBuilder to v3 (#9648)",
    "fixed_code": "def _config_to_json(config: Config) -> str:\\n    return json.dumps(config_schema.dump(config), indent=4)"
  },
  {
    "code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               parse_dates=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               parse_dates=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)"
  },
  {
    "code": "def _get_fmtlist(self):\\n        if self.format_version == 118:\\n            b = 57\\n        elif self.format_version > 113:\\n            b = 49\\n        elif self.format_version > 104:\\n            b = 12\\n        else:\\n            b = 7\\n        return [self._decode(self.path_or_buf.read(b)) for i in range(self.nvar)]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Add dta 119 reading to StataReader (#28542)\\n\\nAdd requirements for reading 119 format files",
    "fixed_code": "def _get_fmtlist(self):\\n        if self.format_version >= 118:\\n            b = 57\\n        elif self.format_version > 113:\\n            b = 49\\n        elif self.format_version > 104:\\n            b = 12\\n        else:\\n            b = 7\\n        return [self._decode(self.path_or_buf.read(b)) for i in range(self.nvar)]"
  },
  {
    "code": "def __del__(self):\\n            if self._state == futures._PENDING and self._log_destroy_pending:\\n                context = {\\n                    'task': self,\\n                    'message': 'Task was destroyed but it is pending!',\\n                }\\n                if self._source_traceback:\\n                    context['source_traceback'] = self._source_traceback\\n                self._loop.call_exception_handler(context)\\n            futures.Future.__del__(self)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __del__(self):\\n            if self._state == futures._PENDING and self._log_destroy_pending:\\n                context = {\\n                    'task': self,\\n                    'message': 'Task was destroyed but it is pending!',\\n                }\\n                if self._source_traceback:\\n                    context['source_traceback'] = self._source_traceback\\n                self._loop.call_exception_handler(context)\\n            futures.Future.__del__(self)"
  },
  {
    "code": "def __init__(self,\\n                 project_id,\\n                 name,\\n                 location,\\n                 gcp_conn_id='google_cloud_default',\\n                 api_version='v2',\\n                 *args,\\n                 **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.project_id = project_id\\n        self.gcp_conn_id = gcp_conn_id\\n        self.location = location\\n        self.api_version = api_version\\n        self.name = name\\n        self._check_input()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self,\\n                 project_id,\\n                 name,\\n                 location,\\n                 gcp_conn_id='google_cloud_default',\\n                 api_version='v2',\\n                 *args,\\n                 **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.project_id = project_id\\n        self.gcp_conn_id = gcp_conn_id\\n        self.location = location\\n        self.api_version = api_version\\n        self.name = name\\n        self._check_input()"
  },
  {
    "code": "def _check_if_db_exists(self, db_name, hook: CloudSqlHook):\\n        try:\\n            return hook.get_database(\\n                project_id=self.project_id,\\n                instance=self.instance,\\n                database=db_name)\\n        except HttpError as e:\\n            status = e.resp.status\\n            if status == 404:\\n                return False\\n            raise e",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_if_db_exists(self, db_name, hook: CloudSqlHook):\\n        try:\\n            return hook.get_database(\\n                project_id=self.project_id,\\n                instance=self.instance,\\n                database=db_name)\\n        except HttpError as e:\\n            status = e.resp.status\\n            if status == 404:\\n                return False\\n            raise e"
  },
  {
    "code": "def where(\\n        self, other, cond, errors=\"raise\", try_cast: bool = False, axis: int = 0\\n    ) -> List[\"Block\"]:\\n        import pandas.core.computation.expressions as expressions\\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\\n        assert errors in [\"raise\", \"ignore\"]\\n        transpose = self.ndim == 2\\n        values = self.values\\n        orig_other = other\\n        if transpose:\\n            values = values.T\\n        other, cond = self._maybe_reshape_where_args(values, other, cond, axis)\\n        if cond.ravel(\"K\").all():\\n            result = values\\n        else:\\n            if (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                pass\\n            elif not self._can_hold_element(other):\\n                block = self.coerce_to_target_dtype(other)\\n                blocks = block.where(\\n                    orig_other, cond, errors=errors, try_cast=try_cast, axis=axis\\n                )\\n                return self._maybe_downcast(blocks, \"infer\")\\n            if not (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                other = convert_scalar_for_putitemlike(other, values.dtype)\\n            result = expressions.where(cond, values, other)\\n        if self._can_hold_na or self.ndim == 1:\\n            if transpose:\\n                result = result.T\\n            return [self.make_block(result)]\\n        axis = cond.ndim - 1\\n        cond = cond.swapaxes(axis, 0)\\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])], dtype=bool)\\n        result_blocks: List[\"Block\"] = []\\n        for m in [mask, ~mask]:\\n            if m.any():\\n                result = cast(np.ndarray, result)  \\n                taken = result.take(m.nonzero()[0], axis=axis)\\n                r = maybe_downcast_numeric(taken, self.dtype)\\n                nb = self.make_block(r.T, placement=self.mgr_locs[m])\\n                result_blocks.append(nb)\\n        return result_blocks",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def where(\\n        self, other, cond, errors=\"raise\", try_cast: bool = False, axis: int = 0\\n    ) -> List[\"Block\"]:\\n        import pandas.core.computation.expressions as expressions\\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\\n        assert errors in [\"raise\", \"ignore\"]\\n        transpose = self.ndim == 2\\n        values = self.values\\n        orig_other = other\\n        if transpose:\\n            values = values.T\\n        other, cond = self._maybe_reshape_where_args(values, other, cond, axis)\\n        if cond.ravel(\"K\").all():\\n            result = values\\n        else:\\n            if (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                pass\\n            elif not self._can_hold_element(other):\\n                block = self.coerce_to_target_dtype(other)\\n                blocks = block.where(\\n                    orig_other, cond, errors=errors, try_cast=try_cast, axis=axis\\n                )\\n                return self._maybe_downcast(blocks, \"infer\")\\n            if not (\\n                (self.is_integer or self.is_bool)\\n                and lib.is_float(other)\\n                and np.isnan(other)\\n            ):\\n                other = convert_scalar_for_putitemlike(other, values.dtype)\\n            result = expressions.where(cond, values, other)\\n        if self._can_hold_na or self.ndim == 1:\\n            if transpose:\\n                result = result.T\\n            return [self.make_block(result)]\\n        axis = cond.ndim - 1\\n        cond = cond.swapaxes(axis, 0)\\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])], dtype=bool)\\n        result_blocks: List[\"Block\"] = []\\n        for m in [mask, ~mask]:\\n            if m.any():\\n                result = cast(np.ndarray, result)  \\n                taken = result.take(m.nonzero()[0], axis=axis)\\n                r = maybe_downcast_numeric(taken, self.dtype)\\n                nb = self.make_block(r.T, placement=self.mgr_locs[m])\\n                result_blocks.append(nb)\\n        return result_blocks"
  },
  {
    "code": "def __init__(self, name, value, group):\\n        vars(self).update(name=name, value=value, group=group)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, name, value, group):\\n        vars(self).update(name=name, value=value, group=group)"
  },
  {
    "code": "def replaceChild(self, newChild, oldChild):\\n        if newChild.nodeType == self.DOCUMENT_FRAGMENT_NODE:\\n            refChild = oldChild.nextSibling\\n            self.removeChild(oldChild)\\n            return self.insertBefore(newChild, refChild)\\n        if newChild.nodeType not in self._child_node_types:\\n            raise xml.dom.HierarchyRequestErr(\\n                \"%s cannot be child of %s\" % (repr(newChild), repr(self)))\\n        if newChild.parentNode is not None:\\n            newChild.parentNode.removeChild(newChild)\\n        if newChild is oldChild:\\n            return\\n        try:\\n            index = self.childNodes.index(oldChild)\\n        except ValueError:\\n            raise xml.dom.NotFoundErr()\\n        self.childNodes[index] = newChild\\n        newChild.parentNode = self\\n        oldChild.parentNode = None\\n        if (newChild.nodeType in _nodeTypes_with_children\\n            or oldChild.nodeType in _nodeTypes_with_children):\\n            _clear_id_cache(self)\\n        newChild.nextSibling = oldChild.nextSibling\\n        newChild.previousSibling = oldChild.previousSibling\\n        oldChild.nextSibling = None\\n        oldChild.previousSibling = None\\n        if newChild.previousSibling:\\n            newChild.previousSibling.nextSibling = newChild\\n        if newChild.nextSibling:\\n            newChild.nextSibling.previousSibling = newChild\\n        return oldChild",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[Patch #1094164] replaceChild(x,x) ends up removing x of the tree.  Add fix from Felix Rabe and a test case",
    "fixed_code": "def replaceChild(self, newChild, oldChild):\\n        if newChild.nodeType == self.DOCUMENT_FRAGMENT_NODE:\\n            refChild = oldChild.nextSibling\\n            self.removeChild(oldChild)\\n            return self.insertBefore(newChild, refChild)\\n        if newChild.nodeType not in self._child_node_types:\\n            raise xml.dom.HierarchyRequestErr(\\n                \"%s cannot be child of %s\" % (repr(newChild), repr(self)))\\n        if newChild is oldChild:\\n            return\\n        if newChild.parentNode is not None:\\n            newChild.parentNode.removeChild(newChild)\\n        try:\\n            index = self.childNodes.index(oldChild)\\n        except ValueError:\\n            raise xml.dom.NotFoundErr()\\n        self.childNodes[index] = newChild\\n        newChild.parentNode = self\\n        oldChild.parentNode = None\\n        if (newChild.nodeType in _nodeTypes_with_children\\n            or oldChild.nodeType in _nodeTypes_with_children):\\n            _clear_id_cache(self)\\n        newChild.nextSibling = oldChild.nextSibling\\n        newChild.previousSibling = oldChild.previousSibling\\n        oldChild.nextSibling = None\\n        oldChild.previousSibling = None\\n        if newChild.previousSibling:\\n            newChild.previousSibling.nextSibling = newChild\\n        if newChild.nextSibling:\\n            newChild.nextSibling.previousSibling = newChild\\n        return oldChild"
  },
  {
    "code": "def _get_new_axes(self):\\n        ndim = self._get_result_dim()\\n        new_axes = [None] * ndim\\n        if self.ignore_index:\\n            concat_axis = None\\n        else:\\n            concat_axis = self._get_concat_axis()\\n        new_axes[self.axis] = concat_axis\\n        if self.join_axes is None:\\n            for i in range(ndim):\\n                if i == self.axis:\\n                    continue\\n                new_axes[i] = self._get_comb_axis(i)\\n        else:\\n            assert(len(self.join_axes) == ndim - 1)\\n            indices = range(ndim)\\n            indices.remove(self.axis)\\n            for i, ax in zip(indices, self.join_axes):\\n                new_axes[i] = ax\\n        return new_axes",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_new_axes(self):\\n        ndim = self._get_result_dim()\\n        new_axes = [None] * ndim\\n        if self.ignore_index:\\n            concat_axis = None\\n        else:\\n            concat_axis = self._get_concat_axis()\\n        new_axes[self.axis] = concat_axis\\n        if self.join_axes is None:\\n            for i in range(ndim):\\n                if i == self.axis:\\n                    continue\\n                new_axes[i] = self._get_comb_axis(i)\\n        else:\\n            assert(len(self.join_axes) == ndim - 1)\\n            indices = range(ndim)\\n            indices.remove(self.axis)\\n            for i, ax in zip(indices, self.join_axes):\\n                new_axes[i] = ax\\n        return new_axes"
  },
  {
    "code": "def __setattr__(self, name, value):\\n        if name in self._internal_names_set:\\n            object.__setattr__(self, name, value)\\n        elif name in self._metadata:\\n            return object.__setattr__(self, name, value)\\n        else:\\n            try:\\n                existing = getattr(self, name)\\n                if isinstance(existing, Index):\\n                    object.__setattr__(self, name, value)\\n                elif name in self._info_axis:\\n                    self[name] = value\\n                else:\\n                    object.__setattr__(self, name, value)\\n            except (AttributeError, TypeError):\\n                object.__setattr__(self, name, value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __setattr__(self, name, value):\\n        if name in self._internal_names_set:\\n            object.__setattr__(self, name, value)\\n        elif name in self._metadata:\\n            return object.__setattr__(self, name, value)\\n        else:\\n            try:\\n                existing = getattr(self, name)\\n                if isinstance(existing, Index):\\n                    object.__setattr__(self, name, value)\\n                elif name in self._info_axis:\\n                    self[name] = value\\n                else:\\n                    object.__setattr__(self, name, value)\\n            except (AttributeError, TypeError):\\n                object.__setattr__(self, name, value)"
  },
  {
    "code": "def connection_lost(self, exc):\\n        self._connection_lost = True\\n        if not self._paused:\\n            return\\n        waiter = self._drain_waiter\\n        if waiter is None:\\n            return\\n        self._drain_waiter = None\\n        if waiter.done():\\n            return\\n        if exc is None:\\n            waiter.set_result(None)\\n        else:\\n            waiter.set_exception(exc)\\n    async def _drain_helper(self):\\n        if self._connection_lost:\\n            raise ConnectionResetError('Connection lost')\\n        if not self._paused:\\n            return\\n        waiter = self._drain_waiter\\n        assert waiter is None or waiter.cancelled()\\n        waiter = self._loop.create_future()\\n        self._drain_waiter = waiter\\n        await waiter",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "GH-74116: Allow multiple drain waiters for asyncio.StreamWriter (GH-94705)",
    "fixed_code": "def connection_lost(self, exc):\\n        self._connection_lost = True\\n        if not self._paused:\\n            return\\n        for waiter in self._drain_waiters:\\n            if not waiter.done():\\n                if exc is None:\\n                    waiter.set_result(None)\\n                else:\\n                    waiter.set_exception(exc)\\n    async def _drain_helper(self):\\n        if self._connection_lost:\\n            raise ConnectionResetError('Connection lost')\\n        if not self._paused:\\n            return\\n        waiter = self._loop.create_future()\\n        self._drain_waiters.append(waiter)\\n        try:\\n            await waiter\\n        finally:\\n            self._drain_waiters.remove(waiter)"
  },
  {
    "code": "def _check_date_fields(year, month, day):\\n    year = _index(year)\\n    month = _index(month)\\n    day = _index(day)\\n    if not MINYEAR <= year <= MAXYEAR:\\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\\n    if not 1 <= month <= 12:\\n        raise ValueError('month must be in 1..12', month)\\n    dim = _days_in_month(year, month)\\n    if not 1 <= day <= dim:\\n        raise ValueError('day must be in 1..%d' % dim, day)\\n    return year, month, day",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_date_fields(year, month, day):\\n    year = _index(year)\\n    month = _index(month)\\n    day = _index(day)\\n    if not MINYEAR <= year <= MAXYEAR:\\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\\n    if not 1 <= month <= 12:\\n        raise ValueError('month must be in 1..12', month)\\n    dim = _days_in_month(year, month)\\n    if not 1 <= day <= dim:\\n        raise ValueError('day must be in 1..%d' % dim, day)\\n    return year, month, day"
  },
  {
    "code": "def _scale_preprocess(X):\\n    X = _make_nonnegative(X)\\n    row_diag = np.asarray(1.0 / np.sqrt(X.sum(axis=1))).squeeze()\\n    col_diag = np.asarray(1.0 / np.sqrt(X.sum(axis=0))).squeeze()\\n    row_diag = np.where(np.isnan(row_diag), 0, row_diag)\\n    col_diag = np.where(np.isnan(col_diag), 0, col_diag)\\n    if issparse(X):\\n        n_rows, n_cols = X.shape\\n        r = lil_matrix((n_rows, n_rows))\\n        c = lil_matrix((n_cols, n_cols))\\n        r.setdiag(row_diag)\\n        c.setdiag(col_diag)\\n        an = r * X * c\\n    else:\\n        an = row_diag[:, np.newaxis] * X * col_diag\\n    return an, row_diag, col_diag",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _scale_preprocess(X):\\n    X = _make_nonnegative(X)\\n    row_diag = np.asarray(1.0 / np.sqrt(X.sum(axis=1))).squeeze()\\n    col_diag = np.asarray(1.0 / np.sqrt(X.sum(axis=0))).squeeze()\\n    row_diag = np.where(np.isnan(row_diag), 0, row_diag)\\n    col_diag = np.where(np.isnan(col_diag), 0, col_diag)\\n    if issparse(X):\\n        n_rows, n_cols = X.shape\\n        r = lil_matrix((n_rows, n_rows))\\n        c = lil_matrix((n_cols, n_cols))\\n        r.setdiag(row_diag)\\n        c.setdiag(col_diag)\\n        an = r * X * c\\n    else:\\n        an = row_diag[:, np.newaxis] * X * col_diag\\n    return an, row_diag, col_diag"
  },
  {
    "code": "def train(hyp, opt, device, tb_writer=None, wandb=None):\\n    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\\n    save_dir, epochs, batch_size, total_batch_size, weights, rank = \\\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank\\n    wdir = save_dir / 'weights'\\n    wdir.mkdir(parents=True, exist_ok=True)  \\n    last = wdir / 'last.pt'\\n    best = wdir / 'best.pt'\\n    results_file = save_dir / 'results.txt'\\n    with open(save_dir / 'hyp.yaml', 'w') as f:\\n        yaml.dump(hyp, f, sort_keys=False)\\n    with open(save_dir / 'opt.yaml', 'w') as f:\\n        yaml.dump(vars(opt), f, sort_keys=False)\\n    plots = not opt.evolve  \\n    cuda = device.type != 'cpu'\\n    init_seeds(2 + rank)\\n    with open(opt.data) as f:\\n        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  \\n    with torch_distributed_zero_first(rank):\\n        check_dataset(data_dict)  \\n    train_path = data_dict['train']\\n    test_path = data_dict['val']\\n    nc = 1 if opt.single_cls else int(data_dict['nc'])  \\n    names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  \\n    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  \\n    pretrained = weights.endswith('.pt')\\n    if pretrained:\\n        with torch_distributed_zero_first(rank):\\n            attempt_download(weights)  \\n        ckpt = torch.load(weights, map_location=device)  \\n        if hyp.get('anchors'):\\n            ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  \\n        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  \\n        exclude = ['anchor'] if opt.cfg or hyp.get('anchors') else []  \\n        state_dict = ckpt['model'].float().state_dict()  \\n        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  \\n        model.load_state_dict(state_dict, strict=False)  \\n        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  \\n    else:\\n        model = Model(opt.cfg, ch=3, nc=nc).to(device)  \\n    freeze = []  \\n    for k, v in model.named_parameters():\\n        v.requires_grad = True  \\n        if any(x in k for x in freeze):\\n            print('freezing %s' % k)\\n            v.requires_grad = False\\n    nbs = 64  \\n    accumulate = max(round(nbs / total_batch_size), 1)  \\n    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  \\n    logger.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\\n    pg0, pg1, pg2 = [], [], []  \\n    for k, v in model.named_modules():\\n        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\\n            pg2.append(v.bias)  \\n        if isinstance(v, nn.BatchNorm2d):\\n            pg0.append(v.weight)  \\n        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\\n            pg1.append(v.weight)  \\n    if opt.adam:\\n        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  \\n    else:\\n        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\\n    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  \\n    optimizer.add_param_group({'params': pg2})  \\n    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\\n    del pg0, pg1, pg2\\n    if opt.linear_lr:\\n        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  \\n    else:\\n        lf = one_cycle(1, hyp['lrf'], epochs)  \\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\\n    if rank in [-1, 0] and wandb and wandb.run is None:\\n        opt.hyp = hyp  \\n        wandb_run = wandb.init(config=opt, resume=\"allow\",\\n                               project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\\n                               name=save_dir.stem,\\n                               id=ckpt.get('wandb_id') if 'ckpt' in locals() else None)\\n    loggers = {'wandb': wandb}  \\n    start_epoch, best_fitness = 0, 0.0\\n    if pretrained:\\n        if ckpt['optimizer'] is not None:\\n            optimizer.load_state_dict(ckpt['optimizer'])\\n            best_fitness = ckpt['best_fitness']\\n        if ckpt.get('training_results') is not None:\\n            with open(results_file, 'w') as file:\\n                file.write(ckpt['training_results'])  \\n        start_epoch = ckpt['epoch'] + 1\\n        if opt.resume:\\n            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)\\n        if epochs < start_epoch:\\n            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\\n                        (weights, ckpt['epoch'], epochs))\\n            epochs += ckpt['epoch']  \\n        del ckpt, state_dict\\n    gs = max(int(model.stride.max()), 32)  \\n    nl = model.model[-1].nl  \\n    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  \\n    if cuda and rank == -1 and torch.cuda.device_count() > 1:\\n        model = torch.nn.DataParallel(model)\\n    if opt.sync_bn and cuda and rank != -1:\\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\\n        logger.info('Using SyncBatchNorm()')\\n    ema = ModelEMA(model) if rank in [-1, 0] else None\\n    if cuda and rank != -1:\\n        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)\\n    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\\n                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,\\n                                            world_size=opt.world_size, workers=opt.workers,\\n                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))\\n    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  \\n    nb = len(dataloader)  \\n    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)\\n    if rank in [-1, 0]:\\n        ema.updates = start_epoch * nb // accumulate  \\n        testloader = create_dataloader(test_path, imgsz_test, batch_size * 2, gs, opt,  \\n                                       hyp=hyp, cache=opt.cache_images and not opt.notest, rect=True, rank=-1,\\n                                       world_size=opt.world_size, workers=opt.workers,\\n                                       pad=0.5, prefix=colorstr('val: '))[0]\\n        if not opt.resume:\\n            labels = np.concatenate(dataset.labels, 0)\\n            c = torch.tensor(labels[:, 0])  \\n            if plots:\\n                plot_labels(labels, save_dir, loggers)\\n                if tb_writer:\\n                    tb_writer.add_histogram('classes', c, 0)\\n            if not opt.noautoanchor:\\n                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\\n    hyp['box'] *= 3. / nl  \\n    hyp['cls'] *= nc / 80. * 3. / nl  \\n    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  \\n    model.nc = nc  \\n    model.hyp = hyp  \\n    model.gr = 1.0  \\n    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  \\n    model.names = names\\n    t0 = time.time()\\n    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  \\n    maps = np.zeros(nc)  \\n    results = (0, 0, 0, 0, 0, 0, 0)  \\n    scheduler.last_epoch = start_epoch - 1  \\n    scaler = amp.GradScaler(enabled=cuda)\\n    compute_loss = ComputeLoss(model)  \\n    logger.info(f'Image sizes {imgsz} train, {imgsz_test} test\\n'\\n                f'Using {dataloader.num_workers} dataloader workers\\n'\\n                f'Logging results to {save_dir}\\n'\\n                f'Starting training for {epochs} epochs...')\\n    for epoch in range(start_epoch, epochs):  \\n        model.train()\\n        if opt.image_weights:\\n            if rank in [-1, 0]:\\n                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  \\n                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  \\n                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  \\n            if rank != -1:\\n                indices = (torch.tensor(dataset.indices) if rank == 0 else torch.zeros(dataset.n)).int()\\n                dist.broadcast(indices, 0)\\n                if rank != 0:\\n                    dataset.indices = indices.cpu().numpy()\\n        mloss = torch.zeros(4, device=device)  \\n        if rank != -1:\\n            dataloader.sampler.set_epoch(epoch)\\n        pbar = enumerate(dataloader)\\n        logger.info(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'targets', 'img_size'))\\n        if rank in [-1, 0]:\\n            pbar = tqdm(pbar, total=nb)  \\n        optimizer.zero_grad()\\n        for i, (imgs, targets, paths, _) in pbar:  \\n            ni = i + nb * epoch  \\n            imgs = imgs.to(device, non_blocking=True).float() / 255.0  \\n            if ni <= nw:\\n                xi = [0, nw]  \\n                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\\n                for j, x in enumerate(optimizer.param_groups):\\n                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\\n                    if 'momentum' in x:\\n                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\\n            if opt.multi_scale:\\n                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  \\n                sf = sz / max(imgs.shape[2:])  \\n                if sf != 1:\\n                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  \\n                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\\n            with amp.autocast(enabled=cuda):\\n                pred = model(imgs)  \\n                loss, loss_items = compute_loss(pred, targets.to(device))  \\n                if rank != -1:\\n                    loss *= opt.world_size  \\n                if opt.quad:\\n                    loss *= 4.\\n            scaler.scale(loss).backward()\\n            if ni % accumulate == 0:\\n                scaler.step(optimizer)  \\n                scaler.update()\\n                optimizer.zero_grad()\\n                if ema:\\n                    ema.update(model)\\n            if rank in [-1, 0]:\\n                mloss = (mloss * i + loss_items) / (i + 1)  \\n                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  \\n                s = ('%10s' * 2 + '%10.4g' * 6) % (\\n                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\\n                pbar.set_description(s)\\n                if plots and ni < 3:\\n                    f = save_dir / f'train_batch{ni}.jpg'  \\n                    Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()\\n                elif plots and ni == 10 and wandb:\\n                    wandb.log({\"Mosaics\": [wandb.Image(str(x), caption=x.name) for x in save_dir.glob('train*.jpg')\\n                                           if x.exists()]}, commit=False)\\n        lr = [x['lr'] for x in optimizer.param_groups]  \\n        scheduler.step()\\n        if rank in [-1, 0]:\\n            if ema:\\n                ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])\\n            final_epoch = epoch + 1 == epochs\\n            if not opt.notest or final_epoch:  \\n                results, maps, times = test.test(opt.data,\\n                                                 batch_size=batch_size * 2,\\n                                                 imgsz=imgsz_test,\\n                                                 model=ema.ema,\\n                                                 single_cls=opt.single_cls,\\n                                                 dataloader=testloader,\\n                                                 save_dir=save_dir,\\n                                                 verbose=nc < 50 and final_epoch,\\n                                                 plots=plots and final_epoch,\\n                                                 log_imgs=opt.log_imgs if wandb else 0,\\n                                                 compute_loss=compute_loss)\\n            with open(results_file, 'a') as f:\\n                f.write(s + '%10.4g' * 7 % results + '\\n')  \\n            if len(opt.name) and opt.bucket:\\n                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\\n            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  \\n                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\\n                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  \\n                    'x/lr0', 'x/lr1', 'x/lr2']  \\n            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\\n                if tb_writer:\\n                    tb_writer.add_scalar(tag, x, epoch)  \\n                if wandb:\\n                    wandb.log({tag: x}, step=epoch, commit=tag == tags[-1])  \\n            fi = fitness(np.array(results).reshape(1, -1))  \\n            if fi > best_fitness:\\n                best_fitness = fi\\n            save = (not opt.nosave) or (final_epoch and not opt.evolve)\\n            if save:\\n                with open(results_file, 'r') as f:  \\n                    ckpt = {'epoch': epoch,\\n                            'best_fitness': best_fitness,\\n                            'training_results': f.read(),\\n                            'model': ema.ema,\\n                            'optimizer': None if final_epoch else optimizer.state_dict(),\\n                            'wandb_id': wandb_run.id if wandb else None}\\n                torch.save(ckpt, last)\\n                if best_fitness == fi:\\n                    torch.save(ckpt, best)\\n                del ckpt\\n    if rank in [-1, 0]:\\n        final = best if best.exists() else last  \\n        for f in [last, best]:\\n            if f.exists():\\n                strip_optimizer(f)  \\n        if opt.bucket:\\n            os.system(f'gsutil cp {final} gs://{opt.bucket}/weights')  \\n        if plots:\\n            plot_results(save_dir=save_dir)  \\n            if wandb:\\n                files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]\\n                wandb.log({\"Results\": [wandb.Image(str(save_dir / f), caption=f) for f in files\\n                                       if (save_dir / f).exists()]})\\n                if opt.log_artifacts:\\n                    wandb.log_artifact(artifact_or_path=str(final), type='model', name=save_dir.stem)\\n        logger.info('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\\n        if opt.data.endswith('coco.yaml') and nc == 80:  \\n            for conf, iou, save_json in ([0.25, 0.45, False], [0.001, 0.65, True]):  \\n                results, _, _ = test.test(opt.data,\\n                                          batch_size=batch_size * 2,\\n                                          imgsz=imgsz_test,\\n                                          conf_thres=conf,\\n                                          iou_thres=iou,\\n                                          model=attempt_load(final, device).half(),\\n                                          single_cls=opt.single_cls,\\n                                          dataloader=testloader,\\n                                          save_dir=save_dir,\\n                                          save_json=save_json,\\n                                          plots=False)\\n    else:\\n        dist.destroy_process_group()\\n    wandb.run.finish() if wandb and wandb.run else None\\n    torch.cuda.empty_cache()\\n    return results",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def train(hyp, opt, device, tb_writer=None, wandb=None):\\n    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\\n    save_dir, epochs, batch_size, total_batch_size, weights, rank = \\\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank\\n    wdir = save_dir / 'weights'\\n    wdir.mkdir(parents=True, exist_ok=True)  \\n    last = wdir / 'last.pt'\\n    best = wdir / 'best.pt'\\n    results_file = save_dir / 'results.txt'\\n    with open(save_dir / 'hyp.yaml', 'w') as f:\\n        yaml.dump(hyp, f, sort_keys=False)\\n    with open(save_dir / 'opt.yaml', 'w') as f:\\n        yaml.dump(vars(opt), f, sort_keys=False)\\n    plots = not opt.evolve  \\n    cuda = device.type != 'cpu'\\n    init_seeds(2 + rank)\\n    with open(opt.data) as f:\\n        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  \\n    with torch_distributed_zero_first(rank):\\n        check_dataset(data_dict)  \\n    train_path = data_dict['train']\\n    test_path = data_dict['val']\\n    nc = 1 if opt.single_cls else int(data_dict['nc'])  \\n    names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  \\n    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  \\n    pretrained = weights.endswith('.pt')\\n    if pretrained:\\n        with torch_distributed_zero_first(rank):\\n            attempt_download(weights)  \\n        ckpt = torch.load(weights, map_location=device)  \\n        if hyp.get('anchors'):\\n            ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  \\n        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  \\n        exclude = ['anchor'] if opt.cfg or hyp.get('anchors') else []  \\n        state_dict = ckpt['model'].float().state_dict()  \\n        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  \\n        model.load_state_dict(state_dict, strict=False)  \\n        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  \\n    else:\\n        model = Model(opt.cfg, ch=3, nc=nc).to(device)  \\n    freeze = []  \\n    for k, v in model.named_parameters():\\n        v.requires_grad = True  \\n        if any(x in k for x in freeze):\\n            print('freezing %s' % k)\\n            v.requires_grad = False\\n    nbs = 64  \\n    accumulate = max(round(nbs / total_batch_size), 1)  \\n    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  \\n    logger.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\\n    pg0, pg1, pg2 = [], [], []  \\n    for k, v in model.named_modules():\\n        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\\n            pg2.append(v.bias)  \\n        if isinstance(v, nn.BatchNorm2d):\\n            pg0.append(v.weight)  \\n        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\\n            pg1.append(v.weight)  \\n    if opt.adam:\\n        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  \\n    else:\\n        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\\n    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  \\n    optimizer.add_param_group({'params': pg2})  \\n    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\\n    del pg0, pg1, pg2\\n    if opt.linear_lr:\\n        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  \\n    else:\\n        lf = one_cycle(1, hyp['lrf'], epochs)  \\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\\n    if rank in [-1, 0] and wandb and wandb.run is None:\\n        opt.hyp = hyp  \\n        wandb_run = wandb.init(config=opt, resume=\"allow\",\\n                               project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\\n                               name=save_dir.stem,\\n                               id=ckpt.get('wandb_id') if 'ckpt' in locals() else None)\\n    loggers = {'wandb': wandb}  \\n    start_epoch, best_fitness = 0, 0.0\\n    if pretrained:\\n        if ckpt['optimizer'] is not None:\\n            optimizer.load_state_dict(ckpt['optimizer'])\\n            best_fitness = ckpt['best_fitness']\\n        if ckpt.get('training_results') is not None:\\n            with open(results_file, 'w') as file:\\n                file.write(ckpt['training_results'])  \\n        start_epoch = ckpt['epoch'] + 1\\n        if opt.resume:\\n            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)\\n        if epochs < start_epoch:\\n            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\\n                        (weights, ckpt['epoch'], epochs))\\n            epochs += ckpt['epoch']  \\n        del ckpt, state_dict\\n    gs = max(int(model.stride.max()), 32)  \\n    nl = model.model[-1].nl  \\n    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  \\n    if cuda and rank == -1 and torch.cuda.device_count() > 1:\\n        model = torch.nn.DataParallel(model)\\n    if opt.sync_bn and cuda and rank != -1:\\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\\n        logger.info('Using SyncBatchNorm()')\\n    ema = ModelEMA(model) if rank in [-1, 0] else None\\n    if cuda and rank != -1:\\n        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)\\n    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\\n                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,\\n                                            world_size=opt.world_size, workers=opt.workers,\\n                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))\\n    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  \\n    nb = len(dataloader)  \\n    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)\\n    if rank in [-1, 0]:\\n        ema.updates = start_epoch * nb // accumulate  \\n        testloader = create_dataloader(test_path, imgsz_test, batch_size * 2, gs, opt,  \\n                                       hyp=hyp, cache=opt.cache_images and not opt.notest, rect=True, rank=-1,\\n                                       world_size=opt.world_size, workers=opt.workers,\\n                                       pad=0.5, prefix=colorstr('val: '))[0]\\n        if not opt.resume:\\n            labels = np.concatenate(dataset.labels, 0)\\n            c = torch.tensor(labels[:, 0])  \\n            if plots:\\n                plot_labels(labels, save_dir, loggers)\\n                if tb_writer:\\n                    tb_writer.add_histogram('classes', c, 0)\\n            if not opt.noautoanchor:\\n                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\\n    hyp['box'] *= 3. / nl  \\n    hyp['cls'] *= nc / 80. * 3. / nl  \\n    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  \\n    model.nc = nc  \\n    model.hyp = hyp  \\n    model.gr = 1.0  \\n    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  \\n    model.names = names\\n    t0 = time.time()\\n    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  \\n    maps = np.zeros(nc)  \\n    results = (0, 0, 0, 0, 0, 0, 0)  \\n    scheduler.last_epoch = start_epoch - 1  \\n    scaler = amp.GradScaler(enabled=cuda)\\n    compute_loss = ComputeLoss(model)  \\n    logger.info(f'Image sizes {imgsz} train, {imgsz_test} test\\n'\\n                f'Using {dataloader.num_workers} dataloader workers\\n'\\n                f'Logging results to {save_dir}\\n'\\n                f'Starting training for {epochs} epochs...')\\n    for epoch in range(start_epoch, epochs):  \\n        model.train()\\n        if opt.image_weights:\\n            if rank in [-1, 0]:\\n                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  \\n                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  \\n                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  \\n            if rank != -1:\\n                indices = (torch.tensor(dataset.indices) if rank == 0 else torch.zeros(dataset.n)).int()\\n                dist.broadcast(indices, 0)\\n                if rank != 0:\\n                    dataset.indices = indices.cpu().numpy()\\n        mloss = torch.zeros(4, device=device)  \\n        if rank != -1:\\n            dataloader.sampler.set_epoch(epoch)\\n        pbar = enumerate(dataloader)\\n        logger.info(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'targets', 'img_size'))\\n        if rank in [-1, 0]:\\n            pbar = tqdm(pbar, total=nb)  \\n        optimizer.zero_grad()\\n        for i, (imgs, targets, paths, _) in pbar:  \\n            ni = i + nb * epoch  \\n            imgs = imgs.to(device, non_blocking=True).float() / 255.0  \\n            if ni <= nw:\\n                xi = [0, nw]  \\n                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\\n                for j, x in enumerate(optimizer.param_groups):\\n                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\\n                    if 'momentum' in x:\\n                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\\n            if opt.multi_scale:\\n                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  \\n                sf = sz / max(imgs.shape[2:])  \\n                if sf != 1:\\n                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  \\n                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\\n            with amp.autocast(enabled=cuda):\\n                pred = model(imgs)  \\n                loss, loss_items = compute_loss(pred, targets.to(device))  \\n                if rank != -1:\\n                    loss *= opt.world_size  \\n                if opt.quad:\\n                    loss *= 4.\\n            scaler.scale(loss).backward()\\n            if ni % accumulate == 0:\\n                scaler.step(optimizer)  \\n                scaler.update()\\n                optimizer.zero_grad()\\n                if ema:\\n                    ema.update(model)\\n            if rank in [-1, 0]:\\n                mloss = (mloss * i + loss_items) / (i + 1)  \\n                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  \\n                s = ('%10s' * 2 + '%10.4g' * 6) % (\\n                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\\n                pbar.set_description(s)\\n                if plots and ni < 3:\\n                    f = save_dir / f'train_batch{ni}.jpg'  \\n                    Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()\\n                elif plots and ni == 10 and wandb:\\n                    wandb.log({\"Mosaics\": [wandb.Image(str(x), caption=x.name) for x in save_dir.glob('train*.jpg')\\n                                           if x.exists()]}, commit=False)\\n        lr = [x['lr'] for x in optimizer.param_groups]  \\n        scheduler.step()\\n        if rank in [-1, 0]:\\n            if ema:\\n                ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])\\n            final_epoch = epoch + 1 == epochs\\n            if not opt.notest or final_epoch:  \\n                results, maps, times = test.test(opt.data,\\n                                                 batch_size=batch_size * 2,\\n                                                 imgsz=imgsz_test,\\n                                                 model=ema.ema,\\n                                                 single_cls=opt.single_cls,\\n                                                 dataloader=testloader,\\n                                                 save_dir=save_dir,\\n                                                 verbose=nc < 50 and final_epoch,\\n                                                 plots=plots and final_epoch,\\n                                                 log_imgs=opt.log_imgs if wandb else 0,\\n                                                 compute_loss=compute_loss)\\n            with open(results_file, 'a') as f:\\n                f.write(s + '%10.4g' * 7 % results + '\\n')  \\n            if len(opt.name) and opt.bucket:\\n                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\\n            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  \\n                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\\n                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  \\n                    'x/lr0', 'x/lr1', 'x/lr2']  \\n            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\\n                if tb_writer:\\n                    tb_writer.add_scalar(tag, x, epoch)  \\n                if wandb:\\n                    wandb.log({tag: x}, step=epoch, commit=tag == tags[-1])  \\n            fi = fitness(np.array(results).reshape(1, -1))  \\n            if fi > best_fitness:\\n                best_fitness = fi\\n            save = (not opt.nosave) or (final_epoch and not opt.evolve)\\n            if save:\\n                with open(results_file, 'r') as f:  \\n                    ckpt = {'epoch': epoch,\\n                            'best_fitness': best_fitness,\\n                            'training_results': f.read(),\\n                            'model': ema.ema,\\n                            'optimizer': None if final_epoch else optimizer.state_dict(),\\n                            'wandb_id': wandb_run.id if wandb else None}\\n                torch.save(ckpt, last)\\n                if best_fitness == fi:\\n                    torch.save(ckpt, best)\\n                del ckpt\\n    if rank in [-1, 0]:\\n        final = best if best.exists() else last  \\n        for f in [last, best]:\\n            if f.exists():\\n                strip_optimizer(f)  \\n        if opt.bucket:\\n            os.system(f'gsutil cp {final} gs://{opt.bucket}/weights')  \\n        if plots:\\n            plot_results(save_dir=save_dir)  \\n            if wandb:\\n                files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]\\n                wandb.log({\"Results\": [wandb.Image(str(save_dir / f), caption=f) for f in files\\n                                       if (save_dir / f).exists()]})\\n                if opt.log_artifacts:\\n                    wandb.log_artifact(artifact_or_path=str(final), type='model', name=save_dir.stem)\\n        logger.info('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\\n        if opt.data.endswith('coco.yaml') and nc == 80:  \\n            for conf, iou, save_json in ([0.25, 0.45, False], [0.001, 0.65, True]):  \\n                results, _, _ = test.test(opt.data,\\n                                          batch_size=batch_size * 2,\\n                                          imgsz=imgsz_test,\\n                                          conf_thres=conf,\\n                                          iou_thres=iou,\\n                                          model=attempt_load(final, device).half(),\\n                                          single_cls=opt.single_cls,\\n                                          dataloader=testloader,\\n                                          save_dir=save_dir,\\n                                          save_json=save_json,\\n                                          plots=False)\\n    else:\\n        dist.destroy_process_group()\\n    wandb.run.finish() if wandb and wandb.run else None\\n    torch.cuda.empty_cache()\\n    return results"
  },
  {
    "code": "def load_data(fobj):\\n    header = _TZifHeader.from_file(fobj)\\n    if header.version == 1:\\n        time_size = 4\\n        time_type = \"l\"\\n    else:\\n        time_size = 8\\n        time_type = \"q\"\\n        skip_bytes = (\\n            header.timecnt * 5  \\n            + header.typecnt * 6  \\n            + header.charcnt  \\n            + header.leapcnt * 8  \\n            + header.isstdcnt  \\n            + header.isutcnt  \\n        )\\n        fobj.seek(skip_bytes, 1)\\n        header = _TZifHeader.from_file(fobj)\\n    typecnt = header.typecnt\\n    timecnt = header.timecnt\\n    charcnt = header.charcnt\\n    if timecnt:\\n        trans_list_utc = struct.unpack(\\n            f\">{timecnt}{time_type}\", fobj.read(timecnt * time_size)\\n        )\\n        trans_idx = struct.unpack(f\">{timecnt}B\", fobj.read(timecnt))\\n    else:\\n        trans_list_utc = ()\\n        trans_idx = ()\\n    if typecnt:\\n        utcoff, isdst, abbrind = zip(\\n            *(struct.unpack(\">lbb\", fobj.read(6)) for i in range(typecnt))\\n        )\\n    else:\\n        utcoff = ()\\n        isdst = ()\\n        abbrind = ()\\n    abbr_vals = {}\\n    abbr_chars = fobj.read(charcnt)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_data(fobj):\\n    header = _TZifHeader.from_file(fobj)\\n    if header.version == 1:\\n        time_size = 4\\n        time_type = \"l\"\\n    else:\\n        time_size = 8\\n        time_type = \"q\"\\n        skip_bytes = (\\n            header.timecnt * 5  \\n            + header.typecnt * 6  \\n            + header.charcnt  \\n            + header.leapcnt * 8  \\n            + header.isstdcnt  \\n            + header.isutcnt  \\n        )\\n        fobj.seek(skip_bytes, 1)\\n        header = _TZifHeader.from_file(fobj)\\n    typecnt = header.typecnt\\n    timecnt = header.timecnt\\n    charcnt = header.charcnt\\n    if timecnt:\\n        trans_list_utc = struct.unpack(\\n            f\">{timecnt}{time_type}\", fobj.read(timecnt * time_size)\\n        )\\n        trans_idx = struct.unpack(f\">{timecnt}B\", fobj.read(timecnt))\\n    else:\\n        trans_list_utc = ()\\n        trans_idx = ()\\n    if typecnt:\\n        utcoff, isdst, abbrind = zip(\\n            *(struct.unpack(\">lbb\", fobj.read(6)) for i in range(typecnt))\\n        )\\n    else:\\n        utcoff = ()\\n        isdst = ()\\n        abbrind = ()\\n    abbr_vals = {}\\n    abbr_chars = fobj.read(charcnt)"
  },
  {
    "code": "def get_value(self, series, key):\\n\\t\\tif is_integer(key):\\n\\t\\t\\treturn series.iat[key]\\n\\t\\tif isinstance(key, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tloc = self._get_string_slice(key)\\n\\t\\t\\t\\treturn series[loc]\\n\\t\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\t\\tpass\\n\\t\\t\\tasdt, reso = parse_time_string(key, self.freq)\\n\\t\\t\\tgrp = resolution.Resolution.get_freq_group(reso)\\n\\t\\t\\tfreqn = resolution.get_freq_group(self.freq)\\n\\t\\t\\tassert grp >= freqn\\n\\t\\t\\tif grp == freqn:\\n\\t\\t\\t\\tkey = Period(asdt, freq=self.freq)\\n\\t\\t\\t\\tloc = self.get_loc(key)\\n\\t\\t\\t\\treturn series.iloc[loc]\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(key)\\n\\t\\telif isinstance(key, Period) or key is NaT:\\n\\t\\t\\tordinal = key.ordinal if key is not NaT else NaT.value\\n\\t\\t\\tloc = self._engine.get_loc(ordinal)\\n\\t\\t\\treturn series[loc]\\n\\t\\tvalue = Index.get_value(self, series, key)\\n\\t\\treturn com.maybe_box(self, value, series, key)\\n\\t@Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: inconsistency between PeriodIndex.get_value vs get_loc (#31172)",
    "fixed_code": "def get_value(self, series, key):\\n\\t\\tif is_integer(key):\\n\\t\\t\\treturn series.iat[key]\\n\\t\\tif isinstance(key, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tloc = self._get_string_slice(key)\\n\\t\\t\\t\\treturn series[loc]\\n\\t\\t\\texcept (TypeError, ValueError, OverflowError):\\n\\t\\t\\t\\tpass\\n\\t\\t\\tasdt, reso = parse_time_string(key, self.freq)\\n\\t\\t\\tgrp = resolution.Resolution.get_freq_group(reso)\\n\\t\\t\\tfreqn = resolution.get_freq_group(self.freq)\\n\\t\\t\\tassert grp >= freqn\\n\\t\\t\\tif grp == freqn:\\n\\t\\t\\t\\tkey = Period(asdt, freq=self.freq)\\n\\t\\t\\t\\tloc = self.get_loc(key)\\n\\t\\t\\t\\treturn series.iloc[loc]\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(key)\\n\\t\\telif isinstance(key, Period) or key is NaT:\\n\\t\\t\\tordinal = key.ordinal if key is not NaT else NaT.value\\n\\t\\t\\tloc = self._engine.get_loc(ordinal)\\n\\t\\t\\treturn series[loc]\\n\\t\\tvalue = Index.get_value(self, series, key)\\n\\t\\treturn com.maybe_box(self, value, series, key)\\n\\t@Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)"
  },
  {
    "code": "def join(a, *p):\\n    sep = _get_sep(a)\\n    path = a\\n    try:\\n        for b in p:\\n            if b.startswith(sep):\\n                path = b\\n            elif not path or path.endswith(sep):\\n                path += b\\n            else:\\n                path += sep + b\\n    except TypeError:\\n        strs = [isinstance(s, str) for s in (a, ) + p]\\n        if any(strs) and not all(strs):\\n            raise TypeError(\"Can't mix strings and bytes in path components.\")\\n        else:\\n            raise\\n    return path",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def join(a, *p):\\n    sep = _get_sep(a)\\n    path = a\\n    try:\\n        for b in p:\\n            if b.startswith(sep):\\n                path = b\\n            elif not path or path.endswith(sep):\\n                path += b\\n            else:\\n                path += sep + b\\n    except TypeError:\\n        strs = [isinstance(s, str) for s in (a, ) + p]\\n        if any(strs) and not all(strs):\\n            raise TypeError(\"Can't mix strings and bytes in path components.\")\\n        else:\\n            raise\\n    return path"
  },
  {
    "code": "def precision_recall_curve(y_true, probas_pred):\\n    fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred)\\n    precision = tps / (tps + fps)\\n    recall = tps / tps[-1]\\n    last_ind = tps.searchsorted(tps[-1])\\n    sl = slice(last_ind, None, -1)\\n    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def precision_recall_curve(y_true, probas_pred):\\n    fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred)\\n    precision = tps / (tps + fps)\\n    recall = tps / tps[-1]\\n    last_ind = tps.searchsorted(tps[-1])\\n    sl = slice(last_ind, None, -1)\\n    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]"
  },
  {
    "code": "def next_execution(args):\\n    dag = get_dag(args)\\n    if dag.is_paused:\\n        print(\"[INFO] Please be reminded this DAG is PAUSED now.\")\\n    if dag.latest_execution_date:\\n        next_execution_dttm = dag.following_schedule(dag.latest_execution_date)\\n        if next_execution_dttm is None:\\n            print(\"[WARN] No following schedule can be found. \" +\\n                  \"This DAG may have schedule interval '@once' or `None`.\")\\n        print(next_execution_dttm)\\n    else:\\n        print(\"[WARN] Only applicable when there is execution record found for the DAG.\")\\n        print(None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3998] Use nested commands in cli. (#4821)",
    "fixed_code": "def next_execution(args):\\n    dag = get_dag(args)\\n    if dag.is_paused:\\n        print(\"[INFO] Please be reminded this DAG is PAUSED now.\")\\n    if dag.latest_execution_date:\\n        next_execution_dttm = dag.following_schedule(dag.latest_execution_date)\\n        if next_execution_dttm is None:\\n            print(\"[WARN] No following schedule can be found. \" +\\n                  \"This DAG may have schedule interval '@once' or `None`.\")\\n        print(next_execution_dttm)\\n    else:\\n        print(\"[WARN] Only applicable when there is execution record found for the DAG.\")\\n        print(None)"
  },
  {
    "code": "def delete(self, req, image_id):\\n\\t\\tself._enforce(req, 'delete_image')\\n\\t\\timage = self._get_image(req.context, image_id)\\n\\t\\tif image['protected']:\\n\\t\\t\\tmsg = _(\"Unable to delete as image %(image_id)s is protected\"\\n\\t\\t\\t\\t\\t% locals())\\n\\t\\t\\traise webob.exc.HTTPForbidden(explanation=msg)\\n\\t\\tstatus = 'deleted'\\n\\t\\tif image['location']:\\n\\t\\t\\tif CONF.delayed_delete:\\n\\t\\t\\t\\tstatus = 'pending_delete'\\n\\t\\t\\t\\tself.store_api.schedule_delayed_delete_from_backend(\\n\\t\\t\\t\\t\\t\\t\\t\\timage['location'], id)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.store_api.safe_delete_from_backend(image['location'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treq.context, id)\\n\\t\\ttry:\\n\\t\\t\\tself.db_api.image_update(req.context, image_id, {'status': status})\\n\\t\\t\\tself.db_api.image_destroy(req.context, image_id)\\n\\t\\texcept (exception.NotFound, exception.Forbidden):\\n\\t\\t\\tmsg = (\"Failed to find image %(image_id)s to delete\" % locals())\\n\\t\\t\\tLOG.info(msg)\\n\\t\\t\\traise webob.exc.HTTPNotFound()\\n\\t\\telse:\\n\\t\\t\\tself.notifier.info('image.delete', image)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Ensure authorization before deleting from store\\n\\nThis fixes bug 1076506.",
    "fixed_code": "def delete(self, req, image_id):\\n\\t\\tself._enforce(req, 'delete_image')\\n\\t\\timage = self._get_image(req.context, image_id)\\n\\t\\tif image['protected']:\\n\\t\\t\\tmsg = _(\"Unable to delete as image %(image_id)s is protected\"\\n\\t\\t\\t\\t\\t% locals())\\n\\t\\t\\traise webob.exc.HTTPForbidden(explanation=msg)\\n\\t\\tif image['location'] and CONF.delayed_delete:\\n\\t\\t\\tstatus = 'pending_delete'\\n\\t\\telse:\\n\\t\\t\\tstatus = 'deleted'\\n\\t\\ttry:\\n\\t\\t\\tself.db_api.image_update(req.context, image_id, {'status': status})\\n\\t\\t\\tself.db_api.image_destroy(req.context, image_id)\\n\\t\\t\\tif image['location']:\\n\\t\\t\\t\\tif CONF.delayed_delete:\\n\\t\\t\\t\\t\\tself.store_api.schedule_delayed_delete_from_backend(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\timage['location'], id)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.store_api.safe_delete_from_backend(image['location'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treq.context, id)\\n\\t\\texcept (exception.NotFound, exception.Forbidden):\\n\\t\\t\\tmsg = (\"Failed to find image %(image_id)s to delete\" % locals())\\n\\t\\t\\tLOG.info(msg)\\n\\t\\t\\traise webob.exc.HTTPNotFound()\\n\\t\\telse:\\n\\t\\t\\tself.notifier.info('image.delete', image)"
  },
  {
    "code": "def _write_private_key_file(self, filename, key, format, password=None):\\n\\t\\tkwargs = dict(flags=os.O_WRONLY | os.O_TRUNC | os.O_CREAT, mode=o600)\\n\\t\\twith os.fdopen(os.open(filename, **kwargs), mode=\"w\") as f:\\n\\t\\t\\tos.chmod(filename, o600)\\n\\t\\t\\tself._write_private_key(f, key, format, password=password)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _write_private_key_file(self, filename, key, format, password=None):\\n\\t\\tkwargs = dict(flags=os.O_WRONLY | os.O_TRUNC | os.O_CREAT, mode=o600)\\n\\t\\twith os.fdopen(os.open(filename, **kwargs), mode=\"w\") as f:\\n\\t\\t\\tos.chmod(filename, o600)\\n\\t\\t\\tself._write_private_key(f, key, format, password=password)"
  },
  {
    "code": "def cov(self, min_periods=None) -> \"DataFrame\":\\n\\t\\tnumeric_df = self._get_numeric_data()\\n\\t\\tcols = numeric_df.columns\\n\\t\\tidx = cols.copy()\\n\\t\\tmat = numeric_df.astype(float, copy=False).to_numpy()\\n\\t\\tif notna(mat).all():\\n\\t\\t\\tif min_periods is not None and min_periods > len(mat):\\n\\t\\t\\t\\tbase_cov = np.empty((mat.shape[1], mat.shape[1]))\\n\\t\\t\\t\\tbase_cov.fill(np.nan)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbase_cov = np.cov(mat.T)\\n\\t\\t\\tbase_cov = base_cov.reshape((len(cols), len(cols)))\\n\\t\\telse:\\n\\t\\t\\tbase_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\\n\\t\\treturn self._constructor(base_cov, index=idx, columns=cols)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cov(self, min_periods=None) -> \"DataFrame\":\\n\\t\\tnumeric_df = self._get_numeric_data()\\n\\t\\tcols = numeric_df.columns\\n\\t\\tidx = cols.copy()\\n\\t\\tmat = numeric_df.astype(float, copy=False).to_numpy()\\n\\t\\tif notna(mat).all():\\n\\t\\t\\tif min_periods is not None and min_periods > len(mat):\\n\\t\\t\\t\\tbase_cov = np.empty((mat.shape[1], mat.shape[1]))\\n\\t\\t\\t\\tbase_cov.fill(np.nan)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbase_cov = np.cov(mat.T)\\n\\t\\t\\tbase_cov = base_cov.reshape((len(cols), len(cols)))\\n\\t\\telse:\\n\\t\\t\\tbase_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\\n\\t\\treturn self._constructor(base_cov, index=idx, columns=cols)"
  },
  {
    "code": "def enterabs(self, time, priority, action, argument):\\n        event = Event(time, priority, action, argument)\\n        heapq.heappush(self._queue, event)\\n        return event",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix 13245: sched.scheduler class constructor's timefunc and delayfunct parameters are now optional. scheduler.enter and scheduler.enterabs methods gained a new kwargs parameter.\\n\\nPatch contributed by Matt Mulsow.",
    "fixed_code": "def enterabs(self, time, priority, action, argument=[], kwargs={}):\\n        event = Event(time, priority, action, argument, kwargs)\\n        heapq.heappush(self._queue, event)\\n        return event"
  },
  {
    "code": "def _wrap_agged_blocks(self, items, blocks):\\t\\tif not self.as_index:\\t\\t\\tindex = np.arange(blocks[0].values.shape[1])\\t\\t\\tmgr = BlockManager(blocks, [items, index])\\t\\t\\tresult = DataFrame(mgr)\\t\\t\\tself._insert_inaxis_grouper_inplace(result)\\t\\t\\tresult = result._consolidate()\\t\\telse:\\t\\t\\tindex = self.grouper.result_index\\t\\t\\tmgr = BlockManager(blocks, [items, index])\\t\\t\\tresult = DataFrame(mgr)\\t\\tif self.axis == 1:\\t\\t\\tresult = result.T\\t\\treturn self._reindex_output(result)._convert(datetime=True)",
    "label": 1,
    "bug_type": "numeric",
    "bug_description": "BUG: first/last lose timezone in groupby with as_index=False (#21573)",
    "fixed_code": "def _wrap_agged_blocks(self, items, blocks):\\n\\t\\tobj = self._obj_with_exclusions\\n\\t\\tnew_axes = list(obj._data.axes)\\n\\t\\tif self.axis == 0:\\n\\t\\t\\tnew_axes[0], new_axes[1] = new_axes[1], self.grouper.result_index\\n\\t\\telse:\\n\\t\\t\\tnew_axes[self.axis] = self.grouper.result_index\\n\\t\\tassert new_axes[0].equals(items)\\n\\t\\tnew_axes[0] = items\\n\\t\\tmgr = BlockManager(blocks, new_axes)\\n\\t\\tnew_obj = type(obj)(mgr)\\n\\t\\treturn self._post_process_cython_aggregate(new_obj)\\n\\t_block_agg_axis = 0"
  },
  {
    "code": "def _compute_plot_data(self):\\n        data = self.data\\n        if isinstance(data, ABCSeries):\\n            label = self.label\\n            if label is None and data.name is None:\\n                label = \"None\"\\n            data = data.to_frame(name=label)\\n        data = data._convert(datetime=True, timedelta=True)\\n        include_type = [np.number, \"datetime\", \"datetimetz\", \"timedelta\"]\\n        if self.include_bool is True:\\n            include_type.append(np.bool_)\\n        exclude_type = None\\n        if self._kind == \"box\":\\n            include_type = [np.number]\\n            exclude_type = [\"timedelta\"]\\n        if self._kind == \"scatter\":\\n            include_type.extend([\"object\", \"category\"])\\n        numeric_data = data.select_dtypes(include=include_type, exclude=exclude_type)\\n        try:\\n            is_empty = numeric_data.columns.empty\\n        except AttributeError:\\n            is_empty = not len(numeric_data)\\n        if is_empty:\\n            raise TypeError(\"no numeric data to plot\")\\n        numeric_data = numeric_data.copy()\\n        for col in numeric_data:\\n            numeric_data[col] = np.asarray(numeric_data[col])\\n        self.data = numeric_data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fixes plotting with nullable integers (#32073) (#38014)",
    "fixed_code": "def _compute_plot_data(self):\\n        data = self.data\\n        if isinstance(data, ABCSeries):\\n            label = self.label\\n            if label is None and data.name is None:\\n                label = \"None\"\\n            data = data.to_frame(name=label)\\n        data = data._convert(datetime=True, timedelta=True)\\n        include_type = [np.number, \"datetime\", \"datetimetz\", \"timedelta\"]\\n        if self.include_bool is True:\\n            include_type.append(np.bool_)\\n        exclude_type = None\\n        if self._kind == \"box\":\\n            include_type = [np.number]\\n            exclude_type = [\"timedelta\"]\\n        if self._kind == \"scatter\":\\n            include_type.extend([\"object\", \"category\"])\\n        numeric_data = data.select_dtypes(include=include_type, exclude=exclude_type)\\n        try:\\n            is_empty = numeric_data.columns.empty\\n        except AttributeError:\\n            is_empty = not len(numeric_data)\\n        if is_empty:\\n            raise TypeError(\"no numeric data to plot\")\\n        self.data = numeric_data.apply(self._convert_to_ndarray)"
  },
  {
    "code": "def stop(verbose: bool, dry_run: bool, preserve_volumes: bool):\\n    command_to_execute = ['docker-compose', 'down', \"--remove-orphans\"]\\n    if not preserve_volumes:\\n        command_to_execute.append(\"--volumes\")\\n    shell_params = ShellParams(verbose=verbose, backend=\"all\", include_mypy_volume=True)\\n    env_variables = get_env_variables_for_docker_commands(shell_params)\\n    run_command(command_to_execute, verbose=verbose, dry_run=dry_run, env=env_variables)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make breeze works with latest docker-compose (#26233)\\n\\nThe latest docker-compose dropped an alias for `docker-compose` and\\nwe need to detect it and use \"docker compose\" instead.",
    "fixed_code": "def stop(verbose: bool, dry_run: bool, preserve_volumes: bool):\\n    perform_environment_checks(verbose=verbose)\\n    command_to_execute = [*DOCKER_COMPOSE_COMMAND, 'down', \"--remove-orphans\"]\\n    if not preserve_volumes:\\n        command_to_execute.append(\"--volumes\")\\n    shell_params = ShellParams(verbose=verbose, backend=\"all\", include_mypy_volume=True)\\n    env_variables = get_env_variables_for_docker_commands(shell_params)\\n    run_command(command_to_execute, verbose=verbose, dry_run=dry_run, env=env_variables)"
  },
  {
    "code": "def create_widgets(self):\\n        self.frame = frame = Frame(self, borderwidth=2, relief='sunken')\\n        frame.pack(side='top', expand=True, fill='both')\\n        frame_buttons = Frame(self)\\n        frame_buttons.pack(side='bottom', fill='x')\\n        self.button_ok = Button(frame_buttons, text='OK',\\n                                width=8, command=self.ok)\\n        self.button_ok.grid(row=0, column=0, padx=5, pady=5)\\n        self.button_cancel = Button(frame_buttons, text='Cancel',\\n                                   width=8, command=self.cancel)\\n        self.button_cancel.grid(row=0, column=1, padx=5, pady=5)\\n        self.frame_keyseq_basic = Frame(frame, name='keyseq_basic')\\n        self.frame_keyseq_basic.grid(row=0, column=0, sticky='nsew',\\n                                      padx=5, pady=5)\\n        basic_title = Label(self.frame_keyseq_basic,\\n                            text=f\"New keys for '{self.action}' :\")\\n        basic_title.pack(anchor='w')\\n        basic_keys = Label(self.frame_keyseq_basic, justify='left',\\n                           textvariable=self.key_string, relief='groove',\\n                           borderwidth=2)\\n        basic_keys.pack(ipadx=5, ipady=5, fill='x')\\n        self.frame_controls_basic = Frame(frame)\\n        self.frame_controls_basic.grid(row=1, column=0, sticky='nsew', padx=5)\\n        self.modifier_checkbuttons = {}\\n        column = 0\\n        for modifier, variable in zip(self.modifiers, self.modifier_vars):\\n            label = self.modifier_label.get(modifier, modifier)\\n            check = Checkbutton(self.frame_controls_basic,\\n                                command=self.build_key_string, text=label,\\n                                variable=variable, onvalue=modifier, offvalue='')\\n            check.grid(row=0, column=column, padx=2, sticky='w')\\n            self.modifier_checkbuttons[modifier] = check\\n            column += 1\\n        help_basic = Label(self.frame_controls_basic, justify='left',\\n                           text=\"Select the desired modifier keys\\n\"+\\n                                \"above, and the final key from the\\n\"+\\n                                \"list on the right.\\n\" +\\n                                \"Use upper case Symbols when using\\n\" +\\n                                \"the Shift modifier.  (Letters will be\\n\" +\\n                                \"converted automatically.)\")\\n        help_basic.grid(row=1, column=0, columnspan=4, padx=2, sticky='w')\\n        self.list_keys_final = Listbox(self.frame_controls_basic, width=15,\\n                                       height=10, selectmode='single')\\n        self.list_keys_final.insert('end', *AVAILABLE_KEYS)\\n        self.list_keys_final.bind('<ButtonRelease-1>', self.final_key_selected)\\n        self.list_keys_final.grid(row=0, column=4, rowspan=4, sticky='ns')\\n        scroll_keys_final = Scrollbar(self.frame_controls_basic,\\n                                      orient='vertical',\\n                                      command=self.list_keys_final.yview)\\n        self.list_keys_final.config(yscrollcommand=scroll_keys_final.set)\\n        scroll_keys_final.grid(row=0, column=5, rowspan=4, sticky='ns')\\n        self.button_clear = Button(self.frame_controls_basic,\\n                                   text='Clear Keys',\\n                                   command=self.clear_key_seq)\\n        self.button_clear.grid(row=2, column=0, columnspan=4)\\n        self.frame_keyseq_advanced = Frame(frame, name='keyseq_advanced')\\n        self.frame_keyseq_advanced.grid(row=0, column=0, sticky='nsew',\\n                                         padx=5, pady=5)\\n        advanced_title = Label(self.frame_keyseq_advanced, justify='left',\\n                               text=f\"Enter new binding(s) for '{self.action}' :\\n\" +\\n                                     \"(These bindings will not be checked for validity!)\")\\n        advanced_title.pack(anchor='w')\\n        self.advanced_keys = Entry(self.frame_keyseq_advanced,\\n                                   textvariable=self.key_string)\\n        self.advanced_keys.pack(fill='x')\\n        self.frame_help_advanced = Frame(frame)\\n        self.frame_help_advanced.grid(row=1, column=0, sticky='nsew', padx=5)\\n        help_advanced = Label(self.frame_help_advanced, justify='left',\\n            text=\"Key bindings are specified using Tkinter keysyms as\\n\"+\\n                 \"in these samples: <Control-f>, <Shift-F2>, <F12>,\\n\"\\n                 \"<Control-space>, <Meta-less>, <Control-Alt-Shift-X>.\\n\"\\n                 \"Upper case is used when the Shift modifier is present!\\n\" +\\n                 \"'Emacs style' multi-keystroke bindings are specified as\\n\" +\\n                 \"follows: <Control-x><Control-y>, where the first key\\n\" +\\n                 \"is the 'do-nothing' keybinding.\\n\" +\\n                 \"Multiple separate bindings for one action should be\\n\"+\\n                 \"separated by a space, eg., <Alt-v> <Meta-v>.\" )\\n        help_advanced.grid(row=0, column=0, sticky='nsew')\\n        self.button_level = Button(frame, command=self.toggle_level,\\n                                  text='<< Basic Key Binding Entry')\\n        self.button_level.grid(row=2, column=0, stick='ew', padx=5, pady=5)\\n        self.toggle_level()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-35675: IDLE  - separate config_key window and frame (#11427)\\n\\nbpo-35598: IDLE: Refactor window and frame class",
    "fixed_code": "def create_widgets(self):\\n        self.frame_keyseq_basic = Frame(self, name='keyseq_basic')\\n        self.frame_keyseq_basic.grid(row=0, column=0, sticky='nsew',\\n                                      padx=5, pady=5)\\n        basic_title = Label(self.frame_keyseq_basic,\\n                            text=f\"New keys for '{self.action}' :\")\\n        basic_title.pack(anchor='w')\\n        basic_keys = Label(self.frame_keyseq_basic, justify='left',\\n                           textvariable=self.key_string, relief='groove',\\n                           borderwidth=2)\\n        basic_keys.pack(ipadx=5, ipady=5, fill='x')\\n        self.frame_controls_basic = Frame(self)\\n        self.frame_controls_basic.grid(row=1, column=0, sticky='nsew', padx=5)\\n        self.modifier_checkbuttons = {}\\n        column = 0\\n        for modifier, variable in zip(self.modifiers, self.modifier_vars):\\n            label = self.modifier_label.get(modifier, modifier)\\n            check = Checkbutton(self.frame_controls_basic,\\n                                command=self.build_key_string, text=label,\\n                                variable=variable, onvalue=modifier, offvalue='')\\n            check.grid(row=0, column=column, padx=2, sticky='w')\\n            self.modifier_checkbuttons[modifier] = check\\n            column += 1\\n        help_basic = Label(self.frame_controls_basic, justify='left',\\n                           text=\"Select the desired modifier keys\\n\"+\\n                                \"above, and the final key from the\\n\"+\\n                                \"list on the right.\\n\" +\\n                                \"Use upper case Symbols when using\\n\" +\\n                                \"the Shift modifier.  (Letters will be\\n\" +\\n                                \"converted automatically.)\")\\n        help_basic.grid(row=1, column=0, columnspan=4, padx=2, sticky='w')\\n        self.list_keys_final = Listbox(self.frame_controls_basic, width=15,\\n                                       height=10, selectmode='single')\\n        self.list_keys_final.insert('end', *AVAILABLE_KEYS)\\n        self.list_keys_final.bind('<ButtonRelease-1>', self.final_key_selected)\\n        self.list_keys_final.grid(row=0, column=4, rowspan=4, sticky='ns')\\n        scroll_keys_final = Scrollbar(self.frame_controls_basic,\\n                                      orient='vertical',\\n                                      command=self.list_keys_final.yview)\\n        self.list_keys_final.config(yscrollcommand=scroll_keys_final.set)\\n        scroll_keys_final.grid(row=0, column=5, rowspan=4, sticky='ns')\\n        self.button_clear = Button(self.frame_controls_basic,\\n                                   text='Clear Keys',\\n                                   command=self.clear_key_seq)\\n        self.button_clear.grid(row=2, column=0, columnspan=4)\\n        self.frame_keyseq_advanced = Frame(self, name='keyseq_advanced')\\n        self.frame_keyseq_advanced.grid(row=0, column=0, sticky='nsew',\\n                                         padx=5, pady=5)\\n        advanced_title = Label(self.frame_keyseq_advanced, justify='left',\\n                               text=f\"Enter new binding(s) for '{self.action}' :\\n\" +\\n                                     \"(These bindings will not be checked for validity!)\")\\n        advanced_title.pack(anchor='w')\\n        self.advanced_keys = Entry(self.frame_keyseq_advanced,\\n                                   textvariable=self.key_string)\\n        self.advanced_keys.pack(fill='x')\\n        self.frame_help_advanced = Frame(self)\\n        self.frame_help_advanced.grid(row=1, column=0, sticky='nsew', padx=5)\\n        help_advanced = Label(self.frame_help_advanced, justify='left',\\n            text=\"Key bindings are specified using Tkinter keysyms as\\n\"+\\n                 \"in these samples: <Control-f>, <Shift-F2>, <F12>,\\n\"\\n                 \"<Control-space>, <Meta-less>, <Control-Alt-Shift-X>.\\n\"\\n                 \"Upper case is used when the Shift modifier is present!\\n\" +\\n                 \"'Emacs style' multi-keystroke bindings are specified as\\n\" +\\n                 \"follows: <Control-x><Control-y>, where the first key\\n\" +\\n                 \"is the 'do-nothing' keybinding.\\n\" +\\n                 \"Multiple separate bindings for one action should be\\n\"+\\n                 \"separated by a space, eg., <Alt-v> <Meta-v>.\" )\\n        help_advanced.grid(row=0, column=0, sticky='nsew')\\n        self.button_level = Button(self, command=self.toggle_level,\\n                                  text='<< Basic Key Binding Entry')\\n        self.button_level.grid(row=2, column=0, stick='ew', padx=5, pady=5)\\n        self.toggle_level()"
  },
  {
    "code": "def get_commands_to_config_vpc(module, vpc, domain, existing):\\n    vpc = dict(vpc)\\n    domain_only = vpc.get('domain')\\n    commands = []\\n    if 'pkl_dest' in vpc:\\n        pkl_command = 'peer-keepalive destination {pkl_dest}'.format(**vpc)\\n        if 'pkl_src' in vpc:\\n            pkl_command += ' source {pkl_src}'.format(**vpc)\\n        if 'pkl_vrf' in vpc:\\n            pkl_command += ' vrf {pkl_vrf}'.format(**vpc)\\n        commands.append(pkl_command)\\n    if 'auto_recovery' in vpc:\\n        if not vpc.get('auto_recovery'):\\n            vpc['auto_recovery'] = 'no'\\n        else:\\n            vpc['auto_recovery'] = ''\\n    if 'peer_gw' in vpc:\\n        if not vpc.get('peer_gw'):\\n            vpc['peer_gw'] = 'no'\\n        else:\\n            vpc['peer_gw'] = ''\\n    for param in vpc:\\n        command = CONFIG_ARGS.get(param)\\n        if command is not None:\\n            command = command.format(**vpc).strip()\\n            if 'peer-gateway' in command:\\n                commands.append('terminal dont-ask')\\n            commands.append(command)\\n    if commands or domain_only:\\n        commands.insert(0, 'vpc domain {0}'.format(domain))\\n    return commands",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_commands_to_config_vpc(module, vpc, domain, existing):\\n    vpc = dict(vpc)\\n    domain_only = vpc.get('domain')\\n    commands = []\\n    if 'pkl_dest' in vpc:\\n        pkl_command = 'peer-keepalive destination {pkl_dest}'.format(**vpc)\\n        if 'pkl_src' in vpc:\\n            pkl_command += ' source {pkl_src}'.format(**vpc)\\n        if 'pkl_vrf' in vpc:\\n            pkl_command += ' vrf {pkl_vrf}'.format(**vpc)\\n        commands.append(pkl_command)\\n    if 'auto_recovery' in vpc:\\n        if not vpc.get('auto_recovery'):\\n            vpc['auto_recovery'] = 'no'\\n        else:\\n            vpc['auto_recovery'] = ''\\n    if 'peer_gw' in vpc:\\n        if not vpc.get('peer_gw'):\\n            vpc['peer_gw'] = 'no'\\n        else:\\n            vpc['peer_gw'] = ''\\n    for param in vpc:\\n        command = CONFIG_ARGS.get(param)\\n        if command is not None:\\n            command = command.format(**vpc).strip()\\n            if 'peer-gateway' in command:\\n                commands.append('terminal dont-ask')\\n            commands.append(command)\\n    if commands or domain_only:\\n        commands.insert(0, 'vpc domain {0}'.format(domain))\\n    return commands"
  },
  {
    "code": "def _is_indexed_like(obj, other):\\n    if isinstance(obj, Series):\\n        return obj.index.equals(other.index)\\n    elif isinstance(obj, DataFrame):\\n        return obj._indexed_same(other)\\nclass Grouping(object):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: tests pass now",
    "fixed_code": "def _is_indexed_like(obj, other):\\n    if isinstance(obj, Series):\\n        if not isinstance(other, Series):\\n            return False\\n        return obj.index.equals(other.index)\\n    elif isinstance(obj, DataFrame):\\n        if not isinstance(other, DataFrame):\\n            return False\\n        return obj._indexed_same(other)\\n    return False\\nclass Grouping(object):"
  },
  {
    "code": "def __ne__(self, other):\\n        return not self.__eq__(other)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Prevent crash in poly1d.__eq__\\n\\nFixes #8760",
    "fixed_code": "def __ne__(self, other):\\n        if not isinstance(other, poly1d):\\n            return NotImplemented\\n        return not self.__eq__(other)"
  },
  {
    "code": "def _get_boto_key(self, path):\\n        b = self._get_boto_bucket()\\n        key_name = '%s%s' % (self.prefix, path)\\n        return threads.deferToThread(b.get_key, key_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "botocore support for S3FilesStore",
    "fixed_code": "def _get_boto_key(self, path):\\n        key_name = '%s%s' % (self.prefix, path)\\n        if self.is_botocore:\\n            return threads.deferToThread(\\n                self.s3_client.head_object,\\n                Bucket=self.bucket,\\n                Key=key_name)\\n        else:\\n            b = self._get_boto_bucket()\\n            return threads.deferToThread(b.get_key, key_name)"
  },
  {
    "code": "def execute_init(self):\\n\\t\\tgalaxy_type = context.CLIARGS['type']\\n\\t\\tinit_path = context.CLIARGS['init_path']\\n\\t\\tforce = context.CLIARGS['force']\\n\\t\\tobj_skeleton = context.CLIARGS['{0}_skeleton'.format(galaxy_type)]\\n\\t\\tobj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\\n\\t\\tinject_data = dict(\\n\\t\\t\\tdescription='your description',\\n\\t\\t\\tansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\\n\\t\\t)\\n\\t\\tif galaxy_type == 'role':\\n\\t\\t\\tinject_data.update(dict(\\n\\t\\t\\t\\tauthor='your name',\\n\\t\\t\\t\\tcompany='your company (optional)',\\n\\t\\t\\t\\tlicense='license (GPL-2.0-or-later, MIT, etc)',\\n\\t\\t\\t\\trole_name=obj_name,\\n\\t\\t\\t\\trole_type=context.CLIARGS['role_type'],\\n\\t\\t\\t\\tissue_tracker_url='http://example.com/issue/tracker',\\n\\t\\t\\t\\trepository_url='http://example.com/repository',\\n\\t\\t\\t\\tdocumentation_url='http://docs.example.com',\\n\\t\\t\\t\\thomepage_url='http://example.com',\\n\\t\\t\\t\\tmin_ansible_version=ansible_version[:3],  \\n\\t\\t\\t))\\n\\t\\t\\tobj_path = os.path.join(init_path, obj_name)\\n\\t\\telif galaxy_type == 'collection':\\n\\t\\t\\tnamespace, collection_name = obj_name.split('.', 1)\\n\\t\\t\\tinject_data.update(dict(\\n\\t\\t\\t\\tnamespace=namespace,\\n\\t\\t\\t\\tcollection_name=collection_name,\\n\\t\\t\\t\\tversion='1.0.0',\\n\\t\\t\\t\\treadme='README.md',\\n\\t\\t\\t\\tauthors=['your name <example@domain.com>'],\\n\\t\\t\\t\\tlicense=['GPL-2.0-or-later'],\\n\\t\\t\\t\\trepository='http://example.com/repository',\\n\\t\\t\\t\\tdocumentation='http://docs.example.com',\\n\\t\\t\\t\\thomepage='http://example.com',\\n\\t\\t\\t\\tissues='http://example.com/issue/tracker',\\n\\t\\t\\t))\\n\\t\\t\\tobj_path = os.path.join(init_path, namespace, collection_name)\\n\\t\\tb_obj_path = to_bytes(obj_path, errors='surrogate_or_strict')\\n\\t\\tif os.path.exists(b_obj_path):\\n\\t\\t\\tif os.path.isfile(obj_path):\\n\\t\\t\\t\\traise AnsibleError(\"- the path %s already exists, but is a file - aborting\" % to_native(obj_path))\\n\\t\\t\\telif not force:\\n\\t\\t\\t\\traise AnsibleError(\"- the directory %s already exists. \"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"You can use --force to re-initialize this directory,\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"however it will reset any main.yml files that may have\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"been modified there already.\" % to_native(obj_path))\\n\\t\\tif obj_skeleton is not None:\\n\\t\\t\\town_skeleton = False\\n\\t\\t\\tskeleton_ignore_expressions = C.GALAXY_ROLE_SKELETON_IGNORE\\n\\t\\telse:\\n\\t\\t\\town_skeleton = True\\n\\t\\t\\tobj_skeleton = self.galaxy.default_role_skeleton_path\\n\\t\\t\\tskeleton_ignore_expressions = ['^.*/.git_keep$']\\n\\t\\tobj_skeleton = os.path.expanduser(obj_skeleton)\\n\\t\\tskeleton_ignore_re = [re.compile(x) for x in skeleton_ignore_expressions]\\n\\t\\tif not os.path.exists(obj_skeleton):\\n\\t\\t\\traise AnsibleError(\"- the skeleton path '{0}' does not exist, cannot init {1}\".format(\\n\\t\\t\\t\\tto_native(obj_skeleton), galaxy_type)\\n\\t\\t\\t)\\n\\t\\ttemplate_env = Environment(loader=FileSystemLoader(obj_skeleton))\\n\\t\\tif not os.path.exists(b_obj_path):\\n\\t\\t\\tos.makedirs(b_obj_path)\\n\\t\\tfor root, dirs, files in os.walk(obj_skeleton, topdown=True):\\n\\t\\t\\trel_root = os.path.relpath(root, obj_skeleton)\\n\\t\\t\\trel_dirs = rel_root.split(os.sep)\\n\\t\\t\\trel_root_dir = rel_dirs[0]\\n\\t\\t\\tif galaxy_type == 'collection':\\n\\t\\t\\t\\tin_templates_dir = rel_root_dir in ['playbooks', 'roles'] and 'templates' in rel_dirs\\n\\t\\t\\telse:\\n\\t\\t\\t\\tin_templates_dir = rel_root_dir == 'templates'\\n\\t\\t\\tdirs[:] = [d for d in dirs if not any(r.match(d) for r in skeleton_ignore_re)]\\n\\t\\t\\tfor f in files:\\n\\t\\t\\t\\tfilename, ext = os.path.splitext(f)\\n\\t\\t\\t\\tif any(r.match(os.path.join(rel_root, f)) for r in skeleton_ignore_re):\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\telif galaxy_type == 'collection' and own_skeleton and rel_root == '.' and f == 'galaxy.yml.j2':\\n\\t\\t\\t\\t\\ttemplate_data = inject_data.copy()\\n\\t\\t\\t\\t\\ttemplate_data['name'] = template_data.pop('collection_name')\\n\\t\\t\\t\\t\\tmeta_value = GalaxyCLI._get_skeleton_galaxy_yml(os.path.join(root, rel_root, f), template_data)\\n\\t\\t\\t\\t\\tb_dest_file = to_bytes(os.path.join(obj_path, rel_root, filename), errors='surrogate_or_strict')\\n\\t\\t\\t\\t\\twith open(b_dest_file, 'wb') as galaxy_obj:\\n\\t\\t\\t\\t\\t\\tgalaxy_obj.write(to_bytes(meta_value, errors='surrogate_or_strict'))\\n\\t\\t\\t\\telif ext == \".j2\" and not in_templates_dir:\\n\\t\\t\\t\\t\\tsrc_template = os.path.join(rel_root, f)\\n\\t\\t\\t\\t\\tdest_file = os.path.join(obj_path, rel_root, filename)\\n\\t\\t\\t\\t\\ttemplate_env.get_template(src_template).stream(inject_data).dump(dest_file, encoding='utf-8')\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tf_rel_path = os.path.relpath(os.path.join(root, f), obj_skeleton)\\n\\t\\t\\t\\t\\tshutil.copyfile(os.path.join(root, f), os.path.join(obj_path, f_rel_path))\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\tb_dir_path = to_bytes(os.path.join(obj_path, rel_root, d), errors='surrogate_or_strict')\\n\\t\\t\\t\\tif not os.path.exists(b_dir_path):\\n\\t\\t\\t\\t\\tos.makedirs(b_dir_path)\\n\\t\\tdisplay.display(\"- %s was created successfully\" % obj_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Minor fixes in galaxy command for collection (#59846)",
    "fixed_code": "def execute_init(self):\\n\\t\\tgalaxy_type = context.CLIARGS['type']\\n\\t\\tinit_path = context.CLIARGS['init_path']\\n\\t\\tforce = context.CLIARGS['force']\\n\\t\\tobj_skeleton = context.CLIARGS['{0}_skeleton'.format(galaxy_type)]\\n\\t\\tobj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\\n\\t\\tinject_data = dict(\\n\\t\\t\\tdescription='your {0} description'.format(galaxy_type),\\n\\t\\t\\tansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\\n\\t\\t)\\n\\t\\tif galaxy_type == 'role':\\n\\t\\t\\tinject_data.update(dict(\\n\\t\\t\\t\\tauthor='your name',\\n\\t\\t\\t\\tcompany='your company (optional)',\\n\\t\\t\\t\\tlicense='license (GPL-2.0-or-later, MIT, etc)',\\n\\t\\t\\t\\trole_name=obj_name,\\n\\t\\t\\t\\trole_type=context.CLIARGS['role_type'],\\n\\t\\t\\t\\tissue_tracker_url='http://example.com/issue/tracker',\\n\\t\\t\\t\\trepository_url='http://example.com/repository',\\n\\t\\t\\t\\tdocumentation_url='http://docs.example.com',\\n\\t\\t\\t\\thomepage_url='http://example.com',\\n\\t\\t\\t\\tmin_ansible_version=ansible_version[:3],  \\n\\t\\t\\t))\\n\\t\\t\\tobj_path = os.path.join(init_path, obj_name)\\n\\t\\telif galaxy_type == 'collection':\\n\\t\\t\\tnamespace, collection_name = obj_name.split('.', 1)\\n\\t\\t\\tinject_data.update(dict(\\n\\t\\t\\t\\tnamespace=namespace,\\n\\t\\t\\t\\tcollection_name=collection_name,\\n\\t\\t\\t\\tversion='1.0.0',\\n\\t\\t\\t\\treadme='README.md',\\n\\t\\t\\t\\tauthors=['your name <example@domain.com>'],\\n\\t\\t\\t\\tlicense=['GPL-2.0-or-later'],\\n\\t\\t\\t\\trepository='http://example.com/repository',\\n\\t\\t\\t\\tdocumentation='http://docs.example.com',\\n\\t\\t\\t\\thomepage='http://example.com',\\n\\t\\t\\t\\tissues='http://example.com/issue/tracker',\\n\\t\\t\\t))\\n\\t\\t\\tobj_path = os.path.join(init_path, namespace, collection_name)\\n\\t\\tb_obj_path = to_bytes(obj_path, errors='surrogate_or_strict')\\n\\t\\tif os.path.exists(b_obj_path):\\n\\t\\t\\tif os.path.isfile(obj_path):\\n\\t\\t\\t\\traise AnsibleError(\"- the path %s already exists, but is a file - aborting\" % to_native(obj_path))\\n\\t\\t\\telif not force:\\n\\t\\t\\t\\traise AnsibleError(\"- the directory %s already exists. \"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"You can use --force to re-initialize this directory,\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"however it will reset any main.yml files that may have\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"been modified there already.\" % to_native(obj_path))\\n\\t\\tif obj_skeleton is not None:\\n\\t\\t\\town_skeleton = False\\n\\t\\t\\tskeleton_ignore_expressions = C.GALAXY_ROLE_SKELETON_IGNORE\\n\\t\\telse:\\n\\t\\t\\town_skeleton = True\\n\\t\\t\\tobj_skeleton = self.galaxy.default_role_skeleton_path\\n\\t\\t\\tskeleton_ignore_expressions = ['^.*/.git_keep$']\\n\\t\\tobj_skeleton = os.path.expanduser(obj_skeleton)\\n\\t\\tskeleton_ignore_re = [re.compile(x) for x in skeleton_ignore_expressions]\\n\\t\\tif not os.path.exists(obj_skeleton):\\n\\t\\t\\traise AnsibleError(\"- the skeleton path '{0}' does not exist, cannot init {1}\".format(\\n\\t\\t\\t\\tto_native(obj_skeleton), galaxy_type)\\n\\t\\t\\t)\\n\\t\\ttemplate_env = Environment(loader=FileSystemLoader(obj_skeleton))\\n\\t\\tif not os.path.exists(b_obj_path):\\n\\t\\t\\tos.makedirs(b_obj_path)\\n\\t\\tfor root, dirs, files in os.walk(obj_skeleton, topdown=True):\\n\\t\\t\\trel_root = os.path.relpath(root, obj_skeleton)\\n\\t\\t\\trel_dirs = rel_root.split(os.sep)\\n\\t\\t\\trel_root_dir = rel_dirs[0]\\n\\t\\t\\tif galaxy_type == 'collection':\\n\\t\\t\\t\\tin_templates_dir = rel_root_dir in ['playbooks', 'roles'] and 'templates' in rel_dirs\\n\\t\\t\\telse:\\n\\t\\t\\t\\tin_templates_dir = rel_root_dir == 'templates'\\n\\t\\t\\tdirs[:] = [d for d in dirs if not any(r.match(d) for r in skeleton_ignore_re)]\\n\\t\\t\\tfor f in files:\\n\\t\\t\\t\\tfilename, ext = os.path.splitext(f)\\n\\t\\t\\t\\tif any(r.match(os.path.join(rel_root, f)) for r in skeleton_ignore_re):\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\telif galaxy_type == 'collection' and own_skeleton and rel_root == '.' and f == 'galaxy.yml.j2':\\n\\t\\t\\t\\t\\ttemplate_data = inject_data.copy()\\n\\t\\t\\t\\t\\ttemplate_data['name'] = template_data.pop('collection_name')\\n\\t\\t\\t\\t\\tmeta_value = GalaxyCLI._get_skeleton_galaxy_yml(os.path.join(root, rel_root, f), template_data)\\n\\t\\t\\t\\t\\tb_dest_file = to_bytes(os.path.join(obj_path, rel_root, filename), errors='surrogate_or_strict')\\n\\t\\t\\t\\t\\twith open(b_dest_file, 'wb') as galaxy_obj:\\n\\t\\t\\t\\t\\t\\tgalaxy_obj.write(to_bytes(meta_value, errors='surrogate_or_strict'))\\n\\t\\t\\t\\telif ext == \".j2\" and not in_templates_dir:\\n\\t\\t\\t\\t\\tsrc_template = os.path.join(rel_root, f)\\n\\t\\t\\t\\t\\tdest_file = os.path.join(obj_path, rel_root, filename)\\n\\t\\t\\t\\t\\ttemplate_env.get_template(src_template).stream(inject_data).dump(dest_file, encoding='utf-8')\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tf_rel_path = os.path.relpath(os.path.join(root, f), obj_skeleton)\\n\\t\\t\\t\\t\\tshutil.copyfile(os.path.join(root, f), os.path.join(obj_path, f_rel_path))\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\tb_dir_path = to_bytes(os.path.join(obj_path, rel_root, d), errors='surrogate_or_strict')\\n\\t\\t\\t\\tif not os.path.exists(b_dir_path):\\n\\t\\t\\t\\t\\tos.makedirs(b_dir_path)\\n\\t\\tdisplay.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))"
  },
  {
    "code": "def _handle_router_snat_rules(self, ri, ex_gw_port, internal_cidrs,\\n\\t\\t\\t\\t\\t\\t\\t\\t  interface_name, action):\\n\\t\\tri.iptables_manager.ipv4['nat'].empty_chain('POSTROUTING')\\n\\t\\tri.iptables_manager.ipv4['nat'].empty_chain('snat')\\n\\t\\tri.iptables_manager.ipv4['nat'].add_rule('snat', '-j $float-snat')\\n\\t\\tif action == 'add_rules' and ex_gw_port:\\n\\t\\t\\tfor ip_addr in ex_gw_port['fixed_ips']:\\n\\t\\t\\t\\tex_gw_ip = ip_addr['ip_address']\\n\\t\\t\\t\\tif netaddr.IPAddress(ex_gw_ip).version == 4:\\n\\t\\t\\t\\t\\trules = self.external_gateway_nat_rules(ex_gw_ip,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tinternal_cidrs,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tinterface_name)\\n\\t\\t\\t\\t\\tfor rule in rules:\\n\\t\\t\\t\\t\\t\\tri.iptables_manager.ipv4['nat'].add_rule(*rule)\\n\\t\\t\\t\\t\\tbreak\\n\\t\\tri.iptables_manager.apply()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _handle_router_snat_rules(self, ri, ex_gw_port, internal_cidrs,\\n\\t\\t\\t\\t\\t\\t\\t\\t  interface_name, action):\\n\\t\\tri.iptables_manager.ipv4['nat'].empty_chain('POSTROUTING')\\n\\t\\tri.iptables_manager.ipv4['nat'].empty_chain('snat')\\n\\t\\tri.iptables_manager.ipv4['nat'].add_rule('snat', '-j $float-snat')\\n\\t\\tif action == 'add_rules' and ex_gw_port:\\n\\t\\t\\tfor ip_addr in ex_gw_port['fixed_ips']:\\n\\t\\t\\t\\tex_gw_ip = ip_addr['ip_address']\\n\\t\\t\\t\\tif netaddr.IPAddress(ex_gw_ip).version == 4:\\n\\t\\t\\t\\t\\trules = self.external_gateway_nat_rules(ex_gw_ip,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tinternal_cidrs,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tinterface_name)\\n\\t\\t\\t\\t\\tfor rule in rules:\\n\\t\\t\\t\\t\\t\\tri.iptables_manager.ipv4['nat'].add_rule(*rule)\\n\\t\\t\\t\\t\\tbreak\\n\\t\\tri.iptables_manager.apply()"
  },
  {
    "code": "def paginator_number(cl,i):\\n\\tif i == DOT:\\n\\t\\treturn '... '\\n\\telif i == cl.page_num:\\n\\t\\treturn format_html('{0}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Start attacking E231 violations",
    "fixed_code": "def paginator_number(cl, i):\\n\\tif i == DOT:\\n\\t\\treturn '... '\\n\\telif i == cl.page_num:\\n\\t\\treturn format_html('{0}"
  },
  {
    "code": "def __init__(self, iterable=None, **kwds):\\n        gallahad')                 \\n        >>> c = Counter({'a': 4, 'b': 2})           \\n        >>> c = Counter(a=4, b=2)                   \\n        '''\\n        self.update(iterable, **kwds)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, iterable=None, **kwds):\\n        gallahad')                 \\n        >>> c = Counter({'a': 4, 'b': 2})           \\n        >>> c = Counter(a=4, b=2)                   \\n        '''\\n        self.update(iterable, **kwds)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        asn=dict(required=True, type='str'),\\n        vrf=dict(required=False, type='str', default='default'),\\n        neighbor=dict(required=True, type='str'),\\n        description=dict(required=False, type='str'),\\n        capability_negotiation=dict(required=False, type='bool'),\\n        connected_check=dict(required=False, type='bool'),\\n        dynamic_capability=dict(required=False, type='bool'),\\n        ebgp_multihop=dict(required=False, type='str'),\\n        local_as=dict(required=False, type='str'),\\n        log_neighbor_changes=dict(required=False, type='str', choices=['enable', 'disable', 'inherit']),\\n        low_memory_exempt=dict(required=False, type='bool'),\\n        maximum_peers=dict(required=False, type='str'),\\n        pwd=dict(required=False, type='str'),\\n        pwd_type=dict(required=False, type='str', choices=['3des', 'cisco_type_7', 'default']),\\n        remote_as=dict(required=False, type='str'),\\n        remove_private_as=dict(required=False, type='str', choices=['enable', 'disable', 'all', 'replace-as']),\\n        shutdown=dict(required=False, type='bool'),\\n        suppress_4_byte_as=dict(required=False, type='bool'),\\n        timers_keepalive=dict(required=False, type='str'),\\n        timers_holdtime=dict(required=False, type='str'),\\n        transport_passive_only=dict(required=False, type='bool'),\\n        update_source=dict(required=False, type='str'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False)\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_together=[['timers_holdtime', 'timers_keepalive'], ['pwd', 'pwd_type']],\\n        supports_check_mode=True,\\n    )\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    state = module.params['state']\\n    if module.params['pwd_type'] == 'default':\\n        module.params['pwd_type'] = '0'\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args, warnings)\\n    if existing.get('asn') and state == 'present':\\n        if existing['asn'] != module.params['asn']:\\n            module.fail_json(msg='Another BGP ASN already exists.',\\n                             proposed_asn=module.params['asn'],\\n                             existing_asn=existing.get('asn'))\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key not in ['asn', 'vrf', 'neighbor', 'pwd_type']:\\n            if str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key, 'default')\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif state == 'absent' and existing:\\n        state_absent(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        load_config(module, candidate)\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    else:\\n        result['commands'] = []\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_bgp_neighbor: Add bfd support (#56932)\\n\\n\\n- Add support for bfd state in nxos_bgp_neighbor\\n\\n- Feature Pull Request\\n\\n- nxos_bgp_neighbor\\n\\n- Tested on platforms: `N3K,N6K,N7K,N9K`",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        asn=dict(required=True, type='str'),\\n        vrf=dict(required=False, type='str', default='default'),\\n        neighbor=dict(required=True, type='str'),\\n        description=dict(required=False, type='str'),\\n        bfd=dict(required=False, type='str', choices=['enable', 'disable']),\\n        capability_negotiation=dict(required=False, type='bool'),\\n        connected_check=dict(required=False, type='bool'),\\n        dynamic_capability=dict(required=False, type='bool'),\\n        ebgp_multihop=dict(required=False, type='str'),\\n        local_as=dict(required=False, type='str'),\\n        log_neighbor_changes=dict(required=False, type='str', choices=['enable', 'disable', 'inherit']),\\n        low_memory_exempt=dict(required=False, type='bool'),\\n        maximum_peers=dict(required=False, type='str'),\\n        pwd=dict(required=False, type='str'),\\n        pwd_type=dict(required=False, type='str', choices=['3des', 'cisco_type_7', 'default']),\\n        remote_as=dict(required=False, type='str'),\\n        remove_private_as=dict(required=False, type='str', choices=['enable', 'disable', 'all', 'replace-as']),\\n        shutdown=dict(required=False, type='bool'),\\n        suppress_4_byte_as=dict(required=False, type='bool'),\\n        timers_keepalive=dict(required=False, type='str'),\\n        timers_holdtime=dict(required=False, type='str'),\\n        transport_passive_only=dict(required=False, type='bool'),\\n        update_source=dict(required=False, type='str'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False)\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_together=[['timers_holdtime', 'timers_keepalive'], ['pwd', 'pwd_type']],\\n        supports_check_mode=True,\\n    )\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    state = module.params['state']\\n    if module.params['pwd_type'] == 'default':\\n        module.params['pwd_type'] = '0'\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args, warnings)\\n    if existing.get('asn') and state == 'present':\\n        if existing['asn'] != module.params['asn']:\\n            module.fail_json(msg='Another BGP ASN already exists.',\\n                             proposed_asn=module.params['asn'],\\n                             existing_asn=existing.get('asn'))\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key not in ['asn', 'vrf', 'neighbor', 'pwd_type']:\\n            if str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key, 'default')\\n            if key == 'bfd':\\n                if existing.get('bfd', 'disable') != value:\\n                    proposed[key] = value\\n            elif existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif state == 'absent' and existing:\\n        state_absent(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        load_config(module, candidate)\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    else:\\n        result['commands'] = []\\n    module.exit_json(**result)"
  },
  {
    "code": "def copy_tree(src, dst, preserve_mode=1, preserve_times=1,\\n              preserve_symlinks=0, update=0, verbose=1, dry_run=0):\\n    from distutils.file_util import copy_file\\n    if not dry_run and not os.path.isdir(src):\\n        raise DistutilsFileError, \\\\n              \"cannot copy tree '%s': not a directory\" % src\\n    try:\\n        names = os.listdir(src)\\n    except os.error, (errno, errstr):\\n        if dry_run:\\n            names = []\\n        else:\\n            raise DistutilsFileError, \\\\n                  \"error listing files in '%s': %s\" % (src, errstr)\\n    if not dry_run:\\n        mkpath(dst, verbose=verbose)\\n    outputs = []\\n    for n in names:\\n        src_name = os.path.join(src, n)\\n        dst_name = os.path.join(dst, n)\\n        if n.startswith('.nfs'):\\n            continue\\n        if preserve_symlinks and os.path.islink(src_name):\\n            link_dest = os.readlink(src_name)\\n            if verbose >= 1:\\n                log.info(\"linking %s -> %s\", dst_name, link_dest)\\n            if not dry_run:\\n                os.symlink(link_dest, dst_name)\\n            outputs.append(dst_name)\\n        elif os.path.isdir(src_name):\\n            outputs.extend(\\n                copy_tree(src_name, dst_name, preserve_mode,\\n                          preserve_times, preserve_symlinks, update,\\n                          verbose=verbose, dry_run=dry_run))\\n        else:\\n            copy_file(src_name, dst_name, preserve_mode,\\n                      preserve_times, update, verbose=verbose,\\n                      dry_run=dry_run)\\n            outputs.append(dst_name)\\n    return outputs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy_tree(src, dst, preserve_mode=1, preserve_times=1,\\n              preserve_symlinks=0, update=0, verbose=1, dry_run=0):\\n    from distutils.file_util import copy_file\\n    if not dry_run and not os.path.isdir(src):\\n        raise DistutilsFileError, \\\\n              \"cannot copy tree '%s': not a directory\" % src\\n    try:\\n        names = os.listdir(src)\\n    except os.error, (errno, errstr):\\n        if dry_run:\\n            names = []\\n        else:\\n            raise DistutilsFileError, \\\\n                  \"error listing files in '%s': %s\" % (src, errstr)\\n    if not dry_run:\\n        mkpath(dst, verbose=verbose)\\n    outputs = []\\n    for n in names:\\n        src_name = os.path.join(src, n)\\n        dst_name = os.path.join(dst, n)\\n        if n.startswith('.nfs'):\\n            continue\\n        if preserve_symlinks and os.path.islink(src_name):\\n            link_dest = os.readlink(src_name)\\n            if verbose >= 1:\\n                log.info(\"linking %s -> %s\", dst_name, link_dest)\\n            if not dry_run:\\n                os.symlink(link_dest, dst_name)\\n            outputs.append(dst_name)\\n        elif os.path.isdir(src_name):\\n            outputs.extend(\\n                copy_tree(src_name, dst_name, preserve_mode,\\n                          preserve_times, preserve_symlinks, update,\\n                          verbose=verbose, dry_run=dry_run))\\n        else:\\n            copy_file(src_name, dst_name, preserve_mode,\\n                      preserve_times, update, verbose=verbose,\\n                      dry_run=dry_run)\\n            outputs.append(dst_name)\\n    return outputs"
  },
  {
    "code": "def __call__(self, request):\\n\\t\\tLOG.info(\"%(method)s %(url)s\" % {\"method\": request.method,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \"url\": request.url})\\n\\t\\ttry:\\n\\t\\t\\taction, args, accept = self.deserializer.deserialize(request)\\n\\t\\texcept exception.InvalidContentType:\\n\\t\\t\\tmsg = _(\"Unsupported Content-Type\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\texcept exception.MalformedRequestBody:\\n\\t\\t\\tmsg = _(\"Malformed request body\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\tproject_id = args.pop(\"project_id\", None)\\n\\t\\tif ('nova.context' in request.environ and project_id\\n\\t\\t\\tand project_id != request.environ['nova.context'].project_id):\\n\\t\\t\\tmsg = _(\"Malformed request url\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\ttry:\\n\\t\\t\\taction_result = self.dispatch(request, action, args)\\n\\t\\texcept faults.Fault as ex:\\n\\t\\t\\tLOG.info(_(\"Fault thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = ex\\n\\t\\texcept webob.exc.HTTPException as ex:\\n\\t\\t\\tLOG.info(_(\"HTTP exception thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = faults.Fault(ex)\\n\\t\\tif type(action_result) is dict or action_result is None:\\n\\t\\t\\tresponse = self.serializer.serialize(action_result,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t accept,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t action=action)\\n\\t\\telse:\\n\\t\\t\\tresponse = action_result\\n\\t\\ttry:\\n\\t\\t\\tmsg_dict = dict(url=request.url, status=response.status_int)\\n\\t\\t\\tmsg = _(\"%(url)s returned with HTTP %(status)d\") % msg_dict\\n\\t\\texcept AttributeError, e:\\n\\t\\t\\tmsg_dict = dict(url=request.url, e=e)\\n\\t\\t\\tmsg = _(\"%(url)s returned a fault: %(e)s\" % msg_dict)\\n\\t\\tLOG.info(msg)\\n\\t\\treturn response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __call__(self, request):\\n\\t\\tLOG.info(\"%(method)s %(url)s\" % {\"method\": request.method,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \"url\": request.url})\\n\\t\\ttry:\\n\\t\\t\\taction, args, accept = self.deserializer.deserialize(request)\\n\\t\\texcept exception.InvalidContentType:\\n\\t\\t\\tmsg = _(\"Unsupported Content-Type\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\texcept exception.MalformedRequestBody:\\n\\t\\t\\tmsg = _(\"Malformed request body\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\tproject_id = args.pop(\"project_id\", None)\\n\\t\\tif ('nova.context' in request.environ and project_id\\n\\t\\t\\tand project_id != request.environ['nova.context'].project_id):\\n\\t\\t\\tmsg = _(\"Malformed request url\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\ttry:\\n\\t\\t\\taction_result = self.dispatch(request, action, args)\\n\\t\\texcept faults.Fault as ex:\\n\\t\\t\\tLOG.info(_(\"Fault thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = ex\\n\\t\\texcept webob.exc.HTTPException as ex:\\n\\t\\t\\tLOG.info(_(\"HTTP exception thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = faults.Fault(ex)\\n\\t\\tif type(action_result) is dict or action_result is None:\\n\\t\\t\\tresponse = self.serializer.serialize(action_result,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t accept,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t action=action)\\n\\t\\telse:\\n\\t\\t\\tresponse = action_result\\n\\t\\ttry:\\n\\t\\t\\tmsg_dict = dict(url=request.url, status=response.status_int)\\n\\t\\t\\tmsg = _(\"%(url)s returned with HTTP %(status)d\") % msg_dict\\n\\t\\texcept AttributeError, e:\\n\\t\\t\\tmsg_dict = dict(url=request.url, e=e)\\n\\t\\t\\tmsg = _(\"%(url)s returned a fault: %(e)s\" % msg_dict)\\n\\t\\tLOG.info(msg)\\n\\t\\treturn response"
  },
  {
    "code": "def _write_private_key_file(self, filename, key, format, password=None):\\n\\t\\twith open(filename, \"w\") as f:\\n\\t\\t\\tos.chmod(filename, o600)\\n\\t\\t\\tself._write_private_key(f, key, format, password=password)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix CVE re: PKey.write_private_key chmod race\\n\\nCVE-2022-24302 (see changelog for link)",
    "fixed_code": "def _write_private_key_file(self, filename, key, format, password=None):\\n\\t\\tkwargs = dict(flags=os.O_WRONLY | os.O_TRUNC | os.O_CREAT, mode=o600)\\n\\t\\twith os.fdopen(os.open(filename, **kwargs), mode=\"w\") as f:\\n\\t\\t\\tos.chmod(filename, o600)\\n\\t\\t\\tself._write_private_key(f, key, format, password=password)"
  },
  {
    "code": "def __prepare__(metacls, cls, bases):\\n        metacls._check_for_existing_members(cls, bases)\\n        enum_dict = _EnumDict()\\n        enum_dict._cls_name = cls\\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\\n        if first_enum is not None:\\n            enum_dict['_generate_next_value_'] = getattr(\\n                    first_enum, '_generate_next_value_', None,\\n                    )\\n        return enum_dict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[Enum] EnumMeta.__prepare__ now accepts **kwds (#23917)",
    "fixed_code": "def __prepare__(metacls, cls, bases, **kwds):\\n        metacls._check_for_existing_members(cls, bases)\\n        enum_dict = _EnumDict()\\n        enum_dict._cls_name = cls\\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\\n        if first_enum is not None:\\n            enum_dict['_generate_next_value_'] = getattr(\\n                    first_enum, '_generate_next_value_', None,\\n                    )\\n        return enum_dict"
  },
  {
    "code": "def cache_page(*args, **kwargs):\\n    cache_alias = kwargs.pop('cache', None)\\n    key_prefix = kwargs.pop('key_prefix', None)\\n    assert not kwargs, \"The only keyword arguments are cache and key_prefix\"\\n    if len(args) > 1:\\n        assert len(args) == 2, \"cache_page accepts at most 2 arguments\"\\n        if callable(args[0]):\\n            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[1], cache_alias=cache_alias, key_prefix=key_prefix)(args[0])\\n        elif callable(args[1]):\\n            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)(args[1])\\n        else:\\n            assert False, \"cache_page must be passed a view function if called with two arguments\"\\n    elif len(args) == 1:\\n        if callable(args[0]):\\n            return decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)(args[0])\\n        else:\\n            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)\\n    else:\\n        return decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Deprecated legacy ways of calling cache_page",
    "fixed_code": "def cache_page(*args, **kwargs):\\n    cache_alias = kwargs.pop('cache', None)\\n    key_prefix = kwargs.pop('key_prefix', None)\\n    assert not kwargs, \"The only keyword arguments are cache and key_prefix\""
  },
  {
    "code": "def inverse_transform(self, Xt):\\n        check_is_fitted(self)\\n        if 'onehot' in self.encode:\\n            Xt = self._encoder.inverse_transform(Xt)\\n        Xinv = check_array(Xt, copy=True, dtype=FLOAT_DTYPES)\\n        n_features = self.n_bins_.shape[0]\\n        if Xinv.shape[1] != n_features:\\n            raise ValueError(\"Incorrect number of features. Expecting {}, \"\\n                             \"received {}.\".format(n_features, Xinv.shape[1]))\\n        for jj in range(n_features):\\n            bin_edges = self.bin_edges_[jj]\\n            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\\n            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]\\n        return Xinv",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add dtype parameter to KBinsDiscretizer to manage the output data type (#16335)",
    "fixed_code": "def inverse_transform(self, Xt):\\n        check_is_fitted(self)\\n        if 'onehot' in self.encode:\\n            Xt = self._encoder.inverse_transform(Xt)\\n        Xinv = check_array(Xt, copy=True, dtype=(np.float64, np.float32))\\n        n_features = self.n_bins_.shape[0]\\n        if Xinv.shape[1] != n_features:\\n            raise ValueError(\"Incorrect number of features. Expecting {}, \"\\n                             \"received {}.\".format(n_features, Xinv.shape[1]))\\n        for jj in range(n_features):\\n            bin_edges = self.bin_edges_[jj]\\n            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\\n            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]\\n        return Xinv"
  },
  {
    "code": "def clear_task_instances(\\n    tis,\\n    session,\\n    activate_dag_runs=True,\\n    dag=None,\\n):\\n    job_ids = []\\n    for ti in tis:\\n        if ti.state == State.RUNNING:\\n            if ti.job_id:\\n                ti.state = State.SHUTDOWN\\n                job_ids.append(ti.job_id)\\n        else:\\n            task_id = ti.task_id\\n            if dag and dag.has_task(task_id):\\n                task = dag.get_task(task_id)\\n                ti.refresh_from_task(task)\\n                task_retries = task.retries\\n                ti.max_tries = ti.try_number + task_retries - 1\\n            else:\\n                ti.max_tries = max(ti.max_tries, ti.prev_attempted_tries)\\n            ti.state = State.NONE\\n            session.merge(ti)\\n        session.query(TR).filter(\\n            TR.dag_id == ti.dag_id,\\n            TR.task_id == ti.task_id,\\n            TR.execution_date == ti.execution_date,\\n            TR.try_number == ti.try_number,\\n        ).delete()\\n    if job_ids:\\n        from airflow.jobs.base_job import BaseJob\\n        for job in session.query(BaseJob).filter(BaseJob.id.in_(job_ids)).all():  \\n            job.state = State.SHUTDOWN\\n    if activate_dag_runs and tis:\\n        from airflow.models.dagrun import DagRun  \\n        drs = (\\n            session.query(DagRun)\\n            .filter(\\n                DagRun.dag_id.in_({ti.dag_id for ti in tis}),\\n                DagRun.execution_date.in_({ti.execution_date for ti in tis}),\\n            )\\n            .all()\\n        )\\n        for dr in drs:\\n            dr.state = State.RUNNING\\n            dr.start_date = timezone.utcnow()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Speed up clear_task_instances by doing a single sql delete for TaskReschedule (#14048)\\n\\nClearing large number of tasks takes a long time. Most of the time is spent at this line in clear_task_instances (more than 95% time). This slowness sometimes causes the webserver to timeout because the web_server_worker_timeout is hit.\\n\\n```\\n        # Clear all reschedules related to the ti to clear\\n        session.query(TR).filter(\\n            TR.dag_id == ti.dag_id,\\n            TR.task_id == ti.task_id,\\n            TR.execution_date == ti.execution_date,\\n            TR.try_number == ti.try_number,\\n        ).delete()\\n```\\nThis line was very slow because it's deleting TaskReschedule rows in a for loop one by one.\\n\\nThis PR simply changes this code to delete TaskReschedule in a single sql query with a bunch of OR conditions. It's effectively doing the same, but now it's much faster. \\n\\nSome profiling showed great speed improvement (something like 40 to 50 times faster) compared to the first iteration. So the overall performance should now be 300 times faster than the original for loop deletion.",
    "fixed_code": "def clear_task_instances(\\n    tis,\\n    session,\\n    activate_dag_runs=True,\\n    dag=None,\\n):\\n    job_ids = []\\n    task_id_by_key = defaultdict(lambda: defaultdict(lambda: defaultdict(set)))\\n    for ti in tis:\\n        if ti.state == State.RUNNING:\\n            if ti.job_id:\\n                ti.state = State.SHUTDOWN\\n                job_ids.append(ti.job_id)\\n        else:\\n            task_id = ti.task_id\\n            if dag and dag.has_task(task_id):\\n                task = dag.get_task(task_id)\\n                ti.refresh_from_task(task)\\n                task_retries = task.retries\\n                ti.max_tries = ti.try_number + task_retries - 1\\n            else:\\n                ti.max_tries = max(ti.max_tries, ti.prev_attempted_tries)\\n            ti.state = State.NONE\\n            session.merge(ti)\\n        task_id_by_key[ti.dag_id][ti.execution_date][ti.try_number].add(ti.task_id)\\n    if task_id_by_key:\\n        conditions = or_(\\n            and_(\\n                TR.dag_id == dag_id,\\n                or_(\\n                    and_(\\n                        TR.execution_date == execution_date,\\n                        or_(\\n                            and_(TR.try_number == try_number, TR.task_id.in_(task_ids))\\n                            for try_number, task_ids in task_tries.items()\\n                        ),\\n                    )\\n                    for execution_date, task_tries in dates.items()\\n                ),\\n            )\\n            for dag_id, dates in task_id_by_key.items()\\n        )\\n        delete_qry = TR.__table__.delete().where(conditions)\\n        session.execute(delete_qry)\\n    if job_ids:\\n        from airflow.jobs.base_job import BaseJob\\n        for job in session.query(BaseJob).filter(BaseJob.id.in_(job_ids)).all():  \\n            job.state = State.SHUTDOWN\\n    if activate_dag_runs and tis:\\n        from airflow.models.dagrun import DagRun  \\n        drs = (\\n            session.query(DagRun)\\n            .filter(\\n                DagRun.dag_id.in_({ti.dag_id for ti in tis}),\\n                DagRun.execution_date.in_({ti.execution_date for ti in tis}),\\n            )\\n            .all()\\n        )\\n        for dr in drs:\\n            dr.state = State.RUNNING\\n            dr.start_date = timezone.utcnow()"
  },
  {
    "code": "def _build_find_pod_label_selector(self, context: Optional[dict] = None) -> str:\\n        labels = self._get_ti_pod_labels(context, include_try_number=False)\\n        label_strings = [f'{label_id}={label}' for label_id, label in sorted(labels.items())]\\n        return ','.join(label_strings) + f',{self.POD_CHECKED_KEY}!=True'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix ``KubernetesPodOperator`` with `KubernetesExecutor`` on 2.3.0 (#23371)\\n\\nKubernetesPodOperator was mistakenly trying to reattach to it's\\nKubernetesExecutor worker, where it would get stuck watching itself for\\nlogs. We will properly filter for KPO's only, and ignore\\nKubernetesExecutor workers for good measure.",
    "fixed_code": "def _build_find_pod_label_selector(self, context: Optional[dict] = None) -> str:\\n        labels = self._get_ti_pod_labels(context, include_try_number=False)\\n        label_strings = [f'{label_id}={label}' for label_id, label in sorted(labels.items())]\\n        return ','.join(label_strings) + f',{self.POD_CHECKED_KEY}!=True,!airflow-worker'"
  },
  {
    "code": "def predict(self, X, n_estimators=-1):\\n        if n_estimators == 0:\\n            raise ValueError(\"``n_estimators`` must not equal zero\")\\n        if not self.estimators_:\\n            raise RuntimeError(\\n                (\"{0} is not initialized. \"\\n                 \"Perform a fit first\").format(self.__class__.__name__))\\n        n_classes = self.n_classes_\\n        classes = self.classes_\\n        pred = None\\n        for i, (weight, estimator) in enumerate(\\n                zip(self.weights_, self.estimators_)):\\n            if i == n_estimators:\\n                break\\n            if self.real:\\n                current_pred = estimator.predict_proba(X) + 1e-200\\n                current_pred = (n_classes - 1) * (\\n                    np.log(current_pred) -\\n                    (1. / n_classes) *\\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\\n            else:\\n                current_pred = estimator.predict(X)\\n                current_pred = (\\n                    current_pred == classes[:, np.newaxis]).T * weight\\n            if pred is None:\\n                pred = current_pred\\n            else:\\n                pred += current_pred\\n        return np.array(classes.take(\\n            np.argmax(pred, axis=1), axis=0))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def predict(self, X, n_estimators=-1):\\n        if n_estimators == 0:\\n            raise ValueError(\"``n_estimators`` must not equal zero\")\\n        if not self.estimators_:\\n            raise RuntimeError(\\n                (\"{0} is not initialized. \"\\n                 \"Perform a fit first\").format(self.__class__.__name__))\\n        n_classes = self.n_classes_\\n        classes = self.classes_\\n        pred = None\\n        for i, (weight, estimator) in enumerate(\\n                zip(self.weights_, self.estimators_)):\\n            if i == n_estimators:\\n                break\\n            if self.real:\\n                current_pred = estimator.predict_proba(X) + 1e-200\\n                current_pred = (n_classes - 1) * (\\n                    np.log(current_pred) -\\n                    (1. / n_classes) *\\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\\n            else:\\n                current_pred = estimator.predict(X)\\n                current_pred = (\\n                    current_pred == classes[:, np.newaxis]).T * weight\\n            if pred is None:\\n                pred = current_pred\\n            else:\\n                pred += current_pred\\n        return np.array(classes.take(\\n            np.argmax(pred, axis=1), axis=0))"
  },
  {
    "code": "def _normalize_galaxy_yml_manifest(\\n        galaxy_yml,  \\n        b_galaxy_yml_path,  \\n        require_build_metadata=True,  \\n):\\n    galaxy_yml_schema = (\\n        get_collections_galaxy_meta_info()\\n    )  \\n    mandatory_keys = set()\\n    string_keys = set()  \\n    list_keys = set()  \\n    dict_keys = set()  \\n    for info in galaxy_yml_schema:\\n        if info.get('required', False):\\n            mandatory_keys.add(info['key'])\\n        key_list_type = {\\n            'str': string_keys,\\n            'list': list_keys,\\n            'dict': dict_keys,\\n        }[info.get('type', 'str')]\\n        key_list_type.add(info['key'])\\n    all_keys = frozenset(list(mandatory_keys) + list(string_keys) + list(list_keys) + list(dict_keys))\\n    set_keys = set(galaxy_yml.keys())\\n    missing_keys = mandatory_keys.difference(set_keys)\\n    if missing_keys:\\n        msg = (\\n            \"The collection galaxy.yml at '%s' is missing the following mandatory keys: %s\"\\n            % (to_native(b_galaxy_yml_path), \", \".join(sorted(missing_keys)))\\n        )\\n        if require_build_metadata:\\n            raise AnsibleError(msg)\\n        display.warning(msg)\\n        raise ValueError(msg)\\n    extra_keys = set_keys.difference(all_keys)\\n    if len(extra_keys) > 0:\\n        display.warning(\"Found unknown keys in collection galaxy.yml at '%s': %s\"\\n                        % (to_text(b_galaxy_yml_path), \", \".join(extra_keys)))\\n    for optional_string in string_keys:\\n        if optional_string not in galaxy_yml:\\n            galaxy_yml[optional_string] = None\\n    for optional_list in list_keys:\\n        list_val = galaxy_yml.get(optional_list, None)\\n        if list_val is None:\\n            galaxy_yml[optional_list] = []\\n        elif not isinstance(list_val, list):\\n            galaxy_yml[optional_list] = [list_val]  \\n    for optional_dict in dict_keys:\\n        if optional_dict not in galaxy_yml:\\n            galaxy_yml[optional_dict] = {}\\n    if not galaxy_yml.get('version'):\\n        galaxy_yml['version'] = '*'\\n    return galaxy_yml",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Relax minimal config to enable manifest functionality (#78574)",
    "fixed_code": "def _normalize_galaxy_yml_manifest(\\n        galaxy_yml,  \\n        b_galaxy_yml_path,  \\n        require_build_metadata=True,  \\n):\\n    galaxy_yml_schema = (\\n        get_collections_galaxy_meta_info()\\n    )  \\n    mandatory_keys = set()\\n    string_keys = set()  \\n    list_keys = set()  \\n    dict_keys = set()  \\n    sentinel_keys = set()  \\n    for info in galaxy_yml_schema:\\n        if info.get('required', False):\\n            mandatory_keys.add(info['key'])\\n        key_list_type = {\\n            'str': string_keys,\\n            'list': list_keys,\\n            'dict': dict_keys,\\n            'sentinel': sentinel_keys,\\n        }[info.get('type', 'str')]\\n        key_list_type.add(info['key'])\\n    all_keys = frozenset(mandatory_keys | string_keys | list_keys | dict_keys | sentinel_keys)\\n    set_keys = set(galaxy_yml.keys())\\n    missing_keys = mandatory_keys.difference(set_keys)\\n    if missing_keys:\\n        msg = (\\n            \"The collection galaxy.yml at '%s' is missing the following mandatory keys: %s\"\\n            % (to_native(b_galaxy_yml_path), \", \".join(sorted(missing_keys)))\\n        )\\n        if require_build_metadata:\\n            raise AnsibleError(msg)\\n        display.warning(msg)\\n        raise ValueError(msg)\\n    extra_keys = set_keys.difference(all_keys)\\n    if len(extra_keys) > 0:\\n        display.warning(\"Found unknown keys in collection galaxy.yml at '%s': %s\"\\n                        % (to_text(b_galaxy_yml_path), \", \".join(extra_keys)))\\n    for optional_string in string_keys:\\n        if optional_string not in galaxy_yml:\\n            galaxy_yml[optional_string] = None\\n    for optional_list in list_keys:\\n        list_val = galaxy_yml.get(optional_list, None)\\n        if list_val is None:\\n            galaxy_yml[optional_list] = []\\n        elif not isinstance(list_val, list):\\n            galaxy_yml[optional_list] = [list_val]  \\n    for optional_dict in dict_keys:\\n        if optional_dict not in galaxy_yml:\\n            galaxy_yml[optional_dict] = {}\\n    for optional_sentinel in sentinel_keys:\\n        if optional_sentinel not in galaxy_yml:\\n            galaxy_yml[optional_sentinel] = Sentinel\\n    if not galaxy_yml.get('version'):\\n        galaxy_yml['version'] = '*'\\n    return galaxy_yml"
  },
  {
    "code": "def is_nested_tuple(tup, labels):\\n    if not isinstance(tup, tuple):\\n        return False\\n    for i, k in enumerate(tup):\\n        if isinstance(k, (tuple, list, slice)):\\n            return isinstance(labels, MultiIndex)\\n    return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix MultiIndex .loc with all numpy arrays (#19772)",
    "fixed_code": "def is_nested_tuple(tup, labels):\\n    if not isinstance(tup, tuple):\\n        return False\\n    for i, k in enumerate(tup):\\n        if is_list_like(k) or isinstance(k, slice):\\n            return isinstance(labels, MultiIndex)\\n    return False"
  },
  {
    "code": "def interpolate_1d(\\n\\txvalues,\\n\\tyvalues,\\n\\tmethod=\"linear\",\\n\\tlimit=None,\\n\\tlimit_direction=\"forward\",\\n\\tlimit_area=None,\\n\\tfill_value=None,\\n\\tbounds_error=False,\\n\\torder=None,\\n\\t**kwargs,\\n):\\n\\tinvalid = isna(yvalues)\\n\\tvalid = ~invalid\\n\\tif not valid.any():\\n\\t\\tresult = np.empty_like(np.asarray(xvalues), dtype=np.float64)\\n\\t\\tresult.fill(np.nan)\\n\\t\\treturn result\\n\\tif valid.all():\\n\\t\\treturn yvalues\\n\\tif method == \"time\":\\n\\t\\tif not getattr(xvalues, \"is_all_dates\", None):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"time-weighted interpolation only works \"\\n\\t\\t\\t\\t\"on Series or DataFrames with a \"\\n\\t\\t\\t\\t\"DatetimeIndex\"\\n\\t\\t\\t)\\n\\t\\tmethod = \"values\"\\n\\tvalid_limit_directions = [\"forward\", \"backward\", \"both\"]\\n\\tlimit_direction = limit_direction.lower()\\n\\tif limit_direction not in valid_limit_directions:\\n\\t\\tmsg = \"Invalid limit_direction: expecting one of {valid!r}, got {invalid!r}.\"\\n\\t\\traise ValueError(\\n\\t\\t\\tmsg.format(valid=valid_limit_directions, invalid=limit_direction)\\n\\t\\t)\\n\\tif limit_area is not None:\\n\\t\\tvalid_limit_areas = [\"inside\", \"outside\"]\\n\\t\\tlimit_area = limit_area.lower()\\n\\t\\tif limit_area not in valid_limit_areas:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"Invalid limit_area: expecting one of {}, got \"\\n\\t\\t\\t\\t\"{}.\".format(valid_limit_areas, limit_area)\\n\\t\\t\\t)\\n\\tlimit = algos._validate_limit(nobs=None, limit=limit)\\n\\tall_nans = set(np.flatnonzero(invalid))\\n\\tstart_nans = set(range(find_valid_index(yvalues, \"first\")))\\n\\tend_nans = set(range(1 + find_valid_index(yvalues, \"last\"), len(valid)))\\n\\tmid_nans = all_nans - start_nans - end_nans\\n\\tif limit_direction == \"forward\":\\n\\t\\tpreserve_nans = start_nans | set(_interp_limit(invalid, limit, 0))\\n\\telif limit_direction == \"backward\":\\n\\t\\tpreserve_nans = end_nans | set(_interp_limit(invalid, 0, limit))\\n\\telse:\\n\\t\\tpreserve_nans = set(_interp_limit(invalid, limit, limit))\\n\\tif limit_area == \"inside\":\\n\\t\\tpreserve_nans |= start_nans | end_nans\\n\\telif limit_area == \"outside\":\\n\\t\\tpreserve_nans |= mid_nans\\n\\tpreserve_nans = sorted(preserve_nans)\\n\\txvalues = getattr(xvalues, \"values\", xvalues)\\n\\tyvalues = getattr(yvalues, \"values\", yvalues)\\n\\tresult = yvalues.copy()\\n\\tif method in [\"linear\", \"time\", \"index\", \"values\"]:\\n\\t\\tif method in (\"values\", \"index\"):\\n\\t\\t\\tinds = np.asarray(xvalues)\\n\\t\\t\\tif needs_i8_conversion(inds.dtype.type):\\n\\t\\t\\t\\tinds = inds.view(np.int64)\\n\\t\\t\\tif inds.dtype == np.object_:\\n\\t\\t\\t\\tinds = lib.maybe_convert_objects(inds)\\n\\t\\telse:\\n\\t\\t\\tinds = xvalues\\n\\t\\tresult[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])\\n\\t\\tresult[preserve_nans] = np.nan\\n\\t\\treturn result\\n\\tsp_methods = [\\n\\t\\t\"nearest\",\\n\\t\\t\"zero\",\\n\\t\\t\"slinear\",\\n\\t\\t\"quadratic\",\\n\\t\\t\"cubic\",\\n\\t\\t\"barycentric\",\\n\\t\\t\"krogh\",\\n\\t\\t\"spline\",\\n\\t\\t\"polynomial\",\\n\\t\\t\"from_derivatives\",\\n\\t\\t\"piecewise_polynomial\",\\n\\t\\t\"pchip\",\\n\\t\\t\"akima\",\\n\\t]\\n\\tif method in sp_methods:\\n\\t\\tinds = np.asarray(xvalues)\\n\\t\\tif issubclass(inds.dtype.type, np.datetime64):\\n\\t\\t\\tinds = inds.view(np.int64)\\n\\t\\tresult[invalid] = _interpolate_scipy_wrapper(\\n\\t\\t\\tinds[valid],\\n\\t\\t\\tyvalues[valid],\\n\\t\\t\\tinds[invalid],\\n\\t\\t\\tmethod=method,\\n\\t\\t\\tfill_value=fill_value,\\n\\t\\t\\tbounds_error=bounds_error,\\n\\t\\t\\torder=order,\\n\\t\\t\\t**kwargs,\\n\\t\\t)\\n\\t\\tresult[preserve_nans] = np.nan\\n\\t\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix Series.interpolate(method='index') with unsorted index (#29943)",
    "fixed_code": "def interpolate_1d(\\n\\txvalues,\\n\\tyvalues,\\n\\tmethod=\"linear\",\\n\\tlimit=None,\\n\\tlimit_direction=\"forward\",\\n\\tlimit_area=None,\\n\\tfill_value=None,\\n\\tbounds_error=False,\\n\\torder=None,\\n\\t**kwargs,\\n):\\n\\tinvalid = isna(yvalues)\\n\\tvalid = ~invalid\\n\\tif not valid.any():\\n\\t\\tresult = np.empty_like(np.asarray(xvalues), dtype=np.float64)\\n\\t\\tresult.fill(np.nan)\\n\\t\\treturn result\\n\\tif valid.all():\\n\\t\\treturn yvalues\\n\\tif method == \"time\":\\n\\t\\tif not getattr(xvalues, \"is_all_dates\", None):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"time-weighted interpolation only works \"\\n\\t\\t\\t\\t\"on Series or DataFrames with a \"\\n\\t\\t\\t\\t\"DatetimeIndex\"\\n\\t\\t\\t)\\n\\t\\tmethod = \"values\"\\n\\tvalid_limit_directions = [\"forward\", \"backward\", \"both\"]\\n\\tlimit_direction = limit_direction.lower()\\n\\tif limit_direction not in valid_limit_directions:\\n\\t\\tmsg = \"Invalid limit_direction: expecting one of {valid!r}, got {invalid!r}.\"\\n\\t\\traise ValueError(\\n\\t\\t\\tmsg.format(valid=valid_limit_directions, invalid=limit_direction)\\n\\t\\t)\\n\\tif limit_area is not None:\\n\\t\\tvalid_limit_areas = [\"inside\", \"outside\"]\\n\\t\\tlimit_area = limit_area.lower()\\n\\t\\tif limit_area not in valid_limit_areas:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"Invalid limit_area: expecting one of {}, got \"\\n\\t\\t\\t\\t\"{}.\".format(valid_limit_areas, limit_area)\\n\\t\\t\\t)\\n\\tlimit = algos._validate_limit(nobs=None, limit=limit)\\n\\tall_nans = set(np.flatnonzero(invalid))\\n\\tstart_nans = set(range(find_valid_index(yvalues, \"first\")))\\n\\tend_nans = set(range(1 + find_valid_index(yvalues, \"last\"), len(valid)))\\n\\tmid_nans = all_nans - start_nans - end_nans\\n\\tif limit_direction == \"forward\":\\n\\t\\tpreserve_nans = start_nans | set(_interp_limit(invalid, limit, 0))\\n\\telif limit_direction == \"backward\":\\n\\t\\tpreserve_nans = end_nans | set(_interp_limit(invalid, 0, limit))\\n\\telse:\\n\\t\\tpreserve_nans = set(_interp_limit(invalid, limit, limit))\\n\\tif limit_area == \"inside\":\\n\\t\\tpreserve_nans |= start_nans | end_nans\\n\\telif limit_area == \"outside\":\\n\\t\\tpreserve_nans |= mid_nans\\n\\tpreserve_nans = sorted(preserve_nans)\\n\\txvalues = getattr(xvalues, \"values\", xvalues)\\n\\tyvalues = getattr(yvalues, \"values\", yvalues)\\n\\tresult = yvalues.copy()\\n\\tif method in [\"linear\", \"time\", \"index\", \"values\"]:\\n\\t\\tif method in (\"values\", \"index\"):\\n\\t\\t\\tinds = np.asarray(xvalues)\\n\\t\\t\\tif needs_i8_conversion(inds.dtype.type):\\n\\t\\t\\t\\tinds = inds.view(np.int64)\\n\\t\\t\\tif inds.dtype == np.object_:\\n\\t\\t\\t\\tinds = lib.maybe_convert_objects(inds)\\n\\t\\telse:\\n\\t\\t\\tinds = xvalues\\n\\t\\tindexer = np.argsort(inds[valid])\\n\\t\\tresult[invalid] = np.interp(\\n\\t\\t\\tinds[invalid], inds[valid][indexer], yvalues[valid][indexer]\\n\\t\\t)\\n\\t\\tresult[preserve_nans] = np.nan\\n\\t\\treturn result\\n\\tsp_methods = [\\n\\t\\t\"nearest\",\\n\\t\\t\"zero\",\\n\\t\\t\"slinear\",\\n\\t\\t\"quadratic\",\\n\\t\\t\"cubic\",\\n\\t\\t\"barycentric\",\\n\\t\\t\"krogh\",\\n\\t\\t\"spline\",\\n\\t\\t\"polynomial\",\\n\\t\\t\"from_derivatives\",\\n\\t\\t\"piecewise_polynomial\",\\n\\t\\t\"pchip\",\\n\\t\\t\"akima\",\\n\\t]\\n\\tif method in sp_methods:\\n\\t\\tinds = np.asarray(xvalues)\\n\\t\\tif issubclass(inds.dtype.type, np.datetime64):\\n\\t\\t\\tinds = inds.view(np.int64)\\n\\t\\tresult[invalid] = _interpolate_scipy_wrapper(\\n\\t\\t\\tinds[valid],\\n\\t\\t\\tyvalues[valid],\\n\\t\\t\\tinds[invalid],\\n\\t\\t\\tmethod=method,\\n\\t\\t\\tfill_value=fill_value,\\n\\t\\t\\tbounds_error=bounds_error,\\n\\t\\t\\torder=order,\\n\\t\\t\\t**kwargs,\\n\\t\\t)\\n\\t\\tresult[preserve_nans] = np.nan\\n\\t\\treturn result"
  },
  {
    "code": "def _rows_to_cols(self, content):\\n        zipped_content = list(lib.to_object_array(content).T)\\n        col_len = len(self.orig_columns)\\n        zip_len = len(zipped_content)\\n        if self._implicit_index:\\n            col_len += len(self.index_col)\\n        if col_len != zip_len:\\n            row_num = -1\\n            i = 0\\n            for (i, l) in enumerate(content):\\n                if len(l) != col_len:\\n                    break\\n            footers = 0\\n            if self.skip_footer:\\n                footers = self.skip_footer\\n                if footers > 0:\\n                    footers = footers - self.pos\\n            row_num = self.pos - (len(content) - i - footers)\\n            msg = ('Expecting %d columns, got %d in row %d' %\\n                   (col_len, zip_len, row_num))\\n            raise ValueError(msg)\\n        return zipped_content",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _rows_to_cols(self, content):\\n        zipped_content = list(lib.to_object_array(content).T)\\n        col_len = len(self.orig_columns)\\n        zip_len = len(zipped_content)\\n        if self._implicit_index:\\n            col_len += len(self.index_col)\\n        if col_len != zip_len:\\n            row_num = -1\\n            i = 0\\n            for (i, l) in enumerate(content):\\n                if len(l) != col_len:\\n                    break\\n            footers = 0\\n            if self.skip_footer:\\n                footers = self.skip_footer\\n                if footers > 0:\\n                    footers = footers - self.pos\\n            row_num = self.pos - (len(content) - i - footers)\\n            msg = ('Expecting %d columns, got %d in row %d' %\\n                   (col_len, zip_len, row_num))\\n            raise ValueError(msg)\\n        return zipped_content"
  },
  {
    "code": "def quantize(self, exp, rounding=None, context=None):\\n        exp = _convert_other(exp, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if rounding is None:\\n            rounding = context.rounding\\n        if self._is_special or exp._is_special:\\n            ans = self._check_nans(exp, context)\\n            if ans:\\n                return ans\\n            if exp._isinfinity() or self._isinfinity():\\n                if exp._isinfinity() and self._isinfinity():\\n                    return Decimal(self)  \\n                return context._raise_error(InvalidOperation,\\n                                        'quantize with one INF')\\n        if not (context.Etiny() <= exp._exp <= context.Emax):\\n            return context._raise_error(InvalidOperation,\\n                   'target exponent out of bounds in quantize')\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', exp._exp)\\n            return ans._fix(context)\\n        self_adjusted = self.adjusted()\\n        if self_adjusted > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if self_adjusted - exp._exp + 1 > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        ans = self._rescale(exp._exp, rounding)\\n        if ans.adjusted() > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if len(ans._int) > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        if ans and ans.adjusted() < context.Emin:\\n            context._raise_error(Subnormal)\\n        if ans._exp > self._exp:\\n            if ans != self:\\n                context._raise_error(Inexact)\\n            context._raise_error(Rounded)\\n        ans = ans._fix(context)\\n        return ans",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quantize(self, exp, rounding=None, context=None):\\n        exp = _convert_other(exp, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if rounding is None:\\n            rounding = context.rounding\\n        if self._is_special or exp._is_special:\\n            ans = self._check_nans(exp, context)\\n            if ans:\\n                return ans\\n            if exp._isinfinity() or self._isinfinity():\\n                if exp._isinfinity() and self._isinfinity():\\n                    return Decimal(self)  \\n                return context._raise_error(InvalidOperation,\\n                                        'quantize with one INF')\\n        if not (context.Etiny() <= exp._exp <= context.Emax):\\n            return context._raise_error(InvalidOperation,\\n                   'target exponent out of bounds in quantize')\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', exp._exp)\\n            return ans._fix(context)\\n        self_adjusted = self.adjusted()\\n        if self_adjusted > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if self_adjusted - exp._exp + 1 > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        ans = self._rescale(exp._exp, rounding)\\n        if ans.adjusted() > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if len(ans._int) > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        if ans and ans.adjusted() < context.Emin:\\n            context._raise_error(Subnormal)\\n        if ans._exp > self._exp:\\n            if ans != self:\\n                context._raise_error(Inexact)\\n            context._raise_error(Rounded)\\n        ans = ans._fix(context)\\n        return ans"
  },
  {
    "code": "def _add_new_block(self, col, value):\\n        loc = self.columns.get_loc(col)\\n        new_block = make_block(value, [loc], self.columns)\\n        self.blocks.append(new_block)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def _add_new_block(self, col, value):\\n        new_block = make_block(value, [col], self.columns)\\n        self.blocks.append(new_block)"
  },
  {
    "code": "def __init__(self,\\n                 job_flow_id,\\n                 *args,\\n                 **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.job_flow_id = job_flow_id",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6541] Use EmrJobFlowSensor for other states (#7146)\\n\\nRemove class constants of EmrBaseSensor: NON_TERMINAL_STATES,\\nFAILED_STATE. Add new parameters: target_states and failed_states.\\nEliminate pylint warnings. Do not change default behaviour of updated\\nsensors.",
    "fixed_code": "def __init__(self,\\n                 job_flow_id: str,\\n                 target_states: Optional[Iterable[str]] = None,\\n                 failed_states: Optional[Iterable[str]] = None,\\n                 *args,\\n                 **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.job_flow_id = job_flow_id\\n        self.target_states = target_states or ['TERMINATED']\\n        self.failed_states = failed_states or ['TERMINATED_WITH_ERRORS']"
  },
  {
    "code": "def delete(self, key):\\n        try:\\n            os.remove(self._key_to_file(key))\\n        except (IOError, OSError):\\n            pass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #6099: the filebased cache backend now uses md5 hashes of keys instead of sanitized filenames. For good measure, keys are partitioned into subdirectories using the first few bits of the hash. Thanks, sherbang.",
    "fixed_code": "def delete(self, key):\\n        try:\\n            self._delete(self._key_to_file(key))\\n        except (IOError, OSError):\\n            pass"
  },
  {
    "code": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\\n\\t\\tif self._config.prune_on_get_work:\\n\\t\\t\\tself.prune()\\n\\t\\tassert worker is not None\\n\\t\\tworker_id = worker\\n\\t\\tself.update(worker_id, {'host': host}, get_work=True)\\n\\t\\tif assistant:\\n\\t\\t\\tself.add_worker(worker_id, [('assistant', assistant)])\\n\\t\\tbatched_params, unbatched_params, batched_tasks, max_batch_size = None, None, [], 1\\n\\t\\tbest_task = None\\n\\t\\tif current_tasks is not None:\\n\\t\\t\\tct_set = set(current_tasks)\\n\\t\\t\\tfor task in sorted(self._state.get_running_tasks(), key=self._rank):\\n\\t\\t\\t\\tif task.worker_running == worker_id and task.id not in ct_set:\\n\\t\\t\\t\\t\\tbest_task = task\\n\\t\\tif current_tasks is not None:\\n\\t\\t\\tself._reset_orphaned_batch_running_tasks(worker_id)\\n\\t\\tlocally_pending_tasks = 0\\n\\t\\trunning_tasks = []\\n\\t\\tupstream_table = {}\\n\\t\\tgreedy_resources = collections.defaultdict(int)\\n\\t\\tn_unique_pending = 0\\n\\t\\tworker = self._state.get_worker(worker_id)\\n\\t\\tif worker.is_trivial_worker(self._state):\\n\\t\\t\\trelevant_tasks = worker.get_pending_tasks(self._state)\\n\\t\\t\\tused_resources = collections.defaultdict(int)\\n\\t\\t\\tgreedy_workers = dict()  \\n\\t\\telse:\\n\\t\\t\\trelevant_tasks = self._state.get_pending_tasks()\\n\\t\\t\\tused_resources = self._used_resources()\\n\\t\\t\\tactivity_limit = time.time() - self._config.worker_disconnect_delay\\n\\t\\t\\tactive_workers = self._state.get_active_workers(last_get_work_gt=activity_limit)\\n\\t\\t\\tgreedy_workers = dict((worker.id, worker.info.get('workers', 1))\\n\\t\\t\\t\\t\\t\\t\\t\\t  for worker in active_workers)\\n\\t\\ttasks = list(relevant_tasks)\\n\\t\\ttasks.sort(key=self._rank, reverse=True)\\n\\t\\tfor task in tasks:\\n\\t\\t\\tin_workers = (assistant and getattr(task, 'runnable', bool(task.workers))) or worker_id in task.workers\\n\\t\\t\\tif task.status == RUNNING and in_workers:\\n\\t\\t\\t\\tother_worker = self._state.get_worker(task.worker_running)\\n\\t\\t\\t\\tmore_info = {'task_id': task.id, 'worker': str(other_worker)}\\n\\t\\t\\t\\tif other_worker is not None:\\n\\t\\t\\t\\t\\tmore_info.update(other_worker.info)\\n\\t\\t\\t\\t\\trunning_tasks.append(more_info)\\n\\t\\t\\tif task.status == PENDING and in_workers:\\n\\t\\t\\t\\tupstream_status = self._upstream_status(task.id, upstream_table)\\n\\t\\t\\t\\tif upstream_status != UPSTREAM_DISABLED:\\n\\t\\t\\t\\t\\tlocally_pending_tasks += 1\\n\\t\\t\\t\\t\\tif len(task.workers) == 1 and not assistant:\\n\\t\\t\\t\\t\\t\\tn_unique_pending += 1\\n\\t\\t\\tif (best_task and batched_params and task.family == best_task.family and\\n\\t\\t\\t\\t\\tlen(batched_tasks) < max_batch_size and task.is_batchable() and all(\\n\\t\\t\\t\\t\\ttask.params.get(name) == value for name, value in unbatched_params.items())):\\n\\t\\t\\t\\tfor name, params in batched_params.items():\\n\\t\\t\\t\\t\\tparams.append(task.params.get(name))\\n\\t\\t\\t\\tbatched_tasks.append(task)\\n\\t\\t\\tif best_task:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif task.status == RUNNING and (task.worker_running in greedy_workers):\\n\\t\\t\\t\\tgreedy_workers[task.worker_running] -= 1\\n\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\tif self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\\n\\t\\t\\t\\tif in_workers and self._has_resources(task.resources, used_resources):\\n\\t\\t\\t\\t\\tbest_task = task\\n\\t\\t\\t\\t\\tbatch_param_names, max_batch_size = self._state.get_batcher(\\n\\t\\t\\t\\t\\t\\tworker_id, task.family)\\n\\t\\t\\t\\t\\tif batch_param_names and task.is_batchable():\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tbatched_params = {\\n\\t\\t\\t\\t\\t\\t\\t\\tname: [task.params[name]] for name in batch_param_names\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\tunbatched_params = {\\n\\t\\t\\t\\t\\t\\t\\t\\tname: value for name, value in task.params.items()\\n\\t\\t\\t\\t\\t\\t\\t\\tif name not in batched_params\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\tbatched_tasks.append(task)\\n\\t\\t\\t\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\t\\t\\t\\tbatched_params, unbatched_params = None, None\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tworkers = itertools.chain(task.workers, [worker_id]) if assistant else task.workers\\n\\t\\t\\t\\t\\tfor task_worker in workers:\\n\\t\\t\\t\\t\\t\\tif greedy_workers.get(task_worker, 0) > 0:\\n\\t\\t\\t\\t\\t\\t\\tgreedy_workers[task_worker] -= 1\\n\\t\\t\\t\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treply = {'n_pending_tasks': locally_pending_tasks,\\n\\t\\t\\t\\t 'running_tasks': running_tasks,\\n\\t\\t\\t\\t 'task_id': None,\\n\\t\\t\\t\\t 'n_unique_pending': n_unique_pending}\\n\\t\\tif len(batched_tasks) > 1:\\n\\t\\t\\tbatch_string = '|'.join(task.id for task in batched_tasks)\\n\\t\\t\\tbatch_id = hashlib.md5(batch_string.encode('utf-8')).hexdigest()\\n\\t\\t\\tfor task in batched_tasks:\\n\\t\\t\\t\\tself._state.set_batch_running(task, batch_id, worker_id)\\n\\t\\t\\tcombined_params = best_task.params.copy()\\n\\t\\t\\tcombined_params.update(batched_params)\\n\\t\\t\\treply['task_id'] = None\\n\\t\\t\\treply['task_family'] = best_task.family\\n\\t\\t\\treply['task_module'] = getattr(best_task, 'module', None)\\n\\t\\t\\treply['task_params'] = combined_params\\n\\t\\t\\treply['batch_id'] = batch_id\\n\\t\\t\\treply['batch_task_ids'] = [task.id for task in batched_tasks]\\n\\t\\telif best_task:\\n\\t\\t\\tself._state.set_status(best_task, RUNNING, self._config)\\n\\t\\t\\tbest_task.worker_running = worker_id\\n\\t\\t\\tbest_task.time_running = time.time()\\n\\t\\t\\tself._update_task_history(best_task, RUNNING, host=host)\\n\\t\\t\\treply['task_id'] = best_task.id\\n\\t\\t\\treply['task_family'] = best_task.family\\n\\t\\t\\treply['task_module'] = getattr(best_task, 'module', None)\\n\\t\\t\\treply['task_params'] = best_task.params\\n\\t\\treturn reply\\n\\t@rpc_method(attempts=1)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Prevents batch tasks from running before their dependencies (#1846)\\n\\nWhen adding tasks to an existing batch in get_work, we neglected to check\\nwhether the tasks were schedulable. This meant that in production I found\\nmy pipelines running batches that included jobs with PENDING dependencies.\\nIn order to fix this, we simply add a check that the task is schedulable.\\n\\nNote that even though self._schedulable(task) gets called twice in the\\nget_work loop, they fall under different branches of whether best_task is\\ndefined, so this shouldn't slow get_work down much.",
    "fixed_code": "def get_work(self, host=None, assistant=False, current_tasks=None, worker=None, **kwargs):\\n\\t\\tif self._config.prune_on_get_work:\\n\\t\\t\\tself.prune()\\n\\t\\tassert worker is not None\\n\\t\\tworker_id = worker\\n\\t\\tself.update(worker_id, {'host': host}, get_work=True)\\n\\t\\tif assistant:\\n\\t\\t\\tself.add_worker(worker_id, [('assistant', assistant)])\\n\\t\\tbatched_params, unbatched_params, batched_tasks, max_batch_size = None, None, [], 1\\n\\t\\tbest_task = None\\n\\t\\tif current_tasks is not None:\\n\\t\\t\\tct_set = set(current_tasks)\\n\\t\\t\\tfor task in sorted(self._state.get_running_tasks(), key=self._rank):\\n\\t\\t\\t\\tif task.worker_running == worker_id and task.id not in ct_set:\\n\\t\\t\\t\\t\\tbest_task = task\\n\\t\\tif current_tasks is not None:\\n\\t\\t\\tself._reset_orphaned_batch_running_tasks(worker_id)\\n\\t\\tlocally_pending_tasks = 0\\n\\t\\trunning_tasks = []\\n\\t\\tupstream_table = {}\\n\\t\\tgreedy_resources = collections.defaultdict(int)\\n\\t\\tn_unique_pending = 0\\n\\t\\tworker = self._state.get_worker(worker_id)\\n\\t\\tif worker.is_trivial_worker(self._state):\\n\\t\\t\\trelevant_tasks = worker.get_pending_tasks(self._state)\\n\\t\\t\\tused_resources = collections.defaultdict(int)\\n\\t\\t\\tgreedy_workers = dict()  \\n\\t\\telse:\\n\\t\\t\\trelevant_tasks = self._state.get_pending_tasks()\\n\\t\\t\\tused_resources = self._used_resources()\\n\\t\\t\\tactivity_limit = time.time() - self._config.worker_disconnect_delay\\n\\t\\t\\tactive_workers = self._state.get_active_workers(last_get_work_gt=activity_limit)\\n\\t\\t\\tgreedy_workers = dict((worker.id, worker.info.get('workers', 1))\\n\\t\\t\\t\\t\\t\\t\\t\\t  for worker in active_workers)\\n\\t\\ttasks = list(relevant_tasks)\\n\\t\\ttasks.sort(key=self._rank, reverse=True)\\n\\t\\tfor task in tasks:\\n\\t\\t\\tin_workers = (assistant and getattr(task, 'runnable', bool(task.workers))) or worker_id in task.workers\\n\\t\\t\\tif task.status == RUNNING and in_workers:\\n\\t\\t\\t\\tother_worker = self._state.get_worker(task.worker_running)\\n\\t\\t\\t\\tmore_info = {'task_id': task.id, 'worker': str(other_worker)}\\n\\t\\t\\t\\tif other_worker is not None:\\n\\t\\t\\t\\t\\tmore_info.update(other_worker.info)\\n\\t\\t\\t\\t\\trunning_tasks.append(more_info)\\n\\t\\t\\tif task.status == PENDING and in_workers:\\n\\t\\t\\t\\tupstream_status = self._upstream_status(task.id, upstream_table)\\n\\t\\t\\t\\tif upstream_status != UPSTREAM_DISABLED:\\n\\t\\t\\t\\t\\tlocally_pending_tasks += 1\\n\\t\\t\\t\\t\\tif len(task.workers) == 1 and not assistant:\\n\\t\\t\\t\\t\\t\\tn_unique_pending += 1\\n\\t\\t\\tif (best_task and batched_params and task.family == best_task.family and\\n\\t\\t\\t\\t\\tlen(batched_tasks) < max_batch_size and task.is_batchable() and all(\\n\\t\\t\\t\\t\\ttask.params.get(name) == value for name, value in unbatched_params.items()) and\\n\\t\\t\\t\\t\\tself._schedulable(task)):\\n\\t\\t\\t\\tfor name, params in batched_params.items():\\n\\t\\t\\t\\t\\tparams.append(task.params.get(name))\\n\\t\\t\\t\\tbatched_tasks.append(task)\\n\\t\\t\\tif best_task:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif task.status == RUNNING and (task.worker_running in greedy_workers):\\n\\t\\t\\t\\tgreedy_workers[task.worker_running] -= 1\\n\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\tif self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\\n\\t\\t\\t\\tif in_workers and self._has_resources(task.resources, used_resources):\\n\\t\\t\\t\\t\\tbest_task = task\\n\\t\\t\\t\\t\\tbatch_param_names, max_batch_size = self._state.get_batcher(\\n\\t\\t\\t\\t\\t\\tworker_id, task.family)\\n\\t\\t\\t\\t\\tif batch_param_names and task.is_batchable():\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tbatched_params = {\\n\\t\\t\\t\\t\\t\\t\\t\\tname: [task.params[name]] for name in batch_param_names\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\tunbatched_params = {\\n\\t\\t\\t\\t\\t\\t\\t\\tname: value for name, value in task.params.items()\\n\\t\\t\\t\\t\\t\\t\\t\\tif name not in batched_params\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\tbatched_tasks.append(task)\\n\\t\\t\\t\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\t\\t\\t\\tbatched_params, unbatched_params = None, None\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tworkers = itertools.chain(task.workers, [worker_id]) if assistant else task.workers\\n\\t\\t\\t\\t\\tfor task_worker in workers:\\n\\t\\t\\t\\t\\t\\tif greedy_workers.get(task_worker, 0) > 0:\\n\\t\\t\\t\\t\\t\\t\\tgreedy_workers[task_worker] -= 1\\n\\t\\t\\t\\t\\t\\t\\tfor resource, amount in six.iteritems((task.resources or {})):\\n\\t\\t\\t\\t\\t\\t\\t\\tgreedy_resources[resource] += amount\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treply = {'n_pending_tasks': locally_pending_tasks,\\n\\t\\t\\t\\t 'running_tasks': running_tasks,\\n\\t\\t\\t\\t 'task_id': None,\\n\\t\\t\\t\\t 'n_unique_pending': n_unique_pending}\\n\\t\\tif len(batched_tasks) > 1:\\n\\t\\t\\tbatch_string = '|'.join(task.id for task in batched_tasks)\\n\\t\\t\\tbatch_id = hashlib.md5(batch_string.encode('utf-8')).hexdigest()\\n\\t\\t\\tfor task in batched_tasks:\\n\\t\\t\\t\\tself._state.set_batch_running(task, batch_id, worker_id)\\n\\t\\t\\tcombined_params = best_task.params.copy()\\n\\t\\t\\tcombined_params.update(batched_params)\\n\\t\\t\\treply['task_id'] = None\\n\\t\\t\\treply['task_family'] = best_task.family\\n\\t\\t\\treply['task_module'] = getattr(best_task, 'module', None)\\n\\t\\t\\treply['task_params'] = combined_params\\n\\t\\t\\treply['batch_id'] = batch_id\\n\\t\\t\\treply['batch_task_ids'] = [task.id for task in batched_tasks]\\n\\t\\telif best_task:\\n\\t\\t\\tself._state.set_status(best_task, RUNNING, self._config)\\n\\t\\t\\tbest_task.worker_running = worker_id\\n\\t\\t\\tbest_task.time_running = time.time()\\n\\t\\t\\tself._update_task_history(best_task, RUNNING, host=host)\\n\\t\\t\\treply['task_id'] = best_task.id\\n\\t\\t\\treply['task_family'] = best_task.family\\n\\t\\t\\treply['task_module'] = getattr(best_task, 'module', None)\\n\\t\\t\\treply['task_params'] = best_task.params\\n\\t\\treturn reply\\n\\t@rpc_method(attempts=1)"
  },
  {
    "code": "def _eintr_retry_call(func, *args):\\n    while True:\\n        try:\\n            return func(*args)\\n        except OSError as e:\\n            if e.errno == errno.EINTR:\\n                continue\\n            raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #12493: subprocess: communicate() handles EINTR\\n\\nsubprocess.Popen.communicate() now also handles EINTR errors if the process has\\nonly one pipe.",
    "fixed_code": "def _eintr_retry_call(func, *args):\\n    while True:\\n        try:\\n            return func(*args)\\n        except (OSError, IOError) as e:\\n            if e.errno == errno.EINTR:\\n                continue\\n            raise"
  },
  {
    "code": "def create_axes(self, axes, obj, validate=True, nan_rep=None, data_columns=None, min_itemsize=None, encoding=None, **kwargs):\\n        if axes is None:\\n            try:\\n                axes = _AXES_MAP[type(obj)]\\n            except:\\n                raise TypeError(\"cannot properly create the storer for: [group->%s,value->%s]\" %\\n                                (self.group._v_name,type(obj)))\\n        axes = [obj._get_axis_number(a) for a in axes]\\n        if self.infer_axes():\\n            existing_table = self.copy()\\n            existing_table.infer_axes()\\n            axes         = [ a.axis for a in existing_table.index_axes]\\n            data_columns = existing_table.data_columns\\n            nan_rep      = existing_table.nan_rep\\n            encoding     = existing_table.encoding\\n            self.info    = copy.copy(existing_table.info)\\n        else:\\n            existing_table = None\\n        if len(axes) != self.ndim - 1:\\n            raise ValueError(\"currently only support ndim-1 indexers in an AppendableTable\")\\n        self.non_index_axes = []\\n        self.data_columns = []\\n        if encoding is None:\\n            if py3compat.PY3:\\n                encoding = _default_encoding\\n        self.encoding = encoding\\n        if nan_rep is None:\\n            nan_rep = 'nan'\\n        self.nan_rep = nan_rep\\n        index_axes_map = dict()\\n        for i, a in enumerate(obj.axes):\\n            if i in axes:\\n                name = obj._AXIS_NAMES[i]\\n                index_axes_map[i] = _convert_index(\\n                    a, self.encoding).set_name(name).set_axis(i)\\n            else:\\n                append_axis = list(a)\\n                if existing_table is not None:\\n                    indexer = len(self.non_index_axes)\\n                    exist_axis = existing_table.non_index_axes[indexer][1]\\n                    if append_axis != exist_axis:\\n                        if sorted(append_axis) == sorted(exist_axis):\\n                            append_axis = exist_axis\\n                self.non_index_axes.append((i, append_axis))\\n        self.index_axes = [index_axes_map[a].set_pos(j).update_info(self.info) for j,\\n                           a in enumerate(axes)]\\n        j = len(self.index_axes)\\n        if validate:\\n            for a in self.axes:\\n                a.maybe_set_size(min_itemsize=min_itemsize)\\n        for a in self.non_index_axes:\\n            obj = obj.reindex_axis(a[1], axis=a[0], copy=False)\\n        block_obj = self.get_object(obj)\\n        blocks = block_obj._data.blocks\\n        if len(self.non_index_axes):\\n            axis, axis_labels = self.non_index_axes[0]\\n            data_columns = self.validate_data_columns(data_columns, min_itemsize)\\n            if len(data_columns):\\n                blocks = block_obj.reindex_axis(Index(axis_labels) - Index(\\n                        data_columns), axis=axis, copy=False)._data.blocks\\n                for c in data_columns:\\n                    blocks.extend(block_obj.reindex_axis(\\n                            [c], axis=axis, copy=False)._data.blocks)\\n        self.values_axes = []\\n        for i, b in enumerate(blocks):\\n            klass = DataCol\\n            name = None\\n            if data_columns and len(b.items) == 1 and b.items[0] in data_columns:\\n                klass = DataIndexableCol\\n                name = b.items[0]\\n                self.data_columns.append(name)\\n            if existing_table is not None and validate:\\n                try:\\n                    existing_col = existing_table.values_axes[i]\\n                except:\\n                    raise ValueError(\"Incompatible appended table [%s] with existing table [%s]\" %\\n                                    (blocks,existing_table.values_axes))\\n            else:\\n                existing_col = None\\n            try:\\n                col = klass.create_for_block(\\n                    i=i, name=name, version=self.version)\\n                col.set_atom(block=b,\\n                             existing_col=existing_col,\\n                             min_itemsize=min_itemsize,\\n                             nan_rep=nan_rep,\\n                             encoding=encoding,\\n                             info=self.info,\\n                             **kwargs)\\n                col.set_pos(j)\\n                self.values_axes.append(col)\\n            except (NotImplementedError, ValueError, TypeError), e:\\n                raise e\\n            except (Exception), detail:\\n                raise Exception(\"cannot find the correct atom type -> [dtype->%s,items->%s] %s\" % (b.dtype.name, b.items, str(detail)))\\n            j += 1\\n        self.validate_min_itemsize(min_itemsize)\\n        if validate:\\n            self.validate(existing_table)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_axes(self, axes, obj, validate=True, nan_rep=None, data_columns=None, min_itemsize=None, encoding=None, **kwargs):\\n        if axes is None:\\n            try:\\n                axes = _AXES_MAP[type(obj)]\\n            except:\\n                raise TypeError(\"cannot properly create the storer for: [group->%s,value->%s]\" %\\n                                (self.group._v_name,type(obj)))\\n        axes = [obj._get_axis_number(a) for a in axes]\\n        if self.infer_axes():\\n            existing_table = self.copy()\\n            existing_table.infer_axes()\\n            axes         = [ a.axis for a in existing_table.index_axes]\\n            data_columns = existing_table.data_columns\\n            nan_rep      = existing_table.nan_rep\\n            encoding     = existing_table.encoding\\n            self.info    = copy.copy(existing_table.info)\\n        else:\\n            existing_table = None\\n        if len(axes) != self.ndim - 1:\\n            raise ValueError(\"currently only support ndim-1 indexers in an AppendableTable\")\\n        self.non_index_axes = []\\n        self.data_columns = []\\n        if encoding is None:\\n            if py3compat.PY3:\\n                encoding = _default_encoding\\n        self.encoding = encoding\\n        if nan_rep is None:\\n            nan_rep = 'nan'\\n        self.nan_rep = nan_rep\\n        index_axes_map = dict()\\n        for i, a in enumerate(obj.axes):\\n            if i in axes:\\n                name = obj._AXIS_NAMES[i]\\n                index_axes_map[i] = _convert_index(\\n                    a, self.encoding).set_name(name).set_axis(i)\\n            else:\\n                append_axis = list(a)\\n                if existing_table is not None:\\n                    indexer = len(self.non_index_axes)\\n                    exist_axis = existing_table.non_index_axes[indexer][1]\\n                    if append_axis != exist_axis:\\n                        if sorted(append_axis) == sorted(exist_axis):\\n                            append_axis = exist_axis\\n                self.non_index_axes.append((i, append_axis))\\n        self.index_axes = [index_axes_map[a].set_pos(j).update_info(self.info) for j,\\n                           a in enumerate(axes)]\\n        j = len(self.index_axes)\\n        if validate:\\n            for a in self.axes:\\n                a.maybe_set_size(min_itemsize=min_itemsize)\\n        for a in self.non_index_axes:\\n            obj = obj.reindex_axis(a[1], axis=a[0], copy=False)\\n        block_obj = self.get_object(obj)\\n        blocks = block_obj._data.blocks\\n        if len(self.non_index_axes):\\n            axis, axis_labels = self.non_index_axes[0]\\n            data_columns = self.validate_data_columns(data_columns, min_itemsize)\\n            if len(data_columns):\\n                blocks = block_obj.reindex_axis(Index(axis_labels) - Index(\\n                        data_columns), axis=axis, copy=False)._data.blocks\\n                for c in data_columns:\\n                    blocks.extend(block_obj.reindex_axis(\\n                            [c], axis=axis, copy=False)._data.blocks)\\n        self.values_axes = []\\n        for i, b in enumerate(blocks):\\n            klass = DataCol\\n            name = None\\n            if data_columns and len(b.items) == 1 and b.items[0] in data_columns:\\n                klass = DataIndexableCol\\n                name = b.items[0]\\n                self.data_columns.append(name)\\n            if existing_table is not None and validate:\\n                try:\\n                    existing_col = existing_table.values_axes[i]\\n                except:\\n                    raise ValueError(\"Incompatible appended table [%s] with existing table [%s]\" %\\n                                    (blocks,existing_table.values_axes))\\n            else:\\n                existing_col = None\\n            try:\\n                col = klass.create_for_block(\\n                    i=i, name=name, version=self.version)\\n                col.set_atom(block=b,\\n                             existing_col=existing_col,\\n                             min_itemsize=min_itemsize,\\n                             nan_rep=nan_rep,\\n                             encoding=encoding,\\n                             info=self.info,\\n                             **kwargs)\\n                col.set_pos(j)\\n                self.values_axes.append(col)\\n            except (NotImplementedError, ValueError, TypeError), e:\\n                raise e\\n            except (Exception), detail:\\n                raise Exception(\"cannot find the correct atom type -> [dtype->%s,items->%s] %s\" % (b.dtype.name, b.items, str(detail)))\\n            j += 1\\n        self.validate_min_itemsize(min_itemsize)\\n        if validate:\\n            self.validate(existing_table)"
  },
  {
    "code": "def config_to_dict(module):\\n    data = get_config(module)\\n    output = None\\n    obj = {'banner': module.params['banner'], 'state': 'absent'}\\n    for line in data.split('\\n'):\\n        if line.startswith('set system login banner %s' % obj['banner']):\\n            match = re.findall(r'%s (.*)' % obj['banner'], line, re.M)\\n            output = match\\n    if output:\\n        obj['text'] = output[0].encode().decode('unicode_escape')\\n        obj['state'] = 'present'\\n    return obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def config_to_dict(module):\\n    data = get_config(module)\\n    output = None\\n    obj = {'banner': module.params['banner'], 'state': 'absent'}\\n    for line in data.split('\\n'):\\n        if line.startswith('set system login banner %s' % obj['banner']):\\n            match = re.findall(r'%s (.*)' % obj['banner'], line, re.M)\\n            output = match\\n    if output:\\n        obj['text'] = output[0].encode().decode('unicode_escape')\\n        obj['state'] = 'present'\\n    return obj"
  },
  {
    "code": "def _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\\n                  owner=None, group=None, logger=None):\\n    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: ''}\\n    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2'}\\n    if compress is not None and compress not in compress_ext.keys():\\n        raise ValueError, \\\\n              (\"bad value for 'compress': must be None, 'gzip' or 'bzip2'\")\\n    archive_name = base_name + '.tar' + compress_ext.get(compress, '')\\n    archive_dir = os.path.dirname(archive_name)\\n    if not os.path.exists(archive_dir):\\n        logger.info(\"creating %s\" % archive_dir)\\n        if not dry_run:\\n            os.makedirs(archive_dir)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\\n                  owner=None, group=None, logger=None):\\n    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: ''}\\n    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2'}\\n    if compress is not None and compress not in compress_ext.keys():\\n        raise ValueError, \\\\n              (\"bad value for 'compress': must be None, 'gzip' or 'bzip2'\")\\n    archive_name = base_name + '.tar' + compress_ext.get(compress, '')\\n    archive_dir = os.path.dirname(archive_name)\\n    if not os.path.exists(archive_dir):\\n        logger.info(\"creating %s\" % archive_dir)\\n        if not dry_run:\\n            os.makedirs(archive_dir)"
  },
  {
    "code": "def _get_combined_index(\\n    indexes: list[Index],\\n    intersect: bool = False,\\n    sort: bool = False,\\n    copy: bool = False,\\n) -> Index:\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = union_indexes(indexes, sort=False)\\n        index = ensure_index(index)\\n    if sort:\\n        index = safe_sort_index(index)\\n    if copy:\\n        index = index.copy()\\n    return index",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: concat materializing index even if sort is not necessary (#47508)",
    "fixed_code": "def _get_combined_index(\\n    indexes: list[Index],\\n    intersect: bool = False,\\n    sort: bool = False,\\n    copy: bool = False,\\n) -> Index:\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = union_indexes(indexes, sort=False)\\n        index = ensure_index(index)\\n    if sort and not index.is_monotonic_increasing:\\n        index = safe_sort_index(index)\\n    if copy:\\n        index = index.copy()\\n    return index"
  },
  {
    "code": "def _signature_internal(obj, follow_wrapper_chains=True, skip_bound_arg=True):\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = _signature_internal(obj.__func__,\\n                                  follow_wrapper_chains,\\n                                  skip_bound_arg)\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if follow_wrapper_chains:\\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            if not isinstance(sig, Signature):\\n                raise TypeError(\\n                    'unexpected object {!r} in __signature__ '\\n                    'attribute'.format(sig))\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        if isinstance(partialmethod, functools.partialmethod):\\n            wrapped_sig = _signature_internal(partialmethod.func,\\n                                              follow_wrapper_chains,\\n                                              skip_bound_arg)\\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n            new_params = (first_wrapped_param,) + tuple(sig.parameters.values())\\n            return sig.replace(parameters=new_params)\\n    if isfunction(obj) or _signature_is_functionlike(obj):\\n        return Signature.from_function(obj)\\n    if _signature_is_builtin(obj):\\n        return _signature_from_builtin(Signature, obj,\\n                                       skip_bound_arg=skip_bound_arg)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = _signature_internal(obj.func,\\n                                          follow_wrapper_chains,\\n                                          skip_bound_arg)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = _signature_internal(call,\\n                                      follow_wrapper_chains,\\n                                      skip_bound_arg)\\n        else:\\n            new = _signature_get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = _signature_internal(new,\\n                                          follow_wrapper_chains,\\n                                          skip_bound_arg)\\n            else:\\n                init = _signature_get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = _signature_internal(init,\\n                                              follow_wrapper_chains,\\n                                              skip_bound_arg)\\n        if sig is None:\\n            for base in obj.__mro__[:-1]:\\n                try:\\n                    text_sig = base.__text_signature__\\n                except AttributeError:\\n                    pass\\n                else:\\n                    if text_sig:\\n                        return _signature_fromstr(Signature, obj, text_sig)\\n            if type not in obj.__mro__:\\n                if obj.__init__ is object.__init__:\\n                    return signature(object)\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            try:\\n                sig = _signature_internal(call,\\n                                          follow_wrapper_chains,\\n                                          skip_bound_arg)\\n            except ValueError as ex:\\n                msg = 'no signature found for {!r}'.format(obj)\\n                raise ValueError(msg) from ex\\n    if sig is not None:\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _signature_internal(obj, follow_wrapper_chains=True, skip_bound_arg=True):\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = _signature_internal(obj.__func__,\\n                                  follow_wrapper_chains,\\n                                  skip_bound_arg)\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if follow_wrapper_chains:\\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            if not isinstance(sig, Signature):\\n                raise TypeError(\\n                    'unexpected object {!r} in __signature__ '\\n                    'attribute'.format(sig))\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        if isinstance(partialmethod, functools.partialmethod):\\n            wrapped_sig = _signature_internal(partialmethod.func,\\n                                              follow_wrapper_chains,\\n                                              skip_bound_arg)\\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n            new_params = (first_wrapped_param,) + tuple(sig.parameters.values())\\n            return sig.replace(parameters=new_params)\\n    if isfunction(obj) or _signature_is_functionlike(obj):\\n        return Signature.from_function(obj)\\n    if _signature_is_builtin(obj):\\n        return _signature_from_builtin(Signature, obj,\\n                                       skip_bound_arg=skip_bound_arg)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = _signature_internal(obj.func,\\n                                          follow_wrapper_chains,\\n                                          skip_bound_arg)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = _signature_internal(call,\\n                                      follow_wrapper_chains,\\n                                      skip_bound_arg)\\n        else:\\n            new = _signature_get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = _signature_internal(new,\\n                                          follow_wrapper_chains,\\n                                          skip_bound_arg)\\n            else:\\n                init = _signature_get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = _signature_internal(init,\\n                                              follow_wrapper_chains,\\n                                              skip_bound_arg)\\n        if sig is None:\\n            for base in obj.__mro__[:-1]:\\n                try:\\n                    text_sig = base.__text_signature__\\n                except AttributeError:\\n                    pass\\n                else:\\n                    if text_sig:\\n                        return _signature_fromstr(Signature, obj, text_sig)\\n            if type not in obj.__mro__:\\n                if obj.__init__ is object.__init__:\\n                    return signature(object)\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _signature_get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            try:\\n                sig = _signature_internal(call,\\n                                          follow_wrapper_chains,\\n                                          skip_bound_arg)\\n            except ValueError as ex:\\n                msg = 'no signature found for {!r}'.format(obj)\\n                raise ValueError(msg) from ex\\n    if sig is not None:\\n        if skip_bound_arg:\\n            return _signature_bound_method(sig)\\n        else:\\n            return sig\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))"
  },
  {
    "code": "def _objective(mle, precision_, alpha):\\n    cost = (-log_likelihood(mle, precision_)\\n            + alpha*np.abs(precision_).sum())\\n    return cost",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Glasso: don't penalize the diagonal",
    "fixed_code": "def _objective(mle, precision_, alpha):\\n    cost = (-log_likelihood(mle, precision_)\\n            + alpha*(np.abs(precision_).sum()\\n                        - np.abs(np.diag(precision_)).sum()))\\n    return cost"
  },
  {
    "code": "def map_obj_to_commands(updates, module):\\n    commands = list()\\n    purge = module.params['purge']\\n    want, have = updates\\n    info = get_capabilities(module).get('device_info')\\n    os_platform = info.get('network_os_platform')\\n    for w in want:\\n        vlan_id = w['vlan_id']\\n        name = w['name']\\n        interfaces = w.get('interfaces') or []\\n        mapped_vni = w['mapped_vni']\\n        mode = w['mode']\\n        vlan_state = w['vlan_state']\\n        admin_state = w['admin_state']\\n        state = w['state']\\n        del w['state']\\n        obj_in_have = search_obj_in_list(vlan_id, have) or {}\\n        if not re.match('N[567]', os_platform) or (not obj_in_have.get('mode') and mode == 'ce'):\\n            mode = w['mode'] = None\\n        if state == 'absent':\\n            if obj_in_have:\\n                commands.append('no vlan {0}'.format(vlan_id))\\n        elif state == 'present':\\n            if not obj_in_have:\\n                commands.append('vlan {0}'.format(vlan_id))\\n                if name and name != 'default':\\n                    commands.append('name {0}'.format(name))\\n                if mode:\\n                    commands.append('mode {0}'.format(mode))\\n                if vlan_state:\\n                    commands.append('state {0}'.format(vlan_state))\\n                if mapped_vni != 'None' and mapped_vni != 'default':\\n                    commands.append('vn-segment {0}'.format(mapped_vni))\\n                if admin_state == 'up':\\n                    commands.append('no shutdown')\\n                if admin_state == 'down':\\n                    commands.append('shutdown')\\n                commands.append('exit')\\n                if interfaces and interfaces[0] != 'default':\\n                    for i in interfaces:\\n                        commands.append('interface {0}'.format(i))\\n                        commands.append('switchport')\\n                        commands.append('switchport mode access')\\n                        commands.append('switchport access vlan {0}'.format(vlan_id))\\n            else:\\n                diff = get_diff(w, obj_in_have)\\n                if diff:\\n                    commands.append('vlan {0}'.format(vlan_id))\\n                    for key, value in diff.items():\\n                        if key == 'name':\\n                            if name != 'default':\\n                                if name is not None:\\n                                    commands.append('name {0}'.format(value))\\n                            else:\\n                                if not is_default_name(obj_in_have, vlan_id):\\n                                    commands.append('no name')\\n                        if key == 'vlan_state' and value:\\n                            commands.append('state {0}'.format(value))\\n                        if key == 'mapped_vni':\\n                            if value == 'default':\\n                                if obj_in_have['mapped_vni'] != 'None':\\n                                    commands.append('no vn-segment')\\n                            elif value != 'None':\\n                                commands.append('vn-segment {0}'.format(value))\\n                        if key == 'admin_state':\\n                            if value == 'up':\\n                                commands.append('no shutdown')\\n                            elif value == 'down':\\n                                commands.append('shutdown')\\n                        if key == 'mode' and value:\\n                            commands.append('mode {0}'.format(value))\\n                    if len(commands) > 1:\\n                        commands.append('exit')\\n                    else:\\n                        del commands[:]\\n                if interfaces and interfaces[0] != 'default':\\n                    if not obj_in_have['interfaces']:\\n                        for i in interfaces:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('switchport access vlan {0}'.format(vlan_id))\\n                    elif set(interfaces) != set(obj_in_have['interfaces']):\\n                        missing_interfaces = list(set(interfaces) - set(obj_in_have['interfaces']))\\n                        for i in missing_interfaces:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('switchport access vlan {0}'.format(vlan_id))\\n                        superfluous_interfaces = list(set(obj_in_have['interfaces']) - set(interfaces))\\n                        for i in superfluous_interfaces:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('no switchport access vlan {0}'.format(vlan_id))\\n                elif interfaces and interfaces[0] == 'default':\\n                    if obj_in_have['interfaces']:\\n                        for i in obj_in_have['interfaces']:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('no switchport access vlan {0}'.format(vlan_id))\\n    if purge:\\n        for h in have:\\n            if h['vlan_id'] == '1':\\n                module.warn(\"Deletion of vlan 1 is not allowed; purge will ignore vlan 1\")\\n                continue\\n            obj_in_want = search_obj_in_list(h['vlan_id'], want)\\n            if not obj_in_want:\\n                commands.append('no vlan {0}'.format(h['vlan_id']))\\n    return commands",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_obj_to_commands(updates, module):\\n    commands = list()\\n    purge = module.params['purge']\\n    want, have = updates\\n    info = get_capabilities(module).get('device_info')\\n    os_platform = info.get('network_os_platform')\\n    for w in want:\\n        vlan_id = w['vlan_id']\\n        name = w['name']\\n        interfaces = w.get('interfaces') or []\\n        mapped_vni = w['mapped_vni']\\n        mode = w['mode']\\n        vlan_state = w['vlan_state']\\n        admin_state = w['admin_state']\\n        state = w['state']\\n        del w['state']\\n        obj_in_have = search_obj_in_list(vlan_id, have) or {}\\n        if not re.match('N[567]', os_platform) or (not obj_in_have.get('mode') and mode == 'ce'):\\n            mode = w['mode'] = None\\n        if state == 'absent':\\n            if obj_in_have:\\n                commands.append('no vlan {0}'.format(vlan_id))\\n        elif state == 'present':\\n            if not obj_in_have:\\n                commands.append('vlan {0}'.format(vlan_id))\\n                if name and name != 'default':\\n                    commands.append('name {0}'.format(name))\\n                if mode:\\n                    commands.append('mode {0}'.format(mode))\\n                if vlan_state:\\n                    commands.append('state {0}'.format(vlan_state))\\n                if mapped_vni != 'None' and mapped_vni != 'default':\\n                    commands.append('vn-segment {0}'.format(mapped_vni))\\n                if admin_state == 'up':\\n                    commands.append('no shutdown')\\n                if admin_state == 'down':\\n                    commands.append('shutdown')\\n                commands.append('exit')\\n                if interfaces and interfaces[0] != 'default':\\n                    for i in interfaces:\\n                        commands.append('interface {0}'.format(i))\\n                        commands.append('switchport')\\n                        commands.append('switchport mode access')\\n                        commands.append('switchport access vlan {0}'.format(vlan_id))\\n            else:\\n                diff = get_diff(w, obj_in_have)\\n                if diff:\\n                    commands.append('vlan {0}'.format(vlan_id))\\n                    for key, value in diff.items():\\n                        if key == 'name':\\n                            if name != 'default':\\n                                if name is not None:\\n                                    commands.append('name {0}'.format(value))\\n                            else:\\n                                if not is_default_name(obj_in_have, vlan_id):\\n                                    commands.append('no name')\\n                        if key == 'vlan_state' and value:\\n                            commands.append('state {0}'.format(value))\\n                        if key == 'mapped_vni':\\n                            if value == 'default':\\n                                if obj_in_have['mapped_vni'] != 'None':\\n                                    commands.append('no vn-segment')\\n                            elif value != 'None':\\n                                commands.append('vn-segment {0}'.format(value))\\n                        if key == 'admin_state':\\n                            if value == 'up':\\n                                commands.append('no shutdown')\\n                            elif value == 'down':\\n                                commands.append('shutdown')\\n                        if key == 'mode' and value:\\n                            commands.append('mode {0}'.format(value))\\n                    if len(commands) > 1:\\n                        commands.append('exit')\\n                    else:\\n                        del commands[:]\\n                if interfaces and interfaces[0] != 'default':\\n                    if not obj_in_have['interfaces']:\\n                        for i in interfaces:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('switchport access vlan {0}'.format(vlan_id))\\n                    elif set(interfaces) != set(obj_in_have['interfaces']):\\n                        missing_interfaces = list(set(interfaces) - set(obj_in_have['interfaces']))\\n                        for i in missing_interfaces:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('switchport access vlan {0}'.format(vlan_id))\\n                        superfluous_interfaces = list(set(obj_in_have['interfaces']) - set(interfaces))\\n                        for i in superfluous_interfaces:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('no switchport access vlan {0}'.format(vlan_id))\\n                elif interfaces and interfaces[0] == 'default':\\n                    if obj_in_have['interfaces']:\\n                        for i in obj_in_have['interfaces']:\\n                            commands.append('vlan {0}'.format(vlan_id))\\n                            commands.append('exit')\\n                            commands.append('interface {0}'.format(i))\\n                            commands.append('switchport')\\n                            commands.append('switchport mode access')\\n                            commands.append('no switchport access vlan {0}'.format(vlan_id))\\n    if purge:\\n        for h in have:\\n            if h['vlan_id'] == '1':\\n                module.warn(\"Deletion of vlan 1 is not allowed; purge will ignore vlan 1\")\\n                continue\\n            obj_in_want = search_obj_in_list(h['vlan_id'], want)\\n            if not obj_in_want:\\n                commands.append('no vlan {0}'.format(h['vlan_id']))\\n    return commands"
  },
  {
    "code": "def webserver(args):\\n    print(settings.HEADER)\\n    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')\\n    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')\\n    num_workers = args.workers or conf.get('webserver', 'workers')\\n    worker_timeout = (args.worker_timeout or\\n                      conf.get('webserver', 'web_server_worker_timeout'))\\n    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')\\n    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')\\n    if not ssl_cert and ssl_key:\\n        raise AirflowException(\\n            'An SSL certificate must also be provided for use with ' + ssl_key)\\n    if ssl_cert and not ssl_key:\\n        raise AirflowException(\\n            'An SSL key must also be provided for use with ' + ssl_cert)\\n    if args.debug:\\n        print(\\n            \"Starting the web server on port {0} and host {1}.\".format(\\n                args.port, args.hostname))\\n        app, _ = create_app(None, testing=conf.getboolean('core', 'unit_test_mode'))\\n        app.run(debug=True, use_reloader=not app.config['TESTING'],\\n                port=args.port, host=args.hostname,\\n                ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None)\\n    else:\\n        os.environ['SKIP_DAGS_PARSING'] = 'True'\\n        app = cached_app(None)\\n        os.environ.pop('SKIP_DAGS_PARSING')\\n        pid_file, stdout, stderr, log_file = setup_locations(\\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file)\\n        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\\n        print(\\n            textwrap.dedent(''.format(num_workers=num_workers, workerclass=args.workerclass,\\n                       hostname=args.hostname, port=args.port,\\n                       worker_timeout=worker_timeout, access_logfile=access_logfile,\\n                       error_logfile=error_logfile)))\\n        run_args = [\\n            'gunicorn',\\n            '-w', str(num_workers),\\n            '-k', str(args.workerclass),\\n            '-t', str(worker_timeout),\\n            '-b', args.hostname + ':' + str(args.port),\\n            '-n', 'airflow-webserver',\\n            '-p', pid_file,\\n            '-c', 'python:airflow.www.gunicorn_config',\\n        ]\\n        if args.access_logfile:\\n            run_args += ['--access-logfile', str(args.access_logfile)]\\n        if args.error_logfile:\\n            run_args += ['--error-logfile', str(args.error_logfile)]\\n        if args.daemon:\\n            run_args += ['-D']\\n        if ssl_cert:\\n            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]\\n        run_args += [\"airflow.www.app:cached_app()\"]\\n        gunicorn_master_proc = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove side-effect of session in FAB (#8940)",
    "fixed_code": "def webserver(args):\\n    print(settings.HEADER)\\n    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')\\n    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')\\n    num_workers = args.workers or conf.get('webserver', 'workers')\\n    worker_timeout = (args.worker_timeout or\\n                      conf.get('webserver', 'web_server_worker_timeout'))\\n    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')\\n    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')\\n    if not ssl_cert and ssl_key:\\n        raise AirflowException(\\n            'An SSL certificate must also be provided for use with ' + ssl_key)\\n    if ssl_cert and not ssl_key:\\n        raise AirflowException(\\n            'An SSL key must also be provided for use with ' + ssl_cert)\\n    if args.debug:\\n        print(\\n            \"Starting the web server on port {0} and host {1}.\".format(\\n                args.port, args.hostname))\\n        app, _ = create_app(testing=conf.getboolean('core', 'unit_test_mode'))\\n        app.run(debug=True, use_reloader=not app.config['TESTING'],\\n                port=args.port, host=args.hostname,\\n                ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None)\\n    else:\\n        os.environ['SKIP_DAGS_PARSING'] = 'True'\\n        app = cached_app(None)\\n        os.environ.pop('SKIP_DAGS_PARSING')\\n        pid_file, stdout, stderr, log_file = setup_locations(\\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file)\\n        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\\n        print(\\n            textwrap.dedent(''.format(num_workers=num_workers, workerclass=args.workerclass,\\n                       hostname=args.hostname, port=args.port,\\n                       worker_timeout=worker_timeout, access_logfile=access_logfile,\\n                       error_logfile=error_logfile)))\\n        run_args = [\\n            'gunicorn',\\n            '-w', str(num_workers),\\n            '-k', str(args.workerclass),\\n            '-t', str(worker_timeout),\\n            '-b', args.hostname + ':' + str(args.port),\\n            '-n', 'airflow-webserver',\\n            '-p', pid_file,\\n            '-c', 'python:airflow.www.gunicorn_config',\\n        ]\\n        if args.access_logfile:\\n            run_args += ['--access-logfile', str(args.access_logfile)]\\n        if args.error_logfile:\\n            run_args += ['--error-logfile', str(args.error_logfile)]\\n        if args.daemon:\\n            run_args += ['-D']\\n        if ssl_cert:\\n            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]\\n        run_args += [\"airflow.www.app:cached_app()\"]\\n        gunicorn_master_proc = None"
  },
  {
    "code": "def predict(self, X, n_estimators=-1):\\n        if n_estimators == 0:\\n            raise ValueError(\"``n_estimators`` must not equal zero\")\\n        if not self.estimators_:\\n            raise RuntimeError(\\n                (\"{0} is not initialized. \"\\n                 \"Perform a fit first\").format(self.__class__.__name__))\\n        classes = self.classes_\\n        pred = None\\n        for i, (weight, estimator) in enumerate(\\n                zip(self.weights_, self.estimators_)):\\n            if i == n_estimators:\\n                break\\n            current_pred = estimator.predict(X)\\n            current_pred = (current_pred == classes[:, np.newaxis]).T * weight\\n            if pred is None:\\n                pred = current_pred\\n            else:\\n                pred += current_pred\\n        return np.array(classes.take(\\n            np.argmax(pred, axis=1), axis=0))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "combine real and discrete algorithms under one class",
    "fixed_code": "def predict(self, X, n_estimators=-1):\\n        if n_estimators == 0:\\n            raise ValueError(\"``n_estimators`` must not equal zero\")\\n        if not self.estimators_:\\n            raise RuntimeError(\\n                (\"{0} is not initialized. \"\\n                 \"Perform a fit first\").format(self.__class__.__name__))\\n        n_classes = self.n_classes_\\n        classes = self.classes_\\n        pred = None\\n        for i, (weight, estimator) in enumerate(\\n                zip(self.weights_, self.estimators_)):\\n            if i == n_estimators:\\n                break\\n            if self.real:\\n                current_pred = estimator.predict_proba(X) + 1e-200\\n                current_pred = (n_classes - 1) * (\\n                    np.log(current_pred) -\\n                    (1. / n_classes) *\\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\\n            else:\\n                current_pred = estimator.predict(X)\\n                current_pred = (\\n                    current_pred == classes[:, np.newaxis]).T * weight\\n            if pred is None:\\n                pred = current_pred\\n            else:\\n                pred += current_pred\\n        return np.array(classes.take(\\n            np.argmax(pred, axis=1), axis=0))"
  },
  {
    "code": "def drop_duplicates(self, cols=None, take_last=False, inplace=False):\\n        duplicated = self.duplicated(cols, take_last=take_last)\\n        if inplace:\\n            inds, = (-duplicated).nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Deprecate cols in to_csv, to_excel, drop_duplicates, and duplicated.  Use decorator.  Update docs and unit tests. [fix #6645, fix#6680]",
    "fixed_code": "def drop_duplicates(self, subset=None, take_last=False, inplace=False):\\n        duplicated = self.duplicated(subset, take_last=take_last)\\n        if inplace:\\n            inds, = (-duplicated).nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]\\n    @deprecate_kwarg(old_arg_name='cols', new_arg_name='subset')"
  },
  {
    "code": "def time_dayofyear(self, tz, freq):\\n        self.ts.dayofyear",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_dayofyear(self, tz, freq):\\n        self.ts.dayofyear"
  },
  {
    "code": "def _network_query(v):\\n    ''\\n    if v.size > 1:\\n        return str(v.network)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ipaddr filter - return a network address when given an address with /32 subnet (#47539)",
    "fixed_code": "def _network_query(v):\\n    ''\\n    return str(v.network)"
  },
  {
    "code": "def set_acls(self, location, public=False, read_tenants=None,\\n\\t\\t\\t\\t write_tenants=None):\\n\\t\\t\"\"\"\\n\\t\\tSets the read and write access control list for an image in the\\n\\t\\tbackend store.\\n\\t\\t:location `glance.store.location.Location` object, supplied",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_acls(self, location, public=False, read_tenants=None,\\n\\t\\t\\t\\t write_tenants=None):\\n\\t\\t\"\"\"\\n\\t\\tSets the read and write access control list for an image in the\\n\\t\\tbackend store.\\n\\t\\t:location `glance.store.location.Location` object, supplied"
  },
  {
    "code": "def _refine_defaults_read(\\n    dialect: Union[str, csv.Dialect],\\n    delimiter: Union[str, object],\\n    delim_whitespace: bool,\\n    engine: str,\\n    sep: Union[str, object],\\n    names: Union[Optional[ArrayLike], object],\\n    prefix: Union[Optional[str], object],\\n    defaults: Dict[str, Any],\\n):\\n    delim_default = defaults[\"delimiter\"]\\n    kwds: Dict[str, Any] = {}\\n    if dialect is not None:\\n        kwds[\"sep_override\"] = delimiter is None and (\\n            sep is lib.no_default or sep == delim_default\\n        )\\n    if delimiter and (sep is not lib.no_default):\\n        raise ValueError(\"Specified a sep and a delimiter; you can only specify one.\")\\n    if names is not lib.no_default and prefix is not lib.no_default:\\n        raise ValueError(\"Specified named and prefix; you can only specify one.\")\\n    kwds[\"names\"] = None if names is lib.no_default else names\\n    kwds[\"prefix\"] = None if prefix is lib.no_default else prefix\\n    if delimiter is None:\\n        delimiter = sep\\n    if delim_whitespace and (delimiter is not lib.no_default):\\n        raise ValueError(\\n            \"Specified a delimiter with both sep and \"\\n            \"delim_whitespace=True; you can only specify one.\"\\n        )\\n    if delimiter is lib.no_default:\\n        kwds[\"delimiter\"] = delim_default\\n    else:\\n        kwds[\"delimiter\"] = delimiter\\n    if engine is not None:\\n        kwds[\"engine_specified\"] = True\\n    else:\\n        kwds[\"engine\"] = \"c\"\\n        kwds[\"engine_specified\"] = False\\n    return kwds",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _refine_defaults_read(\\n    dialect: Union[str, csv.Dialect],\\n    delimiter: Union[str, object],\\n    delim_whitespace: bool,\\n    engine: str,\\n    sep: Union[str, object],\\n    names: Union[Optional[ArrayLike], object],\\n    prefix: Union[Optional[str], object],\\n    defaults: Dict[str, Any],\\n):\\n    delim_default = defaults[\"delimiter\"]\\n    kwds: Dict[str, Any] = {}\\n    if dialect is not None:\\n        kwds[\"sep_override\"] = delimiter is None and (\\n            sep is lib.no_default or sep == delim_default\\n        )\\n    if delimiter and (sep is not lib.no_default):\\n        raise ValueError(\"Specified a sep and a delimiter; you can only specify one.\")\\n    if names is not lib.no_default and prefix is not lib.no_default:\\n        raise ValueError(\"Specified named and prefix; you can only specify one.\")\\n    kwds[\"names\"] = None if names is lib.no_default else names\\n    kwds[\"prefix\"] = None if prefix is lib.no_default else prefix\\n    if delimiter is None:\\n        delimiter = sep\\n    if delim_whitespace and (delimiter is not lib.no_default):\\n        raise ValueError(\\n            \"Specified a delimiter with both sep and \"\\n            \"delim_whitespace=True; you can only specify one.\"\\n        )\\n    if delimiter is lib.no_default:\\n        kwds[\"delimiter\"] = delim_default\\n    else:\\n        kwds[\"delimiter\"] = delimiter\\n    if engine is not None:\\n        kwds[\"engine_specified\"] = True\\n    else:\\n        kwds[\"engine\"] = \"c\"\\n        kwds[\"engine_specified\"] = False\\n    return kwds"
  },
  {
    "code": "def take_2d_multi(arr, row_idx, col_idx, fill_value=np.nan):\\n    dtype_str = arr.dtype.name\\n    take_f = _get_take2d_function(dtype_str, axis='multi')\\n    row_idx = _ensure_int32(row_idx)\\n    col_idx = _ensure_int32(col_idx)\\n    out_shape = len(row_idx), len(col_idx)\\n    if dtype_str in ('int32', 'int64', 'bool'):\\n        row_mask = row_idx == -1\\n        col_mask=  col_idx == -1\\n        needs_masking = row_mask.any() or col_mask.any()\\n        if needs_masking:\\n            return take_2d_multi(_maybe_upcast(arr), row_idx, col_idx,\\n                                 fill_value=fill_value)\\n        else:\\n            out = np.empty(out_shape, dtype=arr.dtype)\\n            take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n            return out\\n    elif dtype_str in ('float64', 'object', 'datetime64[us]'):\\n        out = np.empty(out_shape, dtype=arr.dtype)\\n        take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n        return out\\n    else:\\n        return take_2d(take_2d(arr, row_idx, axis=0, fill_value=fill_value),\\n                       col_idx, axis=1, fill_value=fill_value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: great int32 -> int64 migration, though 0.01% chance of success on 32-bit",
    "fixed_code": "def take_2d_multi(arr, row_idx, col_idx, fill_value=np.nan):\\n    dtype_str = arr.dtype.name\\n    take_f = _get_take2d_function(dtype_str, axis='multi')\\n    row_idx = _ensure_int64(row_idx)\\n    col_idx = _ensure_int64(col_idx)\\n    out_shape = len(row_idx), len(col_idx)\\n    if dtype_str in ('int32', 'int64', 'bool'):\\n        row_mask = row_idx == -1\\n        col_mask=  col_idx == -1\\n        needs_masking = row_mask.any() or col_mask.any()\\n        if needs_masking:\\n            return take_2d_multi(_maybe_upcast(arr), row_idx, col_idx,\\n                                 fill_value=fill_value)\\n        else:\\n            out = np.empty(out_shape, dtype=arr.dtype)\\n            take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n            return out\\n    elif dtype_str in ('float64', 'object', 'datetime64[us]'):\\n        out = np.empty(out_shape, dtype=arr.dtype)\\n        take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n        return out\\n    else:\\n        return take_2d(take_2d(arr, row_idx, axis=0, fill_value=fill_value),\\n                       col_idx, axis=1, fill_value=fill_value)"
  },
  {
    "code": "def get_normalized_value(value, lhs):\\n    from django.db.models import Model\\n    if isinstance(value, Model):\\n        if value.pk is None:\\n            warnings.warn(\\n                \"Passing unsaved model instances to related filters is deprecated.\",\\n                RemovedInDjango50Warning,\\n            )\\n        value_list = []\\n        sources = lhs.output_field.path_infos[-1].target_fields\\n        for source in sources:\\n            while not isinstance(value, source.model) and source.remote_field:\\n                source = source.remote_field.model._meta.get_field(\\n                    source.remote_field.field_name\\n                )\\n            try:\\n                value_list.append(getattr(value, source.attname))\\n            except AttributeError:\\n                return (value.pk,)\\n        return tuple(value_list)\\n    if not isinstance(value, tuple):\\n        return (value,)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_normalized_value(value, lhs):\\n    from django.db.models import Model\\n    if isinstance(value, Model):\\n        if value.pk is None:\\n            warnings.warn(\\n                \"Passing unsaved model instances to related filters is deprecated.\",\\n                RemovedInDjango50Warning,\\n            )\\n        value_list = []\\n        sources = lhs.output_field.path_infos[-1].target_fields\\n        for source in sources:\\n            while not isinstance(value, source.model) and source.remote_field:\\n                source = source.remote_field.model._meta.get_field(\\n                    source.remote_field.field_name\\n                )\\n            try:\\n                value_list.append(getattr(value, source.attname))\\n            except AttributeError:\\n                return (value.pk,)\\n        return tuple(value_list)\\n    if not isinstance(value, tuple):\\n        return (value,)\\n    return value"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        asn=dict(required=True, type='str'),\\n        vrf=dict(required=False, type='str', default='default'),\\n        neighbor=dict(required=True, type='str'),\\n        afi=dict(required=True, type='str'),\\n        safi=dict(required=True, type='str'),\\n        additional_paths_receive=dict(required=False, type='str', choices=['enable', 'disable', 'inherit']),\\n        additional_paths_send=dict(required=False, type='str', choices=['enable', 'disable', 'inherit']),\\n        advertise_map_exist=dict(required=False, type='list'),\\n        advertise_map_non_exist=dict(required=False, type='list'),\\n        allowas_in=dict(required=False, type='bool'),\\n        allowas_in_max=dict(required=False, type='str'),\\n        as_override=dict(required=False, type='bool'),\\n        default_originate=dict(required=False, type='bool'),\\n        default_originate_route_map=dict(required=False, type='str'),\\n        disable_peer_as_check=dict(required=False, type='bool'),\\n        filter_list_in=dict(required=False, type='str'),\\n        filter_list_out=dict(required=False, type='str'),\\n        max_prefix_limit=dict(required=False, type='str'),\\n        max_prefix_interval=dict(required=False, type='str'),\\n        max_prefix_threshold=dict(required=False, type='str'),\\n        max_prefix_warning=dict(required=False, type='bool'),\\n        next_hop_self=dict(required=False, type='bool'),\\n        next_hop_third_party=dict(required=False, type='bool'),\\n        prefix_list_in=dict(required=False, type='str'),\\n        prefix_list_out=dict(required=False, type='str'),\\n        route_map_in=dict(required=False, type='str'),\\n        route_map_out=dict(required=False, type='str'),\\n        route_reflector_client=dict(required=False, type='bool'),\\n        send_community=dict(required=False, choices=['none', 'both', 'extended', 'standard', 'default']),\\n        soft_reconfiguration_in=dict(required=False, type='str', choices=['enable', 'always', 'inherit']),\\n        soo=dict(required=False, type='str'),\\n        suppress_inactive=dict(required=False, type='bool'),\\n        unsuppress_map=dict(required=False, type='str'),\\n        weight=dict(required=False, type='str'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        mutually_exclusive=[['advertise_map_exist', 'advertise_map_non_exist'],\\n                            ['max_prefix_interval', 'max_prefix_warning']],\\n        supports_check_mode=True,\\n    )\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    state = module.params['state']\\n    for key in ['max_prefix_interval', 'max_prefix_warning', 'max_prefix_threshold']:\\n        if module.params[key] and not module.params['max_prefix_limit']:\\n            module.fail_json(\\n                msg='max_prefix_limit is required when using %s' % key\\n            )\\n    if module.params['vrf'] == 'default' and module.params['soo']:\\n        module.fail_json(msg='SOO is only allowed in non-default VRF')\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args, warnings)\\n    if existing.get('asn') and state == 'present':\\n        if existing.get('asn') != module.params['asn']:\\n            module.fail_json(msg='Another BGP ASN already exists.',\\n                             proposed_asn=module.params['asn'],\\n                             existing_asn=existing.get('asn'))\\n    for param in ['advertise_map_exist', 'advertise_map_non_exist']:\\n        if module.params[param] == ['default']:\\n            module.params[param] = 'default'\\n    proposed_args = dict((k, v) for k, v in module.params.items() if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key not in ['asn', 'vrf', 'neighbor']:\\n            if not isinstance(value, list):\\n                if str(value).lower() == 'true':\\n                    value = True\\n                elif str(value).lower() == 'false':\\n                    value = False\\n                elif str(value).lower() == 'default':\\n                    if key in BOOL_PARAMS:\\n                        value = False\\n                    else:\\n                        value = 'default'\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif state == 'absent' and existing:\\n        state_absent(module, existing, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        warnings.extend(load_config(module, candidate))\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    else:\\n        result['commands'] = []\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        asn=dict(required=True, type='str'),\\n        vrf=dict(required=False, type='str', default='default'),\\n        neighbor=dict(required=True, type='str'),\\n        afi=dict(required=True, type='str'),\\n        safi=dict(required=True, type='str'),\\n        additional_paths_receive=dict(required=False, type='str', choices=['enable', 'disable', 'inherit']),\\n        additional_paths_send=dict(required=False, type='str', choices=['enable', 'disable', 'inherit']),\\n        advertise_map_exist=dict(required=False, type='list'),\\n        advertise_map_non_exist=dict(required=False, type='list'),\\n        allowas_in=dict(required=False, type='bool'),\\n        allowas_in_max=dict(required=False, type='str'),\\n        as_override=dict(required=False, type='bool'),\\n        default_originate=dict(required=False, type='bool'),\\n        default_originate_route_map=dict(required=False, type='str'),\\n        disable_peer_as_check=dict(required=False, type='bool'),\\n        filter_list_in=dict(required=False, type='str'),\\n        filter_list_out=dict(required=False, type='str'),\\n        max_prefix_limit=dict(required=False, type='str'),\\n        max_prefix_interval=dict(required=False, type='str'),\\n        max_prefix_threshold=dict(required=False, type='str'),\\n        max_prefix_warning=dict(required=False, type='bool'),\\n        next_hop_self=dict(required=False, type='bool'),\\n        next_hop_third_party=dict(required=False, type='bool'),\\n        prefix_list_in=dict(required=False, type='str'),\\n        prefix_list_out=dict(required=False, type='str'),\\n        route_map_in=dict(required=False, type='str'),\\n        route_map_out=dict(required=False, type='str'),\\n        route_reflector_client=dict(required=False, type='bool'),\\n        send_community=dict(required=False, choices=['none', 'both', 'extended', 'standard', 'default']),\\n        soft_reconfiguration_in=dict(required=False, type='str', choices=['enable', 'always', 'inherit']),\\n        soo=dict(required=False, type='str'),\\n        suppress_inactive=dict(required=False, type='bool'),\\n        unsuppress_map=dict(required=False, type='str'),\\n        weight=dict(required=False, type='str'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        mutually_exclusive=[['advertise_map_exist', 'advertise_map_non_exist'],\\n                            ['max_prefix_interval', 'max_prefix_warning']],\\n        supports_check_mode=True,\\n    )\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    state = module.params['state']\\n    for key in ['max_prefix_interval', 'max_prefix_warning', 'max_prefix_threshold']:\\n        if module.params[key] and not module.params['max_prefix_limit']:\\n            module.fail_json(\\n                msg='max_prefix_limit is required when using %s' % key\\n            )\\n    if module.params['vrf'] == 'default' and module.params['soo']:\\n        module.fail_json(msg='SOO is only allowed in non-default VRF')\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args, warnings)\\n    if existing.get('asn') and state == 'present':\\n        if existing.get('asn') != module.params['asn']:\\n            module.fail_json(msg='Another BGP ASN already exists.',\\n                             proposed_asn=module.params['asn'],\\n                             existing_asn=existing.get('asn'))\\n    for param in ['advertise_map_exist', 'advertise_map_non_exist']:\\n        if module.params[param] == ['default']:\\n            module.params[param] = 'default'\\n    proposed_args = dict((k, v) for k, v in module.params.items() if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key not in ['asn', 'vrf', 'neighbor']:\\n            if not isinstance(value, list):\\n                if str(value).lower() == 'true':\\n                    value = True\\n                elif str(value).lower() == 'false':\\n                    value = False\\n                elif str(value).lower() == 'default':\\n                    if key in BOOL_PARAMS:\\n                        value = False\\n                    else:\\n                        value = 'default'\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif state == 'absent' and existing:\\n        state_absent(module, existing, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        warnings.extend(load_config(module, candidate))\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    else:\\n        result['commands'] = []\\n    module.exit_json(**result)"
  },
  {
    "code": "def importKey(self, externKey):\\n\\t\\tif isinstance(externKey, unicode) and externKey.startswith(\"-----\"):\\n\\t\\t\\texternKey = externKey.encode('ascii')\\n\\t\\tif externKey.startswith(b('-----')):\\n\\t\\t\\tlines = externKey.replace(b(\" \"),b('')).split()\\n\\t\\t\\tder = binascii.a2b_base64(b('').join(lines[1:-1]))\\n\\t\\t\\treturn self._importKeyDER(der)\\n\\t\\tif externKey[0]==b('\\x30')[0]:\\n\\t\\t\\treturn self._importKeyDER(externKey)\\n\\t\\traise ValueError(\"RSA key format is not supported\")\\n_impl = RSAImplementation()\\ngenerate = _impl.generate\\nconstruct = _impl.construct",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def importKey(self, externKey):\\n\\t\\tif isinstance(externKey, unicode) and externKey.startswith(\"-----\"):\\n\\t\\t\\texternKey = externKey.encode('ascii')\\n\\t\\tif externKey.startswith(b('-----')):\\n\\t\\t\\tlines = externKey.replace(b(\" \"),b('')).split()\\n\\t\\t\\tder = binascii.a2b_base64(b('').join(lines[1:-1]))\\n\\t\\t\\treturn self._importKeyDER(der)\\n\\t\\tif externKey[0]==b('\\x30')[0]:\\n\\t\\t\\treturn self._importKeyDER(externKey)\\n\\t\\traise ValueError(\"RSA key format is not supported\")\\n_impl = RSAImplementation()\\ngenerate = _impl.generate\\nconstruct = _impl.construct"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            subscription=dict(required=True),\\n            token=dict(required=True, no_log=True),\\n            room=dict(required=True),\\n            msg=dict(required=True),\\n            notify=dict(required=False,\\n                        choices=[\"56k\", \"bell\", \"bezos\", \"bueller\",\\n                                 \"clowntown\", \"cottoneyejoe\",\\n                                 \"crickets\", \"dadgummit\", \"dangerzone\",\\n                                 \"danielsan\", \"deeper\", \"drama\",\\n                                 \"greatjob\", \"greyjoy\", \"guarantee\",\\n                                 \"heygirl\", \"horn\", \"horror\",\\n                                 \"inconceivable\", \"live\", \"loggins\",\\n                                 \"makeitso\", \"noooo\", \"nyan\", \"ohmy\",\\n                                 \"ohyeah\", \"pushit\", \"rimshot\",\\n                                 \"rollout\", \"rumble\", \"sax\", \"secret\",\\n                                 \"sexyback\", \"story\", \"tada\", \"tmyk\",\\n                                 \"trololo\", \"trombone\", \"unix\",\\n                                 \"vuvuzela\", \"what\", \"whoomp\", \"yeah\",\\n                                 \"yodel\"]),\\n        ),\\n        supports_check_mode=False\\n    )\\n    subscription = module.params[\"subscription\"]\\n    token = module.params[\"token\"]\\n    room = module.params[\"room\"]\\n    msg = module.params[\"msg\"]\\n    notify = module.params[\"notify\"]\\n    URI = \"https://%s.campfirenow.com\" % subscription\\n    NSTR = \"<message><type>SoundMessage</type><body>%s</body></message>\"\\n    MSTR = \"<message><body>%s</body></message>\"\\n    AGENT = \"Ansible/1.2\"\\n    module.params['url_username'] = token\\n    module.params['url_password'] = 'X'\\n    target_url = '%s/room/%s/speak.xml' % (URI, room)\\n    headers = {'Content-Type': 'application/xml',\\n               'User-agent': AGENT}\\n    if notify:\\n        response, info = fetch_url(module, target_url, data=NSTR % cgi.escape(notify), headers=headers)\\n        if info['status'] not in [200, 201]:\\n            module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                             \" returned error code: '%s'\" %\\n                                 (notify, info['status']))\\n    response, info = fetch_url(module, target_url, data=MSTR % cgi.escape(msg), headers=headers)\\n    if info['status'] not in [200, 201]:\\n        module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                         \" returned error code: '%s'\" %\\n                             (msg, info['status']))\\n    module.exit_json(changed=True, room=room, msg=msg, notify=notify)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix AttributeError due to cgi.escape removal in Python 3.8. (#66040)",
    "fixed_code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            subscription=dict(required=True),\\n            token=dict(required=True, no_log=True),\\n            room=dict(required=True),\\n            msg=dict(required=True),\\n            notify=dict(required=False,\\n                        choices=[\"56k\", \"bell\", \"bezos\", \"bueller\",\\n                                 \"clowntown\", \"cottoneyejoe\",\\n                                 \"crickets\", \"dadgummit\", \"dangerzone\",\\n                                 \"danielsan\", \"deeper\", \"drama\",\\n                                 \"greatjob\", \"greyjoy\", \"guarantee\",\\n                                 \"heygirl\", \"horn\", \"horror\",\\n                                 \"inconceivable\", \"live\", \"loggins\",\\n                                 \"makeitso\", \"noooo\", \"nyan\", \"ohmy\",\\n                                 \"ohyeah\", \"pushit\", \"rimshot\",\\n                                 \"rollout\", \"rumble\", \"sax\", \"secret\",\\n                                 \"sexyback\", \"story\", \"tada\", \"tmyk\",\\n                                 \"trololo\", \"trombone\", \"unix\",\\n                                 \"vuvuzela\", \"what\", \"whoomp\", \"yeah\",\\n                                 \"yodel\"]),\\n        ),\\n        supports_check_mode=False\\n    )\\n    subscription = module.params[\"subscription\"]\\n    token = module.params[\"token\"]\\n    room = module.params[\"room\"]\\n    msg = module.params[\"msg\"]\\n    notify = module.params[\"notify\"]\\n    URI = \"https://%s.campfirenow.com\" % subscription\\n    NSTR = \"<message><type>SoundMessage</type><body>%s</body></message>\"\\n    MSTR = \"<message><body>%s</body></message>\"\\n    AGENT = \"Ansible/1.2\"\\n    module.params['url_username'] = token\\n    module.params['url_password'] = 'X'\\n    target_url = '%s/room/%s/speak.xml' % (URI, room)\\n    headers = {'Content-Type': 'application/xml',\\n               'User-agent': AGENT}\\n    if notify:\\n        response, info = fetch_url(module, target_url, data=NSTR % html_escape(notify), headers=headers)\\n        if info['status'] not in [200, 201]:\\n            module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                             \" returned error code: '%s'\" %\\n                                 (notify, info['status']))\\n    response, info = fetch_url(module, target_url, data=MSTR % html_escape(msg), headers=headers)\\n    if info['status'] not in [200, 201]:\\n        module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                         \" returned error code: '%s'\" %\\n                             (msg, info['status']))\\n    module.exit_json(changed=True, room=room, msg=msg, notify=notify)"
  },
  {
    "code": "def preprocess_weights(obj: FrameOrSeries, weights, axis: int) -> np.ndarray:\\n    if isinstance(weights, ABCSeries):\\n        weights = weights.reindex(obj.axes[axis])\\n    if isinstance(weights, str):\\n        if isinstance(obj, ABCDataFrame):\\n            if axis == 0:\\n                try:\\n                    weights = obj[weights]\\n                except KeyError as err:\\n                    raise KeyError(\\n                        \"String passed to weights not a valid column\"\\n                    ) from err\\n            else:\\n                raise ValueError(\\n                    \"Strings can only be passed to \"\\n                    \"weights when sampling from rows on \"\\n                    \"a DataFrame\"\\n                )\\n        else:\\n            raise ValueError(\\n                \"Strings cannot be passed as weights when sampling from a Series.\"\\n            )\\n    if isinstance(obj, ABCSeries):\\n        func = obj._constructor\\n    else:\\n        func = obj._constructor_sliced\\n    weights = func(weights, dtype=\"float64\")._values\\n    if len(weights) != obj.shape[axis]:\\n        raise ValueError(\"Weights and axis to be sampled must be of same length\")\\n    if lib.has_infs(weights):\\n        raise ValueError(\"weight vector may not include `inf` values\")\\n    if (weights < 0).any():\\n        raise ValueError(\"weight vector many not include negative values\")\\n    weights[np.isnan(weights)] = 0\\n    return weights",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: sample modifying `weights` inplace (#42843)",
    "fixed_code": "def preprocess_weights(obj: FrameOrSeries, weights, axis: int) -> np.ndarray:\\n    if isinstance(weights, ABCSeries):\\n        weights = weights.reindex(obj.axes[axis])\\n    if isinstance(weights, str):\\n        if isinstance(obj, ABCDataFrame):\\n            if axis == 0:\\n                try:\\n                    weights = obj[weights]\\n                except KeyError as err:\\n                    raise KeyError(\\n                        \"String passed to weights not a valid column\"\\n                    ) from err\\n            else:\\n                raise ValueError(\\n                    \"Strings can only be passed to \"\\n                    \"weights when sampling from rows on \"\\n                    \"a DataFrame\"\\n                )\\n        else:\\n            raise ValueError(\\n                \"Strings cannot be passed as weights when sampling from a Series.\"\\n            )\\n    if isinstance(obj, ABCSeries):\\n        func = obj._constructor\\n    else:\\n        func = obj._constructor_sliced\\n    weights = func(weights, dtype=\"float64\")._values\\n    if len(weights) != obj.shape[axis]:\\n        raise ValueError(\"Weights and axis to be sampled must be of same length\")\\n    if lib.has_infs(weights):\\n        raise ValueError(\"weight vector may not include `inf` values\")\\n    if (weights < 0).any():\\n        raise ValueError(\"weight vector many not include negative values\")\\n    missing = np.isnan(weights)\\n    if missing.any():\\n        weights = weights.copy()\\n        weights[missing] = 0\\n    return weights"
  },
  {
    "code": "def __len__(self):\\n        return len(list(self.__iter__()))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #27002 -- Prevented double query when rendering ModelChoiceField.",
    "fixed_code": "def __len__(self):\\n        return len(self.subwidgets)"
  },
  {
    "code": "def traverse(ob, method, params):\\n\\tdotted_parts = method.split('.')\\n\\tif len(dotted_parts) != 2:\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\tnamespace, method = dotted_parts\\n\\tif method.startswith('_'):\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\trpcinterface = getattr(ob, namespace, None)\\n\\tif rpcinterface is None:\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\tfunc = getattr(rpcinterface, method, None)\\n\\tif not isinstance(func, types.MethodType):\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\ttry:\\n\\t\\treturn func(*params)\\n\\texcept TypeError:\\n\\t\\traise RPCError(Faults.INCORRECT_PARAMETERS)\\nclass SupervisorTransport(xmlrpclib.Transport):\\n\\tconnection = None\\n\\t_use_datetime = 0",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def traverse(ob, method, params):\\n\\tdotted_parts = method.split('.')\\n\\tif len(dotted_parts) != 2:\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\tnamespace, method = dotted_parts\\n\\tif method.startswith('_'):\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\trpcinterface = getattr(ob, namespace, None)\\n\\tif rpcinterface is None:\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\tfunc = getattr(rpcinterface, method, None)\\n\\tif not isinstance(func, types.MethodType):\\n\\t\\traise RPCError(Faults.UNKNOWN_METHOD)\\n\\ttry:\\n\\t\\treturn func(*params)\\n\\texcept TypeError:\\n\\t\\traise RPCError(Faults.INCORRECT_PARAMETERS)\\nclass SupervisorTransport(xmlrpclib.Transport):\\n\\tconnection = None\\n\\t_use_datetime = 0"
  },
  {
    "code": "def _get_new_index(self):\\n        ax = self.ax\\n        if len(ax) == 0:\\n            values = []\\n        else:\\n            start = ax[0].asfreq(self.freq, how=self.convention)\\n            end = ax[-1].asfreq(self.freq, how='end')\\n            values = period_range(start, end, freq=self.freq).values\\n        return ax._shallow_copy(values, freq=self.freq)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_new_index(self):\\n        ax = self.ax\\n        if len(ax) == 0:\\n            values = []\\n        else:\\n            start = ax[0].asfreq(self.freq, how=self.convention)\\n            end = ax[-1].asfreq(self.freq, how='end')\\n            values = period_range(start, end, freq=self.freq).values\\n        return ax._shallow_copy(values, freq=self.freq)"
  },
  {
    "code": "def get_gce_driver(self):\\n        ''\\n        config = ConfigParser.SafeConfigParser()\\n        config.read(os.path.dirname(os.path.realpath(__file__)) + '/gce.ini')\\n        secrets_path = config.get('gce', 'libcloud_secrets')\\n        secrets_found = False\\n        try:\\n            import secrets\\n            args = getattr(secrets, 'GCE_PARAMS', ())\\n            kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n            secrets_found = True\\n        except:\\n            pass\\n        if not secrets_found and secrets_path:\\n            if not secrets_path.endswith('secrets.py'):\\n                err = \"Must specify libcloud secrets file as \"\\n                err += \"/absolute/path/to/secrets.py\"\\n                print(err)\\n                sys.exit(1)\\n            sys.path.append(os.path.dirname(secrets_path))\\n            try:\\n                import secrets\\n                args = getattr(secrets, 'GCE_PARAMS', ())\\n                kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n                secrets_found = True\\n            except:\\n                pass\\n        if not secrets_found:\\n            args = (\\n                config.get('gce','gce_service_account_email_address'),\\n                config.get('gce','gce_service_account_pem_file_path')\\n            )\\n            kwargs = {'project': config.get('gce','gce_project_id')}\\n        gce = get_driver(Provider.GCE)(*args, **kwargs)\\n        gce.connection.user_agent_append(\"%s/%s\" % (\\n                USER_AGENT_PRODUCT, USER_AGENT_VERSION))\\n        return gce",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fixes #4954 plus updates for gcutil/v1beta16 tests",
    "fixed_code": "def get_gce_driver(self):\\n        ''\\n        gce_ini_default_path = os.path.join(\\n            os.path.dirname(os.path.realpath(__file__)), \"gce.ini\")\\n        gce_ini_path = os.environ.get('GCE_INI_PATH', gce_ini_default_path)\\n        config = ConfigParser.SafeConfigParser()\\n        config.read(gce_ini_path)\\n        secrets_path = config.get('gce', 'libcloud_secrets')\\n        secrets_found = False\\n        try:\\n            import secrets\\n            args = getattr(secrets, 'GCE_PARAMS', ())\\n            kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n            secrets_found = True\\n        except:\\n            pass\\n        if not secrets_found and secrets_path:\\n            if not secrets_path.endswith('secrets.py'):\\n                err = \"Must specify libcloud secrets file as \"\\n                err += \"/absolute/path/to/secrets.py\"\\n                print(err)\\n                sys.exit(1)\\n            sys.path.append(os.path.dirname(secrets_path))\\n            try:\\n                import secrets\\n                args = getattr(secrets, 'GCE_PARAMS', ())\\n                kwargs = getattr(secrets, 'GCE_KEYWORD_PARAMS', {})\\n                secrets_found = True\\n            except:\\n                pass\\n        if not secrets_found:\\n            args = (\\n                config.get('gce','gce_service_account_email_address'),\\n                config.get('gce','gce_service_account_pem_file_path')\\n            )\\n            kwargs = {'project': config.get('gce','gce_project_id')}\\n        gce = get_driver(Provider.GCE)(*args, **kwargs)\\n        gce.connection.user_agent_append(\"%s/%s\" % (\\n                USER_AGENT_PRODUCT, USER_AGENT_VERSION))\\n        return gce"
  },
  {
    "code": "def finalize_options(self):\\n        _sysconfig = __import__('sysconfig')\\n        self.set_undefined_options('build',\\n                                   ('build_lib', 'build_lib'),\\n                                   ('build_temp', 'build_temp'),\\n                                   ('compiler', 'compiler'),\\n                                   ('debug', 'debug'),\\n                                   ('force', 'force'),\\n                                   ('plat_name', 'plat_name'),\\n                                   )\\n        if self.package is None:\\n            self.package = self.distribution.ext_package\\n        self.extensions = self.distribution.ext_modules\\n        py_include = _sysconfig.get_path('include')\\n        plat_py_include = _sysconfig.get_path('platinclude')\\n        if self.include_dirs is None:\\n            self.include_dirs = self.distribution.include_dirs or []\\n        if isinstance(self.include_dirs, str):\\n            self.include_dirs = self.include_dirs.split(os.pathsep)\\n        self.include_dirs.append(py_include)\\n        if plat_py_include != py_include:\\n            self.include_dirs.append(plat_py_include)\\n        if isinstance(self.libraries, str):\\n            self.libraries = [self.libraries]\\n        if self.libraries is None:\\n            self.libraries = []\\n        if self.library_dirs is None:\\n            self.library_dirs = []\\n        elif isinstance(self.library_dirs, str):\\n            self.library_dirs = self.library_dirs.split(os.pathsep)\\n        if self.rpath is None:\\n            self.rpath = []\\n        elif isinstance(self.rpath, str):\\n            self.rpath = self.rpath.split(os.pathsep)\\n        if os.name == 'nt':\\n            self.library_dirs.append(os.path.join(sys.exec_prefix, 'libs'))\\n            if self.debug:\\n                self.build_temp = os.path.join(self.build_temp, \"Debug\")\\n            else:\\n                self.build_temp = os.path.join(self.build_temp, \"Release\")\\n            self.include_dirs.append(os.path.join(sys.exec_prefix, 'PC'))\\n            if MSVC_VERSION == 9:\\n                if self.plat_name == 'win32':\\n                    suffix = ''\\n                else:\\n                    suffix = self.plat_name[4:]\\n                new_lib = os.path.join(sys.exec_prefix, 'PCbuild')\\n                if suffix:\\n                    new_lib = os.path.join(new_lib, suffix)\\n                self.library_dirs.append(new_lib)\\n            elif MSVC_VERSION == 8:\\n                self.library_dirs.append(os.path.join(sys.exec_prefix,\\n                                         'PC', 'VS8.0', 'win32release'))\\n            elif MSVC_VERSION == 7:\\n                self.library_dirs.append(os.path.join(sys.exec_prefix,\\n                                         'PC', 'VS7.1'))\\n            else:\\n                self.library_dirs.append(os.path.join(sys.exec_prefix,\\n                                         'PC', 'VC6'))\\n        if os.name == 'os2':\\n            self.library_dirs.append(os.path.join(sys.exec_prefix, 'Config'))\\n        if sys.platform[:6] == 'cygwin' or sys.platform[:6] == 'atheos':\\n            if sys.executable.startswith(os.path.join(sys.exec_prefix, \"bin\")):\\n                self.library_dirs.append(os.path.join(sys.prefix, \"lib\",\\n                                  \"python\" + _sysconfig.get_python_version(),\\n                                                      \"config\"))\\n            else:\\n                self.library_dirs.append('.')\\n        _sysconfig.get_config_var('Py_ENABLE_SHARED')\\n        if ((sys.platform.startswith('linux') or sys.platform.startswith('gnu')\\n             or sys.platform.startswith('sunos'))\\n            and _sysconfig.get_config_var('Py_ENABLE_SHARED')):\\n            if sys.executable.startswith(os.path.join(sys.exec_prefix, \"bin\")):\\n                self.library_dirs.append(_sysconfig.get_config_var('LIBDIR'))\\n            else:\\n                self.library_dirs.append('.')\\n        if self.define:\\n            defines = self.define.split(',')\\n            self.define = [(symbol, '1') for symbol in defines]\\n        if self.undef:\\n            self.undef = self.undef.split(',')\\n        if self.swig_opts is None:\\n            self.swig_opts = []\\n        else:\\n            self.swig_opts = self.swig_opts.split(' ')\\n        if self.user:\\n            user_include = os.path.join(USER_BASE, \"include\")\\n            user_lib = os.path.join(USER_BASE, \"lib\")\\n            if os.path.isdir(user_include):\\n                self.include_dirs.append(user_include)\\n            if os.path.isdir(user_lib):\\n                self.library_dirs.append(user_lib)\\n                self.rpath.append(user_lib)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "reverting partially distutils to its 2.6.x state so 2.7a4 looks more like the 2.7b1 in this. the whole revert will occur after a4 is tagged",
    "fixed_code": "def finalize_options(self):\\n        from distutils import sysconfig\\n        self.set_undefined_options('build',\\n                                   ('build_lib', 'build_lib'),\\n                                   ('build_temp', 'build_temp'),\\n                                   ('compiler', 'compiler'),\\n                                   ('debug', 'debug'),\\n                                   ('force', 'force'),\\n                                   ('plat_name', 'plat_name'),\\n                                   )\\n        if self.package is None:\\n            self.package = self.distribution.ext_package\\n        self.extensions = self.distribution.ext_modules\\n        py_include = sysconfig.get_python_inc()\\n        plat_py_include = sysconfig.get_python_inc(plat_specific=1)\\n        if self.include_dirs is None:\\n            self.include_dirs = self.distribution.include_dirs or []\\n        if isinstance(self.include_dirs, str):\\n            self.include_dirs = self.include_dirs.split(os.pathsep)\\n        self.include_dirs.append(py_include)\\n        if plat_py_include != py_include:\\n            self.include_dirs.append(plat_py_include)\\n        if isinstance(self.libraries, str):\\n            self.libraries = [self.libraries]\\n        if self.libraries is None:\\n            self.libraries = []\\n        if self.library_dirs is None:\\n            self.library_dirs = []\\n        elif type(self.library_dirs) is StringType:\\n            self.library_dirs = string.split(self.library_dirs, os.pathsep)\\n        if self.rpath is None:\\n            self.rpath = []\\n        elif type(self.rpath) is StringType:\\n            self.rpath = string.split(self.rpath, os.pathsep)\\n        if os.name == 'nt':\\n            self.library_dirs.append(os.path.join(sys.exec_prefix, 'libs'))\\n            if self.debug:\\n                self.build_temp = os.path.join(self.build_temp, \"Debug\")\\n            else:\\n                self.build_temp = os.path.join(self.build_temp, \"Release\")\\n            self.include_dirs.append(os.path.join(sys.exec_prefix, 'PC'))\\n            if MSVC_VERSION == 9:\\n                if self.plat_name == 'win32':\\n                    suffix = ''\\n                else:\\n                    suffix = self.plat_name[4:]\\n                new_lib = os.path.join(sys.exec_prefix, 'PCbuild')\\n                if suffix:\\n                    new_lib = os.path.join(new_lib, suffix)\\n                self.library_dirs.append(new_lib)\\n            elif MSVC_VERSION == 8:\\n                self.library_dirs.append(os.path.join(sys.exec_prefix,\\n                                         'PC', 'VS8.0', 'win32release'))\\n            elif MSVC_VERSION == 7:\\n                self.library_dirs.append(os.path.join(sys.exec_prefix,\\n                                         'PC', 'VS7.1'))\\n            else:\\n                self.library_dirs.append(os.path.join(sys.exec_prefix,\\n                                         'PC', 'VC6'))\\n        if os.name == 'os2':\\n            self.library_dirs.append(os.path.join(sys.exec_prefix, 'Config'))\\n        if sys.platform[:6] == 'cygwin' or sys.platform[:6] == 'atheos':\\n            if sys.executable.startswith(os.path.join(sys.exec_prefix, \"bin\")):\\n                self.library_dirs.append(os.path.join(sys.prefix, \"lib\",\\n                                                      \"python\" + get_python_version(),\\n                                                      \"config\"))\\n            else:\\n                self.library_dirs.append('.')\\n        sysconfig.get_config_var('Py_ENABLE_SHARED')\\n        if ((sys.platform.startswith('linux') or sys.platform.startswith('gnu')\\n             or sys.platform.startswith('sunos'))\\n            and sysconfig.get_config_var('Py_ENABLE_SHARED')):\\n            if sys.executable.startswith(os.path.join(sys.exec_prefix, \"bin\")):\\n                self.library_dirs.append(sysconfig.get_config_var('LIBDIR'))\\n            else:\\n                self.library_dirs.append('.')\\n        if self.define:\\n            defines = self.define.split(',')\\n            self.define = map(lambda symbol: (symbol, '1'), defines)\\n        if self.undef:\\n            self.undef = self.undef.split(',')\\n        if self.swig_opts is None:\\n            self.swig_opts = []\\n        else:\\n            self.swig_opts = self.swig_opts.split(' ')\\n        if self.user:\\n            user_include = os.path.join(USER_BASE, \"include\")\\n            user_lib = os.path.join(USER_BASE, \"lib\")\\n            if os.path.isdir(user_include):\\n                self.include_dirs.append(user_include)\\n            if os.path.isdir(user_lib):\\n                self.library_dirs.append(user_lib)\\n                self.rpath.append(user_lib)"
  },
  {
    "code": "def _wrap_setop_result(self, other, result):\\n        if isinstance(self, (ABCDatetimeIndex, ABCTimedeltaIndex)) and isinstance(\\n            result, np.ndarray\\n        ):\\n            result = type(self._data)._simple_new(result, dtype=self.dtype)\\n        name = get_op_result_name(self, other)\\n        if isinstance(result, Index):\\n            if result.name != name:\\n                return result.rename(name)\\n            return result\\n        else:\\n            return self._shallow_copy(result, name=name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "annotate (#37917)",
    "fixed_code": "def _wrap_setop_result(self, other, result):\\n        if isinstance(self, (ABCDatetimeIndex, ABCTimedeltaIndex)) and isinstance(\\n            result, np.ndarray\\n        ):\\n            result = type(self._data)._simple_new(result, dtype=self.dtype)\\n        elif is_categorical_dtype(self.dtype) and isinstance(result, np.ndarray):\\n            result = Categorical(result, dtype=self.dtype)\\n        name = get_op_result_name(self, other)\\n        if isinstance(result, Index):\\n            if result.name != name:\\n                return result.rename(name)\\n            return result\\n        else:\\n            return self._shallow_copy(result, name=name)"
  },
  {
    "code": "def __init__(self, session_key=None):\\n        self.storage_path = type(self)._get_storage_path()\\n        self.file_prefix = settings.SESSION_COOKIE_NAME\\n        super().__init__(session_key)\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Removed unnecessary type() calls for class methods.",
    "fixed_code": "def __init__(self, session_key=None):\\n        self.storage_path = self._get_storage_path()\\n        self.file_prefix = settings.SESSION_COOKIE_NAME\\n        super().__init__(session_key)\\n    @classmethod"
  },
  {
    "code": "def set_tick_params(self, which='major', reset=False, **kw):\\n\\t\\tcbook._check_in_list(['major', 'minor', 'both'], which=which)\\n\\t\\tkwtrans = self._translate_tick_kw(kw)\\n\\t\\tif reset:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.clear()\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.clear()\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\tself.reset_ticks()\\n\\t\\telse:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.majorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.minorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif 'label1On' in kwtrans or 'label2On' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_visible(\\n\\t\\t\\t\\t\\tself._major_tick_kw.get('label1On', False)\\n\\t\\t\\t\\t\\tor self._major_tick_kw.get('label2On', False))\\n\\t\\t\\tif 'labelcolor' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_color(kwtrans['labelcolor'])\\n\\t\\tself.stale = True\\n\\t@staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_tick_params(self, which='major', reset=False, **kw):\\n\\t\\tcbook._check_in_list(['major', 'minor', 'both'], which=which)\\n\\t\\tkwtrans = self._translate_tick_kw(kw)\\n\\t\\tif reset:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.clear()\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.clear()\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\tself.reset_ticks()\\n\\t\\telse:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.majorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.minorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif 'label1On' in kwtrans or 'label2On' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_visible(\\n\\t\\t\\t\\t\\tself._major_tick_kw.get('label1On', False)\\n\\t\\t\\t\\t\\tor self._major_tick_kw.get('label2On', False))\\n\\t\\t\\tif 'labelcolor' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_color(kwtrans['labelcolor'])\\n\\t\\tself.stale = True\\n\\t@staticmethod"
  },
  {
    "code": "def getfullargspec(func):\\n    builtin_method_param = None\\n    if ismethod(func):\\n        func = func.__func__\\n    elif isbuiltin(func):\\n        text_signature = getattr(func, '__text_signature__', None)\\n        if text_signature and text_signature.startswith('($'):\\n            builtin_method_param = _signature_get_bound_param(text_signature)\\n    try:\\n        sig = signature(func)\\n    except Exception as ex:\\n        raise TypeError('unsupported callable') from ex\\n    args = []\\n    varargs = None\\n    varkw = None\\n    kwonlyargs = []\\n    defaults = ()\\n    annotations = {}\\n    defaults = ()\\n    kwdefaults = {}\\n    if sig.return_annotation is not sig.empty:\\n        annotations['return'] = sig.return_annotation\\n    for param in sig.parameters.values():\\n        kind = param.kind\\n        name = param.name\\n        if kind is _POSITIONAL_ONLY:\\n            args.append(name)\\n        elif kind is _POSITIONAL_OR_KEYWORD:\\n            args.append(name)\\n            if param.default is not param.empty:\\n                defaults += (param.default,)\\n        elif kind is _VAR_POSITIONAL:\\n            varargs = name\\n        elif kind is _KEYWORD_ONLY:\\n            kwonlyargs.append(name)\\n            if param.default is not param.empty:\\n                kwdefaults[name] = param.default\\n        elif kind is _VAR_KEYWORD:\\n            varkw = name\\n        if param.annotation is not param.empty:\\n            annotations[name] = param.annotation\\n    if not kwdefaults:\\n        kwdefaults = None\\n    if not defaults:\\n        defaults = None\\n    if builtin_method_param and (not args or args[0] != builtin_method_param):\\n        args.insert(0, builtin_method_param)\\n    return FullArgSpec(args, varargs, varkw, defaults,\\n                       kwonlyargs, kwdefaults, annotations)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "inspect: Fix getfullargspec() to not to follow __wrapped__ chains\\n\\nInitial patch by Nick Coghlan.",
    "fixed_code": "def getfullargspec(func):\\n    try:\\n        sig = _signature_internal(func,\\n                                  follow_wrapper_chains=False,\\n                                  skip_bound_arg=False)\\n    except Exception as ex:\\n        raise TypeError('unsupported callable') from ex\\n    args = []\\n    varargs = None\\n    varkw = None\\n    kwonlyargs = []\\n    defaults = ()\\n    annotations = {}\\n    defaults = ()\\n    kwdefaults = {}\\n    if sig.return_annotation is not sig.empty:\\n        annotations['return'] = sig.return_annotation\\n    for param in sig.parameters.values():\\n        kind = param.kind\\n        name = param.name\\n        if kind is _POSITIONAL_ONLY:\\n            args.append(name)\\n        elif kind is _POSITIONAL_OR_KEYWORD:\\n            args.append(name)\\n            if param.default is not param.empty:\\n                defaults += (param.default,)\\n        elif kind is _VAR_POSITIONAL:\\n            varargs = name\\n        elif kind is _KEYWORD_ONLY:\\n            kwonlyargs.append(name)\\n            if param.default is not param.empty:\\n                kwdefaults[name] = param.default\\n        elif kind is _VAR_KEYWORD:\\n            varkw = name\\n        if param.annotation is not param.empty:\\n            annotations[name] = param.annotation\\n    if not kwdefaults:\\n        kwdefaults = None\\n    if not defaults:\\n        defaults = None\\n    return FullArgSpec(args, varargs, varkw, defaults,\\n                       kwonlyargs, kwdefaults, annotations)"
  },
  {
    "code": "def findsource(object):\\n    file = getsourcefile(object) or getfile(object)\\n    module = getmodule(object)\\n    if module:\\n        lines = linecache.getlines(file, module.__dict__)\\n    else:\\n        lines = linecache.getlines(file)\\n    if not lines:\\n        raise IOError('could not get source code')\\n    if ismodule(object):\\n        return lines, 0\\n    if isclass(object):\\n        name = object.__name__\\n        pat = re.compile(r'^\\s*class\\s*' + name + r'\\b')\\n        for i in range(len(lines)):\\n            if pat.match(lines[i]): return lines, i\\n        else:\\n            raise IOError('could not find class definition')\\n    if ismethod(object):\\n        object = object.im_func\\n    if isfunction(object):\\n        object = object.func_code\\n    if istraceback(object):\\n        object = object.tb_frame\\n    if isframe(object):\\n        object = object.f_code\\n    if iscode(object):\\n        if not hasattr(object, 'co_firstlineno'):\\n            raise IOError('could not find function definition')\\n        lnum = object.co_firstlineno - 1\\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\\n        while lnum > 0:\\n            if pat.match(lines[lnum]): break\\n            lnum = lnum - 1\\n        return lines, lnum\\n    raise IOError('could not find code object')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix the speed regression in inspect.py by adding another cache to speed up getmodule(). Patch #1553314",
    "fixed_code": "def findsource(object):\\n    file = getsourcefile(object) or getfile(object)\\n    module = getmodule(object, file)\\n    if module:\\n        lines = linecache.getlines(file, module.__dict__)\\n    else:\\n        lines = linecache.getlines(file)\\n    if not lines:\\n        raise IOError('could not get source code')\\n    if ismodule(object):\\n        return lines, 0\\n    if isclass(object):\\n        name = object.__name__\\n        pat = re.compile(r'^\\s*class\\s*' + name + r'\\b')\\n        for i in range(len(lines)):\\n            if pat.match(lines[i]): return lines, i\\n        else:\\n            raise IOError('could not find class definition')\\n    if ismethod(object):\\n        object = object.im_func\\n    if isfunction(object):\\n        object = object.func_code\\n    if istraceback(object):\\n        object = object.tb_frame\\n    if isframe(object):\\n        object = object.f_code\\n    if iscode(object):\\n        if not hasattr(object, 'co_firstlineno'):\\n            raise IOError('could not find function definition')\\n        lnum = object.co_firstlineno - 1\\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\\n        while lnum > 0:\\n            if pat.match(lines[lnum]): break\\n            lnum = lnum - 1\\n        return lines, lnum\\n    raise IOError('could not find code object')"
  },
  {
    "code": "def power(a, b, third=None):\\n    if third is not None:\\n        raise MAError, \"3-argument power not supported.\"\\n    ma = getmask(a)\\n    mb = getmask(b)\\n    m = mask_or(ma, mb)\\n    fa = getdata(a)\\n    fb = getdata(b)\\n    if fb.dtype.char in typecodes[\"Integer\"]:\\n        return masked_array(umath.power(fa, fb), m)\\n    if numpy.abs(fb) < 1.:\\n        md = make_mask((fa < 0), shrink=True)\\n        m = mask_or(m, md)\\n    if m is nomask:\\n        return masked_array(umath.power(fa, fb))\\n    else:\\n        fa = fa.copy()\\n        fa[m] = 1\\n        return masked_array(umath.power(fa, fb), m)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "core : power : mask all negative values when the exponent (b) doesn't satisfy (abs(b-int(b))<np.finfo(float).precision)",
    "fixed_code": "def power(a, b, third=None):\\n    if third is not None:\\n        raise MAError, \"3-argument power not supported.\"\\n    ma = getmask(a)\\n    mb = getmask(b)\\n    m = mask_or(ma, mb)\\n    fa = getdata(a)\\n    fb = getdata(b)\\n    if fb.dtype.char in typecodes[\"Integer\"]:\\n        return masked_array(umath.power(fa, fb), m)\\n    md = (abs(fb-int(fb)) < numpy.finfo(float).precision)\\n    m = mask_or(m, md)\\n    if m is nomask:\\n        return masked_array(umath.power(fa, fb))\\n    else:\\n        fa = fa.copy()\\n        if m.all():\\n            fa[m] = 1\\n        else:\\n            numpy.putmask(fa,m,1)\\n        return masked_array(umath.power(fa, fb), m)"
  },
  {
    "code": "def _round_ceiling(self, prec):\\n        if self._sign:\\n            return self._round_down(prec)\\n        else:\\n            return -self._round_down(prec)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _round_ceiling(self, prec):\\n        if self._sign:\\n            return self._round_down(prec)\\n        else:\\n            return -self._round_down(prec)"
  },
  {
    "code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    if _is_sqlalchemy_engine(con):\\n        return PandasSQLAlchemy(con, schema=schema, meta=meta)\\n    else:\\n        if flavor == 'mysql':\\n            warnings.warn(_MYSQL_WARNING, FutureWarning)\\n        return PandasSQLLegacy(con, flavor, is_cursor=is_cursor)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: SQL class definitions renaming\\n\\n- renaming of PandasSQLAlchemy to SQLDatabase and PandasSQLTable to SQLTable\\n- renaming of SQLDatabase.read_sql to read_query (to be consistent with read_query/read_table)\\n- legacy -> fallback",
    "fixed_code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    if _is_sqlalchemy_engine(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    else:\\n        if flavor == 'mysql':\\n            warnings.warn(_MYSQL_WARNING, FutureWarning)\\n        return SQLiteDatabase(con, flavor, is_cursor=is_cursor)"
  },
  {
    "code": "def is_free(self):\\n        return bool(self.__scope == FREE)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_free(self):\\n        return bool(self.__scope == FREE)"
  },
  {
    "code": "def __init__(self, reactor, contextFactory=WebClientContextFactory(),\\n                 connectTimeout=180, bindAddress=None):\\n        self._reactor = reactor\\n        self._contextFactory = contextFactory\\n        self._connectTimeout = connectTimeout\\n        self._bindAddress = bindAddress",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, reactor, contextFactory=WebClientContextFactory(),\\n                 connectTimeout=180, bindAddress=None):\\n        self._reactor = reactor\\n        self._contextFactory = contextFactory\\n        self._connectTimeout = connectTimeout\\n        self._bindAddress = bindAddress"
  },
  {
    "code": "def database_backwards(self, app_label, schema_editor, from_state, to_state):\\n        if not router.allow_migrate(schema_editor.connection.alias, app_label):\\n            return\\n        schema_editor.execute(\"DROP EXTENSION %s\" % schema_editor.quote_name(self.name))\\n        get_hstore_oids.cache_clear()\\n        get_citext_oids.cache_clear()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #31615 -- Made migrations skip extension operations if not needed.\\n\\n- Don't try to create an existing extension.\\n- Don't try to drop a nonexistent extension.",
    "fixed_code": "def database_backwards(self, app_label, schema_editor, from_state, to_state):\\n        if not router.allow_migrate(schema_editor.connection.alias, app_label):\\n            return\\n        if self.extension_exists(schema_editor, self.name):\\n            schema_editor.execute(\\n                'DROP EXTENSION %s' % schema_editor.quote_name(self.name)\\n            )\\n        get_hstore_oids.cache_clear()\\n        get_citext_oids.cache_clear()"
  },
  {
    "code": "def merge_objects(base_obj, client_obj):\\n    if not base_obj:\\n        return client_obj\\n    if not client_obj:\\n        return base_obj\\n    client_obj_cp = copy.deepcopy(client_obj)\\n    for base_key in base_obj.to_dict().keys():\\n        base_val = getattr(base_obj, base_key, None)\\n        if not getattr(client_obj, base_key, None) and base_val:\\n            setattr(client_obj_cp, base_key, base_val)\\n    return client_obj_cp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add reconcile_metadata to reconcile_pods (#10266)\\n\\nmetadata objects require a more complex merge strategy\\nthen a simple \"merge pods\" for merging labels and other\\nfeatures",
    "fixed_code": "def merge_objects(base_obj, client_obj):\\n    if not base_obj:\\n        return client_obj\\n    if not client_obj:\\n        return base_obj\\n    client_obj_cp = copy.deepcopy(client_obj)\\n    if isinstance(base_obj, dict) and isinstance(client_obj_cp, dict):\\n        client_obj_cp.update(base_obj)\\n        return client_obj_cp\\n    for base_key in base_obj.to_dict().keys():\\n        base_val = getattr(base_obj, base_key, None)\\n        if not getattr(client_obj, base_key, None) and base_val:\\n            if not isinstance(client_obj_cp, dict):\\n                setattr(client_obj_cp, base_key, base_val)\\n            else:\\n                client_obj_cp[base_key] = base_val\\n    return client_obj_cp"
  },
  {
    "code": "def _wrap_setop_result(self, other, result):\\n        name = get_op_result_name(self, other)\\n        return self._shallow_copy(result, name=name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _wrap_setop_result(self, other, result):\\n        name = get_op_result_name(self, other)\\n        return self._shallow_copy(result, name=name)"
  },
  {
    "code": "def read_sas(filepath_or_buffer, format=None, index=None, encoding=None,\\n             chunksize=None, iterator=False):\\n    if format is None:\\n        buffer_error_msg = (\"If this is a buffer object rather \"\\n                            \"than a string name, you must specify \"\\n                            \"a format string\")\\n        filepath_or_buffer = _stringify_path(filepath_or_buffer)\\n        if not isinstance(filepath_or_buffer, compat.string_types):\\n            raise ValueError(buffer_error_msg)\\n        fname = filepath_or_buffer.lower()\\n        if fname.endswith(\".xpt\"):\\n            format = \"xport\"\\n        elif fname.endswith(\".sas7bdat\"):\\n            format = \"sas7bdat\"\\n        else:\\n            raise ValueError(\"unable to infer format of SAS file\")\\n    if format.lower() == 'xport':\\n        from pandas.io.sas.sas_xport import XportReader\\n        reader = XportReader(filepath_or_buffer, index=index,\\n                             encoding=encoding,\\n                             chunksize=chunksize)\\n    elif format.lower() == 'sas7bdat':\\n        from pandas.io.sas.sas7bdat import SAS7BDATReader\\n        reader = SAS7BDATReader(filepath_or_buffer, index=index,\\n                                encoding=encoding,\\n                                chunksize=chunksize)\\n    else:\\n        raise ValueError('unknown SAS format')\\n    if iterator or chunksize:\\n        return reader\\n    data = reader.read()\\n    reader.close()\\n    return data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_sas(filepath_or_buffer, format=None, index=None, encoding=None,\\n             chunksize=None, iterator=False):\\n    if format is None:\\n        buffer_error_msg = (\"If this is a buffer object rather \"\\n                            \"than a string name, you must specify \"\\n                            \"a format string\")\\n        filepath_or_buffer = _stringify_path(filepath_or_buffer)\\n        if not isinstance(filepath_or_buffer, compat.string_types):\\n            raise ValueError(buffer_error_msg)\\n        fname = filepath_or_buffer.lower()\\n        if fname.endswith(\".xpt\"):\\n            format = \"xport\"\\n        elif fname.endswith(\".sas7bdat\"):\\n            format = \"sas7bdat\"\\n        else:\\n            raise ValueError(\"unable to infer format of SAS file\")\\n    if format.lower() == 'xport':\\n        from pandas.io.sas.sas_xport import XportReader\\n        reader = XportReader(filepath_or_buffer, index=index,\\n                             encoding=encoding,\\n                             chunksize=chunksize)\\n    elif format.lower() == 'sas7bdat':\\n        from pandas.io.sas.sas7bdat import SAS7BDATReader\\n        reader = SAS7BDATReader(filepath_or_buffer, index=index,\\n                                encoding=encoding,\\n                                chunksize=chunksize)\\n    else:\\n        raise ValueError('unknown SAS format')\\n    if iterator or chunksize:\\n        return reader\\n    data = reader.read()\\n    reader.close()\\n    return data"
  },
  {
    "code": "def __sub__(self, other):\\n        warnings.warn(\"using '-' to provide set differences with Indexes is deprecated, \"\\n                      \"use .difference()\",FutureWarning)\\n        return self.difference(other)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __sub__(self, other):\\n        warnings.warn(\"using '-' to provide set differences with Indexes is deprecated, \"\\n                      \"use .difference()\",FutureWarning)\\n        return self.difference(other)"
  },
  {
    "code": "def load_current_config(self):\\n        self._os_version = self._get_os_version()\\n        self._current_config = dict()\\n        pfc_config = self._get_pfc_config()\\n        if not pfc_config:\\n            return\\n        if self._os_version >= self.ONYX_API_VERSION:\\n            if len(pfc_config) >= 3:\\n                pfc_config = pfc_config[2]\\n            else:\\n                pfc_config = dict()\\n        else:\\n            if 'Table 2' in pfc_config:\\n                pfc_config = pfc_config['Table 2']\\n        for if_name, if_pfc_data in iteritems(pfc_config):\\n            match = self.PFC_IF_REGEX.match(if_name)\\n            if not match:\\n                continue\\n            if if_pfc_data:\\n                if_pfc_data = if_pfc_data[0]\\n                self._current_config[if_name] = \\\\n                    self._create_if_pfc_data(if_name, if_pfc_data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_current_config(self):\\n        self._os_version = self._get_os_version()\\n        self._current_config = dict()\\n        pfc_config = self._get_pfc_config()\\n        if not pfc_config:\\n            return\\n        if self._os_version >= self.ONYX_API_VERSION:\\n            if len(pfc_config) >= 3:\\n                pfc_config = pfc_config[2]\\n            else:\\n                pfc_config = dict()\\n        else:\\n            if 'Table 2' in pfc_config:\\n                pfc_config = pfc_config['Table 2']\\n        for if_name, if_pfc_data in iteritems(pfc_config):\\n            match = self.PFC_IF_REGEX.match(if_name)\\n            if not match:\\n                continue\\n            if if_pfc_data:\\n                if_pfc_data = if_pfc_data[0]\\n                self._current_config[if_name] = \\\\n                    self._create_if_pfc_data(if_name, if_pfc_data)"
  },
  {
    "code": "def _rfft(input_tensor, fft_length=None, name=None):\\n\\twith _ops.name_scope(name, default_name,\\n\\t\\t\\t\\t\\t\\t [input_tensor, fft_length]) as name:\\n\\t  input_tensor = _ops.convert_to_tensor(input_tensor, _dtypes.float32)\\n\\t  input_tensor.shape.with_rank_at_least(fft_rank)\\n\\t  if fft_length is None:\\n\\t\\tfft_length = _infer_fft_length_for_rfft(input_tensor, fft_rank)\\n\\t  else:\\n\\t\\tfft_length = _ops.convert_to_tensor(fft_length, _dtypes.int32)\\n\\t  input_tensor = _maybe_pad_for_rfft(input_tensor, fft_rank, fft_length)\\n\\t  return fft_fn(input_tensor, fft_length, name)\\n  _rfft.__doc__ = fft_fn.__doc__\\n  return _rfft",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Add complex128 support to RFFT, RFFT2D, RFFT3D, IRFFT, IRFFT2D, and IRFFT3D.\\n\\nFinishes support requested in:\\n- #10749\\n- #17332",
    "fixed_code": "def _rfft(input_tensor, fft_length=None, name=None):\\n\\twith _ops.name_scope(name, default_name,\\n\\t\\t\\t\\t\\t\\t [input_tensor, fft_length]) as name:\\n\\t  input_tensor = _ops.convert_to_tensor(input_tensor,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpreferred_dtype=_dtypes.float32)\\n\\t  real_dtype = input_tensor.dtype\\n\\t  if real_dtype == _dtypes.float32:\\n\\t\\tcomplex_dtype = _dtypes.complex64\\n\\t  elif real_dtype == _dtypes.float64:\\n\\t\\tcomplex_dtype = _dtypes.complex128\\n\\t  input_tensor.shape.with_rank_at_least(fft_rank)\\n\\t  if fft_length is None:\\n\\t\\tfft_length = _infer_fft_length_for_rfft(input_tensor, fft_rank)\\n\\t  else:\\n\\t\\tfft_length = _ops.convert_to_tensor(fft_length, _dtypes.int32)\\n\\t  input_tensor = _maybe_pad_for_rfft(input_tensor, fft_rank, fft_length)\\n\\t  if not compat.forward_compatible(2019, 10, 12):\\n\\t\\treturn fft_fn(input_tensor, fft_length, name=name)\\n\\t  return fft_fn(input_tensor, fft_length, Tcomplex=complex_dtype, name=name)\\n  _rfft.__doc__ = fft_fn.__doc__\\n  return _rfft"
  },
  {
    "code": "def homogeneity_completeness_v_measure(labels_true, labels_pred):\\n    labels_true = np.asanyarray(labels_true)\\n    labels_pred = np.asanyarray(labels_pred)\\n    if labels_true.ndim != 1:\\n        raise ValueError(\\n            \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\\n    if labels_pred.ndim != 1:\\n        raise ValueError(\\n            \"labels_pred must be 1D: shape is %r\" % (labels_pred.shape,))\\n    if labels_true.shape != labels_pred.shape:\\n        raise ValueError(\\n            \"labels_true and labels_true must have same shape, got %r and %r\"\\n            % (labels_true.shape, labels_pred.shape))\\n    n_samples = labels_true.shape[0]\\n    entropy_K_given_C = 0.\\n    entropy_C_given_K = 0.\\n    entropy_C = 0.\\n    entropy_K = 0.\\n    classes = np.unique(labels_true)\\n    clusters = np.unique(labels_pred)\\n    n_C = [float(np.sum(labels_true == c)) for c in classes]\\n    n_K = [float(np.sum(labels_pred == k)) for k in clusters]\\n    for c in classes:\\n        entropy_C -= n_C[c] / n_samples * log(n_C[c] / n_samples)\\n    for k in clusters:\\n        entropy_K -= n_K[k] / n_samples * log(n_K[k] / n_samples)\\n    for c in classes:\\n        for k in clusters:\\n            n_CK = float(np.sum((labels_true == c) * (labels_pred == k)))\\n            if n_CK != 0.0:\\n                entropy_C_given_K -= n_CK / n_samples * log(n_CK / n_K[k])\\n                entropy_K_given_C -= n_CK / n_samples * log(n_CK / n_C[c])\\n    homogeneity = 1.0 - entropy_C_given_K / entropy_C if entropy_C else 1.0\\n    completeness = 1.0 - entropy_K_given_C / entropy_K if entropy_K else 1.0\\n    if homogeneity + completeness == 0.0:\\n        v_measure_score = 0.0\\n    else:\\n        v_measure_score = (2.0 * homogeneity * completeness\\n                           / (homogeneity + completeness))\\n    return homogeneity, completeness, v_measure_score",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "100% test coverage for the new clustering metrics",
    "fixed_code": "def homogeneity_completeness_v_measure(labels_true, labels_pred):\\n    labels_true = np.asanyarray(labels_true)\\n    labels_pred = np.asanyarray(labels_pred)\\n    if labels_true.ndim != 1:\\n        raise ValueError(\\n            \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\\n    if labels_pred.ndim != 1:\\n        raise ValueError(\\n            \"labels_pred must be 1D: shape is %r\" % (labels_pred.shape,))\\n    if labels_true.shape != labels_pred.shape:\\n        raise ValueError(\\n            \"labels_true and labels_pred must have same size, got %d and %d\"\\n            % (labels_true.shape[0], labels_pred.shape[0]))\\n    n_samples = labels_true.shape[0]\\n    entropy_K_given_C = 0.\\n    entropy_C_given_K = 0.\\n    entropy_C = 0.\\n    entropy_K = 0.\\n    classes = np.unique(labels_true)\\n    clusters = np.unique(labels_pred)\\n    n_C = [float(np.sum(labels_true == c)) for c in classes]\\n    n_K = [float(np.sum(labels_pred == k)) for k in clusters]\\n    for c in classes:\\n        entropy_C -= n_C[c] / n_samples * log(n_C[c] / n_samples)\\n    for k in clusters:\\n        entropy_K -= n_K[k] / n_samples * log(n_K[k] / n_samples)\\n    for c in classes:\\n        for k in clusters:\\n            n_CK = float(np.sum((labels_true == c) * (labels_pred == k)))\\n            if n_CK != 0.0:\\n                entropy_C_given_K -= n_CK / n_samples * log(n_CK / n_K[k])\\n                entropy_K_given_C -= n_CK / n_samples * log(n_CK / n_C[c])\\n    homogeneity = 1.0 - entropy_C_given_K / entropy_C if entropy_C else 1.0\\n    completeness = 1.0 - entropy_K_given_C / entropy_K if entropy_K else 1.0\\n    if homogeneity + completeness == 0.0:\\n        v_measure_score = 0.0\\n    else:\\n        v_measure_score = (2.0 * homogeneity * completeness\\n                           / (homogeneity + completeness))\\n    return homogeneity, completeness, v_measure_score"
  },
  {
    "code": "def as_ul_with_errors(self):\\n        \"Returns this form rendered as HTML <li>s, with errors.\"\\n        output = []\\n        if self.errors().get(NON_FIELD_ERRORS):\\n            output.append('<li><ul>%s</ul></li>' % '\\n'.join(['<li>%s</li>' % e for e in self.errors()[NON_FIELD_ERRORS]]))\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = '<li>'\\n            if bf.errors:\\n                line += '<ul>%s</ul>' % '\\n'.join(['<li>%s</li>' % e for e in bf.errors])\\n            line += '%s: %s</li>' % (pretty_name(name), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_ul_with_errors(self):\\n        \"Returns this form rendered as HTML <li>s, with errors.\"\\n        output = []\\n        if self.errors().get(NON_FIELD_ERRORS):\\n            output.append('<li><ul>%s</ul></li>' % '\\n'.join(['<li>%s</li>' % e for e in self.errors()[NON_FIELD_ERRORS]]))\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = '<li>'\\n            if bf.errors:\\n                line += '<ul>%s</ul>' % '\\n'.join(['<li>%s</li>' % e for e in bf.errors])\\n            line += '%s: %s</li>' % (pretty_name(name), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)"
  },
  {
    "code": "def _bar(\\n    data: FrameOrSeries,\\n    align: str | float | int | Callable,\\n    colors: list[str],\\n    width: float,\\n    vmin: float | None,\\n    vmax: float | None,\\n    base_css: str,\\n):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Styler.bar height control (#42483)",
    "fixed_code": "def _bar(\\n    data: FrameOrSeries,\\n    align: str | float | int | Callable,\\n    colors: list[str],\\n    width: float,\\n    height: float,\\n    vmin: float | None,\\n    vmax: float | None,\\n    base_css: str,\\n):"
  },
  {
    "code": "def _signature_from_function(cls, func, skip_bound_arg=True):\\n    is_duck_function = False\\n    if not isfunction(func):\\n        if _signature_is_functionlike(func):\\n            is_duck_function = True\\n        else:\\n            raise TypeError('{!r} is not a Python function'.format(func))\\n    s = getattr(func, \"__text_signature__\", None)\\n    if s:\\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\\n    Parameter = cls._parameter_cls\\n    func_code = func.__code__\\n    pos_count = func_code.co_argcount\\n    arg_names = func_code.co_varnames\\n    posonly_count = func_code.co_posonlyargcount\\n    positional_count = posonly_count + pos_count\\n    positional_only = tuple(arg_names[:posonly_count])\\n    positional = tuple(arg_names[posonly_count:positional_count])\\n    keyword_only_count = func_code.co_kwonlyargcount\\n    keyword_only = arg_names[positional_count:(positional_count + keyword_only_count)]\\n    annotations = func.__annotations__\\n    defaults = func.__defaults__\\n    kwdefaults = func.__kwdefaults__\\n    if defaults:\\n        pos_default_count = len(defaults)\\n    else:\\n        pos_default_count = 0\\n    parameters = []\\n    non_default_count = positional_count - pos_default_count\\n    all_positional = positional_only + positional\\n    posonly_left = posonly_count\\n    for name in all_positional[:non_default_count]:\\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=kind))\\n        if posonly_left:\\n            posonly_left -= 1\\n    for offset, name in enumerate(all_positional[non_default_count:]):\\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=kind,\\n                                    default=defaults[offset]))\\n        if posonly_left:\\n            posonly_left -= 1\\n    if func_code.co_flags & CO_VARARGS:\\n        name = arg_names[positional_count + keyword_only_count]\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=_VAR_POSITIONAL))\\n    for name in keyword_only:\\n        default = _empty\\n        if kwdefaults is not None:\\n            default = kwdefaults.get(name, _empty)\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=_KEYWORD_ONLY,\\n                                    default=default))\\n    if func_code.co_flags & CO_VARKEYWORDS:\\n        index = positional_count + keyword_only_count\\n        if func_code.co_flags & CO_VARARGS:\\n            index += 1\\n        name = arg_names[index]\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=_VAR_KEYWORD))\\n    return cls(parameters,\\n               return_annotation=annotations.get('return', _empty),\\n               __validate_parameters__=is_duck_function)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _signature_from_function(cls, func, skip_bound_arg=True):\\n    is_duck_function = False\\n    if not isfunction(func):\\n        if _signature_is_functionlike(func):\\n            is_duck_function = True\\n        else:\\n            raise TypeError('{!r} is not a Python function'.format(func))\\n    s = getattr(func, \"__text_signature__\", None)\\n    if s:\\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\\n    Parameter = cls._parameter_cls\\n    func_code = func.__code__\\n    pos_count = func_code.co_argcount\\n    arg_names = func_code.co_varnames\\n    posonly_count = func_code.co_posonlyargcount\\n    positional_count = posonly_count + pos_count\\n    positional_only = tuple(arg_names[:posonly_count])\\n    positional = tuple(arg_names[posonly_count:positional_count])\\n    keyword_only_count = func_code.co_kwonlyargcount\\n    keyword_only = arg_names[positional_count:(positional_count + keyword_only_count)]\\n    annotations = func.__annotations__\\n    defaults = func.__defaults__\\n    kwdefaults = func.__kwdefaults__\\n    if defaults:\\n        pos_default_count = len(defaults)\\n    else:\\n        pos_default_count = 0\\n    parameters = []\\n    non_default_count = positional_count - pos_default_count\\n    all_positional = positional_only + positional\\n    posonly_left = posonly_count\\n    for name in all_positional[:non_default_count]:\\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=kind))\\n        if posonly_left:\\n            posonly_left -= 1\\n    for offset, name in enumerate(all_positional[non_default_count:]):\\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=kind,\\n                                    default=defaults[offset]))\\n        if posonly_left:\\n            posonly_left -= 1\\n    if func_code.co_flags & CO_VARARGS:\\n        name = arg_names[positional_count + keyword_only_count]\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=_VAR_POSITIONAL))\\n    for name in keyword_only:\\n        default = _empty\\n        if kwdefaults is not None:\\n            default = kwdefaults.get(name, _empty)\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=_KEYWORD_ONLY,\\n                                    default=default))\\n    if func_code.co_flags & CO_VARKEYWORDS:\\n        index = positional_count + keyword_only_count\\n        if func_code.co_flags & CO_VARARGS:\\n            index += 1\\n        name = arg_names[index]\\n        annotation = annotations.get(name, _empty)\\n        parameters.append(Parameter(name, annotation=annotation,\\n                                    kind=_VAR_KEYWORD))\\n    return cls(parameters,\\n               return_annotation=annotations.get('return', _empty),\\n               __validate_parameters__=is_duck_function)"
  },
  {
    "code": "def hook(self):\\n        remote_conn_id = conf.get('logging', 'REMOTE_LOG_CONN_ID')\\n        try:\\n            from airflow.providers.amazon.aws.hooks.logs import AwsLogsHook\\n            return AwsLogsHook(aws_conn_id=remote_conn_id, region_name=self.region_name)\\n        except Exception as e:\\n            self.log.error(\\n                'Could not create an AwsLogsHook with connection id \"%s\". '\\n                'Please make sure that apache-airflow[aws] is installed and '\\n                'the Cloudwatch logs connection exists. Exception: \"%s\"',\\n                remote_conn_id,\\n                e,\\n            )\\n            return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove redundand catch exception in Amazon Log Task Handlers (#26442)",
    "fixed_code": "def hook(self):\\n        return AwsLogsHook(\\n            aws_conn_id=conf.get('logging', 'REMOTE_LOG_CONN_ID'), region_name=self.region_name\\n        )"
  },
  {
    "code": "def _create_sql_schema(self, frame, table_name):\\n        table = PandasSQLTable(table_name, self, frame=frame)\\n        return str(table.sql_schema())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_sql_schema(self, frame, table_name):\\n        table = PandasSQLTable(table_name, self, frame=frame)\\n        return str(table.sql_schema())"
  },
  {
    "code": "def to_datetime(self, dayfirst=False):\\n        warnings.warn(\"to_datetime is deprecated. Use self.to_timestamp(...)\",\\n                      FutureWarning, stacklevel=2)\\n        return self.to_timestamp()\\n    year = _field_accessor('year', 0, \"The year of the period\")\\n    month = _field_accessor('month', 3, \"The month as January=1, December=12\")\\n    day = _field_accessor('day', 4, \"The days of the period\")\\n    hour = _field_accessor('hour', 5, \"The hour of the period\")\\n    minute = _field_accessor('minute', 6, \"The minute of the period\")\\n    second = _field_accessor('second', 7, \"The second of the period\")\\n    weekofyear = _field_accessor('week', 8, \"The week ordinal of the year\")\\n    week = weekofyear\\n    dayofweek = _field_accessor('dayofweek', 10,\\n                                \"The day of the week with Monday=0, Sunday=6\")\\n    weekday = dayofweek\\n    dayofyear = day_of_year = _field_accessor('dayofyear', 9,\\n                                              \"The ordinal day of the year\")\\n    quarter = _field_accessor('quarter', 2, \"The quarter of the date\")\\n    qyear = _field_accessor('qyear', 1)\\n    days_in_month = _field_accessor('days_in_month', 11,\\n                                    \"The number of days in the month\")\\n    daysinmonth = days_in_month\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_datetime(self, dayfirst=False):\\n        warnings.warn(\"to_datetime is deprecated. Use self.to_timestamp(...)\",\\n                      FutureWarning, stacklevel=2)\\n        return self.to_timestamp()\\n    year = _field_accessor('year', 0, \"The year of the period\")\\n    month = _field_accessor('month', 3, \"The month as January=1, December=12\")\\n    day = _field_accessor('day', 4, \"The days of the period\")\\n    hour = _field_accessor('hour', 5, \"The hour of the period\")\\n    minute = _field_accessor('minute', 6, \"The minute of the period\")\\n    second = _field_accessor('second', 7, \"The second of the period\")\\n    weekofyear = _field_accessor('week', 8, \"The week ordinal of the year\")\\n    week = weekofyear\\n    dayofweek = _field_accessor('dayofweek', 10,\\n                                \"The day of the week with Monday=0, Sunday=6\")\\n    weekday = dayofweek\\n    dayofyear = day_of_year = _field_accessor('dayofyear', 9,\\n                                              \"The ordinal day of the year\")\\n    quarter = _field_accessor('quarter', 2, \"The quarter of the date\")\\n    qyear = _field_accessor('qyear', 1)\\n    days_in_month = _field_accessor('days_in_month', 11,\\n                                    \"The number of days in the month\")\\n    daysinmonth = days_in_month\\n    @property"
  },
  {
    "code": "def shortcut(request, content_type_id, object_id):\\n    try:\\n        content_type = ContentType.objects.get(pk=content_type_id)\\n        if not content_type.model_class():\\n            raise Http404(\\n                _(\"Content type %(ct_id)s object has no associated model\") %\\n                {'ct_id': content_type_id}\\n            )\\n        obj = content_type.get_object_for_this_type(pk=object_id)\\n    except (ObjectDoesNotExist, ValueError):\\n        raise Http404(\\n            _(\"Content type %(ct_id)s object %(obj_id)s doesn't exist\") %\\n            {'ct_id': content_type_id, 'obj_id': object_id}\\n        )\\n    try:\\n        get_absolute_url = obj.get_absolute_url\\n    except AttributeError:\\n        raise Http404(\\n            _(\"%(ct_name)s objects don't have a get_absolute_url() method\") %\\n            {'ct_name': content_type.name}\\n        )\\n    absurl = get_absolute_url()\\n    if absurl.startswith(('http://', 'https://', '//')):\\n        return HttpResponseRedirect(absurl)\\n    object_domain = None\\n    if apps.is_installed('django.contrib.sites'):\\n        Site = apps.get_model('sites.Site')\\n        opts = obj._meta\\n        for field in opts.many_to_many:\\n            if field.remote_field.model is Site:\\n                try:\\n                    object_domain = getattr(obj, field.name).all()[0].domain\\n                except IndexError:\\n                    pass\\n                if object_domain is not None:\\n                    break\\n        if object_domain is None:\\n            for field in obj._meta.fields:\\n                if field.remote_field and field.remote_field.model is Site:\\n                    try:\\n                        site = getattr(obj, field.name)\\n                    except Site.DoesNotExist:\\n                        continue\\n                    if site is not None:\\n                        object_domain = site.domain\\n                    if object_domain is not None:\\n                        break\\n        if object_domain is None:\\n            try:\\n                object_domain = Site.objects.get_current(request).domain\\n            except Site.DoesNotExist:\\n                pass\\n    else:\\n        object_domain = RequestSite(request).domain\\n    if object_domain is not None:\\n        protocol = request.scheme\\n        return HttpResponseRedirect('%s://%s%s' % (protocol, object_domain, absurl))\\n    else:\\n        return HttpResponseRedirect(absurl)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #18620 -- Made ContentTypes shortcut view prefer current site if available.\\n\\nThanks Mike Tigas (mtigas) for the initial patch.",
    "fixed_code": "def shortcut(request, content_type_id, object_id):\\n    try:\\n        content_type = ContentType.objects.get(pk=content_type_id)\\n        if not content_type.model_class():\\n            raise Http404(\\n                _(\"Content type %(ct_id)s object has no associated model\") %\\n                {'ct_id': content_type_id}\\n            )\\n        obj = content_type.get_object_for_this_type(pk=object_id)\\n    except (ObjectDoesNotExist, ValueError):\\n        raise Http404(\\n            _(\"Content type %(ct_id)s object %(obj_id)s doesn't exist\") %\\n            {'ct_id': content_type_id, 'obj_id': object_id}\\n        )\\n    try:\\n        get_absolute_url = obj.get_absolute_url\\n    except AttributeError:\\n        raise Http404(\\n            _(\"%(ct_name)s objects don't have a get_absolute_url() method\") %\\n            {'ct_name': content_type.name}\\n        )\\n    absurl = get_absolute_url()\\n    if absurl.startswith(('http://', 'https://', '//')):\\n        return HttpResponseRedirect(absurl)\\n    try:\\n        object_domain = get_current_site(request).domain\\n    except ObjectDoesNotExist:\\n        object_domain = None\\n    if apps.is_installed('django.contrib.sites'):\\n        Site = apps.get_model('sites.Site')\\n        opts = obj._meta\\n        for field in opts.many_to_many:\\n            if field.remote_field.model is Site:\\n                site_qs = getattr(obj, field.name).all()\\n                if object_domain and site_qs.filter(domain=object_domain).exists():\\n                    break\\n                site = site_qs.first()\\n                if site:\\n                    object_domain = site.domain\\n                    break\\n        else:\\n            for field in obj._meta.fields:\\n                if field.remote_field and field.remote_field.model is Site:\\n                    try:\\n                        site = getattr(obj, field.name)\\n                    except Site.DoesNotExist:\\n                        continue\\n                    if site is not None:\\n                        object_domain = site.domain\\n                        break\\n    if object_domain is not None:\\n        protocol = request.scheme\\n        return HttpResponseRedirect('%s://%s%s' % (protocol, object_domain, absurl))\\n    else:\\n        return HttpResponseRedirect(absurl)"
  },
  {
    "code": "def get_srid_info(srid, connection):\\n    global _srid_cache\\n    try:\\n        SpatialRefSys = connection.ops.spatial_ref_sys()\\n    except NotImplementedError:\\n        return None, None, None\\n    if connection.alias not in _srid_cache:\\n        _srid_cache[connection.alias] = {}\\n    if srid not in _srid_cache[connection.alias]:\\n        sr = SpatialRefSys.objects.using(connection.alias).get(srid=srid)\\n        units, units_name = sr.units\\n        spheroid = SpatialRefSys.get_spheroid(sr.wkt)\\n        _srid_cache[connection.alias][srid] = (units, units_name, spheroid)\\n    return _srid_cache[connection.alias][srid]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #27576 -- Made get_srid_info() fallback to GDAL if SpatialRefSys is unavailable.",
    "fixed_code": "def get_srid_info(srid, connection):\\n    global _srid_cache\\n    try:\\n        SpatialRefSys = connection.ops.spatial_ref_sys()\\n    except NotImplementedError:\\n        SpatialRefSys = None\\n    alias, get_srs = (\\n        (connection.alias, lambda srid: SpatialRefSys.objects.using(connection.alias).get(srid=srid).srs)\\n        if SpatialRefSys else\\n        (None, SpatialReference)\\n    )\\n    if srid not in _srid_cache[alias]:\\n        srs = get_srs(srid)\\n        units, units_name = srs.units\\n        sphere_name = srs['spheroid']\\n        spheroid = 'SPHEROID[\"%s\",%s,%s]' % (sphere_name, srs.semi_major, srs.inverse_flattening)\\n        _srid_cache[alias][srid] = (units, units_name, spheroid)\\n    return _srid_cache[alias][srid]"
  },
  {
    "code": "def make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\\n                   repr=True, eq=True, order=False, unsafe_hash=False,\\n                   frozen=False, match_args=True):\\n    if namespace is None:\\n        namespace = {}\\n    else:\\n        namespace = namespace.copy()\\n    seen = set()\\n    anns = {}\\n    for item in fields:\\n        if isinstance(item, str):\\n            name = item\\n            tp = 'typing.Any'\\n        elif len(item) == 2:\\n            name, tp, = item\\n        elif len(item) == 3:\\n            name, tp, spec = item\\n            namespace[name] = spec\\n        else:\\n            raise TypeError(f'Invalid field: {item!r}')\\n        if not isinstance(name, str) or not name.isidentifier():\\n            raise TypeError(f'Field names must be valid identifiers: {name!r}')\\n        if keyword.iskeyword(name):\\n            raise TypeError(f'Field names must not be keywords: {name!r}')\\n        if name in seen:\\n            raise TypeError(f'Field name duplicated: {name!r}')\\n        seen.add(name)\\n        anns[name] = tp\\n    namespace['__annotations__'] = anns\\n    cls = types.new_class(cls_name, bases, {}, lambda ns: ns.update(namespace))\\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\\n                     unsafe_hash=unsafe_hash, frozen=frozen,\\n                     match_args=match_args)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\\n                   repr=True, eq=True, order=False, unsafe_hash=False,\\n                   frozen=False, match_args=True):\\n    if namespace is None:\\n        namespace = {}\\n    else:\\n        namespace = namespace.copy()\\n    seen = set()\\n    anns = {}\\n    for item in fields:\\n        if isinstance(item, str):\\n            name = item\\n            tp = 'typing.Any'\\n        elif len(item) == 2:\\n            name, tp, = item\\n        elif len(item) == 3:\\n            name, tp, spec = item\\n            namespace[name] = spec\\n        else:\\n            raise TypeError(f'Invalid field: {item!r}')\\n        if not isinstance(name, str) or not name.isidentifier():\\n            raise TypeError(f'Field names must be valid identifiers: {name!r}')\\n        if keyword.iskeyword(name):\\n            raise TypeError(f'Field names must not be keywords: {name!r}')\\n        if name in seen:\\n            raise TypeError(f'Field name duplicated: {name!r}')\\n        seen.add(name)\\n        anns[name] = tp\\n    namespace['__annotations__'] = anns\\n    cls = types.new_class(cls_name, bases, {}, lambda ns: ns.update(namespace))\\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\\n                     unsafe_hash=unsafe_hash, frozen=frozen,\\n                     match_args=match_args)"
  },
  {
    "code": "def boxplot(data, column=None, by=None, ax=None, fontsize=None,\\n            rot=0, grid=True, figsize=None, layout=None, return_type=None,\\n            **kwds):\\n    valid_types = (None, 'axes', 'dict', 'both')\\n    if return_type not in valid_types:\\n        raise ValueError(\"return_type\")\\n    from pandas import Series, DataFrame\\n    if isinstance(data, Series):\\n        data = DataFrame({'x': data})\\n        column = 'x'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH/CLN: add BoxPlot class inheriting MPLPlot",
    "fixed_code": "def boxplot(data, column=None, by=None, ax=None, fontsize=None,\\n            rot=0, grid=True, figsize=None, layout=None, return_type=None,\\n            **kwds):\\n    valid_types = (None, 'axes', 'dict', 'both')\\n    if return_type not in BoxPlot._valid_return_types:\\n        raise ValueError(\"return_type must be {None, 'axes', 'dict', 'both'}\")\\n    from pandas import Series, DataFrame\\n    if isinstance(data, Series):\\n        data = DataFrame({'x': data})\\n        column = 'x'"
  },
  {
    "code": "def _set_list(self, length, items):\\n        ndim = self._cs.dims\\n        hasz = self._cs.hasz  \\n        srid = self.srid\\n        cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz)\\n        for i, c in enumerate(items):\\n            cs[i] = c\\n        ptr = self._init_func(cs.ptr)\\n        if ptr:\\n            capi.destroy_geom(self.ptr)\\n            self.ptr = ptr\\n            if srid is not None:\\n                self.srid = srid\\n            self._post_init()\\n        else:\\n            raise GEOSException('Geometry resulting from slice deletion was invalid.')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _set_list(self, length, items):\\n        ndim = self._cs.dims\\n        hasz = self._cs.hasz  \\n        srid = self.srid\\n        cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz)\\n        for i, c in enumerate(items):\\n            cs[i] = c\\n        ptr = self._init_func(cs.ptr)\\n        if ptr:\\n            capi.destroy_geom(self.ptr)\\n            self.ptr = ptr\\n            if srid is not None:\\n                self.srid = srid\\n            self._post_init()\\n        else:\\n            raise GEOSException('Geometry resulting from slice deletion was invalid.')"
  },
  {
    "code": "def send_commenting_notifications(self, changes):\\n\\t\\trelevant_comment_ids = []\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\\n\\t\\tif (not changes['new_comments']\\n\\t\\t\\t\\tand not changes['deleted_comments']\\n\\t\\t\\t\\tand not changes['resolved_comments']\\n\\t\\t\\t\\tand not changes['new_replies']):\\n\\t\\t\\treturn\\n\\t\\tsubscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\\n\\t\\tglobal_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\\n\\t\\treplies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\\n\\t\\tcomments = Comment.objects.filter(id__in=relevant_comment_ids)\\n\\t\\tthread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).filter(\\n\\t\\t\\tQ(comment_replies__comment_id__in=relevant_comment_ids) | Q(**{('%s__pk__in' % COMMENTS_RELATION_NAME): relevant_comment_ids})\\n\\t\\t).prefetch_related(\\n\\t\\t\\tPrefetch('comment_replies', queryset=replies),\\n\\t\\t\\tPrefetch(COMMENTS_RELATION_NAME, queryset=comments)\\n\\t\\t)\\n\\t\\tif not (global_recipient_users or thread_users):\\n\\t\\t\\treturn\\n\\t\\tthread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\\n\\t\\tmailed_users = set()\\n\\t\\tfor current_user, current_threads in thread_users:\\n\\t\\t\\tif current_user in mailed_users:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tusers = [current_user]\\n\\t\\t\\tmailed_users.add(current_user)\\n\\t\\t\\tfor user, threads in thread_users:\\n\\t\\t\\t\\tif user not in mailed_users and threads == current_threads:\\n\\t\\t\\t\\t\\tusers.append(user)\\n\\t\\t\\t\\t\\tmailed_users.add(user)\\n\\t\\t\\tsend_notification(users, 'updated_comments', {\\n\\t\\t\\t\\t'page': self.page,\\n\\t\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t\\t'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'deleted_comments': [],\\n\\t\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t\\t\\tif comment.pk in threads\\n\\t\\t\\t\\t]\\n\\t\\t\\t})\\n\\t\\treturn send_notification(global_recipient_users, 'updated_comments', {\\n\\t\\t\\t'page': self.page,\\n\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t'new_comments': changes['new_comments'],\\n\\t\\t\\t'resolved_comments': changes['resolved_comments'],\\n\\t\\t\\t'deleted_comments': changes['deleted_comments'],\\n\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t]\\n\\t\\t})",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def send_commenting_notifications(self, changes):\\n\\t\\trelevant_comment_ids = []\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\\n\\t\\tif (not changes['new_comments']\\n\\t\\t\\t\\tand not changes['deleted_comments']\\n\\t\\t\\t\\tand not changes['resolved_comments']\\n\\t\\t\\t\\tand not changes['new_replies']):\\n\\t\\t\\treturn\\n\\t\\tsubscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\\n\\t\\tglobal_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\\n\\t\\treplies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\\n\\t\\tcomments = Comment.objects.filter(id__in=relevant_comment_ids)\\n\\t\\tthread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).filter(\\n\\t\\t\\tQ(comment_replies__comment_id__in=relevant_comment_ids) | Q(**{('%s__pk__in' % COMMENTS_RELATION_NAME): relevant_comment_ids})\\n\\t\\t).prefetch_related(\\n\\t\\t\\tPrefetch('comment_replies', queryset=replies),\\n\\t\\t\\tPrefetch(COMMENTS_RELATION_NAME, queryset=comments)\\n\\t\\t)\\n\\t\\tif not (global_recipient_users or thread_users):\\n\\t\\t\\treturn\\n\\t\\tthread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\\n\\t\\tmailed_users = set()\\n\\t\\tfor current_user, current_threads in thread_users:\\n\\t\\t\\tif current_user in mailed_users:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tusers = [current_user]\\n\\t\\t\\tmailed_users.add(current_user)\\n\\t\\t\\tfor user, threads in thread_users:\\n\\t\\t\\t\\tif user not in mailed_users and threads == current_threads:\\n\\t\\t\\t\\t\\tusers.append(user)\\n\\t\\t\\t\\t\\tmailed_users.add(user)\\n\\t\\t\\tsend_notification(users, 'updated_comments', {\\n\\t\\t\\t\\t'page': self.page,\\n\\t\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t\\t'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'deleted_comments': [],\\n\\t\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t\\t\\tif comment.pk in threads\\n\\t\\t\\t\\t]\\n\\t\\t\\t})\\n\\t\\treturn send_notification(global_recipient_users, 'updated_comments', {\\n\\t\\t\\t'page': self.page,\\n\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t'new_comments': changes['new_comments'],\\n\\t\\t\\t'resolved_comments': changes['resolved_comments'],\\n\\t\\t\\t'deleted_comments': changes['deleted_comments'],\\n\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t]\\n\\t\\t})"
  },
  {
    "code": "def as_blocks(self):\\n        self._consolidate_inplace()\\n        bd = {}\\n        for b in self._data.blocks:\\n            bd.setdefault(str(b.dtype), []).append(b)\\n        result = {}\\n        for dtype, blocks in bd.items():\\n            combined = self._data.combine(blocks, copy=True)\\n            result[dtype] = self._constructor(combined).__finalize__(self)\\n        return result\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: allow as_blocks to take a copy argument (#9607)",
    "fixed_code": "def as_blocks(self, copy=True):\\n        self._consolidate_inplace()\\n        bd = {}\\n        for b in self._data.blocks:\\n            bd.setdefault(str(b.dtype), []).append(b)\\n        result = {}\\n        for dtype, blocks in bd.items():\\n            combined = self._data.combine(blocks, copy=copy)\\n            result[dtype] = self._constructor(combined).__finalize__(self)\\n        return result\\n    @property"
  },
  {
    "code": "def parse_qsl(qs, keep_blank_values=0, strict_parsing=0):\\n    name_value_pairs = string.splitfields(qs, '&')\\n    r=[]\\n    for name_value in name_value_pairs:\\n        nv = string.splitfields(name_value, '=', 1)\\n        if len(nv) != 2:\\n            if strict_parsing:\\n                raise ValueError, \"bad query field: %s\" % `name_value`\\n            continue\\n        if len(nv[1]) or keep_blank_values:\\n            name = urllib.unquote(string.replace(nv[0], '+', ' '))\\n            value = urllib.unquote(string.replace(nv[1], '+', ' '))\\n            r.append((name, value))\\n    return r",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_qsl(qs, keep_blank_values=0, strict_parsing=0):\\n    name_value_pairs = string.splitfields(qs, '&')\\n    r=[]\\n    for name_value in name_value_pairs:\\n        nv = string.splitfields(name_value, '=', 1)\\n        if len(nv) != 2:\\n            if strict_parsing:\\n                raise ValueError, \"bad query field: %s\" % `name_value`\\n            continue\\n        if len(nv[1]) or keep_blank_values:\\n            name = urllib.unquote(string.replace(nv[0], '+', ' '))\\n            value = urllib.unquote(string.replace(nv[1], '+', ' '))\\n            r.append((name, value))\\n    return r"
  },
  {
    "code": "def _prep_ndarray(values, copy=True):\\n    if not isinstance(values, np.ndarray):\\n        arr = np.asarray(values)\\n        if issubclass(arr.dtype.type, basestring):\\n            arr = np.array(values, dtype=object, copy=True)\\n        values = arr\\n    else:\\n        values = np.asarray(values)\\n        if copy:\\n            values = values.copy()\\n    if values.ndim == 1:\\n        N = values.shape[0]\\n        if N == 0:\\n            values = values.reshape((values.shape[0], 0))\\n        else:\\n            values = values.reshape((values.shape[0], 1))\\n    elif values.ndim != 2:\\n        raise Exception('Must pass 2-d input')\\n    return values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _prep_ndarray(values, copy=True):\\n    if not isinstance(values, np.ndarray):\\n        arr = np.asarray(values)\\n        if issubclass(arr.dtype.type, basestring):\\n            arr = np.array(values, dtype=object, copy=True)\\n        values = arr\\n    else:\\n        values = np.asarray(values)\\n        if copy:\\n            values = values.copy()\\n    if values.ndim == 1:\\n        N = values.shape[0]\\n        if N == 0:\\n            values = values.reshape((values.shape[0], 0))\\n        else:\\n            values = values.reshape((values.shape[0], 1))\\n    elif values.ndim != 2:\\n        raise Exception('Must pass 2-d input')\\n    return values"
  },
  {
    "code": "def _read_body(self, code, headers, delegate):\\n\\t\\tif \"Content-Length\" in headers:\\n\\t\\t\\tif \"Transfer-Encoding\" in headers:\\n\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\"Response with both Transfer-Encoding and Content-Length\")\\n\\t\\t\\tif \",\" in headers[\"Content-Length\"]:\\n\\t\\t\\t\\tpieces = re.split(r',\\s*', headers[\"Content-Length\"])\\n\\t\\t\\t\\tif any(i != pieces[0] for i in pieces):\\n\\t\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\\t\"Multiple unequal Content-Lengths: %r\" %\\n\\t\\t\\t\\t\\t\\theaders[\"Content-Length\"])\\n\\t\\t\\t\\theaders[\"Content-Length\"] = pieces[0]\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcontent_length = int(headers[\"Content-Length\"])\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\"Only integer Content-Length is allowed: %s\" % headers[\"Content-Length\"])\\n\\t\\t\\tif content_length > self._max_body_size:\\n\\t\\t\\t\\traise httputil.HTTPInputError(\"Content-Length too long\")\\n\\t\\telse:\\n\\t\\t\\tcontent_length = None\\n\\t\\tif code == 204:\\n\\t\\t\\tif (\"Transfer-Encoding\" in headers or\\n\\t\\t\\t\\t\\tcontent_length not in (None, 0)):\\n\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\"Response with code %d should not have body\" % code)\\n\\t\\t\\tcontent_length = 0\\n\\t\\tif content_length is not None:\\n\\t\\t\\treturn self._read_fixed_body(content_length, delegate)\\n\\t\\tif headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\\n\\t\\t\\treturn self._read_chunked_body(delegate)\\n\\t\\tif self.is_client:\\n\\t\\t\\treturn self._read_body_until_close(delegate)\\n\\t\\treturn None\\n\\t@gen.coroutine",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read_body(self, code, headers, delegate):\\n\\t\\tif \"Content-Length\" in headers:\\n\\t\\t\\tif \"Transfer-Encoding\" in headers:\\n\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\"Response with both Transfer-Encoding and Content-Length\")\\n\\t\\t\\tif \",\" in headers[\"Content-Length\"]:\\n\\t\\t\\t\\tpieces = re.split(r',\\s*', headers[\"Content-Length\"])\\n\\t\\t\\t\\tif any(i != pieces[0] for i in pieces):\\n\\t\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\\t\"Multiple unequal Content-Lengths: %r\" %\\n\\t\\t\\t\\t\\t\\theaders[\"Content-Length\"])\\n\\t\\t\\t\\theaders[\"Content-Length\"] = pieces[0]\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcontent_length = int(headers[\"Content-Length\"])\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\"Only integer Content-Length is allowed: %s\" % headers[\"Content-Length\"])\\n\\t\\t\\tif content_length > self._max_body_size:\\n\\t\\t\\t\\traise httputil.HTTPInputError(\"Content-Length too long\")\\n\\t\\telse:\\n\\t\\t\\tcontent_length = None\\n\\t\\tif code == 204:\\n\\t\\t\\tif (\"Transfer-Encoding\" in headers or\\n\\t\\t\\t\\t\\tcontent_length not in (None, 0)):\\n\\t\\t\\t\\traise httputil.HTTPInputError(\\n\\t\\t\\t\\t\\t\"Response with code %d should not have body\" % code)\\n\\t\\t\\tcontent_length = 0\\n\\t\\tif content_length is not None:\\n\\t\\t\\treturn self._read_fixed_body(content_length, delegate)\\n\\t\\tif headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\\n\\t\\t\\treturn self._read_chunked_body(delegate)\\n\\t\\tif self.is_client:\\n\\t\\t\\treturn self._read_body_until_close(delegate)\\n\\t\\treturn None\\n\\t@gen.coroutine"
  },
  {
    "code": "def _update_env_var(self, section, name, new_value):\\n        env_var = self._env_var_name(section, name)\\n        if env_var in os.environ:\\n            os.environ[env_var] = new_value\\n            return\\n        if not self.has_section(section):\\n            self.add_section(section)\\n        self.set(section, name, new_value)\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _update_env_var(self, section, name, new_value):\\n        env_var = self._env_var_name(section, name)\\n        if env_var in os.environ:\\n            os.environ[env_var] = new_value\\n            return\\n        if not self.has_section(section):\\n            self.add_section(section)\\n        self.set(section, name, new_value)\\n    @staticmethod"
  },
  {
    "code": "def changeform_view(self, request, object_id=None, form_url='', extra_context=None):\\n\\t\\tto_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))\\n\\t\\tif to_field and not self.to_field_allowed(request, to_field):\\n\\t\\t\\traise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\\n\\t\\tmodel = self.model\\n\\t\\topts = model._meta\\n\\t\\tif request.method == 'POST' and '_saveasnew' in request.POST:\\n\\t\\t\\tobject_id = None\\n\\t\\tadd = object_id is None\\n\\t\\tif add:\\n\\t\\t\\tif not self.has_add_permission(request):\\n\\t\\t\\t\\traise PermissionDenied\\n\\t\\t\\tobj = None\\n\\t\\telse:\\n\\t\\t\\tobj = self.get_object(request, unquote(object_id), to_field)\\n\\t\\t\\tif not self.has_change_permission(request, obj):\\n\\t\\t\\t\\traise PermissionDenied\\n\\t\\t\\tif obj is None:\\n\\t\\t\\t\\traise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {\\n\\t\\t\\t\\t\\t'name': force_text(opts.verbose_name), 'key': escape(object_id)})\\n\\t\\tModelForm = self.get_form(request, obj)\\n\\t\\tif request.method == 'POST':\\n\\t\\t\\tform = ModelForm(request.POST, request.FILES, instance=obj)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\tform_validated = True\\n\\t\\t\\t\\tnew_object = self.save_form(request, form, change=not add)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tform_validated = False\\n\\t\\t\\t\\tnew_object = form.instance\\n\\t\\t\\tformsets, inline_instances = self._create_formsets(request, new_object, change=not add)\\n\\t\\t\\tif all_valid(formsets) and form_validated:\\n\\t\\t\\t\\tself.save_model(request, new_object, form, not add)\\n\\t\\t\\t\\tself.save_related(request, form, formsets, not add)\\n\\t\\t\\t\\tchange_message = self.construct_change_message(request, form, formsets, add)\\n\\t\\t\\t\\tif add:\\n\\t\\t\\t\\t\\tself.log_addition(request, new_object, change_message)\\n\\t\\t\\t\\t\\treturn self.response_add(request, new_object)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.log_change(request, new_object, change_message)\\n\\t\\t\\t\\t\\treturn self.response_change(request, new_object)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tform_validated = False\\n\\t\\telse:\\n\\t\\t\\tif add:\\n\\t\\t\\t\\tinitial = self.get_changeform_initial_data(request)\\n\\t\\t\\t\\tform = ModelForm(initial=initial)\\n\\t\\t\\t\\tformsets, inline_instances = self._create_formsets(request, form.instance, change=False)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tform = ModelForm(instance=obj)\\n\\t\\t\\t\\tformsets, inline_instances = self._create_formsets(request, obj, change=True)\\n\\t\\tadminForm = helpers.AdminForm(\\n\\t\\t\\tform,\\n\\t\\t\\tlist(self.get_fieldsets(request, obj)),\\n\\t\\t\\tself.get_prepopulated_fields(request, obj),\\n\\t\\t\\tself.get_readonly_fields(request, obj),\\n\\t\\t\\tmodel_admin=self)\\n\\t\\tmedia = self.media + adminForm.media\\n\\t\\tinline_formsets = self.get_inline_formsets(request, formsets, inline_instances, obj)\\n\\t\\tfor inline_formset in inline_formsets:\\n\\t\\t\\tmedia = media + inline_formset.media\\n\\t\\tcontext = dict(self.admin_site.each_context(request),\\n\\t\\t\\ttitle=(_('Add %s') if add else _('Change %s')) % force_text(opts.verbose_name),\\n\\t\\t\\tadminform=adminForm,\\n\\t\\t\\tobject_id=object_id,\\n\\t\\t\\toriginal=obj,\\n\\t\\t\\tis_popup=(IS_POPUP_VAR in request.POST or\\n\\t\\t\\t\\t\\t  IS_POPUP_VAR in request.GET),\\n\\t\\t\\tto_field=to_field,\\n\\t\\t\\tmedia=media,\\n\\t\\t\\tinline_admin_formsets=inline_formsets,\\n\\t\\t\\terrors=helpers.AdminErrorList(form, formsets),\\n\\t\\t\\tpreserved_filters=self.get_preserved_filters(request),\\n\\t\\t)\\n\\t\\tif request.method == 'POST' and not form_validated and \"_saveasnew\" in request.POST:\\n\\t\\t\\tcontext['show_save'] = False\\n\\t\\t\\tcontext['show_save_and_continue'] = False\\n\\t\\t\\tadd = False\\n\\t\\tcontext.update(extra_context or {})\\n\\t\\treturn self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def changeform_view(self, request, object_id=None, form_url='', extra_context=None):\\n\\t\\tto_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))\\n\\t\\tif to_field and not self.to_field_allowed(request, to_field):\\n\\t\\t\\traise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\\n\\t\\tmodel = self.model\\n\\t\\topts = model._meta\\n\\t\\tif request.method == 'POST' and '_saveasnew' in request.POST:\\n\\t\\t\\tobject_id = None\\n\\t\\tadd = object_id is None\\n\\t\\tif add:\\n\\t\\t\\tif not self.has_add_permission(request):\\n\\t\\t\\t\\traise PermissionDenied\\n\\t\\t\\tobj = None\\n\\t\\telse:\\n\\t\\t\\tobj = self.get_object(request, unquote(object_id), to_field)\\n\\t\\t\\tif not self.has_change_permission(request, obj):\\n\\t\\t\\t\\traise PermissionDenied\\n\\t\\t\\tif obj is None:\\n\\t\\t\\t\\traise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {\\n\\t\\t\\t\\t\\t'name': force_text(opts.verbose_name), 'key': escape(object_id)})\\n\\t\\tModelForm = self.get_form(request, obj)\\n\\t\\tif request.method == 'POST':\\n\\t\\t\\tform = ModelForm(request.POST, request.FILES, instance=obj)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\tform_validated = True\\n\\t\\t\\t\\tnew_object = self.save_form(request, form, change=not add)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tform_validated = False\\n\\t\\t\\t\\tnew_object = form.instance\\n\\t\\t\\tformsets, inline_instances = self._create_formsets(request, new_object, change=not add)\\n\\t\\t\\tif all_valid(formsets) and form_validated:\\n\\t\\t\\t\\tself.save_model(request, new_object, form, not add)\\n\\t\\t\\t\\tself.save_related(request, form, formsets, not add)\\n\\t\\t\\t\\tchange_message = self.construct_change_message(request, form, formsets, add)\\n\\t\\t\\t\\tif add:\\n\\t\\t\\t\\t\\tself.log_addition(request, new_object, change_message)\\n\\t\\t\\t\\t\\treturn self.response_add(request, new_object)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.log_change(request, new_object, change_message)\\n\\t\\t\\t\\t\\treturn self.response_change(request, new_object)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tform_validated = False\\n\\t\\telse:\\n\\t\\t\\tif add:\\n\\t\\t\\t\\tinitial = self.get_changeform_initial_data(request)\\n\\t\\t\\t\\tform = ModelForm(initial=initial)\\n\\t\\t\\t\\tformsets, inline_instances = self._create_formsets(request, form.instance, change=False)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tform = ModelForm(instance=obj)\\n\\t\\t\\t\\tformsets, inline_instances = self._create_formsets(request, obj, change=True)\\n\\t\\tadminForm = helpers.AdminForm(\\n\\t\\t\\tform,\\n\\t\\t\\tlist(self.get_fieldsets(request, obj)),\\n\\t\\t\\tself.get_prepopulated_fields(request, obj),\\n\\t\\t\\tself.get_readonly_fields(request, obj),\\n\\t\\t\\tmodel_admin=self)\\n\\t\\tmedia = self.media + adminForm.media\\n\\t\\tinline_formsets = self.get_inline_formsets(request, formsets, inline_instances, obj)\\n\\t\\tfor inline_formset in inline_formsets:\\n\\t\\t\\tmedia = media + inline_formset.media\\n\\t\\tcontext = dict(self.admin_site.each_context(request),\\n\\t\\t\\ttitle=(_('Add %s') if add else _('Change %s')) % force_text(opts.verbose_name),\\n\\t\\t\\tadminform=adminForm,\\n\\t\\t\\tobject_id=object_id,\\n\\t\\t\\toriginal=obj,\\n\\t\\t\\tis_popup=(IS_POPUP_VAR in request.POST or\\n\\t\\t\\t\\t\\t  IS_POPUP_VAR in request.GET),\\n\\t\\t\\tto_field=to_field,\\n\\t\\t\\tmedia=media,\\n\\t\\t\\tinline_admin_formsets=inline_formsets,\\n\\t\\t\\terrors=helpers.AdminErrorList(form, formsets),\\n\\t\\t\\tpreserved_filters=self.get_preserved_filters(request),\\n\\t\\t)\\n\\t\\tif request.method == 'POST' and not form_validated and \"_saveasnew\" in request.POST:\\n\\t\\t\\tcontext['show_save'] = False\\n\\t\\t\\tcontext['show_save_and_continue'] = False\\n\\t\\t\\tadd = False\\n\\t\\tcontext.update(extra_context or {})\\n\\t\\treturn self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)"
  },
  {
    "code": "def get_safe_url(url):\\n\\tvalid_schemes = ['http', 'https', '']\\n\\tvalid_netlocs = [request.host, '']\\n\\tif not url:\\n\\t\\treturn url_for('Airflow.index')\\n\\tparsed = urlparse(url)\\n\\tquery = parse_qsl(parsed.query, keep_blank_values=True)\\n\\turl = parsed._replace(query=urlencode(query)).geturl()\\n\\tif parsed.scheme in valid_schemes and parsed.netloc in valid_netlocs:\\n\\t\\treturn url\\n\\treturn url_for('Airflow.index')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Webserver: Sanitize string passed to origin param (#14738)\\n\\nFollow-up of #12459 & #10334\\n\\nalso removed ';' as query argument separator, we remove query arguments\\nwith semicolons.",
    "fixed_code": "def get_safe_url(url):\\n\\tvalid_schemes = ['http', 'https', '']\\n\\tvalid_netlocs = [request.host, '']\\n\\tif not url:\\n\\t\\treturn url_for('Airflow.index')\\n\\tparsed = urlparse(url)\\n\\tif parsed.netloc == '' and parsed.scheme == '' and ';' in unquote(url):\\n\\t\\treturn url_for('Airflow.index')\\n\\tquery = parse_qsl(parsed.query, keep_blank_values=True)\\n\\tsanitized_query = [query_arg for query_arg in query if ';' not in query_arg[1]]\\n\\turl = parsed._replace(query=urlencode(sanitized_query)).geturl()\\n\\tif parsed.scheme in valid_schemes and parsed.netloc in valid_netlocs:\\n\\t\\treturn url\\n\\treturn url_for('Airflow.index')"
  },
  {
    "code": "def __getattr__(self, name):\\n        file = self.__dict__['file']\\n        a = getattr(file, name)\\n        if type(a) != type(0):\\n            setattr(self, name, a)\\n        return a",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getattr__(self, name):\\n        file = self.__dict__['file']\\n        a = getattr(file, name)\\n        if type(a) != type(0):\\n            setattr(self, name, a)\\n        return a"
  },
  {
    "code": "class _PythonLoopChecker(object):\\n  def __init__(self):\\n\\tself.iterations = 0\\n\\tself.check_inefficient_unroll = WARN_INEFFICIENT_UNROLL\\n\\tself.check_op_count_after_iteration = False",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Add the infinite loop verification to Python for loops, and include a section about the warning in the reference documentation. Addresses #33724. Streamline these loop verifications to reduce their overhead.",
    "fixed_code": "class _PythonLoopChecker(object):\\n  __slots__ = (\\n\\t  'iterations',\\n\\t  'check_inefficient_unroll',\\n\\t  'check_op_count_after_iteration',\\n\\t  'ops_before_iteration',\\n\\t  )\\n  def __init__(self):\\n\\tself.iterations = 1\\n\\tself.check_inefficient_unroll = WARN_INEFFICIENT_UNROLL\\n\\tself.check_op_count_after_iteration = False"
  },
  {
    "code": "def import_helper(filepath):\\n    with open(filepath, 'r') as varfile:\\n        var = varfile.read()\\n    try:\\n        d = json.loads(var)\\n    except Exception:\\n        print(\"Invalid variables file.\")\\n    else:\\n        suc_count = fail_count = 0\\n        for k, v in d.items():\\n            try:\\n                Variable.set(k, v, serialize_json=not isinstance(v, str))\\n            except Exception as e:\\n                print('Variable import failed: {}'.format(repr(e)))\\n                fail_count += 1\\n            else:\\n                suc_count += 1\\n        print(\"{} of {} variables successfully updated.\".format(suc_count, len(d)))\\n        if fail_count:\\n            print(\"{} variable(s) failed to be updated.\".format(fail_count))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def import_helper(filepath):\\n    with open(filepath, 'r') as varfile:\\n        data = varfile.read()\\n    try:\\n        var_json = json.loads(data)\\n    except Exception:  \\n        print(\"Invalid variables file.\")\\n    else:\\n        suc_count = fail_count = 0\\n        for k, v in var_json.items():\\n            try:\\n                Variable.set(k, v, serialize_json=not isinstance(v, str))\\n            except Exception as e:  \\n                print('Variable import failed: {}'.format(repr(e)))\\n                fail_count += 1\\n            else:\\n                suc_count += 1\\n        print(\"{} of {} variables successfully updated.\".format(suc_count, len(var_json)))\\n        if fail_count:\\n            print(\"{} variable(s) failed to be updated.\".format(fail_count))"
  },
  {
    "code": "def is_indexed_slices(grad):\\n\\t\\t\\t\\t\\t\\t\\treturn type(grad).__name__ == 'IndexedSlices'\\n\\t\\t\\t\\t\\t\\tgrads = [\\n\\t\\t\\t\\t\\t\\t\\tgrad.values if is_indexed_slices(grad) else grad\\n\\t\\t\\t\\t\\t\\t\\tfor grad in grads]\\n\\t\\t\\t\\t\\t\\ttf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\\n\\t\\t\\t\\t\\tif self.write_images:\\n\\t\\t\\t\\t\\t\\tw_img = tf.squeeze(weight)\\n\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\tif len(shape) == 2:  \\n\\t\\t\\t\\t\\t\\t\\tif shape[0] > shape[1]:\\n\\t\\t\\t\\t\\t\\t\\t\\tw_img = tf.transpose(w_img)\\n\\t\\t\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\t\\tw_img = tf.reshape(w_img, [1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[0],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[1],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1])\\n\\t\\t\\t\\t\\t\\telif len(shape) == 3:  \\n\\t\\t\\t\\t\\t\\t\\tif K.image_data_format() == 'channels_last':\\n\\t\\t\\t\\t\\t\\t\\t\\tw_img = tf.transpose(w_img, perm=[2, 0, 1])\\n\\t\\t\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\t\\tw_img = tf.reshape(w_img, [shape[0],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[1],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[2],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1])\\n\\t\\t\\t\\t\\t\\telif len(shape) == 1:  \\n\\t\\t\\t\\t\\t\\t\\tw_img = tf.reshape(w_img, [1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[0],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1])\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\tassert len(shape) == 4 and shape[-1] in [1, 3, 4]\\n\\t\\t\\t\\t\\t\\ttf.summary.image(mapped_weight_name, w_img)\\n\\t\\t\\t\\tif hasattr(layer, 'output'):\\n\\t\\t\\t\\t\\ttf.summary.histogram('{}_out'.format(layer.name),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t layer.output)\\n\\t\\tself.merged = tf.summary.merge_all()\\n\\t\\tif self.write_graph:\\n\\t\\t\\tself.writer = tf.summary.FileWriter(self.log_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself.sess.graph)\\n\\t\\telse:\\n\\t\\t\\tself.writer = tf.summary.FileWriter(self.log_dir)\\n\\t\\tif self.embeddings_freq:\\n\\t\\t\\tembeddings_layer_names = self.embeddings_layer_names\\n\\t\\t\\tif not embeddings_layer_names:\\n\\t\\t\\t\\tembeddings_layer_names = [layer.name for layer in self.model.layers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  if type(layer).__name__ == 'Embedding']\\n\\t\\t\\tembeddings = {layer.name: layer.weights[0]\\n\\t\\t\\t\\t\\t\\t  for layer in self.model.layers\\n\\t\\t\\t\\t\\t\\t  if layer.name in embeddings_layer_names}\\n\\t\\t\\tself.saver = tf.train.Saver(list(embeddings.values()))\\n\\t\\t\\tembeddings_metadata = {}\\n\\t\\t\\tif not isinstance(self.embeddings_metadata, str):\\n\\t\\t\\t\\tembeddings_metadata = self.embeddings_metadata\\n\\t\\t\\telse:\\n\\t\\t\\t\\tembeddings_metadata = {layer_name: self.embeddings_metadata\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   for layer_name in embeddings.keys()}\\n\\t\\t\\tconfig = projector.ProjectorConfig()\\n\\t\\t\\tself.embeddings_ckpt_path = os.path.join(self.log_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t 'keras_embedding.ckpt')\\n\\t\\t\\tfor layer_name, tensor in embeddings.items():\\n\\t\\t\\t\\tembedding = config.embeddings.add()\\n\\t\\t\\t\\tembedding.tensor_name = tensor.name\\n\\t\\t\\t\\tif layer_name in embeddings_metadata:\\n\\t\\t\\t\\t\\tembedding.metadata_path = embeddings_metadata[layer_name]\\n\\t\\t\\tprojector.visualize_embeddings(self.writer, config)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix TensorBoard callback with unit test (#10173)",
    "fixed_code": "def is_indexed_slices(grad):\\n\\t\\t\\t\\t\\t\\t\\treturn type(grad).__name__ == 'IndexedSlices'\\n\\t\\t\\t\\t\\t\\tgrads = [\\n\\t\\t\\t\\t\\t\\t\\tgrad.values if is_indexed_slices(grad) else grad\\n\\t\\t\\t\\t\\t\\t\\tfor grad in grads]\\n\\t\\t\\t\\t\\t\\ttf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\\n\\t\\t\\t\\t\\tif self.write_images:\\n\\t\\t\\t\\t\\t\\tw_img = tf.squeeze(weight)\\n\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\tif len(shape) == 2:  \\n\\t\\t\\t\\t\\t\\t\\tif shape[0] > shape[1]:\\n\\t\\t\\t\\t\\t\\t\\t\\tw_img = tf.transpose(w_img)\\n\\t\\t\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\t\\tw_img = tf.reshape(w_img, [1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[0],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[1],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1])\\n\\t\\t\\t\\t\\t\\telif len(shape) == 3:  \\n\\t\\t\\t\\t\\t\\t\\tif K.image_data_format() == 'channels_last':\\n\\t\\t\\t\\t\\t\\t\\t\\tw_img = tf.transpose(w_img, perm=[2, 0, 1])\\n\\t\\t\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\t\\tw_img = tf.reshape(w_img, [shape[0],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[1],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[2],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1])\\n\\t\\t\\t\\t\\t\\telif len(shape) == 1:  \\n\\t\\t\\t\\t\\t\\t\\tw_img = tf.reshape(w_img, [1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   shape[0],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   1])\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\t\\tshape = K.int_shape(w_img)\\n\\t\\t\\t\\t\\t\\tassert len(shape) == 4 and shape[-1] in [1, 3, 4]\\n\\t\\t\\t\\t\\t\\ttf.summary.image(mapped_weight_name, w_img)\\n\\t\\t\\t\\tif hasattr(layer, 'output'):\\n\\t\\t\\t\\t\\tif isinstance(layer.output, list):\\n\\t\\t\\t\\t\\t\\tfor i, output in enumerate(layer.output):\\n\\t\\t\\t\\t\\t\\t\\ttf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttf.summary.histogram('{}_out'.format(layer.name),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t layer.output)\\n\\t\\tself.merged = tf.summary.merge_all()\\n\\t\\tif self.write_graph:\\n\\t\\t\\tself.writer = tf.summary.FileWriter(self.log_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself.sess.graph)\\n\\t\\telse:\\n\\t\\t\\tself.writer = tf.summary.FileWriter(self.log_dir)\\n\\t\\tif self.embeddings_freq:\\n\\t\\t\\tembeddings_layer_names = self.embeddings_layer_names\\n\\t\\t\\tif not embeddings_layer_names:\\n\\t\\t\\t\\tembeddings_layer_names = [layer.name for layer in self.model.layers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  if type(layer).__name__ == 'Embedding']\\n\\t\\t\\tembeddings = {layer.name: layer.weights[0]\\n\\t\\t\\t\\t\\t\\t  for layer in self.model.layers\\n\\t\\t\\t\\t\\t\\t  if layer.name in embeddings_layer_names}\\n\\t\\t\\tself.saver = tf.train.Saver(list(embeddings.values()))\\n\\t\\t\\tembeddings_metadata = {}\\n\\t\\t\\tif not isinstance(self.embeddings_metadata, str):\\n\\t\\t\\t\\tembeddings_metadata = self.embeddings_metadata\\n\\t\\t\\telse:\\n\\t\\t\\t\\tembeddings_metadata = {layer_name: self.embeddings_metadata\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   for layer_name in embeddings.keys()}\\n\\t\\t\\tconfig = projector.ProjectorConfig()\\n\\t\\t\\tself.embeddings_ckpt_path = os.path.join(self.log_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t 'keras_embedding.ckpt')\\n\\t\\t\\tfor layer_name, tensor in embeddings.items():\\n\\t\\t\\t\\tembedding = config.embeddings.add()\\n\\t\\t\\t\\tembedding.tensor_name = tensor.name\\n\\t\\t\\t\\tif layer_name in embeddings_metadata:\\n\\t\\t\\t\\t\\tembedding.metadata_path = embeddings_metadata[layer_name]\\n\\t\\t\\tprojector.visualize_embeddings(self.writer, config)"
  },
  {
    "code": "def _apply_rule(self, dates):\\n        if self.observance is not None:\\n            return map(lambda d: self.observance(d), dates)\\n        if self.offset is not None:\\n            if not isinstance(self.offset, list):\\n                offsets =   [self.offset]\\n            else:\\n                offsets =   self.offset\\n            for offset in offsets:\\n                dates = list(map(lambda d: d + offset, dates))\\n        return dates\\nholiday_calendars = {}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _apply_rule(self, dates):\\n        if self.observance is not None:\\n            return map(lambda d: self.observance(d), dates)\\n        if self.offset is not None:\\n            if not isinstance(self.offset, list):\\n                offsets =   [self.offset]\\n            else:\\n                offsets =   self.offset\\n            for offset in offsets:\\n                dates = list(map(lambda d: d + offset, dates))\\n        return dates\\nholiday_calendars = {}"
  },
  {
    "code": "def _aggregate_series_fast(self, obj: Series, func: F) -> np.ndarray:\\n        raise NotImplementedError(\\n            \"This should not be reached; use _aggregate_series_pure_python\"\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_series_fast(self, obj: Series, func: F) -> np.ndarray:\\n        raise NotImplementedError(\\n            \"This should not be reached; use _aggregate_series_pure_python\"\\n        )"
  },
  {
    "code": "def _check_unroll_limits(self):\\n\\tif LIMIT_PYTHON_ITERATIONS and self.iterations > PYTHON_MAX_ITERATIONS:\\n\\t  raise ValueError('iteration limit exceeded')",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Add the infinite loop verification to Python for loops, and include a section about the warning in the reference documentation. Addresses #33724. Streamline these loop verifications to reduce their overhead.",
    "fixed_code": "def _check_unroll_limits(self):\\n\\tif self.iterations > PYTHON_MAX_ITERATIONS:\\n\\t  raise ValueError('iteration limit exceeded')"
  },
  {
    "code": "def compose_node(self, parent, index):\\n        node = Composer.compose_node(self, parent, index)\\n        if isinstance(node, ScalarNode):\\n            node.__datasource__ = self.name\\n            node.__line__ = self.line\\n            if self.indents:\\n                node.__column__ = self.indent + 1\\n            else:\\n                node.__column__ = self.column +1\\n        elif isinstance(node, MappingNode):\\n            node.__datasource__ = self.name\\n            try:\\n                (cur_line, cur_column) = self.__mapping_starts.pop()\\n            except:\\n                cur_line = None\\n                cur_column = None\\n            node.__line__   = cur_line\\n            node.__column__ = cur_column\\n        return node",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use the node's start_mark to determine line and column.\\n\\n  consistently: Column is the start of the entry's value (so for\\n  strings, the first non-space after the entry beginning, for dicts, the\\n  first character of the first key)",
    "fixed_code": "def compose_node(self, parent, index):\\n        node = Composer.compose_node(self, parent, index)\\n        if isinstance(node, (ScalarNode, MappingNode)):\\n            node.__datasource__ = self.name\\n            node.__line__ = self.line\\n            node.__column__ = node.start_mark.column + 1\\n            node.__line__ = node.start_mark.line + 1\\n        return node"
  },
  {
    "code": "def _bootstrap(*, root=None, upgrade=False, user=False,\\n              altinstall=False, default_pip=False,\\n              verbosity=0):\\n    if altinstall and default_pip:\\n        raise ValueError(\"Cannot use altinstall and default_pip together\")\\n    sys.audit(\"ensurepip.bootstrap\", root)\\n    _disable_pip_configuration_settings()\\n    if altinstall:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"altinstall\"\\n    elif not default_pip:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"install\"\\n    with tempfile.TemporaryDirectory() as tmpdir:\\n        additional_paths = []\\n        for project, version in _PROJECTS:\\n            wheel_name = \"{}-{}-py2.py3-none-any.whl\".format(project, version)\\n            whl = resources.read_binary(\\n                _bundled,\\n                wheel_name,\\n            )\\n            with open(os.path.join(tmpdir, wheel_name), \"wb\") as fp:\\n                fp.write(whl)\\n            additional_paths.append(os.path.join(tmpdir, wheel_name))\\n        args = [\"install\", \"--no-cache-dir\", \"--no-index\", \"--find-links\", tmpdir]\\n        if root:\\n            args += [\"--root\", root]\\n        if upgrade:\\n            args += [\"--upgrade\"]\\n        if user:\\n            args += [\"--user\"]\\n        if verbosity:\\n            args += [\"-\" + \"v\" * verbosity]\\n        return _run_pip(args + [p[0] for p in _PROJECTS], additional_paths)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _bootstrap(*, root=None, upgrade=False, user=False,\\n              altinstall=False, default_pip=False,\\n              verbosity=0):\\n    if altinstall and default_pip:\\n        raise ValueError(\"Cannot use altinstall and default_pip together\")\\n    sys.audit(\"ensurepip.bootstrap\", root)\\n    _disable_pip_configuration_settings()\\n    if altinstall:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"altinstall\"\\n    elif not default_pip:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"install\"\\n    with tempfile.TemporaryDirectory() as tmpdir:\\n        additional_paths = []\\n        for project, version in _PROJECTS:\\n            wheel_name = \"{}-{}-py2.py3-none-any.whl\".format(project, version)\\n            whl = resources.read_binary(\\n                _bundled,\\n                wheel_name,\\n            )\\n            with open(os.path.join(tmpdir, wheel_name), \"wb\") as fp:\\n                fp.write(whl)\\n            additional_paths.append(os.path.join(tmpdir, wheel_name))\\n        args = [\"install\", \"--no-cache-dir\", \"--no-index\", \"--find-links\", tmpdir]\\n        if root:\\n            args += [\"--root\", root]\\n        if upgrade:\\n            args += [\"--upgrade\"]\\n        if user:\\n            args += [\"--user\"]\\n        if verbosity:\\n            args += [\"-\" + \"v\" * verbosity]\\n        return _run_pip(args + [p[0] for p in _PROJECTS], additional_paths)"
  },
  {
    "code": "def _generate_cache_key(request, headerlist, key_prefix):\\n    ctx = md5_constructor()\\n    for header in headerlist:\\n        value = request.META.get(header, None)\\n        if value is not None:\\n            ctx.update(value)\\n    return 'views.decorators.cache.cache_page.%s.%s.%s' % (\\n               key_prefix, iri_to_uri(request.path), ctx.hexdigest())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #10016: the cache middleware no longer vomits when handed long URLs. Thanks, Matt Croydon.",
    "fixed_code": "def _generate_cache_key(request, headerlist, key_prefix):\\n    ctx = md5_constructor()\\n    for header in headerlist:\\n        value = request.META.get(header, None)\\n        if value is not None:\\n            ctx.update(value)\\n    path = md5_constructor(iri_to_uri(request.path))\\n    return 'views.decorators.cache.cache_page.%s.%s.%s' % (\\n               key_prefix, path.hexdigest(), ctx.hexdigest())"
  },
  {
    "code": "def fingerprint(logcan, sendcan, num_pandas):\\n  fixed_fingerprint = os.environ.get('FINGERPRINT', \"\")\\n  skip_fw_query = os.environ.get('SKIP_FW_QUERY', False)\\n  ecu_rx_addrs = set()\\n  if not skip_fw_query:\\n    bus = 1\\n    cached_params = Params().get(\"CarParamsCache\")\\n    if cached_params is not None:\\n      cached_params = car.CarParams.from_bytes(cached_params)\\n      if cached_params.carName == \"mock\":\\n        cached_params = None\\n    if cached_params is not None and len(cached_params.carFw) > 0 and cached_params.carVin is not VIN_UNKNOWN:\\n      cloudlog.warning(\"Using cached CarParams\")\\n      vin, vin_rx_addr = cached_params.carVin, 0\\n      car_fw = list(cached_params.carFw)\\n      cached = True\\n    else:\\n      cloudlog.warning(\"Getting VIN & FW versions\")\\n      vin_rx_addr, vin = get_vin(logcan, sendcan, bus)\\n      ecu_rx_addrs = get_present_ecus(logcan, sendcan)\\n      car_fw = get_fw_versions_ordered(logcan, sendcan, ecu_rx_addrs, num_pandas=num_pandas)\\n      cached = False\\n    exact_fw_match, fw_candidates = match_fw_to_car(car_fw)\\n  else:\\n    vin, vin_rx_addr = VIN_UNKNOWN, 0\\n    exact_fw_match, fw_candidates, car_fw = True, set(), []\\n    cached = False\\n  if not is_valid_vin(vin):\\n    cloudlog.event(\"Malformed VIN\", vin=vin, error=True)\\n    vin = VIN_UNKNOWN\\n  cloudlog.warning(\"VIN %s\", vin)\\n  Params().put(\"CarVin\", vin)\\n  finger = gen_empty_fingerprint()\\n  candidate_cars = {i: all_legacy_fingerprint_cars() for i in [0, 1]}  \\n  frame = 0\\n  frame_fingerprint = 100  \\n  car_fingerprint = None\\n  done = False\\n  messaging.drain_sock_raw(logcan)\\n  while not done:\\n    a = get_one_can(logcan)\\n    for can in a.can:\\n      if can.src < 128:\\n        if can.src not in finger:\\n          finger[can.src] = {}\\n        finger[can.src][can.address] = len(can.dat)\\n      for b in candidate_cars:\\n        if can.src == b and can.address < 0x800 and can.address not in (0x7df, 0x7e0, 0x7e8):\\n          candidate_cars[b] = eliminate_incompatible_cars(can, candidate_cars[b])\\n    for b in candidate_cars:\\n      if len(candidate_cars[b]) == 1 and frame > frame_fingerprint:\\n        car_fingerprint = candidate_cars[b][0]\\n    failed = (all(len(cc) == 0 for cc in candidate_cars.values()) and frame > frame_fingerprint) or frame > 200\\n    succeeded = car_fingerprint is not None\\n    done = failed or succeeded\\n    frame += 1\\n  exact_match = True\\n  source = car.CarParams.FingerprintSource.can\\n  if len(fw_candidates) == 1:\\n    car_fingerprint = list(fw_candidates)[0]\\n    source = car.CarParams.FingerprintSource.fw\\n    exact_match = exact_fw_match\\n  if fixed_fingerprint:\\n    car_fingerprint = fixed_fingerprint\\n    source = car.CarParams.FingerprintSource.fixed\\n  cloudlog.event(\"fingerprinted\", car_fingerprint=car_fingerprint, source=source, fuzzy=not exact_match, cached=cached,\\n                 fw_count=len(car_fw), ecu_responses=list(ecu_rx_addrs), vin_rx_addr=vin_rx_addr, error=True)\\n  return car_fingerprint, finger, vin, car_fw, source, exact_match",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fingerprint(logcan, sendcan, num_pandas):\\n  fixed_fingerprint = os.environ.get('FINGERPRINT', \"\")\\n  skip_fw_query = os.environ.get('SKIP_FW_QUERY', False)\\n  ecu_rx_addrs = set()\\n  if not skip_fw_query:\\n    bus = 1\\n    cached_params = Params().get(\"CarParamsCache\")\\n    if cached_params is not None:\\n      cached_params = car.CarParams.from_bytes(cached_params)\\n      if cached_params.carName == \"mock\":\\n        cached_params = None\\n    if cached_params is not None and len(cached_params.carFw) > 0 and cached_params.carVin is not VIN_UNKNOWN:\\n      cloudlog.warning(\"Using cached CarParams\")\\n      vin, vin_rx_addr = cached_params.carVin, 0\\n      car_fw = list(cached_params.carFw)\\n      cached = True\\n    else:\\n      cloudlog.warning(\"Getting VIN & FW versions\")\\n      vin_rx_addr, vin = get_vin(logcan, sendcan, bus)\\n      ecu_rx_addrs = get_present_ecus(logcan, sendcan)\\n      car_fw = get_fw_versions_ordered(logcan, sendcan, ecu_rx_addrs, num_pandas=num_pandas)\\n      cached = False\\n    exact_fw_match, fw_candidates = match_fw_to_car(car_fw)\\n  else:\\n    vin, vin_rx_addr = VIN_UNKNOWN, 0\\n    exact_fw_match, fw_candidates, car_fw = True, set(), []\\n    cached = False\\n  if not is_valid_vin(vin):\\n    cloudlog.event(\"Malformed VIN\", vin=vin, error=True)\\n    vin = VIN_UNKNOWN\\n  cloudlog.warning(\"VIN %s\", vin)\\n  Params().put(\"CarVin\", vin)\\n  finger = gen_empty_fingerprint()\\n  candidate_cars = {i: all_legacy_fingerprint_cars() for i in [0, 1]}  \\n  frame = 0\\n  frame_fingerprint = 100  \\n  car_fingerprint = None\\n  done = False\\n  messaging.drain_sock_raw(logcan)\\n  while not done:\\n    a = get_one_can(logcan)\\n    for can in a.can:\\n      if can.src < 128:\\n        if can.src not in finger:\\n          finger[can.src] = {}\\n        finger[can.src][can.address] = len(can.dat)\\n      for b in candidate_cars:\\n        if can.src == b and can.address < 0x800 and can.address not in (0x7df, 0x7e0, 0x7e8):\\n          candidate_cars[b] = eliminate_incompatible_cars(can, candidate_cars[b])\\n    for b in candidate_cars:\\n      if len(candidate_cars[b]) == 1 and frame > frame_fingerprint:\\n        car_fingerprint = candidate_cars[b][0]\\n    failed = (all(len(cc) == 0 for cc in candidate_cars.values()) and frame > frame_fingerprint) or frame > 200\\n    succeeded = car_fingerprint is not None\\n    done = failed or succeeded\\n    frame += 1\\n  exact_match = True\\n  source = car.CarParams.FingerprintSource.can\\n  if len(fw_candidates) == 1:\\n    car_fingerprint = list(fw_candidates)[0]\\n    source = car.CarParams.FingerprintSource.fw\\n    exact_match = exact_fw_match\\n  if fixed_fingerprint:\\n    car_fingerprint = fixed_fingerprint\\n    source = car.CarParams.FingerprintSource.fixed\\n  cloudlog.event(\"fingerprinted\", car_fingerprint=car_fingerprint, source=source, fuzzy=not exact_match, cached=cached,\\n                 fw_count=len(car_fw), ecu_responses=list(ecu_rx_addrs), vin_rx_addr=vin_rx_addr, error=True)\\n  return car_fingerprint, finger, vin, car_fw, source, exact_match"
  },
  {
    "code": "def include_router(\\n\\t\\tself,\\n\\t\\trouter: \"APIRouter\",\\n\\t\\t*,\\n\\t\\tprefix: str = \"\",\\n\\t\\ttags: List[str] = None,\\n\\t\\tresponses: Dict[Union[int, str], Dict[str, Any]] = None,\\n\\t) -> None:\\n\\t\\tif prefix:\\n\\t\\t\\tassert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\\n\\t\\t\\tassert not prefix.endswith(\\n\\t\\t\\t\\t\"/\"\\n\\t\\t\\t), \"A path prefix must not end with '/', as the routes will start with '/'\"\\n\\t\\tif responses is None:\\n\\t\\t\\tresponses = {}\\n\\t\\tfor route in router.routes:\\n\\t\\t\\tif isinstance(route, APIRoute):\\n\\t\\t\\t\\tcombined_responses = {**responses, **route.responses}\\n\\t\\t\\t\\tself.add_api_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tresponse_model=route.response_model,\\n\\t\\t\\t\\t\\tstatus_code=route.status_code,\\n\\t\\t\\t\\t\\ttags=(route.tags or []) + (tags or []),\\n\\t\\t\\t\\t\\tsummary=route.summary,\\n\\t\\t\\t\\t\\tdescription=route.description,\\n\\t\\t\\t\\t\\tresponse_description=route.response_description,\\n\\t\\t\\t\\t\\tresponses=combined_responses,\\n\\t\\t\\t\\t\\tdeprecated=route.deprecated,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\toperation_id=route.operation_id,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tcontent_type=route.content_type,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.Route):\\n\\t\\t\\t\\tself.add_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.WebSocketRoute):\\n\\t\\t\\t\\tself.add_websocket_route(\\n\\t\\t\\t\\t\\tprefix + route.path, route.endpoint, name=route.name\\n\\t\\t\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def include_router(\\n\\t\\tself,\\n\\t\\trouter: \"APIRouter\",\\n\\t\\t*,\\n\\t\\tprefix: str = \"\",\\n\\t\\ttags: List[str] = None,\\n\\t\\tresponses: Dict[Union[int, str], Dict[str, Any]] = None,\\n\\t) -> None:\\n\\t\\tif prefix:\\n\\t\\t\\tassert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\\n\\t\\t\\tassert not prefix.endswith(\\n\\t\\t\\t\\t\"/\"\\n\\t\\t\\t), \"A path prefix must not end with '/', as the routes will start with '/'\"\\n\\t\\tif responses is None:\\n\\t\\t\\tresponses = {}\\n\\t\\tfor route in router.routes:\\n\\t\\t\\tif isinstance(route, APIRoute):\\n\\t\\t\\t\\tcombined_responses = {**responses, **route.responses}\\n\\t\\t\\t\\tself.add_api_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tresponse_model=route.response_model,\\n\\t\\t\\t\\t\\tstatus_code=route.status_code,\\n\\t\\t\\t\\t\\ttags=(route.tags or []) + (tags or []),\\n\\t\\t\\t\\t\\tsummary=route.summary,\\n\\t\\t\\t\\t\\tdescription=route.description,\\n\\t\\t\\t\\t\\tresponse_description=route.response_description,\\n\\t\\t\\t\\t\\tresponses=combined_responses,\\n\\t\\t\\t\\t\\tdeprecated=route.deprecated,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\toperation_id=route.operation_id,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tcontent_type=route.content_type,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.Route):\\n\\t\\t\\t\\tself.add_route(\\n\\t\\t\\t\\t\\tprefix + route.path,\\n\\t\\t\\t\\t\\troute.endpoint,\\n\\t\\t\\t\\t\\tmethods=route.methods,\\n\\t\\t\\t\\t\\tinclude_in_schema=route.include_in_schema,\\n\\t\\t\\t\\t\\tname=route.name,\\n\\t\\t\\t\\t)\\n\\t\\t\\telif isinstance(route, routing.WebSocketRoute):\\n\\t\\t\\t\\tself.add_websocket_route(\\n\\t\\t\\t\\t\\tprefix + route.path, route.endpoint, name=route.name\\n\\t\\t\\t\\t)"
  },
  {
    "code": "def mktype(typecode, modulename=None):\\n    if modulename:\\n        module = __import__(modulename)\\n        codenamemapper = module._classdeclarations\\n        classtype = codenamemapper.get(b2i(typecode), None)\\n        if classtype:\\n            return classtype\\n    return aetypes.mktype(typecode)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mktype(typecode, modulename=None):\\n    if modulename:\\n        module = __import__(modulename)\\n        codenamemapper = module._classdeclarations\\n        classtype = codenamemapper.get(b2i(typecode), None)\\n        if classtype:\\n            return classtype\\n    return aetypes.mktype(typecode)"
  },
  {
    "code": "def __init__(self, file=None, name=None, content_type=None, size=None, charset=None):\\n        super(UploadedFile, self).__init__(file, name)\\n        self.size = size\\n        self.content_type = content_type\\n        self.charset = charset",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #13721 -- Added UploadedFile.content_type_extra.\\n\\nThanks Waldemar Kornewald and mvschaik for work on the patch.",
    "fixed_code": "def __init__(self, file=None, name=None, content_type=None, size=None, charset=None, content_type_extra=None):\\n        super(UploadedFile, self).__init__(file, name)\\n        self.size = size\\n        self.content_type = content_type\\n        self.charset = charset\\n        self.content_type_extra = content_type_extra"
  },
  {
    "code": "def decode(input, output, header = 0):\\n    if a2b_qp is not None:\\n        data = input.read()\\n        odata = a2b_qp(data, header = header)\\n        output.write(odata)\\n        return\\n    new = b''\\n    while 1:\\n        line = input.readline()\\n        if not line: break\\n        i, n = 0, len(line)\\n        if n > 0 and line[n-1:n] == b'\\n':\\n            partial = 0; n = n-1\\n            while n > 0 and line[n-1:n] in b\" \\t\\r\":\\n                n = n-1\\n        else:\\n            partial = 1\\n        while i < n:\\n            c = line[i:i+1]\\n            if c == b'_' and header:\\n                new = new + b' '; i = i+1\\n            elif c != ESCAPE:\\n                new = new + c; i = i+1\\n            elif i+1 == n and not partial:\\n                partial = 1; break\\n            elif i+1 < n and line[i+1] == ESCAPE:\\n                new = new + ESCAPE; i = i+2\\n            elif i+2 < n and ishex(line[i+1:i+2]) and ishex(line[i+2:i+3]):\\n                new = new + bytes((unhex(line[i+1:i+3]),)); i = i+3\\n            else: \\n                new = new + c; i = i+1\\n        if not partial:\\n            output.write(new + b'\\n')\\n            new = b''\\n    if new:\\n        output.write(new)\\ndef decodestring(s, header = 0):\\n    if a2b_qp is not None:\\n        return a2b_qp(s, header = header)\\n    from io import BytesIO\\n    infp = BytesIO(s)\\n    outfp = BytesIO()\\n    decode(infp, outfp, header = header)\\n    return outfp.getvalue()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decode(input, output, header = 0):\\n    if a2b_qp is not None:\\n        data = input.read()\\n        odata = a2b_qp(data, header = header)\\n        output.write(odata)\\n        return\\n    new = b''\\n    while 1:\\n        line = input.readline()\\n        if not line: break\\n        i, n = 0, len(line)\\n        if n > 0 and line[n-1:n] == b'\\n':\\n            partial = 0; n = n-1\\n            while n > 0 and line[n-1:n] in b\" \\t\\r\":\\n                n = n-1\\n        else:\\n            partial = 1\\n        while i < n:\\n            c = line[i:i+1]\\n            if c == b'_' and header:\\n                new = new + b' '; i = i+1\\n            elif c != ESCAPE:\\n                new = new + c; i = i+1\\n            elif i+1 == n and not partial:\\n                partial = 1; break\\n            elif i+1 < n and line[i+1] == ESCAPE:\\n                new = new + ESCAPE; i = i+2\\n            elif i+2 < n and ishex(line[i+1:i+2]) and ishex(line[i+2:i+3]):\\n                new = new + bytes((unhex(line[i+1:i+3]),)); i = i+3\\n            else: \\n                new = new + c; i = i+1\\n        if not partial:\\n            output.write(new + b'\\n')\\n            new = b''\\n    if new:\\n        output.write(new)\\ndef decodestring(s, header = 0):\\n    if a2b_qp is not None:\\n        return a2b_qp(s, header = header)\\n    from io import BytesIO\\n    infp = BytesIO(s)\\n    outfp = BytesIO()\\n    decode(infp, outfp, header = header)\\n    return outfp.getvalue()"
  },
  {
    "code": "def run_grant_dataset_view_access(self,\\n                                      source_dataset,\\n                                      view_dataset,\\n                                      view_table,\\n                                      source_project=None,\\n                                      view_project=None):\\n        source_project = source_project if source_project else self.project_id\\n        view_project = view_project if view_project else self.project_id\\n        source_dataset_resource = self.service.datasets().get(\\n            projectId=source_project, datasetId=source_dataset).execute()\\n        access = source_dataset_resource[\\n            'access'] if 'access' in source_dataset_resource else []\\n        view_access = {\\n            'view': {\\n                'projectId': view_project,\\n                'datasetId': view_dataset,\\n                'tableId': view_table\\n            }\\n        }\\n        if view_access not in access:\\n            self.log.info(\\n                'Granting table %s:%s.%s authorized view access to %s:%s dataset.',\\n                view_project, view_dataset, view_table, source_project,\\n                source_dataset)\\n            access.append(view_access)\\n            return self.service.datasets().patch(\\n                projectId=source_project,\\n                datasetId=source_dataset,\\n                body={\\n                    'access': access\\n                }).execute()\\n        else:\\n            self.log.info(\\n                'Table %s:%s.%s already has authorized view access to %s:%s dataset.',\\n                view_project, view_dataset, view_table, source_project, source_dataset)\\n            return source_dataset_resource",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4335] Add default num_retries to GCP connection (#5117)\\n\\nAdd default num_retries to GCP connection",
    "fixed_code": "def run_grant_dataset_view_access(self,\\n                                      source_dataset,\\n                                      view_dataset,\\n                                      view_table,\\n                                      source_project=None,\\n                                      view_project=None):\\n        source_project = source_project if source_project else self.project_id\\n        view_project = view_project if view_project else self.project_id\\n        source_dataset_resource = self.service.datasets().get(\\n            projectId=source_project, datasetId=source_dataset).execute(num_retries=self.num_retries)\\n        access = source_dataset_resource[\\n            'access'] if 'access' in source_dataset_resource else []\\n        view_access = {\\n            'view': {\\n                'projectId': view_project,\\n                'datasetId': view_dataset,\\n                'tableId': view_table\\n            }\\n        }\\n        if view_access not in access:\\n            self.log.info(\\n                'Granting table %s:%s.%s authorized view access to %s:%s dataset.',\\n                view_project, view_dataset, view_table, source_project,\\n                source_dataset)\\n            access.append(view_access)\\n            return self.service.datasets().patch(\\n                projectId=source_project,\\n                datasetId=source_dataset,\\n                body={\\n                    'access': access\\n                }).execute(num_retries=self.num_retries)\\n        else:\\n            self.log.info(\\n                'Table %s:%s.%s already has authorized view access to %s:%s dataset.',\\n                view_project, view_dataset, view_table, source_project, source_dataset)\\n            return source_dataset_resource"
  },
  {
    "code": "def copy(self):\\n        return make_block(self.values.copy(), self.columns,\\n                          self.ref_columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy(self):\\n        return make_block(self.values.copy(), self.columns,\\n                          self.ref_columns)"
  },
  {
    "code": "def _maybe_rename_join(self, other, lsuffix, rsuffix, exclude=None,\\n                           copydata=True):\\n        to_rename = self.items.intersection(other.items)\\n        if exclude is not None:\\n            to_rename = to_rename - exclude\\n        if len(to_rename) > 0:\\n            if not lsuffix and not rsuffix:\\n                raise Exception('columns overlap: %s' % to_rename)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_rename_join(self, other, lsuffix, rsuffix, exclude=None,\\n                           copydata=True):\\n        to_rename = self.items.intersection(other.items)\\n        if exclude is not None:\\n            to_rename = to_rename - exclude\\n        if len(to_rename) > 0:\\n            if not lsuffix and not rsuffix:\\n                raise Exception('columns overlap: %s' % to_rename)"
  },
  {
    "code": "def is_nested_tuple(tup, labels):\\n    if not isinstance(tup, tuple):\\n        return False\\n    for i, k in enumerate(tup):\\n        if is_list_like(k) or isinstance(k, slice):\\n            return isinstance(labels, MultiIndex)\\n    return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_nested_tuple(tup, labels):\\n    if not isinstance(tup, tuple):\\n        return False\\n    for i, k in enumerate(tup):\\n        if is_list_like(k) or isinstance(k, slice):\\n            return isinstance(labels, MultiIndex)\\n    return False"
  },
  {
    "code": "def compare(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if (self._is_special or other and other._is_special):\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n        return Decimal(self.__cmp__(other))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def compare(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if (self._is_special or other and other._is_special):\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n        return Decimal(self.__cmp__(other))"
  },
  {
    "code": "def andrews_curves(frame, class_column, ax=None, samples=200, color=None,\\n                   colormap=None, **kwds):\\n    from math import sqrt, pi, sin, cos\\n    import matplotlib.pyplot as plt",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def andrews_curves(frame, class_column, ax=None, samples=200, color=None,\\n                   colormap=None, **kwds):\\n    from math import sqrt, pi, sin, cos\\n    import matplotlib.pyplot as plt"
  },
  {
    "code": "def main(argv=None):\\n    import getopt\\n    if argv is None:\\n        argv = sys.argv\\n    try:\\n        opts, prog_argv = getopt.getopt(argv[1:], \"tcrRf:d:msC:lTg\",\\n                                        [\"help\", \"version\", \"trace\", \"count\",\\n                                         \"report\", \"no-report\", \"summary\",\\n                                         \"file=\", \"missing\",\\n                                         \"ignore-module=\", \"ignore-dir=\",\\n                                         \"coverdir=\", \"listfuncs\",\\n                                         \"trackcalls\", \"timing\"])\\n    except getopt.error as msg:\\n        sys.stderr.write(\"%s: %s\\n\" % (sys.argv[0], msg))\\n        sys.stderr.write(\"Try `%s --help' for more information\\n\"\\n                         % sys.argv[0])\\n        sys.exit(1)\\n    trace = 0\\n    count = 0\\n    report = 0\\n    no_report = 0\\n    counts_file = None\\n    missing = 0\\n    ignore_modules = []\\n    ignore_dirs = []\\n    coverdir = None\\n    summary = 0\\n    listfuncs = False\\n    countcallers = False\\n    timing = False\\n    for opt, val in opts:\\n        if opt == \"--help\":\\n            usage(sys.stdout)\\n            sys.exit(0)\\n        if opt == \"--version\":\\n            sys.stdout.write(\"trace 2.0\\n\")\\n            sys.exit(0)\\n        if opt == \"-T\" or opt == \"--trackcalls\":\\n            countcallers = True\\n            continue\\n        if opt == \"-l\" or opt == \"--listfuncs\":\\n            listfuncs = True\\n            continue\\n        if opt == \"-g\" or opt == \"--timing\":\\n            timing = True\\n            continue\\n        if opt == \"-t\" or opt == \"--trace\":\\n            trace = 1\\n            continue\\n        if opt == \"-c\" or opt == \"--count\":\\n            count = 1\\n            continue\\n        if opt == \"-r\" or opt == \"--report\":\\n            report = 1\\n            continue\\n        if opt == \"-R\" or opt == \"--no-report\":\\n            no_report = 1\\n            continue\\n        if opt == \"-f\" or opt == \"--file\":\\n            counts_file = val\\n            continue\\n        if opt == \"-m\" or opt == \"--missing\":\\n            missing = 1\\n            continue\\n        if opt == \"-C\" or opt == \"--coverdir\":\\n            coverdir = val\\n            continue\\n        if opt == \"-s\" or opt == \"--summary\":\\n            summary = 1\\n            continue\\n        if opt == \"--ignore-module\":\\n            for mod in val.split(\",\"):\\n                ignore_modules.append(mod.strip())\\n            continue\\n        if opt == \"--ignore-dir\":\\n            for s in val.split(os.pathsep):\\n                s = os.path.expandvars(s)\\n                s = s.replace(\"$prefix\",\\n                              os.path.join(sys.prefix, \"lib\",\\n                                           \"python\" + sys.version[:3]))\\n                s = s.replace(\"$exec_prefix\",\\n                              os.path.join(sys.exec_prefix, \"lib\",\\n                                           \"python\" + sys.version[:3]))\\n                s = os.path.normpath(s)\\n                ignore_dirs.append(s)\\n            continue\\n        assert 0, \"Should never get here\"\\n    if listfuncs and (count or trace):\\n        _err_exit(\"cannot specify both --listfuncs and (--trace or --count)\")\\n    if not (count or trace or report or listfuncs or countcallers):\\n        _err_exit(\"must specify one of --trace, --count, --report, \"\\n                  \"--listfuncs, or --trackcalls\")\\n    if report and no_report:\\n        _err_exit(\"cannot specify both --report and --no-report\")\\n    if report and not counts_file:\\n        _err_exit(\"--report requires a --file\")\\n    if no_report and len(prog_argv) == 0:\\n        _err_exit(\"missing name of file to run\")\\n    if report:\\n        results = CoverageResults(infile=counts_file, outfile=counts_file)\\n        results.write_results(missing, summary=summary, coverdir=coverdir)\\n    else:\\n        sys.argv = prog_argv\\n        progname = prog_argv[0]\\n        sys.path[0] = os.path.split(progname)[0]\\n        t = Trace(count, trace, countfuncs=listfuncs,\\n                  countcallers=countcallers, ignoremods=ignore_modules,\\n                  ignoredirs=ignore_dirs, infile=counts_file,\\n                  outfile=counts_file, timing=timing)\\n        try:\\n            fp = open(progname)\\n            try:\\n                script = fp.read()\\n            finally:\\n                fp.close()\\n            t.run('exec(%r)' % (script,))\\n        except IOError as err:\\n            _err_exit(\"Cannot run file %r because: %s\" % (sys.argv[0], err))\\n        except SystemExit:\\n            pass\\n        results = t.results()\\n        if not no_report:\\n            results.write_results(missing, summary=summary, coverdir=coverdir)\\nif __name__=='__main__':\\n    main()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main(argv=None):\\n    import getopt\\n    if argv is None:\\n        argv = sys.argv\\n    try:\\n        opts, prog_argv = getopt.getopt(argv[1:], \"tcrRf:d:msC:lTg\",\\n                                        [\"help\", \"version\", \"trace\", \"count\",\\n                                         \"report\", \"no-report\", \"summary\",\\n                                         \"file=\", \"missing\",\\n                                         \"ignore-module=\", \"ignore-dir=\",\\n                                         \"coverdir=\", \"listfuncs\",\\n                                         \"trackcalls\", \"timing\"])\\n    except getopt.error as msg:\\n        sys.stderr.write(\"%s: %s\\n\" % (sys.argv[0], msg))\\n        sys.stderr.write(\"Try `%s --help' for more information\\n\"\\n                         % sys.argv[0])\\n        sys.exit(1)\\n    trace = 0\\n    count = 0\\n    report = 0\\n    no_report = 0\\n    counts_file = None\\n    missing = 0\\n    ignore_modules = []\\n    ignore_dirs = []\\n    coverdir = None\\n    summary = 0\\n    listfuncs = False\\n    countcallers = False\\n    timing = False\\n    for opt, val in opts:\\n        if opt == \"--help\":\\n            usage(sys.stdout)\\n            sys.exit(0)\\n        if opt == \"--version\":\\n            sys.stdout.write(\"trace 2.0\\n\")\\n            sys.exit(0)\\n        if opt == \"-T\" or opt == \"--trackcalls\":\\n            countcallers = True\\n            continue\\n        if opt == \"-l\" or opt == \"--listfuncs\":\\n            listfuncs = True\\n            continue\\n        if opt == \"-g\" or opt == \"--timing\":\\n            timing = True\\n            continue\\n        if opt == \"-t\" or opt == \"--trace\":\\n            trace = 1\\n            continue\\n        if opt == \"-c\" or opt == \"--count\":\\n            count = 1\\n            continue\\n        if opt == \"-r\" or opt == \"--report\":\\n            report = 1\\n            continue\\n        if opt == \"-R\" or opt == \"--no-report\":\\n            no_report = 1\\n            continue\\n        if opt == \"-f\" or opt == \"--file\":\\n            counts_file = val\\n            continue\\n        if opt == \"-m\" or opt == \"--missing\":\\n            missing = 1\\n            continue\\n        if opt == \"-C\" or opt == \"--coverdir\":\\n            coverdir = val\\n            continue\\n        if opt == \"-s\" or opt == \"--summary\":\\n            summary = 1\\n            continue\\n        if opt == \"--ignore-module\":\\n            for mod in val.split(\",\"):\\n                ignore_modules.append(mod.strip())\\n            continue\\n        if opt == \"--ignore-dir\":\\n            for s in val.split(os.pathsep):\\n                s = os.path.expandvars(s)\\n                s = s.replace(\"$prefix\",\\n                              os.path.join(sys.prefix, \"lib\",\\n                                           \"python\" + sys.version[:3]))\\n                s = s.replace(\"$exec_prefix\",\\n                              os.path.join(sys.exec_prefix, \"lib\",\\n                                           \"python\" + sys.version[:3]))\\n                s = os.path.normpath(s)\\n                ignore_dirs.append(s)\\n            continue\\n        assert 0, \"Should never get here\"\\n    if listfuncs and (count or trace):\\n        _err_exit(\"cannot specify both --listfuncs and (--trace or --count)\")\\n    if not (count or trace or report or listfuncs or countcallers):\\n        _err_exit(\"must specify one of --trace, --count, --report, \"\\n                  \"--listfuncs, or --trackcalls\")\\n    if report and no_report:\\n        _err_exit(\"cannot specify both --report and --no-report\")\\n    if report and not counts_file:\\n        _err_exit(\"--report requires a --file\")\\n    if no_report and len(prog_argv) == 0:\\n        _err_exit(\"missing name of file to run\")\\n    if report:\\n        results = CoverageResults(infile=counts_file, outfile=counts_file)\\n        results.write_results(missing, summary=summary, coverdir=coverdir)\\n    else:\\n        sys.argv = prog_argv\\n        progname = prog_argv[0]\\n        sys.path[0] = os.path.split(progname)[0]\\n        t = Trace(count, trace, countfuncs=listfuncs,\\n                  countcallers=countcallers, ignoremods=ignore_modules,\\n                  ignoredirs=ignore_dirs, infile=counts_file,\\n                  outfile=counts_file, timing=timing)\\n        try:\\n            fp = open(progname)\\n            try:\\n                script = fp.read()\\n            finally:\\n                fp.close()\\n            t.run('exec(%r)' % (script,))\\n        except IOError as err:\\n            _err_exit(\"Cannot run file %r because: %s\" % (sys.argv[0], err))\\n        except SystemExit:\\n            pass\\n        results = t.results()\\n        if not no_report:\\n            results.write_results(missing, summary=summary, coverdir=coverdir)\\nif __name__=='__main__':\\n    main()"
  },
  {
    "code": "def tokenize(self):\\n        \"Return a list of tokens from a given template_string\"\\n        in_tag = False\\n        result = []\\n        for bit in tag_re.split(self.template_string):\\n            if bit:\\n                result.append(self.create_token(bit, in_tag))\\n            in_tag = not in_tag\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def tokenize(self):\\n        \"Return a list of tokens from a given template_string\"\\n        in_tag = False\\n        result = []\\n        for bit in tag_re.split(self.template_string):\\n            if bit:\\n                result.append(self.create_token(bit, in_tag))\\n            in_tag = not in_tag\\n        return result"
  },
  {
    "code": "def pairwise_distances_argmin(X, Y=None, axis=1, metric=\"euclidean\",\\n                              batch_size_x=500, batch_size_y=500,\\n                              **kwargs):\\n    dist_func = None\\n    if metric in PAIRWISE_DISTANCE_FUNCTIONS:\\n        dist_func = PAIRWISE_DISTANCE_FUNCTIONS[metric]\\n    elif not callable(metric):\\n        raise ValueError(\"'metric' must be a string or a callable\")\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        batch_size_x, batch_size_y = batch_size_y, batch_size_x\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(np.infty)\\n    for chunk_x in gen_batches(X.shape[0], batch_size_x):\\n        X_chunk = X[chunk_x, :]\\n        for chunk_y in gen_batches(Y.shape[0], batch_size_y):\\n            Y_chunk = Y[chunk_y, :]\\n            if dist_func is not None:\\n                if metric == 'euclidean':  \\n                    dist_chunk = np.dot(X_chunk, Y_chunk.T)\\n                    dist_chunk *= -2\\n                    dist_chunk += (X_chunk * X_chunk\\n                                   ).sum(axis=1)[:, np.newaxis]\\n                    dist_chunk += (Y_chunk * Y_chunk\\n                                   ).sum(axis=1)[np.newaxis, :]\\n                    np.maximum(dist_chunk, 0, dist_chunk)\\n                else:\\n                    dist_chunk = dist_func(X_chunk, Y_chunk, **kwargs)\\n            else:\\n                dist_chunk = np.empty((X_chunk.shape[0], Y_chunk.shape[0]),\\n                                dtype='float')\\n                for n_x in range(X_chunk.shape[0]):\\n                    start = 0\\n                    if X is Y:\\n                        start = n_x\\n                    for n_y in range(start, Y_chunk.shape[0]):\\n                        dist_chunk[n_x, n_y] = metric(X_chunk[n_x],\\n                                                      Y_chunk[n_y],\\n                                                      **kwargs)\\n                        if X is Y:\\n                            dist_chunk[n_y, n_x] = dist_chunk[n_x, n_y]\\n            min_indices = dist_chunk.argmin(axis=1)\\n            min_values = dist_chunk[range(chunk_x.stop - chunk_x.start),\\n                                    min_indices]\\n            flags = values[chunk_x] > min_values\\n            indices[chunk_x] = np.where(\\n                flags, min_indices + chunk_y.start, indices[chunk_x])\\n            values[chunk_x] = np.where(\\n                flags, min_values, values[chunk_x])\\n    if metric == \"euclidean\" and not kwargs.get(\"squared\", False):\\n        values = np.sqrt(values)\\n    return indices, values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pairwise_distances_argmin(X, Y=None, axis=1, metric=\"euclidean\",\\n                              batch_size_x=500, batch_size_y=500,\\n                              **kwargs):\\n    dist_func = None\\n    if metric in PAIRWISE_DISTANCE_FUNCTIONS:\\n        dist_func = PAIRWISE_DISTANCE_FUNCTIONS[metric]\\n    elif not callable(metric):\\n        raise ValueError(\"'metric' must be a string or a callable\")\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        batch_size_x, batch_size_y = batch_size_y, batch_size_x\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(np.infty)\\n    for chunk_x in gen_batches(X.shape[0], batch_size_x):\\n        X_chunk = X[chunk_x, :]\\n        for chunk_y in gen_batches(Y.shape[0], batch_size_y):\\n            Y_chunk = Y[chunk_y, :]\\n            if dist_func is not None:\\n                if metric == 'euclidean':  \\n                    dist_chunk = np.dot(X_chunk, Y_chunk.T)\\n                    dist_chunk *= -2\\n                    dist_chunk += (X_chunk * X_chunk\\n                                   ).sum(axis=1)[:, np.newaxis]\\n                    dist_chunk += (Y_chunk * Y_chunk\\n                                   ).sum(axis=1)[np.newaxis, :]\\n                    np.maximum(dist_chunk, 0, dist_chunk)\\n                else:\\n                    dist_chunk = dist_func(X_chunk, Y_chunk, **kwargs)\\n            else:\\n                dist_chunk = np.empty((X_chunk.shape[0], Y_chunk.shape[0]),\\n                                dtype='float')\\n                for n_x in range(X_chunk.shape[0]):\\n                    start = 0\\n                    if X is Y:\\n                        start = n_x\\n                    for n_y in range(start, Y_chunk.shape[0]):\\n                        dist_chunk[n_x, n_y] = metric(X_chunk[n_x],\\n                                                      Y_chunk[n_y],\\n                                                      **kwargs)\\n                        if X is Y:\\n                            dist_chunk[n_y, n_x] = dist_chunk[n_x, n_y]\\n            min_indices = dist_chunk.argmin(axis=1)\\n            min_values = dist_chunk[range(chunk_x.stop - chunk_x.start),\\n                                    min_indices]\\n            flags = values[chunk_x] > min_values\\n            indices[chunk_x] = np.where(\\n                flags, min_indices + chunk_y.start, indices[chunk_x])\\n            values[chunk_x] = np.where(\\n                flags, min_values, values[chunk_x])\\n    if metric == \"euclidean\" and not kwargs.get(\"squared\", False):\\n        values = np.sqrt(values)\\n    return indices, values"
  },
  {
    "code": "class RequestToCommandArgs:\\n\\tdef __init__(self):\\n\\t\\tfield_infos = [\\n\\t\\t\\tFieldInfo.factory(key)\\n\\t\\t\\tfor key in list(request.form.keys()) + list(request.files.keys())\\n\\t\\t]\\n\\t\\tself.field_infos = list(sorted(field_infos))",
    "label": 1,
    "bug_type": "security",
    "bug_description": "better CSRF protection; change delete route to POST",
    "fixed_code": "class RequestToCommandArgs:\\n\\tdef __init__(self):\\n\\t\\tkeys = [key for key in list(request.form.keys()) + list(request.files.keys())]\\n\\t\\tfield_infos = [FieldInfo.factory(key) for key in keys if key != \"csrf_token\"]\\n\\t\\tself.field_infos = list(sorted(field_infos))"
  },
  {
    "code": "def _check_success_task(self):\\n        response = self._get_job_description()\\n        jobs = response.get(\"jobs\")\\n        matching_jobs = [job for job in jobs if job[\"jobId\"] == self.jobId]\\n        if not matching_jobs:\\n            raise AirflowException(\\n                \"Job ({}) has no job description {}\".format(self.jobId, response)\\n            )\\n        job = matching_jobs[0]\\n        self.log.info(\"AWS Batch stopped, check status: %s\", job)\\n        job_status = job[\"status\"]\\n        if job_status == \"FAILED\":\\n            reason = job[\"statusReason\"]\\n            raise AirflowException(\"Job ({}) failed with status {}\".format(self.jobId, reason))\\n        elif job_status in [\"SUBMITTED\", \"PENDING\", \"RUNNABLE\", \"STARTING\", \"RUNNING\"]:\\n            raise AirflowException(\\n                \"Job ({}) is still pending {}\".format(self.jobId, job_status)\\n            )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_success_task(self):\\n        response = self._get_job_description()\\n        jobs = response.get(\"jobs\")\\n        matching_jobs = [job for job in jobs if job[\"jobId\"] == self.jobId]\\n        if not matching_jobs:\\n            raise AirflowException(\\n                \"Job ({}) has no job description {}\".format(self.jobId, response)\\n            )\\n        job = matching_jobs[0]\\n        self.log.info(\"AWS Batch stopped, check status: %s\", job)\\n        job_status = job[\"status\"]\\n        if job_status == \"FAILED\":\\n            reason = job[\"statusReason\"]\\n            raise AirflowException(\"Job ({}) failed with status {}\".format(self.jobId, reason))\\n        elif job_status in [\"SUBMITTED\", \"PENDING\", \"RUNNABLE\", \"STARTING\", \"RUNNING\"]:\\n            raise AirflowException(\\n                \"Job ({}) is still pending {}\".format(self.jobId, job_status)\\n            )"
  },
  {
    "code": "def load_boston():\\n    module_path = dirname(__file__)\\n    data_file = csv.reader(open(join(module_path, 'data',\\n                                     'boston_house_prices.csv')))\\n    fdescr = open(join(module_path, 'descr', 'boston_house_prices.rst'))\\n    temp = next(data_file)\\n    n_samples = int(temp[0])\\n    n_features = int(temp[1])\\n    data = np.empty((n_samples, n_features))\\n    target = np.empty((n_samples,))\\n    temp = next(data_file)  \\n    feature_names = np.array(temp)\\n    for i, d in enumerate(data_file):\\n        data[i] = np.asarray(d[:-1], dtype=np.float)\\n        target[i] = np.asarray(d[-1], dtype=np.float)\\n    return Bunch(data=data,\\n                 target=target,\\n                 feature_names=feature_names[:-1],\\n                 DESCR=fdescr.read())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Changed f=open() to with open() as f to eliminate ResourceWarnings. Fixes #3410.",
    "fixed_code": "def load_boston():\\n    module_path = dirname(__file__)\\n    fdescr_name = join(module_path, 'descr', 'boston_house_prices.rst')\\n    with open(fdescr_name) as f:\\n        descr_text = f.read()\\n    data_file_name = join(module_path, 'data', 'boston_house_prices.csv')\\n    with open(data_file_name) as f:\\n        data_file = csv.reader(f)\\n        temp = next(data_file)\\n        n_samples = int(temp[0])\\n        n_features = int(temp[1])\\n        data = np.empty((n_samples, n_features))\\n        target = np.empty((n_samples,))\\n        temp = next(data_file)  \\n        feature_names = np.array(temp)\\n        for i, d in enumerate(data_file):\\n            data[i] = np.asarray(d[:-1], dtype=np.float)\\n            target[i] = np.asarray(d[-1], dtype=np.float)\\n    return Bunch(data=data,\\n                 target=target,\\n                 feature_names=feature_names[:-1],\\n                 DESCR=descr_text)"
  },
  {
    "code": "def floatformat(text, arg=-1):\\n    try:\\n        f = float(text)\\n    except (ValueError, TypeError):\\n        return u''\\n    try:\\n        d = int(arg)\\n    except ValueError:\\n        return force_unicode(f)\\n    m = f - int(f)\\n    if not m and d < 0:\\n        return mark_safe(u'%d' % int(f))\\n    else:\\n        formatstr = u'%%.%df' % abs(d)\\n        return mark_safe(formatstr % f)\\nfloatformat.is_safe = True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def floatformat(text, arg=-1):\\n    try:\\n        f = float(text)\\n    except (ValueError, TypeError):\\n        return u''\\n    try:\\n        d = int(arg)\\n    except ValueError:\\n        return force_unicode(f)\\n    m = f - int(f)\\n    if not m and d < 0:\\n        return mark_safe(u'%d' % int(f))\\n    else:\\n        formatstr = u'%%.%df' % abs(d)\\n        return mark_safe(formatstr % f)\\nfloatformat.is_safe = True"
  },
  {
    "code": "def set_inverted(self, inverted):\\n\\t\\traise NotImplementedError('Derived must override')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_inverted(self, inverted):\\n\\t\\traise NotImplementedError('Derived must override')"
  },
  {
    "code": "def time_len(self, dtype):\\n        self.s.str.len()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_len(self, dtype):\\n        self.s.str.len()"
  },
  {
    "code": "def process_subdir(subdir):\\n    if subdir:\\n        subdir = subdir.replace('DAGS_FOLDER', DAGS_FOLDER)\\n        subdir = os.path.abspath(os.path.expanduser(subdir))\\n        return subdir",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def process_subdir(subdir):\\n    if subdir:\\n        subdir = subdir.replace('DAGS_FOLDER', DAGS_FOLDER)\\n        subdir = os.path.abspath(os.path.expanduser(subdir))\\n    return subdir"
  },
  {
    "code": "def transform(self, coord_trans, clone=False):\\n        if clone:\\n            klone = self.clone()\\n            klone.transform(coord_trans)\\n            return klone\\n        if isinstance(coord_trans, CoordTransform):\\n            capi.geom_transform(self.ptr, coord_trans.ptr)\\n        elif isinstance(coord_trans, SpatialReference):\\n            capi.geom_transform_to(self.ptr, coord_trans.ptr)\\n        elif isinstance(coord_trans, (int, long, basestring)):\\n            sr = SpatialReference(coord_trans)\\n            capi.geom_transform_to(self.ptr, sr.ptr)\\n        else:\\n            raise TypeError('Transform only accepts CoordTransform, SpatialReference, string, and integer objects.')\\n    def transform_to(self, srs):\\n        \"For backwards-compatibility.\"\\n        self.transform(srs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "The `OGRGeometry.coord_dim` property may now be set; implemented a work-around for an OGR bug that changed geometries to 3D after transformation.  Refs #11433.",
    "fixed_code": "def transform(self, coord_trans, clone=False):\\n        if clone:\\n            klone = self.clone()\\n            klone.transform(coord_trans)\\n            return klone\\n        orig_dim = self.coord_dim\\n        if isinstance(coord_trans, CoordTransform):\\n            capi.geom_transform(self.ptr, coord_trans.ptr)\\n        elif isinstance(coord_trans, SpatialReference):\\n            capi.geom_transform_to(self.ptr, coord_trans.ptr)\\n        elif isinstance(coord_trans, (int, long, basestring)):\\n            sr = SpatialReference(coord_trans)\\n            capi.geom_transform_to(self.ptr, sr.ptr)\\n        else:\\n            raise TypeError('Transform only accepts CoordTransform, SpatialReference, string, and integer objects.')\\n        if self.coord_dim != orig_dim:\\n            self.coord_dim = orig_dim\\n    def transform_to(self, srs):\\n        \"For backwards-compatibility.\"\\n        self.transform(srs)"
  },
  {
    "code": "def minus(self, a):\\n        a = _convert_other(a, raiseit=True)\\n        return a.__neg__(context=self)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def minus(self, a):\\n        a = _convert_other(a, raiseit=True)\\n        return a.__neg__(context=self)"
  },
  {
    "code": "def _check_table_uniqueness(self, **kwargs):\\n        if isinstance(self.remote_field.through, six.string_types):\\n            return []\\n        registered_tables = {\\n            model._meta.db_table: model\\n            for model in self.opts.apps.get_models(include_auto_created=True)\\n            if model != self.remote_field.through\\n        }\\n        m2m_db_table = self.m2m_db_table()\\n        if m2m_db_table in registered_tables:\\n            model = registered_tables[m2m_db_table]\\n            if model._meta.auto_created:",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #27204 -- Made clashing m2m intermediary table checks ignore unmanaged models.",
    "fixed_code": "def _check_table_uniqueness(self, **kwargs):\\n        if isinstance(self.remote_field.through, six.string_types) or not self.remote_field.through._meta.managed:\\n            return []\\n        registered_tables = {\\n            model._meta.db_table: model\\n            for model in self.opts.apps.get_models(include_auto_created=True)\\n            if model != self.remote_field.through and model._meta.managed\\n        }\\n        m2m_db_table = self.m2m_db_table()\\n        if m2m_db_table in registered_tables:\\n            model = registered_tables[m2m_db_table]\\n            if model._meta.auto_created:"
  },
  {
    "code": "def update_contenttypes(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs):\\n    if not app_config.models_module:\\n        return\\n    try:\\n        ContentType = apps.get_model('contenttypes', 'ContentType')\\n    except LookupError:\\n        return\\n    if not router.allow_migrate(using, ContentType):\\n        return\\n    ContentType.objects.clear_cache()\\n    app_label = app_config.label\\n    app_models = dict(\\n        (model._meta.model_name, model)\\n        for model in app_config.get_models())\\n    if not app_models:\\n        return\\n    content_types = dict(\\n        (ct.model, ct)\\n        for ct in ContentType.objects.using(using).filter(app_label=app_label)\\n    )\\n    to_remove = [\\n        ct\\n        for (model_name, ct) in six.iteritems(content_types)\\n        if model_name not in app_models\\n    ]\\n    cts = [\\n        ContentType(\\n            name=smart_text(model._meta.verbose_name_raw),\\n            app_label=app_label,\\n            model=model_name,\\n        )\\n        for (model_name, model) in six.iteritems(app_models)\\n        if model_name not in content_types\\n    ]\\n    ContentType.objects.using(using).bulk_create(cts)\\n    if verbosity >= 2:\\n        for ct in cts:\\n            print(\"Adding content type '%s | %s'\" % (ct.app_label, ct.model))\\n    if to_remove:\\n        if interactive:\\n            content_type_display = '\\n'.join(\\n                '    %s | %s' % (ct.app_label, ct.model)\\n                for ct in to_remove\\n            )\\n            ok_to_delete = input( % content_type_display)\\n        else:\\n            ok_to_delete = False\\n        if ok_to_delete == 'yes':\\n            for ct in to_remove:\\n                if verbosity >= 2:\\n                    print(\"Deleting stale content type '%s | %s'\" % (ct.app_label, ct.model))\\n                ct.delete()\\n        else:\\n            if verbosity >= 2:\\n                print(\"Stale content types remain.\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_contenttypes(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs):\\n    if not app_config.models_module:\\n        return\\n    try:\\n        ContentType = apps.get_model('contenttypes', 'ContentType')\\n    except LookupError:\\n        return\\n    if not router.allow_migrate(using, ContentType):\\n        return\\n    ContentType.objects.clear_cache()\\n    app_label = app_config.label\\n    app_models = dict(\\n        (model._meta.model_name, model)\\n        for model in app_config.get_models())\\n    if not app_models:\\n        return\\n    content_types = dict(\\n        (ct.model, ct)\\n        for ct in ContentType.objects.using(using).filter(app_label=app_label)\\n    )\\n    to_remove = [\\n        ct\\n        for (model_name, ct) in six.iteritems(content_types)\\n        if model_name not in app_models\\n    ]\\n    cts = [\\n        ContentType(\\n            name=smart_text(model._meta.verbose_name_raw),\\n            app_label=app_label,\\n            model=model_name,\\n        )\\n        for (model_name, model) in six.iteritems(app_models)\\n        if model_name not in content_types\\n    ]\\n    ContentType.objects.using(using).bulk_create(cts)\\n    if verbosity >= 2:\\n        for ct in cts:\\n            print(\"Adding content type '%s | %s'\" % (ct.app_label, ct.model))\\n    if to_remove:\\n        if interactive:\\n            content_type_display = '\\n'.join(\\n                '    %s | %s' % (ct.app_label, ct.model)\\n                for ct in to_remove\\n            )\\n            ok_to_delete = input( % content_type_display)\\n        else:\\n            ok_to_delete = False\\n        if ok_to_delete == 'yes':\\n            for ct in to_remove:\\n                if verbosity >= 2:\\n                    print(\"Deleting stale content type '%s | %s'\" % (ct.app_label, ct.model))\\n                ct.delete()\\n        else:\\n            if verbosity >= 2:\\n                print(\"Stale content types remain.\")"
  },
  {
    "code": "def irc_PRIVMSG(self, prefix, params):\\n\\t\\tuser = prefix\\n\\t\\tchannel = params[0]\\n\\t\\tmessage = params[-1]\\n\\t\\tif not message:\\n\\t\\t\\treturn\\n\\t\\tif message[0] == X_DELIM:\\n\\t\\t\\tm = ctcpExtract(message)\\n\\t\\t\\tif m['extended']:\\n\\t\\t\\t\\tself.ctcpQuery(user, channel, m['extended'])\\n\\t\\t\\tif not m['normal']:\\n\\t\\t\\t\\treturn\\n\\t\\t\\tmessage = string.join(m['normal'], ' ')\\n\\t\\tself.privmsg(user, channel, message)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def irc_PRIVMSG(self, prefix, params):\\n\\t\\tuser = prefix\\n\\t\\tchannel = params[0]\\n\\t\\tmessage = params[-1]\\n\\t\\tif not message:\\n\\t\\t\\treturn\\n\\t\\tif message[0] == X_DELIM:\\n\\t\\t\\tm = ctcpExtract(message)\\n\\t\\t\\tif m['extended']:\\n\\t\\t\\t\\tself.ctcpQuery(user, channel, m['extended'])\\n\\t\\t\\tif not m['normal']:\\n\\t\\t\\t\\treturn\\n\\t\\t\\tmessage = string.join(m['normal'], ' ')\\n\\t\\tself.privmsg(user, channel, message)"
  },
  {
    "code": "def sort_index(self, axis=0, by=None, ascending=True):\\n        from pandas.core.groupby import _lexsort_indexer\\n        labels = self._get_axis(axis)\\n        if by is not None:\\n            assert(axis == 0)\\n            if isinstance(by, (tuple, list)):\\n                keys = [self[x].values for x in by]\\n                indexer = _lexsort_indexer(keys)\\n            else:\\n                indexer = self[by].values.argsort()\\n        else:\\n            indexer = labels.argsort()\\n        if not ascending:\\n            indexer = indexer[::-1]\\n        return self.take(indexer, axis=axis)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: inplace option for sort_index",
    "fixed_code": "def sort_index(self, axis=0, by=None, ascending=True, inplace=False):\\n        from pandas.core.groupby import _lexsort_indexer\\n        labels = self._get_axis(axis)\\n        if by is not None:\\n            assert(axis == 0)\\n            if isinstance(by, (tuple, list)):\\n                keys = [self[x].values for x in by]\\n                indexer = _lexsort_indexer(keys)\\n            else:\\n                indexer = self[by].values.argsort()\\n        else:\\n            indexer = labels.argsort()\\n        if not ascending:\\n            indexer = indexer[::-1]\\n        if inplace:\\n            if axis == 1:\\n                self._data = self._data.reindex_items(self._data.items[indexer],\\n                                                      copy=False)\\n            elif axis == 0:\\n                self._data = self._data.take(indexer)\\n            else:\\n                raise ValueError('Axis must be 0 or 1, got %s' % str(axis))\\n            self._clear_item_cache()\\n            return self\\n        else:\\n            return self.take(indexer, axis=axis)"
  },
  {
    "code": "def parse_parts(self, parts):\\n        parsed = []\\n        sep = self.sep\\n        altsep = self.altsep\\n        drv = root = ''\\n        it = reversed(parts)\\n        for part in it:\\n            if not part:\\n                continue\\n            if altsep:\\n                part = part.replace(altsep, sep)\\n            drv, root, rel = self.splitroot(part)\\n            if sep in rel:\\n                for x in reversed(rel.split(sep)):\\n                    if x and x != '.':\\n                        parsed.append(sys.intern(x))\\n            else:\\n                if rel and rel != '.':\\n                    parsed.append(sys.intern(rel))\\n            if drv or root:\\n                if not drv:\\n                    for part in it:\\n                        if not part:\\n                            continue\\n                        if altsep:\\n                            part = part.replace(altsep, sep)\\n                        drv = self.splitroot(part)[0]\\n                        if drv:\\n                            break\\n                break\\n        if drv or root:\\n            parsed.append(drv + root)\\n        parsed.reverse()\\n        return drv, root, parsed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-94909: fix joining of absolute and relative Windows paths in pathlib  (GH-95450)\\n\\nHave pathlib use `os.path.join()` to join arguments to the `PurePath` initialiser, which fixes a minor bug when handling relative paths with drives.\\n\\nPreviously:\\n\\n```python\\n>>> from pathlib import PureWindowsPath\\n>>> a = 'C:/a/b'\\n>>> b = 'C:x/y'\\n>>> PureWindowsPath(a, b)\\nPureWindowsPath('C:x/y')\\n```\\n\\nNow:\\n\\n```python\\n>>> PureWindowsPath(a, b)\\nPureWindowsPath('C:/a/b/x/y')\\n```",
    "fixed_code": "def parse_parts(self, parts):\\n        if not parts:\\n            return '', '', []\\n        sep = self.sep\\n        altsep = self.altsep\\n        path = self.pathmod.join(*parts)\\n        if altsep:\\n            path = path.replace(altsep, sep)\\n        drv, root, rel = self.splitroot(path)\\n        unfiltered_parsed = [drv + root] + rel.split(sep)\\n        parsed = [sys.intern(x) for x in unfiltered_parsed if x and x != '.']\\n        return drv, root, parsed"
  },
  {
    "code": "def configure_user(self, request, user):\\n        return user",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def configure_user(self, request, user):\\n        return user"
  },
  {
    "code": "def __init__(self,\\n                 sql,\\n                 gcp_conn_id='google_cloud_default',\\n                 bigquery_conn_id=None,\\n                 use_legacy_sql=True,\\n                 *args, **kwargs):\\n        super().__init__(sql=sql, *args, **kwargs)\\n        if not bigquery_conn_id:\\n            warnings.warn(\\n                \"The bigquery_conn_id parameter has been deprecated. You should pass \"\\n                \"the gcp_conn_id parameter.\", DeprecationWarning, stacklevel=3)\\n            gcp_conn_id = bigquery_conn_id\\n        self.gcp_conn_id = gcp_conn_id\\n        self.sql = sql\\n        self.use_legacy_sql = use_legacy_sql",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self,\\n                 sql,\\n                 gcp_conn_id='google_cloud_default',\\n                 bigquery_conn_id=None,\\n                 use_legacy_sql=True,\\n                 *args, **kwargs):\\n        super().__init__(sql=sql, *args, **kwargs)\\n        if not bigquery_conn_id:\\n            warnings.warn(\\n                \"The bigquery_conn_id parameter has been deprecated. You should pass \"\\n                \"the gcp_conn_id parameter.\", DeprecationWarning, stacklevel=3)\\n            gcp_conn_id = bigquery_conn_id\\n        self.gcp_conn_id = gcp_conn_id\\n        self.sql = sql\\n        self.use_legacy_sql = use_legacy_sql"
  },
  {
    "code": "def ravel(self, order='C'):\\n        return self.values.ravel(order=order)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ravel(self, order='C'):\\n        return self.values.ravel(order=order)"
  },
  {
    "code": "def write_array(fp, array, version=None, allow_pickle=True, pickle_kwargs=None):\\n\\t_check_version(version)\\n\\tused_ver = _write_array_header(fp, header_data_from_array_1_0(array),\\n\\t\\t\\t\\t\\t\\t\\t\\t   version)\\n\\tif version != (2, 0) and used_ver == (2, 0):\\n\\t\\twarnings.warn(\"Stored array in format 2.0. It can only be\"\\n\\t\\t\\t\\t\\t  \"read by NumPy >= 1.9\", UserWarning)\\n\\tbuffersize = max(16 * 1024 ** 2 // array.itemsize, 1)\\n\\tif array.dtype.hasobject:\\n\\t\\tif not allow_pickle:\\n\\t\\t\\traise ValueError(\"Object arrays cannot be saved when \"\\n\\t\\t\\t\\t\\t\\t\\t \"allow_pickle=False\")\\n\\t\\tif pickle_kwargs is None:\\n\\t\\t\\tpickle_kwargs = {}\\n\\t\\tpickle.dump(array, fp, protocol=2, **pickle_kwargs)\\n\\telif array.flags.f_contiguous and not array.flags.c_contiguous:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray.T.tofile(fp)\\n\\t\\telse:\\n\\t\\t\\tfor chunk in numpy.nditer(\\n\\t\\t\\t\\t\\tarray, flags=['external_loop', 'buffered', 'zerosize_ok'],\\n\\t\\t\\t\\t\\tbuffersize=buffersize, order='F'):\\n\\t\\t\\t\\tfp.write(chunk.tobytes('C'))\\n\\telse:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray.tofile(fp)\\n\\t\\telse:\\n\\t\\t\\tfor chunk in numpy.nditer(\\n\\t\\t\\t\\t\\tarray, flags=['external_loop', 'buffered', 'zerosize_ok'],\\n\\t\\t\\t\\t\\tbuffersize=buffersize, order='C'):\\n\\t\\t\\t\\tfp.write(chunk.tobytes('C'))",
    "label": 1,
    "bug_type": "corruption - memory",
    "bug_description": "BUG: Fixes to reading and writing of empty arrays, and in particular arrays with empty dtypes. See #6430",
    "fixed_code": "def write_array(fp, array, version=None, allow_pickle=True, pickle_kwargs=None):\\n\\t_check_version(version)\\n\\tused_ver = _write_array_header(fp, header_data_from_array_1_0(array),\\n\\t\\t\\t\\t\\t\\t\\t\\t   version)\\n\\tif version != (2, 0) and used_ver == (2, 0):\\n\\t\\twarnings.warn(\"Stored array in format 2.0. It can only be\"\\n\\t\\t\\t\\t\\t  \"read by NumPy >= 1.9\", UserWarning)\\n\\tif array.itemsize == 0:\\n\\t\\tbuffersize = 0\\n\\telse:\\n\\t\\tbuffersize = max(16 * 1024 ** 2 // array.itemsize, 1)\\n\\tif array.dtype.hasobject:\\n\\t\\tif not allow_pickle:\\n\\t\\t\\traise ValueError(\"Object arrays cannot be saved when \"\\n\\t\\t\\t\\t\\t\\t\\t \"allow_pickle=False\")\\n\\t\\tif pickle_kwargs is None:\\n\\t\\t\\tpickle_kwargs = {}\\n\\t\\tpickle.dump(array, fp, protocol=2, **pickle_kwargs)\\n\\telif array.flags.f_contiguous and not array.flags.c_contiguous:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray.T.tofile(fp)\\n\\t\\telse:\\n\\t\\t\\tfor chunk in numpy.nditer(\\n\\t\\t\\t\\t\\tarray, flags=['external_loop', 'buffered', 'zerosize_ok'],\\n\\t\\t\\t\\t\\tbuffersize=buffersize, order='F'):\\n\\t\\t\\t\\tfp.write(chunk.tobytes('C'))\\n\\telse:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray.tofile(fp)\\n\\t\\telse:\\n\\t\\t\\tfor chunk in numpy.nditer(\\n\\t\\t\\t\\t\\tarray, flags=['external_loop', 'buffered', 'zerosize_ok'],\\n\\t\\t\\t\\t\\tbuffersize=buffersize, order='C'):\\n\\t\\t\\t\\tfp.write(chunk.tobytes('C'))"
  },
  {
    "code": "def _add_margins(\\n    table: Union[\"Series\", \"DataFrame\"],\\n    data,\\n    values,\\n    rows,\\n    cols,\\n    aggfunc,\\n    observed=None,\\n    margins_name: str = \"All\",\\n    fill_value=None,\\n):\\n    if not isinstance(margins_name, str):\\n        raise ValueError(\"margins_name argument must be a string\")\\n    msg = 'Conflicting name \"{name}\" in margins'.format(name=margins_name)\\n    for level in table.index.names:\\n        if margins_name in table.index.get_level_values(level):\\n            raise ValueError(msg)\\n    grand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\\n    if table.ndim == 2:\\n        for level in table.columns.names[1:]:\\n            if margins_name in table.columns.get_level_values(level):\\n                raise ValueError(msg)\\n    key: Union[str, Tuple[str, ...]]\\n    if len(rows) > 1:\\n        key = (margins_name,) + (\"\",) * (len(rows) - 1)\\n    else:\\n        key = margins_name\\n    if not values and isinstance(table, ABCSeries):\\n        return table.append(Series({key: grand_margin[margins_name]}))\\n    elif values:\\n        marginal_result_set = _generate_marginal_results(\\n            table,\\n            data,\\n            values,\\n            rows,\\n            cols,\\n            aggfunc,\\n            observed,\\n            grand_margin,\\n            margins_name,\\n        )\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    else:\\n        assert isinstance(table, ABCDataFrame)\\n        marginal_result_set = _generate_marginal_results_without_values(\\n            table, data, rows, cols, aggfunc, observed, margins_name\\n        )\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    row_margin = row_margin.reindex(result.columns, fill_value=fill_value)\\n    for k in margin_keys:\\n        if isinstance(k, str):\\n            row_margin[k] = grand_margin[k]\\n        else:\\n            row_margin[k] = grand_margin[k[0]]\\n    from pandas import DataFrame\\n    margin_dummy = DataFrame(row_margin, columns=[key]).T\\n    row_names = result.index.names\\n    try:\\n        for dtype in set(result.dtypes):\\n            cols = result.select_dtypes([dtype]).columns\\n            margin_dummy[cols] = margin_dummy[cols].apply(\\n                maybe_downcast_to_dtype, args=(dtype,)\\n            )\\n        result = result.append(margin_dummy)\\n    except TypeError:\\n        result.index = result.index._to_safe_for_reshape()\\n        result = result.append(margin_dummy)\\n    result.index.names = row_names\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: df.pivot_table fails when margin is True and only columns is defined (#31088)",
    "fixed_code": "def _add_margins(\\n    table: Union[\"Series\", \"DataFrame\"],\\n    data,\\n    values,\\n    rows,\\n    cols,\\n    aggfunc,\\n    observed=None,\\n    margins_name: str = \"All\",\\n    fill_value=None,\\n):\\n    if not isinstance(margins_name, str):\\n        raise ValueError(\"margins_name argument must be a string\")\\n    msg = 'Conflicting name \"{name}\" in margins'.format(name=margins_name)\\n    for level in table.index.names:\\n        if margins_name in table.index.get_level_values(level):\\n            raise ValueError(msg)\\n    grand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\\n    if table.ndim == 2:\\n        for level in table.columns.names[1:]:\\n            if margins_name in table.columns.get_level_values(level):\\n                raise ValueError(msg)\\n    key: Union[str, Tuple[str, ...]]\\n    if len(rows) > 1:\\n        key = (margins_name,) + (\"\",) * (len(rows) - 1)\\n    else:\\n        key = margins_name\\n    if not values and isinstance(table, ABCSeries):\\n        return table.append(Series({key: grand_margin[margins_name]}))\\n    elif values:\\n        marginal_result_set = _generate_marginal_results(\\n            table, data, values, rows, cols, aggfunc, observed, margins_name,\\n        )\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    else:\\n        assert isinstance(table, ABCDataFrame)\\n        marginal_result_set = _generate_marginal_results_without_values(\\n            table, data, rows, cols, aggfunc, observed, margins_name\\n        )\\n        if not isinstance(marginal_result_set, tuple):\\n            return marginal_result_set\\n        result, margin_keys, row_margin = marginal_result_set\\n    row_margin = row_margin.reindex(result.columns, fill_value=fill_value)\\n    for k in margin_keys:\\n        if isinstance(k, str):\\n            row_margin[k] = grand_margin[k]\\n        else:\\n            row_margin[k] = grand_margin[k[0]]\\n    from pandas import DataFrame\\n    margin_dummy = DataFrame(row_margin, columns=[key]).T\\n    row_names = result.index.names\\n    try:\\n        for dtype in set(result.dtypes):\\n            cols = result.select_dtypes([dtype]).columns\\n            margin_dummy[cols] = margin_dummy[cols].apply(\\n                maybe_downcast_to_dtype, args=(dtype,)\\n            )\\n        result = result.append(margin_dummy)\\n    except TypeError:\\n        result.index = result.index._to_safe_for_reshape()\\n        result = result.append(margin_dummy)\\n    result.index.names = row_names\\n    return result"
  },
  {
    "code": "def _get_protocol_attrs(self):\\n        protocol_bases = []\\n        for c in self.__mro__:\\n            if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':\\n                protocol_bases.append(c)\\n        attrs = set()\\n        for base in protocol_bases:\\n            for attr in base.__dict__.keys():\\n                for c in self.__mro__:\\n                    if (c is not base and attr in c.__dict__ and\\n                            not getattr(c, '_is_protocol', False)):\\n                        break\\n                else:\\n                    if (not attr.startswith('_abc_') and\\n                            attr != '__abstractmethods__' and\\n                            attr != '_is_protocol' and\\n                            attr != '__dict__' and\\n                            attr != '__args__' and\\n                            attr != '__slots__' and\\n                            attr != '_get_protocol_attrs' and\\n                            attr != '__next_in_mro__' and\\n                            attr != '__parameters__' and\\n                            attr != '__origin__' and\\n                            attr != '__module__'):\\n                        attrs.add(attr)\\n        return attrs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_protocol_attrs(self):\\n        protocol_bases = []\\n        for c in self.__mro__:\\n            if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':\\n                protocol_bases.append(c)\\n        attrs = set()\\n        for base in protocol_bases:\\n            for attr in base.__dict__.keys():\\n                for c in self.__mro__:\\n                    if (c is not base and attr in c.__dict__ and\\n                            not getattr(c, '_is_protocol', False)):\\n                        break\\n                else:\\n                    if (not attr.startswith('_abc_') and\\n                            attr != '__abstractmethods__' and\\n                            attr != '_is_protocol' and\\n                            attr != '__dict__' and\\n                            attr != '__args__' and\\n                            attr != '__slots__' and\\n                            attr != '_get_protocol_attrs' and\\n                            attr != '__next_in_mro__' and\\n                            attr != '__parameters__' and\\n                            attr != '__origin__' and\\n                            attr != '__module__'):\\n                        attrs.add(attr)\\n        return attrs"
  },
  {
    "code": "def configure_logging(logging_config, logging_settings):\\n    if not sys.warnoptions:\\n        logging.captureWarnings(True)\\n        warnings.simplefilter(\"default\", RemovedInNextVersionWarning)\\n    if logging_config:\\n        logging_config_func = import_string(logging_config)\\n        dictConfig(DEFAULT_LOGGING)\\n        if logging_settings:\\n            logging_config_func(logging_settings)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Removed old import aliases.",
    "fixed_code": "def configure_logging(logging_config, logging_settings):\\n    if not sys.warnoptions:\\n        logging.captureWarnings(True)\\n        warnings.simplefilter(\"default\", RemovedInNextVersionWarning)\\n    if logging_config:\\n        logging_config_func = import_string(logging_config)\\n        logging.config.dictConfig(DEFAULT_LOGGING)\\n        if logging_settings:\\n            logging_config_func(logging_settings)"
  },
  {
    "code": "def min(self, axis: int = 0, *args, **kwargs) -> Scalar:\\n        nv.validate_min(args, kwargs)\\n        return self._min_max(\"min\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: SparseArray.min/max skipna (#44829)",
    "fixed_code": "def min(self, *, axis: int | None = None, skipna: bool = True):\\n        nv.validate_minmax_axis(axis, self.ndim)\\n        return self._min_max(\"min\", skipna=skipna)"
  },
  {
    "code": "def compute_interference_graph(ops):\\tg = nx.DiGraph()\\tfor i, op in enumerate(ops):\\t\\tg.add_node(i, op=op)\\tfor i, parent_op in enumerate(ops):\\t\\tfor j, child_op in enumerate(ops):\\t\\t\\tif i == j:\\t\\t\\t\\tcontinue\\t\\t\\tif any(output in child_op.input for output in parent_op.output):\\t\\t\\t\\tdeps = set(child_op.input).intersection(parent_op.output)\\t\\t\\t\\tg.add_edge(i, j, deps=deps)\\t\\t\\t\\tassert nx.is_directed_acyclic_graph(g), child_op\\treturn g",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "Fixed a bug in compute_interference_graph() when using with multiple in-place operators.\\n\\nSummary:\\ncompute_interference_graph() was not able to handle the case when a blob is reused twice for operators supporting in-place parameters. For example, for the following network with operators Mul and Sub\\n\\n(blob) -> [Mul] -> (blob) -> [Sub] -> (blob)\\n\\nan incorrect edge will be added from [Sub] to [Mul] and causes nx.is_directed_acyclic_graph() to fail.\\n\\nReviewed By: ajtulloch\\n\\nDifferential Revision: D5271604\\n\\nfbshipit-source-id: f6095b6f8e1dba556ba223a82c8170be7f744529",
    "fixed_code": "def compute_interference_graph(ops):\\n\\tg = nx.DiGraph()\\n\\tfor i, op in enumerate(ops):\\n\\t\\tg.add_node(i, op=op)\\n\\tfor i, parent_op in enumerate(ops):\\n\\t\\tfor j, child_op in enumerate(ops):\\n\\t\\t\\tif i >= j:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif any(output in child_op.input for output in parent_op.output):\\n\\t\\t\\t\\tdeps = set(child_op.input).intersection(parent_op.output)\\n\\t\\t\\t\\tg.add_edge(i, j, deps=deps)\\n\\t\\t\\t\\tassert nx.is_directed_acyclic_graph(g), child_op\\n\\treturn g"
  },
  {
    "code": "def clip(self, upper=None, lower=None):\\n        return self.apply(lambda x: x.clip(lower=lower, upper=upper))\\n    def clip_upper(self, threshold):\\n        return self.apply(lambda x: x.clip_upper(threshold))\\n    def clip_lower(self, threshold):\\n        return self.apply(lambda x: x.clip_lower(threshold))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DataFrame.clip arguments were inconsistent to numpy and Series.clip, closes GH #2747",
    "fixed_code": "def clip(self, lower=None, upper=None):\\n        return self.apply(lambda x: x.clip(lower=lower, upper=upper))\\n    def clip_upper(self, threshold):\\n        return self.apply(lambda x: x.clip_upper(threshold))\\n    def clip_lower(self, threshold):\\n        return self.apply(lambda x: x.clip_lower(threshold))"
  },
  {
    "code": "def check_required_arguments(argument_spec, module_parameters):\\n\\tmissing = []\\n\\tif argument_spec is None:\\n\\t\\treturn missing\\n\\tfor (k, v) in argument_spec.items():\\n\\t\\trequired = v.get('required', False)\\n\\t\\tif required and k not in module_parameters:\\n\\t\\t\\tmissing.append(k)\\n\\tif missing:\\n\\t\\tmsg = \"missing required arguments: %s\" % \", \".join(missing)\\n\\t\\traise TypeError(to_native(msg))\\n\\treturn missing",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix and add tests for some module_utils.common.validation (#67771)\\n\\n\\nAsserting inside of the `with` context of `pytest.raises`\\ndoesn't actually have any effect. So we move the assert\\nout, using the exception that gets placed into the scope\\nafter we leave the context, and ensure that it actually gets\\nchecked.\\n\\nThis is also what the pytest documentation says to do:",
    "fixed_code": "def check_required_arguments(argument_spec, module_parameters):\\n\\tmissing = []\\n\\tif argument_spec is None:\\n\\t\\treturn missing\\n\\tfor (k, v) in argument_spec.items():\\n\\t\\trequired = v.get('required', False)\\n\\t\\tif required and k not in module_parameters:\\n\\t\\t\\tmissing.append(k)\\n\\tif missing:\\n\\t\\tmsg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\\n\\t\\traise TypeError(to_native(msg))\\n\\treturn missing"
  },
  {
    "code": "def min_mag(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special or other._is_special:\\n            sn = self._isnan()\\n            on = other._isnan()\\n            if sn or on:\\n                if on == 1 and sn != 2:\\n                    return self._fix_nan(context)\\n                if sn == 1 and on != 2:\\n                    return other._fix_nan(context)\\n                return self._check_nans(other, context)\\n        c = self.copy_abs().__cmp__(other.copy_abs())\\n        if c == 0:\\n            c = self.compare_total(other)\\n        if c == -1:\\n            ans = self\\n        else:\\n            ans = other\\n        if context._rounding_decision == ALWAYS_ROUND:\\n            return ans._fix(context)\\n        return ans",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 59512-59540 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r59513 | raymond.hettinger | 2007-12-15 01:07:25 +0100 (Sat, 15 Dec 2007) | 6 lines\\n\\n  Optimize PyList_AsTuple(). Improve cache performance by doing the\\n  pointer copy and object increment in one pass.  For small lists,\\n  save the overhead of the call to memcpy() -- this comes up in\\n  calls like f(*listcomp).\\n........\\n  r59519 | christian.heimes | 2007-12-15 06:38:35 +0100 (Sat, 15 Dec 2007) | 2 lines\\n\\n  Fixed #1624: Remove output comparison for test_pep277\\n  I had to modify Brett's patch slightly.\\n........\\n  r59520 | georg.brandl | 2007-12-15 10:34:59 +0100 (Sat, 15 Dec 2007) | 2 lines\\n\\n  Add note about future import needed for with statement.\\n........\\n  r59522 | georg.brandl | 2007-12-15 10:36:37 +0100 (Sat, 15 Dec 2007) | 2 lines\\n\\n  Argh, wrong version.\\n........\\n  r59524 | georg.brandl | 2007-12-16 12:06:09 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Dummy commit to investigate #1617.\\n........\\n  r59525 | georg.brandl | 2007-12-16 12:21:48 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Revert dummy commit now that the build slave is building.\\n........\\n  r59527 | georg.brandl | 2007-12-16 16:47:46 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove orphaned footnote reference.\\n........\\n  r59528 | georg.brandl | 2007-12-16 16:53:49 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove gratuitous unicode character.\\n........\\n  r59529 | georg.brandl | 2007-12-16 16:59:19 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove another unnecessary Unicode character.\\n........\\n  r59530 | georg.brandl | 2007-12-16 17:00:36 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Remove curious space-like characters.\\n........\\n  r59532 | georg.brandl | 2007-12-16 20:36:51 +0100 (Sun, 16 Dec 2007) | 2 lines\\n\\n  Adapt conf.py to new option names.\\n........\\n  r59533 | christian.heimes | 2007-12-16 22:39:43 +0100 (Sun, 16 Dec 2007) | 1 line\\n\\n  Fixed #1638: %zd configure test fails on Linux\\n........\\n  r59536 | georg.brandl | 2007-12-17 00:11:16 +0100 (Mon, 17 Dec 2007) | 2 lines\\n\\n  Simplify.\\n........\\n  r59537 | georg.brandl | 2007-12-17 00:13:29 +0100 (Mon, 17 Dec 2007) | 2 lines\\n\\n  Use PEP 8.\\n........\\n  r59539 | georg.brandl | 2007-12-17 00:15:07 +0100 (Mon, 17 Dec 2007) | 2 lines\\n\\n  Don't use quotes for non-string code.\\n........\\n  r59540 | facundo.batista | 2007-12-17 15:18:42 +0100 (Mon, 17 Dec 2007) | 4 lines\\n\\n\\n  Removed the private _rounding_decision: it was not needed, and the code\\n  is now simpler.  Thanks Mark Dickinson.\\n........",
    "fixed_code": "def min_mag(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special or other._is_special:\\n            sn = self._isnan()\\n            on = other._isnan()\\n            if sn or on:\\n                if on == 1 and sn != 2:\\n                    return self._fix_nan(context)\\n                if sn == 1 and on != 2:\\n                    return other._fix_nan(context)\\n                return self._check_nans(other, context)\\n        c = self.copy_abs().__cmp__(other.copy_abs())\\n        if c == 0:\\n            c = self.compare_total(other)\\n        if c == -1:\\n            ans = self\\n        else:\\n            ans = other\\n        return ans._fix(context)"
  },
  {
    "code": "def load_string(self,\\n                    string_data,\\n                    key,\\n                    bucket_name=None,\\n                    replace=False,\\n                    encrypt=False,\\n                    encoding='utf-8'):\\n        self.load_bytes(string_data.encode(encoding),\\n                        key=key,\\n                        bucket_name=bucket_name,\\n                        replace=replace,\\n                        encrypt=encrypt)\\n    @provide_bucket_name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_string(self,\\n                    string_data,\\n                    key,\\n                    bucket_name=None,\\n                    replace=False,\\n                    encrypt=False,\\n                    encoding='utf-8'):\\n        self.load_bytes(string_data.encode(encoding),\\n                        key=key,\\n                        bucket_name=bucket_name,\\n                        replace=replace,\\n                        encrypt=encrypt)\\n    @provide_bucket_name"
  },
  {
    "code": "def write_array_empty(self, key: str, value: ArrayLike):\\n        arr = np.empty((1,) * value.ndim)\\n        self._handle.create_array(self.group, key, arr)\\n        node = getattr(self.group, key)\\n        node._v_attrs.value_type = str(value.dtype)\\n        node._v_attrs.shape = value.shape\\n    def write_array(self, key: str, value: ArrayLike, items: Optional[Index] = None):\\n        assert isinstance(value, (np.ndarray, ABCExtensionArray)), type(value)\\n        if key in self.group:\\n            self._handle.remove_node(self.group, key)\\n        empty_array = value.size == 0\\n        transposed = False\\n        if is_categorical_dtype(value.dtype):\\n            raise NotImplementedError(\\n                \"Cannot store a category dtype in a HDF5 dataset that uses format=\"\\n                '\"fixed\". Use format=\"table\".'\\n            )\\n        if not empty_array:\\n            if hasattr(value, \"T\"):\\n                value = value.T\\n                transposed = True\\n        atom = None\\n        if self._filters is not None:\\n            with suppress(ValueError):\\n                atom = _tables().Atom.from_dtype(value.dtype)\\n        if atom is not None:\\n            if not empty_array:\\n                ca = self._handle.create_carray(\\n                    self.group, key, atom, value.shape, filters=self._filters\\n                )\\n                ca[:] = value\\n            else:\\n                self.write_array_empty(key, value)\\n        elif value.dtype.type == np.object_:\\n            inferred_type = lib.infer_dtype(value, skipna=False)\\n            if empty_array:\\n                pass\\n            elif inferred_type == \"string\":\\n                pass\\n            else:\\n                ws = performance_doc % (inferred_type, key, items)\\n                warnings.warn(ws, PerformanceWarning, stacklevel=7)\\n            vlarr = self._handle.create_vlarray(self.group, key, _tables().ObjectAtom())\\n            vlarr.append(value)\\n        elif is_datetime64_dtype(value.dtype):\\n            self._handle.create_array(self.group, key, value.view(\"i8\"))\\n            getattr(self.group, key)._v_attrs.value_type = \"datetime64\"\\n        elif is_datetime64tz_dtype(value.dtype):\\n            self._handle.create_array(self.group, key, value.asi8)\\n            node = getattr(self.group, key)\\n            node._v_attrs.tz = _get_tz(value.tz)\\n            node._v_attrs.value_type = \"datetime64\"\\n        elif is_timedelta64_dtype(value.dtype):\\n            self._handle.create_array(self.group, key, value.view(\"i8\"))\\n            getattr(self.group, key)._v_attrs.value_type = \"timedelta64\"\\n        elif empty_array:\\n            self.write_array_empty(key, value)\\n        else:\\n            self._handle.create_array(self.group, key, value)\\n        getattr(self.group, key)._v_attrs.transposed = transposed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: preserve timezone info when writing empty tz-aware series to HDF5  (#37072)",
    "fixed_code": "def write_array_empty(self, key: str, value: ArrayLike):\\n        arr = np.empty((1,) * value.ndim)\\n        self._handle.create_array(self.group, key, arr)\\n        node = getattr(self.group, key)\\n        node._v_attrs.value_type = str(value.dtype)\\n        node._v_attrs.shape = value.shape\\n    def write_array(self, key: str, obj: FrameOrSeries, items: Optional[Index] = None):\\n        value = extract_array(obj, extract_numpy=True)\\n        if key in self.group:\\n            self._handle.remove_node(self.group, key)\\n        empty_array = value.size == 0\\n        transposed = False\\n        if is_categorical_dtype(value.dtype):\\n            raise NotImplementedError(\\n                \"Cannot store a category dtype in a HDF5 dataset that uses format=\"\\n                '\"fixed\". Use format=\"table\".'\\n            )\\n        if not empty_array:\\n            if hasattr(value, \"T\"):\\n                value = value.T\\n                transposed = True\\n        atom = None\\n        if self._filters is not None:\\n            with suppress(ValueError):\\n                atom = _tables().Atom.from_dtype(value.dtype)\\n        if atom is not None:\\n            if not empty_array:\\n                ca = self._handle.create_carray(\\n                    self.group, key, atom, value.shape, filters=self._filters\\n                )\\n                ca[:] = value\\n            else:\\n                self.write_array_empty(key, value)\\n        elif value.dtype.type == np.object_:\\n            inferred_type = lib.infer_dtype(value, skipna=False)\\n            if empty_array:\\n                pass\\n            elif inferred_type == \"string\":\\n                pass\\n            else:\\n                ws = performance_doc % (inferred_type, key, items)\\n                warnings.warn(ws, PerformanceWarning, stacklevel=7)\\n            vlarr = self._handle.create_vlarray(self.group, key, _tables().ObjectAtom())\\n            vlarr.append(value)\\n        elif is_datetime64_dtype(value.dtype):\\n            self._handle.create_array(self.group, key, value.view(\"i8\"))\\n            getattr(self.group, key)._v_attrs.value_type = \"datetime64\"\\n        elif is_datetime64tz_dtype(value.dtype):\\n            self._handle.create_array(self.group, key, value.asi8)\\n            node = getattr(self.group, key)\\n            node._v_attrs.tz = _get_tz(value.tz)\\n            node._v_attrs.value_type = \"datetime64\"\\n        elif is_timedelta64_dtype(value.dtype):\\n            self._handle.create_array(self.group, key, value.view(\"i8\"))\\n            getattr(self.group, key)._v_attrs.value_type = \"timedelta64\"\\n        elif empty_array:\\n            self.write_array_empty(key, value)\\n        else:\\n            self._handle.create_array(self.group, key, value)\\n        getattr(self.group, key)._v_attrs.transposed = transposed"
  },
  {
    "code": "def add_lazy_relation(cls, field, relation, operation):\\n    if relation == RECURSIVE_RELATIONSHIP_CONSTANT:\\n        app_label = cls._meta.app_label\\n        model_name = cls.__name__\\n    else:\\n        if isinstance(relation, six.string_types):\\n            try:\\n                app_label, model_name = relation.split(\".\")\\n            except ValueError:\\n                app_label = cls._meta.app_label\\n                model_name = relation\\n        else:\\n            app_label = relation._meta.app_label\\n            model_name = relation._meta.object_name\\n    model = cls._meta.app_cache.get_model(app_label, model_name,\\n                      seed_cache=False, only_installed=False)\\n    if model:\\n        operation(field, model, cls)\\n    else:\\n        key = (app_label, model_name)\\n        value = (cls, field, operation)\\n        cls._meta.app_cache.pending_lookups.setdefault(key, []).append(value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def add_lazy_relation(cls, field, relation, operation):\\n    if relation == RECURSIVE_RELATIONSHIP_CONSTANT:\\n        app_label = cls._meta.app_label\\n        model_name = cls.__name__\\n    else:\\n        if isinstance(relation, six.string_types):\\n            try:\\n                app_label, model_name = relation.split(\".\")\\n            except ValueError:\\n                app_label = cls._meta.app_label\\n                model_name = relation\\n        else:\\n            app_label = relation._meta.app_label\\n            model_name = relation._meta.object_name\\n    model = cls._meta.app_cache.get_model(app_label, model_name,\\n                      seed_cache=False, only_installed=False)\\n    if model:\\n        operation(field, model, cls)\\n    else:\\n        key = (app_label, model_name)\\n        value = (cls, field, operation)\\n        cls._meta.app_cache.pending_lookups.setdefault(key, []).append(value)"
  },
  {
    "code": "def plot(self, subplots=False, sharex=True, sharey=False, use_index=True,\\n             figsize=None, grid=True, legend=True, rot=30, ax=None, title=None,\\n             xlim=None, ylim=None, xticks=None, yticks=None, kind='line',\\n             sort_columns=True, fontsize=None, **kwds):\\n        import matplotlib.pyplot as plt\\n        import pandas.tools.plotting as gfx\\n        if subplots:\\n            fig, axes = gfx.subplots(nrows=len(self.columns),\\n                                     sharex=sharex, sharey=sharey,\\n                                     figsize=figsize)\\n        else:\\n            if ax is None:\\n                fig = plt.figure(figsize=figsize)\\n                ax = fig.add_subplot(111)\\n                axes = [ax]\\n            else:\\n                fig = ax.get_figure()\\n                axes = fig.get_axes()\\n        if kind == 'line':\\n            if use_index:\\n                if self.index.is_numeric() or self.index.is_datetype():\\n                    need_to_set_xticklabels = False\\n                    x = self.index\\n                else:\\n                    need_to_set_xticklabels = True\\n                    x = range(len(self))\\n            else:\\n                need_to_set_xticklabels = False\\n                x = range(len(self))\\n            if sort_columns:\\n                columns = _try_sort(self.columns)\\n            else:\\n                columns = self.columns\\n            for i, col in enumerate(columns):\\n                empty = self[col].count() == 0\\n                y = self[col].values if not empty else np.zeros(x.shape)\\n                if subplots:\\n                    ax = axes[i]\\n                    ax.plot(x, y, 'k', label=str(col), **kwds)\\n                    ax.legend(loc='best')\\n                else:\\n                    ax.plot(x, y, label=str(col), **kwds)\\n                ax.grid(grid)\\n            if legend and not subplots:\\n                ax.legend(loc='best')\\n            if need_to_set_xticklabels:\\n                xticklabels = [gfx._stringify(key) for key in self.index]\\n                for ax_ in axes:\\n                    ax_.set_xticks(x)\\n                    ax_.set_xticklabels(xticklabels, rotation=rot)\\n        elif kind == 'bar':\\n            self._bar_plot(axes, subplots=subplots, grid=grid, rot=rot,\\n                           legend=legend, ax=ax, fontsize=fontsize)\\n        if self.index.is_all_dates and not subplots or (subplots and sharex):\\n            try:\\n                fig.autofmt_xdate()\\n            except Exception:  \\n                pass\\n        if yticks is not None:\\n            ax.set_yticks(yticks)\\n        if xticks is not None:\\n            ax.set_xticks(xticks)\\n        if ylim is not None:\\n            ax.set_ylim(ylim)\\n        if xlim is not None:\\n            ax.set_xlim(xlim)\\n        if title and not subplots:\\n            ax.set_title(title)\\n        plt.draw_if_interactive()\\n        if subplots:\\n            return axes\\n        else:\\n            return ax",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def plot(self, subplots=False, sharex=True, sharey=False, use_index=True,\\n             figsize=None, grid=True, legend=True, rot=30, ax=None, title=None,\\n             xlim=None, ylim=None, xticks=None, yticks=None, kind='line',\\n             sort_columns=True, fontsize=None, **kwds):\\n        import matplotlib.pyplot as plt\\n        import pandas.tools.plotting as gfx\\n        if subplots:\\n            fig, axes = gfx.subplots(nrows=len(self.columns),\\n                                     sharex=sharex, sharey=sharey,\\n                                     figsize=figsize)\\n        else:\\n            if ax is None:\\n                fig = plt.figure(figsize=figsize)\\n                ax = fig.add_subplot(111)\\n                axes = [ax]\\n            else:\\n                fig = ax.get_figure()\\n                axes = fig.get_axes()\\n        if kind == 'line':\\n            if use_index:\\n                if self.index.is_numeric() or self.index.is_datetype():\\n                    need_to_set_xticklabels = False\\n                    x = self.index\\n                else:\\n                    need_to_set_xticklabels = True\\n                    x = range(len(self))\\n            else:\\n                need_to_set_xticklabels = False\\n                x = range(len(self))\\n            if sort_columns:\\n                columns = _try_sort(self.columns)\\n            else:\\n                columns = self.columns\\n            for i, col in enumerate(columns):\\n                empty = self[col].count() == 0\\n                y = self[col].values if not empty else np.zeros(x.shape)\\n                if subplots:\\n                    ax = axes[i]\\n                    ax.plot(x, y, 'k', label=str(col), **kwds)\\n                    ax.legend(loc='best')\\n                else:\\n                    ax.plot(x, y, label=str(col), **kwds)\\n                ax.grid(grid)\\n            if legend and not subplots:\\n                ax.legend(loc='best')\\n            if need_to_set_xticklabels:\\n                xticklabels = [gfx._stringify(key) for key in self.index]\\n                for ax_ in axes:\\n                    ax_.set_xticks(x)\\n                    ax_.set_xticklabels(xticklabels, rotation=rot)\\n        elif kind == 'bar':\\n            self._bar_plot(axes, subplots=subplots, grid=grid, rot=rot,\\n                           legend=legend, ax=ax, fontsize=fontsize)\\n        if self.index.is_all_dates and not subplots or (subplots and sharex):\\n            try:\\n                fig.autofmt_xdate()\\n            except Exception:  \\n                pass\\n        if yticks is not None:\\n            ax.set_yticks(yticks)\\n        if xticks is not None:\\n            ax.set_xticks(xticks)\\n        if ylim is not None:\\n            ax.set_ylim(ylim)\\n        if xlim is not None:\\n            ax.set_xlim(xlim)\\n        if title and not subplots:\\n            ax.set_title(title)\\n        plt.draw_if_interactive()\\n        if subplots:\\n            return axes\\n        else:\\n            return ax"
  },
  {
    "code": "def main():\\n    argument_spec = ec2_argument_spec()\\n    argument_spec.update(\\n        dict(\\n            name = dict(default=None, required=True),\\n            shards = dict(default=None, required=False, type='int'),\\n            retention_period = dict(default=None, required=False, type='int'),\\n            tags = dict(default=None, required=False, type='dict', aliases=['resource_tags']),\\n            wait = dict(default=True, required=False, type='bool'),\\n            wait_timeout = dict(default=300, required=False, type='int'),\\n            state = dict(default='present', choices=['present', 'absent']),\\n        )\\n    )\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    retention_period = module.params.get('retention_period')\\n    stream_name = module.params.get('name')\\n    shards = module.params.get('shards')\\n    state = module.params.get('state')\\n    tags = module.params.get('tags')\\n    wait = module.params.get('wait')\\n    wait_timeout = module.params.get('wait_timeout')\\n    if state == 'present' and not shards:\\n        module.fail_json(msg='shards is required when state == present.')\\n    if not HAS_BOTO3:\\n        module.fail_json(msg='boto3 is required.')\\n    check_mode = module.check_mode\\n    try:\\n        region, ec2_url, aws_connect_kwargs = (\\n            get_aws_connection_info(module, boto3=True)\\n        )\\n        client = (\\n            boto3_conn(\\n                module, conn_type='client', resource='kinesis',\\n                region=region, endpoint=ec2_url, **aws_connect_kwargs\\n            )\\n        )\\n    except botocore.exceptions.ClientError, e:\\n        err_msg = 'Boto3 Client Error - {0}'.format(str(e.msg))\\n        module.fail_json(\\n            success=False, changed=False, result={}, msg=err_msg\\n        )\\n    if state == 'present':\\n        success, changed, err_msg, results = (\\n            create_stream(\\n                client, stream_name, shards, retention_period, tags,\\n                wait, wait_timeout, check_mode\\n            )\\n        )\\n    elif state == 'absent':\\n        success, changed, err_msg, results = (\\n            delete_stream(client, stream_name, wait, wait_timeout, check_mode)\\n        )\\n    if success:\\n        module.exit_json(\\n            success=success, changed=changed, msg=err_msg, **results\\n        )\\n    else:\\n        module.fail_json(\\n            success=success, changed=changed, msg=err_msg, result=results\\n        )\\nfrom ansible.module_utils.basic import *\\nfrom ansible.module_utils.ec2 import *\\nif __name__ == '__main__':\\n    main()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added test to kinesis_stream module.",
    "fixed_code": "def main():\\n    argument_spec = ec2_argument_spec()\\n    argument_spec.update(\\n        dict(\\n            name = dict(default=None, required=True),\\n            shards = dict(default=None, required=False, type='int'),\\n            retention_period = dict(default=None, required=False, type='int'),\\n            tags = dict(default=None, required=False, type='dict', aliases=['resource_tags']),\\n            wait = dict(default=True, required=False, type='bool'),\\n            wait_timeout = dict(default=300, required=False, type='int'),\\n            state = dict(default='present', choices=['present', 'absent']),\\n        )\\n    )\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    retention_period = module.params.get('retention_period')\\n    stream_name = module.params.get('name')\\n    shards = module.params.get('shards')\\n    state = module.params.get('state')\\n    tags = module.params.get('tags')\\n    wait = module.params.get('wait')\\n    wait_timeout = module.params.get('wait_timeout')\\n    if state == 'present' and not shards:\\n        module.fail_json(msg='Shards is required when state == present.')\\n    if retention_period:\\n        if retention_period < 24:\\n            module.fail_json(msg='Retention period can not be less than 24 hours.')\\n    if not HAS_BOTO3:\\n        module.fail_json(msg='boto3 is required.')\\n    check_mode = module.check_mode\\n    try:\\n        region, ec2_url, aws_connect_kwargs = (\\n            get_aws_connection_info(module, boto3=True)\\n        )\\n        client = (\\n            boto3_conn(\\n                module, conn_type='client', resource='kinesis',\\n                region=region, endpoint=ec2_url, **aws_connect_kwargs\\n            )\\n        )\\n    except botocore.exceptions.ClientError, e:\\n        err_msg = 'Boto3 Client Error - {0}'.format(str(e.msg))\\n        module.fail_json(\\n            success=False, changed=False, result={}, msg=err_msg\\n        )\\n    if state == 'present':\\n        success, changed, err_msg, results = (\\n            create_stream(\\n                client, stream_name, shards, retention_period, tags,\\n                wait, wait_timeout, check_mode\\n            )\\n        )\\n    elif state == 'absent':\\n        success, changed, err_msg, results = (\\n            delete_stream(client, stream_name, wait, wait_timeout, check_mode)\\n        )\\n    if success:\\n        module.exit_json(\\n            success=success, changed=changed, msg=err_msg, **results\\n        )\\n    else:\\n        module.fail_json(\\n            success=success, changed=changed, msg=err_msg, result=results\\n        )\\nfrom ansible.module_utils.basic import *\\nfrom ansible.module_utils.ec2 import *\\nif __name__ == '__main__':\\n    main()"
  },
  {
    "code": "def _create_pseudo_member_(cls, value):\\n        pseudo_member = cls._value2member_map_.get(value, None)\\n        if pseudo_member is None:\\n            pseudo_member = object.__new__(cls)\\n            pseudo_member._name_ = None\\n            pseudo_member._value_ = value\\n            cls._value2member_map_[value] = pseudo_member\\n        return pseudo_member",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "issue23591: fix flag decomposition and repr",
    "fixed_code": "def _create_pseudo_member_(cls, value):\\n        pseudo_member = cls._value2member_map_.get(value, None)\\n        if pseudo_member is None:\\n            _, extra_flags = _decompose(cls, value)\\n            if extra_flags:\\n                raise ValueError(\"%r is not a valid %s\" % (value, cls.__name__))\\n            pseudo_member = object.__new__(cls)\\n            pseudo_member._name_ = None\\n            pseudo_member._value_ = value\\n            cls._value2member_map_[value] = pseudo_member\\n        return pseudo_member"
  },
  {
    "code": "def flex_comp_method_FRAME(cls: Type[\"DataFrame\"], op, special: bool):\\n    assert not special  \\n    op_name = _get_op_name(op, special)\\n    default_axis = _get_frame_op_default_axis(op_name)\\n    assert default_axis == \"columns\", default_axis  \\n    doc = _flex_comp_doc_FRAME.format(\\n        op_name=op_name, desc=_op_descriptions[op_name][\"desc\"]\\n    )\\n    @Appender(doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: make DataFrame.__boolop__ default_axis match DataFrame.__arithop__ default_axis (#36793)",
    "fixed_code": "def flex_comp_method_FRAME(cls: Type[\"DataFrame\"], op, special: bool):\\n    assert not special  \\n    op_name = _get_op_name(op, special)\\n    default_axis = \"columns\"  \\n    doc = _flex_comp_doc_FRAME.format(\\n        op_name=op_name, desc=_op_descriptions[op_name][\"desc\"]\\n    )\\n    @Appender(doc)"
  },
  {
    "code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    err = err1 = err2 = None\\n    code = code1 = code2 = None\\n    try:\\n        code = compiler(source, filename, symbol)\\n    except SyntaxError, err:\\n        pass\\n    try:\\n        code1 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err1:\\n        pass\\n    try:\\n        code2 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err2:\\n        pass\\n    if code:\\n        return code\\n    if not code1 and repr(err1) == repr(err2):\\n        raise SyntaxError, err1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    err = err1 = err2 = None\\n    code = code1 = code2 = None\\n    try:\\n        code = compiler(source, filename, symbol)\\n    except SyntaxError, err:\\n        pass\\n    try:\\n        code1 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err1:\\n        pass\\n    try:\\n        code2 = compiler(source + \"\\n\", filename, symbol)\\n    except SyntaxError, err2:\\n        pass\\n    if code:\\n        return code\\n    if not code1 and repr(err1) == repr(err2):\\n        raise SyntaxError, err1"
  },
  {
    "code": "def __init__(self, precision=None, use_eng_prefix=False):\\n        self.precision = precision\\n        self.use_eng_prefix = use_eng_prefix",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, precision=None, use_eng_prefix=False):\\n        self.precision = precision\\n        self.use_eng_prefix = use_eng_prefix"
  },
  {
    "code": "def _sort_mixed(values) -> AnyArrayLike:\\n    str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)\\n    null_pos = np.array([isna(x) for x in values], dtype=bool)\\n    num_pos = ~str_pos & ~null_pos\\n    str_argsort = np.argsort(values[str_pos])\\n    num_argsort = np.argsort(values[num_pos])\\n    str_locs = str_pos.nonzero()[0].take(str_argsort)\\n    num_locs = num_pos.nonzero()[0].take(num_argsort)\\n    null_locs = null_pos.nonzero()[0]\\n    locs = np.concatenate([num_locs, str_locs, null_locs])\\n    return values.take(locs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _sort_mixed(values) -> AnyArrayLike:\\n    str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)\\n    null_pos = np.array([isna(x) for x in values], dtype=bool)\\n    num_pos = ~str_pos & ~null_pos\\n    str_argsort = np.argsort(values[str_pos])\\n    num_argsort = np.argsort(values[num_pos])\\n    str_locs = str_pos.nonzero()[0].take(str_argsort)\\n    num_locs = num_pos.nonzero()[0].take(num_argsort)\\n    null_locs = null_pos.nonzero()[0]\\n    locs = np.concatenate([num_locs, str_locs, null_locs])\\n    return values.take(locs)"
  },
  {
    "code": "def _index_name(self, index, index_label):\\n        if index is True:\\n            nlevels = self.frame.index.nlevels\\n            if index_label is not None:\\n                if not isinstance(index_label, list):\\n                    index_label = [index_label]\\n                if len(index_label) != nlevels:\\n                    raise ValueError(\\n                        \"Length of 'index_label' should match number of \"\\n                        \"levels, which is {0}\".format(nlevels))\\n                else:\\n                    return index_label\\n            if nlevels == 1 and 'index' not in self.frame.columns and self.frame.index.name is None:\\n                return ['index']\\n            else:\\n                return [l if l is not None else \"level_{0}\".format(i)\\n                        for i, l in enumerate(self.frame.index.names)]\\n        elif isinstance(index, string_types):\\n            return index\\n        else:\\n            return None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _index_name(self, index, index_label):\\n        if index is True:\\n            nlevels = self.frame.index.nlevels\\n            if index_label is not None:\\n                if not isinstance(index_label, list):\\n                    index_label = [index_label]\\n                if len(index_label) != nlevels:\\n                    raise ValueError(\\n                        \"Length of 'index_label' should match number of \"\\n                        \"levels, which is {0}\".format(nlevels))\\n                else:\\n                    return index_label\\n            if nlevels == 1 and 'index' not in self.frame.columns and self.frame.index.name is None:\\n                return ['index']\\n            else:\\n                return [l if l is not None else \"level_{0}\".format(i)\\n                        for i, l in enumerate(self.frame.index.names)]\\n        elif isinstance(index, string_types):\\n            return index\\n        else:\\n            return None"
  },
  {
    "code": "def get_export_symbols (self, ext):\\n        initfunc_name = \"init\" + string.split(ext.name,'.')[-1]\\n        if initfunc_name not in ext.export_symbols:\\n            ext.export_symbols.append(initfunc_name)\\n        return ext.export_symbols",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 73946 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r73946 | tarek.ziade | 2009-07-11 12:55:27 +0200 (Sat, 11 Jul 2009) | 1 line\\n\\n  fixed #6459: distutils.command.build_ext.get_export_symbols now uses 'PyInit'\\n........",
    "fixed_code": "def get_export_symbols (self, ext):\\n        initfunc_name = \"PyInit_\" + ext.name.split('.')[-1]\\n        if initfunc_name not in ext.export_symbols:\\n            ext.export_symbols.append(initfunc_name)\\n        return ext.export_symbols"
  },
  {
    "code": "def tell(self):\\n        if self.level > 0:\\n            return self.lastpos\\n        return self.fp.tell() - self.start",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def tell(self):\\n        if self.level > 0:\\n            return self.lastpos\\n        return self.fp.tell() - self.start"
  },
  {
    "code": "def _make_cat_accessor(self):\\n        if not com.is_categorical_dtype(self.dtype):\\n            raise AttributeError(\"Can only use .cat accessor with a \"\\n                                 \"'category' dtype\")\\n        return CategoricalAccessor(self.values, self.index)\\n    cat = base.AccessorProperty(CategoricalAccessor, _make_cat_accessor)\\nSeries._setup_axes(['index'], info_axis=0, stat_axis=0,\\n                   aliases={'rows': 0})\\nSeries._add_numeric_operations()\\n_INDEX_TYPES = ndarray, Index, list, tuple",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _make_cat_accessor(self):\\n        if not com.is_categorical_dtype(self.dtype):\\n            raise AttributeError(\"Can only use .cat accessor with a \"\\n                                 \"'category' dtype\")\\n        return CategoricalAccessor(self.values, self.index)\\n    cat = base.AccessorProperty(CategoricalAccessor, _make_cat_accessor)\\nSeries._setup_axes(['index'], info_axis=0, stat_axis=0,\\n                   aliases={'rows': 0})\\nSeries._add_numeric_operations()\\n_INDEX_TYPES = ndarray, Index, list, tuple"
  },
  {
    "code": "def nan_euclidean_distances(X, Y=None, squared=False,\\n                            missing_values=np.nan, copy=True):\\n    force_all_finite = 'allow-nan' if is_scalar_nan(missing_values) else True\\n    X, Y = check_pairwise_arrays(X, Y, accept_sparse=False,\\n                                 force_all_finite=force_all_finite, copy=copy)\\n    missing_X = _get_mask(X, missing_values)\\n    missing_Y = missing_X if Y is X else _get_mask(Y, missing_values)\\n    X[missing_X] = 0\\n    Y[missing_Y] = 0\\n    distances = euclidean_distances(X, Y, squared=True)\\n    XX = X * X\\n    YY = Y * Y\\n    distances -= np.dot(XX, missing_Y.T)\\n    distances -= np.dot(missing_X, YY.T)\\n    np.clip(distances, 0, None, out=distances)\\n    if X is Y:\\n        np.fill_diagonal(distances, 0.0)\\n    present_X = 1 - missing_X\\n    present_Y = present_X if Y is X else ~missing_Y\\n    present_count = np.dot(present_X, present_Y.T)\\n    distances[present_count == 0] = np.nan\\n    np.maximum(1, present_count, out=present_count)\\n    distances /= present_count\\n    distances *= X.shape[1]\\n    if not squared:\\n        np.sqrt(distances, out=distances)\\n    return distances",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def nan_euclidean_distances(X, Y=None, squared=False,\\n                            missing_values=np.nan, copy=True):\\n    force_all_finite = 'allow-nan' if is_scalar_nan(missing_values) else True\\n    X, Y = check_pairwise_arrays(X, Y, accept_sparse=False,\\n                                 force_all_finite=force_all_finite, copy=copy)\\n    missing_X = _get_mask(X, missing_values)\\n    missing_Y = missing_X if Y is X else _get_mask(Y, missing_values)\\n    X[missing_X] = 0\\n    Y[missing_Y] = 0\\n    distances = euclidean_distances(X, Y, squared=True)\\n    XX = X * X\\n    YY = Y * Y\\n    distances -= np.dot(XX, missing_Y.T)\\n    distances -= np.dot(missing_X, YY.T)\\n    np.clip(distances, 0, None, out=distances)\\n    if X is Y:\\n        np.fill_diagonal(distances, 0.0)\\n    present_X = 1 - missing_X\\n    present_Y = present_X if Y is X else ~missing_Y\\n    present_count = np.dot(present_X, present_Y.T)\\n    distances[present_count == 0] = np.nan\\n    np.maximum(1, present_count, out=present_count)\\n    distances /= present_count\\n    distances *= X.shape[1]\\n    if not squared:\\n        np.sqrt(distances, out=distances)\\n    return distances"
  },
  {
    "code": "def maybe_set_size(self, min_itemsize=None, **kwargs):\\n        if self.kind == u'string':\\n            if isinstance(min_itemsize, dict):\\n                min_itemsize = min_itemsize.get(self.name)\\n            if min_itemsize is not None and self.typ.itemsize < min_itemsize:\\n                self.typ = _tables(\\n                ).StringCol(itemsize=min_itemsize, pos=self.pos)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_set_size(self, min_itemsize=None, **kwargs):\\n        if self.kind == u'string':\\n            if isinstance(min_itemsize, dict):\\n                min_itemsize = min_itemsize.get(self.name)\\n            if min_itemsize is not None and self.typ.itemsize < min_itemsize:\\n                self.typ = _tables(\\n                ).StringCol(itemsize=min_itemsize, pos=self.pos)"
  },
  {
    "code": "def _run_code(code, run_globals, init_globals,\\n              mod_name, mod_fname, mod_loader):\\n    if init_globals is not None:\\n        run_globals.update(init_globals)\\n    run_globals.update(__name__ = mod_name,\\n                       __file__ = mod_fname,\\n                       __loader__ = mod_loader)\\n    exec code in run_globals\\n    return run_globals",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _run_code(code, run_globals, init_globals,\\n              mod_name, mod_fname, mod_loader):\\n    if init_globals is not None:\\n        run_globals.update(init_globals)\\n    run_globals.update(__name__ = mod_name,\\n                       __file__ = mod_fname,\\n                       __loader__ = mod_loader)\\n    exec code in run_globals\\n    return run_globals"
  },
  {
    "code": "class ReduceLROnPlateau(Callback):\\n\\tdef __init__(self, monitor='val_loss', factor=0.1, patience=10,\\n\\t\\t\\t\\t verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\\n\\t\\t\\t\\t **kwargs):\\n\\t\\tsuper(ReduceLROnPlateau, self).__init__()\\n\\t\\tself.monitor = monitor\\n\\t\\tif factor >= 1.0:\\n\\t\\t\\traise ValueError('ReduceLROnPlateau '\\n\\t\\t\\t\\t\\t\\t\\t 'does not support a factor >= 1.0.')\\n\\t\\tif 'epsilon' in kwargs:\\n\\t\\t\\tmin_delta = kwargs.pop('epsilon')\\n\\t\\t\\twarnings.warn('`epsilon` argument is deprecated and '\\n\\t\\t\\t\\t\\t\\t  'will be removed, use `min_delta` insted.')\\n\\t\\tself.factor = factor\\n\\t\\tself.min_lr = min_lr\\n\\t\\tself.min_delta = min_delta\\n\\t\\tself.patience = patience\\n\\t\\tself.verbose = verbose\\n\\t\\tself.cooldown = cooldown\\n\\t\\tself.cooldown_counter = 0  \\n\\t\\tself.wait = 0\\n\\t\\tself.best = 0\\n\\t\\tself.mode = mode\\n\\t\\tself.monitor_op = None\\n\\t\\tself._reset()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class ReduceLROnPlateau(Callback):\\n\\tdef __init__(self, monitor='val_loss', factor=0.1, patience=10,\\n\\t\\t\\t\\t verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\\n\\t\\t\\t\\t **kwargs):\\n\\t\\tsuper(ReduceLROnPlateau, self).__init__()\\n\\t\\tself.monitor = monitor\\n\\t\\tif factor >= 1.0:\\n\\t\\t\\traise ValueError('ReduceLROnPlateau '\\n\\t\\t\\t\\t\\t\\t\\t 'does not support a factor >= 1.0.')\\n\\t\\tif 'epsilon' in kwargs:\\n\\t\\t\\tmin_delta = kwargs.pop('epsilon')\\n\\t\\t\\twarnings.warn('`epsilon` argument is deprecated and '\\n\\t\\t\\t\\t\\t\\t  'will be removed, use `min_delta` insted.')\\n\\t\\tself.factor = factor\\n\\t\\tself.min_lr = min_lr\\n\\t\\tself.min_delta = min_delta\\n\\t\\tself.patience = patience\\n\\t\\tself.verbose = verbose\\n\\t\\tself.cooldown = cooldown\\n\\t\\tself.cooldown_counter = 0  \\n\\t\\tself.wait = 0\\n\\t\\tself.best = 0\\n\\t\\tself.mode = mode\\n\\t\\tself.monitor_op = None\\n\\t\\tself._reset()"
  },
  {
    "code": "def _join_multi(self, other, how, return_indexers=True):\\n\\t\\tfrom pandas.core.indexes.multi import MultiIndex\\n\\t\\tfrom pandas.core.reshape.merge import _restore_dropped_levels_multijoin\\n\\t\\tself_names = set(com.not_none(*self.names))\\n\\t\\tother_names = set(com.not_none(*other.names))\\n\\t\\toverlap = self_names & other_names\\n\\t\\tif not overlap:\\n\\t\\t\\traise ValueError(\"cannot join with no overlapping index names\")\\n\\t\\tself_is_mi = isinstance(self, ABCMultiIndex)\\n\\t\\tother_is_mi = isinstance(other, ABCMultiIndex)\\n\\t\\tif self_is_mi and other_is_mi:\\n\\t\\t\\tldrop_names = list(self_names - overlap)\\n\\t\\t\\trdrop_names = list(other_names - overlap)\\n\\t\\t\\tif not len(ldrop_names + rdrop_names):\\n\\t\\t\\t\\tself_jnlevels = self\\n\\t\\t\\t\\tother_jnlevels = other.reorder_levels(self.names)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself_jnlevels = self.droplevel(ldrop_names)\\n\\t\\t\\t\\tother_jnlevels = other.droplevel(rdrop_names)\\n\\t\\t\\tjoin_idx, lidx, ridx = self_jnlevels.join(\\n\\t\\t\\t\\tother_jnlevels, how, return_indexers=True\\n\\t\\t\\t)\\n\\t\\t\\tdropped_names = ldrop_names + rdrop_names\\n\\t\\t\\tlevels, codes, names = _restore_dropped_levels_multijoin(\\n\\t\\t\\t\\tself, other, dropped_names, join_idx, lidx, ridx\\n\\t\\t\\t)\\n\\t\\t\\tmulti_join_idx = MultiIndex(\\n\\t\\t\\t\\tlevels=levels, codes=codes, names=names, verify_integrity=False\\n\\t\\t\\t)\\n\\t\\t\\tmulti_join_idx = multi_join_idx.remove_unused_levels()\\n\\t\\t\\treturn multi_join_idx, lidx, ridx\\n\\t\\tjl = list(overlap)[0]\\n\\t\\tflip_order = False\\n\\t\\tif self_is_mi:\\n\\t\\t\\tself, other = other, self\\n\\t\\t\\tflip_order = True\\n\\t\\t\\thow = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\\n\\t\\tlevel = other.names.index(jl)\\n\\t\\tresult = self._join_level(\\n\\t\\t\\tother, level, how=how, return_indexers=return_indexers\\n\\t\\t)\\n\\t\\tif flip_order:\\n\\t\\t\\tif isinstance(result, tuple):\\n\\t\\t\\t\\treturn result[0], result[2], result[1]\\n\\t\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fixed __join_multi always returning indexers (#34074) (#34075)",
    "fixed_code": "def _join_multi(self, other, how, return_indexers=True):\\n\\t\\tfrom pandas.core.indexes.multi import MultiIndex\\n\\t\\tfrom pandas.core.reshape.merge import _restore_dropped_levels_multijoin\\n\\t\\tself_names = set(com.not_none(*self.names))\\n\\t\\tother_names = set(com.not_none(*other.names))\\n\\t\\toverlap = self_names & other_names\\n\\t\\tif not overlap:\\n\\t\\t\\traise ValueError(\"cannot join with no overlapping index names\")\\n\\t\\tself_is_mi = isinstance(self, ABCMultiIndex)\\n\\t\\tother_is_mi = isinstance(other, ABCMultiIndex)\\n\\t\\tif self_is_mi and other_is_mi:\\n\\t\\t\\tldrop_names = list(self_names - overlap)\\n\\t\\t\\trdrop_names = list(other_names - overlap)\\n\\t\\t\\tif not len(ldrop_names + rdrop_names):\\n\\t\\t\\t\\tself_jnlevels = self\\n\\t\\t\\t\\tother_jnlevels = other.reorder_levels(self.names)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself_jnlevels = self.droplevel(ldrop_names)\\n\\t\\t\\t\\tother_jnlevels = other.droplevel(rdrop_names)\\n\\t\\t\\tjoin_idx, lidx, ridx = self_jnlevels.join(\\n\\t\\t\\t\\tother_jnlevels, how, return_indexers=True\\n\\t\\t\\t)\\n\\t\\t\\tdropped_names = ldrop_names + rdrop_names\\n\\t\\t\\tlevels, codes, names = _restore_dropped_levels_multijoin(\\n\\t\\t\\t\\tself, other, dropped_names, join_idx, lidx, ridx\\n\\t\\t\\t)\\n\\t\\t\\tmulti_join_idx = MultiIndex(\\n\\t\\t\\t\\tlevels=levels, codes=codes, names=names, verify_integrity=False\\n\\t\\t\\t)\\n\\t\\t\\tmulti_join_idx = multi_join_idx.remove_unused_levels()\\n\\t\\t\\tif return_indexers:\\n\\t\\t\\t\\treturn multi_join_idx, lidx, ridx\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn multi_join_idx\\n\\t\\tjl = list(overlap)[0]\\n\\t\\tflip_order = False\\n\\t\\tif self_is_mi:\\n\\t\\t\\tself, other = other, self\\n\\t\\t\\tflip_order = True\\n\\t\\t\\thow = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\\n\\t\\tlevel = other.names.index(jl)\\n\\t\\tresult = self._join_level(\\n\\t\\t\\tother, level, how=how, return_indexers=return_indexers\\n\\t\\t)\\n\\t\\tif flip_order:\\n\\t\\t\\tif isinstance(result, tuple):\\n\\t\\t\\t\\treturn result[0], result[2], result[1]\\n\\t\\treturn result"
  },
  {
    "code": "def dag_link(attr):\\n    dag_id = attr.get('dag_id')\\n    execution_date = attr.get('execution_date')\\n    url = url_for('Airflow.graph', dag_id=dag_id, execution_date=execution_date)\\n    return Markup('<a href=\"{}\">{}</a>').format(url, dag_id) if dag_id else Markup('None')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def dag_link(attr):\\n    dag_id = attr.get('dag_id')\\n    execution_date = attr.get('execution_date')\\n    url = url_for('Airflow.graph', dag_id=dag_id, execution_date=execution_date)\\n    return Markup('<a href=\"{}\">{}</a>').format(url, dag_id) if dag_id else Markup('None')"
  },
  {
    "code": "def header_encode(header_bytes, charset='iso-8859-1'):\\n    if not header_bytes:\\n        return ''\\n    encoded = header_bytes.decode('latin1').translate(_QUOPRI_HEADER_MAP)\\n    return '=?%s?q?%s?=' % (charset, encoded)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def header_encode(header_bytes, charset='iso-8859-1'):\\n    if not header_bytes:\\n        return ''\\n    encoded = header_bytes.decode('latin1').translate(_QUOPRI_HEADER_MAP)\\n    return '=?%s?q?%s?=' % (charset, encoded)"
  },
  {
    "code": "def duplicated(self, keep='first'):\\n        keys = com._ensure_object(self.values)\\n        duplicated = lib.duplicated(keys, keep=keep)\\n        try:\\n            return self._constructor(duplicated,\\n                                     index=self.index).__finalize__(self)\\n        except AttributeError:\\n            return np.array(duplicated, dtype=bool)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def duplicated(self, keep='first'):\\n        keys = com._ensure_object(self.values)\\n        duplicated = lib.duplicated(keys, keep=keep)\\n        try:\\n            return self._constructor(duplicated,\\n                                     index=self.index).__finalize__(self)\\n        except AttributeError:\\n            return np.array(duplicated, dtype=bool)"
  },
  {
    "code": "def _add_subcommand(cls, subparsers, sub):\\n        dag_parser = False\\n        sp = subparsers.add_parser(sub.get('name') or sub['func'].__name__, help=sub['help'])\\n        sp.formatter_class = RawTextHelpFormatter\\n        subcommands = sub.get('subcommands', [])\\n        if subcommands:\\n            sub_subparsers = sp.add_subparsers(dest='subcommand')\\n            sub_subparsers.required = True\\n            for command in subcommands:\\n                cls._add_subcommand(sub_subparsers, command)\\n        else:\\n            for arg in sub['args']:\\n                if 'dag_id' in arg and dag_parser:\\n                    continue\\n                arg = cls.args[arg]\\n                kwargs = {\\n                    f: v\\n                    for f, v in vars(arg).items() if f != 'flags' and v}\\n                sp.add_argument(*arg.flags, **kwargs)\\n            sp.set_defaults(func=sub['func'])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def _add_subcommand(cls, subparsers, sub):\\n        dag_parser = False\\n        sub_proc = subparsers.add_parser(sub.get('name') or sub['func'].__name__, help=sub['help'])\\n        sub_proc.formatter_class = RawTextHelpFormatter\\n        subcommands = sub.get('subcommands', [])\\n        if subcommands:\\n            sub_subparsers = sub_proc.add_subparsers(dest='subcommand')\\n            sub_subparsers.required = True\\n            for command in subcommands:\\n                cls._add_subcommand(sub_subparsers, command)\\n        else:\\n            for arg in sub['args']:\\n                if 'dag_id' in arg and dag_parser:\\n                    continue\\n                arg = cls.args[arg]\\n                kwargs = {\\n                    f: v\\n                    for f, v in vars(arg).items() if f != 'flags' and v}\\n                sub_proc.add_argument(*arg.flags, **kwargs)\\n            sub_proc.set_defaults(func=sub['func'])"
  },
  {
    "code": "def time_get_dummies(self, dtype):\\n        self.s.str.get_dummies(\"|\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_get_dummies(self, dtype):\\n        self.s.str.get_dummies(\"|\")"
  },
  {
    "code": "def get_modified_time(self, name):\\n        raise NotImplementedError('subclasses of Storage must provide a get_modified_time() method')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_modified_time(self, name):\\n        raise NotImplementedError('subclasses of Storage must provide a get_modified_time() method')"
  },
  {
    "code": "def parallel_coordinates(\\n    frame,\\n    class_column,\\n    cols=None,\\n    ax=None,\\n    color=None,\\n    use_columns=False,\\n    xticks=None,\\n    colormap=None,\\n    axvlines=True,\\n    axvlines_kwds=None,\\n    sort_labels=False,\\n    **kwargs\\n):\\n    plot_backend = _get_plot_backend(\"matplotlib\")\\n    return plot_backend.parallel_coordinates(\\n        frame=frame,\\n        class_column=class_column,\\n        cols=cols,\\n        ax=ax,\\n        color=color,\\n        use_columns=use_columns,\\n        xticks=xticks,\\n        colormap=colormap,\\n        axvlines=axvlines,\\n        axvlines_kwds=axvlines_kwds,\\n        sort_labels=sort_labels,\\n        **kwargs\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parallel_coordinates(\\n    frame,\\n    class_column,\\n    cols=None,\\n    ax=None,\\n    color=None,\\n    use_columns=False,\\n    xticks=None,\\n    colormap=None,\\n    axvlines=True,\\n    axvlines_kwds=None,\\n    sort_labels=False,\\n    **kwargs\\n):\\n    plot_backend = _get_plot_backend(\"matplotlib\")\\n    return plot_backend.parallel_coordinates(\\n        frame=frame,\\n        class_column=class_column,\\n        cols=cols,\\n        ax=ax,\\n        color=color,\\n        use_columns=use_columns,\\n        xticks=xticks,\\n        colormap=colormap,\\n        axvlines=axvlines,\\n        axvlines_kwds=axvlines_kwds,\\n        sort_labels=sort_labels,\\n        **kwargs\\n    )"
  },
  {
    "code": "def split_arguments(argstr):\\n    arguments = []\\n    bracket_counts = {'(': 0, '[': 0}\\n    current_argument = []\\n    state = 0\\n    i = 0",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT: Review F401,F841,F842 flake8 errors (unused variables and imports) (#12448)",
    "fixed_code": "def split_arguments(argstr):\\n    arguments = []\\n    current_argument = []\\n    i = 0"
  },
  {
    "code": "def is_quoted(data):\\n    return len(data) > 0 and (data[0] == '\"' and data[-1] == '\"' or data[0] == \"'\" and data[-1] == \"'\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Test unquote works as expected and fix two bugs: * escaped end quote * a single quote character",
    "fixed_code": "def is_quoted(data):\\n    return len(data) > 1 and data[0] == data[-1] and data[0] in ('\"', \"'\") and data[-2] != '\\\\'"
  },
  {
    "code": "def _is_safe_url(url, allowed_hosts, require_https=False):\\n\\tif url.startswith('///'):\\n\\t\\treturn False\\n\\turl_info = _urlparse(url)\\n\\tif not url_info.netloc and url_info.scheme:\\n\\t\\treturn False\\n\\tif unicodedata.category(url[0])[0] == 'C':\\n\\t\\treturn False\\n\\tscheme = url_info.scheme\\n\\tif not url_info.scheme and url_info.netloc:\\n\\t\\tscheme = 'http'\\n\\tvalid_schemes = ['https'] if require_https else ['http', 'https']\\n\\treturn ((not url_info.netloc or url_info.netloc in allowed_hosts) and\\n\\t\\t\\t(not scheme or scheme in valid_schemes))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #28142 -- Fixed is_safe_url() crash on invalid IPv6 URLs.",
    "fixed_code": "def _is_safe_url(url, allowed_hosts, require_https=False):\\n\\tif url.startswith('///'):\\n\\t\\treturn False\\n\\ttry:\\n\\t\\turl_info = _urlparse(url)\\n\\texcept ValueError:  \\n\\t\\treturn False\\n\\tif not url_info.netloc and url_info.scheme:\\n\\t\\treturn False\\n\\tif unicodedata.category(url[0])[0] == 'C':\\n\\t\\treturn False\\n\\tscheme = url_info.scheme\\n\\tif not url_info.scheme and url_info.netloc:\\n\\t\\tscheme = 'http'\\n\\tvalid_schemes = ['https'] if require_https else ['http', 'https']\\n\\treturn ((not url_info.netloc or url_info.netloc in allowed_hosts) and\\n\\t\\t\\t(not scheme or scheme in valid_schemes))"
  },
  {
    "code": "def _to_wide_homogeneous(self, mask):\\n        values = np.empty(self.shape, dtype=self.values.dtype)\\n        if not issubclass(self.values.dtype.type, np.integer):\\n            values.fill(np.nan)\\n        for i in xrange(len(self.items)):\\n            values[i].flat[mask] = self.values[:, i]\\n        return WidePanel(values, self.items, self.major_axis, self.minor_axis)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: more docs work on tutorial",
    "fixed_code": "def _to_wide_homogeneous(self, mask):\\n        values = np.empty(self.wide_shape, dtype=self.values.dtype)\\n        if not issubclass(self.values.dtype.type, np.integer):\\n            values.fill(np.nan)\\n        for i in xrange(len(self.items)):\\n            values[i].flat[mask] = self.values[:, i]\\n        return WidePanel(values, self.items, self.major_axis, self.minor_axis)"
  },
  {
    "code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.filename = filename\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        if self.mode == WRITE:\\n            self._write_gzip_header()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.filename = filename\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        if self.mode == WRITE:\\n            self._write_gzip_header()"
  },
  {
    "code": "def can_handle_archive(self):\\n        if not self.cmd_path:\\n            return False, 'Command \"unzip\" not found.'\\n        cmd = [self.cmd_path, '-l', self.src]\\n        rc, out, err = self.module.run_command(cmd)\\n        if rc == 0:\\n            return True, None\\n        return False, 'Command \"%s\" could not handle archive.' % self.cmd_path",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unarchive - do not fail in init when trying to find required binary (#74892)\\n\\nTest for the required binaries in the can_handle_archive() method and fail there. This\\nprevents failures for missing binaries unrelated to the archive type.",
    "fixed_code": "def can_handle_archive(self):\\n        binaries = (\\n            ('unzip', 'cmd_path'),\\n            ('zipinfo', 'zipinfo_cmd_path'),\\n        )\\n        missing = []\\n        for b in binaries:\\n            try:\\n                setattr(self, b[1], get_bin_path(b[0]))\\n            except ValueError:\\n                missing.append(b[0])\\n        if missing:\\n            return False, \"Unable to find required '{missing}' binary in the path.\".format(missing=\"' or '\".join(missing))\\n        cmd = [self.cmd_path, '-l', self.src]\\n        rc, out, err = self.module.run_command(cmd)\\n        if rc == 0:\\n            return True, None\\n        return False, 'Command \"%s\" could not handle archive.' % self.cmd_path"
  },
  {
    "code": "def __init__(self, n_atoms, transform_method='omp', split_sign=False):\\n        self.n_atoms = n_atoms\\n        self.transform_method = transform_method\\n        self.split_sign = split_sign",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "rename coding_method, transform_method to fit/transform_algorithm",
    "fixed_code": "def __init__(self, n_atoms, transform_algorithm='omp', split_sign=False):\\n        self.n_atoms = n_atoms\\n        self.transform_algorithm = transform_algorithm\\n        self.split_sign = split_sign"
  },
  {
    "code": "def read_table(filepath_or_buffer, sep='\\t', header=0, index_col=None,\\n               names=None, skiprows=None, na_values=None, parse_dates=False,\\n               date_parser=None, nrows=None, iterator=False, chunksize=None,\\n               skip_footer=0, converters=None, verbose=False, delimiter=None):\\n    return read_csv(filepath_or_buffer, sep=sep, header=header,\\n                    skiprows=skiprows, index_col=index_col,\\n                    na_values=na_values, date_parser=date_parser,\\n                    names=names, parse_dates=parse_dates,\\n                    nrows=nrows, iterator=iterator, chunksize=chunksize,\\n                    skip_footer=skip_footer, converters=converters,\\n                    verbose=verbose, delimiter=delimiter)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_table(filepath_or_buffer, sep='\\t', header=0, index_col=None,\\n               names=None, skiprows=None, na_values=None, parse_dates=False,\\n               date_parser=None, nrows=None, iterator=False, chunksize=None,\\n               skip_footer=0, converters=None, verbose=False, delimiter=None):\\n    return read_csv(filepath_or_buffer, sep=sep, header=header,\\n                    skiprows=skiprows, index_col=index_col,\\n                    na_values=na_values, date_parser=date_parser,\\n                    names=names, parse_dates=parse_dates,\\n                    nrows=nrows, iterator=iterator, chunksize=chunksize,\\n                    skip_footer=skip_footer, converters=converters,\\n                    verbose=verbose, delimiter=delimiter)"
  },
  {
    "code": "def get_field_type(self, data_type, description):\\n        if data_type == cx_Oracle.NUMBER:\\n            precision, scale = description[4:6]\\n            if scale == 0:\\n                if precision > 11:\\n                    return 'BigIntegerField'\\n                elif precision == 1:\\n                    return 'BooleanField'\\n                else:\\n                    return 'IntegerField'\\n            elif scale == -127:\\n                return 'FloatField'\\n        return super(DatabaseIntrospection, self).get_field_type(data_type, description)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_field_type(self, data_type, description):\\n        if data_type == cx_Oracle.NUMBER:\\n            precision, scale = description[4:6]\\n            if scale == 0:\\n                if precision > 11:\\n                    return 'BigIntegerField'\\n                elif precision == 1:\\n                    return 'BooleanField'\\n                else:\\n                    return 'IntegerField'\\n            elif scale == -127:\\n                return 'FloatField'\\n        return super(DatabaseIntrospection, self).get_field_type(data_type, description)"
  },
  {
    "code": "def handle_command_def(self,line):\\n        cmd, arg, line = self.parseline(line)\\n        if not cmd:\\n            return\\n        if cmd == 'silent':\\n            self.commands_silent[self.commands_bnum] = True\\n            return \\n        elif cmd == 'end':\\n            self.cmdqueue = []\\n            return 1 \\n        cmdlist = self.commands[self.commands_bnum]\\n        if arg:\\n            cmdlist.append(cmd+' '+arg)\\n        else:\\n            cmdlist.append(cmd)\\n        try:\\n            func = getattr(self, 'do_' + cmd)\\n        except AttributeError:\\n            func = self.default\\n        if func.func_name in self.commands_resuming:\\n            self.commands_doprompt[self.commands_bnum] = False\\n            self.cmdqueue = []\\n            return 1\\n        return\\n    do_h = cmd.Cmd.do_help",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def handle_command_def(self,line):\\n        cmd, arg, line = self.parseline(line)\\n        if not cmd:\\n            return\\n        if cmd == 'silent':\\n            self.commands_silent[self.commands_bnum] = True\\n            return \\n        elif cmd == 'end':\\n            self.cmdqueue = []\\n            return 1 \\n        cmdlist = self.commands[self.commands_bnum]\\n        if arg:\\n            cmdlist.append(cmd+' '+arg)\\n        else:\\n            cmdlist.append(cmd)\\n        try:\\n            func = getattr(self, 'do_' + cmd)\\n        except AttributeError:\\n            func = self.default\\n        if func.func_name in self.commands_resuming:\\n            self.commands_doprompt[self.commands_bnum] = False\\n            self.cmdqueue = []\\n            return 1\\n        return\\n    do_h = cmd.Cmd.do_help"
  },
  {
    "code": "def _dt_unbox(key):\\n    ''\\n    if not isinstance(key, datetime):\\n        key = to_timestamp(key)\\n    return np.datetime64(lib.pydt_to_i8(key))\\ndef _dt_unbox_array(arr):\\n    if arr is None:\\n        return arr\\n    unboxer = np.frompyfunc(_dt_unbox, 1, 1)\\n    return unboxer(arr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _dt_unbox(key):\\n    ''\\n    if not isinstance(key, datetime):\\n        key = to_timestamp(key)\\n    return np.datetime64(lib.pydt_to_i8(key))\\ndef _dt_unbox_array(arr):\\n    if arr is None:\\n        return arr\\n    unboxer = np.frompyfunc(_dt_unbox, 1, 1)\\n    return unboxer(arr)"
  },
  {
    "code": "def pop(self, key, *args):\\n        return self._session.pop(key, *args)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #4729 -- Restored functionality to the Session class so that popping a value marks it as modified. This was accidentally lost in the changes in [6333]. Thanks, __hawkeye__.",
    "fixed_code": "def pop(self, key, *args):\\n        self.modified = self.modified or key in self._session\\n        return self._session.pop(key, *args)"
  },
  {
    "code": "def main():\\n    hashed_password_spec = dict(\\n        type=dict(type='int', required=True),\\n        value=dict(no_log=True, required=True)\\n    )\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        hashed_password=dict(no_log=True, type='dict', options=hashed_password_spec),\\n        nopassword=dict(type='bool'),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        password_type=dict(default='secret', choices=['secret', 'password']),\\n        privilege=dict(type='int'),\\n        view=dict(aliases=['role']),\\n        sshkey=dict(),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection']),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'aggregate'), ('nopassword', 'hashed_password', 'configured_password')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    if module.params['password'] and not module.params['configured_password']:\\n        warnings.append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_users = [x['name'] for x in want]\\n        have_users = [x['name'] for x in have]\\n        for item in set(have_users).difference(want_users):\\n            if item != 'admin':\\n                commands.append(user_del_cmd(item))\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ios_user: Add support for multiple sshkeys (#51173)",
    "fixed_code": "def main():\\n    hashed_password_spec = dict(\\n        type=dict(type='int', required=True),\\n        value=dict(no_log=True, required=True)\\n    )\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        hashed_password=dict(no_log=True, type='dict', options=hashed_password_spec),\\n        nopassword=dict(type='bool'),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        password_type=dict(default='secret', choices=['secret', 'password']),\\n        privilege=dict(type='int'),\\n        view=dict(aliases=['role']),\\n        sshkey=dict(type='list'),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection']),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'aggregate'), ('nopassword', 'hashed_password', 'configured_password')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    if module.params['password'] and not module.params['configured_password']:\\n        warnings.append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_users = [x['name'] for x in want]\\n        have_users = [x['name'] for x in have]\\n        for item in set(have_users).difference(want_users):\\n            if item != 'admin':\\n                commands.append(user_del_cmd(item))\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    module.exit_json(**result)"
  },
  {
    "code": "def cv_estimate(n_folds=3):\\n    cv = KFold(n_folds=n_folds)\\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\\n    for train, test in cv.split(X_train, y_train):\\n        cv_clf.fit(X_train[train], y_train[train])\\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\\n    val_scores /= n_folds\\n    return val_scores",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cv_estimate(n_folds=3):\\n    cv = KFold(n_folds=n_folds)\\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\\n    for train, test in cv.split(X_train, y_train):\\n        cv_clf.fit(X_train[train], y_train[train])\\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\\n    val_scores /= n_folds\\n    return val_scores"
  },
  {
    "code": "def _generate_regular_range(start, end, periods, offset):\\n    if isinstance(offset, Tick):\\n        stride = offset.nanos\\n        if periods is None:\\n            b = Timestamp(start).value\\n            e = Timestamp(end).value\\n            e += stride - e % stride\\n            tz = start.tz\\n        elif start is not None:\\n            b = Timestamp(start).value\\n            e = b + np.int64(periods) * stride\\n            tz = start.tz\\n        elif end is not None:\\n            e = Timestamp(end).value + stride\\n            b = e - np.int64(periods) * stride\\n            tz = end.tz\\n        else:\\n            raise NotImplementedError\\n        data = np.arange(b, e, stride, dtype=np.int64)\\n        data = DatetimeIndex._simple_new(data, None, tz=tz)\\n    else:\\n        if isinstance(start, Timestamp):\\n            start = start.to_pydatetime()\\n        if isinstance(end, Timestamp):\\n            end = end.to_pydatetime()\\n        xdr = generate_range(start=start, end=end,\\n                             periods=periods, offset=offset)\\n        dates = list(xdr)\\n        data = tools.to_datetime(dates)\\n    return data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _generate_regular_range(start, end, periods, offset):\\n    if isinstance(offset, Tick):\\n        stride = offset.nanos\\n        if periods is None:\\n            b = Timestamp(start).value\\n            e = Timestamp(end).value\\n            e += stride - e % stride\\n            tz = start.tz\\n        elif start is not None:\\n            b = Timestamp(start).value\\n            e = b + np.int64(periods) * stride\\n            tz = start.tz\\n        elif end is not None:\\n            e = Timestamp(end).value + stride\\n            b = e - np.int64(periods) * stride\\n            tz = end.tz\\n        else:\\n            raise NotImplementedError\\n        data = np.arange(b, e, stride, dtype=np.int64)\\n        data = DatetimeIndex._simple_new(data, None, tz=tz)\\n    else:\\n        if isinstance(start, Timestamp):\\n            start = start.to_pydatetime()\\n        if isinstance(end, Timestamp):\\n            end = end.to_pydatetime()\\n        xdr = generate_range(start=start, end=end,\\n                             periods=periods, offset=offset)\\n        dates = list(xdr)\\n        data = tools.to_datetime(dates)\\n    return data"
  },
  {
    "code": "def __init__(self, expression, delimiter, **extra):\\n        delimiter_expr = Value(str(delimiter))\\n        super().__init__(expression, delimiter_expr, **extra)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #10929 -- Deprecated forced empty result value for PostgreSQL aggregates.\\n\\nThis deprecates forcing a return value for ArrayAgg, JSONBAgg, and\\nStringAgg when there are no rows in the query. Now that we have a\\n``default`` argument for aggregates, we want to revert to returning the\\ndefault of ``None`` which most aggregate functions return and leave it\\nup to the user to decide what they want to be returned by default.",
    "fixed_code": "def __init__(self, *expressions, default=NOT_PROVIDED, **extra):\\n        if default is NOT_PROVIDED:\\n            default = None\\n            self._default_provided = False\\n        else:\\n            self._default_provided = True\\n        super().__init__(*expressions, default=default, **extra)"
  },
  {
    "code": "def cpu_count():\\n    ''\\n    if sys.platform == 'win32':\\n        try:\\n            num = int(os.environ['NUMBER_OF_PROCESSORS'])\\n        except (ValueError, KeyError):\\n            num = 0\\n    elif 'bsd' in sys.platform or sys.platform == 'darwin':\\n        try:\\n            with os.popen('sysctl -n hw.ncpu') as p:\\n                num = int(p.read())\\n        except ValueError:\\n            num = 0\\n    else:\\n        try:\\n            num = os.sysconf('SC_NPROCESSORS_ONLN')\\n        except (ValueError, OSError, AttributeError):\\n            num = 0\\n    if num >= 1:\\n        return num\\n    else:\\n        raise NotImplementedError('cannot determine number of cpus')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cpu_count():\\n    ''\\n    if sys.platform == 'win32':\\n        try:\\n            num = int(os.environ['NUMBER_OF_PROCESSORS'])\\n        except (ValueError, KeyError):\\n            num = 0\\n    elif 'bsd' in sys.platform or sys.platform == 'darwin':\\n        try:\\n            with os.popen('sysctl -n hw.ncpu') as p:\\n                num = int(p.read())\\n        except ValueError:\\n            num = 0\\n    else:\\n        try:\\n            num = os.sysconf('SC_NPROCESSORS_ONLN')\\n        except (ValueError, OSError, AttributeError):\\n            num = 0\\n    if num >= 1:\\n        return num\\n    else:\\n        raise NotImplementedError('cannot determine number of cpus')"
  },
  {
    "code": "def _clean_na_values(na_values, keep_default_na=True):\\n    if na_values is None:\\n        if keep_default_na:\\n            na_values = _NA_VALUES\\n        else:\\n            na_values = set()\\n        na_fvalues = set()\\n    elif isinstance(na_values, dict):\\n        old_na_values = na_values.copy()\\n        na_values = {}  \\n        for k, v in old_na_values.items():\\n            if not is_list_like(v):\\n                v = [v]\\n            if keep_default_na:\\n                v = set(v) | _NA_VALUES\\n            na_values[k] = v\\n        na_fvalues = {k: _floatify_na_values(v) for k, v in na_values.items()}\\n    else:\\n        if not is_list_like(na_values):\\n            na_values = [na_values]\\n        na_values = _stringify_na_values(na_values)\\n        if keep_default_na:\\n            na_values = na_values | _NA_VALUES\\n        na_fvalues = _floatify_na_values(na_values)\\n    return na_values, na_fvalues",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: define NA_VALUES in libparsers (#30373)",
    "fixed_code": "def _clean_na_values(na_values, keep_default_na=True):\\n    if na_values is None:\\n        if keep_default_na:\\n            na_values = STR_NA_VALUES\\n        else:\\n            na_values = set()\\n        na_fvalues = set()\\n    elif isinstance(na_values, dict):\\n        old_na_values = na_values.copy()\\n        na_values = {}  \\n        for k, v in old_na_values.items():\\n            if not is_list_like(v):\\n                v = [v]\\n            if keep_default_na:\\n                v = set(v) | STR_NA_VALUES\\n            na_values[k] = v\\n        na_fvalues = {k: _floatify_na_values(v) for k, v in na_values.items()}\\n    else:\\n        if not is_list_like(na_values):\\n            na_values = [na_values]\\n        na_values = _stringify_na_values(na_values)\\n        if keep_default_na:\\n            na_values = na_values | STR_NA_VALUES\\n        na_fvalues = _floatify_na_values(na_values)\\n    return na_values, na_fvalues"
  },
  {
    "code": "def shift(self, type: int, value: Text, newstate: int, context: Context) -> None:\\n        dfa, state, node = self.stack[-1]\\n        rawnode: RawNode = (type, value, context, None)\\n        newnode = convert(self.grammar, rawnode)\\n        assert node[-1] is not None\\n        node[-1].append(newnode)\\n        self.stack[-1] = (dfa, newstate, node)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Speed up new backtracking parser (#2728)",
    "fixed_code": "def shift(self, type: int, value: Text, newstate: int, context: Context) -> None:\\n        if self.is_backtracking:\\n            dfa, state, _ = self.stack[-1]\\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\\n        else:\\n            dfa, state, node = self.stack[-1]\\n            rawnode: RawNode = (type, value, context, None)\\n            newnode = convert(self.grammar, rawnode)\\n            assert node[-1] is not None\\n            node[-1].append(newnode)\\n            self.stack[-1] = (dfa, newstate, node)"
  },
  {
    "code": "def get_conn(self) -> Connection:\\n\\t\\tconn_md = self.get_connection(getattr(self, self.conn_name_attr))\\n\\t\\tcreds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\\n\\t\\tif \"/\" in conn_md.host or \"&\" in conn_md.host:\\n\\t\\t\\traise ValueError(\"Drill host should not contain '/&' characters\")\\n\\t\\tengine = create_engine(\\n\\t\\t\\tf'{conn_md.extra_dejson.get(\"dialect_driver\", \"drill+sadrill\")}://{creds}'\\n\\t\\t\\tf\"{conn_md.host}:{conn_md.port}/\"\\n\\t\\t\\tf'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\\n\\t\\t)\\n\\t\\tself.log.info(\\n\\t\\t\\t\"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\\n\\t\\t)\\n\\t\\treturn engine.raw_connection()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Validate database URL passed to create_engine of Drill hook's connection (#33074)\\n\\nThe database URL passed as an argument to the create_engine should\\nnot contain query parameters as it is not intended.",
    "fixed_code": "def get_conn(self) -> Connection:\\n\\t\\tconn_md = self.get_connection(getattr(self, self.conn_name_attr))\\n\\t\\tcreds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\\n\\t\\tdatabase_url = (\\n\\t\\t\\tf\"{conn_md.extra_dejson.get('dialect_driver', 'drill+sadrill')}://{creds}\"\\n\\t\\t\\tf\"{conn_md.host}:{conn_md.port}/\"\\n\\t\\t\\tf'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\\n\\t\\t)\\n\\t\\tif \"?\" in database_url:\\n\\t\\t\\traise ValueError(\"Drill database_url should not contain a '?'\")\\n\\t\\tengine = create_engine(database_url)\\n\\t\\tself.log.info(\\n\\t\\t\\t\"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\\n\\t\\t)\\n\\t\\treturn engine.raw_connection()"
  },
  {
    "code": "def json_dumps(value, indent=None):\\n\\tif isinstance(value, QuerySet):\\n\\t\\tresult = serialize('json', value, indent=indent)\\n\\telse:\\n\\t\\tresult = simplejson.dumps(value, indent=indent)\\n\\treturn mark_safe(result)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix a XSS vulnerability with bad input to json_dumps.\\n\\nDjango's JSON serialization does not handle escaping of any characters\\nto make them safe for injecting into HTML. This allows an attacker who\\ncan provide part of a JSON-serializable object to craft a string that\\ncan break out of a <script> tag and create its own, injecting a custom\\nscript.\\n\\nTo fix this, we escape '<', '>', and '&' characters in the resulting\\nstring, preventing a </script> from executing.",
    "fixed_code": "def json_dumps(value, indent=None):\\n\\tif isinstance(value, QuerySet):\\n\\t\\tresult = serialize('json', value, indent=indent)\\n\\telse:\\n\\t\\tresult = simplejson.dumps(value, indent=indent)\\n\\treturn mark_safe(force_text(result).translate(_safe_js_escapes))"
  },
  {
    "code": "def arith_method_FRAME(cls: Type[\"DataFrame\"], op, special: bool):\\n    op_name = _get_op_name(op, special)\\n    default_axis = _get_frame_op_default_axis(op_name)\\n    na_op = get_array_op(op)\\n    if op_name in _op_descriptions:\\n        doc = _make_flex_doc(op_name, \"dataframe\")\\n    else:\\n        doc = _arith_doc_FRAME % op_name\\n    @Appender(doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: make DataFrame.__boolop__ default_axis match DataFrame.__arithop__ default_axis (#36793)",
    "fixed_code": "def arith_method_FRAME(cls: Type[\"DataFrame\"], op, special: bool):\\n    op_name = _get_op_name(op, special)\\n    default_axis = None if special else \"columns\"\\n    na_op = get_array_op(op)\\n    if op_name in _op_descriptions:\\n        doc = _make_flex_doc(op_name, \"dataframe\")\\n    else:\\n        doc = _arith_doc_FRAME % op_name\\n    @Appender(doc)"
  },
  {
    "code": "def __init__(self, required=True, widget=None, label=None):\\n        if label is not None:\\n            label = smart_unicode(label)\\n        self.required, self.label = required, label\\n        widget = widget or self.widget\\n        if isinstance(widget, type):\\n            widget = widget()\\n        extra_attrs = self.widget_attrs(widget)\\n        if extra_attrs:\\n            widget.attrs.update(extra_attrs)\\n        self.widget = widget\\n        self.creation_counter = Field.creation_counter\\n        Field.creation_counter += 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, required=True, widget=None, label=None):\\n        if label is not None:\\n            label = smart_unicode(label)\\n        self.required, self.label = required, label\\n        widget = widget or self.widget\\n        if isinstance(widget, type):\\n            widget = widget()\\n        extra_attrs = self.widget_attrs(widget)\\n        if extra_attrs:\\n            widget.attrs.update(extra_attrs)\\n        self.widget = widget\\n        self.creation_counter = Field.creation_counter\\n        Field.creation_counter += 1"
  },
  {
    "code": "def pprint(object, stream=None, indent=1, width=80, depth=None):\\n    printer = PrettyPrinter(\\n        stream=stream, indent=indent, width=width, depth=depth)\\n    printer.pprint(object)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #19132: The pprint module now supports compact mode.",
    "fixed_code": "def pprint(object, stream=None, indent=1, width=80, depth=None, *,\\n           compact=False):\\n    printer = PrettyPrinter(\\n        stream=stream, indent=indent, width=width, depth=depth,\\n        compact=compact)\\n    printer.pprint(object)"
  },
  {
    "code": "def rename_vserver(self):\\n        vserver_rename = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'vserver-rename', **{'vserver-name': self.parameters['from_name'],\\n                                 'new-name': self.parameters['name']})\\n        try:\\n            self.server.invoke_successfully(vserver_rename,\\n                                            enable_tunneling=False)\\n        except netapp_utils.zapi.NaApiError as e:\\n            self.module.fail_json(msg='Error renaming SVM %s: %s'\\n                                  % (self.parameters['from_name'], to_native(e)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rename_vserver(self):\\n        vserver_rename = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'vserver-rename', **{'vserver-name': self.parameters['from_name'],\\n                                 'new-name': self.parameters['name']})\\n        try:\\n            self.server.invoke_successfully(vserver_rename,\\n                                            enable_tunneling=False)\\n        except netapp_utils.zapi.NaApiError as e:\\n            self.module.fail_json(msg='Error renaming SVM %s: %s'\\n                                  % (self.parameters['from_name'], to_native(e)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def __getattr__(self, name):\\n        file = self.__dict__['file']\\n        a = getattr(file, name)\\n        if type(a) != type(0):\\n            setattr(self, name, a)\\n        return a\\n    if _os.name != 'nt':\\n        unlink = _os.unlink",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Backport relevant part of issue 2021 fix (r60695): Support with statement properly in tempfile.NamedTemporaryFile",
    "fixed_code": "def __getattr__(self, name):\\n        file = self.__dict__['file']\\n        a = getattr(file, name)\\n        if type(a) != type(0):\\n            setattr(self, name, a)\\n        return a"
  },
  {
    "code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            model_list = self._get_models_cache[cache_key]\\n            if self.available_apps is not None and only_installed:\\n                model_list = [\\n                    m for m in model_list\\n                    if self.app_configs[m._meta.app_label].name in self.available_apps\\n                ]\\n            return model_list\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        if self.available_apps is not None and only_installed:\\n            model_list = [\\n                m for m in model_list\\n                if self.app_configs[m._meta.app_label].name in self.available_apps\\n            ]\\n        return model_list",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactored INSTALLED_APPS overrides.\\n\\n  INSTALLED_APPS setting.\\n  with [un]set_installed_apps.\\n  this situation with public methods of the new app cache.",
    "fixed_code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            return self._get_models_cache[cache_key]\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        return model_list"
  },
  {
    "code": "def quote_value(self, value):\\n        if isinstance(value, (datetime.date, datetime.time, datetime.datetime)):\\n            return \"'%s'\" % value\\n        elif isinstance(value, str):\\n            return \"'%s'\" % value.replace(\"\\'\", \"\\'\\'\").replace('%', '%%')\\n        elif isinstance(value, (bytes, bytearray, memoryview)):\\n            return \"'%s'\" % value.hex()\\n        elif isinstance(value, bool):\\n            return \"1\" if value else \"0\"\\n        else:\\n            return str(value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #33358 -- Fixed handling timedelta < 1 day in schema operations on Oracle.",
    "fixed_code": "def quote_value(self, value):\\n        if isinstance(value, (datetime.date, datetime.time, datetime.datetime)):\\n            return \"'%s'\" % value\\n        elif isinstance(value, datetime.timedelta):\\n            return \"'%s'\" % duration_iso_string(value)\\n        elif isinstance(value, str):\\n            return \"'%s'\" % value.replace(\"\\'\", \"\\'\\'\").replace('%', '%%')\\n        elif isinstance(value, (bytes, bytearray, memoryview)):\\n            return \"'%s'\" % value.hex()\\n        elif isinstance(value, bool):\\n            return \"1\" if value else \"0\"\\n        else:\\n            return str(value)"
  },
  {
    "code": "def match(self, pat, case=True, flags=0, na=np.nan, as_indexer=None):\\n        result = str_match(self._parent, pat, case=case, flags=flags, na=na,\\n                           as_indexer=as_indexer)\\n        return self._wrap_result(result)\\n    @copy(str_replace)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN/DEPR: removed deprecated as_indexer arg from str.match() (#22626)",
    "fixed_code": "def match(self, pat, case=True, flags=0, na=np.nan):\\n        result = str_match(self._parent, pat, case=case, flags=flags, na=na)\\n        return self._wrap_result(result)\\n    @copy(str_replace)"
  },
  {
    "code": "def _build_vectors_and_vocab(self, raw_documents):\\n        term_counts_per_doc = []\\n        term_counts = {}\\n        document_counts = {}\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_dict = {}  \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n                term_counts[term] = term_counts.get(term, 0) + 1\\n            if max_df is not None:\\n                for term in term_count_dict.iterkeys():\\n                    document_counts[term] = document_counts.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        n_doc = len(term_counts_per_doc)\\n        stop_words = set()\\n        if max_df is not None:\\n            max_document_count = max_df * n_doc\\n            for t, dc in sorted(document_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if dc <= max_document_count:\\n                    break\\n                stop_words.add(t)\\n        if max_features is not None:\\n            terms = set()\\n            for t, tc in sorted(term_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        else:\\n            terms = set(term_counts.keys())\\n            terms -= stop_words\\n        vocabulary = dict(((t, i) for i, t in enumerate(terms)))  \\n        matrix = self._term_count_dicts_to_matrix(term_counts_per_doc, vocabulary)\\n        return matrix, vocabulary",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_vectors_and_vocab(self, raw_documents):\\n        term_counts_per_doc = []\\n        term_counts = {}\\n        document_counts = {}\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_dict = {}  \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n                term_counts[term] = term_counts.get(term, 0) + 1\\n            if max_df is not None:\\n                for term in term_count_dict.iterkeys():\\n                    document_counts[term] = document_counts.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        n_doc = len(term_counts_per_doc)\\n        stop_words = set()\\n        if max_df is not None:\\n            max_document_count = max_df * n_doc\\n            for t, dc in sorted(document_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if dc <= max_document_count:\\n                    break\\n                stop_words.add(t)\\n        if max_features is not None:\\n            terms = set()\\n            for t, tc in sorted(term_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        else:\\n            terms = set(term_counts.keys())\\n            terms -= stop_words\\n        vocabulary = dict(((t, i) for i, t in enumerate(terms)))  \\n        matrix = self._term_count_dicts_to_matrix(term_counts_per_doc, vocabulary)\\n        return matrix, vocabulary"
  },
  {
    "code": "def text_to_word_sequence(text,\\n\\t\\t\\t\\t\\t\\t  filters='!\"\\n',\\n\\t\\t\\t\\t\\t\\t  lower=True, split=\" \"):\\n\\tif lower:\\n\\t\\ttext = text.lower()\\n\\tif sys.version_info < (3,) and isinstance(text, unicode):\\n\\t\\ttranslate_map = dict((ord(c), unicode(split)) for c in filters)\\n\\telse:\\n\\t\\ttranslate_map = maketrans(filters, split * len(filters))\\n\\ttext = text.translate(translate_map)\\n\\tseq = text.split(split)\\n\\treturn [i for i in seq if i]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Correct tokenization with multi-character `split` (#9585)",
    "fixed_code": "def text_to_word_sequence(text,\\n\\t\\t\\t\\t\\t\\t  filters='!\"\\n',\\n\\t\\t\\t\\t\\t\\t  lower=True, split=\" \"):\\n\\tif lower:\\n\\t\\ttext = text.lower()\\n\\tif sys.version_info < (3,):\\n\\t\\tif isinstance(text, unicode):\\n\\t\\t\\ttranslate_map = dict((ord(c), unicode(split)) for c in filters)\\n\\t\\t\\ttext = text.translate(translate_map)\\n\\t\\telif len(split) == 1:\\n\\t\\t\\ttranslate_map = maketrans(filters, split * len(filters))\\n\\t\\t\\ttext = text.translate(translate_map)\\n\\t\\telse:\\n\\t\\t\\tfor c in filters:\\n\\t\\t\\t\\ttext = text.replace(c, split)\\n\\telse:\\n\\t\\ttranslate_dict = dict((c, split) for c in filters)\\n\\t\\ttranslate_map = maketrans(translate_dict)\\n\\t\\ttext = text.translate(translate_map)\\n\\tseq = text.split(split)\\n\\treturn [i for i in seq if i]"
  },
  {
    "code": "def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\\n    ''\\n    from ansible.playbook.block import Block\\n    from ansible.playbook.handler import Handler\\n    from ansible.playbook.task import Task\\n    from ansible.playbook.task_include import TaskInclude\\n    from ansible.playbook.role_include import IncludeRole\\n    from ansible.playbook.handler_task_include import HandlerTaskInclude\\n    from ansible.template import Templar\\n    from ansible.utils.plugin_docs import get_versioned_doclink\\n    if not isinstance(ds, list):\\n        raise AnsibleAssertionError('The ds (%s) should be a list but was a %s' % (ds, type(ds)))\\n    task_list = []\\n    for task_ds in ds:\\n        if not isinstance(task_ds, dict):\\n            raise AnsibleAssertionError('The ds (%s) should be a dict but was a %s' % (ds, type(ds)))\\n        if 'block' in task_ds:\\n            t = Block.load(\\n                task_ds,\\n                play=play,\\n                parent_block=block,\\n                role=role,\\n                task_include=task_include,\\n                use_handlers=use_handlers,\\n                variable_manager=variable_manager,\\n                loader=loader,\\n            )\\n            task_list.append(t)\\n        else:\\n            args_parser = ModuleArgsParser(task_ds)\\n            try:\\n                (action, args, delegate_to) = args_parser.parse(skip_action_validation=True)\\n            except AnsibleParserError as e:\\n                if e.obj:\\n                    raise\\n                raise AnsibleParserError(to_native(e), obj=task_ds, orig_exc=e)\\n            if action in C._ACTION_ALL_INCLUDE_IMPORT_TASKS:\\n                if use_handlers:\\n                    include_class = HandlerTaskInclude\\n                else:\\n                    include_class = TaskInclude\\n                t = include_class.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader\\n                )\\n                all_vars = variable_manager.get_vars(play=play, task=t)\\n                templar = Templar(loader=loader, variables=all_vars)\\n                if action in C._ACTION_INCLUDE_TASKS:\\n                    is_static = False\\n                elif action in C._ACTION_IMPORT_TASKS:\\n                    is_static = True\\n                else:\\n                    include_link = get_versioned_doclink('user_guide/playbooks_reuse_includes.html')\\n                    display.deprecated('\"include\" is deprecated, use include_tasks/import_tasks instead. See %s for details' % include_link, \"2.16\")\\n                    is_static = not templar.is_template(t.args['_raw_params']) and t.all_parents_static() and not t.loop\\n                if is_static:\\n                    if t.loop is not None:\\n                        if action in C._ACTION_IMPORT_TASKS:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_tasks' statements. You should use 'include_tasks' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include with a loop\", obj=task_ds)\\n                    t.statically_loaded = True\\n                    parent_include = block\\n                    cumulative_path = None\\n                    found = False\\n                    subdir = 'tasks'\\n                    if use_handlers:\\n                        subdir = 'handlers'\\n                    while parent_include is not None:\\n                        if not isinstance(parent_include, TaskInclude):\\n                            parent_include = parent_include._parent\\n                            continue\\n                        try:\\n                            parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\\n                        except AnsibleUndefinedVariable as e:\\n                            if not parent_include.statically_loaded:\\n                                raise AnsibleParserError(\\n                                    \"Error when evaluating variable in dynamic parent include path: %s. \"\\n                                    \"When using static imports, the parent dynamic include cannot utilize host facts \"\\n                                    \"or variables from inventory\" % parent_include.args.get('_raw_params'),\\n                                    obj=task_ds,\\n                                    suppress_extended_error=True,\\n                                    orig_exc=e\\n                                )\\n                            raise\\n                        if cumulative_path is None:\\n                            cumulative_path = parent_include_dir\\n                        elif not os.path.isabs(cumulative_path):\\n                            cumulative_path = os.path.join(parent_include_dir, cumulative_path)\\n                        include_target = templar.template(t.args['_raw_params'])\\n                        if t._role:\\n                            new_basedir = os.path.join(t._role._role_path, subdir, cumulative_path)\\n                            include_file = loader.path_dwim_relative(new_basedir, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim_relative(loader.get_basedir(), cumulative_path, include_target)\\n                        if os.path.exists(include_file):\\n                            found = True\\n                            break\\n                        else:\\n                            parent_include = parent_include._parent\\n                    if not found:\\n                        try:\\n                            include_target = templar.template(t.args['_raw_params'])\\n                        except AnsibleUndefinedVariable as e:\\n                            raise AnsibleParserError(\\n                                \"Error when evaluating variable in import path: %s.\\n\"\\n                                \"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\\n                                \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\\n                                \"sources like group or host vars.\" % t.args['_raw_params'],\\n                                obj=task_ds,\\n                                suppress_extended_error=True,\\n                                orig_exc=e)\\n                        if t._role:\\n                            include_file = loader.path_dwim_relative(t._role._role_path, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim(include_target)\\n                    data = loader.load_from_file(include_file)\\n                    if not data:\\n                        display.warning('file %s is empty and had no tasks to include' % include_file)\\n                        continue\\n                    elif not isinstance(data, list):\\n                        raise AnsibleParserError(\"included task files must contain a list of tasks\", obj=data)\\n                    display.vv(\"statically imported: %s\" % include_file)\\n                    ti_copy = t.copy(exclude_parent=True)\\n                    ti_copy._parent = block\\n                    included_blocks = load_list_of_blocks(\\n                        data,\\n                        play=play,\\n                        parent_block=None,\\n                        task_include=ti_copy,\\n                        role=role,\\n                        use_handlers=use_handlers,\\n                        loader=loader,\\n                        variable_manager=variable_manager,\\n                    )\\n                    tags = ti_copy.tags[:]\\n                    for b in included_blocks:\\n                        b.tags = list(set(b.tags).union(tags))\\n                    if use_handlers:\\n                        for b in included_blocks:\\n                            task_list.extend(b.block)\\n                    else:\\n                        task_list.extend(included_blocks)\\n                else:\\n                    t.is_static = False\\n                    task_list.append(t)\\n            elif action in C._ACTION_ALL_PROPER_INCLUDE_IMPORT_ROLES:\\n                if use_handlers:\\n                    raise AnsibleParserError(f\"Using '{action}' as a handler is not supported.\", obj=task_ds)\\n                ir = IncludeRole.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader,\\n                )\\n                is_static = False\\n                if action in C._ACTION_IMPORT_ROLE:\\n                    is_static = True\\n                if is_static:\\n                    if ir.loop is not None:\\n                        if action in C._ACTION_IMPORT_ROLE:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_role' statements. You should use 'include_role' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include_role with a loop\", obj=task_ds)\\n                    ir.statically_loaded = True\\n                    all_vars = variable_manager.get_vars(play=play, task=ir)\\n                    templar = Templar(loader=loader, variables=all_vars)\\n                    ir._role_name = templar.template(ir._role_name)\\n                    blocks, _ = ir.get_block_list(variable_manager=variable_manager, loader=loader)\\n                    task_list.extend(blocks)\\n                else:\\n                    task_list.append(ir)\\n            else:\\n                if use_handlers:\\n                    t = Handler.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                else:\\n                    t = Task.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                task_list.append(t)\\n    return task_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\\n    ''\\n    from ansible.playbook.block import Block\\n    from ansible.playbook.handler import Handler\\n    from ansible.playbook.task import Task\\n    from ansible.playbook.task_include import TaskInclude\\n    from ansible.playbook.role_include import IncludeRole\\n    from ansible.playbook.handler_task_include import HandlerTaskInclude\\n    from ansible.template import Templar\\n    from ansible.utils.plugin_docs import get_versioned_doclink\\n    if not isinstance(ds, list):\\n        raise AnsibleAssertionError('The ds (%s) should be a list but was a %s' % (ds, type(ds)))\\n    task_list = []\\n    for task_ds in ds:\\n        if not isinstance(task_ds, dict):\\n            raise AnsibleAssertionError('The ds (%s) should be a dict but was a %s' % (ds, type(ds)))\\n        if 'block' in task_ds:\\n            t = Block.load(\\n                task_ds,\\n                play=play,\\n                parent_block=block,\\n                role=role,\\n                task_include=task_include,\\n                use_handlers=use_handlers,\\n                variable_manager=variable_manager,\\n                loader=loader,\\n            )\\n            task_list.append(t)\\n        else:\\n            args_parser = ModuleArgsParser(task_ds)\\n            try:\\n                (action, args, delegate_to) = args_parser.parse(skip_action_validation=True)\\n            except AnsibleParserError as e:\\n                if e.obj:\\n                    raise\\n                raise AnsibleParserError(to_native(e), obj=task_ds, orig_exc=e)\\n            if action in C._ACTION_ALL_INCLUDE_IMPORT_TASKS:\\n                if use_handlers:\\n                    include_class = HandlerTaskInclude\\n                else:\\n                    include_class = TaskInclude\\n                t = include_class.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader\\n                )\\n                all_vars = variable_manager.get_vars(play=play, task=t)\\n                templar = Templar(loader=loader, variables=all_vars)\\n                if action in C._ACTION_INCLUDE_TASKS:\\n                    is_static = False\\n                elif action in C._ACTION_IMPORT_TASKS:\\n                    is_static = True\\n                else:\\n                    include_link = get_versioned_doclink('user_guide/playbooks_reuse_includes.html')\\n                    display.deprecated('\"include\" is deprecated, use include_tasks/import_tasks instead. See %s for details' % include_link, \"2.16\")\\n                    is_static = not templar.is_template(t.args['_raw_params']) and t.all_parents_static() and not t.loop\\n                if is_static:\\n                    if t.loop is not None:\\n                        if action in C._ACTION_IMPORT_TASKS:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_tasks' statements. You should use 'include_tasks' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include with a loop\", obj=task_ds)\\n                    t.statically_loaded = True\\n                    parent_include = block\\n                    cumulative_path = None\\n                    found = False\\n                    subdir = 'tasks'\\n                    if use_handlers:\\n                        subdir = 'handlers'\\n                    while parent_include is not None:\\n                        if not isinstance(parent_include, TaskInclude):\\n                            parent_include = parent_include._parent\\n                            continue\\n                        try:\\n                            parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\\n                        except AnsibleUndefinedVariable as e:\\n                            if not parent_include.statically_loaded:\\n                                raise AnsibleParserError(\\n                                    \"Error when evaluating variable in dynamic parent include path: %s. \"\\n                                    \"When using static imports, the parent dynamic include cannot utilize host facts \"\\n                                    \"or variables from inventory\" % parent_include.args.get('_raw_params'),\\n                                    obj=task_ds,\\n                                    suppress_extended_error=True,\\n                                    orig_exc=e\\n                                )\\n                            raise\\n                        if cumulative_path is None:\\n                            cumulative_path = parent_include_dir\\n                        elif not os.path.isabs(cumulative_path):\\n                            cumulative_path = os.path.join(parent_include_dir, cumulative_path)\\n                        include_target = templar.template(t.args['_raw_params'])\\n                        if t._role:\\n                            new_basedir = os.path.join(t._role._role_path, subdir, cumulative_path)\\n                            include_file = loader.path_dwim_relative(new_basedir, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim_relative(loader.get_basedir(), cumulative_path, include_target)\\n                        if os.path.exists(include_file):\\n                            found = True\\n                            break\\n                        else:\\n                            parent_include = parent_include._parent\\n                    if not found:\\n                        try:\\n                            include_target = templar.template(t.args['_raw_params'])\\n                        except AnsibleUndefinedVariable as e:\\n                            raise AnsibleParserError(\\n                                \"Error when evaluating variable in import path: %s.\\n\"\\n                                \"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\\n                                \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\\n                                \"sources like group or host vars.\" % t.args['_raw_params'],\\n                                obj=task_ds,\\n                                suppress_extended_error=True,\\n                                orig_exc=e)\\n                        if t._role:\\n                            include_file = loader.path_dwim_relative(t._role._role_path, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim(include_target)\\n                    data = loader.load_from_file(include_file)\\n                    if not data:\\n                        display.warning('file %s is empty and had no tasks to include' % include_file)\\n                        continue\\n                    elif not isinstance(data, list):\\n                        raise AnsibleParserError(\"included task files must contain a list of tasks\", obj=data)\\n                    display.vv(\"statically imported: %s\" % include_file)\\n                    ti_copy = t.copy(exclude_parent=True)\\n                    ti_copy._parent = block\\n                    included_blocks = load_list_of_blocks(\\n                        data,\\n                        play=play,\\n                        parent_block=None,\\n                        task_include=ti_copy,\\n                        role=role,\\n                        use_handlers=use_handlers,\\n                        loader=loader,\\n                        variable_manager=variable_manager,\\n                    )\\n                    tags = ti_copy.tags[:]\\n                    for b in included_blocks:\\n                        b.tags = list(set(b.tags).union(tags))\\n                    if use_handlers:\\n                        for b in included_blocks:\\n                            task_list.extend(b.block)\\n                    else:\\n                        task_list.extend(included_blocks)\\n                else:\\n                    t.is_static = False\\n                    task_list.append(t)\\n            elif action in C._ACTION_ALL_PROPER_INCLUDE_IMPORT_ROLES:\\n                if use_handlers:\\n                    raise AnsibleParserError(f\"Using '{action}' as a handler is not supported.\", obj=task_ds)\\n                ir = IncludeRole.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader,\\n                )\\n                is_static = False\\n                if action in C._ACTION_IMPORT_ROLE:\\n                    is_static = True\\n                if is_static:\\n                    if ir.loop is not None:\\n                        if action in C._ACTION_IMPORT_ROLE:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_role' statements. You should use 'include_role' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include_role with a loop\", obj=task_ds)\\n                    ir.statically_loaded = True\\n                    all_vars = variable_manager.get_vars(play=play, task=ir)\\n                    templar = Templar(loader=loader, variables=all_vars)\\n                    ir._role_name = templar.template(ir._role_name)\\n                    blocks, _ = ir.get_block_list(variable_manager=variable_manager, loader=loader)\\n                    task_list.extend(blocks)\\n                else:\\n                    task_list.append(ir)\\n            else:\\n                if use_handlers:\\n                    t = Handler.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                else:\\n                    t = Task.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                task_list.append(t)\\n    return task_list"
  },
  {
    "code": "def _add_margins(\\n\\ttable: Union[\"Series\", \"DataFrame\"],\\n\\tdata,\\n\\tvalues,\\n\\trows,\\n\\tcols,\\n\\taggfunc,\\n\\tobserved=None,\\n\\tmargins_name: str = \"All\",\\n\\tfill_value=None,\\n):\\n\\tif not isinstance(margins_name, str):\\n\\t\\traise ValueError(\"margins_name argument must be a string\")\\n\\tmsg = 'Conflicting name \"{name}\" in margins'.format(name=margins_name)\\n\\tfor level in table.index.names:\\n\\t\\tif margins_name in table.index.get_level_values(level):\\n\\t\\t\\traise ValueError(msg)\\n\\tgrand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\\n\\tif table.ndim == 2:\\n\\t\\tfor level in table.columns.names[1:]:\\n\\t\\t\\tif margins_name in table.columns.get_level_values(level):\\n\\t\\t\\t\\traise ValueError(msg)\\n\\tkey: Union[str, Tuple[str, ...]]\\n\\tif len(rows) > 1:\\n\\t\\tkey = (margins_name,) + (\"\",) * (len(rows) - 1)\\n\\telse:\\n\\t\\tkey = margins_name\\n\\tif not values and isinstance(table, ABCSeries):\\n\\t\\treturn table.append(Series({key: grand_margin[margins_name]}))\\n\\telif values:\\n\\t\\tmarginal_result_set = _generate_marginal_results(\\n\\t\\t\\ttable,\\n\\t\\t\\tdata,\\n\\t\\t\\tvalues,\\n\\t\\t\\trows,\\n\\t\\t\\tcols,\\n\\t\\t\\taggfunc,\\n\\t\\t\\tobserved,\\n\\t\\t\\tgrand_margin,\\n\\t\\t\\tmargins_name,\\n\\t\\t)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\telse:\\n\\t\\tassert isinstance(table, ABCDataFrame)\\n\\t\\tmarginal_result_set = _generate_marginal_results_without_values(\\n\\t\\t\\ttable, data, rows, cols, aggfunc, observed, margins_name\\n\\t\\t)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\trow_margin = row_margin.reindex(result.columns, fill_value=fill_value)\\n\\tfor k in margin_keys:\\n\\t\\tif isinstance(k, str):\\n\\t\\t\\trow_margin[k] = grand_margin[k]\\n\\t\\telse:\\n\\t\\t\\trow_margin[k] = grand_margin[k[0]]\\n\\tfrom pandas import DataFrame\\n\\tmargin_dummy = DataFrame(row_margin, columns=[key]).T\\n\\trow_names = result.index.names\\n\\ttry:\\n\\t\\tfor dtype in set(result.dtypes):\\n\\t\\t\\tcols = result.select_dtypes([dtype]).columns\\n\\t\\t\\tmargin_dummy[cols] = margin_dummy[cols].astype(dtype)\\n\\t\\tresult = result.append(margin_dummy)\\n\\texcept TypeError:\\n\\t\\tresult.index = result.index._to_safe_for_reshape()\\n\\t\\tresult = result.append(margin_dummy)\\n\\tresult.index.names = row_names\\n\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pivot_table not returning correct type when margin=True and aggfunc='mean'  (#28248)",
    "fixed_code": "def _add_margins(\\n\\ttable: Union[\"Series\", \"DataFrame\"],\\n\\tdata,\\n\\tvalues,\\n\\trows,\\n\\tcols,\\n\\taggfunc,\\n\\tobserved=None,\\n\\tmargins_name: str = \"All\",\\n\\tfill_value=None,\\n):\\n\\tif not isinstance(margins_name, str):\\n\\t\\traise ValueError(\"margins_name argument must be a string\")\\n\\tmsg = 'Conflicting name \"{name}\" in margins'.format(name=margins_name)\\n\\tfor level in table.index.names:\\n\\t\\tif margins_name in table.index.get_level_values(level):\\n\\t\\t\\traise ValueError(msg)\\n\\tgrand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\\n\\tif table.ndim == 2:\\n\\t\\tfor level in table.columns.names[1:]:\\n\\t\\t\\tif margins_name in table.columns.get_level_values(level):\\n\\t\\t\\t\\traise ValueError(msg)\\n\\tkey: Union[str, Tuple[str, ...]]\\n\\tif len(rows) > 1:\\n\\t\\tkey = (margins_name,) + (\"\",) * (len(rows) - 1)\\n\\telse:\\n\\t\\tkey = margins_name\\n\\tif not values and isinstance(table, ABCSeries):\\n\\t\\treturn table.append(Series({key: grand_margin[margins_name]}))\\n\\telif values:\\n\\t\\tmarginal_result_set = _generate_marginal_results(\\n\\t\\t\\ttable,\\n\\t\\t\\tdata,\\n\\t\\t\\tvalues,\\n\\t\\t\\trows,\\n\\t\\t\\tcols,\\n\\t\\t\\taggfunc,\\n\\t\\t\\tobserved,\\n\\t\\t\\tgrand_margin,\\n\\t\\t\\tmargins_name,\\n\\t\\t)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\telse:\\n\\t\\tassert isinstance(table, ABCDataFrame)\\n\\t\\tmarginal_result_set = _generate_marginal_results_without_values(\\n\\t\\t\\ttable, data, rows, cols, aggfunc, observed, margins_name\\n\\t\\t)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\trow_margin = row_margin.reindex(result.columns, fill_value=fill_value)\\n\\tfor k in margin_keys:\\n\\t\\tif isinstance(k, str):\\n\\t\\t\\trow_margin[k] = grand_margin[k]\\n\\t\\telse:\\n\\t\\t\\trow_margin[k] = grand_margin[k[0]]\\n\\tfrom pandas import DataFrame\\n\\tmargin_dummy = DataFrame(row_margin, columns=[key]).T\\n\\trow_names = result.index.names\\n\\ttry:\\n\\t\\tfor dtype in set(result.dtypes):\\n\\t\\t\\tcols = result.select_dtypes([dtype]).columns\\n\\t\\t\\tmargin_dummy[cols] = margin_dummy[cols].apply(\\n\\t\\t\\t\\tmaybe_downcast_to_dtype, args=(dtype,)\\n\\t\\t\\t)\\n\\t\\tresult = result.append(margin_dummy)\\n\\texcept TypeError:\\n\\t\\tresult.index = result.index._to_safe_for_reshape()\\n\\t\\tresult = result.append(margin_dummy)\\n\\tresult.index.names = row_names\\n\\treturn result"
  },
  {
    "code": "def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\\n        if (\\n            self._lambda_argument_depths\\n            and self._lambda_argument_depths[-1] == self.depth\\n            and leaf.type == token.COLON\\n        ):\\n            self.depth -= 1\\n            self._lambda_argument_depths.pop()\\n            return True\\n        return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\\n        if (\\n            self._lambda_argument_depths\\n            and self._lambda_argument_depths[-1] == self.depth\\n            and leaf.type == token.COLON\\n        ):\\n            self.depth -= 1\\n            self._lambda_argument_depths.pop()\\n            return True\\n        return False"
  },
  {
    "code": "def insert(self, loc, item):\\n        _self = np.asarray(self)\\n        item_idx = Index([item], dtype=self.dtype).values\\n        idx = np.concatenate(\\n            (_self[:loc], item_idx, _self[loc:]))\\n        return Index(idx, name=self.name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Drop & insert on subtypes of index return their subtypes, #10620",
    "fixed_code": "def insert(self, loc, item):\\n        indexes=[self[:loc],\\n                 Index([item]),\\n                 self[loc:]]\\n        return indexes[0].append(indexes[1]).append(indexes[2])"
  },
  {
    "code": "def melt(\\n    frame: \"DataFrame\",\\n    id_vars=None,\\n    value_vars=None,\\n    var_name=None,\\n    value_name=\"value\",\\n    col_level=None,\\n) -> \"DataFrame\":\\n    if isinstance(frame.columns, MultiIndex):\\n        cols = [x for c in frame.columns for x in c]\\n    else:\\n        cols = list(frame.columns)\\n    if value_name in frame.columns:\\n        warnings.warn(\\n            \"This dataframe has a column name that matches the 'value_name' column \"\\n            \"name of the resultiing Dataframe. \"\\n            \"In the future this will raise an error, please set the 'value_name' \"\\n            \"parameter of DataFrame.melt to a unique name.\",\\n            FutureWarning,\\n            stacklevel=3,\\n        )\\n    if id_vars is not None:\\n        if not is_list_like(id_vars):\\n            id_vars = [id_vars]\\n        elif isinstance(frame.columns, MultiIndex) and not isinstance(id_vars, list):\\n            raise ValueError(\\n                \"id_vars must be a list of tuples when columns are a MultiIndex\"\\n            )\\n        else:\\n            id_vars = list(id_vars)\\n            missing = Index(com.flatten(id_vars)).difference(cols)\\n            if not missing.empty:\\n                raise KeyError(\\n                    \"The following 'id_vars' are not present \"\\n                    f\"in the DataFrame: {list(missing)}\"\\n                )\\n    else:\\n        id_vars = []\\n    if value_vars is not None:\\n        if not is_list_like(value_vars):\\n            value_vars = [value_vars]\\n        elif isinstance(frame.columns, MultiIndex) and not isinstance(value_vars, list):\\n            raise ValueError(\\n                \"value_vars must be a list of tuples when columns are a MultiIndex\"\\n            )\\n        else:\\n            value_vars = list(value_vars)\\n            missing = Index(com.flatten(value_vars)).difference(cols)\\n            if not missing.empty:\\n                raise KeyError(\\n                    \"The following 'value_vars' are not present in \"\\n                    f\"the DataFrame: {list(missing)}\"\\n                )\\n        if col_level is not None:\\n            idx = frame.columns.get_level_values(col_level).get_indexer(\\n                id_vars + value_vars\\n            )\\n        else:\\n            idx = frame.columns.get_indexer(id_vars + value_vars)\\n        frame = frame.iloc[:, idx]\\n    else:\\n        frame = frame.copy()\\n    if col_level is not None:  \\n        frame.columns = frame.columns.get_level_values(col_level)\\n    if var_name is None:\\n        if isinstance(frame.columns, MultiIndex):\\n            if len(frame.columns.names) == len(set(frame.columns.names)):\\n                var_name = frame.columns.names\\n            else:\\n                var_name = [f\"variable_{i}\" for i in range(len(frame.columns.names))]\\n        else:\\n            var_name = [\\n                frame.columns.name if frame.columns.name is not None else \"variable\"\\n            ]\\n    if isinstance(var_name, str):\\n        var_name = [var_name]\\n    N, K = frame.shape\\n    K -= len(id_vars)\\n    mdata = {}\\n    for col in id_vars:\\n        id_data = frame.pop(col)\\n        if is_extension_array_dtype(id_data):\\n            id_data = cast(\"Series\", concat([id_data] * K, ignore_index=True))\\n        else:\\n            id_data = np.tile(id_data._values, K)\\n        mdata[col] = id_data\\n    mcolumns = id_vars + var_name + [value_name]\\n    mdata[value_name] = frame._values.ravel(\"F\")\\n    for i, col in enumerate(var_name):\\n        mdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)\\n    return frame._constructor(mdata, columns=mcolumns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Add optional argument index to pd.melt to maintain index values (#33659)",
    "fixed_code": "def melt(\\n    frame: \"DataFrame\",\\n    id_vars=None,\\n    value_vars=None,\\n    var_name=None,\\n    value_name=\"value\",\\n    col_level=None,\\n    ignore_index: bool = True,\\n) -> \"DataFrame\":\\n    if isinstance(frame.columns, MultiIndex):\\n        cols = [x for c in frame.columns for x in c]\\n    else:\\n        cols = list(frame.columns)\\n    if value_name in frame.columns:\\n        warnings.warn(\\n            \"This dataframe has a column name that matches the 'value_name' column \"\\n            \"name of the resultiing Dataframe. \"\\n            \"In the future this will raise an error, please set the 'value_name' \"\\n            \"parameter of DataFrame.melt to a unique name.\",\\n            FutureWarning,\\n            stacklevel=3,\\n        )\\n    if id_vars is not None:\\n        if not is_list_like(id_vars):\\n            id_vars = [id_vars]\\n        elif isinstance(frame.columns, MultiIndex) and not isinstance(id_vars, list):\\n            raise ValueError(\\n                \"id_vars must be a list of tuples when columns are a MultiIndex\"\\n            )\\n        else:\\n            id_vars = list(id_vars)\\n            missing = Index(com.flatten(id_vars)).difference(cols)\\n            if not missing.empty:\\n                raise KeyError(\\n                    \"The following 'id_vars' are not present \"\\n                    f\"in the DataFrame: {list(missing)}\"\\n                )\\n    else:\\n        id_vars = []\\n    if value_vars is not None:\\n        if not is_list_like(value_vars):\\n            value_vars = [value_vars]\\n        elif isinstance(frame.columns, MultiIndex) and not isinstance(value_vars, list):\\n            raise ValueError(\\n                \"value_vars must be a list of tuples when columns are a MultiIndex\"\\n            )\\n        else:\\n            value_vars = list(value_vars)\\n            missing = Index(com.flatten(value_vars)).difference(cols)\\n            if not missing.empty:\\n                raise KeyError(\\n                    \"The following 'value_vars' are not present in \"\\n                    f\"the DataFrame: {list(missing)}\"\\n                )\\n        if col_level is not None:\\n            idx = frame.columns.get_level_values(col_level).get_indexer(\\n                id_vars + value_vars\\n            )\\n        else:\\n            idx = frame.columns.get_indexer(id_vars + value_vars)\\n        frame = frame.iloc[:, idx]\\n    else:\\n        frame = frame.copy()\\n    if col_level is not None:  \\n        frame.columns = frame.columns.get_level_values(col_level)\\n    if var_name is None:\\n        if isinstance(frame.columns, MultiIndex):\\n            if len(frame.columns.names) == len(set(frame.columns.names)):\\n                var_name = frame.columns.names\\n            else:\\n                var_name = [f\"variable_{i}\" for i in range(len(frame.columns.names))]\\n        else:\\n            var_name = [\\n                frame.columns.name if frame.columns.name is not None else \"variable\"\\n            ]\\n    if isinstance(var_name, str):\\n        var_name = [var_name]\\n    N, K = frame.shape\\n    K -= len(id_vars)\\n    mdata = {}\\n    for col in id_vars:\\n        id_data = frame.pop(col)\\n        if is_extension_array_dtype(id_data):\\n            id_data = cast(\"Series\", concat([id_data] * K, ignore_index=True))\\n        else:\\n            id_data = np.tile(id_data._values, K)\\n        mdata[col] = id_data\\n    mcolumns = id_vars + var_name + [value_name]\\n    mdata[value_name] = frame._values.ravel(\"F\")\\n    for i, col in enumerate(var_name):\\n        mdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)\\n    result = frame._constructor(mdata, columns=mcolumns)\\n    if not ignore_index:\\n        result.index = _tile_compat(frame.index, K)\\n    return result"
  },
  {
    "code": "def _get_period_bins(self, ax):\\n\\t\\tif not isinstance(ax, PeriodIndex):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"axis must be a PeriodIndex, but got \"\\n\\t\\t\\t\\tf\"an instance of {type(ax).__name__}\"\\n\\t\\t\\t)\\n\\t\\tmemb = ax.asfreq(self.freq, how=self.convention)\\n\\t\\tnat_count = 0\\n\\t\\tif memb.hasnans:\\n\\t\\t\\tnat_count = np.sum(memb._isnan)\\n\\t\\t\\tmemb = memb[~memb._isnan]\\n\\t\\tif not len(memb):\\n\\t\\t\\tbinner = labels = PeriodIndex(data=[], freq=self.freq, name=ax.name)\\n\\t\\t\\treturn binner, [], labels\\n\\t\\tfreq_mult = self.freq.n\\n\\t\\tstart = ax.min().asfreq(self.freq, how=self.convention)\\n\\t\\tend = ax.max().asfreq(self.freq, how=\"end\")\\n\\t\\tbin_shift = 0\\n\\t\\tif self.base:\\n\\t\\t\\tp_start, end = _get_period_range_edges(\\n\\t\\t\\t\\tstart, end, self.freq, closed=self.closed, base=self.base\\n\\t\\t\\t)\\n\\t\\t\\tstart_offset = Period(start, self.freq) - Period(p_start, self.freq)\\n\\t\\t\\tbin_shift = start_offset.n % freq_mult\\n\\t\\t\\tstart = p_start\\n\\t\\tlabels = binner = period_range(\\n\\t\\t\\tstart=start, end=end, freq=self.freq, name=ax.name\\n\\t\\t)\\n\\t\\ti8 = memb.asi8\\n\\t\\texpected_bins_count = len(binner) * freq_mult\\n\\t\\ti8_extend = expected_bins_count - (i8[-1] - i8[0])\\n\\t\\trng = np.arange(i8[0], i8[-1] + i8_extend, freq_mult)\\n\\t\\trng += freq_mult\\n\\t\\trng -= bin_shift\\n\\t\\tbins = memb.searchsorted(rng, side=\"left\")\\n\\t\\tif nat_count > 0:\\n\\t\\t\\tbins += nat_count\\n\\t\\t\\tbins = np.insert(bins, 0, nat_count)\\n\\t\\t\\tbinner = binner.insert(0, NaT)\\n\\t\\t\\tlabels = labels.insert(0, NaT)\\n\\t\\treturn binner, bins, labels",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: PeriodIndex.searchsorted accepting invalid inputs (#30763)",
    "fixed_code": "def _get_period_bins(self, ax):\\n\\t\\tif not isinstance(ax, PeriodIndex):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"axis must be a PeriodIndex, but got \"\\n\\t\\t\\t\\tf\"an instance of {type(ax).__name__}\"\\n\\t\\t\\t)\\n\\t\\tmemb = ax.asfreq(self.freq, how=self.convention)\\n\\t\\tnat_count = 0\\n\\t\\tif memb.hasnans:\\n\\t\\t\\tnat_count = np.sum(memb._isnan)\\n\\t\\t\\tmemb = memb[~memb._isnan]\\n\\t\\tif not len(memb):\\n\\t\\t\\tbinner = labels = PeriodIndex(data=[], freq=self.freq, name=ax.name)\\n\\t\\t\\treturn binner, [], labels\\n\\t\\tfreq_mult = self.freq.n\\n\\t\\tstart = ax.min().asfreq(self.freq, how=self.convention)\\n\\t\\tend = ax.max().asfreq(self.freq, how=\"end\")\\n\\t\\tbin_shift = 0\\n\\t\\tif self.base:\\n\\t\\t\\tp_start, end = _get_period_range_edges(\\n\\t\\t\\t\\tstart, end, self.freq, closed=self.closed, base=self.base\\n\\t\\t\\t)\\n\\t\\t\\tstart_offset = Period(start, self.freq) - Period(p_start, self.freq)\\n\\t\\t\\tbin_shift = start_offset.n % freq_mult\\n\\t\\t\\tstart = p_start\\n\\t\\tlabels = binner = period_range(\\n\\t\\t\\tstart=start, end=end, freq=self.freq, name=ax.name\\n\\t\\t)\\n\\t\\ti8 = memb.asi8\\n\\t\\texpected_bins_count = len(binner) * freq_mult\\n\\t\\ti8_extend = expected_bins_count - (i8[-1] - i8[0])\\n\\t\\trng = np.arange(i8[0], i8[-1] + i8_extend, freq_mult)\\n\\t\\trng += freq_mult\\n\\t\\trng -= bin_shift\\n\\t\\tprng = type(memb._data)(rng, dtype=memb.dtype)\\n\\t\\tbins = memb.searchsorted(prng, side=\"left\")\\n\\t\\tif nat_count > 0:\\n\\t\\t\\tbins += nat_count\\n\\t\\t\\tbins = np.insert(bins, 0, nat_count)\\n\\t\\t\\tbinner = binner.insert(0, NaT)\\n\\t\\t\\tlabels = labels.insert(0, NaT)\\n\\t\\treturn binner, bins, labels"
  },
  {
    "code": "def luks_add_key(self):\\n        if (self.device is None or\\n                (self._module.params['keyfile'] is None and\\n                 self._module.params['passphrase'] is None) or\\n                (self._module.params['new_keyfile'] is None and\\n                 self._module.params['new_passphrase'] is None)):\\n            return False\\n        if self._module.params['state'] == 'absent':\\n            self._module.fail_json(msg=\"Contradiction in setup: Asking to \"\\n                                   \"add a key to absent LUKS.\")\\n        return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def luks_add_key(self):\\n        if (self.device is None or\\n                (self._module.params['keyfile'] is None and\\n                 self._module.params['passphrase'] is None) or\\n                (self._module.params['new_keyfile'] is None and\\n                 self._module.params['new_passphrase'] is None)):\\n            return False\\n        if self._module.params['state'] == 'absent':\\n            self._module.fail_json(msg=\"Contradiction in setup: Asking to \"\\n                                   \"add a key to absent LUKS.\")\\n        return True"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        interface=dict(required=True, type='str'),\\n        ospf=dict(required=True, type='str'),\\n        area=dict(required=True, type='str'),\\n        cost=dict(required=False, type='str'),\\n        hello_interval=dict(required=False, type='str'),\\n        dead_interval=dict(required=False, type='str'),\\n        passive_interface=dict(required=False, type='bool'),\\n        network=dict(required=False, type='str', choices=['broadcast', 'point-to-point']),\\n        message_digest=dict(required=False, type='bool'),\\n        message_digest_key_id=dict(required=False, type='str'),\\n        message_digest_algorithm_type=dict(required=False, type='str', choices=['md5', 'default']),\\n        message_digest_encryption_type=dict(required=False, type='str', choices=['cisco_type_7', '3des', 'default']),\\n        message_digest_password=dict(required=False, type='str', no_log=True),\\n        state=dict(choices=['present', 'absent'], default='present', required=False)\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           required_together=[['message_digest_key_id',\\n                                               'message_digest_algorithm_type',\\n                                               'message_digest_encryption_type',\\n                                               'message_digest_password']],\\n                           supports_check_mode=True)\\n    if re.match(r'(port-channel|loopback)', module.params['interface'], re.I):\\n        module.params['interface'] = module.params['interface'].lower()\\n    else:\\n        module.params['interface'] = module.params['interface'].capitalize()\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = {'changed': False, 'commands': [], 'warnings': warnings}\\n    for param in ['message_digest_encryption_type',\\n                  'message_digest_algorithm_type',\\n                  'message_digest_password']:\\n        if module.params[param] == 'default' and module.params['message_digest_key_id'] != 'default':\\n            module.exit_json(msg='Use message_digest_key_id=default to remove an existing authentication configuration')\\n    state = module.params['state']\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args)\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key != 'interface':\\n            if str(value).lower() == 'true':\\n                value = True\\n            elif str(value).lower() == 'false':\\n                value = False\\n            elif str(value).lower() == 'default':\\n                value = 'default'\\n            if existing.get(key) or (not existing.get(key) and value):\\n                proposed[key] = value\\n            elif 'passive_interface' in key and existing.get(key) is None and value is False:\\n                proposed[key] = value\\n    proposed['area'] = normalize_area(proposed['area'], module)\\n    if 'hello_interval' in proposed and proposed['hello_interval'] == '10':\\n        proposed['hello_interval'] = 'default'\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif state == 'absent' and existing.get('ospf') == proposed['ospf'] and existing.get('area') == proposed['area']:\\n        state_absent(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        if not module.check_mode:\\n            load_config(module, candidate)\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_interface_ospf: Add bfd support (#56807)\\n\\n\\nAdd support for `bfd` state in `nxos_interface_ospf`\\n\\n- Feature Pull Request\\n\\n`nxos_interface_ospf`",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        interface=dict(required=True, type='str'),\\n        ospf=dict(required=True, type='str'),\\n        area=dict(required=True, type='str'),\\n        bfd=dict(choices=['enable', 'disable', 'default'], required=False, type='str'),\\n        cost=dict(required=False, type='str'),\\n        hello_interval=dict(required=False, type='str'),\\n        dead_interval=dict(required=False, type='str'),\\n        passive_interface=dict(required=False, type='bool'),\\n        network=dict(required=False, type='str', choices=['broadcast', 'point-to-point']),\\n        message_digest=dict(required=False, type='bool'),\\n        message_digest_key_id=dict(required=False, type='str'),\\n        message_digest_algorithm_type=dict(required=False, type='str', choices=['md5', 'default']),\\n        message_digest_encryption_type=dict(required=False, type='str', choices=['cisco_type_7', '3des', 'default']),\\n        message_digest_password=dict(required=False, type='str', no_log=True),\\n        state=dict(choices=['present', 'absent'], default='present', required=False)\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           required_together=[['message_digest_key_id',\\n                                               'message_digest_algorithm_type',\\n                                               'message_digest_encryption_type',\\n                                               'message_digest_password']],\\n                           supports_check_mode=True)\\n    if re.match(r'(port-channel|loopback)', module.params['interface'], re.I):\\n        module.params['interface'] = module.params['interface'].lower()\\n    else:\\n        module.params['interface'] = module.params['interface'].capitalize()\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = {'changed': False, 'commands': [], 'warnings': warnings}\\n    for param in ['message_digest_encryption_type',\\n                  'message_digest_algorithm_type',\\n                  'message_digest_password']:\\n        if module.params[param] == 'default' and module.params['message_digest_key_id'] != 'default':\\n            module.exit_json(msg='Use message_digest_key_id=default to remove an existing authentication configuration')\\n    state = module.params['state']\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args)\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key != 'interface':\\n            if str(value).lower() == 'true':\\n                value = True\\n            elif str(value).lower() == 'false':\\n                value = False\\n            elif str(value).lower() == 'default':\\n                value = 'default'\\n            elif key == 'bfd':\\n                value = str(value).lower()\\n            if existing.get(key) or (not existing.get(key) and value):\\n                proposed[key] = value\\n            elif 'passive_interface' in key and existing.get(key) is None and value is False:\\n                proposed[key] = value\\n    proposed['area'] = normalize_area(proposed['area'], module)\\n    if 'hello_interval' in proposed and proposed['hello_interval'] == '10':\\n        proposed['hello_interval'] = 'default'\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif state == 'absent' and existing.get('ospf') == proposed['ospf'] and existing.get('area') == proposed['area']:\\n        state_absent(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        if not module.check_mode:\\n            load_config(module, candidate)\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    module.exit_json(**result)"
  },
  {
    "code": "def melt(\\n\\tframe: DataFrame,\\n\\tid_vars=None,\\n\\tvalue_vars=None,\\n\\tvar_name=None,\\n\\tvalue_name=\"value\",\\n\\tcol_level=None,\\n) -> DataFrame:\\n\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\tcols = [x for c in frame.columns for x in c]\\n\\telse:\\n\\t\\tcols = list(frame.columns)\\n\\tif id_vars is not None:\\n\\t\\tif not is_list_like(id_vars):\\n\\t\\t\\tid_vars = [id_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(id_vars, list):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"id_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tid_vars = list(id_vars)\\n\\t\\t\\tmissing = Index(np.ravel(id_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'id_vars' are not present\"\\n\\t\\t\\t\\t\\t\" in the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\telse:\\n\\t\\tid_vars = []\\n\\tif value_vars is not None:\\n\\t\\tif not is_list_like(value_vars):\\n\\t\\t\\tvalue_vars = [value_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(\\n\\t\\t\\tvalue_vars, list\\n\\t\\t):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"value_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tvalue_vars = list(value_vars)\\n\\t\\t\\tmissing = Index(np.ravel(value_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'value_vars' are not present in\"\\n\\t\\t\\t\\t\\t\" the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\t\\tframe = frame.loc[:, id_vars + value_vars]\\n\\telse:\\n\\t\\tframe = frame.copy()\\n\\tif col_level is not None:  \\n\\t\\tframe.columns = frame.columns.get_level_values(col_level)\\n\\tif var_name is None:\\n\\t\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\t\\tif len(frame.columns.names) == len(set(frame.columns.names)):\\n\\t\\t\\t\\tvar_name = frame.columns.names\\n\\t\\t\\telse:\\n\\t\\t\\t\\tvar_name = [\\n\\t\\t\\t\\t\\t\"variable_{i}\".format(i=i) for i in range(len(frame.columns.names))\\n\\t\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tvar_name = [\\n\\t\\t\\t\\tframe.columns.name if frame.columns.name is not None else \"variable\"\\n\\t\\t\\t]\\n\\tif isinstance(var_name, str):\\n\\t\\tvar_name = [var_name]\\n\\tN, K = frame.shape\\n\\tK -= len(id_vars)\\n\\tmdata = {}\\n\\tfor col in id_vars:\\n\\t\\tid_data = frame.pop(col)\\n\\t\\tif is_extension_array_dtype(id_data):\\n\\t\\t\\tid_data = concat([id_data] * K, ignore_index=True)\\n\\t\\telse:\\n\\t\\t\\tid_data = np.tile(id_data.values, K)\\n\\t\\tmdata[col] = id_data\\n\\tmcolumns = id_vars + var_name + [value_name]\\n\\tmdata[value_name] = frame.values.ravel(\"F\")\\n\\tfor i, col in enumerate(var_name):\\n\\t\\tmdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)\\n\\treturn frame._constructor(mdata, columns=mcolumns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix melt with mixed int/str columns (#29792)",
    "fixed_code": "def melt(\\n\\tframe: DataFrame,\\n\\tid_vars=None,\\n\\tvalue_vars=None,\\n\\tvar_name=None,\\n\\tvalue_name=\"value\",\\n\\tcol_level=None,\\n) -> DataFrame:\\n\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\tcols = [x for c in frame.columns for x in c]\\n\\telse:\\n\\t\\tcols = list(frame.columns)\\n\\tif id_vars is not None:\\n\\t\\tif not is_list_like(id_vars):\\n\\t\\t\\tid_vars = [id_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(id_vars, list):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"id_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tid_vars = list(id_vars)\\n\\t\\t\\tmissing = Index(com.flatten(id_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'id_vars' are not present\"\\n\\t\\t\\t\\t\\t\" in the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\telse:\\n\\t\\tid_vars = []\\n\\tif value_vars is not None:\\n\\t\\tif not is_list_like(value_vars):\\n\\t\\t\\tvalue_vars = [value_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(\\n\\t\\t\\tvalue_vars, list\\n\\t\\t):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"value_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tvalue_vars = list(value_vars)\\n\\t\\t\\tmissing = Index(com.flatten(value_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'value_vars' are not present in\"\\n\\t\\t\\t\\t\\t\" the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\t\\tframe = frame.loc[:, id_vars + value_vars]\\n\\telse:\\n\\t\\tframe = frame.copy()\\n\\tif col_level is not None:  \\n\\t\\tframe.columns = frame.columns.get_level_values(col_level)\\n\\tif var_name is None:\\n\\t\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\t\\tif len(frame.columns.names) == len(set(frame.columns.names)):\\n\\t\\t\\t\\tvar_name = frame.columns.names\\n\\t\\t\\telse:\\n\\t\\t\\t\\tvar_name = [\\n\\t\\t\\t\\t\\t\"variable_{i}\".format(i=i) for i in range(len(frame.columns.names))\\n\\t\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tvar_name = [\\n\\t\\t\\t\\tframe.columns.name if frame.columns.name is not None else \"variable\"\\n\\t\\t\\t]\\n\\tif isinstance(var_name, str):\\n\\t\\tvar_name = [var_name]\\n\\tN, K = frame.shape\\n\\tK -= len(id_vars)\\n\\tmdata = {}\\n\\tfor col in id_vars:\\n\\t\\tid_data = frame.pop(col)\\n\\t\\tif is_extension_array_dtype(id_data):\\n\\t\\t\\tid_data = concat([id_data] * K, ignore_index=True)\\n\\t\\telse:\\n\\t\\t\\tid_data = np.tile(id_data.values, K)\\n\\t\\tmdata[col] = id_data\\n\\tmcolumns = id_vars + var_name + [value_name]\\n\\tmdata[value_name] = frame.values.ravel(\"F\")\\n\\tfor i, col in enumerate(var_name):\\n\\t\\tmdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)\\n\\treturn frame._constructor(mdata, columns=mcolumns)"
  },
  {
    "code": "def __init__(self, *, missing_args_message=None, called_from_command_line=None, **kwargs):\\n        self.missing_args_message = missing_args_message\\n        self.called_from_command_line = called_from_command_line\\n        super().__init__(**kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #29501 -- Allowed customizing exit status for management commands.",
    "fixed_code": "def __init__(self, *args, returncode=1, **kwargs):\\n        self.returncode = returncode\\n        super().__init__(*args, **kwargs)"
  },
  {
    "code": "def _set_new_attribute(cls, name, value):\\n    if name in cls.__dict__:\\n        return True\\n    setattr(cls, name, value)\\n    return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-41747: Ensure all dataclass methods uses their parents' qualname (GH-22155)",
    "fixed_code": "def _set_new_attribute(cls, name, value):\\n    if name in cls.__dict__:\\n        return True\\n    _set_qualname(cls, value)\\n    setattr(cls, name, value)\\n    return False"
  },
  {
    "code": "def getframeinfo(frame, context=1):\\t\\tif istraceback(frame):\\t\\tlineno = frame.tb_lineno\\t\\tframe = frame.tb_frame\\telse:\\t\\tlineno = frame.f_lineno\\tif not isframe(frame):\\t\\traise TypeError('{!r} is not a frame or traceback object'.format(frame))\\tfilename = getsourcefile(frame) or getfile(frame)\\tif context > 0:\\t\\tstart = lineno - 1 - context//2\\t\\ttry:\\t\\t\\tlines, lnum = findsource(frame)\\t\\texcept OSError:\\t\\t\\tlines = index = None\\t\\telse:\\t\\t\\tstart = max(start, 1)\\t\\t\\tstart = max(0, min(start, len(lines) - context))\\t\\t\\tlines = lines[start:start+context]\\t\\t\\tindex = lineno - 1 - start\\telse:\\t\\tlines = index = None\\treturn Traceback(filename, lineno, frame.f_code.co_name, lines, index)",
    "label": 1,
    "bug_type": "numeric",
    "bug_description": "Issue #15812: inspect.getframeinfo() now correctly shows the first line of a context\\n\\nPatch by Sam Breese.",
    "fixed_code": "def getframeinfo(frame, context=1):\\n\\tif istraceback(frame):\\n\\t\\tlineno = frame.tb_lineno\\n\\t\\tframe = frame.tb_frame\\n\\telse:\\n\\t\\tlineno = frame.f_lineno\\n\\tif not isframe(frame):\\n\\t\\traise TypeError('{!r} is not a frame or traceback object'.format(frame))\\n\\tfilename = getsourcefile(frame) or getfile(frame)\\n\\tif context > 0:\\n\\t\\tstart = lineno - 1 - context//2\\n\\t\\ttry:\\n\\t\\t\\tlines, lnum = findsource(frame)\\n\\t\\texcept OSError:\\n\\t\\t\\tlines = index = None\\n\\t\\telse:\\n\\t\\t\\tstart = max(start, 0)\\n\\t\\t\\tstart = max(0, min(start, len(lines) - context))\\n\\t\\t\\tlines = lines[start:start+context]\\n\\t\\t\\tindex = lineno - 1 - start\\n\\telse:\\n\\t\\tlines = index = None\\n\\treturn Traceback(filename, lineno, frame.f_code.co_name, lines, index)"
  },
  {
    "code": "def fit_generator(model,\\n\\t\\t\\t\\t  generator,\\n\\t\\t\\t\\t  steps_per_epoch=None,\\n\\t\\t\\t\\t  epochs=1,\\n\\t\\t\\t\\t  verbose=1,\\n\\t\\t\\t\\t  callbacks=None,\\n\\t\\t\\t\\t  validation_data=None,\\n\\t\\t\\t\\t  validation_steps=None,\\n\\t\\t\\t\\t  class_weight=None,\\n\\t\\t\\t\\t  max_queue_size=10,\\n\\t\\t\\t\\t  workers=1,\\n\\t\\t\\t\\t  use_multiprocessing=False,\\n\\t\\t\\t\\t  shuffle=True,\\n\\t\\t\\t\\t  initial_epoch=0):\\n\\twait_time = 0.01  \\n\\tepoch = initial_epoch\\n\\tdo_validation = bool(validation_data)\\n\\tmodel._make_train_function()\\n\\tif do_validation:\\n\\t\\tmodel._make_test_function()\\n\\tis_sequence = isinstance(generator, Sequence)\\n\\tif not is_sequence and use_multiprocessing and workers > 1:\\n\\t\\twarnings.warn(\\n\\t\\t\\tUserWarning('Using a generator with `use_multiprocessing=True`'\\n\\t\\t\\t\\t\\t\\t' and multiple workers may duplicate your data.'\\n\\t\\t\\t\\t\\t\\t' Please consider using the`keras.utils.Sequence'\\n\\t\\t\\t\\t\\t\\t' class.'))\\n\\tif steps_per_epoch is None:\\n\\t\\tif is_sequence:\\n\\t\\t\\tsteps_per_epoch = len(generator)\\n\\t\\telse:\\n\\t\\t\\traise ValueError('`steps_per_epoch=None` is only valid for a'\\n\\t\\t\\t\\t\\t\\t\\t ' generator based on the '\\n\\t\\t\\t\\t\\t\\t\\t '`keras.utils.Sequence`'\\n\\t\\t\\t\\t\\t\\t\\t ' class. Please specify `steps_per_epoch` '\\n\\t\\t\\t\\t\\t\\t\\t 'or use the `keras.utils.Sequence` class.')\\n\\tval_gen = (hasattr(validation_data, 'next') or\\n\\t\\t\\t   hasattr(validation_data, '__next__') or\\n\\t\\t\\t   isinstance(validation_data, Sequence))\\n\\tif (val_gen and not isinstance(validation_data, Sequence) and\\n\\t\\t\\tnot validation_steps):\\n\\t\\traise ValueError('`validation_steps=None` is only valid for a'\\n\\t\\t\\t\\t\\t\\t ' generator based on the `keras.utils.Sequence`'\\n\\t\\t\\t\\t\\t\\t ' class. Please specify `validation_steps` or use'\\n\\t\\t\\t\\t\\t\\t ' the `keras.utils.Sequence` class.')\\n\\tout_labels = model.metrics_names\\n\\tcallback_metrics = out_labels + ['val_' + n for n in out_labels]\\n\\tmodel.history = cbks.History()\\n\\t_callbacks = [cbks.BaseLogger(\\n\\t\\tstateful_metrics=model.stateful_metric_names)]\\n\\tif verbose:\\n\\t\\t_callbacks.append(\\n\\t\\t\\tcbks.ProgbarLogger(\\n\\t\\t\\t\\tcount_mode='steps',\\n\\t\\t\\t\\tstateful_metrics=model.stateful_metric_names))\\n\\t_callbacks += (callbacks or []) + [model.history]\\n\\tcallbacks = cbks.CallbackList(_callbacks)\\n\\tif hasattr(model, 'callback_model') and model.callback_model:\\n\\t\\tcallback_model = model.callback_model\\n\\telse:\\n\\t\\tcallback_model = model\\n\\tcallbacks.set_model(callback_model)\\n\\tcallbacks.set_params({\\n\\t\\t'epochs': epochs,\\n\\t\\t'steps': steps_per_epoch,\\n\\t\\t'verbose': verbose,\\n\\t\\t'do_validation': do_validation,\\n\\t\\t'metrics': callback_metrics,\\n\\t})\\n\\tcallbacks.on_train_begin()\\n\\tenqueuer = None\\n\\tval_enqueuer = None\\n\\ttry:\\n\\t\\tif do_validation:\\n\\t\\t\\tif val_gen and workers > 0:\\n\\t\\t\\t\\tval_data = validation_data\\n\\t\\t\\t\\tif isinstance(val_data, Sequence):\\n\\t\\t\\t\\t\\tval_enqueuer = OrderedEnqueuer(\\n\\t\\t\\t\\t\\t\\tval_data,\\n\\t\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing)\\n\\t\\t\\t\\t\\tvalidation_steps = validation_steps or len(val_data)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tval_enqueuer = GeneratorEnqueuer(\\n\\t\\t\\t\\t\\t\\tval_data,\\n\\t\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing)\\n\\t\\t\\t\\tval_enqueuer.start(workers=workers,\\n\\t\\t\\t\\t\\t\\t\\t\\t   max_queue_size=max_queue_size)\\n\\t\\t\\t\\tval_enqueuer_gen = val_enqueuer.get()\\n\\t\\t\\telif val_gen:\\n\\t\\t\\t\\tval_data = validation_data\\n\\t\\t\\t\\tif isinstance(val_data, Sequence):\\n\\t\\t\\t\\t\\tval_enqueuer_gen = iter_sequence_infinite(val_data)\\n\\t\\t\\t\\t\\tvalidation_steps = validation_steps or len(val_data)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tval_enqueuer_gen = val_data\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif len(validation_data) == 2:\\n\\t\\t\\t\\t\\tval_x, val_y = validation_data\\n\\t\\t\\t\\t\\tval_sample_weight = None\\n\\t\\t\\t\\telif len(validation_data) == 3:\\n\\t\\t\\t\\t\\tval_x, val_y, val_sample_weight = validation_data\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise ValueError('`validation_data` should be a tuple '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t '`(val_x, val_y, val_sample_weight)` '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'or `(val_x, val_y)`. Found: ' +\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t str(validation_data))\\n\\t\\t\\t\\tval_x, val_y, val_sample_weights = model._standardize_user_data(\\n\\t\\t\\t\\t\\tval_x, val_y, val_sample_weight)\\n\\t\\t\\t\\tval_data = val_x + val_y + val_sample_weights\\n\\t\\t\\t\\tif model.uses_learning_phase and not isinstance(K.learning_phase(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tint):\\n\\t\\t\\t\\t\\tval_data += [0.]\\n\\t\\t\\t\\tfor cbk in callbacks:\\n\\t\\t\\t\\t\\tcbk.validation_data = val_data\\n\\t\\tif workers > 0:\\n\\t\\t\\tif is_sequence:\\n\\t\\t\\t\\tenqueuer = OrderedEnqueuer(\\n\\t\\t\\t\\t\\tgenerator,\\n\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing,\\n\\t\\t\\t\\t\\tshuffle=shuffle)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tenqueuer = GeneratorEnqueuer(\\n\\t\\t\\t\\t\\tgenerator,\\n\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing,\\n\\t\\t\\t\\t\\twait_time=wait_time)\\n\\t\\t\\tenqueuer.start(workers=workers, max_queue_size=max_queue_size)\\n\\t\\t\\toutput_generator = enqueuer.get()\\n\\t\\telse:\\n\\t\\t\\tif is_sequence:\\n\\t\\t\\t\\toutput_generator = iter_sequence_infinite(generator)\\n\\t\\t\\telse:\\n\\t\\t\\t\\toutput_generator = generator\\n\\t\\tcallback_model.stop_training = False\\n\\t\\tepoch_logs = {}\\n\\t\\twhile epoch < epochs:\\n\\t\\t\\tfor m in model.stateful_metric_functions:\\n\\t\\t\\t\\tm.reset_states()\\n\\t\\t\\tcallbacks.on_epoch_begin(epoch)\\n\\t\\t\\tsteps_done = 0\\n\\t\\t\\tbatch_index = 0\\n\\t\\t\\twhile steps_done < steps_per_epoch:\\n\\t\\t\\t\\tgenerator_output = next(output_generator)\\n\\t\\t\\t\\tif not hasattr(generator_output, '__len__'):\\n\\t\\t\\t\\t\\traise ValueError('Output of generator should be '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'a tuple `(x, y, sample_weight)` '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'or `(x, y)`. Found: ' +\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t str(generator_output))\\n\\t\\t\\t\\tif len(generator_output) == 2:\\n\\t\\t\\t\\t\\tx, y = generator_output\\n\\t\\t\\t\\t\\tsample_weight = None\\n\\t\\t\\t\\telif len(generator_output) == 3:\\n\\t\\t\\t\\t\\tx, y, sample_weight = generator_output\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise ValueError('Output of generator should be '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'a tuple `(x, y, sample_weight)` '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'or `(x, y)`. Found: ' +\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t str(generator_output))\\n\\t\\t\\t\\tbatch_logs = {}\\n\\t\\t\\t\\tif x is None or len(x) == 0:\\n\\t\\t\\t\\t\\tbatch_size = 1\\n\\t\\t\\t\\telif isinstance(x, list):\\n\\t\\t\\t\\t\\tbatch_size = x[0].shape[0]\\n\\t\\t\\t\\telif isinstance(x, dict):\\n\\t\\t\\t\\t\\tbatch_size = list(x.values())[0].shape[0]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tbatch_size = x.shape[0]\\n\\t\\t\\t\\tbatch_logs['batch'] = batch_index\\n\\t\\t\\t\\tbatch_logs['size'] = batch_size\\n\\t\\t\\t\\tcallbacks.on_batch_begin(batch_index, batch_logs)\\n\\t\\t\\t\\touts = model.train_on_batch(x, y,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tsample_weight=sample_weight,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tclass_weight=class_weight)\\n\\t\\t\\t\\touts = to_list(outs)\\n\\t\\t\\t\\tfor l, o in zip(out_labels, outs):\\n\\t\\t\\t\\t\\tbatch_logs[l] = o\\n\\t\\t\\t\\tcallbacks.on_batch_end(batch_index, batch_logs)\\n\\t\\t\\t\\tbatch_index += 1\\n\\t\\t\\t\\tsteps_done += 1\\n\\t\\t\\t\\tif steps_done >= steps_per_epoch and do_validation:\\n\\t\\t\\t\\t\\tif val_gen:\\n\\t\\t\\t\\t\\t\\tval_outs = model.evaluate_generator(\\n\\t\\t\\t\\t\\t\\t\\tval_enqueuer_gen,\\n\\t\\t\\t\\t\\t\\t\\tvalidation_steps,\\n\\t\\t\\t\\t\\t\\t\\tworkers=0)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tval_outs = model.evaluate(\\n\\t\\t\\t\\t\\t\\t\\tval_x, val_y,\\n\\t\\t\\t\\t\\t\\t\\tbatch_size=batch_size,\\n\\t\\t\\t\\t\\t\\t\\tsample_weight=val_sample_weights,\\n\\t\\t\\t\\t\\t\\t\\tverbose=0)\\n\\t\\t\\t\\t\\tval_outs = to_list(val_outs)\\n\\t\\t\\t\\t\\tfor l, o in zip(out_labels, val_outs):\\n\\t\\t\\t\\t\\t\\tepoch_logs['val_' + l] = o\\n\\t\\t\\t\\tif callback_model.stop_training:\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tcallbacks.on_epoch_end(epoch, epoch_logs)\\n\\t\\t\\tepoch += 1\\n\\t\\t\\tif callback_model.stop_training:\\n\\t\\t\\t\\tbreak\\n\\tfinally:\\n\\t\\ttry:\\n\\t\\t\\tif enqueuer is not None:\\n\\t\\t\\t\\tenqueuer.stop()\\n\\t\\tfinally:\\n\\t\\t\\tif val_enqueuer is not None:\\n\\t\\t\\t\\tval_enqueuer.stop()\\n\\tcallbacks.on_train_end()\\n\\treturn model.history",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit_generator(model,\\n\\t\\t\\t\\t  generator,\\n\\t\\t\\t\\t  steps_per_epoch=None,\\n\\t\\t\\t\\t  epochs=1,\\n\\t\\t\\t\\t  verbose=1,\\n\\t\\t\\t\\t  callbacks=None,\\n\\t\\t\\t\\t  validation_data=None,\\n\\t\\t\\t\\t  validation_steps=None,\\n\\t\\t\\t\\t  class_weight=None,\\n\\t\\t\\t\\t  max_queue_size=10,\\n\\t\\t\\t\\t  workers=1,\\n\\t\\t\\t\\t  use_multiprocessing=False,\\n\\t\\t\\t\\t  shuffle=True,\\n\\t\\t\\t\\t  initial_epoch=0):\\n\\twait_time = 0.01  \\n\\tepoch = initial_epoch\\n\\tdo_validation = bool(validation_data)\\n\\tmodel._make_train_function()\\n\\tif do_validation:\\n\\t\\tmodel._make_test_function()\\n\\tis_sequence = isinstance(generator, Sequence)\\n\\tif not is_sequence and use_multiprocessing and workers > 1:\\n\\t\\twarnings.warn(\\n\\t\\t\\tUserWarning('Using a generator with `use_multiprocessing=True`'\\n\\t\\t\\t\\t\\t\\t' and multiple workers may duplicate your data.'\\n\\t\\t\\t\\t\\t\\t' Please consider using the`keras.utils.Sequence'\\n\\t\\t\\t\\t\\t\\t' class.'))\\n\\tif steps_per_epoch is None:\\n\\t\\tif is_sequence:\\n\\t\\t\\tsteps_per_epoch = len(generator)\\n\\t\\telse:\\n\\t\\t\\traise ValueError('`steps_per_epoch=None` is only valid for a'\\n\\t\\t\\t\\t\\t\\t\\t ' generator based on the '\\n\\t\\t\\t\\t\\t\\t\\t '`keras.utils.Sequence`'\\n\\t\\t\\t\\t\\t\\t\\t ' class. Please specify `steps_per_epoch` '\\n\\t\\t\\t\\t\\t\\t\\t 'or use the `keras.utils.Sequence` class.')\\n\\tval_gen = (hasattr(validation_data, 'next') or\\n\\t\\t\\t   hasattr(validation_data, '__next__') or\\n\\t\\t\\t   isinstance(validation_data, Sequence))\\n\\tif (val_gen and not isinstance(validation_data, Sequence) and\\n\\t\\t\\tnot validation_steps):\\n\\t\\traise ValueError('`validation_steps=None` is only valid for a'\\n\\t\\t\\t\\t\\t\\t ' generator based on the `keras.utils.Sequence`'\\n\\t\\t\\t\\t\\t\\t ' class. Please specify `validation_steps` or use'\\n\\t\\t\\t\\t\\t\\t ' the `keras.utils.Sequence` class.')\\n\\tout_labels = model.metrics_names\\n\\tcallback_metrics = out_labels + ['val_' + n for n in out_labels]\\n\\tmodel.history = cbks.History()\\n\\t_callbacks = [cbks.BaseLogger(\\n\\t\\tstateful_metrics=model.stateful_metric_names)]\\n\\tif verbose:\\n\\t\\t_callbacks.append(\\n\\t\\t\\tcbks.ProgbarLogger(\\n\\t\\t\\t\\tcount_mode='steps',\\n\\t\\t\\t\\tstateful_metrics=model.stateful_metric_names))\\n\\t_callbacks += (callbacks or []) + [model.history]\\n\\tcallbacks = cbks.CallbackList(_callbacks)\\n\\tif hasattr(model, 'callback_model') and model.callback_model:\\n\\t\\tcallback_model = model.callback_model\\n\\telse:\\n\\t\\tcallback_model = model\\n\\tcallbacks.set_model(callback_model)\\n\\tcallbacks.set_params({\\n\\t\\t'epochs': epochs,\\n\\t\\t'steps': steps_per_epoch,\\n\\t\\t'verbose': verbose,\\n\\t\\t'do_validation': do_validation,\\n\\t\\t'metrics': callback_metrics,\\n\\t})\\n\\tcallbacks.on_train_begin()\\n\\tenqueuer = None\\n\\tval_enqueuer = None\\n\\ttry:\\n\\t\\tif do_validation:\\n\\t\\t\\tif val_gen and workers > 0:\\n\\t\\t\\t\\tval_data = validation_data\\n\\t\\t\\t\\tif isinstance(val_data, Sequence):\\n\\t\\t\\t\\t\\tval_enqueuer = OrderedEnqueuer(\\n\\t\\t\\t\\t\\t\\tval_data,\\n\\t\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing)\\n\\t\\t\\t\\t\\tvalidation_steps = validation_steps or len(val_data)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tval_enqueuer = GeneratorEnqueuer(\\n\\t\\t\\t\\t\\t\\tval_data,\\n\\t\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing)\\n\\t\\t\\t\\tval_enqueuer.start(workers=workers,\\n\\t\\t\\t\\t\\t\\t\\t\\t   max_queue_size=max_queue_size)\\n\\t\\t\\t\\tval_enqueuer_gen = val_enqueuer.get()\\n\\t\\t\\telif val_gen:\\n\\t\\t\\t\\tval_data = validation_data\\n\\t\\t\\t\\tif isinstance(val_data, Sequence):\\n\\t\\t\\t\\t\\tval_enqueuer_gen = iter_sequence_infinite(val_data)\\n\\t\\t\\t\\t\\tvalidation_steps = validation_steps or len(val_data)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tval_enqueuer_gen = val_data\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif len(validation_data) == 2:\\n\\t\\t\\t\\t\\tval_x, val_y = validation_data\\n\\t\\t\\t\\t\\tval_sample_weight = None\\n\\t\\t\\t\\telif len(validation_data) == 3:\\n\\t\\t\\t\\t\\tval_x, val_y, val_sample_weight = validation_data\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise ValueError('`validation_data` should be a tuple '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t '`(val_x, val_y, val_sample_weight)` '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'or `(val_x, val_y)`. Found: ' +\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t str(validation_data))\\n\\t\\t\\t\\tval_x, val_y, val_sample_weights = model._standardize_user_data(\\n\\t\\t\\t\\t\\tval_x, val_y, val_sample_weight)\\n\\t\\t\\t\\tval_data = val_x + val_y + val_sample_weights\\n\\t\\t\\t\\tif model.uses_learning_phase and not isinstance(K.learning_phase(),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tint):\\n\\t\\t\\t\\t\\tval_data += [0.]\\n\\t\\t\\t\\tfor cbk in callbacks:\\n\\t\\t\\t\\t\\tcbk.validation_data = val_data\\n\\t\\tif workers > 0:\\n\\t\\t\\tif is_sequence:\\n\\t\\t\\t\\tenqueuer = OrderedEnqueuer(\\n\\t\\t\\t\\t\\tgenerator,\\n\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing,\\n\\t\\t\\t\\t\\tshuffle=shuffle)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tenqueuer = GeneratorEnqueuer(\\n\\t\\t\\t\\t\\tgenerator,\\n\\t\\t\\t\\t\\tuse_multiprocessing=use_multiprocessing,\\n\\t\\t\\t\\t\\twait_time=wait_time)\\n\\t\\t\\tenqueuer.start(workers=workers, max_queue_size=max_queue_size)\\n\\t\\t\\toutput_generator = enqueuer.get()\\n\\t\\telse:\\n\\t\\t\\tif is_sequence:\\n\\t\\t\\t\\toutput_generator = iter_sequence_infinite(generator)\\n\\t\\t\\telse:\\n\\t\\t\\t\\toutput_generator = generator\\n\\t\\tcallback_model.stop_training = False\\n\\t\\tepoch_logs = {}\\n\\t\\twhile epoch < epochs:\\n\\t\\t\\tfor m in model.stateful_metric_functions:\\n\\t\\t\\t\\tm.reset_states()\\n\\t\\t\\tcallbacks.on_epoch_begin(epoch)\\n\\t\\t\\tsteps_done = 0\\n\\t\\t\\tbatch_index = 0\\n\\t\\t\\twhile steps_done < steps_per_epoch:\\n\\t\\t\\t\\tgenerator_output = next(output_generator)\\n\\t\\t\\t\\tif not hasattr(generator_output, '__len__'):\\n\\t\\t\\t\\t\\traise ValueError('Output of generator should be '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'a tuple `(x, y, sample_weight)` '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'or `(x, y)`. Found: ' +\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t str(generator_output))\\n\\t\\t\\t\\tif len(generator_output) == 2:\\n\\t\\t\\t\\t\\tx, y = generator_output\\n\\t\\t\\t\\t\\tsample_weight = None\\n\\t\\t\\t\\telif len(generator_output) == 3:\\n\\t\\t\\t\\t\\tx, y, sample_weight = generator_output\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise ValueError('Output of generator should be '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'a tuple `(x, y, sample_weight)` '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t 'or `(x, y)`. Found: ' +\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t str(generator_output))\\n\\t\\t\\t\\tbatch_logs = {}\\n\\t\\t\\t\\tif x is None or len(x) == 0:\\n\\t\\t\\t\\t\\tbatch_size = 1\\n\\t\\t\\t\\telif isinstance(x, list):\\n\\t\\t\\t\\t\\tbatch_size = x[0].shape[0]\\n\\t\\t\\t\\telif isinstance(x, dict):\\n\\t\\t\\t\\t\\tbatch_size = list(x.values())[0].shape[0]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tbatch_size = x.shape[0]\\n\\t\\t\\t\\tbatch_logs['batch'] = batch_index\\n\\t\\t\\t\\tbatch_logs['size'] = batch_size\\n\\t\\t\\t\\tcallbacks.on_batch_begin(batch_index, batch_logs)\\n\\t\\t\\t\\touts = model.train_on_batch(x, y,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tsample_weight=sample_weight,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tclass_weight=class_weight)\\n\\t\\t\\t\\touts = to_list(outs)\\n\\t\\t\\t\\tfor l, o in zip(out_labels, outs):\\n\\t\\t\\t\\t\\tbatch_logs[l] = o\\n\\t\\t\\t\\tcallbacks.on_batch_end(batch_index, batch_logs)\\n\\t\\t\\t\\tbatch_index += 1\\n\\t\\t\\t\\tsteps_done += 1\\n\\t\\t\\t\\tif steps_done >= steps_per_epoch and do_validation:\\n\\t\\t\\t\\t\\tif val_gen:\\n\\t\\t\\t\\t\\t\\tval_outs = model.evaluate_generator(\\n\\t\\t\\t\\t\\t\\t\\tval_enqueuer_gen,\\n\\t\\t\\t\\t\\t\\t\\tvalidation_steps,\\n\\t\\t\\t\\t\\t\\t\\tworkers=0)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tval_outs = model.evaluate(\\n\\t\\t\\t\\t\\t\\t\\tval_x, val_y,\\n\\t\\t\\t\\t\\t\\t\\tbatch_size=batch_size,\\n\\t\\t\\t\\t\\t\\t\\tsample_weight=val_sample_weights,\\n\\t\\t\\t\\t\\t\\t\\tverbose=0)\\n\\t\\t\\t\\t\\tval_outs = to_list(val_outs)\\n\\t\\t\\t\\t\\tfor l, o in zip(out_labels, val_outs):\\n\\t\\t\\t\\t\\t\\tepoch_logs['val_' + l] = o\\n\\t\\t\\t\\tif callback_model.stop_training:\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tcallbacks.on_epoch_end(epoch, epoch_logs)\\n\\t\\t\\tepoch += 1\\n\\t\\t\\tif callback_model.stop_training:\\n\\t\\t\\t\\tbreak\\n\\tfinally:\\n\\t\\ttry:\\n\\t\\t\\tif enqueuer is not None:\\n\\t\\t\\t\\tenqueuer.stop()\\n\\t\\tfinally:\\n\\t\\t\\tif val_enqueuer is not None:\\n\\t\\t\\t\\tval_enqueuer.stop()\\n\\tcallbacks.on_train_end()\\n\\treturn model.history"
  },
  {
    "code": "def __new__(\\n\\t\\tcls,\\n\\t\\tdata=None,\\n\\t\\tunit=None,\\n\\t\\tfreq=None,\\n\\t\\tclosed=None,\\n\\t\\tdtype=_TD_DTYPE,\\n\\t\\tcopy=False,\\n\\t\\tname=None,\\n\\t):\\n\\t\\tname = maybe_extract_name(name, data, cls)\\n\\t\\tif is_scalar(data):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\tf\"{cls.__name__}() must be called with a \"\\n\\t\\t\\t\\tf\"collection of some kind, {repr(data)} was passed\"\\n\\t\\t\\t)\\n\\t\\tif unit in {\"Y\", \"y\", \"M\"}:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"Units 'M' and 'Y' are no longer supported, as they do not \"\\n\\t\\t\\t\\t\"represent unambiguous timedelta values durations.\"\\n\\t\\t\\t)\\n\\t\\tif isinstance(data, TimedeltaArray):\\n\\t\\t\\tif copy:\\n\\t\\t\\t\\tdata = data.copy()\\n\\t\\t\\treturn cls._simple_new(data, name=name, freq=freq)\\n\\t\\tif isinstance(data, TimedeltaIndex) and freq is None and name is None:\\n\\t\\t\\tif copy:\\n\\t\\t\\t\\treturn data.copy()\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn data._shallow_copy()\\n\\t\\ttdarr = TimedeltaArray._from_sequence(\\n\\t\\t\\tdata, freq=freq, unit=unit, dtype=dtype, copy=copy\\n\\t\\t)\\n\\t\\treturn cls._simple_new(tdarr, name=name)\\n\\t@classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: passing TDA and wrong freq to TimedeltaIndex (#31268)",
    "fixed_code": "def __new__(\\n\\t\\tcls,\\n\\t\\tdata=None,\\n\\t\\tunit=None,\\n\\t\\tfreq=None,\\n\\t\\tclosed=None,\\n\\t\\tdtype=_TD_DTYPE,\\n\\t\\tcopy=False,\\n\\t\\tname=None,\\n\\t):\\n\\t\\tname = maybe_extract_name(name, data, cls)\\n\\t\\tif is_scalar(data):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\tf\"{cls.__name__}() must be called with a \"\\n\\t\\t\\t\\tf\"collection of some kind, {repr(data)} was passed\"\\n\\t\\t\\t)\\n\\t\\tif unit in {\"Y\", \"y\", \"M\"}:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"Units 'M' and 'Y' are no longer supported, as they do not \"\\n\\t\\t\\t\\t\"represent unambiguous timedelta values durations.\"\\n\\t\\t\\t)\\n\\t\\tif isinstance(data, TimedeltaArray) and freq is None:\\n\\t\\t\\tif copy:\\n\\t\\t\\t\\tdata = data.copy()\\n\\t\\t\\treturn cls._simple_new(data, name=name, freq=freq)\\n\\t\\tif isinstance(data, TimedeltaIndex) and freq is None and name is None:\\n\\t\\t\\tif copy:\\n\\t\\t\\t\\treturn data.copy()\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn data._shallow_copy()\\n\\t\\ttdarr = TimedeltaArray._from_sequence(\\n\\t\\t\\tdata, freq=freq, unit=unit, dtype=dtype, copy=copy\\n\\t\\t)\\n\\t\\treturn cls._simple_new(tdarr, name=name)\\n\\t@classmethod"
  },
  {
    "code": "def parse_check_update(check_update_output):\\n        out = '\\n'.join((l for l in check_update_output.splitlines() if l))\\n        out = re.sub(r'\\n\\W+(.*)', r' \\1', out)\\n        updates = {}\\n        obsoletes = {}\\n        for line in out.split('\\n'):\\n            line = line.split()\\n            if '*' in line or len(line) not in [3, 6] or '.' not in line[0]:\\n                continue\\n            pkg, version, repo = line[0], line[1], line[2]\\n            name, dist = pkg.rsplit('.', 1)\\n            if name not in updates:\\n                updates[name] = []\\n            updates[name].append({'version': version, 'dist': dist, 'repo': repo})\\n            if len(line) == 6:\\n                obsolete_pkg, obsolete_version, obsolete_repo = line[3], line[4], line[5]\\n                obsolete_name, obsolete_dist = obsolete_pkg.rsplit('.', 1)\\n                if obsolete_name not in obsoletes:\\n                    obsoletes[obsolete_name] = []\\n                obsoletes[obsolete_name].append({'version': obsolete_version, 'dist': obsolete_dist, 'repo': obsolete_repo})\\n        return updates, obsoletes",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_check_update(check_update_output):\\n        out = '\\n'.join((l for l in check_update_output.splitlines() if l))\\n        out = re.sub(r'\\n\\W+(.*)', r' \\1', out)\\n        updates = {}\\n        obsoletes = {}\\n        for line in out.split('\\n'):\\n            line = line.split()\\n            if '*' in line or len(line) not in [3, 6] or '.' not in line[0]:\\n                continue\\n            pkg, version, repo = line[0], line[1], line[2]\\n            name, dist = pkg.rsplit('.', 1)\\n            if name not in updates:\\n                updates[name] = []\\n            updates[name].append({'version': version, 'dist': dist, 'repo': repo})\\n            if len(line) == 6:\\n                obsolete_pkg, obsolete_version, obsolete_repo = line[3], line[4], line[5]\\n                obsolete_name, obsolete_dist = obsolete_pkg.rsplit('.', 1)\\n                if obsolete_name not in obsoletes:\\n                    obsoletes[obsolete_name] = []\\n                obsoletes[obsolete_name].append({'version': obsolete_version, 'dist': obsolete_dist, 'repo': obsolete_repo})\\n        return updates, obsoletes"
  },
  {
    "code": "def cacheContext(self):\\n\\t\\tif self._context is None:\\n\\t\\t\\tctx = self._contextFactory(self.sslmethod)\\n\\t\\t\\tctx.set_options(SSL.OP_NO_SSLv2)\\n\\t\\t\\tctx.use_certificate_file(self.certificateFileName)\\n\\t\\t\\tctx.use_privatekey_file(self.privateKeyFileName)\\n\\t\\t\\tself._context = ctx",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cacheContext(self):\\n\\t\\tif self._context is None:\\n\\t\\t\\tctx = self._contextFactory(self.sslmethod)\\n\\t\\t\\tctx.set_options(SSL.OP_NO_SSLv2)\\n\\t\\t\\tctx.use_certificate_file(self.certificateFileName)\\n\\t\\t\\tctx.use_privatekey_file(self.privateKeyFileName)\\n\\t\\t\\tself._context = ctx"
  },
  {
    "code": "def model_from_yaml(yaml_string, custom_objects=None):\\n  raise RuntimeError(\\n\\t  'Method `model_from_yaml()` has been removed due to security risk of '\\n\\t  'arbitrary code execution. Please use `Model.to_json()` and '\\n\\t  '`model_from_json()` instead.'\\n  )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def model_from_yaml(yaml_string, custom_objects=None):\\n  raise RuntimeError(\\n\\t  'Method `model_from_yaml()` has been removed due to security risk of '\\n\\t  'arbitrary code execution. Please use `Model.to_json()` and '\\n\\t  '`model_from_json()` instead.'\\n  )"
  },
  {
    "code": "def generate_groups(data, label_list, shape, axis=0, factory=lambda x: x):\\n    sorted_data, sorted_labels = _group_reorder(data, label_list, axis=axis)\\n    gen = _generate_groups(sorted_data, sorted_labels, shape,\\n                           0, len(label_list[0]), axis=axis, which=0,\\n                           factory=factory)\\n    for key, group in gen:\\n        yield key, group",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_groups(data, label_list, shape, axis=0, factory=lambda x: x):\\n    sorted_data, sorted_labels = _group_reorder(data, label_list, axis=axis)\\n    gen = _generate_groups(sorted_data, sorted_labels, shape,\\n                           0, len(label_list[0]), axis=axis, which=0,\\n                           factory=factory)\\n    for key, group in gen:\\n        yield key, group"
  },
  {
    "code": "def display_for_field(value, field):\\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\\n    from django.contrib.admin.views.main import EMPTY_CHANGELIST_VALUE\\n    if field.flatchoices:\\n        return dict(field.flatchoices).get(value, EMPTY_CHANGELIST_VALUE)\\n    elif isinstance(field, models.BooleanField) or isinstance(field, models.NullBooleanField):\\n        return _boolean_icon(value)\\n    elif value is None:\\n        return EMPTY_CHANGELIST_VALUE\\n    elif isinstance(field, models.DateTimeField):\\n        return formats.localize(timezone.localtime(value))\\n    elif isinstance(field, models.DateField) or isinstance(field, models.TimeField):\\n        return formats.localize(value)\\n    elif isinstance(field, models.DecimalField):\\n        return formats.number_format(value, field.decimal_places)\\n    elif isinstance(field, models.FloatField):\\n        return formats.number_format(value)\\n    else:\\n        return smart_unicode(value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def display_for_field(value, field):\\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\\n    from django.contrib.admin.views.main import EMPTY_CHANGELIST_VALUE\\n    if field.flatchoices:\\n        return dict(field.flatchoices).get(value, EMPTY_CHANGELIST_VALUE)\\n    elif isinstance(field, models.BooleanField) or isinstance(field, models.NullBooleanField):\\n        return _boolean_icon(value)\\n    elif value is None:\\n        return EMPTY_CHANGELIST_VALUE\\n    elif isinstance(field, models.DateTimeField):\\n        return formats.localize(timezone.localtime(value))\\n    elif isinstance(field, models.DateField) or isinstance(field, models.TimeField):\\n        return formats.localize(value)\\n    elif isinstance(field, models.DecimalField):\\n        return formats.number_format(value, field.decimal_places)\\n    elif isinstance(field, models.FloatField):\\n        return formats.number_format(value)\\n    else:\\n        return smart_unicode(value)"
  },
  {
    "code": "def _coerce_scalar_to_timedelta_type(r, unit='ns', box=True, errors='raise'):\\n    try:\\n        result = Timedelta(r, unit)\\n        if not box:\\n            result = result.asm8\\n    except ValueError:\\n        if errors == 'raise':\\n            raise\\n        elif errors == 'ignore':\\n            return r\\n        result = pd.NaT\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _coerce_scalar_to_timedelta_type(r, unit='ns', box=True, errors='raise'):\\n    try:\\n        result = Timedelta(r, unit)\\n        if not box:\\n            result = result.asm8\\n    except ValueError:\\n        if errors == 'raise':\\n            raise\\n        elif errors == 'ignore':\\n            return r\\n        result = pd.NaT\\n    return result"
  },
  {
    "code": "def apply(self, func, *args, **kwargs):\\n        return self._python_apply_general(func, *args, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def apply(self, func, *args, **kwargs):\\n        return self._python_apply_general(func, *args, **kwargs)"
  },
  {
    "code": "def insert_after_rule(service, old_rule, new_rule):\\n    index = 0\\n    change_count = 0\\n    result = {'action': 'insert_after_rule'}\\n    changed = False\\n    for rule in service.rules:\\n        if (old_rule.rule_type == rule.rule_type and\\n                old_rule.rule_control == rule.rule_control and\\n                old_rule.rule_module_path == rule.rule_module_path):\\n            if (new_rule.rule_type != service.rules[index + 1].rule_type or\\n                    new_rule.rule_control !=\\n                    service.rules[index + 1].rule_control or\\n                    new_rule.rule_module_path !=\\n                    service.rules[index + 1].rule_module_path):\\n                service.rules.insert(index + 1, new_rule)\\n                changed = True\\n            if changed:\\n                result['new_rule'] = str(new_rule)\\n                result['after_rule_' + str(change_count)] = str(rule)\\n                change_count += 1\\n        index += 1\\n    result['change_count'] = change_count\\n    return changed, result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix pamd error when inserting a new rule at the end. Fixes #28487 (#28488)\\n\\nthe last rule, to avoid a list index out of range error when attempting to\\naccess the next rule.",
    "fixed_code": "def insert_after_rule(service, old_rule, new_rule):\\n    index = 0\\n    change_count = 0\\n    result = {'action': 'insert_after_rule'}\\n    changed = False\\n    for rule in service.rules:\\n        if (old_rule.rule_type == rule.rule_type and\\n                old_rule.rule_control == rule.rule_control and\\n                old_rule.rule_module_path == rule.rule_module_path):\\n            if (index == len(service.rules) - 1):\\n                service.rules.insert(len(service.rules), new_rule)\\n                changed = True\\n            elif (new_rule.rule_type != service.rules[index + 1].rule_type or\\n                    new_rule.rule_control !=\\n                    service.rules[index + 1].rule_control or\\n                    new_rule.rule_module_path !=\\n                    service.rules[index + 1].rule_module_path):\\n                service.rules.insert(index + 1, new_rule)\\n                changed = True\\n            if changed:\\n                result['new_rule'] = str(new_rule)\\n                result['after_rule_' + str(change_count)] = str(rule)\\n                change_count += 1\\n        index += 1\\n    result['change_count'] = change_count\\n    return changed, result"
  },
  {
    "code": "def _fit(self, X, skip_num_points=0):\\n        if self.method not in ['barnes_hut', 'exact']:\\n            raise ValueError(\"'method' must be 'barnes_hut' or 'exact'\")\\n        if self.angle < 0.0 or self.angle > 1.0:\\n            raise ValueError(\"'angle' must be between 0.0 - 1.0\")\\n        if self.square_distances not in [True, 'legacy']:\\n            raise ValueError(\"'square_distances' must be True or 'legacy'.\")\\n        if self.metric != \"euclidean\" and self.square_distances is not True:\\n            warnings.warn((\"'square_distances' has been introduced in 0.24\"\\n                           \"to help phase out legacy squaring behavior. The \"\\n                           \"'legacy' setting will be removed in 0.26, and the \"\\n                           \"default setting will be changed to True. In 0.28, \"\\n                           \"'square_distances' will be removed altogether,\"\\n                           \"and distances will be squared by default. Set \"\\n                           \"'square_distances'=True to silence this warning.\"),\\n                          FutureWarning)\\n        if self.method == 'barnes_hut':\\n            X = self._validate_data(X, accept_sparse=['csr'],\\n                                    ensure_min_samples=2,\\n                                    dtype=[np.float32, np.float64])\\n        else:\\n            X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\\n                                    dtype=[np.float32, np.float64])\\n        if self.metric == \"precomputed\":\\n            if isinstance(self.init, str) and self.init == 'pca':\\n                raise ValueError(\"The parameter init=\\\"pca\\\" cannot be \"\\n                                 \"used with metric=\\\"precomputed\\\".\")\\n            if X.shape[0] != X.shape[1]:\\n                raise ValueError(\"X should be a square distance matrix\")\\n            check_non_negative(X, \"TSNE.fit(). With metric='precomputed', X \"\\n                                  \"should contain positive distances.\")\\n            if self.method == \"exact\" and issparse(X):\\n                raise TypeError(\\n                    'TSNE with method=\"exact\" does not accept sparse '\\n                    'precomputed distance matrix. Use method=\"barnes_hut\" '\\n                    'or provide the dense distance matrix.')\\n        if self.method == 'barnes_hut' and self.n_components > 3:\\n            raise ValueError(\"'n_components' should be inferior to 4 for the \"\\n                             \"barnes_hut algorithm as it relies on \"\\n                             \"quad-tree or oct-tree.\")\\n        random_state = check_random_state(self.random_state)\\n        if self.early_exaggeration < 1.0:\\n            raise ValueError(\"early_exaggeration must be at least 1, but is {}\"\\n                             .format(self.early_exaggeration))\\n        if self.n_iter < 250:\\n            raise ValueError(\"n_iter should be at least 250\")\\n        n_samples = X.shape[0]\\n        neighbors_nn = None\\n        if self.method == \"exact\":\\n            if self.metric == \"precomputed\":\\n                distances = X\\n            else:\\n                if self.verbose:\\n                    print(\"[t-SNE] Computing pairwise distances...\")\\n                if self.metric == \"euclidean\":\\n                    distances = pairwise_distances(X, metric=self.metric,\\n                                                   squared=True)\\n                else:\\n                    distances = pairwise_distances(X, metric=self.metric,\\n                                                   n_jobs=self.n_jobs)\\n            if np.any(distances < 0):\\n                raise ValueError(\"All distances should be positive, the \"\\n                                 \"metric given is not correct\")\\n            if self.metric != \"euclidean\" and self.square_distances is True:\\n                distances **= 2\\n            P = _joint_probabilities(distances, self.perplexity, self.verbose)\\n            assert np.all(np.isfinite(P)), \"All probabilities should be finite\"\\n            assert np.all(P >= 0), \"All probabilities should be non-negative\"\\n            assert np.all(P <= 1), (\"All probabilities should be less \"\\n                                    \"or then equal to one\")\\n        else:\\n            n_neighbors = min(n_samples - 1, int(3. * self.perplexity + 1))\\n            if self.verbose:\\n                print(\"[t-SNE] Computing {} nearest neighbors...\"\\n                      .format(n_neighbors))\\n            knn = NearestNeighbors(algorithm='auto',\\n                                   n_jobs=self.n_jobs,\\n                                   n_neighbors=n_neighbors,\\n                                   metric=self.metric)\\n            t0 = time()\\n            knn.fit(X)\\n            duration = time() - t0\\n            if self.verbose:\\n                print(\"[t-SNE] Indexed {} samples in {:.3f}s...\".format(\\n                    n_samples, duration))\\n            t0 = time()\\n            distances_nn = knn.kneighbors_graph(mode='distance')\\n            duration = time() - t0\\n            if self.verbose:\\n                print(\"[t-SNE] Computed neighbors for {} samples \"\\n                      \"in {:.3f}s...\".format(n_samples, duration))\\n            del knn\\n            if self.square_distances is True or self.metric == \"euclidean\":\\n                distances_nn.data **= 2\\n            P = _joint_probabilities_nn(distances_nn, self.perplexity,\\n                                        self.verbose)\\n        if isinstance(self.init, np.ndarray):\\n            X_embedded = self.init\\n        elif self.init == 'pca':\\n            pca = PCA(n_components=self.n_components, svd_solver='randomized',\\n                      random_state=random_state)\\n            X_embedded = pca.fit_transform(X).astype(np.float32, copy=False)\\n        elif self.init == 'random':\\n            X_embedded = 1e-4 * random_state.randn(\\n                n_samples, self.n_components).astype(np.float32)\\n        else:\\n            raise ValueError(\"'init' must be 'pca', 'random', or \"\\n                             \"a numpy array\")\\n        degrees_of_freedom = max(self.n_components - 1, 1)\\n        return self._tsne(P, degrees_of_freedom, n_samples,\\n                          X_embedded=X_embedded,\\n                          neighbors=neighbors_nn,\\n                          skip_num_points=skip_num_points)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fit(self, X, skip_num_points=0):\\n        if self.method not in ['barnes_hut', 'exact']:\\n            raise ValueError(\"'method' must be 'barnes_hut' or 'exact'\")\\n        if self.angle < 0.0 or self.angle > 1.0:\\n            raise ValueError(\"'angle' must be between 0.0 - 1.0\")\\n        if self.square_distances not in [True, 'legacy']:\\n            raise ValueError(\"'square_distances' must be True or 'legacy'.\")\\n        if self.metric != \"euclidean\" and self.square_distances is not True:\\n            warnings.warn((\"'square_distances' has been introduced in 0.24\"\\n                           \"to help phase out legacy squaring behavior. The \"\\n                           \"'legacy' setting will be removed in 0.26, and the \"\\n                           \"default setting will be changed to True. In 0.28, \"\\n                           \"'square_distances' will be removed altogether,\"\\n                           \"and distances will be squared by default. Set \"\\n                           \"'square_distances'=True to silence this warning.\"),\\n                          FutureWarning)\\n        if self.method == 'barnes_hut':\\n            X = self._validate_data(X, accept_sparse=['csr'],\\n                                    ensure_min_samples=2,\\n                                    dtype=[np.float32, np.float64])\\n        else:\\n            X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\\n                                    dtype=[np.float32, np.float64])\\n        if self.metric == \"precomputed\":\\n            if isinstance(self.init, str) and self.init == 'pca':\\n                raise ValueError(\"The parameter init=\\\"pca\\\" cannot be \"\\n                                 \"used with metric=\\\"precomputed\\\".\")\\n            if X.shape[0] != X.shape[1]:\\n                raise ValueError(\"X should be a square distance matrix\")\\n            check_non_negative(X, \"TSNE.fit(). With metric='precomputed', X \"\\n                                  \"should contain positive distances.\")\\n            if self.method == \"exact\" and issparse(X):\\n                raise TypeError(\\n                    'TSNE with method=\"exact\" does not accept sparse '\\n                    'precomputed distance matrix. Use method=\"barnes_hut\" '\\n                    'or provide the dense distance matrix.')\\n        if self.method == 'barnes_hut' and self.n_components > 3:\\n            raise ValueError(\"'n_components' should be inferior to 4 for the \"\\n                             \"barnes_hut algorithm as it relies on \"\\n                             \"quad-tree or oct-tree.\")\\n        random_state = check_random_state(self.random_state)\\n        if self.early_exaggeration < 1.0:\\n            raise ValueError(\"early_exaggeration must be at least 1, but is {}\"\\n                             .format(self.early_exaggeration))\\n        if self.n_iter < 250:\\n            raise ValueError(\"n_iter should be at least 250\")\\n        n_samples = X.shape[0]\\n        neighbors_nn = None\\n        if self.method == \"exact\":\\n            if self.metric == \"precomputed\":\\n                distances = X\\n            else:\\n                if self.verbose:\\n                    print(\"[t-SNE] Computing pairwise distances...\")\\n                if self.metric == \"euclidean\":\\n                    distances = pairwise_distances(X, metric=self.metric,\\n                                                   squared=True)\\n                else:\\n                    distances = pairwise_distances(X, metric=self.metric,\\n                                                   n_jobs=self.n_jobs)\\n            if np.any(distances < 0):\\n                raise ValueError(\"All distances should be positive, the \"\\n                                 \"metric given is not correct\")\\n            if self.metric != \"euclidean\" and self.square_distances is True:\\n                distances **= 2\\n            P = _joint_probabilities(distances, self.perplexity, self.verbose)\\n            assert np.all(np.isfinite(P)), \"All probabilities should be finite\"\\n            assert np.all(P >= 0), \"All probabilities should be non-negative\"\\n            assert np.all(P <= 1), (\"All probabilities should be less \"\\n                                    \"or then equal to one\")\\n        else:\\n            n_neighbors = min(n_samples - 1, int(3. * self.perplexity + 1))\\n            if self.verbose:\\n                print(\"[t-SNE] Computing {} nearest neighbors...\"\\n                      .format(n_neighbors))\\n            knn = NearestNeighbors(algorithm='auto',\\n                                   n_jobs=self.n_jobs,\\n                                   n_neighbors=n_neighbors,\\n                                   metric=self.metric)\\n            t0 = time()\\n            knn.fit(X)\\n            duration = time() - t0\\n            if self.verbose:\\n                print(\"[t-SNE] Indexed {} samples in {:.3f}s...\".format(\\n                    n_samples, duration))\\n            t0 = time()\\n            distances_nn = knn.kneighbors_graph(mode='distance')\\n            duration = time() - t0\\n            if self.verbose:\\n                print(\"[t-SNE] Computed neighbors for {} samples \"\\n                      \"in {:.3f}s...\".format(n_samples, duration))\\n            del knn\\n            if self.square_distances is True or self.metric == \"euclidean\":\\n                distances_nn.data **= 2\\n            P = _joint_probabilities_nn(distances_nn, self.perplexity,\\n                                        self.verbose)\\n        if isinstance(self.init, np.ndarray):\\n            X_embedded = self.init\\n        elif self.init == 'pca':\\n            pca = PCA(n_components=self.n_components, svd_solver='randomized',\\n                      random_state=random_state)\\n            X_embedded = pca.fit_transform(X).astype(np.float32, copy=False)\\n        elif self.init == 'random':\\n            X_embedded = 1e-4 * random_state.randn(\\n                n_samples, self.n_components).astype(np.float32)\\n        else:\\n            raise ValueError(\"'init' must be 'pca', 'random', or \"\\n                             \"a numpy array\")\\n        degrees_of_freedom = max(self.n_components - 1, 1)\\n        return self._tsne(P, degrees_of_freedom, n_samples,\\n                          X_embedded=X_embedded,\\n                          neighbors=neighbors_nn,\\n                          skip_num_points=skip_num_points)"
  },
  {
    "code": "def get_language_info(lang_code):\\n    from django.conf.locale import LANG_INFO\\n    try:\\n        return LANG_INFO[lang_code]\\n    except KeyError:\\n        if '-' in lang_code:\\n            splited_lang_code = lang_code.split('-')[0]\\n            try:\\n                return LANG_INFO[splited_lang_code]\\n            except KeyError:\\n                raise KeyError(\"Unknown language code %r and %r.\" % (lang_code, splited_lang_code))\\n        raise KeyError(\"Unknown language code %r.\" % lang_code)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_language_info(lang_code):\\n    from django.conf.locale import LANG_INFO\\n    try:\\n        return LANG_INFO[lang_code]\\n    except KeyError:\\n        if '-' in lang_code:\\n            splited_lang_code = lang_code.split('-')[0]\\n            try:\\n                return LANG_INFO[splited_lang_code]\\n            except KeyError:\\n                raise KeyError(\"Unknown language code %r and %r.\" % (lang_code, splited_lang_code))\\n        raise KeyError(\"Unknown language code %r.\" % lang_code)"
  },
  {
    "code": "def test_smoothing():\\t\\ttimer = DiscreteTimer()",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "fix smoothing test",
    "fixed_code": "def test_smoothing():\\n\\ttimer = DiscreteTimer()\\n\\twith closing(StringIO()) as our_file:\\n\\t\\twith tqdm(_range(3), file=our_file, smoothing=None, leave=True) as t:\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\tfor _ in t:\\n\\t\\t\\t\\tpass\\n\\t\\tour_file.seek(0)\\n\\t\\tassert '| 3/3 ' in our_file.read()\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=None, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\twith tqdm(_range(3), file=our_file, smoothing=None, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0) as t2:\\n\\t\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\t\\tt.update()\\n\\t\\t\\tn_old = len(tqdm._instances)\\n\\t\\t\\tt.close()\\n\\t\\t\\tassert len(tqdm._instances) == n_old - 1\\n\\t\\t\\ta = progressbar_rate(get_bar(our_file, 3))\\n\\t\\ta2 = progressbar_rate(get_bar(our_file2, 3))\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=1, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\twith tqdm(_range(3), file=our_file, smoothing=1, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0) as t2:\\n\\t\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\t\\tt.update()\\n\\t\\t\\tt.close()\\n\\t\\t\\tb = progressbar_rate(get_bar(our_file, 3))\\n\\t\\tb2 = progressbar_rate(get_bar(our_file2, 3))\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=0.5, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\tt2 = tqdm(_range(3), file=our_file, smoothing=0.5, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\tt.update()\\n\\t\\t\\tt2.close()\\n\\t\\t\\tt.close()\\n\\t\\t\\tc = progressbar_rate(get_bar(our_file, 3))\\n\\t\\tc2 = progressbar_rate(get_bar(our_file2, 3))\\n\\tassert a == c < b\\n\\tassert a2 < c2 < b2"
  },
  {
    "code": "def get_callable(lookup_view, can_fail=False):\\n    if not callable(lookup_view):\\n        mod_name, func_name = get_mod_func(lookup_view)\\n        try:\\n            if func_name != '':\\n                lookup_view = getattr(__import__(mod_name, {}, {}, ['']), func_name)\\n        except (ImportError, AttributeError):\\n            if not can_fail:\\n                raise\\n    return lookup_view\\nget_callable = memoize(get_callable, _callable_cache)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_callable(lookup_view, can_fail=False):\\n    if not callable(lookup_view):\\n        mod_name, func_name = get_mod_func(lookup_view)\\n        try:\\n            if func_name != '':\\n                lookup_view = getattr(__import__(mod_name, {}, {}, ['']), func_name)\\n        except (ImportError, AttributeError):\\n            if not can_fail:\\n                raise\\n    return lookup_view\\nget_callable = memoize(get_callable, _callable_cache)"
  },
  {
    "code": "def serialize_headers(self):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def serialize_headers(self):"
  },
  {
    "code": "def poke(self, context):\\n        conn = BaseHook.get_connection(self.conn_id)\\n        allowed_conn_type = {'google_cloud_platform', 'jdbc', 'mssql',\\n                             'mysql', 'oracle', 'postgres',\\n                             'presto', 'sqlite', 'vertica'}\\n        if conn.conn_type not in allowed_conn_type:\\n            raise AirflowException(\"The connection type is not supported by SqlSensor. \" +\\n                                   \"Supported connection types: {}\".format(list(allowed_conn_type)))\\n        hook = conn.get_hook()\\n        self.log.info('Poking: %s (with parameters %s)', self.sql, self.parameters)\\n        records = hook.get_records(self.sql, self.parameters)\\n        if not records:\\n            return False\\n        return str(records[0][0]) not in ('0', '')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def poke(self, context):\\n        conn = BaseHook.get_connection(self.conn_id)\\n        allowed_conn_type = {'google_cloud_platform', 'jdbc', 'mssql',\\n                             'mysql', 'oracle', 'postgres',\\n                             'presto', 'sqlite', 'vertica'}\\n        if conn.conn_type not in allowed_conn_type:\\n            raise AirflowException(\"The connection type is not supported by SqlSensor. \" +\\n                                   \"Supported connection types: {}\".format(list(allowed_conn_type)))\\n        hook = conn.get_hook()\\n        self.log.info('Poking: %s (with parameters %s)', self.sql, self.parameters)\\n        records = hook.get_records(self.sql, self.parameters)\\n        if not records:\\n            return False\\n        return str(records[0][0]) not in ('0', '')"
  },
  {
    "code": "def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\\n\\t\\tif mode is None:\\n\\t\\t\\treturn changed\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tif expand:\\n\\t\\t\\tb_path = os.path.expanduser(os.path.expandvars(b_path))\\n\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\tif self.check_file_absent_if_check_mode(b_path):\\n\\t\\t\\treturn True\\n\\t\\tif not isinstance(mode, int):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tmode = int(mode, 8)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tmode = self._symbolic_mode_to_octal(path_stat, mode)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path,\\n\\t\\t\\t\\t\\t\\t\\t\\t   msg=\"mode must be in octal or symbolic form\",\\n\\t\\t\\t\\t\\t\\t\\t\\t   details=to_native(e))\\n\\t\\t\\t\\tif mode != stat.S_IMODE(mode):\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\\n\\t\\tprev_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\tif prev_mode != mode:\\n\\t\\t\\tif diff is not None:\\n\\t\\t\\t\\tif 'before' not in diff:\\n\\t\\t\\t\\t\\tdiff['before'] = {}\\n\\t\\t\\t\\tdiff['before']['mode'] = '0%03o' % prev_mode\\n\\t\\t\\t\\tif 'after' not in diff:\\n\\t\\t\\t\\t\\tdiff['after'] = {}\\n\\t\\t\\t\\tdiff['after']['mode'] = '0%03o' % mode\\n\\t\\t\\tif self.check_mode:\\n\\t\\t\\t\\treturn True\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif hasattr(os, 'lchmod'):\\n\\t\\t\\t\\t\\tos.lchmod(b_path, mode)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not os.path.islink(b_path):\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tunderlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\t\\tnew_underlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tif underlying_stat.st_mode != new_underlying_stat.st_mode:\\n\\t\\t\\t\\t\\t\\t\\tos.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\\n\\t\\t\\texcept OSError as e:\\n\\t\\t\\t\\tif os.path.islink(b_path) and e.errno in (errno.EPERM, errno.EROFS):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telif e.errno in (errno.ENOENT, errno.ELOOP):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\tself.fail_json(path=path, msg='chmod failed', details=to_native(e),\\n\\t\\t\\t\\t\\t\\t\\t   exception=traceback.format_exc())\\n\\t\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\t\\tnew_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\t\\tif new_mode != prev_mode:\\n\\t\\t\\t\\tchanged = True\\n\\t\\treturn changed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "AnsibleModule.set_mode_if_different: handle symlink is in a sticky directory (#45198)\\n\\n\\n\\nThe builtins import was removed since it was unused, but it is now needed.",
    "fixed_code": "def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\\n\\t\\tif mode is None:\\n\\t\\t\\treturn changed\\n\\t\\tb_path = to_bytes(path, errors='surrogate_or_strict')\\n\\t\\tif expand:\\n\\t\\t\\tb_path = os.path.expanduser(os.path.expandvars(b_path))\\n\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\tif self.check_file_absent_if_check_mode(b_path):\\n\\t\\t\\treturn True\\n\\t\\tif not isinstance(mode, int):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tmode = int(mode, 8)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tmode = self._symbolic_mode_to_octal(path_stat, mode)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path,\\n\\t\\t\\t\\t\\t\\t\\t\\t   msg=\"mode must be in octal or symbolic form\",\\n\\t\\t\\t\\t\\t\\t\\t\\t   details=to_native(e))\\n\\t\\t\\t\\tif mode != stat.S_IMODE(mode):\\n\\t\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\t\\tself.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\\n\\t\\tprev_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\tif prev_mode != mode:\\n\\t\\t\\tif diff is not None:\\n\\t\\t\\t\\tif 'before' not in diff:\\n\\t\\t\\t\\t\\tdiff['before'] = {}\\n\\t\\t\\t\\tdiff['before']['mode'] = '0%03o' % prev_mode\\n\\t\\t\\t\\tif 'after' not in diff:\\n\\t\\t\\t\\t\\tdiff['after'] = {}\\n\\t\\t\\t\\tdiff['after']['mode'] = '0%03o' % mode\\n\\t\\t\\tif self.check_mode:\\n\\t\\t\\t\\treturn True\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif hasattr(os, 'lchmod'):\\n\\t\\t\\t\\t\\tos.lchmod(b_path, mode)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not os.path.islink(b_path):\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tunderlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tos.chmod(b_path, mode)\\n\\t\\t\\t\\t\\t\\tnew_underlying_stat = os.stat(b_path)\\n\\t\\t\\t\\t\\t\\tif underlying_stat.st_mode != new_underlying_stat.st_mode:\\n\\t\\t\\t\\t\\t\\t\\tos.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\\n\\t\\t\\texcept OSError as e:\\n\\t\\t\\t\\tif os.path.islink(b_path) and e.errno in (\\n\\t\\t\\t\\t\\terrno.EACCES,  \\n\\t\\t\\t\\t\\terrno.EPERM,  \\n\\t\\t\\t\\t\\terrno.EROFS,  \\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telif e.errno in (errno.ENOENT, errno.ELOOP):  \\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tpath = to_text(b_path)\\n\\t\\t\\t\\tself.fail_json(path=path, msg='chmod failed', details=to_native(e),\\n\\t\\t\\t\\t\\t\\t\\t   exception=traceback.format_exc())\\n\\t\\t\\tpath_stat = os.lstat(b_path)\\n\\t\\t\\tnew_mode = stat.S_IMODE(path_stat.st_mode)\\n\\t\\t\\tif new_mode != prev_mode:\\n\\t\\t\\t\\tchanged = True\\n\\t\\treturn changed"
  },
  {
    "code": "def serve(request, path, document_root=None, show_indexes=False):\\n\\tpath = posixpath.normpath(unquote(path))\\n\\tpath = path.lstrip('/')\\n\\tnewpath = ''\\n\\tfor part in path.split('/'):\\n\\t\\tif not part:\\n\\t\\t\\tcontinue\\n\\t\\tdrive, part = os.path.splitdrive(part)\\n\\t\\thead, part = os.path.split(part)\\n\\t\\tif part in (os.curdir, os.pardir):\\n\\t\\t\\tcontinue\\n\\t\\tnewpath = os.path.join(newpath, part).replace('\\\\', '/')\\n\\tif newpath and path != newpath:\\n\\t\\treturn HttpResponseRedirect(newpath)\\n\\tfullpath = os.path.join(document_root, newpath)\\n\\tif os.path.isdir(fullpath):\\n\\t\\tif show_indexes:\\n\\t\\t\\treturn directory_index(newpath, fullpath)\\n\\t\\traise Http404(_(\"Directory indexes are not allowed here.\"))\\n\\tif not os.path.exists(fullpath):\\n\\t\\traise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\\n\\tstatobj = os.stat(fullpath)\\n\\tif not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\\n\\t\\t\\t\\t\\t\\t\\t  statobj.st_mtime, statobj.st_size):\\n\\t\\treturn HttpResponseNotModified()\\n\\tcontent_type, encoding = mimetypes.guess_type(fullpath)\\n\\tcontent_type = content_type or 'application/octet-stream'\\n\\tresponse = FileResponse(open(fullpath, 'rb'), content_type=content_type)\\n\\tresponse[\"Last-Modified\"] = http_date(statobj.st_mtime)\\n\\tif stat.S_ISREG(statobj.st_mode):\\n\\t\\tresponse[\"Content-Length\"] = statobj.st_size\\n\\tif encoding:\\n\\t\\tresponse[\"Content-Encoding\"] = encoding\\n\\treturn response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[1.10.x] Fixed CVE-2017-7234 -- Fixed open redirect vulnerability in views.static.serve().\\n\\nThis is a security fix.",
    "fixed_code": "def serve(request, path, document_root=None, show_indexes=False):\\n\\tpath = posixpath.normpath(unquote(path)).lstrip('/')\\n\\tfullpath = safe_join(document_root, path)\\n\\tif os.path.isdir(fullpath):\\n\\t\\tif show_indexes:\\n\\t\\t\\treturn directory_index(path, fullpath)\\n\\t\\traise Http404(_(\"Directory indexes are not allowed here.\"))\\n\\tif not os.path.exists(fullpath):\\n\\t\\traise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\\n\\tstatobj = os.stat(fullpath)\\n\\tif not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\\n\\t\\t\\t\\t\\t\\t\\t  statobj.st_mtime, statobj.st_size):\\n\\t\\treturn HttpResponseNotModified()\\n\\tcontent_type, encoding = mimetypes.guess_type(fullpath)\\n\\tcontent_type = content_type or 'application/octet-stream'\\n\\tresponse = FileResponse(open(fullpath, 'rb'), content_type=content_type)\\n\\tresponse[\"Last-Modified\"] = http_date(statobj.st_mtime)\\n\\tif stat.S_ISREG(statobj.st_mode):\\n\\t\\tresponse[\"Content-Length\"] = statobj.st_size\\n\\tif encoding:\\n\\t\\tresponse[\"Content-Encoding\"] = encoding\\n\\treturn response"
  },
  {
    "code": "def __getitem__(self, key):\\n        if key != DEFAULTSECT and not self.has_section(key):\\n            raise KeyError(key)\\n        return self._proxies[key]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "configparser: the name of the DEFAULT section is now customizable",
    "fixed_code": "def __getitem__(self, key):\\n        if key != self._default_section and not self.has_section(key):\\n            raise KeyError(key)\\n        return self._proxies[key]"
  },
  {
    "code": "def do_setup_package_providers(provider_module: str, deps: List[str], extras: Dict[str, List[str]]):\\n    setup.write_version()\\n    copy_provider_sources()\\n    provider_package_name = get_provider_package_name(provider_module)\\n    package_name = f'{provider_package_name}' if provider_module != \"providers\" \\\\n        else f'apache-airflow-providers'\\n    package_prefix = f'airflow.providers.{provider_module}' if provider_module != 'providers' \\\\n        else 'airflow.providers'\\n    found_packages = find_packages()\\n    found_packages = [package for package in found_packages if package.startswith(package_prefix)]\\n    setuptools_setup(\\n        name=package_name,\\n        description=f'Back-porting ${package_name} package for Airflow 1.10.*',\\n        long_description=f,\\n        long_description_content_type='text/markdown',\\n        license='Apache License 2.0',\\n        version='0.0.1',\\n        packages=found_packages,\\n        include_package_data=True,\\n        zip_safe=False,\\n        install_requires=['apache-airflow~=1.10'] + deps,\\n        extras_require=extras,\\n        classifiers=[\\n            'Development Status :: 5 - Production/Stable',\\n            'Environment :: Console',\\n            'Intended Audience :: Developers',\\n            'Intended Audience :: System Administrators',\\n            'License :: OSI Approved :: Apache Software License',\\n            'Programming Language :: Python :: 3.6',\\n            'Programming Language :: Python :: 3.7',\\n            'Topic :: System :: Monitoring',\\n        ],\\n        python_requires='>=3.6',\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6980] Improve system tests and building providers package (#7615)\\n\\nThis PR removes initdb from system tests setup as it seems unneccessary operation.\\nAlso some automatic code changes has been added before building providers package.\\n\\nfixup! [AIRFLOW-6980] Improve system tests and building providers package\\n\\nfixup! [AIRFLOW-6980] Improve system tests and building providers package\\n\\nfixup! fixup! [AIRFLOW-6980] Improve system tests and building providers package",
    "fixed_code": "def do_setup_package_providers(provider_module: str, deps: List[str], extras: Dict[str, List[str]]):\\n    setup.write_version()\\n    provider_package_name = get_provider_package_name(provider_module)\\n    package_name = f'{provider_package_name}' if provider_module != \"providers\" \\\\n        else f'apache-airflow-providers'\\n    package_prefix = f'airflow.providers.{provider_module}' if provider_module != 'providers' \\\\n        else 'airflow.providers'\\n    found_packages = find_packages()\\n    found_packages = [package for package in found_packages if package.startswith(package_prefix)]\\n    setuptools_setup(\\n        name=package_name,\\n        description=f'Back-porting ${package_name} package for Airflow 1.10.*',\\n        long_description=f,\\n        long_description_content_type='text/markdown',\\n        license='Apache License 2.0',\\n        version='0.0.1',\\n        packages=found_packages,\\n        include_package_data=True,\\n        zip_safe=False,\\n        install_requires=['apache-airflow~=1.10'] + deps,\\n        extras_require=extras,\\n        classifiers=[\\n            'Development Status :: 5 - Production/Stable',\\n            'Environment :: Console',\\n            'Intended Audience :: Developers',\\n            'Intended Audience :: System Administrators',\\n            'License :: OSI Approved :: Apache Software License',\\n            'Programming Language :: Python :: 3.6',\\n            'Programming Language :: Python :: 3.7',\\n            'Topic :: System :: Monitoring',\\n        ],\\n        python_requires='>=3.6',\\n    )"
  },
  {
    "code": "def permutation_importance(\\n    estimator,\\n    X,\\n    y,\\n    *,\\n    scoring=None,\\n    n_repeats=5,\\n    n_jobs=None,\\n    random_state=None,\\n    sample_weight=None,\\n):\\n    if not hasattr(X, \"iloc\"):\\n        X = check_array(X, force_all_finite=\"allow-nan\", dtype=None)\\n    random_state = check_random_state(random_state)\\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\\n    if callable(scoring):\\n        scorer = scoring\\n    elif scoring is None or isinstance(scoring, str):\\n        scorer = check_scoring(estimator, scoring=scoring)\\n    else:\\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\\n        scorer = _MultimetricScorer(**scorers_dict)\\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\\n    scores = Parallel(n_jobs=n_jobs)(\\n        delayed(_calculate_permutation_scores)(\\n            estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer\\n        )\\n        for col_idx in range(X.shape[1])\\n    )\\n    if isinstance(baseline_score, dict):\\n        return {\\n            name: _create_importances_bunch(\\n                baseline_score[name],\\n                np.array([scores[col_idx][name] for col_idx in range(X.shape[1])]),\\n            )\\n            for name in baseline_score\\n        }\\n    else:\\n        return _create_importances_bunch(baseline_score, np.array(scores))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH add max_samples parameters in permutation_importances (#20431)",
    "fixed_code": "def permutation_importance(\\n    estimator,\\n    X,\\n    y,\\n    *,\\n    scoring=None,\\n    n_repeats=5,\\n    n_jobs=None,\\n    random_state=None,\\n    sample_weight=None,\\n    max_samples=1.0,\\n):\\n    if not hasattr(X, \"iloc\"):\\n        X = check_array(X, force_all_finite=\"allow-nan\", dtype=None)\\n    random_state = check_random_state(random_state)\\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\\n    if not isinstance(max_samples, numbers.Integral):\\n        max_samples = int(max_samples * X.shape[0])\\n    elif not (0 < max_samples <= X.shape[0]):\\n        raise ValueError(\"max_samples must be in (0, n_samples]\")\\n    if callable(scoring):\\n        scorer = scoring\\n    elif scoring is None or isinstance(scoring, str):\\n        scorer = check_scoring(estimator, scoring=scoring)\\n    else:\\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\\n        scorer = _MultimetricScorer(**scorers_dict)\\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\\n    scores = Parallel(n_jobs=n_jobs)(\\n        delayed(_calculate_permutation_scores)(\\n            estimator,\\n            X,\\n            y,\\n            sample_weight,\\n            col_idx,\\n            random_seed,\\n            n_repeats,\\n            scorer,\\n            max_samples,\\n        )\\n        for col_idx in range(X.shape[1])\\n    )\\n    if isinstance(baseline_score, dict):\\n        return {\\n            name: _create_importances_bunch(\\n                baseline_score[name],\\n                np.array([scores[col_idx][name] for col_idx in range(X.shape[1])]),\\n            )\\n            for name in baseline_score\\n        }\\n    else:\\n        return _create_importances_bunch(baseline_score, np.array(scores))"
  },
  {
    "code": "def _cursor(self):\\n        cursor = None\\n        if not self._valid_connection():\\n            conn_string = convert_unicode(self._connect_string())\\n            self.connection = Database.connect(conn_string, **self.settings_dict['DATABASE_OPTIONS'])\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n            cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS' \"\\n                           \"NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF' \"\\n                           \"NLS_TERRITORY = 'AMERICA'\")\\n            try:\\n                self.oracle_version = int(self.connection.version.split('.')[0])\\n                if self.oracle_version <= 9:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_9\\n                else:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_10\\n            except ValueError:\\n                pass\\n            try:\\n                self.connection.stmtcachesize = 20\\n            except:\\n                pass\\n            connection_created.send(sender=self.__class__)\\n        if not cursor:\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n        return cursor",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _cursor(self):\\n        cursor = None\\n        if not self._valid_connection():\\n            conn_string = convert_unicode(self._connect_string())\\n            self.connection = Database.connect(conn_string, **self.settings_dict['DATABASE_OPTIONS'])\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n            cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS' \"\\n                           \"NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF' \"\\n                           \"NLS_TERRITORY = 'AMERICA'\")\\n            try:\\n                self.oracle_version = int(self.connection.version.split('.')[0])\\n                if self.oracle_version <= 9:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_9\\n                else:\\n                    self.ops.regex_lookup = self.ops.regex_lookup_10\\n            except ValueError:\\n                pass\\n            try:\\n                self.connection.stmtcachesize = 20\\n            except:\\n                pass\\n            connection_created.send(sender=self.__class__)\\n        if not cursor:\\n            cursor = FormatStylePlaceholderCursor(self.connection)\\n        return cursor"
  },
  {
    "code": "def __str__(self):\\n        template = '(NOT (%s: %s))' if self.negated else '(%s: %s)'\\n        return force_str(template % (self.connector, ', '.join(force_text(c) for c in self.children)))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __str__(self):\\n        template = '(NOT (%s: %s))' if self.negated else '(%s: %s)'\\n        return force_str(template % (self.connector, ', '.join(force_text(c) for c in self.children)))"
  },
  {
    "code": "def func_supports_parameter(func, parameter):\\n    return parameter in _get_signature(func).parameters",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #31732 -- Fixed django.utils.inspect caching for bound methods.\\n\\nThanks Alexandr Artemyev for the report, and Simon Charette for the\\noriginal patch.",
    "fixed_code": "def func_supports_parameter(func, name):\\n    return any(param.name == name for param in _get_callable_parameters(func))"
  },
  {
    "code": "def map_params_to_obj(module):\\n    text = module.params['text']\\n    if text:\\n        text = \"%r\" % (str(text).strip())\\n    return {\\n        'banner': module.params['banner'],\\n        'text': text,\\n        'state': module.params['state']\\n    }",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_params_to_obj(module):\\n    text = module.params['text']\\n    if text:\\n        text = \"%r\" % (str(text).strip())\\n    return {\\n        'banner': module.params['banner'],\\n        'text': text,\\n        'state': module.params['state']\\n    }"
  },
  {
    "code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_cols=None, parse_dates=False, date_parser=None,\\n                    na_values=None, thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        should_parse = {}\\n        for row in sheet.iter_rows():\\n            row_data = []\\n            for j, cell in enumerate(row):\\n                if parse_cols is not None and j not in should_parse:\\n                    should_parse[j] = self._should_parse(j, parse_cols)\\n                if parse_cols is None or should_parse[j]:\\n                    row_data.append(cell.internal_value)\\n            data.append(row_data)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    def _parse_xls(self, sheetname, header=0, skiprows=None, index_col=None,\\n                   parse_cols=None, parse_dates=False, date_parser=None,\\n                   na_values=None, thousands=None, chunksize=None):\\n        from datetime import MINYEAR, time, datetime\\n        from xlrd import xldate_as_tuple, XL_CELL_DATE, XL_CELL_ERROR\\n        datemode = self.book.datemode\\n        sheet = self.book.sheet_by_name(sheetname)\\n        data = []\\n        should_parse = {}\\n        for i in range(sheet.nrows):\\n            row = []\\n            for j, (value, typ) in enumerate(izip(sheet.row_values(i),\\n                                                  sheet.row_types(i))):\\n                if parse_cols is not None and j not in should_parse:\\n                    should_parse[j] = self._should_parse(j, parse_cols)\\n                if parse_cols is None or should_parse[j]:\\n                    if typ == XL_CELL_DATE:\\n                        dt = xldate_as_tuple(value, datemode)\\n                        if dt[0] < MINYEAR: \\n                            value = time(*dt[3:])\\n                        else:\\n                            value = datetime(*dt)\\n                    if typ == XL_CELL_ERROR:\\n                        value = np.nan\\n                    row.append(value)\\n            data.append(row)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_cols=None, parse_dates=False, date_parser=None,\\n                    na_values=None, thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        should_parse = {}\\n        for row in sheet.iter_rows():\\n            row_data = []\\n            for j, cell in enumerate(row):\\n                if parse_cols is not None and j not in should_parse:\\n                    should_parse[j] = self._should_parse(j, parse_cols)\\n                if parse_cols is None or should_parse[j]:\\n                    row_data.append(cell.internal_value)\\n            data.append(row_data)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    def _parse_xls(self, sheetname, header=0, skiprows=None, index_col=None,\\n                   parse_cols=None, parse_dates=False, date_parser=None,\\n                   na_values=None, thousands=None, chunksize=None):\\n        from datetime import MINYEAR, time, datetime\\n        from xlrd import xldate_as_tuple, XL_CELL_DATE, XL_CELL_ERROR\\n        datemode = self.book.datemode\\n        sheet = self.book.sheet_by_name(sheetname)\\n        data = []\\n        should_parse = {}\\n        for i in range(sheet.nrows):\\n            row = []\\n            for j, (value, typ) in enumerate(izip(sheet.row_values(i),\\n                                                  sheet.row_types(i))):\\n                if parse_cols is not None and j not in should_parse:\\n                    should_parse[j] = self._should_parse(j, parse_cols)\\n                if parse_cols is None or should_parse[j]:\\n                    if typ == XL_CELL_DATE:\\n                        dt = xldate_as_tuple(value, datemode)\\n                        if dt[0] < MINYEAR: \\n                            value = time(*dt[3:])\\n                        else:\\n                            value = datetime(*dt)\\n                    if typ == XL_CELL_ERROR:\\n                        value = np.nan\\n                    row.append(value)\\n            data.append(row)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    @property"
  },
  {
    "code": "def _construct_divmod_result(left, result, index, name, dtype=None):\\n    return (\\n        _construct_result(left, result[0], index=index, name=name,\\n                          dtype=dtype),\\n        _construct_result(left, result[1], index=index, name=name,\\n                          dtype=dtype),\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _construct_divmod_result(left, result, index, name, dtype=None):\\n    return (\\n        _construct_result(left, result[0], index=index, name=name,\\n                          dtype=dtype),\\n        _construct_result(left, result[1], index=index, name=name,\\n                          dtype=dtype),\\n    )"
  },
  {
    "code": "def do_deactivate_user(\\n\\tuser_profile: UserProfile, _cascade: bool = True, *, acting_user: Optional[UserProfile]\\n) -> None:\\n\\tif not user_profile.is_active:\\n\\t\\treturn\\n\\tif _cascade:\\n\\t\\tbot_profiles = get_active_bots_owned_by_user(user_profile)\\n\\t\\tfor profile in bot_profiles:\\n\\t\\t\\tdo_deactivate_user(profile, _cascade=False, acting_user=acting_user)\\n\\twith transaction.atomic():\\n\\t\\tif user_profile.realm.is_zephyr_mirror_realm:  \\n\\t\\t\\tuser_profile.is_mirror_dummy = True\\n\\t\\t\\tuser_profile.save(update_fields=[\"is_mirror_dummy\"])\\n\\t\\tchange_user_is_active(user_profile, False)\\n\\t\\tclear_scheduled_emails(user_profile.id)\\n\\t\\tevent_time = timezone_now()\\n\\t\\tRealmAuditLog.objects.create(\\n\\t\\t\\trealm=user_profile.realm,\\n\\t\\t\\tmodified_user=user_profile,\\n\\t\\t\\tacting_user=acting_user,\\n\\t\\t\\tevent_type=RealmAuditLog.USER_DEACTIVATED,\\n\\t\\t\\tevent_time=event_time,\\n\\t\\t\\textra_data=orjson.dumps(\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tRealmAuditLog.ROLE_COUNT: realm_user_count_by_role(user_profile.realm),\\n\\t\\t\\t\\t}\\n\\t\\t\\t).decode(),\\n\\t\\t)\\n\\t\\tdo_increment_logging_stat(\\n\\t\\t\\tuser_profile.realm,\\n\\t\\t\\tCOUNT_STATS[\"active_users_log:is_bot:day\"],\\n\\t\\t\\tuser_profile.is_bot,\\n\\t\\t\\tevent_time,\\n\\t\\t\\tincrement=-1,\\n\\t\\t)\\n\\t\\tif settings.BILLING_ENABLED:\\n\\t\\t\\tupdate_license_ledger_if_needed(user_profile.realm, event_time)\\n\\tdelete_user_sessions(user_profile)\\n\\tevent = dict(\\n\\t\\ttype=\"realm_user\",\\n\\t\\top=\"remove\",\\n\\t\\tperson=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t)\\n\\tsend_event(user_profile.realm, event, active_user_ids(user_profile.realm_id))\\n\\tif user_profile.is_bot:\\n\\t\\tevent = dict(\\n\\t\\t\\ttype=\"realm_bot\",\\n\\t\\t\\top=\"remove\",\\n\\t\\t\\tbot=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t\\t)\\n\\t\\tsend_event(user_profile.realm, event, bot_owner_user_ids(user_profile))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_deactivate_user(\\n\\tuser_profile: UserProfile, _cascade: bool = True, *, acting_user: Optional[UserProfile]\\n) -> None:\\n\\tif not user_profile.is_active:\\n\\t\\treturn\\n\\tif _cascade:\\n\\t\\tbot_profiles = get_active_bots_owned_by_user(user_profile)\\n\\t\\tfor profile in bot_profiles:\\n\\t\\t\\tdo_deactivate_user(profile, _cascade=False, acting_user=acting_user)\\n\\twith transaction.atomic():\\n\\t\\tif user_profile.realm.is_zephyr_mirror_realm:  \\n\\t\\t\\tuser_profile.is_mirror_dummy = True\\n\\t\\t\\tuser_profile.save(update_fields=[\"is_mirror_dummy\"])\\n\\t\\tchange_user_is_active(user_profile, False)\\n\\t\\tclear_scheduled_emails(user_profile.id)\\n\\t\\tevent_time = timezone_now()\\n\\t\\tRealmAuditLog.objects.create(\\n\\t\\t\\trealm=user_profile.realm,\\n\\t\\t\\tmodified_user=user_profile,\\n\\t\\t\\tacting_user=acting_user,\\n\\t\\t\\tevent_type=RealmAuditLog.USER_DEACTIVATED,\\n\\t\\t\\tevent_time=event_time,\\n\\t\\t\\textra_data=orjson.dumps(\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tRealmAuditLog.ROLE_COUNT: realm_user_count_by_role(user_profile.realm),\\n\\t\\t\\t\\t}\\n\\t\\t\\t).decode(),\\n\\t\\t)\\n\\t\\tdo_increment_logging_stat(\\n\\t\\t\\tuser_profile.realm,\\n\\t\\t\\tCOUNT_STATS[\"active_users_log:is_bot:day\"],\\n\\t\\t\\tuser_profile.is_bot,\\n\\t\\t\\tevent_time,\\n\\t\\t\\tincrement=-1,\\n\\t\\t)\\n\\t\\tif settings.BILLING_ENABLED:\\n\\t\\t\\tupdate_license_ledger_if_needed(user_profile.realm, event_time)\\n\\tdelete_user_sessions(user_profile)\\n\\tevent = dict(\\n\\t\\ttype=\"realm_user\",\\n\\t\\top=\"remove\",\\n\\t\\tperson=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t)\\n\\tsend_event(user_profile.realm, event, active_user_ids(user_profile.realm_id))\\n\\tif user_profile.is_bot:\\n\\t\\tevent = dict(\\n\\t\\t\\ttype=\"realm_bot\",\\n\\t\\t\\top=\"remove\",\\n\\t\\t\\tbot=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t\\t)\\n\\t\\tsend_event(user_profile.realm, event, bot_owner_user_ids(user_profile))"
  },
  {
    "code": "def _field_assign(frozen, name, value, self_name):\\n    if frozen:\\n        return f'__builtins__.object.__setattr__({self_name},{name!r},{value})'\\n    return f'{self_name}.{name}={value}'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34776: Fix dataclasses to support __future__ \"annotations\" mode (#9518)",
    "fixed_code": "def _field_assign(frozen, name, value, self_name):\\n    if frozen:\\n        return f'BUILTINS.object.__setattr__({self_name},{name!r},{value})'\\n    return f'{self_name}.{name}={value}'"
  },
  {
    "code": "def _ensure_datetimelike_to_i8(other, to_utc=False):\\n    from pandas import Index\\n    from pandas.core.arrays import PeriodArray\\n    if lib.is_scalar(other) and isna(other):\\n        return iNaT\\n    elif isinstance(other, (PeriodArray, ABCIndexClass,\\n                            DatetimeLikeArrayMixin)):\\n        if getattr(other, 'tz', None) is not None:\\n            if to_utc:\\n                other = other.tz_convert('UTC')\\n            else:\\n                other = other.tz_localize(None)\\n    else:\\n        try:\\n            return np.array(other, copy=False).view('i8')\\n        except TypeError:\\n            other = Index(other)\\n    return other.asi8",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ensure_datetimelike_to_i8(other, to_utc=False):\\n    from pandas import Index\\n    from pandas.core.arrays import PeriodArray\\n    if lib.is_scalar(other) and isna(other):\\n        return iNaT\\n    elif isinstance(other, (PeriodArray, ABCIndexClass,\\n                            DatetimeLikeArrayMixin)):\\n        if getattr(other, 'tz', None) is not None:\\n            if to_utc:\\n                other = other.tz_convert('UTC')\\n            else:\\n                other = other.tz_localize(None)\\n    else:\\n        try:\\n            return np.array(other, copy=False).view('i8')\\n        except TypeError:\\n            other = Index(other)\\n    return other.asi8"
  },
  {
    "code": "def get(self, key, default=None):\\n\\t\\ttry:\\n\\t\\t\\treturn self[key]\\n\\t\\texcept (KeyError, ValueError):\\n\\t\\t\\treturn default",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: allow get default value upon IndexError, GH #7725",
    "fixed_code": "def get(self, key, default=None):\\n\\t\\ttry:\\n\\t\\t\\treturn self[key]\\n\\t\\texcept (KeyError, ValueError, IndexError):\\n\\t\\t\\treturn default"
  },
  {
    "code": "def OnDoubleClick(self):\\n        \"Open a module in an editor window when double clicked.\"\\n        if not is_browseable_extension(self.file):\\n            return\\n        if not os.path.exists(self.file):\\n            return\\n        file_open(self.file)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def OnDoubleClick(self):\\n        \"Open a module in an editor window when double clicked.\"\\n        if not is_browseable_extension(self.file):\\n            return\\n        if not os.path.exists(self.file):\\n            return\\n        file_open(self.file)"
  },
  {
    "code": "def _parse_entry(field_value, field_type):\\n    if field_value is None or field_value == 'null':\\n        return None\\n    if field_type == 'INTEGER':\\n        return int(field_value)\\n    elif field_type == 'FLOAT':\\n        return float(field_value)\\n    elif field_type == 'TIMESTAMP':\\n        timestamp = datetime.utcfromtimestamp(float(field_value))\\n        return np.datetime64(timestamp)\\n    elif field_type == 'BOOLEAN':\\n        return field_value == 'true'\\n    return field_value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_entry(field_value, field_type):\\n    if field_value is None or field_value == 'null':\\n        return None\\n    if field_type == 'INTEGER':\\n        return int(field_value)\\n    elif field_type == 'FLOAT':\\n        return float(field_value)\\n    elif field_type == 'TIMESTAMP':\\n        timestamp = datetime.utcfromtimestamp(float(field_value))\\n        return np.datetime64(timestamp)\\n    elif field_type == 'BOOLEAN':\\n        return field_value == 'true'\\n    return field_value"
  },
  {
    "code": "def main():\\n\\tsys.stdout = sys.stderr\\t\\t\\n\\ttry:\\n\\t\\targs = getoptions()\\n\\texcept string.atoi_error, value:\\n\\t\\tusage(string.atoi_error, value)\\n\\texcept getopt.error, msg:\\n\\t\\tusage(getopt.error, msg)\\n\\tif args:\\n\\t\\trealtime = 0\\n\\t\\thours = string.atoi(args[0])\\n\\t\\tminutes = seconds = 0\\n\\t\\tif args[1:]: minutes = string.atoi(args[1])\\n\\t\\tif args[2:]: seconds = string.atoi(args[2])\\n\\t\\tlocaltime = ((hours*60)+minutes)*60+seconds\\n\\telse:\\n\\t\\trealtime = 1\\n\\tif Gl.title == '':\\n\\t\\tif realtime:\\n\\t\\t\\tGl.title = TITLE\\n\\t\\telse:\\n\\t\\t\\ttitle = ''\\n\\t\\t\\tfor arg in args: title = title + ' ' + arg\\n\\t\\t\\tGl.title = title[1:]\\n\\t\\t\\tdel title\\n\\twid = makewindow()\\n\\tGl.ox,Gl.oy = getorigin()\\n\\tGl.cx,Gl.cy = getsize()\\n\\tinitmenu()\\n\\tclearall()\\n\\tif not Gl.update:\\n\\t\\tGl.update = 60\\n\\tif Gl.update <= 1:\\n\\t\\tGl.timernoise = 6\\n\\telse:\\n\\t\\tGl.timernoise = 60\\n\\tnoise(TIMER0, Gl.timernoise)\\n\\tqdevice(WINSHUT)\\n\\tqdevice(WINQUIT)\\n\\tqdevice(ESCKEY)\\n\\tif realtime:\\n\\t\\tqdevice(TIMER0)\\n\\tqdevice(REDRAW)\\n\\tqdevice(WINFREEZE)\\n\\tqdevice(WINTHAW)\\n\\tqdevice(MENUBUTTON)\\t\\n\\tqdevice(MOUSE3)\\t\\t\\n\\tqdevice(MOUSE2)\\t\\t\\n\\tunqdevice(INPUTCHANGE)\\n\\tlasttime = 0\\n\\tGl.change = 1\\n\\twhile 1:\\n\\t\\tif realtime:\\n\\t\\t\\tlocaltime = time.time() - Gl.tzdiff\\n\\t\\tif Gl.alarm_set:\\n\\t\\t\\tif localtime%(24*HOUR) == Gl.alarm_time:\\n\\t\\t\\t\\tif Gl.debug:\\n\\t\\t\\t\\t\\tprint 'Rrrringg!'\\n\\t\\t\\t\\tGl.alarm_on = 1\\n\\t\\t\\t\\tif Gl.alarm_cmd <> '':\\n\\t\\t\\t\\t\\td = os.system(Gl.alarm_cmd+' '+`Gl.alarm_time/3600`+' '+`(Gl.alarm_time/60)%60` + ' &')\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\t\\t\\tclearall()\\n\\t\\tif Gl.alarm_on:\\n\\t\\t\\tif (localtime - Gl.alarm_time) % (24*HOUR) > 300:\\n\\t\\t\\t\\tGl.alarm_on = 0\\n\\t\\t\\t\\tif Gl.debug:\\n\\t\\t\\t\\t\\tprint 'Alarm turned off'\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\t\\t\\tclearall()\\n\\t\\t\\t\\tGl.indices = R, G, B\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif localtime % 2 == 0:\\n\\t\\t\\t\\t  Gl.indices = Gl.indices[2:] + Gl.indices[:2]\\n\\t\\t\\t\\t  Gl.change = 1\\n\\t\\tif Gl.gong_cmd <> '' and localtime%Gl.gong_int == 0:\\n\\t\\t\\td = os.system(Gl.gong_cmd+' '+`(localtime/3600)%24`+' '+`(localtime/60)%60` + ' &')\\n\\t\\tif localtime/Gl.update <> lasttime/Gl.update:\\n\\t\\t\\tif Gl.debug: print 'new time'\\n\\t\\t\\tGl.change = 1\\n\\t\\tif Gl.change:\\n\\t\\t\\tif Gl.debug: print 'drawing'\\n\\t\\t\\tdoit(localtime)\\n\\t\\t\\tlasttime = localtime\\n\\t\\t\\tGl.change = 0\\n\\t\\tdev, data = qread()\\n\\t\\tif Gl.debug and dev <> TIMER0:\\n\\t\\t\\tprint dev, data\\n\\t\\tif dev == TIMER0:\\n\\t\\t\\tif Gl.debug > 1:\\n\\t\\t\\t\\tprint dev, data\\n\\t\\telif dev == MOUSE3:\\n\\t\\t\\tmousex = getvaluator(MOUSEX)\\n\\t\\t\\tmousey = getvaluator(MOUSEY)\\n\\t\\t\\tif mouseclick(3, data, mousex, mousey):\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\telif dev == MOUSE2:\\n\\t\\t\\tmousex = getvaluator(MOUSEX)\\n\\t\\t\\tmousey = getvaluator(MOUSEY)\\n\\t\\t\\tif mouseclick(2, data, mousex, mousey):\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\telif dev == MOUSEX:\\n\\t\\t\\tmousex = data\\n\\t\\t\\tif Gl.mouse2down:\\n\\t\\t\\t\\tmouse2track(mousex, mousey)\\n\\t\\t\\tif Gl.mouse3down:\\n\\t\\t\\t\\tmouse3track(mousex, mousey)\\n\\t\\telif dev == MOUSEY:\\n\\t\\t\\tmousey = data\\n\\t\\t\\tif Gl.mouse2down:\\n\\t\\t\\t\\tmouse2track(mousex, mousey)\\n\\t\\t\\tif Gl.mouse3down:\\n\\t\\t\\t\\tmouse3track(mousex, mousey)\\n\\t\\telif dev == REDRAW or dev == REDRAWICONIC:\\n\\t\\t\\tif Gl.debug:\\n\\t\\t\\t\\tif dev == REDRAW: print 'REDRAW'\\n\\t\\t\\t\\telse: print 'REDRAWICONIC'\\n\\t\\t\\treshapeviewport()\\n\\t\\t\\tGl.ox,Gl.oy = getorigin()\\n\\t\\t\\tGl.cx,Gl.cy = getsize()\\n\\t\\t\\tGl.change = 1\\n\\t\\t\\tclearall()\\n\\t\\telif dev == MENUBUTTON:\\n\\t\\t\\tif Gl.debug: print 'MENUBUTTON'\\n\\t\\t\\thandlemenu()\\n\\t\\telif dev == WINFREEZE:\\n\\t\\t\\tif Gl.debug: print 'WINFREEZE'\\n\\t\\t\\tGl.iconic = 1\\n\\t\\t\\tnoise(TIMER0, 60*60) \\n\\t\\telif dev == WINTHAW:\\n\\t\\t\\tif Gl.debug: print 'WINTHAW'\\n\\t\\t\\tGl.iconic = 0\\n\\t\\t\\tnoise(TIMER0, Gl.timernoise)\\n\\t\\t\\tGl.change = 1\\n\\t\\telif dev == ESCKEY or dev == WINSHUT or dev == WINQUIT:\\n\\t\\t\\tif Gl.debug: print 'Exit'\\n\\t\\t\\tsys.exit(0)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "*** empty log message ***",
    "fixed_code": "def main():\\n\\tsys.stdout = sys.stderr\\t\\t\\n\\ttry:\\n\\t\\targs = getoptions()\\n\\texcept string.atoi_error, value:\\n\\t\\tusage(string.atoi_error, value)\\n\\texcept getopt.error, msg:\\n\\t\\tusage(getopt.error, msg)\\n\\tif args:\\n\\t\\trealtime = 0\\n\\t\\thours = string.atoi(args[0])\\n\\t\\tminutes = seconds = 0\\n\\t\\tif args[1:]: minutes = string.atoi(args[1])\\n\\t\\tif args[2:]: seconds = string.atoi(args[2])\\n\\t\\tlocaltime = ((hours*60)+minutes)*60+seconds\\n\\telse:\\n\\t\\trealtime = 1\\n\\tif Gl.title == '':\\n\\t\\tif realtime:\\n\\t\\t\\tGl.title = TITLE\\n\\t\\telse:\\n\\t\\t\\ttitle = ''\\n\\t\\t\\tfor arg in args: title = title + ' ' + arg\\n\\t\\t\\tGl.title = title[1:]\\n\\t\\t\\tdel title\\n\\twid = makewindow()\\n\\tGl.ox,Gl.oy = getorigin()\\n\\tGl.cx,Gl.cy = getsize()\\n\\tinitmenu()\\n\\tclearall()\\n\\tif not Gl.update:\\n\\t\\tGl.update = 60\\n\\tif Gl.update <= 1:\\n\\t\\tGl.timernoise = 6\\n\\telse:\\n\\t\\tGl.timernoise = 60\\n\\tnoise(TIMER0, Gl.timernoise)\\n\\tqdevice(WINSHUT)\\n\\tqdevice(WINQUIT)\\n\\tqdevice(ESCKEY)\\n\\tif realtime:\\n\\t\\tqdevice(TIMER0)\\n\\tqdevice(REDRAW)\\n\\tqdevice(WINFREEZE)\\n\\tqdevice(WINTHAW)\\n\\tqdevice(MENUBUTTON)\\t\\n\\tqdevice(MOUSE3)\\t\\t\\n\\tqdevice(MOUSE2)\\t\\t\\n\\tunqdevice(INPUTCHANGE)\\n\\tlasttime = 0\\n\\tGl.change = 1\\n\\twhile 1:\\n\\t\\tif realtime:\\n\\t\\t\\tlocaltime = int(time.time() - Gl.tzdiff)\\n\\t\\tif Gl.alarm_set:\\n\\t\\t\\tif localtime%(24*HOUR) == Gl.alarm_time:\\n\\t\\t\\t\\tif Gl.debug:\\n\\t\\t\\t\\t\\tprint 'Rrrringg!'\\n\\t\\t\\t\\tGl.alarm_on = 1\\n\\t\\t\\t\\tif Gl.alarm_cmd <> '':\\n\\t\\t\\t\\t\\td = os.system(Gl.alarm_cmd+' '+`Gl.alarm_time/3600`+' '+`(Gl.alarm_time/60)%60` + ' &')\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\t\\t\\tclearall()\\n\\t\\tif Gl.alarm_on:\\n\\t\\t\\tif (localtime - Gl.alarm_time) % (24*HOUR) > 300:\\n\\t\\t\\t\\tGl.alarm_on = 0\\n\\t\\t\\t\\tif Gl.debug:\\n\\t\\t\\t\\t\\tprint 'Alarm turned off'\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\t\\t\\tclearall()\\n\\t\\t\\t\\tGl.indices = R, G, B\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif localtime % 2 == 0:\\n\\t\\t\\t\\t  Gl.indices = Gl.indices[2:] + Gl.indices[:2]\\n\\t\\t\\t\\t  Gl.change = 1\\n\\t\\tif Gl.gong_cmd <> '' and localtime%Gl.gong_int == 0:\\n\\t\\t\\td = os.system(Gl.gong_cmd+' '+`(localtime/3600)%24`+' '+`(localtime/60)%60` + ' &')\\n\\t\\tif localtime/Gl.update <> lasttime/Gl.update:\\n\\t\\t\\tif Gl.debug: print 'new time'\\n\\t\\t\\tGl.change = 1\\n\\t\\tif Gl.change:\\n\\t\\t\\tif Gl.debug: print 'drawing'\\n\\t\\t\\tdoit(localtime)\\n\\t\\t\\tlasttime = localtime\\n\\t\\t\\tGl.change = 0\\n\\t\\tdev, data = qread()\\n\\t\\tif Gl.debug and dev <> TIMER0:\\n\\t\\t\\tprint dev, data\\n\\t\\tif dev == TIMER0:\\n\\t\\t\\tif Gl.debug > 1:\\n\\t\\t\\t\\tprint dev, data\\n\\t\\telif dev == MOUSE3:\\n\\t\\t\\tmousex = getvaluator(MOUSEX)\\n\\t\\t\\tmousey = getvaluator(MOUSEY)\\n\\t\\t\\tif mouseclick(3, data, mousex, mousey):\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\telif dev == MOUSE2:\\n\\t\\t\\tmousex = getvaluator(MOUSEX)\\n\\t\\t\\tmousey = getvaluator(MOUSEY)\\n\\t\\t\\tif mouseclick(2, data, mousex, mousey):\\n\\t\\t\\t\\tGl.change = 1\\n\\t\\telif dev == MOUSEX:\\n\\t\\t\\tmousex = data\\n\\t\\t\\tif Gl.mouse2down:\\n\\t\\t\\t\\tmouse2track(mousex, mousey)\\n\\t\\t\\tif Gl.mouse3down:\\n\\t\\t\\t\\tmouse3track(mousex, mousey)\\n\\t\\telif dev == MOUSEY:\\n\\t\\t\\tmousey = data\\n\\t\\t\\tif Gl.mouse2down:\\n\\t\\t\\t\\tmouse2track(mousex, mousey)\\n\\t\\t\\tif Gl.mouse3down:\\n\\t\\t\\t\\tmouse3track(mousex, mousey)\\n\\t\\telif dev == REDRAW or dev == REDRAWICONIC:\\n\\t\\t\\tif Gl.debug:\\n\\t\\t\\t\\tif dev == REDRAW: print 'REDRAW'\\n\\t\\t\\t\\telse: print 'REDRAWICONIC'\\n\\t\\t\\treshapeviewport()\\n\\t\\t\\tGl.ox,Gl.oy = getorigin()\\n\\t\\t\\tGl.cx,Gl.cy = getsize()\\n\\t\\t\\tGl.change = 1\\n\\t\\t\\tclearall()\\n\\t\\telif dev == MENUBUTTON:\\n\\t\\t\\tif Gl.debug: print 'MENUBUTTON'\\n\\t\\t\\thandlemenu()\\n\\t\\telif dev == WINFREEZE:\\n\\t\\t\\tif Gl.debug: print 'WINFREEZE'\\n\\t\\t\\tGl.iconic = 1\\n\\t\\t\\tnoise(TIMER0, 60*60) \\n\\t\\telif dev == WINTHAW:\\n\\t\\t\\tif Gl.debug: print 'WINTHAW'\\n\\t\\t\\tGl.iconic = 0\\n\\t\\t\\tnoise(TIMER0, Gl.timernoise)\\n\\t\\t\\tGl.change = 1\\n\\t\\telif dev == ESCKEY or dev == WINSHUT or dev == WINQUIT:\\n\\t\\t\\tif Gl.debug: print 'Exit'\\n\\t\\t\\tsys.exit(0)"
  },
  {
    "code": "def median(self):\\n        arr = self.values\\n        arr = arr[notnull(arr)]\\n        return tseries.median(arr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def median(self):\\n        arr = self.values\\n        arr = arr[notnull(arr)]\\n        return tseries.median(arr)"
  },
  {
    "code": "def create(self, size):\\n\\t\\tself._run_command([\"create\"], None, str(size))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create(self, size):\\n\\t\\tself._run_command([\"create\"], None, str(size))"
  },
  {
    "code": "def format_file_in_place(\\n\\tsrc: Path,\\n\\tline_length: int,\\n\\tfast: bool,\\n\\twrite_back: WriteBack = WriteBack.NO,\\n\\tlock: Any = None,  \\n) -> bool:\\n\\twith tokenize.open(src) as src_buffer:\\n\\t\\tsrc_contents = src_buffer.read()\\n\\ttry:\\n\\t\\tdst_contents = format_file_contents(\\n\\t\\t\\tsrc_contents, line_length=line_length, fast=fast\\n\\t\\t)\\n\\texcept NothingChanged:\\n\\t\\treturn False\\n\\tif write_back == write_back.YES:\\n\\t\\twith open(src, \"w\", encoding=src_buffer.encoding) as f:\\n\\t\\t\\tf.write(dst_contents)\\n\\telif write_back == write_back.DIFF:\\n\\t\\tsrc_name = f\"{src.name}  (original)\"\\n\\t\\tdst_name = f\"{src.name}  (formatted)\"\\n\\t\\tdiff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n\\t\\tif lock:\\n\\t\\t\\tlock.acquire()\\n\\t\\ttry:\\n\\t\\t\\tsys.stdout.write(diff_contents)\\n\\t\\tfinally:\\n\\t\\t\\tif lock:\\n\\t\\t\\t\\tlock.release()\\n\\treturn True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Show full path on diffs\\n\\nFixes #130",
    "fixed_code": "def format_file_in_place(\\n\\tsrc: Path,\\n\\tline_length: int,\\n\\tfast: bool,\\n\\twrite_back: WriteBack = WriteBack.NO,\\n\\tlock: Any = None,  \\n) -> bool:\\n\\twith tokenize.open(src) as src_buffer:\\n\\t\\tsrc_contents = src_buffer.read()\\n\\ttry:\\n\\t\\tdst_contents = format_file_contents(\\n\\t\\t\\tsrc_contents, line_length=line_length, fast=fast\\n\\t\\t)\\n\\texcept NothingChanged:\\n\\t\\treturn False\\n\\tif write_back == write_back.YES:\\n\\t\\twith open(src, \"w\", encoding=src_buffer.encoding) as f:\\n\\t\\t\\tf.write(dst_contents)\\n\\telif write_back == write_back.DIFF:\\n\\t\\tsrc_name = f\"{src}  (original)\"\\n\\t\\tdst_name = f\"{src}  (formatted)\"\\n\\t\\tdiff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n\\t\\tif lock:\\n\\t\\t\\tlock.acquire()\\n\\t\\ttry:\\n\\t\\t\\tsys.stdout.write(diff_contents)\\n\\t\\tfinally:\\n\\t\\t\\tif lock:\\n\\t\\t\\t\\tlock.release()\\n\\treturn True"
  },
  {
    "code": "def get_full_path(self):\\n        return '%s%s' % (self.path, ('?' + iri_to_uri(self.META.get('QUERY_STRING', ''))) if self.META.get('QUERY_STRING', '') else '')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_full_path(self):\\n        return '%s%s' % (self.path, ('?' + iri_to_uri(self.META.get('QUERY_STRING', ''))) if self.META.get('QUERY_STRING', '') else '')"
  },
  {
    "code": "def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\\n    container: Optional[LN] = container_of(leaf)\\n    while container is not None and container.type != token.ENDMARKER:\\n        is_fmt_on = False\\n        for comment in list_comments(container.prefix, is_endmarker=False):\\n            if comment.value in FMT_ON:\\n                is_fmt_on = True\\n            elif comment.value in FMT_OFF:\\n                is_fmt_on = False\\n        if is_fmt_on:\\n            return\\n        yield container\\n        container = container.next_sibling",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix for \"# fmt: on\" with decorators (#1325)",
    "fixed_code": "def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\\n    container: Optional[LN] = container_of(leaf)\\n    while container is not None and container.type != token.ENDMARKER:\\n        if fmt_on(container):\\n            return\\n        if contains_fmt_on_at_column(container, leaf.column):\\n            for child in container.children:\\n                if contains_fmt_on_at_column(child, leaf.column):\\n                    return\\n                yield child\\n        else:\\n            yield container\\n            container = container.next_sibling"
  },
  {
    "code": "def read_text(self, *args, **kwargs):\\n        with self.open('r', *args, **kwargs) as strm:\\n            return strm.read()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-43651: Fix EncodingWarning in zipfile (GH-25650)",
    "fixed_code": "def read_text(self, *args, **kwargs):\\n        kwargs[\"encoding\"] = io.text_encoding(kwargs.get(\"encoding\"))\\n        with self.open('r', *args, **kwargs) as strm:\\n            return strm.read()"
  },
  {
    "code": "def build_tokenizer(self):\\n        if self.tokenizer is not None:\\n            return self.tokenizer\\n        token_pattern = re.compile(self.token_pattern)\\n        return token_pattern.findall",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX raise an error in CountVectorizer with a custom token pattern that captures several group (#15427)",
    "fixed_code": "def build_tokenizer(self):\\n        if self.tokenizer is not None:\\n            return self.tokenizer\\n        token_pattern = re.compile(self.token_pattern)\\n        if token_pattern.groups > 1:\\n            raise ValueError(\\n                \"More than 1 capturing group in token pattern. Only a single \"\\n                \"group should be captured.\"\\n            )\\n        return token_pattern.findall"
  },
  {
    "code": "def logistic_regression_path(X, y, Cs=10, fit_intercept=True,\\n                             max_iter=100, gtol=1e-4, verbose=0,\\n                             solver='liblinear', callback=None,\\n                             coef=None):\\n    \"\"\"\\n    Compute a Logistic Regression model for a list of regularization\\n    parameters.\\n    This is an implementation that uses the result of the previous model\\n    to speed up computations along the set of solutions, making it faster\\n    than sequentially calling LogisticRegression for the different parameters.\\n    Parameters\\n    ----------\\n    X : array-like or sparse matrix, shape (n_samples, n_features)\\n        Input data\\n    y : array-like, shape (n_samples,)\\n        Input data, target values\\n    Cs : array-like or integer of shape (n_cs,)\\n        List of values for the regularization parameter or integer specifying\\n        the number of regularization parameters that should be used. In this\\n        case, the parameters will be chosen in a logarithmic scale between\\n        1e-4 and 1e4.\\n    fit_intercept : boolean\\n        Whether to fit an intercept for the model. In this case the shape of\\n        the returned array is (n_cs, n_features + 1).\\n    max_iter : integer\\n        Maximum number of iterations for the solver.\\n    gtol : float\\n        Stopping criterion. The iteration will stop when\\n        ``max{|g_i | i = 1, ..., n} <= gtol``\\n        where ``g_i`` is the i-th component of the gradient. Only used\\n        by the methods 'lbfgs' and 'trust-ncg'\\n    verbose: int\\n        Print convergence message if True.\\n    solver : {'lbfgs', 'newton-cg', 'liblinear'}\\n        Numerical solver to use.\\n    callback : callable\\n        Function to be called before and after the fit of each regularization\\n        parameter. Must have the signature callback(w, X, y, alpha).\\n    coef: array-lime, shape (n_features,)\\n        Initialization value for coefficients of logistic regression.\\n    Returns\\n    -------\\n    coefs: array of shape (n_cs, n_features) or (n_cs, n_features + 1)\\n        List of coefficients for the Logistic Regression model. If\\n        fit_intercept is set to True then the seconds dimension will be\\n        n_features + 1, where the last item represents the intercept.",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def logistic_regression_path(X, y, Cs=10, fit_intercept=True,\\n                             max_iter=100, gtol=1e-4, verbose=0,\\n                             solver='liblinear', callback=None,\\n                             coef=None):\\n    \"\"\"\\n    Compute a Logistic Regression model for a list of regularization\\n    parameters.\\n    This is an implementation that uses the result of the previous model\\n    to speed up computations along the set of solutions, making it faster\\n    than sequentially calling LogisticRegression for the different parameters.\\n    Parameters\\n    ----------\\n    X : array-like or sparse matrix, shape (n_samples, n_features)\\n        Input data\\n    y : array-like, shape (n_samples,)\\n        Input data, target values\\n    Cs : array-like or integer of shape (n_cs,)\\n        List of values for the regularization parameter or integer specifying\\n        the number of regularization parameters that should be used. In this\\n        case, the parameters will be chosen in a logarithmic scale between\\n        1e-4 and 1e4.\\n    fit_intercept : boolean\\n        Whether to fit an intercept for the model. In this case the shape of\\n        the returned array is (n_cs, n_features + 1).\\n    max_iter : integer\\n        Maximum number of iterations for the solver.\\n    gtol : float\\n        Stopping criterion. The iteration will stop when\\n        ``max{|g_i | i = 1, ..., n} <= gtol``\\n        where ``g_i`` is the i-th component of the gradient. Only used\\n        by the methods 'lbfgs' and 'trust-ncg'\\n    verbose: int\\n        Print convergence message if True.\\n    solver : {'lbfgs', 'newton-cg', 'liblinear'}\\n        Numerical solver to use.\\n    callback : callable\\n        Function to be called before and after the fit of each regularization\\n        parameter. Must have the signature callback(w, X, y, alpha).\\n    coef: array-lime, shape (n_features,)\\n        Initialization value for coefficients of logistic regression.\\n    Returns\\n    -------\\n    coefs: array of shape (n_cs, n_features) or (n_cs, n_features + 1)\\n        List of coefficients for the Logistic Regression model. If\\n        fit_intercept is set to True then the seconds dimension will be\\n        n_features + 1, where the last item represents the intercept."
  },
  {
    "code": "def _make_na_block(self, items, ref_items, placement=None, fill_value=None):\\n        if fill_value is None:\\n            fill_value = np.nan\\n        block_shape = list(self.shape)\\n        block_shape[0] = len(items)\\n        dtype, fill_value = com._infer_dtype_from_scalar(fill_value)\\n        block_values = np.empty(block_shape, dtype=dtype)\\n        block_values.fill(fill_value)\\n        return make_block(block_values, items, ref_items, placement=placement)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _make_na_block(self, items, ref_items, placement=None, fill_value=None):\\n        if fill_value is None:\\n            fill_value = np.nan\\n        block_shape = list(self.shape)\\n        block_shape[0] = len(items)\\n        dtype, fill_value = com._infer_dtype_from_scalar(fill_value)\\n        block_values = np.empty(block_shape, dtype=dtype)\\n        block_values.fill(fill_value)\\n        return make_block(block_values, items, ref_items, placement=placement)"
  },
  {
    "code": "def _build_tree(is_classification, X, y, criterion,\\n               max_depth, min_split, max_features, n_classes, random_state):\\n    n_samples, n_features = X.shape\\n    if len(y) != len(X):\\n        raise ValueError(\"Number of labels=%d does not match \"\\n                          \"number of features=%d\"\\n                         % (len(y), len(X)))\\n    y = np.array(y, dtype=np.float64, order=\"c\")\\n    feature_mask = np.ones((n_features,), dtype=np.bool, order=\"c\")\\n    if max_features is not None:\\n        if max_features <= 0 or max_features > n_features:\\n            raise ValueError(\"max_features=%d must be in range (0..%d]. \"\\n                             \"Did you mean to use None to signal no \"\\n                             \"max_features?\"\\n                             % (max_features, n_features))\\n        permutation = random_state.permutation(n_features)\\n        sample_dims = np.sort(permutation[-max_features:])\\n        feature_mask[sample_dims] = False\\n        feature_mask = np.logical_not(feature_mask)\\n    feature_mask = feature_mask.astype(np.int32)\\n    if not X.flags[\"F_CONTIGUOUS\"]:\\n        X = np.array(X, order=\"F\")\\n    if min_split <= 0:\\n        raise ValueError(\"min_split must be greater than zero. \"\\n                         \"min_split is %s.\" % min_split)\\n    if max_depth <= 0:\\n        raise ValueError(\"max_depth must be greater than zero. \"\\n                         \"max_depth is %s.\" % max_depth)\\n    Node.class_counter = 0",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_tree(is_classification, X, y, criterion,\\n               max_depth, min_split, max_features, n_classes, random_state):\\n    n_samples, n_features = X.shape\\n    if len(y) != len(X):\\n        raise ValueError(\"Number of labels=%d does not match \"\\n                          \"number of features=%d\"\\n                         % (len(y), len(X)))\\n    y = np.array(y, dtype=np.float64, order=\"c\")\\n    feature_mask = np.ones((n_features,), dtype=np.bool, order=\"c\")\\n    if max_features is not None:\\n        if max_features <= 0 or max_features > n_features:\\n            raise ValueError(\"max_features=%d must be in range (0..%d]. \"\\n                             \"Did you mean to use None to signal no \"\\n                             \"max_features?\"\\n                             % (max_features, n_features))\\n        permutation = random_state.permutation(n_features)\\n        sample_dims = np.sort(permutation[-max_features:])\\n        feature_mask[sample_dims] = False\\n        feature_mask = np.logical_not(feature_mask)\\n    feature_mask = feature_mask.astype(np.int32)\\n    if not X.flags[\"F_CONTIGUOUS\"]:\\n        X = np.array(X, order=\"F\")\\n    if min_split <= 0:\\n        raise ValueError(\"min_split must be greater than zero. \"\\n                         \"min_split is %s.\" % min_split)\\n    if max_depth <= 0:\\n        raise ValueError(\"max_depth must be greater than zero. \"\\n                         \"max_depth is %s.\" % max_depth)\\n    Node.class_counter = 0"
  },
  {
    "code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            return self._get_models_cache[cache_key]\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        return model_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            return self._get_models_cache[cache_key]\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        return model_list"
  },
  {
    "code": "def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\\n        if not lib.is_integer(periods):\\n            if not (\\n                is_float(periods)\\n                and periods.is_integer()  \\n            ):\\n                raise ValueError(\"periods must be an integer\")\\n            periods = int(periods)\\n        axis = self._get_axis_number(axis)\\n        if axis == 1 and periods != 0:\\n            return self - self.shift(periods, axis=axis)\\n        new_data = self._mgr.diff(n=periods, axis=axis)\\n        return self._constructor(new_data).__finalize__(self, \"diff\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\\n        if not lib.is_integer(periods):\\n            if not (\\n                is_float(periods)\\n                and periods.is_integer()  \\n            ):\\n                raise ValueError(\"periods must be an integer\")\\n            periods = int(periods)\\n        axis = self._get_axis_number(axis)\\n        if axis == 1 and periods != 0:\\n            return self - self.shift(periods, axis=axis)\\n        new_data = self._mgr.diff(n=periods, axis=axis)\\n        return self._constructor(new_data).__finalize__(self, \"diff\")"
  },
  {
    "code": "def _check_disjoint_resolver_names(resolver_keys, local_keys, global_keys):\\n    res_locals = list(com.intersection(resolver_keys, local_keys))\\n    if res_locals:\\n        msg = \"resolvers and locals overlap on names {0}\".format(res_locals)\\n        raise NameResolutionError(msg)\\n    res_globals = list(com.intersection(resolver_keys, global_keys))\\n    if res_globals:\\n        msg = \"resolvers and globals overlap on names {0}\".format(res_globals)\\n        raise NameResolutionError(msg)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_disjoint_resolver_names(resolver_keys, local_keys, global_keys):\\n    res_locals = list(com.intersection(resolver_keys, local_keys))\\n    if res_locals:\\n        msg = \"resolvers and locals overlap on names {0}\".format(res_locals)\\n        raise NameResolutionError(msg)\\n    res_globals = list(com.intersection(resolver_keys, global_keys))\\n    if res_globals:\\n        msg = \"resolvers and globals overlap on names {0}\".format(res_globals)\\n        raise NameResolutionError(msg)"
  },
  {
    "code": "def execute(self, context: 'Context'):\\n        self.hook = PostgresHook(postgres_conn_id=self.postgres_conn_id, schema=self.database)\\n        self.hook.run(self.sql, self.autocommit, parameters=self.parameters)\\n        for output in self.hook.conn.notices:\\n            self.log.info(output)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "adds ability to pass config params to postgres operator (#21551)",
    "fixed_code": "def execute(self, context: 'Context'):\\n        self.hook = PostgresHook(postgres_conn_id=self.postgres_conn_id, schema=self.database)\\n        if self.runtime_parameters:\\n            final_sql = []\\n            sql_param = {}\\n            for param in self.runtime_parameters:\\n                set_param_sql = f\"SET {{}} TO %({param})s;\"\\n                dynamic_sql = SQL(set_param_sql).format(Identifier(f\"{param}\"))\\n                final_sql.append(dynamic_sql)\\n            for param, val in self.runtime_parameters.items():\\n                sql_param.update({f\"{param}\": f\"{val}\"})\\n            if self.parameters:\\n                sql_param.update(self.parameters)\\n            if isinstance(self.sql, str):\\n                final_sql.append(SQL(self.sql))\\n            else:\\n                final_sql.extend(list(map(SQL, self.sql)))\\n            self.hook.run(final_sql, self.autocommit, parameters=sql_param)\\n        else:\\n            self.hook.run(self.sql, self.autocommit, parameters=self.parameters)\\n        for output in self.hook.conn.notices:\\n            self.log.info(output)"
  },
  {
    "code": "def __init__(self, *args):\\n        Exception.__init__(self, *args)\\n        try:\\n            self.response = args[0]\\n        except IndexError:\\n            self.response = 'No response given'\\nclass NNTPReplyError(NNTPError):\\n    pass\\nclass NNTPTemporaryError(NNTPError):\\n    pass\\nclass NNTPPermanentError(NNTPError):\\n    pass\\nclass NNTPProtocolError(NNTPError):\\n    pass\\nclass NNTPDataError(NNTPError):\\n    pass\\nerror_reply = NNTPReplyError\\nerror_temp = NNTPTemporaryError\\nerror_perm = NNTPPermanentError\\nerror_proto = NNTPProtocolError\\nerror_data = NNTPDataError",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def __init__(self, *args):\\n        Exception.__init__(self, *args)\\n        try:\\n            self.response = args[0]\\n        except IndexError:\\n            self.response = 'No response given'\\nclass NNTPReplyError(NNTPError):\\n    pass\\nclass NNTPTemporaryError(NNTPError):\\n    pass\\nclass NNTPPermanentError(NNTPError):\\n    pass\\nclass NNTPProtocolError(NNTPError):\\n    pass\\nclass NNTPDataError(NNTPError):\\n    pass"
  },
  {
    "code": "def _get_match(self, check_values, record, tolerance=None) -> bool:\\n        if record is None and self.accept_none:\\n            record = 0\\n        match_boolean = True\\n        if \"geq_to\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record >= check_values[\"geq_to\"] * (1 - tolerance)\\n            else:\\n                match_boolean = record >= check_values[\"geq_to\"]\\n        elif \"greater_than\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record > check_values[\"greater_than\"] * (1 - tolerance)\\n            else:\\n                match_boolean = record > check_values[\"greater_than\"]\\n        if \"leq_to\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record <= check_values[\"leq_to\"] * (1 + tolerance) and match_boolean\\n            else:\\n                match_boolean = record <= check_values[\"leq_to\"] and match_boolean\\n        elif \"less_than\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record < check_values[\"less_than\"] * (1 + tolerance) and match_boolean\\n            else:\\n                match_boolean = record < check_values[\"less_than\"] and match_boolean\\n        if \"equal_to\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = (\\n                    check_values[\"equal_to\"] * (1 - tolerance)\\n                    <= record\\n                    <= check_values[\"equal_to\"] * (1 + tolerance)\\n                ) and match_boolean\\n            else:\\n                match_boolean = record == check_values[\"equal_to\"] and match_boolean\\n        return match_boolean",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_match(self, check_values, record, tolerance=None) -> bool:\\n        if record is None and self.accept_none:\\n            record = 0\\n        match_boolean = True\\n        if \"geq_to\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record >= check_values[\"geq_to\"] * (1 - tolerance)\\n            else:\\n                match_boolean = record >= check_values[\"geq_to\"]\\n        elif \"greater_than\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record > check_values[\"greater_than\"] * (1 - tolerance)\\n            else:\\n                match_boolean = record > check_values[\"greater_than\"]\\n        if \"leq_to\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record <= check_values[\"leq_to\"] * (1 + tolerance) and match_boolean\\n            else:\\n                match_boolean = record <= check_values[\"leq_to\"] and match_boolean\\n        elif \"less_than\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = record < check_values[\"less_than\"] * (1 + tolerance) and match_boolean\\n            else:\\n                match_boolean = record < check_values[\"less_than\"] and match_boolean\\n        if \"equal_to\" in check_values:\\n            if tolerance is not None:\\n                match_boolean = (\\n                    check_values[\"equal_to\"] * (1 - tolerance)\\n                    <= record\\n                    <= check_values[\"equal_to\"] * (1 + tolerance)\\n                ) and match_boolean\\n            else:\\n                match_boolean = record == check_values[\"equal_to\"] and match_boolean\\n        return match_boolean"
  },
  {
    "code": "def read_gbq(\\n    query: str,\\n    project_id: Optional[str] = None,\\n    index_col: Optional[str] = None,\\n    col_order: Optional[List[str]] = None,\\n    reauth: bool = False,\\n    auth_local_webserver: bool = False,\\n    dialect: Optional[str] = None,\\n    location: Optional[str] = None,\\n    configuration: Optional[Dict[str, Any]] = None,\\n    credentials=None,\\n    use_bqstorage_api: Optional[bool] = None,\\n    private_key=None,\\n    verbose=None,\\n    progress_bar_type: Optional[str] = None,\\n) -> \"DataFrame\":\\n    pandas_gbq = _try_import()\\n    kwargs: Dict[str, Union[str, bool]] = {}\\n    if use_bqstorage_api is not None:\\n        kwargs[\"use_bqstorage_api\"] = use_bqstorage_api\\n    if progress_bar_type is not None:\\n        kwargs[\"progress_bar_type\"] = progress_bar_type\\n    return pandas_gbq.read_gbq(\\n        query,\\n        project_id=project_id,\\n        index_col=index_col,\\n        col_order=col_order,\\n        reauth=reauth,\\n        auth_local_webserver=auth_local_webserver,\\n        dialect=dialect,\\n        location=location,\\n        configuration=configuration,\\n        credentials=credentials,\\n        **kwargs,\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Send None parameter to pandas-gbq to set no progress bar (#33477)",
    "fixed_code": "def read_gbq(\\n    query: str,\\n    project_id: Optional[str] = None,\\n    index_col: Optional[str] = None,\\n    col_order: Optional[List[str]] = None,\\n    reauth: bool = False,\\n    auth_local_webserver: bool = False,\\n    dialect: Optional[str] = None,\\n    location: Optional[str] = None,\\n    configuration: Optional[Dict[str, Any]] = None,\\n    credentials=None,\\n    use_bqstorage_api: Optional[bool] = None,\\n    private_key=None,\\n    verbose=None,\\n    progress_bar_type: Optional[str] = None,\\n) -> \"DataFrame\":\\n    pandas_gbq = _try_import()\\n    kwargs: Dict[str, Union[str, bool, None]] = {}\\n    if use_bqstorage_api is not None:\\n        kwargs[\"use_bqstorage_api\"] = use_bqstorage_api\\n    kwargs[\"progress_bar_type\"] = progress_bar_type\\n    return pandas_gbq.read_gbq(\\n        query,\\n        project_id=project_id,\\n        index_col=index_col,\\n        col_order=col_order,\\n        reauth=reauth,\\n        auth_local_webserver=auth_local_webserver,\\n        dialect=dialect,\\n        location=location,\\n        configuration=configuration,\\n        credentials=credentials,\\n        **kwargs,\\n    )"
  },
  {
    "code": "def select_as_multiple(self, keys, where=None, selector=None, columns=None, **kwargs):\\n        if isinstance(keys, (list, tuple)) and len(keys) == 1:\\n            keys = keys[0]\\n        if isinstance(keys, basestring):\\n            return self.select(key=keys, where=where, columns=columns, **kwargs)\\n        if not isinstance(keys, (list, tuple)):\\n            raise Exception(\"keys must be a list/tuple\")\\n        if len(keys) == 0:\\n            raise Exception(\"keys must have a non-zero length\")\\n        if selector is None:\\n            selector = keys[0]\\n        tbls = [ self.get_storer(k) for k in keys ]\\n        nrows = tbls[0].nrows\\n        for t in tbls:\\n            if t.nrows != nrows:\\n                raise Exception(\"all tables must have exactly the same nrows!\")\\n            if not t.is_table:\\n                raise Exception(\"object [%s] is not a table, and cannot be used in all select as multiple\" % t.pathname)\\n        c = self.select_as_coordinates(selector, where)\\n        objs = [t.read(where=c, columns=columns) for t in tbls]\\n        axis = list(set([t.non_index_axes[0][0] for t in tbls]))[0]\\n        return concat(objs, axis=axis, verify_integrity=True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: New keywords ``iterator=boolean``, and ``chunksize=number_in_a_chunk`` are      provided to support iteration on ``select`` and ``select_as_multiple`` (GH3076_)",
    "fixed_code": "def select_as_multiple(self, keys, where=None, selector=None, columns=None, start=None, stop=None, iterator=False, chunksize=None, **kwargs):\\n        if isinstance(keys, (list, tuple)) and len(keys) == 1:\\n            keys = keys[0]\\n        if isinstance(keys, basestring):\\n            return self.select(key=keys, where=where, columns=columns, start=start, stop=stop, iterator=iterator, chunksize=chunksize, **kwargs)\\n        if not isinstance(keys, (list, tuple)):\\n            raise Exception(\"keys must be a list/tuple\")\\n        if len(keys) == 0:\\n            raise Exception(\"keys must have a non-zero length\")\\n        if selector is None:\\n            selector = keys[0]\\n        tbls = [ self.get_storer(k) for k in keys ]\\n        if tbls[0] is None:\\n            raise Exception(\"no valid tables to select as multiple\")\\n        nrows = tbls[0].nrows\\n        for t in tbls:\\n            if t.nrows != nrows:\\n                raise Exception(\"all tables must have exactly the same nrows!\")\\n            if not t.is_table:\\n                raise Exception(\"object [%s] is not a table, and cannot be used in all select as multiple\" % t.pathname)\\n        c = self.select_as_coordinates(selector, where, start=start, stop=stop)\\n        nrows = len(c)"
  },
  {
    "code": "def copy_position_info(obj):\\n        ''\\n        assert isinstance(obj, AnsibleBaseYAMLObject)\\n        (src, line, col) = obj.get_position_info()\\n        self._data_source   = src\\n        self._line_number   = line\\n        self._column_number = col\\nclass AnsibleMapping(AnsibleBaseYAMLObject, dict):\\n    ''\\n    pass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Adding role deps to v2 Role class and fixing some bugs",
    "fixed_code": "def copy_position_info(self, obj):\\n        ''\\n        assert isinstance(obj, AnsibleBaseYAMLObject)\\n        (src, line, col) = obj.get_position_info()\\n        self._data_source   = src\\n        self._line_number   = line\\n        self._column_number = col\\nclass AnsibleMapping(AnsibleBaseYAMLObject, dict):\\n    ''\\n    pass"
  },
  {
    "code": "def __init__(self, settings_module):\\n        for setting in dir(global_settings):\\n            if setting.isupper():\\n                setattr(self, setting, getattr(global_settings, setting))\\n        self.SETTINGS_MODULE = settings_module\\n        mod = importlib.import_module(self.SETTINGS_MODULE)\\n        tuple_settings = (\\n            \"INSTALLED_APPS\",\\n            \"TEMPLATE_DIRS\",\\n            \"LOCALE_PATHS\",\\n        )\\n        self._explicit_settings = set()\\n        for setting in dir(mod):\\n            if setting.isupper():\\n                setting_value = getattr(mod, setting)\\n                if (setting in tuple_settings and\\n                        not isinstance(setting_value, (list, tuple))):\\n                    raise ImproperlyConfigured(\"The %s setting must be a list or a tuple. \" % setting)\\n                setattr(self, setting, setting_value)\\n                self._explicit_settings.add(setting)\\n        if not self.SECRET_KEY:\\n            raise ImproperlyConfigured(\"The SECRET_KEY setting must not be empty.\")\\n        if self.is_overridden('DEFAULT_CONTENT_TYPE'):\\n            warnings.warn(DEFAULT_CONTENT_TYPE_DEPRECATED_MSG, RemovedInDjango30Warning)\\n        if self.is_overridden('FILE_CHARSET'):\\n            warnings.warn(FILE_CHARSET_DEPRECATED_MSG, RemovedInDjango31Warning)\\n        if hasattr(time, 'tzset') and self.TIME_ZONE:\\n            zoneinfo_root = Path('/usr/share/zoneinfo')\\n            zone_info_file = zoneinfo_root.joinpath(*self.TIME_ZONE.split('/'))\\n            if zoneinfo_root.exists() and not zone_info_file.exists():\\n                raise ValueError(\"Incorrect timezone setting: %s\" % self.TIME_ZONE)\\n            os.environ['TZ'] = self.TIME_ZONE\\n            time.tzset()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, settings_module):\\n        for setting in dir(global_settings):\\n            if setting.isupper():\\n                setattr(self, setting, getattr(global_settings, setting))\\n        self.SETTINGS_MODULE = settings_module\\n        mod = importlib.import_module(self.SETTINGS_MODULE)\\n        tuple_settings = (\\n            \"INSTALLED_APPS\",\\n            \"TEMPLATE_DIRS\",\\n            \"LOCALE_PATHS\",\\n        )\\n        self._explicit_settings = set()\\n        for setting in dir(mod):\\n            if setting.isupper():\\n                setting_value = getattr(mod, setting)\\n                if (setting in tuple_settings and\\n                        not isinstance(setting_value, (list, tuple))):\\n                    raise ImproperlyConfigured(\"The %s setting must be a list or a tuple. \" % setting)\\n                setattr(self, setting, setting_value)\\n                self._explicit_settings.add(setting)\\n        if not self.SECRET_KEY:\\n            raise ImproperlyConfigured(\"The SECRET_KEY setting must not be empty.\")\\n        if self.is_overridden('DEFAULT_CONTENT_TYPE'):\\n            warnings.warn(DEFAULT_CONTENT_TYPE_DEPRECATED_MSG, RemovedInDjango30Warning)\\n        if self.is_overridden('FILE_CHARSET'):\\n            warnings.warn(FILE_CHARSET_DEPRECATED_MSG, RemovedInDjango31Warning)\\n        if hasattr(time, 'tzset') and self.TIME_ZONE:\\n            zoneinfo_root = Path('/usr/share/zoneinfo')\\n            zone_info_file = zoneinfo_root.joinpath(*self.TIME_ZONE.split('/'))\\n            if zoneinfo_root.exists() and not zone_info_file.exists():\\n                raise ValueError(\"Incorrect timezone setting: %s\" % self.TIME_ZONE)\\n            os.environ['TZ'] = self.TIME_ZONE\\n            time.tzset()"
  },
  {
    "code": "def _post_plot_logic_common(self, ax, data):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Plotting use FixedLocator (#26185)\\n\\ncloses #7612\\ncloses #15912\\ncloses #22334",
    "fixed_code": "def _post_plot_logic_common(self, ax, data):\\n        from matplotlib.ticker import FixedLocator, FixedFormatter"
  },
  {
    "code": "def _run_packaging_install(path):\\n    dist = Distribution()\\n    dist.parse_config_files()\\n    try:\\n        dist.run_command('install_dist')\\n    except (IOError, os.error, PackagingError, CCompilerError) as msg:\\n        raise SystemExit(\"error: \" + str(msg))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "make sure we check for write access before starting the install, and add correct exit code",
    "fixed_code": "def _run_packaging_install(path):\\n    dist = Distribution()\\n    dist.parse_config_files()\\n    try:\\n        dist.run_command('install_dist')\\n        name = dist.metadata['name']\\n        return database.get_distribution(name) is not None\\n    except (IOError, os.error, PackagingError, CCompilerError) as msg:\\n        raise ValueError(\"Failed to install, \" + str(msg))"
  },
  {
    "code": "def _convert_scalar_indexer(self, key, kind=None):\\n\\t\\tif self.categories._defer_to_indexing:\\n\\t\\t\\treturn self.categories._convert_scalar_indexer(key, kind=kind)\\n\\t\\treturn super()._convert_scalar_indexer(key, kind=kind)\\n\\t@Appender(_index_shared_docs[\"_convert_list_indexer\"])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: loc-indexing with a CategoricalIndex with non-string categories (#29922)",
    "fixed_code": "def _convert_scalar_indexer(self, key, kind=None):\\n\\t\\tif kind == \"loc\":\\n\\t\\t\\ttry:\\n\\t\\t\\t\\treturn self.categories._convert_scalar_indexer(key, kind=kind)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tself._invalid_indexer(\"label\", key)\\n\\t\\treturn super()._convert_scalar_indexer(key, kind=kind)\\n\\t@Appender(_index_shared_docs[\"_convert_list_indexer\"])"
  },
  {
    "code": "def reindex_columns_from(self, new_columns):\\n        indexer, mask = self.columns.get_indexer(new_columns)\\n        masked_idx = indexer[mask]\\n        new_values = self.values.take(masked_idx, axis=1)\\n        new_locs = np.arange(len(new_columns))[mask]\\n        return make_block(new_values, new_locs, new_columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def reindex_columns_from(self, new_columns):\\n        indexer, mask = self.columns.get_indexer(new_columns)\\n        masked_idx = indexer[mask]\\n        new_values = self.values.take(masked_idx, axis=1)\\n        new_cols = self.columns.take(masked_idx)\\n        return make_block(new_values, new_cols, new_columns)"
  },
  {
    "code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               dialect=None,\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               comment=None,\\n               parse_dates=False,\\n               keep_date_col=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None,\\n               squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               dialect=None,\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               comment=None,\\n               parse_dates=False,\\n               keep_date_col=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None,\\n               squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)"
  },
  {
    "code": "def __rmul__(self, b):\\n        if not isinstance(b, Kernel):\\n            return Product(ConstantKernel(b), self)\\n        return Product(b, self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REFACTOR GP kernels use separate specification of bounds and params (no joint param_space)",
    "fixed_code": "def __rmul__(self, b):\\n        if not isinstance(b, Kernel):\\n            return Product(ConstantKernel.from_literal(b), self)\\n        return Product(b, self)"
  },
  {
    "code": "def get_validation_errors(outfile, app=None):\\n    from django.db import models, connection\\n    from django.db.models.loading import get_app_errors\\n    from django.db.models.deletion import SET_NULL, SET_DEFAULT\\n    e = ModelErrorCollection(outfile)\\n    for (app_name, error) in get_app_errors().items():\\n        e.add(app_name, error)\\n    for cls in models.get_models(app, include_swapped=True):\\n        opts = cls._meta\\n        if opts.swapped:\\n            try:\\n                app_label, model_name = opts.swapped.split('.')\\n            except ValueError:\\n                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\\n                continue\\n            if not models.get_model(app_label, model_name):\\n                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\\n            continue\\n        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\\n            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\\n                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\\n            if not opts.get_field(cls.USERNAME_FIELD).unique:\\n                e.add(opts, 'The USERNAME_FIELD must be unique. Add unique=True to the field parameters.')\\n        for f in opts.local_fields:\\n            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\\n                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\\n            if f.name.endswith('_'):\\n                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\\n            if (f.primary_key and f.null and\\n                    not connection.features.interprets_empty_strings_as_nulls):\\n                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\\n            if isinstance(f, models.CharField):\\n                try:\\n                    max_length = int(f.max_length)\\n                    if max_length <= 0:\\n                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n                except (ValueError, TypeError):\\n                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n            if isinstance(f, models.DecimalField):\\n                decimalp_ok, mdigits_ok = False, False\\n                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\\n                try:\\n                    decimal_places = int(f.decimal_places)\\n                    if decimal_places < 0:\\n                        e.add(opts, decimalp_msg % f.name)\\n                    else:\\n                        decimalp_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, decimalp_msg % f.name)\\n                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\\n                try:\\n                    max_digits = int(f.max_digits)\\n                    if max_digits <= 0:\\n                        e.add(opts,  mdigits_msg % f.name)\\n                    else:\\n                        mdigits_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, mdigits_msg % f.name)\\n                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\\n                if decimalp_ok and mdigits_ok:\\n                    if decimal_places > max_digits:\\n                        e.add(opts, invalid_values_msg % f.name)\\n            if isinstance(f, models.FileField) and not f.upload_to:\\n                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\\n            if isinstance(f, models.ImageField):\\n                try:\\n                    from django.utils.image import Image\\n                except ImportError:\\n                    e.add(opts, '\"%s\": To use ImageFields, you need to install Pillow. Get it at https://pypi.python.org/pypi/Pillow.' % f.name)\\n            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\\n                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\\n            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\\n                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\\n            if isinstance(f, models.GenericIPAddressField) and not getattr(f, 'null', False) and getattr(f, 'blank', False):\\n                e.add(opts, '\"%s\": GenericIPAddressField can not accept blank values if null values are not allowed, as blank values are stored as null.' % f.name)\\n            if f.choices:\\n                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\\n                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\\n                else:\\n                    for c in f.choices:\\n                        if isinstance(c, six.string_types) or not is_iterable(c) or len(c) != 2:\\n                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).' % f.name)\\n            if f.db_index not in (None, True, False):\\n                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\\n            connection.validation.validate_field(e, opts, f)\\n            if f.rel and hasattr(f.rel, 'on_delete'):\\n                if f.rel.on_delete == SET_NULL and not f.null:\\n                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\\n                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\\n                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\\n            if f.rel:\\n                if f.rel.to not in models.get_models():\\n                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                    else:\\n                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n                if f.requires_unique_target:\\n                    if len(f.foreign_related_fields) > 1:\\n                        has_unique_field = False\\n                        for rel_field in f.foreign_related_fields:\\n                            has_unique_field = has_unique_field or rel_field.unique\\n                        if not has_unique_field:\\n                            e.add(opts, \"Field combination '%s' under model '%s' must have a unique=True constraint\" % (','.join([rel_field.name for rel_field in f.foreign_related_fields]), f.rel.to.__name__))\\n                    else:\\n                        if not f.foreign_related_fields[0].unique:\\n                            e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.foreign_related_fields[0].name, f.rel.to.__name__))\\n                rel_opts = f.rel.to._meta\\n                rel_name = f.related.get_accessor_name()\\n                rel_query_name = f.related_query_name()\\n                if not f.rel.is_hidden():\\n                    for r in rel_opts.fields:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.local_many_to_many:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.get_all_related_many_to_many_objects():\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    for r in rel_opts.get_all_related_objects():\\n                        if r.field is not f:\\n                            if r.get_accessor_name() == rel_name:\\n                                e.add(opts, \"Accessor for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                            if r.get_accessor_name() == rel_query_name:\\n                                e.add(opts, \"Reverse query name for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        seen_intermediary_signatures = []\\n        for i, f in enumerate(opts.local_many_to_many):\\n            if f.rel.to not in models.get_models():\\n                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                else:\\n                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n            if f.unique:\\n                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\\n            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\\n                from_model, to_model = cls, f.rel.to\\n                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\\n                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\\n                seen_from, seen_to, seen_self = False, False, 0\\n                for inter_field in f.rel.through._meta.fields:\\n                    rel_to = getattr(inter_field.rel, 'to', None)\\n                    if from_model == to_model:  \\n                        if rel_to == from_model:\\n                            seen_self += 1\\n                        if seen_self > 2:\\n                            e.add(opts, \"Intermediary model %s has more than \"\\n                                \"two foreign keys to %s, which is ambiguous \"\\n                                \"and is not permitted.\" % (\\n                                    f.rel.through._meta.object_name,\\n                                    from_model._meta.object_name\\n                                )\\n                            )\\n                    else:\\n                        if rel_to == from_model:\\n                            if seen_from:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                         from_model._meta.object_name\\n                                     )\\n                                 )\\n                            else:\\n                                seen_from = True\\n                        elif rel_to == to_model:\\n                            if seen_to:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                        rel_to._meta.object_name\\n                                    )\\n                                )\\n                            else:\\n                                seen_to = True\\n                if f.rel.through not in models.get_models(include_auto_created=True):\\n                    e.add(opts, \"'%s' specifies an m2m relation through model \"\\n                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\\n                    )\\n                signature = (f.rel.to, cls, f.rel.through)\\n                if signature in seen_intermediary_signatures:\\n                    e.add(opts, \"The model %s has two manually-defined m2m \"\\n                        \"relations through the model %s, which is not \"\\n                        \"permitted. Please consider using an extra field on \"\\n                        \"your intermediary model instead.\" % (\\n                            cls._meta.object_name,\\n                            f.rel.through._meta.object_name\\n                        )\\n                    )\\n                else:\\n                    seen_intermediary_signatures.append(signature)\\n                if not f.rel.through._meta.auto_created:\\n                    seen_related_fk, seen_this_fk = False, False\\n                    for field in f.rel.through._meta.fields:\\n                        if field.rel:\\n                            if not seen_related_fk and field.rel.to == f.rel.to:\\n                                seen_related_fk = True\\n                            elif field.rel.to == cls:\\n                                seen_this_fk = True\\n                    if not seen_related_fk or not seen_this_fk:\\n                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\\n                            \"through model %s, which does not have foreign keys \"\\n                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\\n                                f.rel.to._meta.object_name, cls._meta.object_name)\\n                        )\\n            elif isinstance(f.rel.through, six.string_types):\\n                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\\n                    \"which has not been installed\" % (f.name, f.rel.through)\\n                )\\n            rel_opts = f.rel.to._meta\\n            rel_name = f.related.get_accessor_name()\\n            rel_query_name = f.related_query_name()\\n            if rel_name is not None:\\n                for r in rel_opts.fields:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.local_many_to_many:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.get_all_related_many_to_many_objects():\\n                    if r.field is not f:\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                for r in rel_opts.get_all_related_objects():\\n                    if r.get_accessor_name() == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    if r.get_accessor_name() == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        if opts.ordering:\\n            for field_name in opts.ordering:\\n                if field_name == '?':\\n                    continue\\n                if field_name.startswith('-'):\\n                    field_name = field_name[1:]\\n                if opts.order_with_respect_to and field_name == '_order':\\n                    continue\\n                if '__' in field_name:\\n                    continue\\n                if field_name == 'pk':\\n                    continue\\n                try:\\n                    opts.get_field(field_name, many_to_many=False)\\n                except models.FieldDoesNotExist:\\n                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\\n        for ut in opts.unique_together:\\n            validate_local_fields(e, opts, \"unique_together\", ut)\\n        if not isinstance(opts.index_together, collections.Sequence):\\n            e.add(opts, '\"index_together\" must a sequence')\\n        else:\\n            for it in opts.index_together:\\n                validate_local_fields(e, opts, \"index_together\", it)\\n    return len(e.errors)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19925 - Added validation for REQUIRED_FIELDS being a list\\n\\nThanks Roman Alexander for the suggestion.",
    "fixed_code": "def get_validation_errors(outfile, app=None):\\n    from django.db import models, connection\\n    from django.db.models.loading import get_app_errors\\n    from django.db.models.deletion import SET_NULL, SET_DEFAULT\\n    e = ModelErrorCollection(outfile)\\n    for (app_name, error) in get_app_errors().items():\\n        e.add(app_name, error)\\n    for cls in models.get_models(app, include_swapped=True):\\n        opts = cls._meta\\n        if opts.swapped:\\n            try:\\n                app_label, model_name = opts.swapped.split('.')\\n            except ValueError:\\n                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\\n                continue\\n            if not models.get_model(app_label, model_name):\\n                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\\n            continue\\n        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\\n            if not isinstance(cls.REQUIRED_FIELDS, (list, tuple)):\\n                e.add(opts, 'The REQUIRED_FIELDS must be a list or tuple.')\\n            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\\n                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\\n            if not opts.get_field(cls.USERNAME_FIELD).unique:\\n                e.add(opts, 'The USERNAME_FIELD must be unique. Add unique=True to the field parameters.')\\n        for f in opts.local_fields:\\n            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\\n                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\\n            if f.name.endswith('_'):\\n                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\\n            if (f.primary_key and f.null and\\n                    not connection.features.interprets_empty_strings_as_nulls):\\n                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\\n            if isinstance(f, models.CharField):\\n                try:\\n                    max_length = int(f.max_length)\\n                    if max_length <= 0:\\n                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n                except (ValueError, TypeError):\\n                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n            if isinstance(f, models.DecimalField):\\n                decimalp_ok, mdigits_ok = False, False\\n                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\\n                try:\\n                    decimal_places = int(f.decimal_places)\\n                    if decimal_places < 0:\\n                        e.add(opts, decimalp_msg % f.name)\\n                    else:\\n                        decimalp_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, decimalp_msg % f.name)\\n                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\\n                try:\\n                    max_digits = int(f.max_digits)\\n                    if max_digits <= 0:\\n                        e.add(opts,  mdigits_msg % f.name)\\n                    else:\\n                        mdigits_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, mdigits_msg % f.name)\\n                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\\n                if decimalp_ok and mdigits_ok:\\n                    if decimal_places > max_digits:\\n                        e.add(opts, invalid_values_msg % f.name)\\n            if isinstance(f, models.FileField) and not f.upload_to:\\n                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\\n            if isinstance(f, models.ImageField):\\n                try:\\n                    from django.utils.image import Image\\n                except ImportError:\\n                    e.add(opts, '\"%s\": To use ImageFields, you need to install Pillow. Get it at https://pypi.python.org/pypi/Pillow.' % f.name)\\n            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\\n                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\\n            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\\n                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\\n            if isinstance(f, models.GenericIPAddressField) and not getattr(f, 'null', False) and getattr(f, 'blank', False):\\n                e.add(opts, '\"%s\": GenericIPAddressField can not accept blank values if null values are not allowed, as blank values are stored as null.' % f.name)\\n            if f.choices:\\n                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\\n                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\\n                else:\\n                    for c in f.choices:\\n                        if isinstance(c, six.string_types) or not is_iterable(c) or len(c) != 2:\\n                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).' % f.name)\\n            if f.db_index not in (None, True, False):\\n                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\\n            connection.validation.validate_field(e, opts, f)\\n            if f.rel and hasattr(f.rel, 'on_delete'):\\n                if f.rel.on_delete == SET_NULL and not f.null:\\n                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\\n                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\\n                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\\n            if f.rel:\\n                if f.rel.to not in models.get_models():\\n                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                    else:\\n                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n                if f.requires_unique_target:\\n                    if len(f.foreign_related_fields) > 1:\\n                        has_unique_field = False\\n                        for rel_field in f.foreign_related_fields:\\n                            has_unique_field = has_unique_field or rel_field.unique\\n                        if not has_unique_field:\\n                            e.add(opts, \"Field combination '%s' under model '%s' must have a unique=True constraint\" % (','.join([rel_field.name for rel_field in f.foreign_related_fields]), f.rel.to.__name__))\\n                    else:\\n                        if not f.foreign_related_fields[0].unique:\\n                            e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.foreign_related_fields[0].name, f.rel.to.__name__))\\n                rel_opts = f.rel.to._meta\\n                rel_name = f.related.get_accessor_name()\\n                rel_query_name = f.related_query_name()\\n                if not f.rel.is_hidden():\\n                    for r in rel_opts.fields:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.local_many_to_many:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.get_all_related_many_to_many_objects():\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    for r in rel_opts.get_all_related_objects():\\n                        if r.field is not f:\\n                            if r.get_accessor_name() == rel_name:\\n                                e.add(opts, \"Accessor for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                            if r.get_accessor_name() == rel_query_name:\\n                                e.add(opts, \"Reverse query name for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        seen_intermediary_signatures = []\\n        for i, f in enumerate(opts.local_many_to_many):\\n            if f.rel.to not in models.get_models():\\n                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                else:\\n                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n            if f.unique:\\n                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\\n            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\\n                from_model, to_model = cls, f.rel.to\\n                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\\n                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\\n                seen_from, seen_to, seen_self = False, False, 0\\n                for inter_field in f.rel.through._meta.fields:\\n                    rel_to = getattr(inter_field.rel, 'to', None)\\n                    if from_model == to_model:  \\n                        if rel_to == from_model:\\n                            seen_self += 1\\n                        if seen_self > 2:\\n                            e.add(opts, \"Intermediary model %s has more than \"\\n                                \"two foreign keys to %s, which is ambiguous \"\\n                                \"and is not permitted.\" % (\\n                                    f.rel.through._meta.object_name,\\n                                    from_model._meta.object_name\\n                                )\\n                            )\\n                    else:\\n                        if rel_to == from_model:\\n                            if seen_from:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                         from_model._meta.object_name\\n                                     )\\n                                 )\\n                            else:\\n                                seen_from = True\\n                        elif rel_to == to_model:\\n                            if seen_to:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                        rel_to._meta.object_name\\n                                    )\\n                                )\\n                            else:\\n                                seen_to = True\\n                if f.rel.through not in models.get_models(include_auto_created=True):\\n                    e.add(opts, \"'%s' specifies an m2m relation through model \"\\n                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\\n                    )\\n                signature = (f.rel.to, cls, f.rel.through)\\n                if signature in seen_intermediary_signatures:\\n                    e.add(opts, \"The model %s has two manually-defined m2m \"\\n                        \"relations through the model %s, which is not \"\\n                        \"permitted. Please consider using an extra field on \"\\n                        \"your intermediary model instead.\" % (\\n                            cls._meta.object_name,\\n                            f.rel.through._meta.object_name\\n                        )\\n                    )\\n                else:\\n                    seen_intermediary_signatures.append(signature)\\n                if not f.rel.through._meta.auto_created:\\n                    seen_related_fk, seen_this_fk = False, False\\n                    for field in f.rel.through._meta.fields:\\n                        if field.rel:\\n                            if not seen_related_fk and field.rel.to == f.rel.to:\\n                                seen_related_fk = True\\n                            elif field.rel.to == cls:\\n                                seen_this_fk = True\\n                    if not seen_related_fk or not seen_this_fk:\\n                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\\n                            \"through model %s, which does not have foreign keys \"\\n                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\\n                                f.rel.to._meta.object_name, cls._meta.object_name)\\n                        )\\n            elif isinstance(f.rel.through, six.string_types):\\n                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\\n                    \"which has not been installed\" % (f.name, f.rel.through)\\n                )\\n            rel_opts = f.rel.to._meta\\n            rel_name = f.related.get_accessor_name()\\n            rel_query_name = f.related_query_name()\\n            if rel_name is not None:\\n                for r in rel_opts.fields:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.local_many_to_many:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.get_all_related_many_to_many_objects():\\n                    if r.field is not f:\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                for r in rel_opts.get_all_related_objects():\\n                    if r.get_accessor_name() == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    if r.get_accessor_name() == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        if opts.ordering:\\n            for field_name in opts.ordering:\\n                if field_name == '?':\\n                    continue\\n                if field_name.startswith('-'):\\n                    field_name = field_name[1:]\\n                if opts.order_with_respect_to and field_name == '_order':\\n                    continue\\n                if '__' in field_name:\\n                    continue\\n                if field_name == 'pk':\\n                    continue\\n                try:\\n                    opts.get_field(field_name, many_to_many=False)\\n                except models.FieldDoesNotExist:\\n                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\\n        for ut in opts.unique_together:\\n            validate_local_fields(e, opts, \"unique_together\", ut)\\n        if not isinstance(opts.index_together, collections.Sequence):\\n            e.add(opts, '\"index_together\" must a sequence')\\n        else:\\n            for it in opts.index_together:\\n                validate_local_fields(e, opts, \"index_together\", it)\\n    return len(e.errors)"
  },
  {
    "code": "def _write_frame_table(self, group, df, append=False, comp=None, axes=None, **kwargs):\\n        if axes is None:\\n            axes = [0]\\n        t = create_table(self, group, typ = 'appendable_frame')\\n        t.write(axes=axes, obj=df, append=append, compression=comp, **kwargs)\\n    _read_frame_table = _read_ndim_table",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _write_frame_table(self, group, df, append=False, comp=None, axes=None, **kwargs):\\n        if axes is None:\\n            axes = [0]\\n        t = create_table(self, group, typ = 'appendable_frame')\\n        t.write(axes=axes, obj=df, append=append, compression=comp, **kwargs)\\n    _read_frame_table = _read_ndim_table"
  },
  {
    "code": "def setup(self, f, tb):\\n        self.forget()\\n        self.stack, self.curindex = self.get_stack(f, tb)\\n        while tb:\\n            lineno = lasti2lineno(tb.tb_frame.f_code, tb.tb_lasti)\\n            self.tb_lineno[tb.tb_frame] = lineno\\n            tb = tb.tb_next\\n        self.curframe = self.stack[self.curindex][0]\\n        self.curframe_locals = self.curframe.f_locals\\n        return self.execRcLines()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setup(self, f, tb):\\n        self.forget()\\n        self.stack, self.curindex = self.get_stack(f, tb)\\n        while tb:\\n            lineno = lasti2lineno(tb.tb_frame.f_code, tb.tb_lasti)\\n            self.tb_lineno[tb.tb_frame] = lineno\\n            tb = tb.tb_next\\n        self.curframe = self.stack[self.curindex][0]\\n        self.curframe_locals = self.curframe.f_locals\\n        return self.execRcLines()"
  },
  {
    "code": "def generate_range(start=None, end=None, periods=None, offset=BDay()):\\n    from pandas.tseries.frequencies import to_offset\\n    offset = to_offset(offset)\\n    start = to_datetime(start)\\n    end = to_datetime(end)\\n    if start and not offset.onOffset(start):\\n        start = offset.rollforward(start)\\n    elif end and not offset.onOffset(end):\\n        end = offset.rollback(end)\\n    if periods is None and end < start and offset.n >= 0:\\n        end = None\\n        periods = 0\\n    if end is None:\\n        end = start + (periods - 1) * offset\\n    if start is None:\\n        start = end - (periods - 1) * offset\\n    cur = start\\n    if offset.n >= 0:\\n        while cur <= end:\\n            yield cur\\n            if cur == end:\\n                break\\n            next_date = offset.apply(cur)\\n            if next_date <= cur:\\n                raise ValueError('Offset {offset} did not increment date'\\n                                 .format(offset=offset))\\n            cur = next_date\\n    else:\\n        while cur >= end:\\n            yield cur\\n            if cur == end:\\n                break\\n            next_date = offset.apply(cur)\\n            if next_date >= cur:\\n                raise ValueError('Offset {offset} did not decrement date'\\n                                 .format(offset=offset))\\n            cur = next_date",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_range(start=None, end=None, periods=None, offset=BDay()):\\n    from pandas.tseries.frequencies import to_offset\\n    offset = to_offset(offset)\\n    start = to_datetime(start)\\n    end = to_datetime(end)\\n    if start and not offset.onOffset(start):\\n        start = offset.rollforward(start)\\n    elif end and not offset.onOffset(end):\\n        end = offset.rollback(end)\\n    if periods is None and end < start and offset.n >= 0:\\n        end = None\\n        periods = 0\\n    if end is None:\\n        end = start + (periods - 1) * offset\\n    if start is None:\\n        start = end - (periods - 1) * offset\\n    cur = start\\n    if offset.n >= 0:\\n        while cur <= end:\\n            yield cur\\n            if cur == end:\\n                break\\n            next_date = offset.apply(cur)\\n            if next_date <= cur:\\n                raise ValueError('Offset {offset} did not increment date'\\n                                 .format(offset=offset))\\n            cur = next_date\\n    else:\\n        while cur >= end:\\n            yield cur\\n            if cur == end:\\n                break\\n            next_date = offset.apply(cur)\\n            if next_date >= cur:\\n                raise ValueError('Offset {offset} did not decrement date'\\n                                 .format(offset=offset))\\n            cur = next_date"
  },
  {
    "code": "def format(self, record):\\n        super().format(record)\\n        record_dict = {label: getattr(record, label, None) for label in self.json_fields}\\n        merged_record = merge_dicts(record_dict, self.extras)\\n        return json.dumps(merged_record)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add Traceback in LogRecord in ``JSONFormatter`` (#15414)\\n\\nCurrently traceback is not included when ``JSONFormatter`` is used.\\n(`[logging] json_format = True`) . However, the default Handler\\nincludes the Stacktrace. To currently include the trace we need to\\nadd `json_fields = asctime, filename, lineno, levelname, message, exc_text`.\\n\\nThis is a bigger problem when using Elasticsearch Logging with:\\n\\n```ini\\n[elasticsearch]\\nwrite_stdout = True\\njson_format = True\\njson_fields = asctime, filename, lineno, levelname, message, exc_text\\n\\n[logging]\\nlog_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s - %(exc_text)s\\n```\\n\\nRunning the following DAG with the above config won't show trace:\\n\\n```python\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\nfrom airflow.utils.dates import days_ago\\n\\nwith DAG(\\n    dag_id='example_error',\\n    schedule_interval=None,\\n    start_date=days_ago(2),\\n) as dag:\\n\\n    def raise_error(**kwargs):\\n        raise Exception(\"I am an exception from task logs\")\\n\\n    task_1 = PythonOperator(\\n        task_id='task_1',\\n        python_callable=raise_error,\\n    )\\n```\\n\\nBefore:\\n\\n```\\n[2021-04-17 00:11:00,152] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: example_python_operator.print_the_context 2021-04-17T00:10:57.110189+00:00 [queued]>\\n...\\n...\\n[2021-04-17 00:11:00,298] {taskinstance.py:1482} ERROR - Task failed with exception\\n[2021-04-17 00:11:00,300] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=example_python_operator, task_id=print_the_context, execution_date=20210417T001057, start_date=20210417T001100, end_date=20210417T001100\\n[2021-04-17 00:11:00,325] {local_task_job.py:146} INFO - Task exited with return code 1\\n```\\n\\nAfter:\\n\\n```\\n[2021-04-17 00:11:00,152] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: example_python_operator.print_the_context 2021-04-17T00:10:57.110189+00:00 [queued]>\\n...\\n...\\n[2021-04-17 00:11:00,298] {taskinstance.py:1482} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 1138, in _run_raw_task\\n    self._prepare_and_execute_task_with_callbacks(context, task)\\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 1311, in _prepare_and_execute_task_with_callbacks\\n    result = self._execute_task(context, task_copy)\\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 1341, in _execute_task\\n    result = task_copy.execute(context=context)\\n  File \"/usr/local/lib/python3.7/site-packages/airflow/operators/python.py\", line 117, in execute\\n    return_value = self.execute_callable()\\n  File \"/usr/local/lib/python3.7/site-packages/airflow/operators/python.py\", line 128, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"/usr/local/airflow/dags/eg-2.py\", line 25, in print_context\\n    raise Exception(\"I am an exception from task logs\")\\nException: I am an exception from task logs\\n[2021-04-17 00:11:00,300] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=example_python_operator, task_id=print_the_context, execution_date=20210417T001057, start_date=20210417T001100, end_date=20210417T001100\\n[2021-04-17 00:11:00,325] {local_task_job.py:146} INFO - Task exited with return code 1\\n```",
    "fixed_code": "def format(self, record):\\n        super().format(record)\\n        record_dict = {label: getattr(record, label, None) for label in self.json_fields}\\n        if \"message\" in self.json_fields:\\n            msg = record_dict[\"message\"]\\n            if record.exc_text:\\n                if msg[-1:] != \"\\n\":\\n                    msg = msg + \"\\n\"\\n                msg = msg + record.exc_text\\n            if record.stack_info:\\n                if msg[-1:] != \"\\n\":\\n                    msg = msg + \"\\n\"\\n                msg = msg + self.formatStack(record.stack_info)\\n            record_dict[\"message\"] = msg\\n        merged_record = merge_dicts(record_dict, self.extras)\\n        return json.dumps(merged_record)"
  },
  {
    "code": "def _combine_const(self, other, func, errors='raise', try_cast=True):\\n        assert lib.is_scalar(other) or np.ndim(other) == 0\\n        return ops.dispatch_to_series(self, other, func)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert \"Use align_method in comp_method_FRAME (#22880)\" (#23120)",
    "fixed_code": "def _combine_const(self, other, func, errors='raise', try_cast=True):\\n        if lib.is_scalar(other) or np.ndim(other) == 0:\\n            return ops.dispatch_to_series(self, other, func)\\n        new_data = self._data.eval(func=func, other=other,\\n                                   errors=errors,\\n                                   try_cast=try_cast)\\n        return self._constructor(new_data)"
  },
  {
    "code": "def _wait_for_job_done(self, project_id, job_id, interval=30):\\n        assert interval > 0\\n        while True:\\n            job = self._get_job(project_id, job_id)\\n            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\\n                return job\\n            time.sleep(interval)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _wait_for_job_done(self, project_id, job_id, interval=30):\\n        assert interval > 0\\n        while True:\\n            job = self._get_job(project_id, job_id)\\n            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\\n                return job\\n            time.sleep(interval)"
  },
  {
    "code": "def create_version(self, project_name, model_name, version_spec):\\n        parent_name = 'projects/{}/models/{}'.format(project_name, model_name)\\n        create_request = self._cloudml.projects().models().versions().create(\\n            parent=parent_name, body=version_spec)\\n        response = create_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_version(self, project_name, model_name, version_spec):\\n        parent_name = 'projects/{}/models/{}'.format(project_name, model_name)\\n        create_request = self._cloudml.projects().models().versions().create(\\n            parent=parent_name, body=version_spec)\\n        response = create_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)"
  },
  {
    "code": "def ispythonsource(self, filename):\\n        return 0",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-30617: IDLE: docstrings and unittest for outwin.py (#2046)\\n\\nMove some data and functions from the class to module level. Patch by Cheryl Sabella.",
    "fixed_code": "def ispythonsource(self, filename):\\n        \"Python source is only part of output: do not colorize.\"\\n        return False"
  },
  {
    "code": "def reshape(self, *args, **kwargs):\\n\\t\\tif len(args) == 1 and hasattr(args[0], '__iter__'):\\n\\t\\t\\tshape = args[0]\\n\\t\\telse:\\n\\t\\t\\tshape = args\\n\\t\\tif tuple(shape) == self.shape:\\n\\t\\t\\treturn self\\n\\t\\treturn self.values.reshape(shape, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reshape(self, *args, **kwargs):\\n\\t\\tif len(args) == 1 and hasattr(args[0], '__iter__'):\\n\\t\\t\\tshape = args[0]\\n\\t\\telse:\\n\\t\\t\\tshape = args\\n\\t\\tif tuple(shape) == self.shape:\\n\\t\\t\\treturn self\\n\\t\\treturn self.values.reshape(shape, **kwargs)"
  },
  {
    "code": "def read_array(fp, allow_pickle=True, pickle_kwargs=None):\\n\\tversion = read_magic(fp)\\n\\t_check_version(version)\\n\\tshape, fortran_order, dtype = _read_array_header(fp, version)\\n\\tif len(shape) == 0:\\n\\t\\tcount = 1\\n\\telse:\\n\\t\\tcount = numpy.multiply.reduce(shape)\\n\\tif dtype.hasobject:\\n\\t\\tif not allow_pickle:\\n\\t\\t\\traise ValueError(\"Object arrays cannot be loaded when \"\\n\\t\\t\\t\\t\\t\\t\\t \"allow_pickle=False\")\\n\\t\\tif pickle_kwargs is None:\\n\\t\\t\\tpickle_kwargs = {}\\n\\t\\ttry:\\n\\t\\t\\tarray = pickle.load(fp, **pickle_kwargs)\\n\\t\\texcept UnicodeError as err:\\n\\t\\t\\tif sys.version_info[0] >= 3:\\n\\t\\t\\t\\traise UnicodeError(\"Unpickling a python object failed: %r\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"You may need to pass the encoding= option \"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"to numpy.load\" % (err,))\\n\\t\\t\\traise\\n\\telse:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray = numpy.fromfile(fp, dtype=dtype, count=count)\\n\\t\\telse:\\n\\t\\t\\tmax_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dtype.itemsize)\\n\\t\\t\\tarray = numpy.empty(count, dtype=dtype)\\n\\t\\t\\tfor i in range(0, count, max_read_count):\\n\\t\\t\\t\\tread_count = min(max_read_count, count - i)\\n\\t\\t\\t\\tread_size = int(read_count * dtype.itemsize)\\n\\t\\t\\t\\tdata = _read_bytes(fp, read_size, \"array data\")\\n\\t\\t\\t\\tarray[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t count=read_count)\\n\\t\\tif fortran_order:\\n\\t\\t\\tarray.shape = shape[::-1]\\n\\t\\t\\tarray = array.transpose()\\n\\t\\telse:\\n\\t\\t\\tarray.shape = shape\\n\\treturn array",
    "label": 1,
    "bug_type": "security",
    "bug_description": "BUG: Cast size to int64 when loading from archive\\n\\nPrevents overflow errors for large arrays on systems\\nwhere the default int type is int32.",
    "fixed_code": "def read_array(fp, allow_pickle=True, pickle_kwargs=None):\\n\\tversion = read_magic(fp)\\n\\t_check_version(version)\\n\\tshape, fortran_order, dtype = _read_array_header(fp, version)\\n\\tif len(shape) == 0:\\n\\t\\tcount = 1\\n\\telse:\\n\\t\\tcount = numpy.multiply.reduce(shape, dtype=numpy.int64)\\n\\tif dtype.hasobject:\\n\\t\\tif not allow_pickle:\\n\\t\\t\\traise ValueError(\"Object arrays cannot be loaded when \"\\n\\t\\t\\t\\t\\t\\t\\t \"allow_pickle=False\")\\n\\t\\tif pickle_kwargs is None:\\n\\t\\t\\tpickle_kwargs = {}\\n\\t\\ttry:\\n\\t\\t\\tarray = pickle.load(fp, **pickle_kwargs)\\n\\t\\texcept UnicodeError as err:\\n\\t\\t\\tif sys.version_info[0] >= 3:\\n\\t\\t\\t\\traise UnicodeError(\"Unpickling a python object failed: %r\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"You may need to pass the encoding= option \"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"to numpy.load\" % (err,))\\n\\t\\t\\traise\\n\\telse:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray = numpy.fromfile(fp, dtype=dtype, count=count)\\n\\t\\telse:\\n\\t\\t\\tmax_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dtype.itemsize)\\n\\t\\t\\tarray = numpy.empty(count, dtype=dtype)\\n\\t\\t\\tfor i in range(0, count, max_read_count):\\n\\t\\t\\t\\tread_count = min(max_read_count, count - i)\\n\\t\\t\\t\\tread_size = int(read_count * dtype.itemsize)\\n\\t\\t\\t\\tdata = _read_bytes(fp, read_size, \"array data\")\\n\\t\\t\\t\\tarray[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t count=read_count)\\n\\t\\tif fortran_order:\\n\\t\\t\\tarray.shape = shape[::-1]\\n\\t\\t\\tarray = array.transpose()\\n\\t\\telse:\\n\\t\\t\\tarray.shape = shape\\n\\treturn array"
  },
  {
    "code": "def resolve_lookup_value(self, value, can_reuse, allow_joins):\\n\\t\\tif hasattr(value, 'resolve_expression'):\\n\\t\\t\\tvalue = value.resolve_expression(\\n\\t\\t\\t\\tself, reuse=can_reuse, allow_joins=allow_joins,\\n\\t\\t\\t)\\n\\t\\telif isinstance(value, (list, tuple)):\\n\\t\\t\\tvalues = (\\n\\t\\t\\t\\tself.resolve_lookup_value(sub_value, can_reuse, allow_joins)\\n\\t\\t\\t\\tfor sub_value in value\\n\\t\\t\\t)\\n\\t\\t\\ttype_ = type(value)\\n\\t\\t\\tif hasattr(type_, '_make'):  \\n\\t\\t\\t\\treturn type_(*values)\\n\\t\\t\\treturn type_(values)\\n\\t\\treturn value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def resolve_lookup_value(self, value, can_reuse, allow_joins):\\n\\t\\tif hasattr(value, 'resolve_expression'):\\n\\t\\t\\tvalue = value.resolve_expression(\\n\\t\\t\\t\\tself, reuse=can_reuse, allow_joins=allow_joins,\\n\\t\\t\\t)\\n\\t\\telif isinstance(value, (list, tuple)):\\n\\t\\t\\tvalues = (\\n\\t\\t\\t\\tself.resolve_lookup_value(sub_value, can_reuse, allow_joins)\\n\\t\\t\\t\\tfor sub_value in value\\n\\t\\t\\t)\\n\\t\\t\\ttype_ = type(value)\\n\\t\\t\\tif hasattr(type_, '_make'):  \\n\\t\\t\\t\\treturn type_(*values)\\n\\t\\t\\treturn type_(values)\\n\\t\\treturn value"
  },
  {
    "code": "def convert_value(self, value, expression, connection, context):\\n        if value is None:\\n            return value\\n        return float(value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24649 -- Allowed using Avg aggregate on non-numeric field types.",
    "fixed_code": "def convert_value(self, value, expression, connection, context):\\n        if value is None:\\n            return 0\\n        return int(value)"
  },
  {
    "code": "def get_features_used(  \\n    node: Node, *, future_imports: Optional[Set[str]] = None\\n) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    if future_imports:\\n        features |= {\\n            FUTURE_FLAG_TO_FEATURE[future_import]\\n            for future_import in future_imports\\n            if future_import in FUTURE_FLAG_TO_FEATURE\\n        }\\n    for n in node.pre_order():\\n        if is_string_token(n):\\n            value_head = n.value[:2]\\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            assert isinstance(n, Leaf)\\n            if \"_\" in n.value:\\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n        elif (\\n            n.type in {syms.return_stmt, syms.yield_expr}\\n            and len(n.children) >= 2\\n            and n.children[1].type == syms.testlist_star_expr\\n            and any(child.type == syms.star_expr for child in n.children[1].children)\\n        ):\\n            features.add(Feature.UNPACKING_ON_FLOW)\\n        elif (\\n            n.type == syms.annassign\\n            and len(n.children) >= 4\\n            and n.children[3].type == syms.testlist_star_expr\\n        ):\\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\\n        elif (\\n            n.type == syms.except_clause\\n            and len(n.children) >= 2\\n            and n.children[1].type == token.STAR\\n        ):\\n            features.add(Feature.EXCEPT_STAR)\\n    return features",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_features_used(  \\n    node: Node, *, future_imports: Optional[Set[str]] = None\\n) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    if future_imports:\\n        features |= {\\n            FUTURE_FLAG_TO_FEATURE[future_import]\\n            for future_import in future_imports\\n            if future_import in FUTURE_FLAG_TO_FEATURE\\n        }\\n    for n in node.pre_order():\\n        if is_string_token(n):\\n            value_head = n.value[:2]\\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            assert isinstance(n, Leaf)\\n            if \"_\" in n.value:\\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n        elif (\\n            n.type in {syms.return_stmt, syms.yield_expr}\\n            and len(n.children) >= 2\\n            and n.children[1].type == syms.testlist_star_expr\\n            and any(child.type == syms.star_expr for child in n.children[1].children)\\n        ):\\n            features.add(Feature.UNPACKING_ON_FLOW)\\n        elif (\\n            n.type == syms.annassign\\n            and len(n.children) >= 4\\n            and n.children[3].type == syms.testlist_star_expr\\n        ):\\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\\n        elif (\\n            n.type == syms.except_clause\\n            and len(n.children) >= 2\\n            and n.children[1].type == token.STAR\\n        ):\\n            features.add(Feature.EXCEPT_STAR)\\n    return features"
  },
  {
    "code": "def maybe_cast_result(\\n    result: ArrayLike,\\n    dtype: DtypeObj,\\n    numeric_only: bool = False,\\n    how: str = \"\",\\n    same_dtype: bool = True,\\n) -> ArrayLike:\\n    dtype = maybe_cast_result_dtype(dtype, how)\\n    assert not is_scalar(result)\\n    if isinstance(dtype, ExtensionDtype):\\n        if not is_categorical_dtype(dtype) and dtype.kind != \"M\":\\n            cls = dtype.construct_array_type()\\n            if same_dtype:\\n                result = maybe_cast_to_extension_array(cls, result, dtype=dtype)\\n            else:\\n                result = maybe_cast_to_extension_array(cls, result)\\n    elif (numeric_only and is_numeric_dtype(dtype)) or not numeric_only:\\n        result = maybe_downcast_to_dtype(result, dtype)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_cast_result(\\n    result: ArrayLike,\\n    dtype: DtypeObj,\\n    numeric_only: bool = False,\\n    how: str = \"\",\\n    same_dtype: bool = True,\\n) -> ArrayLike:\\n    dtype = maybe_cast_result_dtype(dtype, how)\\n    assert not is_scalar(result)\\n    if isinstance(dtype, ExtensionDtype):\\n        if not is_categorical_dtype(dtype) and dtype.kind != \"M\":\\n            cls = dtype.construct_array_type()\\n            if same_dtype:\\n                result = maybe_cast_to_extension_array(cls, result, dtype=dtype)\\n            else:\\n                result = maybe_cast_to_extension_array(cls, result)\\n    elif (numeric_only and is_numeric_dtype(dtype)) or not numeric_only:\\n        result = maybe_downcast_to_dtype(result, dtype)\\n    return result"
  },
  {
    "code": "def _ensure_arraylike(values):\\n    if not isinstance(values, (np.ndarray, ABCCategorical,\\n                               ABCIndexClass, ABCSeries)):\\n        inferred = lib.infer_dtype(values)\\n        if inferred in ['mixed', 'string', 'unicode']:\\n            if isinstance(values, tuple):\\n                values = list(values)\\n            values = lib.list_to_object_array(values)\\n        else:\\n            values = np.asarray(values)\\n    return values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ensure_arraylike(values):\\n    if not isinstance(values, (np.ndarray, ABCCategorical,\\n                               ABCIndexClass, ABCSeries)):\\n        inferred = lib.infer_dtype(values)\\n        if inferred in ['mixed', 'string', 'unicode']:\\n            if isinstance(values, tuple):\\n                values = list(values)\\n            values = lib.list_to_object_array(values)\\n        else:\\n            values = np.asarray(values)\\n    return values"
  },
  {
    "code": "def deb_version(self):\\n        v = self._parsed_version\\n        match = self._parsed_regex_match\\n        if v.is_prerelease or match.group('dev'):\\n            if match.group('pre'):\\n                tag_value = match.group('pre')\\n                tag_type = match.group('pre_l')\\n                if match.group('dev'):\\n                    tag_value += ('~%s' % match.group('dev').strip('.'))\\n            elif match.group('dev'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('dev').strip('.')\\n            else:\\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\\n        elif v.is_postrelease:\\n            raise Exception(\"post-release identifiers are not supported\")\\n        else:\\n            tag_type = None\\n            tag_value = ''\\n        if not tag_type:\\n            return '{base_version}'.format(base_version=self.base_version)\\n        return '{base_version}~{tag_value}'.format(base_version=self.base_version, tag_value=tag_value)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "add .post support to rpm/deb version munger (#42444)",
    "fixed_code": "def deb_version(self):\\n        v = self._parsed_version\\n        match = self._parsed_regex_match\\n        if v.is_prerelease or match.group('dev') or match.group('post'):\\n            if match.group('dev') and match.group('post'):\\n                raise Exception(\"dev and post may not currently be used together\")\\n            if match.group('pre'):\\n                tag_value = match.group('pre')\\n                tag_type = match.group('pre_l')\\n                if match.group('dev'):\\n                    tag_value += ('~%s' % match.group('dev').strip('.'))\\n                if match.group('post'):\\n                    tag_value += ('~%s' % match.group('post').strip('.'))\\n            elif match.group('dev'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('dev').strip('.')\\n            elif match.group('post'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('post').strip('.')\\n            else:\\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\\n        else:\\n            tag_type = None\\n            tag_value = ''\\n        if not tag_type:\\n            return '{base_version}'.format(base_version=self.base_version)\\n        return '{base_version}~{tag_value}'.format(base_version=self.base_version, tag_value=tag_value)\\n    @property"
  },
  {
    "code": "def load_data(dtype=np.float32, order='F'):\\n    if not os.path.exists(original_archive):\\n        import urllib\\n        print(\"Downloading data, Please Wait (11MB)...\")\\n        opener = urllib.urlopen(\\n            'http://archive.ics.uci.edu/ml/'\\n            'machine-learning-databases/covtype/covtype.data.gz')\\n        open(original_archive, 'wb').write(opener.read())\\n    print(\"Loading dataset...\")\\n    import gzip\\n    f = gzip.open(original_archive)\\n    X = np.fromstring(f.read().replace(\",\", \" \"), dtype=dtype, sep=\" \",\\n                      count=-1)\\n    X = X.reshape((581012, 55))\\n    if order.lower() == 'f':\\n        X = np.asfortranarray(X)\\n    f.close()\\n    y = np.ones(X.shape[0]) * -1\\n    y[np.where(X[:, -1] == 1)] = 1\\n    X = X[:, :-1]\\n    print(\"Creating train-test split...\")\\n    idx = np.arange(X.shape[0])\\n    rng.shuffle(idx)\\n    train_idx = idx[:522911]\\n    test_idx = idx[522911:]\\n    X_train = X[train_idx]\\n    y_train = y[train_idx]\\n    X_test = X[test_idx]\\n    y_test = y[test_idx]\\n    del X\\n    del y\\n    mean = X_train.mean(axis=0)\\n    std = X_train.std(axis=0)\\n    mean[10:] = 0.0\\n    std[10:] = 1.0\\n    X_train = (X_train - mean) / std\\n    X_test = (X_test - mean) / std\\n    return X_train, X_test, y_train, y_test",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_data(dtype=np.float32, order='F'):\\n    if not os.path.exists(original_archive):\\n        import urllib\\n        print(\"Downloading data, Please Wait (11MB)...\")\\n        opener = urllib.urlopen(\\n            'http://archive.ics.uci.edu/ml/'\\n            'machine-learning-databases/covtype/covtype.data.gz')\\n        open(original_archive, 'wb').write(opener.read())\\n    print(\"Loading dataset...\")\\n    import gzip\\n    f = gzip.open(original_archive)\\n    X = np.fromstring(f.read().replace(\",\", \" \"), dtype=dtype, sep=\" \",\\n                      count=-1)\\n    X = X.reshape((581012, 55))\\n    if order.lower() == 'f':\\n        X = np.asfortranarray(X)\\n    f.close()\\n    y = np.ones(X.shape[0]) * -1\\n    y[np.where(X[:, -1] == 1)] = 1\\n    X = X[:, :-1]\\n    print(\"Creating train-test split...\")\\n    idx = np.arange(X.shape[0])\\n    rng.shuffle(idx)\\n    train_idx = idx[:522911]\\n    test_idx = idx[522911:]\\n    X_train = X[train_idx]\\n    y_train = y[train_idx]\\n    X_test = X[test_idx]\\n    y_test = y[test_idx]\\n    del X\\n    del y\\n    mean = X_train.mean(axis=0)\\n    std = X_train.std(axis=0)\\n    mean[10:] = 0.0\\n    std[10:] = 1.0\\n    X_train = (X_train - mean) / std\\n    X_test = (X_test - mean) / std\\n    return X_train, X_test, y_train, y_test"
  },
  {
    "code": "def _ensure_datetimelike_to_i8(other, to_utc=False):\\n    from pandas import Index\\n    from pandas.core.arrays import PeriodArray\\n    if lib.is_scalar(other) and isna(other):\\n        return iNaT\\n    elif isinstance(other, (PeriodArray, ABCIndexClass)):\\n        if getattr(other, 'tz', None) is not None:\\n            if to_utc:\\n                other = other.tz_convert('UTC')\\n            else:\\n                other = other.tz_localize(None)\\n    else:\\n        try:\\n            return np.array(other, copy=False).view('i8')\\n        except TypeError:\\n            other = Index(other)\\n    return other.asi8",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Assorted DTA/TDA/PI bugfixes (#24397)",
    "fixed_code": "def _ensure_datetimelike_to_i8(other, to_utc=False):\\n    from pandas import Index\\n    from pandas.core.arrays import PeriodArray\\n    if lib.is_scalar(other) and isna(other):\\n        return iNaT\\n    elif isinstance(other, (PeriodArray, ABCIndexClass,\\n                            DatetimeLikeArrayMixin)):\\n        if getattr(other, 'tz', None) is not None:\\n            if to_utc:\\n                other = other.tz_convert('UTC')\\n            else:\\n                other = other.tz_localize(None)\\n    else:\\n        try:\\n            return np.array(other, copy=False).view('i8')\\n        except TypeError:\\n            other = Index(other)\\n    return other.asi8"
  },
  {
    "code": "def _getitem_lowerdim(self, tup):\\n\\t\\tif self.axis is not None:\\n\\t\\t\\taxis = self.obj._get_axis_number(self.axis)\\n\\t\\t\\treturn self._getitem_axis(tup, axis=axis)\\n\\t\\tif self._is_nested_tuple_indexer(tup):\\n\\t\\t\\treturn self._getitem_nested_tuple(tup)\\n\\t\\tax0 = self.obj._get_axis(0)\\n\\t\\tif isinstance(ax0, MultiIndex):\\n\\t\\t\\tresult = self._handle_lowerdim_multi_index_axis0(tup)\\n\\t\\t\\tif result is not None:\\n\\t\\t\\t\\treturn result\\n\\t\\tif len(tup) > self.obj.ndim:\\n\\t\\t\\traise IndexingError(\"Too many indexers. handle elsewhere\")\\n\\t\\tfor i, key in enumerate(tup):\\n\\t\\t\\tif is_label_like(key) or isinstance(key, tuple):\\n\\t\\t\\t\\tsection = self._getitem_axis(key, axis=i)\\n\\t\\t\\t\\tif not is_list_like_indexer(section):\\n\\t\\t\\t\\t\\treturn section\\n\\t\\t\\t\\telif section.ndim == self.ndim:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + (_NS,) + tup[i + 1:]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + tup[i + 1:]\\n\\t\\t\\t\\t\\tif (isinstance(section, ABCDataFrame) and i > 0 and\\n\\t\\t\\t\\t\\t\\t\\tlen(new_key) == 2):\\n\\t\\t\\t\\t\\t\\ta, b = new_key\\n\\t\\t\\t\\t\\t\\tnew_key = b, a\\n\\t\\t\\t\\t\\tif len(new_key) == 1:\\n\\t\\t\\t\\t\\t\\tnew_key, = new_key\\n\\t\\t\\t\\treturn getattr(section, self.name)[new_key]\\n\\t\\traise IndexingError('not applicable')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: iloc fails with non lex-sorted MultiIndex #13797\\n\\ncloses #13797\\n\\nAuthor: YG-Riku <rikuhiro@gmail.com>\\n\\nCloses #14038 from YG-Riku/mi-iloc-fix and squashes the following commits:\\n\\n1cf86a1 [YG-Riku] BUG: iloc fails with non lex-sorted MultiIndex #13797\\na23c736 [YG-Riku] BUG: iloc fails with non lex-sorted MultiIndex #13797\\n072d2a3 [YG-Riku] BUG: iloc fails with non lex-sorted MultiIndex #13797",
    "fixed_code": "def _getitem_lowerdim(self, tup):\\n\\t\\tif self.axis is not None:\\n\\t\\t\\taxis = self.obj._get_axis_number(self.axis)\\n\\t\\t\\treturn self._getitem_axis(tup, axis=axis)\\n\\t\\tif self._is_nested_tuple_indexer(tup):\\n\\t\\t\\treturn self._getitem_nested_tuple(tup)\\n\\t\\tax0 = self.obj._get_axis(0)\\n\\t\\tif isinstance(ax0, MultiIndex) and self.name != 'iloc':\\n\\t\\t\\tresult = self._handle_lowerdim_multi_index_axis0(tup)\\n\\t\\t\\tif result is not None:\\n\\t\\t\\t\\treturn result\\n\\t\\tif len(tup) > self.obj.ndim:\\n\\t\\t\\traise IndexingError(\"Too many indexers. handle elsewhere\")\\n\\t\\tfor i, key in enumerate(tup):\\n\\t\\t\\tif is_label_like(key) or isinstance(key, tuple):\\n\\t\\t\\t\\tsection = self._getitem_axis(key, axis=i)\\n\\t\\t\\t\\tif not is_list_like_indexer(section):\\n\\t\\t\\t\\t\\treturn section\\n\\t\\t\\t\\telif section.ndim == self.ndim:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + (_NS,) + tup[i + 1:]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + tup[i + 1:]\\n\\t\\t\\t\\t\\tif (isinstance(section, ABCDataFrame) and i > 0 and\\n\\t\\t\\t\\t\\t\\t\\tlen(new_key) == 2):\\n\\t\\t\\t\\t\\t\\ta, b = new_key\\n\\t\\t\\t\\t\\t\\tnew_key = b, a\\n\\t\\t\\t\\t\\tif len(new_key) == 1:\\n\\t\\t\\t\\t\\t\\tnew_key, = new_key\\n\\t\\t\\t\\treturn getattr(section, self.name)[new_key]\\n\\t\\traise IndexingError('not applicable')"
  },
  {
    "code": "def groupby_withnull(index, mapper):\\n    index = np.asarray(index)\\n    mapped_index = np.array([mapper(x) for x in index])\\n    if issubclass(mapped_index.dtype.type, basestring):\\n        mapped_index = mapped_index.astype(object)\\n    result = GroupDict()\\n    mask = isnull(mapped_index)\\n    nullkeys = index[mask]\\n    if nullkeys:\\n        result[np.NaN] = nullkeys\\n    notmask = -mask\\n    index = index[notmask]\\n    mapped_index = mapped_index[notmask]\\n    for idx, key in zip(index, mapped_index):\\n        result.setdefault(key, []).append(idx)\\n    return result\\nclass GroupDict(dict):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "miscellaneous fixes and cleanup",
    "fixed_code": "def groupby_withnull(index, mapper):\\n    index = np.asarray(index)\\n    mapped_index = np.array([mapper(x) for x in index])\\n    if issubclass(mapped_index.dtype.type, basestring):\\n        mapped_index = mapped_index.astype(object)\\n    result = GroupDict()\\n    mask = isnull(mapped_index)\\n    nullkeys = index[mask]\\n    if len(nullkeys) > 0:\\n        result[np.NaN] = nullkeys\\n    notmask = -mask\\n    index = index[notmask]\\n    mapped_index = mapped_index[notmask]\\n    for idx, key in zip(index, mapped_index):\\n        result.setdefault(key, []).append(idx)\\n    return result\\nclass GroupDict(dict):"
  },
  {
    "code": "def __init__(self, body, aws_conn_id='aws_default', default_schedule=False):\\n        self.body = body\\n        self.aws_conn_id = aws_conn_id\\n        self.default_schedule = default_schedule",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, body, aws_conn_id='aws_default', default_schedule=False):\\n        self.body = body\\n        self.aws_conn_id = aws_conn_id\\n        self.default_schedule = default_schedule"
  },
  {
    "code": "def _remove_universal_flags(_config_vars):\\n    for cv in _UNIVERSAL_CONFIG_VARS:\\n        if cv in _config_vars and cv not in os.environ:\\n            flags = _config_vars[cv]\\n            flags = re.sub(r'-arch\\s+\\w+\\s', ' ', flags, flags=re.ASCII)\\n            flags = re.sub(r'-isysroot\\s*\\S+', ' ', flags)\\n            _save_modified_value(_config_vars, cv, flags)\\n    return _config_vars",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _remove_universal_flags(_config_vars):\\n    for cv in _UNIVERSAL_CONFIG_VARS:\\n        if cv in _config_vars and cv not in os.environ:\\n            flags = _config_vars[cv]\\n            flags = re.sub(r'-arch\\s+\\w+\\s', ' ', flags, flags=re.ASCII)\\n            flags = re.sub(r'-isysroot\\s*\\S+', ' ', flags)\\n            _save_modified_value(_config_vars, cv, flags)\\n    return _config_vars"
  },
  {
    "code": "def find_spec(self, fullname, path, target=None):\\n        loader = self._get_loader(fullname, path)\\n        if loader is None:\\n            return None\\n        spec = spec_from_loader(fullname, loader)\\n        if spec is not None and hasattr(loader, '_subpackage_search_paths'):\\n            spec.submodule_search_locations = loader._subpackage_search_paths\\n        return spec",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def find_spec(self, fullname, path, target=None):\\n        loader = self._get_loader(fullname, path)\\n        if loader is None:\\n            return None\\n        spec = spec_from_loader(fullname, loader)\\n        if spec is not None and hasattr(loader, '_subpackage_search_paths'):\\n            spec.submodule_search_locations = loader._subpackage_search_paths\\n        return spec"
  },
  {
    "code": "def wrapped(self, other):\\n\\t\\tresult = meth(self, other)\\n\\t\\tif result is NotImplemented:\\n\\t\\t\\treturn NotImplemented\\n\\t\\tnew_freq = self._get_addsub_freq(other, result)\\n\\t\\tresult._freq = new_freq\\n\\t\\treturn result\\n\\twrapped.__name__ = opname\\n\\treturn wrapped",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def wrapped(self, other):\\n\\t\\tresult = meth(self, other)\\n\\t\\tif result is NotImplemented:\\n\\t\\t\\treturn NotImplemented\\n\\t\\tnew_freq = self._get_addsub_freq(other, result)\\n\\t\\tresult._freq = new_freq\\n\\t\\treturn result\\n\\twrapped.__name__ = opname\\n\\treturn wrapped"
  },
  {
    "code": "def plot_partial_dependence(\\n    estimator,\\n    X,\\n    features,\\n    *,\\n    feature_names=None,\\n    target=None,\\n    response_method=\"auto\",\\n    n_cols=3,\\n    grid_resolution=100,\\n    percentiles=(0.05, 0.95),\\n    method=\"auto\",\\n    n_jobs=None,\\n    verbose=0,\\n    line_kw=None,\\n    contour_kw=None,\\n    ax=None,\\n    kind=\"average\",\\n    subsample=1000,\\n    random_state=None,\\n):\\n    check_matplotlib_support(\"plot_partial_dependence\")  \\n    import matplotlib.pyplot as plt  \\n    if hasattr(estimator, \"classes_\") and np.size(estimator.classes_) > 2:\\n        if target is None:\\n            raise ValueError(\"target must be specified for multi-class\")\\n        target_idx = np.searchsorted(estimator.classes_, target)\\n        if (\\n            not (0 <= target_idx < len(estimator.classes_))\\n            or estimator.classes_[target_idx] != target\\n        ):\\n            raise ValueError(\"target not in est.classes_, got {}\".format(target))\\n    else:\\n        target_idx = 0\\n    if not (hasattr(X, \"__array__\") or sparse.issparse(X)):\\n        X = check_array(X, force_all_finite=\"allow-nan\", dtype=object)\\n    n_features = X.shape[1]\\n    if feature_names is None:\\n        if hasattr(X, \"loc\"):\\n            feature_names = X.columns.tolist()\\n        else:\\n            feature_names = [str(i) for i in range(n_features)]\\n    elif hasattr(feature_names, \"tolist\"):\\n        feature_names = feature_names.tolist()\\n    if len(set(feature_names)) != len(feature_names):\\n        raise ValueError(\"feature_names should not contain duplicates.\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add kwargs to format ICE and PD lines separately in partial dependence plots (#19428)",
    "fixed_code": "def plot_partial_dependence(\\n    estimator,\\n    X,\\n    features,\\n    *,\\n    feature_names=None,\\n    target=None,\\n    response_method=\"auto\",\\n    n_cols=3,\\n    grid_resolution=100,\\n    percentiles=(0.05, 0.95),\\n    method=\"auto\",\\n    n_jobs=None,\\n    verbose=0,\\n    line_kw=None,\\n    ice_lines_kw=None,\\n    pd_line_kw=None,\\n    contour_kw=None,\\n    ax=None,\\n    kind=\"average\",\\n    subsample=1000,\\n    random_state=None,\\n):\\n    check_matplotlib_support(\"plot_partial_dependence\")  \\n    import matplotlib.pyplot as plt  \\n    if hasattr(estimator, \"classes_\") and np.size(estimator.classes_) > 2:\\n        if target is None:\\n            raise ValueError(\"target must be specified for multi-class\")\\n        target_idx = np.searchsorted(estimator.classes_, target)\\n        if (\\n            not (0 <= target_idx < len(estimator.classes_))\\n            or estimator.classes_[target_idx] != target\\n        ):\\n            raise ValueError(\"target not in est.classes_, got {}\".format(target))\\n    else:\\n        target_idx = 0\\n    if not (hasattr(X, \"__array__\") or sparse.issparse(X)):\\n        X = check_array(X, force_all_finite=\"allow-nan\", dtype=object)\\n    n_features = X.shape[1]\\n    if feature_names is None:\\n        if hasattr(X, \"loc\"):\\n            feature_names = X.columns.tolist()\\n        else:\\n            feature_names = [str(i) for i in range(n_features)]\\n    elif hasattr(feature_names, \"tolist\"):\\n        feature_names = feature_names.tolist()\\n    if len(set(feature_names)) != len(feature_names):\\n        raise ValueError(\"feature_names should not contain duplicates.\")"
  },
  {
    "code": "def map_config_to_obj(module):\\n    data = get_config(module, flags=['| section username'])\\n    match = re.findall(r'(?:^(?:u|\\s{2}u))sername (\\S+)', data, re.M)\\n    if not match:\\n        return list()\\n    instances = list()\\n    for user in set(match):\\n        regex = r'username %s .+$' % user\\n        cfg = re.findall(regex, data, re.M)\\n        cfg = '\\n'.join(cfg)\\n        obj = {\\n            'name': user,\\n            'state': 'present',\\n            'nopassword': 'nopassword' in cfg,\\n            'configured_password': None,\\n            'hashed_password': None,\\n            'password_type': parse_password_type(cfg),\\n            'sshkey': parse_sshkey(data, user),\\n            'privilege': parse_privilege(cfg),\\n            'view': parse_view(cfg)\\n        }\\n        instances.append(obj)\\n    return instances",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_config_to_obj(module):\\n    data = get_config(module, flags=['| section username'])\\n    match = re.findall(r'(?:^(?:u|\\s{2}u))sername (\\S+)', data, re.M)\\n    if not match:\\n        return list()\\n    instances = list()\\n    for user in set(match):\\n        regex = r'username %s .+$' % user\\n        cfg = re.findall(regex, data, re.M)\\n        cfg = '\\n'.join(cfg)\\n        obj = {\\n            'name': user,\\n            'state': 'present',\\n            'nopassword': 'nopassword' in cfg,\\n            'configured_password': None,\\n            'hashed_password': None,\\n            'password_type': parse_password_type(cfg),\\n            'sshkey': parse_sshkey(data, user),\\n            'privilege': parse_privilege(cfg),\\n            'view': parse_view(cfg)\\n        }\\n        instances.append(obj)\\n    return instances"
  },
  {
    "code": "def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\\n\\t\\tcan_edit_parent = self.has_change_permission(request, obj) if obj else self.has_add_permission(request)\\n\\t\\tinline_admin_formsets = []\\n\\t\\tfor inline, formset in zip(inline_instances, formsets):\\n\\t\\t\\tfieldsets = list(inline.get_fieldsets(request, obj))\\n\\t\\t\\treadonly = list(inline.get_readonly_fields(request, obj))\\n\\t\\t\\tif can_edit_parent:\\n\\t\\t\\t\\thas_add_permission = inline.has_add_permission(request, obj)\\n\\t\\t\\t\\thas_change_permission = inline.has_change_permission(request, obj)\\n\\t\\t\\t\\thas_delete_permission = inline.has_delete_permission(request, obj)\\n\\t\\t\\telse:\\n\\t\\t\\t\\thas_add_permission = has_change_permission = has_delete_permission = False\\n\\t\\t\\t\\tformset.extra = formset.max_num = 0\\n\\t\\t\\thas_view_permission = inline.has_view_permission(request, obj)\\n\\t\\t\\tprepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n\\t\\t\\tinline_admin_formset = helpers.InlineAdminFormSet(\\n\\t\\t\\t\\tinline, formset, fieldsets, prepopulated, readonly, model_admin=self,\\n\\t\\t\\t\\thas_add_permission=has_add_permission, has_change_permission=has_change_permission,\\n\\t\\t\\t\\thas_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\\n\\t\\t\\t)\\n\\t\\t\\tinline_admin_formsets.append(inline_admin_formset)\\n\\t\\treturn inline_admin_formsets",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\\n\\t\\tcan_edit_parent = self.has_change_permission(request, obj) if obj else self.has_add_permission(request)\\n\\t\\tinline_admin_formsets = []\\n\\t\\tfor inline, formset in zip(inline_instances, formsets):\\n\\t\\t\\tfieldsets = list(inline.get_fieldsets(request, obj))\\n\\t\\t\\treadonly = list(inline.get_readonly_fields(request, obj))\\n\\t\\t\\tif can_edit_parent:\\n\\t\\t\\t\\thas_add_permission = inline.has_add_permission(request, obj)\\n\\t\\t\\t\\thas_change_permission = inline.has_change_permission(request, obj)\\n\\t\\t\\t\\thas_delete_permission = inline.has_delete_permission(request, obj)\\n\\t\\t\\telse:\\n\\t\\t\\t\\thas_add_permission = has_change_permission = has_delete_permission = False\\n\\t\\t\\t\\tformset.extra = formset.max_num = 0\\n\\t\\t\\thas_view_permission = inline.has_view_permission(request, obj)\\n\\t\\t\\tprepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n\\t\\t\\tinline_admin_formset = helpers.InlineAdminFormSet(\\n\\t\\t\\t\\tinline, formset, fieldsets, prepopulated, readonly, model_admin=self,\\n\\t\\t\\t\\thas_add_permission=has_add_permission, has_change_permission=has_change_permission,\\n\\t\\t\\t\\thas_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\\n\\t\\t\\t)\\n\\t\\t\\tinline_admin_formsets.append(inline_admin_formset)\\n\\t\\treturn inline_admin_formsets"
  },
  {
    "code": "def shift(self, periods=1, freq=None, axis=0):\\n        if periods == 0:\\n            return self.copy()\\n        block_axis = self._get_block_manager_axis(axis)\\n        if freq is None:\\n            new_data = self._data.shift(periods=periods, axis=block_axis)\\n        else:\\n            return self.tshift(periods, freq)\\n        return self._constructor(new_data).__finalize__(self)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def shift(self, periods=1, freq=None, axis=0):\\n        if periods == 0:\\n            return self.copy()\\n        block_axis = self._get_block_manager_axis(axis)\\n        if freq is None:\\n            new_data = self._data.shift(periods=periods, axis=block_axis)\\n        else:\\n            return self.tshift(periods, freq)\\n        return self._constructor(new_data).__finalize__(self)"
  },
  {
    "code": "def setter(self, vmin, vmax, ignore=False):\\n\\t\\tif ignore:\\n\\t\\t\\tsetattr(getattr(self.axes, lim_name), attr_name, (vmin, vmax))\\n\\t\\telse:\\n\\t\\t\\toldmin, oldmax = getter(self)\\n\\t\\t\\tif oldmin < oldmax:\\n\\t\\t\\t\\tsetter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\\n\\t\\t\\t\\t\\t   ignore=True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tsetter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\\n\\t\\t\\t\\t\\t   ignore=True)\\n\\t\\tself.stale = True\\n\\tgetter.__name__ = f\"get_{method_name}_interval\"\\n\\tsetter.__name__ = f\"set_{method_name}_interval\"\\n\\treturn getter, setter",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Don't misclip axis when calling set_ticks on inverted axes.\\n\\nA small inversion in 0e41317...",
    "fixed_code": "def setter(self, vmin, vmax, ignore=False):\\n\\t\\tif ignore:\\n\\t\\t\\tsetattr(getattr(self.axes, lim_name), attr_name, (vmin, vmax))\\n\\t\\telse:\\n\\t\\t\\toldmin, oldmax = getter(self)\\n\\t\\t\\tif oldmin < oldmax:\\n\\t\\t\\t\\tsetter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\\n\\t\\t\\t\\t\\t   ignore=True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tsetter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\\n\\t\\t\\t\\t\\t   ignore=True)\\n\\t\\tself.stale = True\\n\\tgetter.__name__ = f\"get_{method_name}_interval\"\\n\\tsetter.__name__ = f\"set_{method_name}_interval\"\\n\\treturn getter, setter"
  },
  {
    "code": "def _join_on(self, other, on, how, lsuffix, rsuffix):\\n        if how not in ['left', 'inner']:\\n            raise Exception('Only inner / left joins currently supported')\\n        if isinstance(other, Series):\\n            assert(other.name is not None)\\n            other = DataFrame({other.name : other})\\n        if isinstance(on, (list, tuple)):\\n            if len(on) == 1:\\n                join_key = self[on[0]].values\\n            else:\\n                join_key = zip(*[self[k] for k in on])\\n                join_key = common._asarray_tuplesafe(join_key,\\n                                                     dtype=np.object_)\\n        else:\\n            join_key = self[on].values\\n        new_data = self._data.join_on(other._data, join_key, how=how, axis=1,\\n                                      lsuffix=lsuffix, rsuffix=rsuffix)\\n        return self._constructor(new_data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: frame.py test coverage",
    "fixed_code": "def _join_on(self, other, on, how, lsuffix, rsuffix):\\n        if how not in ('left', 'inner'):  \\n            raise Exception('Only inner / left joins currently supported')\\n        if isinstance(other, Series):\\n            assert(other.name is not None)\\n            other = DataFrame({other.name : other})\\n        if isinstance(on, (list, tuple)):\\n            if len(on) == 1:\\n                join_key = self[on[0]].values\\n            else:\\n                join_key = zip(*[self[k] for k in on])\\n                join_key = common._asarray_tuplesafe(join_key,\\n                                                     dtype=np.object_)\\n        else:\\n            join_key = self[on].values\\n        new_data = self._data.join_on(other._data, join_key, how=how, axis=1,\\n                                      lsuffix=lsuffix, rsuffix=rsuffix)\\n        return self._constructor(new_data)"
  },
  {
    "code": "def _set_url(self, url):\\n\\t\\tif not isinstance(url, six.string_types):\\n\\t\\t\\traise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\\n\\t\\ts = safe_url_string(url, self.encoding)\\n\\t\\tself._url = escape_ajax(s)\\n\\t\\tif ('://' not in self._url) and (not self._url.startswith('data:')):\\n\\t\\t\\traise ValueError('Missing scheme in request url: %s' % self._url)\\n\\turl = property(_get_url, obsolete_setter(_set_url, 'url'))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _set_url(self, url):\\n\\t\\tif not isinstance(url, six.string_types):\\n\\t\\t\\traise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\\n\\t\\ts = safe_url_string(url, self.encoding)\\n\\t\\tself._url = escape_ajax(s)\\n\\t\\tif ('://' not in self._url) and (not self._url.startswith('data:')):\\n\\t\\t\\traise ValueError('Missing scheme in request url: %s' % self._url)\\n\\turl = property(_get_url, obsolete_setter(_set_url, 'url'))"
  },
  {
    "code": "def __del__(self):\\n            if self._state == futures._PENDING:\\n                context = {\\n                    'task': self,\\n                    'message': 'Task was destroyed but it is pending!',\\n                }\\n                if self._source_traceback:\\n                    context['source_traceback'] = self._source_traceback\\n                self._loop.call_exception_handler(context)\\n            futures.Future.__del__(self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #21163: BaseEventLoop.run_until_complete() and test_utils.run_briefly() don't log the \"destroy pending task\" message anymore. The log is redundant for run_until_complete() and useless in run_briefly().",
    "fixed_code": "def __del__(self):\\n            if self._state == futures._PENDING and self._log_destroy_pending:\\n                context = {\\n                    'task': self,\\n                    'message': 'Task was destroyed but it is pending!',\\n                }\\n                if self._source_traceback:\\n                    context['source_traceback'] = self._source_traceback\\n                self._loop.call_exception_handler(context)\\n            futures.Future.__del__(self)"
  },
  {
    "code": "def _nanmin(values, axis=None, skipna=True):\\n\\tmask = isnull(values)\\n\\tdtype = values.dtype\\n\\tif skipna and not issubclass(dtype.type,\\n\\t\\t\\t\\t\\t\\t\\t\\t (np.integer, np.datetime64)):\\n\\t\\tvalues = values.copy()\\n\\t\\tnp.putmask(values, mask, np.inf)\\n\\tif issubclass(dtype.type, np.datetime64):\\n\\t\\tvalues = values.view(np.int64)\\n\\tif (values.dtype == np.object_\\n\\t\\tand sys.version_info[0] >= 3):  \\n\\t\\timport __builtin__\\n\\t\\tif values.ndim > 1:\\n\\t\\t\\tapply_ax = axis if axis is not None else 0\\n\\t\\t\\tresult = np.apply_along_axis(__builtin__.min, apply_ax, values)\\n\\t\\telse:\\n\\t\\t\\tresult = __builtin__.min(values)\\n\\telse:\\n\\t\\tif ((axis is not None and values.shape[axis] == 0)\\n\\t\\t\\t or values.size == 0):\\n\\t\\t\\tresult = com.ensure_float(values.sum(axis))\\n\\t\\t\\tresult.fill(np.nan)\\n\\t\\telse:\\n\\t\\t\\tresult = values.min(axis)\\n\\tif issubclass(dtype.type, np.datetime64):\\n\\t\\tif not isinstance(result, np.ndarray):\\n\\t\\t\\tresult = lib.Timestamp(result)\\n\\t\\telse:\\n\\t\\t\\tresult = result.view(dtype)\\n\\treturn _maybe_null_out(result, axis, mask)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _nanmin(values, axis=None, skipna=True):\\n\\tmask = isnull(values)\\n\\tdtype = values.dtype\\n\\tif skipna and not issubclass(dtype.type,\\n\\t\\t\\t\\t\\t\\t\\t\\t (np.integer, np.datetime64)):\\n\\t\\tvalues = values.copy()\\n\\t\\tnp.putmask(values, mask, np.inf)\\n\\tif issubclass(dtype.type, np.datetime64):\\n\\t\\tvalues = values.view(np.int64)\\n\\tif (values.dtype == np.object_\\n\\t\\tand sys.version_info[0] >= 3):  \\n\\t\\timport __builtin__\\n\\t\\tif values.ndim > 1:\\n\\t\\t\\tapply_ax = axis if axis is not None else 0\\n\\t\\t\\tresult = np.apply_along_axis(__builtin__.min, apply_ax, values)\\n\\t\\telse:\\n\\t\\t\\tresult = __builtin__.min(values)\\n\\telse:\\n\\t\\tif ((axis is not None and values.shape[axis] == 0)\\n\\t\\t\\t or values.size == 0):\\n\\t\\t\\tresult = com.ensure_float(values.sum(axis))\\n\\t\\t\\tresult.fill(np.nan)\\n\\t\\telse:\\n\\t\\t\\tresult = values.min(axis)\\n\\tif issubclass(dtype.type, np.datetime64):\\n\\t\\tif not isinstance(result, np.ndarray):\\n\\t\\t\\tresult = lib.Timestamp(result)\\n\\t\\telse:\\n\\t\\t\\tresult = result.view(dtype)\\n\\treturn _maybe_null_out(result, axis, mask)"
  },
  {
    "code": "def url(parser, token):\\n    bits = token.split_contents()\\n    if len(bits) < 2:\\n        raise TemplateSyntaxError(\"'%s' takes at least one argument\"\\n                                  \" (path to a view)\" % bits[0])\\n    viewname = parser.compile_filter(bits[1])\\n    args = []\\n    kwargs = {}\\n    asvar = None\\n    bits = bits[2:]\\n    if len(bits) >= 2 and bits[-2] == 'as':\\n        asvar = bits[-1]\\n        bits = bits[:-2]\\n    if len(bits):\\n        for bit in bits:\\n            match = kwarg_re.match(bit)\\n            if not match:\\n                raise TemplateSyntaxError(\"Malformed arguments to url tag\")\\n            name, value = match.groups()\\n            if name:\\n                kwargs[name] = parser.compile_filter(value)\\n            else:\\n                args.append(parser.compile_filter(value))\\n    return URLNode(viewname, args, kwargs, asvar, legacy_view_name=False)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def url(parser, token):\\n    bits = token.split_contents()\\n    if len(bits) < 2:\\n        raise TemplateSyntaxError(\"'%s' takes at least one argument\"\\n                                  \" (path to a view)\" % bits[0])\\n    viewname = parser.compile_filter(bits[1])\\n    args = []\\n    kwargs = {}\\n    asvar = None\\n    bits = bits[2:]\\n    if len(bits) >= 2 and bits[-2] == 'as':\\n        asvar = bits[-1]\\n        bits = bits[:-2]\\n    if len(bits):\\n        for bit in bits:\\n            match = kwarg_re.match(bit)\\n            if not match:\\n                raise TemplateSyntaxError(\"Malformed arguments to url tag\")\\n            name, value = match.groups()\\n            if name:\\n                kwargs[name] = parser.compile_filter(value)\\n            else:\\n                args.append(parser.compile_filter(value))\\n    return URLNode(viewname, args, kwargs, asvar, legacy_view_name=False)"
  },
  {
    "code": "def get_commands(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    for key, value in proposed_commands.items():\\n        command = ''\\n        if key == 'ip pim ssm range':\\n            if value == 'default':\\n                command = 'no ip pim ssm range none'\\n            elif value == 'none':\\n                command = 'ip pim ssm range none'\\n            elif value:\\n                command = 'ip pim ssm range {0}'.format(value)\\n        elif key == 'ip pim bfd':\\n            no_cmd = 'no ' if value == 'disable' else ''\\n            command = no_cmd + key\\n        if command:\\n            commands.append(command)\\n    if commands:\\n        candidate.add(commands, parents=[])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_commands(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    for key, value in proposed_commands.items():\\n        command = ''\\n        if key == 'ip pim ssm range':\\n            if value == 'default':\\n                command = 'no ip pim ssm range none'\\n            elif value == 'none':\\n                command = 'ip pim ssm range none'\\n            elif value:\\n                command = 'ip pim ssm range {0}'.format(value)\\n        elif key == 'ip pim bfd':\\n            no_cmd = 'no ' if value == 'disable' else ''\\n            command = no_cmd + key\\n        if command:\\n            commands.append(command)\\n    if commands:\\n        candidate.add(commands, parents=[])"
  },
  {
    "code": "def shift(self, type: int, value: Text, newstate: int, context: Context) -> None:\\n        if self.is_backtracking:\\n            dfa, state, _ = self.stack[-1]\\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\\n        else:\\n            dfa, state, node = self.stack[-1]\\n            rawnode: RawNode = (type, value, context, None)\\n            newnode = convert(self.grammar, rawnode)\\n            assert node[-1] is not None\\n            node[-1].append(newnode)\\n            self.stack[-1] = (dfa, newstate, node)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def shift(self, type: int, value: Text, newstate: int, context: Context) -> None:\\n        if self.is_backtracking:\\n            dfa, state, _ = self.stack[-1]\\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\\n        else:\\n            dfa, state, node = self.stack[-1]\\n            rawnode: RawNode = (type, value, context, None)\\n            newnode = convert(self.grammar, rawnode)\\n            assert node[-1] is not None\\n            node[-1].append(newnode)\\n            self.stack[-1] = (dfa, newstate, node)"
  },
  {
    "code": "def lars_path(X, y, Xy=None, Gram=None, max_iter=500,\\n\\t\\t\\t  alpha_min=0, method='lar', copy_X=True,\\n\\t\\t\\t  eps=np.finfo(np.float).eps,\\n\\t\\t\\t  copy_Gram=True, verbose=0, return_path=True,\\n\\t\\t\\t  return_n_iter=False):\\n\\tn_features = X.shape[1]\\n\\tn_samples = y.size\\n\\tmax_features = min(max_iter, n_features)\\n\\tif return_path:\\n\\t\\tcoefs = np.zeros((max_features + 1, n_features))\\n\\t\\talphas = np.zeros(max_features + 1)\\n\\telse:\\n\\t\\tcoef, prev_coef = np.zeros(n_features), np.zeros(n_features)\\n\\t\\talpha, prev_alpha = np.array([0.]), np.array([0.])  \\n\\tn_iter, n_active = 0, 0\\n\\tactive, indices = list(), np.arange(n_features)\\n\\tsign_active = np.empty(max_features, dtype=np.int8)\\n\\tdrop = False\\n\\tL = np.zeros((max_features, max_features), dtype=X.dtype)\\n\\tswap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\\n\\tsolve_cholesky, = get_lapack_funcs(('potrs',), (X,))\\n\\tif Gram is None:\\n\\t\\tif copy_X:\\n\\t\\t\\tX = X.copy('F')\\n\\telif Gram == 'auto':\\n\\t\\tGram = None\\n\\t\\tif X.shape[0] > X.shape[1]:\\n\\t\\t\\tGram = np.dot(X.T, X)\\n\\telif copy_Gram:\\n\\t\\t\\tGram = Gram.copy()\\n\\tif Xy is None:\\n\\t\\tCov = np.dot(X.T, y)\\n\\telse:\\n\\t\\tCov = Xy.copy()\\n\\tif verbose:\\n\\t\\tif verbose > 1:\\n\\t\\t\\tprint(\"Step\\t\\tAdded\\t\\tDropped\\t\\tActive set size\\t\\tC\")\\n\\t\\telse:\\n\\t\\t\\tsys.stdout.write('.')\\n\\t\\t\\tsys.stdout.flush()\\n\\ttiny = np.finfo(np.float).tiny  \\n\\ttiny32 = np.finfo(np.float32).tiny  \\n\\twhile True:\\n\\t\\tif Cov.size:\\n\\t\\t\\tC_idx = np.argmax(np.abs(Cov))\\n\\t\\t\\tC_ = Cov[C_idx]\\n\\t\\t\\tC = np.fabs(C_)\\n\\t\\telse:\\n\\t\\t\\tC = 0.\\n\\t\\tif return_path:\\n\\t\\t\\talpha = alphas[n_iter, np.newaxis]\\n\\t\\t\\tcoef = coefs[n_iter]\\n\\t\\t\\tprev_alpha = alphas[n_iter - 1, np.newaxis]\\n\\t\\t\\tprev_coef = coefs[n_iter - 1]\\n\\t\\talpha[0] = C / n_samples\\n\\t\\tif alpha[0] <= alpha_min:  \\n\\t\\t\\tif not (abs(alpha[0] - alpha_min) < 10 * np.finfo(np.float32).eps):\\n\\t\\t\\t\\tif n_iter > 0:\\n\\t\\t\\t\\t\\tss = ((prev_alpha[0] - alpha_min) /\\n\\t\\t\\t\\t\\t\\t  (prev_alpha[0] - alpha[0]))\\n\\t\\t\\t\\t\\tcoef[:] = prev_coef + ss * (coef - prev_coef)\\n\\t\\t\\t\\talpha[0] = alpha_min\\n\\t\\t\\tif return_path:\\n\\t\\t\\t\\tcoefs[n_iter] = coef\\n\\t\\t\\tbreak\\n\\t\\tif n_iter >= max_iter or n_active >= n_features:\\n\\t\\t\\tbreak\\n\\t\\tif not drop:\\n\\t\\t\\tsign_active[n_active] = np.sign(C_)\\n\\t\\t\\tm, n = n_active, C_idx + n_active\\n\\t\\t\\tCov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\\n\\t\\t\\tindices[n], indices[m] = indices[m], indices[n]\\n\\t\\t\\tCov_not_shortened = Cov\\n\\t\\t\\tCov = Cov[1:]  \\n\\t\\t\\tif Gram is None:\\n\\t\\t\\t\\tX.T[n], X.T[m] = swap(X.T[n], X.T[m])\\n\\t\\t\\t\\tc = nrm2(X.T[n_active]) ** 2\\n\\t\\t\\t\\tL[n_active, :n_active] = \\\\n\\t\\t\\t\\t\\tnp.dot(X.T[n_active], X.T[:n_active].T)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tGram[m], Gram[n] = swap(Gram[m], Gram[n])\\n\\t\\t\\t\\tGram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])\\n\\t\\t\\t\\tc = Gram[n_active, n_active]\\n\\t\\t\\t\\tL[n_active, :n_active] = Gram[n_active, :n_active]\\n\\t\\t\\tif n_active:\\n\\t\\t\\t\\tlinalg.solve_triangular(L[:n_active, :n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tL[n_active, :n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttrans=0, lower=1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toverwrite_b=True,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t**solve_triangular_args)\\n\\t\\t\\tv = np.dot(L[n_active, :n_active], L[n_active, :n_active])\\n\\t\\t\\tdiag = max(np.sqrt(np.abs(c - v)), eps)\\n\\t\\t\\tL[n_active, n_active] = diag\\n\\t\\t\\tif diag < 1e-7:\\n\\t\\t\\t\\twarnings.warn('Regressors in active set degenerate. '\\n\\t\\t\\t\\t\\t\\t\\t  'Dropping a regressor, after %i iterations, '\\n\\t\\t\\t\\t\\t\\t\\t  'i.e. alpha=%.3e, '\\n\\t\\t\\t\\t\\t\\t\\t  'with an active set of %i regressors, and '\\n\\t\\t\\t\\t\\t\\t\\t  'the smallest cholesky pivot element being %.3e'\\n\\t\\t\\t\\t\\t\\t\\t  % (n_iter, alpha, n_active, diag),\\n\\t\\t\\t\\t\\t\\t\\t  ConvergenceWarning)\\n\\t\\t\\t\\tCov = Cov_not_shortened\\n\\t\\t\\t\\tCov[0] = 0\\n\\t\\t\\t\\tCov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tactive.append(indices[n_active])\\n\\t\\t\\tn_active += 1\\n\\t\\t\\tif verbose > 1:\\n\\t\\t\\t\\tprint(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, active[-1], '',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  n_active, C))\\n\\t\\tif method == 'lasso' and n_iter > 0 and prev_alpha[0] < alpha[0]:\\n\\t\\t\\twarnings.warn('Early stopping the lars path, as the residues '\\n\\t\\t\\t\\t\\t\\t  'are small and the current value of alpha is no '\\n\\t\\t\\t\\t\\t\\t  'longer well controlled. %i iterations, alpha=%.3e, '\\n\\t\\t\\t\\t\\t\\t  'previous alpha=%.3e, with an active set of %i '\\n\\t\\t\\t\\t\\t\\t  'regressors.'\\n\\t\\t\\t\\t\\t\\t  % (n_iter, alpha, prev_alpha, n_active),\\n\\t\\t\\t\\t\\t\\t  ConvergenceWarning)\\n\\t\\t\\tbreak\\n\\t\\tleast_squares, info = solve_cholesky(L[:n_active, :n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t sign_active[:n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t lower=True)\\n\\t\\tif least_squares.size == 1 and least_squares == 0:\\n\\t\\t\\tleast_squares[...] = 1\\n\\t\\t\\tAA = 1.\\n\\t\\telse:\\n\\t\\t\\tAA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))\\n\\t\\t\\tif not np.isfinite(AA):\\n\\t\\t\\t\\ti = 0\\n\\t\\t\\t\\tL_ = L[:n_active, :n_active].copy()\\n\\t\\t\\t\\twhile not np.isfinite(AA):\\n\\t\\t\\t\\t\\tL_.flat[::n_active + 1] += (2 ** i) * eps\\n\\t\\t\\t\\t\\tleast_squares, info = solve_cholesky(\\n\\t\\t\\t\\t\\t\\tL_, sign_active[:n_active], lower=True)\\n\\t\\t\\t\\t\\ttmp = max(np.sum(least_squares * sign_active[:n_active]),\\n\\t\\t\\t\\t\\t\\t\\t  eps)\\n\\t\\t\\t\\t\\tAA = 1. / np.sqrt(tmp)\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\tleast_squares *= AA\\n\\t\\tif Gram is None:\\n\\t\\t\\teq_dir = np.dot(X.T[:n_active].T, least_squares)\\n\\t\\t\\tcorr_eq_dir = np.dot(X.T[n_active:], eq_dir)\\n\\t\\telse:\\n\\t\\t\\tcorr_eq_dir = np.dot(Gram[:n_active, n_active:].T,\\n\\t\\t\\t\\t\\t\\t\\t\\t least_squares)\\n\\t\\tg1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\\n\\t\\tg2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\\n\\t\\tgamma_ = min(g1, g2, C / AA)\\n\\t\\tdrop = False\\n\\t\\tz = -coef[active] / (least_squares + tiny32)\\n\\t\\tz_pos = arrayfuncs.min_pos(z)\\n\\t\\tif z_pos < gamma_:\\n\\t\\t\\tidx = np.where(z == z_pos)[0][::-1]\\n\\t\\t\\tsign_active[idx] = -sign_active[idx]\\n\\t\\t\\tif method == 'lasso':\\n\\t\\t\\t\\tgamma_ = z_pos\\n\\t\\t\\tdrop = True\\n\\t\\tn_iter += 1\\n\\t\\tif return_path:\\n\\t\\t\\tif n_iter >= coefs.shape[0]:\\n\\t\\t\\t\\tdel coef, alpha, prev_alpha, prev_coef\\n\\t\\t\\t\\tadd_features = 2 * max(1, (max_features - n_active))\\n\\t\\t\\t\\tcoefs = np.resize(coefs, (n_iter + add_features, n_features))\\n\\t\\t\\t\\talphas = np.resize(alphas, n_iter + add_features)\\n\\t\\t\\tcoef = coefs[n_iter]\\n\\t\\t\\tprev_coef = coefs[n_iter - 1]\\n\\t\\t\\talpha = alphas[n_iter, np.newaxis]\\n\\t\\t\\tprev_alpha = alphas[n_iter - 1, np.newaxis]\\n\\t\\telse:\\n\\t\\t\\tprev_coef = coef\\n\\t\\t\\tprev_alpha[0] = alpha[0]\\n\\t\\t\\tcoef = np.zeros_like(coef)\\n\\t\\tcoef[active] = prev_coef[active] + gamma_ * least_squares\\n\\t\\tCov -= gamma_ * corr_eq_dir\\n\\t\\tif drop and method == 'lasso':\\n\\t\\t\\t[arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in\\n\\t\\t\\t\\tidx]\\n\\t\\t\\tn_active -= 1\\n\\t\\t\\tm, n = idx, n_active\\n\\t\\t\\tdrop_idx = [active.pop(ii) for ii in idx]\\n\\t\\t\\tif Gram is None:\\n\\t\\t\\t\\tfor ii in idx:\\n\\t\\t\\t\\t\\tfor i in range(ii, n_active):\\n\\t\\t\\t\\t\\t\\tX.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])\\n\\t\\t\\t\\t\\t\\tindices[i], indices[i + 1] = indices[i + 1], indices[i]\\n\\t\\t\\t\\tresidual = y - np.dot(X[:, :n_active], coef[active])\\n\\t\\t\\t\\ttemp = np.dot(X.T[n_active], residual)\\n\\t\\t\\t\\tCov = np.r_[temp, Cov]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfor ii in idx:\\n\\t\\t\\t\\t\\tfor i in range(ii, n_active):\\n\\t\\t\\t\\t\\t\\tindices[i], indices[i + 1] = indices[i + 1], indices[i]\\n\\t\\t\\t\\t\\t\\tGram[i], Gram[i + 1] = swap(Gram[i], Gram[i+1])\\n\\t\\t\\t\\t\\t\\tGram[:, i], Gram[:, i + 1] = swap(Gram[:, i],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  Gram[:, i + 1])\\n\\t\\t\\t\\tresidual = y - np.dot(X, coef)\\n\\t\\t\\t\\ttemp = np.dot(X.T[drop_idx], residual)\\n\\t\\t\\t\\tCov = np.r_[temp, Cov]\\n\\t\\t\\tsign_active = np.delete(sign_active, idx)\\n\\t\\t\\tsign_active = np.append(sign_active, 0.)  \\n\\t\\t\\tif verbose > 1:\\n\\t\\t\\t\\tprint(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, '', drop_idx,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  n_active, abs(temp)))\\n\\tif return_path:\\n\\t\\talphas = alphas[:n_iter + 1]\\n\\t\\tcoefs = coefs[:n_iter + 1]\\n\\t\\tif return_n_iter:\\n\\t\\t\\treturn alphas, active, coefs.T, n_iter\\n\\t\\telse:\\n\\t\\t\\treturn alphas, active, coefs.T\\n\\telse:\\n\\t\\tif return_n_iter:\\n\\t\\t\\treturn alpha, active, coef, n_iter\\n\\t\\telse:\\n\\t\\t\\treturn alpha, active, coef",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX #3370: better lars alpha path inequality checks for 32 bit support",
    "fixed_code": "def lars_path(X, y, Xy=None, Gram=None, max_iter=500,\\n\\t\\t\\t  alpha_min=0, method='lar', copy_X=True,\\n\\t\\t\\t  eps=np.finfo(np.float).eps,\\n\\t\\t\\t  copy_Gram=True, verbose=0, return_path=True,\\n\\t\\t\\t  return_n_iter=False):\\n\\tn_features = X.shape[1]\\n\\tn_samples = y.size\\n\\tmax_features = min(max_iter, n_features)\\n\\tif return_path:\\n\\t\\tcoefs = np.zeros((max_features + 1, n_features))\\n\\t\\talphas = np.zeros(max_features + 1)\\n\\telse:\\n\\t\\tcoef, prev_coef = np.zeros(n_features), np.zeros(n_features)\\n\\t\\talpha, prev_alpha = np.array([0.]), np.array([0.])  \\n\\tn_iter, n_active = 0, 0\\n\\tactive, indices = list(), np.arange(n_features)\\n\\tsign_active = np.empty(max_features, dtype=np.int8)\\n\\tdrop = False\\n\\tL = np.zeros((max_features, max_features), dtype=X.dtype)\\n\\tswap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\\n\\tsolve_cholesky, = get_lapack_funcs(('potrs',), (X,))\\n\\tif Gram is None:\\n\\t\\tif copy_X:\\n\\t\\t\\tX = X.copy('F')\\n\\telif Gram == 'auto':\\n\\t\\tGram = None\\n\\t\\tif X.shape[0] > X.shape[1]:\\n\\t\\t\\tGram = np.dot(X.T, X)\\n\\telif copy_Gram:\\n\\t\\t\\tGram = Gram.copy()\\n\\tif Xy is None:\\n\\t\\tCov = np.dot(X.T, y)\\n\\telse:\\n\\t\\tCov = Xy.copy()\\n\\tif verbose:\\n\\t\\tif verbose > 1:\\n\\t\\t\\tprint(\"Step\\t\\tAdded\\t\\tDropped\\t\\tActive set size\\t\\tC\")\\n\\t\\telse:\\n\\t\\t\\tsys.stdout.write('.')\\n\\t\\t\\tsys.stdout.flush()\\n\\ttiny = np.finfo(np.float).tiny  \\n\\ttiny32 = np.finfo(np.float32).tiny  \\n\\tequality_tolerance = np.finfo(np.float32).eps\\n\\twhile True:\\n\\t\\tif Cov.size:\\n\\t\\t\\tC_idx = np.argmax(np.abs(Cov))\\n\\t\\t\\tC_ = Cov[C_idx]\\n\\t\\t\\tC = np.fabs(C_)\\n\\t\\telse:\\n\\t\\t\\tC = 0.\\n\\t\\tif return_path:\\n\\t\\t\\talpha = alphas[n_iter, np.newaxis]\\n\\t\\t\\tcoef = coefs[n_iter]\\n\\t\\t\\tprev_alpha = alphas[n_iter - 1, np.newaxis]\\n\\t\\t\\tprev_coef = coefs[n_iter - 1]\\n\\t\\talpha[0] = C / n_samples\\n\\t\\tif alpha[0] <= alpha_min + equality_tolerance:  \\n\\t\\t\\tif abs(alpha[0] - alpha_min) > equality_tolerance:\\n\\t\\t\\t\\tif n_iter > 0:\\n\\t\\t\\t\\t\\tss = ((prev_alpha[0] - alpha_min) /\\n\\t\\t\\t\\t\\t\\t  (prev_alpha[0] - alpha[0]))\\n\\t\\t\\t\\t\\tcoef[:] = prev_coef + ss * (coef - prev_coef)\\n\\t\\t\\t\\talpha[0] = alpha_min\\n\\t\\t\\tif return_path:\\n\\t\\t\\t\\tcoefs[n_iter] = coef\\n\\t\\t\\tbreak\\n\\t\\tif n_iter >= max_iter or n_active >= n_features:\\n\\t\\t\\tbreak\\n\\t\\tif not drop:\\n\\t\\t\\tsign_active[n_active] = np.sign(C_)\\n\\t\\t\\tm, n = n_active, C_idx + n_active\\n\\t\\t\\tCov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\\n\\t\\t\\tindices[n], indices[m] = indices[m], indices[n]\\n\\t\\t\\tCov_not_shortened = Cov\\n\\t\\t\\tCov = Cov[1:]  \\n\\t\\t\\tif Gram is None:\\n\\t\\t\\t\\tX.T[n], X.T[m] = swap(X.T[n], X.T[m])\\n\\t\\t\\t\\tc = nrm2(X.T[n_active]) ** 2\\n\\t\\t\\t\\tL[n_active, :n_active] = \\\\n\\t\\t\\t\\t\\tnp.dot(X.T[n_active], X.T[:n_active].T)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tGram[m], Gram[n] = swap(Gram[m], Gram[n])\\n\\t\\t\\t\\tGram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])\\n\\t\\t\\t\\tc = Gram[n_active, n_active]\\n\\t\\t\\t\\tL[n_active, :n_active] = Gram[n_active, :n_active]\\n\\t\\t\\tif n_active:\\n\\t\\t\\t\\tlinalg.solve_triangular(L[:n_active, :n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tL[n_active, :n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttrans=0, lower=1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toverwrite_b=True,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t**solve_triangular_args)\\n\\t\\t\\tv = np.dot(L[n_active, :n_active], L[n_active, :n_active])\\n\\t\\t\\tdiag = max(np.sqrt(np.abs(c - v)), eps)\\n\\t\\t\\tL[n_active, n_active] = diag\\n\\t\\t\\tif diag < 1e-7:\\n\\t\\t\\t\\twarnings.warn('Regressors in active set degenerate. '\\n\\t\\t\\t\\t\\t\\t\\t  'Dropping a regressor, after %i iterations, '\\n\\t\\t\\t\\t\\t\\t\\t  'i.e. alpha=%.3e, '\\n\\t\\t\\t\\t\\t\\t\\t  'with an active set of %i regressors, and '\\n\\t\\t\\t\\t\\t\\t\\t  'the smallest cholesky pivot element being %.3e'\\n\\t\\t\\t\\t\\t\\t\\t  % (n_iter, alpha, n_active, diag),\\n\\t\\t\\t\\t\\t\\t\\t  ConvergenceWarning)\\n\\t\\t\\t\\tCov = Cov_not_shortened\\n\\t\\t\\t\\tCov[0] = 0\\n\\t\\t\\t\\tCov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tactive.append(indices[n_active])\\n\\t\\t\\tn_active += 1\\n\\t\\t\\tif verbose > 1:\\n\\t\\t\\t\\tprint(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, active[-1], '',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  n_active, C))\\n\\t\\tif method == 'lasso' and n_iter > 0 and prev_alpha[0] < alpha[0]:\\n\\t\\t\\twarnings.warn('Early stopping the lars path, as the residues '\\n\\t\\t\\t\\t\\t\\t  'are small and the current value of alpha is no '\\n\\t\\t\\t\\t\\t\\t  'longer well controlled. %i iterations, alpha=%.3e, '\\n\\t\\t\\t\\t\\t\\t  'previous alpha=%.3e, with an active set of %i '\\n\\t\\t\\t\\t\\t\\t  'regressors.'\\n\\t\\t\\t\\t\\t\\t  % (n_iter, alpha, prev_alpha, n_active),\\n\\t\\t\\t\\t\\t\\t  ConvergenceWarning)\\n\\t\\t\\tbreak\\n\\t\\tleast_squares, info = solve_cholesky(L[:n_active, :n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t sign_active[:n_active],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t lower=True)\\n\\t\\tif least_squares.size == 1 and least_squares == 0:\\n\\t\\t\\tleast_squares[...] = 1\\n\\t\\t\\tAA = 1.\\n\\t\\telse:\\n\\t\\t\\tAA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))\\n\\t\\t\\tif not np.isfinite(AA):\\n\\t\\t\\t\\ti = 0\\n\\t\\t\\t\\tL_ = L[:n_active, :n_active].copy()\\n\\t\\t\\t\\twhile not np.isfinite(AA):\\n\\t\\t\\t\\t\\tL_.flat[::n_active + 1] += (2 ** i) * eps\\n\\t\\t\\t\\t\\tleast_squares, info = solve_cholesky(\\n\\t\\t\\t\\t\\t\\tL_, sign_active[:n_active], lower=True)\\n\\t\\t\\t\\t\\ttmp = max(np.sum(least_squares * sign_active[:n_active]),\\n\\t\\t\\t\\t\\t\\t\\t  eps)\\n\\t\\t\\t\\t\\tAA = 1. / np.sqrt(tmp)\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\tleast_squares *= AA\\n\\t\\tif Gram is None:\\n\\t\\t\\teq_dir = np.dot(X.T[:n_active].T, least_squares)\\n\\t\\t\\tcorr_eq_dir = np.dot(X.T[n_active:], eq_dir)\\n\\t\\telse:\\n\\t\\t\\tcorr_eq_dir = np.dot(Gram[:n_active, n_active:].T,\\n\\t\\t\\t\\t\\t\\t\\t\\t least_squares)\\n\\t\\tg1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\\n\\t\\tg2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\\n\\t\\tgamma_ = min(g1, g2, C / AA)\\n\\t\\tdrop = False\\n\\t\\tz = -coef[active] / (least_squares + tiny32)\\n\\t\\tz_pos = arrayfuncs.min_pos(z)\\n\\t\\tif z_pos < gamma_:\\n\\t\\t\\tidx = np.where(z == z_pos)[0][::-1]\\n\\t\\t\\tsign_active[idx] = -sign_active[idx]\\n\\t\\t\\tif method == 'lasso':\\n\\t\\t\\t\\tgamma_ = z_pos\\n\\t\\t\\tdrop = True\\n\\t\\tn_iter += 1\\n\\t\\tif return_path:\\n\\t\\t\\tif n_iter >= coefs.shape[0]:\\n\\t\\t\\t\\tdel coef, alpha, prev_alpha, prev_coef\\n\\t\\t\\t\\tadd_features = 2 * max(1, (max_features - n_active))\\n\\t\\t\\t\\tcoefs = np.resize(coefs, (n_iter + add_features, n_features))\\n\\t\\t\\t\\talphas = np.resize(alphas, n_iter + add_features)\\n\\t\\t\\tcoef = coefs[n_iter]\\n\\t\\t\\tprev_coef = coefs[n_iter - 1]\\n\\t\\t\\talpha = alphas[n_iter, np.newaxis]\\n\\t\\t\\tprev_alpha = alphas[n_iter - 1, np.newaxis]\\n\\t\\telse:\\n\\t\\t\\tprev_coef = coef\\n\\t\\t\\tprev_alpha[0] = alpha[0]\\n\\t\\t\\tcoef = np.zeros_like(coef)\\n\\t\\tcoef[active] = prev_coef[active] + gamma_ * least_squares\\n\\t\\tCov -= gamma_ * corr_eq_dir\\n\\t\\tif drop and method == 'lasso':\\n\\t\\t\\t[arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in\\n\\t\\t\\t\\tidx]\\n\\t\\t\\tn_active -= 1\\n\\t\\t\\tm, n = idx, n_active\\n\\t\\t\\tdrop_idx = [active.pop(ii) for ii in idx]\\n\\t\\t\\tif Gram is None:\\n\\t\\t\\t\\tfor ii in idx:\\n\\t\\t\\t\\t\\tfor i in range(ii, n_active):\\n\\t\\t\\t\\t\\t\\tX.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])\\n\\t\\t\\t\\t\\t\\tindices[i], indices[i + 1] = indices[i + 1], indices[i]\\n\\t\\t\\t\\tresidual = y - np.dot(X[:, :n_active], coef[active])\\n\\t\\t\\t\\ttemp = np.dot(X.T[n_active], residual)\\n\\t\\t\\t\\tCov = np.r_[temp, Cov]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfor ii in idx:\\n\\t\\t\\t\\t\\tfor i in range(ii, n_active):\\n\\t\\t\\t\\t\\t\\tindices[i], indices[i + 1] = indices[i + 1], indices[i]\\n\\t\\t\\t\\t\\t\\tGram[i], Gram[i + 1] = swap(Gram[i], Gram[i+1])\\n\\t\\t\\t\\t\\t\\tGram[:, i], Gram[:, i + 1] = swap(Gram[:, i],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  Gram[:, i + 1])\\n\\t\\t\\t\\tresidual = y - np.dot(X, coef)\\n\\t\\t\\t\\ttemp = np.dot(X.T[drop_idx], residual)\\n\\t\\t\\t\\tCov = np.r_[temp, Cov]\\n\\t\\t\\tsign_active = np.delete(sign_active, idx)\\n\\t\\t\\tsign_active = np.append(sign_active, 0.)  \\n\\t\\t\\tif verbose > 1:\\n\\t\\t\\t\\tprint(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, '', drop_idx,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  n_active, abs(temp)))\\n\\tif return_path:\\n\\t\\talphas = alphas[:n_iter + 1]\\n\\t\\tcoefs = coefs[:n_iter + 1]\\n\\t\\tif return_n_iter:\\n\\t\\t\\treturn alphas, active, coefs.T, n_iter\\n\\t\\telse:\\n\\t\\t\\treturn alphas, active, coefs.T\\n\\telse:\\n\\t\\tif return_n_iter:\\n\\t\\t\\treturn alpha, active, coef, n_iter\\n\\t\\telse:\\n\\t\\t\\treturn alpha, active, coef"
  },
  {
    "code": "def render(self, name, value, attrs=None):\\n        try:\\n            year_val, month_val, day_val = value.year, value.month, value.day\\n        except AttributeError:\\n            year_val = month_val = day_val = None\\n            if isinstance(value, six.string_types):\\n                if settings.USE_L10N:\\n                    try:\\n                        input_format = get_format('DATE_INPUT_FORMATS')[0]\\n                        v = datetime.datetime.strptime(force_str(value), input_format)\\n                        year_val, month_val, day_val = v.year, v.month, v.day\\n                    except ValueError:\\n                        pass\\n                else:\\n                    match = RE_DATE.match(value)\\n                    if match:\\n                        year_val, month_val, day_val = [int(v) for v in match.groups()]\\n        choices = [(i, i) for i in self.years]\\n        year_html = self.create_select(name, self.year_field, value, year_val, choices)\\n        choices = list(six.iteritems(MONTHS))\\n        month_html = self.create_select(name, self.month_field, value, month_val, choices)\\n        choices = [(i, i) for i in range(1, 32)]\\n        day_html = self.create_select(name, self.day_field, value, day_val,  choices)\\n        output = []\\n        for field in _parse_date_fmt():\\n            if field == 'year':\\n                output.append(year_html)\\n            elif field == 'month':\\n                output.append(month_html)\\n            elif field == 'day':\\n                output.append(day_html)\\n        return mark_safe('\\n'.join(output))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #20986 -- Enabled SelectDateWidget to use custom months\\n\\nReviewed by Trac alias MarkusH.",
    "fixed_code": "def render(self, name, value, attrs=None):\\n        try:\\n            year_val, month_val, day_val = value.year, value.month, value.day\\n        except AttributeError:\\n            year_val = month_val = day_val = None\\n            if isinstance(value, six.string_types):\\n                if settings.USE_L10N:\\n                    try:\\n                        input_format = get_format('DATE_INPUT_FORMATS')[0]\\n                        v = datetime.datetime.strptime(force_str(value), input_format)\\n                        year_val, month_val, day_val = v.year, v.month, v.day\\n                    except ValueError:\\n                        pass\\n                else:\\n                    match = RE_DATE.match(value)\\n                    if match:\\n                        year_val, month_val, day_val = [int(v) for v in match.groups()]\\n        choices = [(i, i) for i in self.years]\\n        year_html = self.create_select(name, self.year_field, value, year_val, choices)\\n        choices = list(six.iteritems(self.months))\\n        month_html = self.create_select(name, self.month_field, value, month_val, choices)\\n        choices = [(i, i) for i in range(1, 32)]\\n        day_html = self.create_select(name, self.day_field, value, day_val,  choices)\\n        output = []\\n        for field in _parse_date_fmt():\\n            if field == 'year':\\n                output.append(year_html)\\n            elif field == 'month':\\n                output.append(month_html)\\n            elif field == 'day':\\n                output.append(day_html)\\n        return mark_safe('\\n'.join(output))"
  },
  {
    "code": "def asof(self, where, subset=None):\\n\\t\\tif isinstance(where, str):\\n\\t\\t\\twhere = Timestamp(where)\\n\\t\\tif not self.index.is_monotonic:\\n\\t\\t\\traise ValueError(\"asof requires a sorted index\")\\n\\t\\tis_series = isinstance(self, ABCSeries)\\n\\t\\tif is_series:\\n\\t\\t\\tif subset is not None:\\n\\t\\t\\t\\traise ValueError(\"subset is not valid for Series\")\\n\\t\\telse:\\n\\t\\t\\tif subset is None:\\n\\t\\t\\t\\tsubset = self.columns\\n\\t\\t\\tif not is_list_like(subset):\\n\\t\\t\\t\\tsubset = [subset]\\n\\t\\tis_list = is_list_like(where)\\n\\t\\tif not is_list:\\n\\t\\t\\tstart = self.index[0]\\n\\t\\t\\tif isinstance(self.index, PeriodIndex):\\n\\t\\t\\t\\twhere = Period(where, freq=self.index.freq)\\n\\t\\t\\tif where < start:\\n\\t\\t\\t\\tif not is_series:\\n\\t\\t\\t\\t\\tfrom pandas import Series\\n\\t\\t\\t\\t\\treturn Series(index=self.columns, name=where, dtype=np.float64)\\n\\t\\t\\t\\treturn np.nan\\n\\t\\t\\tif is_series:\\n\\t\\t\\t\\tloc = self.index.searchsorted(where, side=\"right\")\\n\\t\\t\\t\\tif loc > 0:\\n\\t\\t\\t\\t\\tloc -= 1\\n\\t\\t\\t\\tvalues = self._values\\n\\t\\t\\t\\twhile loc > 0 and isna(values[loc]):\\n\\t\\t\\t\\t\\tloc -= 1\\n\\t\\t\\t\\treturn values[loc]\\n\\t\\tif not isinstance(where, Index):\\n\\t\\t\\twhere = Index(where) if is_list else Index([where])\\n\\t\\tnulls = self.isna() if is_series else self[subset].isna().any(1)\\n\\t\\tif nulls.all():\\n\\t\\t\\tif is_series:\\n\\t\\t\\t\\treturn self._constructor(np.nan, index=where, name=self.name)\\n\\t\\t\\telif is_list:\\n\\t\\t\\t\\tfrom pandas import DataFrame\\n\\t\\t\\t\\treturn DataFrame(np.nan, index=where, columns=self.columns)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfrom pandas import Series\\n\\t\\t\\t\\treturn Series(np.nan, index=self.columns, name=where[0])\\n\\t\\tlocs = self.index.asof_locs(where, ~(nulls.values))\\n\\t\\tmissing = locs == -1\\n\\t\\td = self.take(locs)\\n\\t\\tdata = d.copy()\\n\\t\\tdata.index = where\\n\\t\\tdata.loc[missing] = np.nan\\n\\t\\treturn data if is_list else data.iloc[-1]\\n\\t_shared_docs[\\n\\t\\t\"isna\"\\n\\t] = \\n\\t@Appender(_shared_docs[\"isna\"] % _shared_doc_kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def asof(self, where, subset=None):\\n\\t\\tif isinstance(where, str):\\n\\t\\t\\twhere = Timestamp(where)\\n\\t\\tif not self.index.is_monotonic:\\n\\t\\t\\traise ValueError(\"asof requires a sorted index\")\\n\\t\\tis_series = isinstance(self, ABCSeries)\\n\\t\\tif is_series:\\n\\t\\t\\tif subset is not None:\\n\\t\\t\\t\\traise ValueError(\"subset is not valid for Series\")\\n\\t\\telse:\\n\\t\\t\\tif subset is None:\\n\\t\\t\\t\\tsubset = self.columns\\n\\t\\t\\tif not is_list_like(subset):\\n\\t\\t\\t\\tsubset = [subset]\\n\\t\\tis_list = is_list_like(where)\\n\\t\\tif not is_list:\\n\\t\\t\\tstart = self.index[0]\\n\\t\\t\\tif isinstance(self.index, PeriodIndex):\\n\\t\\t\\t\\twhere = Period(where, freq=self.index.freq)\\n\\t\\t\\tif where < start:\\n\\t\\t\\t\\tif not is_series:\\n\\t\\t\\t\\t\\tfrom pandas import Series\\n\\t\\t\\t\\t\\treturn Series(index=self.columns, name=where, dtype=np.float64)\\n\\t\\t\\t\\treturn np.nan\\n\\t\\t\\tif is_series:\\n\\t\\t\\t\\tloc = self.index.searchsorted(where, side=\"right\")\\n\\t\\t\\t\\tif loc > 0:\\n\\t\\t\\t\\t\\tloc -= 1\\n\\t\\t\\t\\tvalues = self._values\\n\\t\\t\\t\\twhile loc > 0 and isna(values[loc]):\\n\\t\\t\\t\\t\\tloc -= 1\\n\\t\\t\\t\\treturn values[loc]\\n\\t\\tif not isinstance(where, Index):\\n\\t\\t\\twhere = Index(where) if is_list else Index([where])\\n\\t\\tnulls = self.isna() if is_series else self[subset].isna().any(1)\\n\\t\\tif nulls.all():\\n\\t\\t\\tif is_series:\\n\\t\\t\\t\\treturn self._constructor(np.nan, index=where, name=self.name)\\n\\t\\t\\telif is_list:\\n\\t\\t\\t\\tfrom pandas import DataFrame\\n\\t\\t\\t\\treturn DataFrame(np.nan, index=where, columns=self.columns)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfrom pandas import Series\\n\\t\\t\\t\\treturn Series(np.nan, index=self.columns, name=where[0])\\n\\t\\tlocs = self.index.asof_locs(where, ~(nulls.values))\\n\\t\\tmissing = locs == -1\\n\\t\\td = self.take(locs)\\n\\t\\tdata = d.copy()\\n\\t\\tdata.index = where\\n\\t\\tdata.loc[missing] = np.nan\\n\\t\\treturn data if is_list else data.iloc[-1]\\n\\t_shared_docs[\\n\\t\\t\"isna\"\\n\\t] = \\n\\t@Appender(_shared_docs[\"isna\"] % _shared_doc_kwargs)"
  },
  {
    "code": "def find_commands(management_dir):\\n    command_dir = os.path.join(management_dir, 'commands')\\n    sys.path_importer_cache.pop(command_dir, None)\\n    return [name for _, name, is_pkg in pkgutil.iter_modules([command_dir])\\n            if not is_pkg and not name.startswith('_')]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Removed compatibility with Python 3.2.",
    "fixed_code": "def find_commands(management_dir):\\n    command_dir = os.path.join(management_dir, 'commands')\\n    return [name for _, name, is_pkg in pkgutil.iter_modules([command_dir])\\n            if not is_pkg and not name.startswith('_')]"
  },
  {
    "code": "def read_fwf(\\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\\n    colspecs: Sequence[tuple[int, int]] | str | None = \"infer\",\\n    widths: Sequence[int] | None = None,\\n    infer_nrows: int = 100,\\n    **kwds,\\n) -> DataFrame | TextFileReader:\\n    r\\n    if colspecs is None and widths is None:\\n        raise ValueError(\"Must specify either colspecs or widths\")\\n    elif colspecs not in (None, \"infer\") and widths is not None:\\n        raise ValueError(\"You must specify only one of 'widths' and 'colspecs'\")\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append((col, col + w))\\n            col += w\\n    assert colspecs is not None\\n    names = kwds.get(\"names\")\\n    if names is not None:\\n        if len(names) != len(colspecs) and colspecs != \"infer\":\\n            len_index = 0\\n            if kwds.get(\"index_col\") is not None:\\n                index_col: Any = kwds.get(\"index_col\")\\n                if index_col is not False:\\n                    if not is_list_like(index_col):\\n                        len_index = 1\\n                    else:\\n                        len_index = len(index_col)\\n            if kwds.get(\"usecols\") is None and len(names) + len_index != len(colspecs):\\n                raise ValueError(\"Length of colspecs must match length of names\")\\n    kwds[\"colspecs\"] = colspecs\\n    kwds[\"infer_nrows\"] = infer_nrows\\n    kwds[\"engine\"] = \"python-fwf\"\\n    return _read(filepath_or_buffer, kwds)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_fwf(\\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\\n    colspecs: Sequence[tuple[int, int]] | str | None = \"infer\",\\n    widths: Sequence[int] | None = None,\\n    infer_nrows: int = 100,\\n    **kwds,\\n) -> DataFrame | TextFileReader:\\n    r\\n    if colspecs is None and widths is None:\\n        raise ValueError(\"Must specify either colspecs or widths\")\\n    elif colspecs not in (None, \"infer\") and widths is not None:\\n        raise ValueError(\"You must specify only one of 'widths' and 'colspecs'\")\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append((col, col + w))\\n            col += w\\n    assert colspecs is not None\\n    names = kwds.get(\"names\")\\n    if names is not None:\\n        if len(names) != len(colspecs) and colspecs != \"infer\":\\n            len_index = 0\\n            if kwds.get(\"index_col\") is not None:\\n                index_col: Any = kwds.get(\"index_col\")\\n                if index_col is not False:\\n                    if not is_list_like(index_col):\\n                        len_index = 1\\n                    else:\\n                        len_index = len(index_col)\\n            if kwds.get(\"usecols\") is None and len(names) + len_index != len(colspecs):\\n                raise ValueError(\"Length of colspecs must match length of names\")\\n    kwds[\"colspecs\"] = colspecs\\n    kwds[\"infer_nrows\"] = infer_nrows\\n    kwds[\"engine\"] = \"python-fwf\"\\n    return _read(filepath_or_buffer, kwds)"
  },
  {
    "code": "def setup(self, use_numexpr, threads):\\n        self.df = DataFrame(np.random.randn(20000, 100))\\n        self.df2 = DataFrame(np.random.randn(20000, 100))\\n        if threads != \"default\":\\n            expr.set_numexpr_threads(threads)\\n        if not use_numexpr:\\n            expr.set_use_numexpr(False)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PERF: implement scalar ops blockwise (#29853)",
    "fixed_code": "def setup(self, dtype, scalar, op):\\n        arr = np.random.randn(20000, 100)\\n        self.df = DataFrame(arr.astype(dtype))"
  },
  {
    "code": "def f(self, other, axis=default_axis, level=None, fill_value=None):\\n\\t\\tif _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\\n\\t\\t\\treturn _frame_arith_method_with_reindex(self, other, op)\\n\\t\\tself, other = _align_method_FRAME(self, other, axis, flex=True, level=level)\\n\\t\\tif isinstance(other, ABCDataFrame):\\n\\t\\t\\tpass_op = op if should_series_dispatch(self, other, op) else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tnew_data = self._combine_frame(other, pass_op, fill_value)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\t\\telif isinstance(other, ABCSeries):\\n\\t\\t\\tpass_op = op if axis in [0, \"columns\", None] else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\traise NotImplementedError(f\"fill_value {fill_value} not supported.\")\\n\\t\\t\\taxis = self._get_axis_number(axis) if axis is not None else 1\\n\\t\\t\\treturn _combine_series_frame(\\n\\t\\t\\t\\tself, other, pass_op, axis=axis, str_rep=str_rep\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\tself = self.fillna(fill_value)\\n\\t\\t\\tnew_data = dispatch_to_series(self, other, op, str_rep)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\tf.__name__ = op_name\\n\\treturn f",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: arithmetic with reindex pow (#32734)",
    "fixed_code": "def f(self, other, axis=default_axis, level=None, fill_value=None):\\n\\t\\tif _should_reindex_frame_op(\\n\\t\\t\\tself, other, op, axis, default_axis, fill_value, level\\n\\t\\t):\\n\\t\\t\\treturn _frame_arith_method_with_reindex(self, other, op)\\n\\t\\tself, other = _align_method_FRAME(self, other, axis, flex=True, level=level)\\n\\t\\tif isinstance(other, ABCDataFrame):\\n\\t\\t\\tpass_op = op if should_series_dispatch(self, other, op) else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tnew_data = self._combine_frame(other, pass_op, fill_value)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\t\\telif isinstance(other, ABCSeries):\\n\\t\\t\\tpass_op = op if axis in [0, \"columns\", None] else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\traise NotImplementedError(f\"fill_value {fill_value} not supported.\")\\n\\t\\t\\taxis = self._get_axis_number(axis) if axis is not None else 1\\n\\t\\t\\treturn _combine_series_frame(\\n\\t\\t\\t\\tself, other, pass_op, axis=axis, str_rep=str_rep\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\tself = self.fillna(fill_value)\\n\\t\\t\\tnew_data = dispatch_to_series(self, other, op, str_rep)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\tf.__name__ = op_name\\n\\treturn f"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            group_id=dict(required=True),\\n            artifact_id=dict(required=True),\\n            version=dict(default=None),\\n            version_by_spec=dict(default=None),\\n            classifier=dict(default=''),\\n            extension=dict(default='jar'),\\n            repository_url=dict(default='http://repo1.maven.org/maven2'),\\n            username=dict(default=None, aliases=['aws_secret_key']),\\n            password=dict(default=None, no_log=True, aliases=['aws_secret_access_key']),\\n            headers=dict(type='dict'),\\n            force_basic_auth=dict(default=False, type='bool'),\\n            state=dict(default=\"present\", choices=[\"present\", \"absent\"]),  \\n            timeout=dict(default=10, type='int'),\\n            dest=dict(type=\"path\", required=True),\\n            validate_certs=dict(required=False, default=True, type='bool'),\\n            keep_name=dict(required=False, default=False, type='bool'),\\n            verify_checksum=dict(required=False, default='download', choices=['never', 'download', 'change', 'always'])\\n        ),\\n        add_file_common_args=True,\\n        mutually_exclusive=([('version', 'version_by_spec')])\\n    )\\n    if not HAS_LXML_ETREE:\\n        module.fail_json(msg=missing_required_lib('lxml'), exception=LXML_ETREE_IMP_ERR)\\n    if module.params['version_by_spec'] and not HAS_SEMANTIC_VERSION:\\n        module.fail_json(msg=missing_required_lib('semantic_version'), exception=SEMANTIC_VERSION_IMP_ERR)\\n    repository_url = module.params[\"repository_url\"]\\n    if not repository_url:\\n        repository_url = \"http://repo1.maven.org/maven2\"\\n    try:\\n        parsed_url = urlparse(repository_url)\\n    except AttributeError as e:\\n        module.fail_json(msg='url parsing went wrong %s' % e)\\n    local = parsed_url.scheme == \"file\"\\n    if parsed_url.scheme == 's3' and not HAS_BOTO:\\n        module.fail_json(msg=missing_required_lib('boto3', reason='when using s3:// repository URLs'),\\n                         exception=BOTO_IMP_ERR)\\n    group_id = module.params[\"group_id\"]\\n    artifact_id = module.params[\"artifact_id\"]\\n    version = module.params[\"version\"]\\n    version_by_spec = module.params[\"version_by_spec\"]\\n    classifier = module.params[\"classifier\"]\\n    extension = module.params[\"extension\"]\\n    headers = module.params['headers']\\n    state = module.params[\"state\"]\\n    dest = module.params[\"dest\"]\\n    b_dest = to_bytes(dest, errors='surrogate_or_strict')\\n    keep_name = module.params[\"keep_name\"]\\n    verify_checksum = module.params[\"verify_checksum\"]\\n    verify_download = verify_checksum in ['download', 'always']\\n    verify_change = verify_checksum in ['change', 'always']\\n    downloader = MavenDownloader(module, repository_url, local, headers)\\n    if not version_by_spec and not version:\\n        version = \"latest\"\\n    try:\\n        artifact = Artifact(group_id, artifact_id, version, version_by_spec, classifier, extension)\\n    except ValueError as e:\\n        module.fail_json(msg=e.args[0])\\n    changed = False\\n    prev_state = \"absent\"\\n    if dest.endswith(os.sep):\\n        b_dest = to_bytes(dest, errors='surrogate_or_strict')\\n        if not os.path.exists(b_dest):\\n            (pre_existing_dir, new_directory_list) = split_pre_existing_dir(dest)\\n            os.makedirs(b_dest)\\n            directory_args = module.load_file_common_arguments(module.params)\\n            directory_mode = module.params[\"directory_mode\"]\\n            if directory_mode is not None:\\n                directory_args['mode'] = directory_mode\\n            else:\\n                directory_args['mode'] = None\\n            changed = adjust_recursive_directory_permissions(pre_existing_dir, new_directory_list, module, directory_args, changed)\\n    if os.path.isdir(b_dest):\\n        version_part = version\\n        if version == 'latest':\\n            version_part = downloader.find_latest_version_available(artifact)\\n        elif version_by_spec:\\n            version_part = downloader.find_version_by_spec(artifact)\\n        filename = \"{artifact_id}{version_part}{classifier}.{extension}\".format(\\n            artifact_id=artifact_id,\\n            version_part=\"-{0}\".format(version_part) if keep_name else \"\",\\n            classifier=\"-{0}\".format(classifier) if classifier else \"\",\\n            extension=extension\\n        )\\n        dest = posixpath.join(dest, filename)\\n        b_dest = to_bytes(dest, errors='surrogate_or_strict')\\n    if os.path.lexists(b_dest) and ((not verify_change) or not downloader.is_invalid_md5(dest, downloader.find_uri_for_artifact(artifact))):\\n        prev_state = \"present\"\\n    if prev_state == \"absent\":\\n        try:\\n            download_error = downloader.download(module.tmpdir, artifact, verify_download, b_dest)\\n            if download_error is None:\\n                changed = True\\n            else:\\n                module.fail_json(msg=\"Cannot retrieve the artifact to destination: \" + download_error)\\n        except ValueError as e:\\n            module.fail_json(msg=e.args[0])\\n    module.params['dest'] = dest\\n    file_args = module.load_file_common_arguments(module.params)\\n    changed = module.set_fs_attributes_if_different(file_args, changed)\\n    if changed:\\n        module.exit_json(state=state, dest=dest, group_id=group_id, artifact_id=artifact_id, version=version, classifier=classifier,\\n                         extension=extension, repository_url=repository_url, changed=changed)\\n    else:\\n        module.exit_json(state=state, dest=dest, changed=changed)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Maven Central requires TLS (#66611)\\n\\nFixes: #66609\\nMaven requires TLS as of 2019-01-15",
    "fixed_code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            group_id=dict(required=True),\\n            artifact_id=dict(required=True),\\n            version=dict(default=None),\\n            version_by_spec=dict(default=None),\\n            classifier=dict(default=''),\\n            extension=dict(default='jar'),\\n            repository_url=dict(default='https://repo1.maven.org/maven2'),\\n            username=dict(default=None, aliases=['aws_secret_key']),\\n            password=dict(default=None, no_log=True, aliases=['aws_secret_access_key']),\\n            headers=dict(type='dict'),\\n            force_basic_auth=dict(default=False, type='bool'),\\n            state=dict(default=\"present\", choices=[\"present\", \"absent\"]),  \\n            timeout=dict(default=10, type='int'),\\n            dest=dict(type=\"path\", required=True),\\n            validate_certs=dict(required=False, default=True, type='bool'),\\n            keep_name=dict(required=False, default=False, type='bool'),\\n            verify_checksum=dict(required=False, default='download', choices=['never', 'download', 'change', 'always'])\\n        ),\\n        add_file_common_args=True,\\n        mutually_exclusive=([('version', 'version_by_spec')])\\n    )\\n    if not HAS_LXML_ETREE:\\n        module.fail_json(msg=missing_required_lib('lxml'), exception=LXML_ETREE_IMP_ERR)\\n    if module.params['version_by_spec'] and not HAS_SEMANTIC_VERSION:\\n        module.fail_json(msg=missing_required_lib('semantic_version'), exception=SEMANTIC_VERSION_IMP_ERR)\\n    repository_url = module.params[\"repository_url\"]\\n    if not repository_url:\\n        repository_url = \"https://repo1.maven.org/maven2\"\\n    try:\\n        parsed_url = urlparse(repository_url)\\n    except AttributeError as e:\\n        module.fail_json(msg='url parsing went wrong %s' % e)\\n    local = parsed_url.scheme == \"file\"\\n    if parsed_url.scheme == 's3' and not HAS_BOTO:\\n        module.fail_json(msg=missing_required_lib('boto3', reason='when using s3:// repository URLs'),\\n                         exception=BOTO_IMP_ERR)\\n    group_id = module.params[\"group_id\"]\\n    artifact_id = module.params[\"artifact_id\"]\\n    version = module.params[\"version\"]\\n    version_by_spec = module.params[\"version_by_spec\"]\\n    classifier = module.params[\"classifier\"]\\n    extension = module.params[\"extension\"]\\n    headers = module.params['headers']\\n    state = module.params[\"state\"]\\n    dest = module.params[\"dest\"]\\n    b_dest = to_bytes(dest, errors='surrogate_or_strict')\\n    keep_name = module.params[\"keep_name\"]\\n    verify_checksum = module.params[\"verify_checksum\"]\\n    verify_download = verify_checksum in ['download', 'always']\\n    verify_change = verify_checksum in ['change', 'always']\\n    downloader = MavenDownloader(module, repository_url, local, headers)\\n    if not version_by_spec and not version:\\n        version = \"latest\"\\n    try:\\n        artifact = Artifact(group_id, artifact_id, version, version_by_spec, classifier, extension)\\n    except ValueError as e:\\n        module.fail_json(msg=e.args[0])\\n    changed = False\\n    prev_state = \"absent\"\\n    if dest.endswith(os.sep):\\n        b_dest = to_bytes(dest, errors='surrogate_or_strict')\\n        if not os.path.exists(b_dest):\\n            (pre_existing_dir, new_directory_list) = split_pre_existing_dir(dest)\\n            os.makedirs(b_dest)\\n            directory_args = module.load_file_common_arguments(module.params)\\n            directory_mode = module.params[\"directory_mode\"]\\n            if directory_mode is not None:\\n                directory_args['mode'] = directory_mode\\n            else:\\n                directory_args['mode'] = None\\n            changed = adjust_recursive_directory_permissions(pre_existing_dir, new_directory_list, module, directory_args, changed)\\n    if os.path.isdir(b_dest):\\n        version_part = version\\n        if version == 'latest':\\n            version_part = downloader.find_latest_version_available(artifact)\\n        elif version_by_spec:\\n            version_part = downloader.find_version_by_spec(artifact)\\n        filename = \"{artifact_id}{version_part}{classifier}.{extension}\".format(\\n            artifact_id=artifact_id,\\n            version_part=\"-{0}\".format(version_part) if keep_name else \"\",\\n            classifier=\"-{0}\".format(classifier) if classifier else \"\",\\n            extension=extension\\n        )\\n        dest = posixpath.join(dest, filename)\\n        b_dest = to_bytes(dest, errors='surrogate_or_strict')\\n    if os.path.lexists(b_dest) and ((not verify_change) or not downloader.is_invalid_md5(dest, downloader.find_uri_for_artifact(artifact))):\\n        prev_state = \"present\"\\n    if prev_state == \"absent\":\\n        try:\\n            download_error = downloader.download(module.tmpdir, artifact, verify_download, b_dest)\\n            if download_error is None:\\n                changed = True\\n            else:\\n                module.fail_json(msg=\"Cannot retrieve the artifact to destination: \" + download_error)\\n        except ValueError as e:\\n            module.fail_json(msg=e.args[0])\\n    module.params['dest'] = dest\\n    file_args = module.load_file_common_arguments(module.params)\\n    changed = module.set_fs_attributes_if_different(file_args, changed)\\n    if changed:\\n        module.exit_json(state=state, dest=dest, group_id=group_id, artifact_id=artifact_id, version=version, classifier=classifier,\\n                         extension=extension, repository_url=repository_url, changed=changed)\\n    else:\\n        module.exit_json(state=state, dest=dest, changed=changed)"
  },
  {
    "code": "def __getstate__(self):\\n        d = {attr: getattr(self, attr) for attr in self.__slots__}\\n        d['is_safe'] = d['is_safe'].value\\n        return d",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-30977: rework code changes according to post-merge code review (GH-9106)\\n\\nalso mention the change and its consequences in What's New",
    "fixed_code": "def __getstate__(self):\\n        d = {'int': self.int}\\n        if self.is_safe != SafeUUID.unknown:\\n            d['is_safe'] = self.is_safe.value\\n        return d"
  },
  {
    "code": "def fit(self, X, y=None):\\n        X = check_array(X)\\n        self.cluster_centers_, self.labels_ = \\\\n            mean_shift(X, bandwidth=self.bandwidth, seeds=self.seeds,\\n                       min_bin_freq=self.min_bin_freq,\\n                       bin_seeding=self.bin_seeding,\\n                       cluster_all=self.cluster_all, n_jobs=self.n_jobs)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, y=None):\\n        X = check_array(X)\\n        self.cluster_centers_, self.labels_ = \\\\n            mean_shift(X, bandwidth=self.bandwidth, seeds=self.seeds,\\n                       min_bin_freq=self.min_bin_freq,\\n                       bin_seeding=self.bin_seeding,\\n                       cluster_all=self.cluster_all, n_jobs=self.n_jobs)\\n        return self"
  },
  {
    "code": "def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\\n        max_allowed = 1\\n        if current_line.depth == 0:\\n            max_allowed = 2\\n        if current_line.leaves:\\n            first_leaf = current_line.leaves[0]\\n            before = first_leaf.prefix.count(\"\\n\")\\n            before = min(before, max_allowed)\\n            first_leaf.prefix = \"\"\\n        else:\\n            before = 0\\n        depth = current_line.depth\\n        while self.previous_defs and self.previous_defs[-1] >= depth:\\n            self.previous_defs.pop()\\n            before = 1 if depth else 2\\n        is_decorator = current_line.is_decorator\\n        if is_decorator or current_line.is_def or current_line.is_class:\\n            if not is_decorator:\\n                self.previous_defs.append(depth)\\n            if self.previous_line is None:\\n                return 0, 0\\n            if self.previous_line.is_decorator:\\n                return 0, 0\\n            if (\\n                self.previous_line.is_comment\\n                and self.previous_line.depth == current_line.depth\\n                and before == 0\\n            ):\\n                return 0, 0\\n            newlines = 2\\n            if current_line.depth:\\n                newlines -= 1\\n            return newlines, 0\\n        if (\\n            self.previous_line\\n            and self.previous_line.is_import\\n            and not current_line.is_import\\n            and depth == self.previous_line.depth\\n        ):\\n            return (before or 1), 0\\n        return before, 0",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add support for pyi files (#210)\\n\\nFixes #207",
    "fixed_code": "def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\\n        max_allowed = 1\\n        if current_line.depth == 0:\\n            max_allowed = 1 if self.is_pyi else 2\\n        if current_line.leaves:\\n            first_leaf = current_line.leaves[0]\\n            before = first_leaf.prefix.count(\"\\n\")\\n            before = min(before, max_allowed)\\n            first_leaf.prefix = \"\"\\n        else:\\n            before = 0\\n        depth = current_line.depth\\n        while self.previous_defs and self.previous_defs[-1] >= depth:\\n            self.previous_defs.pop()\\n            if self.is_pyi:\\n                before = 0 if depth else 1\\n            else:\\n                before = 1 if depth else 2\\n        is_decorator = current_line.is_decorator\\n        if is_decorator or current_line.is_def or current_line.is_class:\\n            if not is_decorator:\\n                self.previous_defs.append(depth)\\n            if self.previous_line is None:\\n                return 0, 0\\n            if self.previous_line.is_decorator:\\n                return 0, 0\\n            if (\\n                self.previous_line.is_comment\\n                and self.previous_line.depth == current_line.depth\\n                and before == 0\\n            ):\\n                return 0, 0\\n            if self.is_pyi:\\n                if self.previous_line.depth > current_line.depth:\\n                    newlines = 1\\n                elif current_line.is_class or self.previous_line.is_class:\\n                    if (\\n                        current_line.is_trivial_class\\n                        and self.previous_line.is_trivial_class\\n                    ):\\n                        newlines = 0\\n                    else:\\n                        newlines = 1\\n                else:\\n                    newlines = 0\\n            else:\\n                newlines = 2\\n            if current_line.depth and newlines:\\n                newlines -= 1\\n            return newlines, 0\\n        if (\\n            self.previous_line\\n            and self.previous_line.is_import\\n            and not current_line.is_import\\n            and depth == self.previous_line.depth\\n        ):\\n            return (before or 1), 0\\n        return before, 0"
  },
  {
    "code": "def latest(self, field_name=None):\\n        return self._earliest_or_latest(field_name=field_name, direction=\"-\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #11557 -- Added support for a list of fields in Meta.get_latest_by and QuerySet.earliest()/latest().",
    "fixed_code": "def latest(self, *fields, field_name=None):\\n        return self.reverse()._earliest_or_latest(*fields, field_name=field_name)"
  },
  {
    "code": "def read_excel(\\n    io,\\n    sheet_name=0,\\n    header=0,\\n    names=None,\\n    index_col=None,\\n    usecols=None,\\n    squeeze=False,\\n    dtype=None,\\n    engine=None,\\n    converters=None,\\n    true_values=None,\\n    false_values=None,\\n    skiprows=None,\\n    nrows=None,\\n    na_values=None,\\n    keep_default_na=True,\\n    verbose=False,\\n    parse_dates=False,\\n    date_parser=None,\\n    thousands=None,\\n    comment=None,\\n    skipfooter=0,\\n    convert_float=True,\\n    mangle_dupe_cols=True,\\n    **kwds,\\n):\\n    for arg in (\"sheet\", \"sheetname\", \"parse_cols\"):\\n        if arg in kwds:\\n            raise TypeError(\\n                \"read_excel() got an unexpected keyword argument `{}`\".format(arg)\\n            )\\n    if not isinstance(io, ExcelFile):\\n        io = ExcelFile(io, engine=engine)\\n    elif engine and engine != io.engine:\\n        raise ValueError(\\n            \"Engine should not be specified when passing \"\\n            \"an ExcelFile - ExcelFile already has the engine set\"\\n        )\\n    return io.parse(\\n        sheet_name=sheet_name,\\n        header=header,\\n        names=names,\\n        index_col=index_col,\\n        usecols=usecols,\\n        squeeze=squeeze,\\n        dtype=dtype,\\n        converters=converters,\\n        true_values=true_values,\\n        false_values=false_values,\\n        skiprows=skiprows,\\n        nrows=nrows,\\n        na_values=na_values,\\n        keep_default_na=keep_default_na,\\n        verbose=verbose,\\n        parse_dates=parse_dates,\\n        date_parser=date_parser,\\n        thousands=thousands,\\n        comment=comment,\\n        skipfooter=skipfooter,\\n        convert_float=convert_float,\\n        mangle_dupe_cols=mangle_dupe_cols,\\n        **kwds,\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: Use fstring instead of .format in io/excel and test/generic (#30601)",
    "fixed_code": "def read_excel(\\n    io,\\n    sheet_name=0,\\n    header=0,\\n    names=None,\\n    index_col=None,\\n    usecols=None,\\n    squeeze=False,\\n    dtype=None,\\n    engine=None,\\n    converters=None,\\n    true_values=None,\\n    false_values=None,\\n    skiprows=None,\\n    nrows=None,\\n    na_values=None,\\n    keep_default_na=True,\\n    verbose=False,\\n    parse_dates=False,\\n    date_parser=None,\\n    thousands=None,\\n    comment=None,\\n    skipfooter=0,\\n    convert_float=True,\\n    mangle_dupe_cols=True,\\n    **kwds,\\n):\\n    for arg in (\"sheet\", \"sheetname\", \"parse_cols\"):\\n        if arg in kwds:\\n            raise TypeError(f\"read_excel() got an unexpected keyword argument `{arg}`\")\\n    if not isinstance(io, ExcelFile):\\n        io = ExcelFile(io, engine=engine)\\n    elif engine and engine != io.engine:\\n        raise ValueError(\\n            \"Engine should not be specified when passing \"\\n            \"an ExcelFile - ExcelFile already has the engine set\"\\n        )\\n    return io.parse(\\n        sheet_name=sheet_name,\\n        header=header,\\n        names=names,\\n        index_col=index_col,\\n        usecols=usecols,\\n        squeeze=squeeze,\\n        dtype=dtype,\\n        converters=converters,\\n        true_values=true_values,\\n        false_values=false_values,\\n        skiprows=skiprows,\\n        nrows=nrows,\\n        na_values=na_values,\\n        keep_default_na=keep_default_na,\\n        verbose=verbose,\\n        parse_dates=parse_dates,\\n        date_parser=date_parser,\\n        thousands=thousands,\\n        comment=comment,\\n        skipfooter=skipfooter,\\n        convert_float=convert_float,\\n        mangle_dupe_cols=mangle_dupe_cols,\\n        **kwds,\\n    )"
  },
  {
    "code": "def _deprecate(self, attr: str) -> None:\\n        warnings.warn(\\n            f\"{attr} is not part of the public API, usage can give in unexpected \"\\n            \"results and will be removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: ExcelWriter.book not settable (#48943)",
    "fixed_code": "def _deprecate(self, attr: str) -> None:\\n        warnings.warn(\\n            f\"{attr} is not part of the public API, usage can give unexpected \"\\n            \"results and will be removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    def _deprecate_set_book(self) -> None:\\n        warnings.warn(\\n            \"Setting the `book` attribute is not part of the public API, \"\\n            \"usage can give unexpected or corrupted results and will be \"\\n            \"removed in a future version\",\\n            FutureWarning,\\n            stacklevel=find_stack_level(inspect.currentframe()),\\n        )\\n    @property"
  },
  {
    "code": "def bracket_split_build_line(\\n\\tleaves: List[Leaf], original: Line, opening_bracket: Leaf, *, is_body: bool = False\\n) -> Line:\\n\\tresult = Line(depth=original.depth)\\n\\tif is_body:\\n\\t\\tresult.inside_brackets = True\\n\\t\\tresult.depth += 1\\n\\t\\tif leaves:\\n\\t\\t\\tnormalize_prefix(leaves[0], inside_brackets=True)\\n\\t\\t\\tif original.is_import:\\n\\t\\t\\t\\tfor i in range(len(leaves) - 1, -1, -1):\\n\\t\\t\\t\\t\\tif leaves[i].type == STANDALONE_COMMENT:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\telif leaves[i].type == token.COMMA:\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tleaves.insert(i + 1, Leaf(token.COMMA, \",\"))\\n\\t\\t\\t\\t\\t\\tbreak\\n\\tfor leaf in leaves:\\n\\t\\tresult.append(leaf, preformatted=True)\\n\\t\\tfor comment_after in original.comments_after(leaf):\\n\\t\\t\\tresult.append(comment_after, preformatted=True)\\n\\tif is_body:\\n\\t\\tresult.should_explode = should_explode(result, opening_bracket)\\n\\treturn result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def bracket_split_build_line(\\n\\tleaves: List[Leaf], original: Line, opening_bracket: Leaf, *, is_body: bool = False\\n) -> Line:\\n\\tresult = Line(depth=original.depth)\\n\\tif is_body:\\n\\t\\tresult.inside_brackets = True\\n\\t\\tresult.depth += 1\\n\\t\\tif leaves:\\n\\t\\t\\tnormalize_prefix(leaves[0], inside_brackets=True)\\n\\t\\t\\tif original.is_import:\\n\\t\\t\\t\\tfor i in range(len(leaves) - 1, -1, -1):\\n\\t\\t\\t\\t\\tif leaves[i].type == STANDALONE_COMMENT:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\telif leaves[i].type == token.COMMA:\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tleaves.insert(i + 1, Leaf(token.COMMA, \",\"))\\n\\t\\t\\t\\t\\t\\tbreak\\n\\tfor leaf in leaves:\\n\\t\\tresult.append(leaf, preformatted=True)\\n\\t\\tfor comment_after in original.comments_after(leaf):\\n\\t\\t\\tresult.append(comment_after, preformatted=True)\\n\\tif is_body:\\n\\t\\tresult.should_explode = should_explode(result, opening_bracket)\\n\\treturn result"
  },
  {
    "code": "def maybe_color_bp(bp):\\n        if 'color' not in kwds :\\n            from matplotlib.artist import setp\\n            setp(bp['boxes'],color=colors[0],alpha=1)\\n            setp(bp['whiskers'],color=colors[0],alpha=1)\\n            setp(bp['medians'],color=colors[2],alpha=1)\\n    BP = namedtuple(\"Boxplot\", ['ax', 'lines'])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH/CLN: add BoxPlot class inheriting MPLPlot",
    "fixed_code": "def maybe_color_bp(self, bp):\\n        if isinstance(self.color, dict):\\n            boxes = self.color.get('boxes', self._boxes_c)\\n            whiskers = self.color.get('whiskers', self._whiskers_c)\\n            medians = self.color.get('medians', self._medians_c)\\n            caps = self.color.get('caps', self._caps_c)\\n        else:\\n            boxes = self.color or self._boxes_c\\n            whiskers = self.color or self._whiskers_c\\n            medians = self.color or self._medians_c\\n            caps = self.color or self._caps_c\\n        from matplotlib.artist import setp\\n        setp(bp['boxes'], color=boxes, alpha=1)\\n        setp(bp['whiskers'], color=whiskers, alpha=1)\\n        setp(bp['medians'], color=medians, alpha=1)\\n        setp(bp['caps'], color=caps, alpha=1)"
  },
  {
    "code": "def __init__(self, api_version=\"v1\", gcp_conn_id=\"google_cloud_default\", delegate_to=None):\\n        super(CloudBuildHook, self).__init__(gcp_conn_id, delegate_to)\\n        self.api_version = api_version\\n        self.num_retries = self._get_field(\"num_retries\", 5)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4945] Use super() syntax (#5579)",
    "fixed_code": "def __init__(self, api_version=\"v1\", gcp_conn_id=\"google_cloud_default\", delegate_to=None):\\n        super().__init__(gcp_conn_id, delegate_to)\\n        self.api_version = api_version\\n        self.num_retries = self._get_field(\"num_retries\", 5)"
  },
  {
    "code": "def config_realtime_process(core: int, priority: int) -> None:\\n  gc.disable()\\n  set_realtime_priority(priority)\\n  set_core_affinity(core)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "paramsd: fix variable cpu usage (#24281)",
    "fixed_code": "def config_realtime_process(cores: Union[int, List[int]], priority: int) -> None:\\n  gc.disable()\\n  set_realtime_priority(priority)\\n  c = cores if isinstance(cores, list) else [cores, ]\\n  set_core_affinity(c)"
  },
  {
    "code": "def is_recursion_error(etype, value, records):\\n\\ttry:\\n\\t\\trecursion_error_type = RecursionError\\n\\texcept NameError:\\n\\t\\trecursion_error_type = RuntimeError\\n\\treturn (etype is recursion_error_type) \\\\n\\t\\t   and \"recursion\" in str(value).lower() \\\\n\\t\\t   and len(records) > 500",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix state leakage and speedup in test-suite\\n\\nThis is mostly visible when running the test via pytest that\\ncollect-then-run vs nose that runs as it collects.\\n\\nWhile I was at it I've improved some tests to be faster by changing the\\nmaximum recursion limit.",
    "fixed_code": "def is_recursion_error(etype, value, records):\\n\\ttry:\\n\\t\\trecursion_error_type = RecursionError\\n\\texcept NameError:\\n\\t\\trecursion_error_type = RuntimeError\\n\\treturn (etype is recursion_error_type) \\\\n\\t\\t   and \"recursion\" in str(value).lower() \\\\n\\t\\t   and len(records) > _FRAME_RECURSION_LIMIT"
  },
  {
    "code": "def _get_dependency_info() -> dict[str, JSONSerializable]:\\n    deps = [\\n        \"pandas\",\\n        \"numpy\",\\n        \"pytz\",\\n        \"dateutil\",\\n        \"setuptools\",\\n        \"pip\",\\n        \"Cython\",\\n        \"pytest\",\\n        \"hypothesis\",\\n        \"sphinx\",\\n        \"blosc\",\\n        \"feather\",\\n        \"xlsxwriter\",\\n        \"lxml.etree\",\\n        \"html5lib\",\\n        \"pymysql\",\\n        \"psycopg2\",\\n        \"jinja2\",\\n        \"IPython\",\\n        \"pandas_datareader\",\\n    ]\\n    deps.extend(list(VERSIONS))\\n    result: dict[str, JSONSerializable] = {}\\n    for modname in deps:\\n        mod = import_optional_dependency(modname, errors=\"ignore\")\\n        result[modname] = get_version(mod) if mod else None\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_dependency_info() -> dict[str, JSONSerializable]:\\n    deps = [\\n        \"pandas\",\\n        \"numpy\",\\n        \"pytz\",\\n        \"dateutil\",\\n        \"setuptools\",\\n        \"pip\",\\n        \"Cython\",\\n        \"pytest\",\\n        \"hypothesis\",\\n        \"sphinx\",\\n        \"blosc\",\\n        \"feather\",\\n        \"xlsxwriter\",\\n        \"lxml.etree\",\\n        \"html5lib\",\\n        \"pymysql\",\\n        \"psycopg2\",\\n        \"jinja2\",\\n        \"IPython\",\\n        \"pandas_datareader\",\\n    ]\\n    deps.extend(list(VERSIONS))\\n    result: dict[str, JSONSerializable] = {}\\n    for modname in deps:\\n        mod = import_optional_dependency(modname, errors=\"ignore\")\\n        result[modname] = get_version(mod) if mod else None\\n    return result"
  },
  {
    "code": "def model_unpickle(model_id, attrs, factory):\\n    if isinstance(model_id, tuple):\\n        if not apps.ready:\\n            apps.populate(settings.INSTALLED_APPS)\\n        model = apps.get_model(*model_id)\\n    else:\\n        model = model_id\\n    cls = factory(model, attrs)\\n    return cls.__new__(cls)\\nmodel_unpickle.__safe_for_unpickle__ = True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def model_unpickle(model_id, attrs, factory):\\n    if isinstance(model_id, tuple):\\n        if not apps.ready:\\n            apps.populate(settings.INSTALLED_APPS)\\n        model = apps.get_model(*model_id)\\n    else:\\n        model = model_id\\n    cls = factory(model, attrs)\\n    return cls.__new__(cls)\\nmodel_unpickle.__safe_for_unpickle__ = True"
  },
  {
    "code": "def importKey(self, externKey):\\n\\t\\tif externKey.startswith(b('-----')):\\n\\t\\t\\tlines = externKey.replace(b(\" \"),b('')).split()\\n\\t\\t\\tder = binascii.a2b_base64(b('').join(lines[1:-1]))\\n\\t\\t\\treturn self._importKeyDER(der)\\n\\t\\tif externKey[0]==b('\\x30')[0]:\\n\\t\\t\\treturn self._importKeyDER(externKey)\\n\\t\\traise ValueError(\"RSA key format is not supported\")\\n_impl = RSAImplementation()\\ngenerate = _impl.generate\\nconstruct = _impl.construct",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Py3k: fix when RSA.importKey is called with a str argument, instead of bytes",
    "fixed_code": "def importKey(self, externKey):\\n\\t\\tif isinstance(externKey, unicode) and externKey.startswith(\"-----\"):\\n\\t\\t\\texternKey = externKey.encode('ascii')\\n\\t\\tif externKey.startswith(b('-----')):\\n\\t\\t\\tlines = externKey.replace(b(\" \"),b('')).split()\\n\\t\\t\\tder = binascii.a2b_base64(b('').join(lines[1:-1]))\\n\\t\\t\\treturn self._importKeyDER(der)\\n\\t\\tif externKey[0]==b('\\x30')[0]:\\n\\t\\t\\treturn self._importKeyDER(externKey)\\n\\t\\traise ValueError(\"RSA key format is not supported\")\\n_impl = RSAImplementation()\\ngenerate = _impl.generate\\nconstruct = _impl.construct"
  },
  {
    "code": "def __nonzero__(self):\\n        raise ValueError(\"The truth value of an array is ambiguous. Use a.empty, a.item(), a.any() or a.all().\")\\n    __bool__ = __nonzero__",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: allow single element boolean Series to mimic numpy behavior( related GH4657)\\n\\nTST: test for single element with null-like\\n\\nDOC: deprecation message for using bool(Series([True]))",
    "fixed_code": "def __nonzero__(self):\\n        raise ValueError(\"The truth value of a {0} is ambiguous. \"\\n                         \"Use a.empty, a.item(), a.any() or a.all().\".format(self.__class__.__name__))\\n    __bool__ = __nonzero__"
  },
  {
    "code": "def to_eng_string(self, context=None):\\n        return self.__str__(eng=True, context=context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_eng_string(self, context=None):\\n        return self.__str__(eng=True, context=context)"
  },
  {
    "code": "def merge(self, right, how='inner', on=None, left_on=None, right_on=None,\\n              left_index=False, right_index=False, sort=False,\\n              suffixes=('_x', '_y'), copy=True):\\n        from pandas.tools.merge import merge\\n        return merge(self, right, how=how, on=on,\\n                     left_on=left_on, right_on=right_on,\\n                     left_index=left_index, right_index=right_index, sort=sort,\\n                     suffixes=suffixes, copy=copy)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add merge indicator to DataFrame.merge",
    "fixed_code": "def merge(self, right, how='inner', on=None, left_on=None, right_on=None,\\n              left_index=False, right_index=False, sort=False,\\n              suffixes=('_x', '_y'), copy=True, indicator=False):\\n        from pandas.tools.merge import merge\\n        return merge(self, right, how=how, on=on,\\n                     left_on=left_on, right_on=right_on,\\n                     left_index=left_index, right_index=right_index, sort=sort,\\n                     suffixes=suffixes, copy=copy, indicator=indicator)"
  },
  {
    "code": "def main():\\n    spec = ArgumentSpec()\\n    module = AnsibleModule(\\n        argument_spec=spec.argument_spec,\\n        supports_check_mode=spec.supports_check_mode,\\n    )\\n    try:\\n        mm = ModuleManager(module=module)\\n        results = mm.exec_module()\\n        module.exit_json(**results)\\n    except F5ModuleError as ex:\\n        module.fail_json(msg=str(ex))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactors main() function and module manager in multiple modules in line with recent changes (#53982)\\n\\nAdds variable types to docs\\nRefactors unit tests to remove deprecated parameters",
    "fixed_code": "def main():\\n    spec = ArgumentSpec()\\n    module = AnsibleModule(\\n        argument_spec=spec.argument_spec,\\n        supports_check_mode=spec.supports_check_mode,\\n        mutually_exclusive=spec.mutually_exclusive\\n    )\\n    try:\\n        mm = ModuleManager(module=module)\\n        results = mm.exec_module()\\n        module.exit_json(**results)\\n    except F5ModuleError as ex:\\n        module.fail_json(msg=str(ex))"
  },
  {
    "code": "def parse_header(self, header_plus):\\n\\t\\tindex = header_plus.find(b\"\\r\\n\")\\n\\t\\tif index >= 0:\\n\\t\\t\\tfirst_line = header_plus[:index].rstrip()\\n\\t\\t\\theader = header_plus[index + 2 :]\\n\\t\\telse:\\n\\t\\t\\traise ParsingError(\"HTTP message header invalid\")\\n\\t\\tif b\"\\r\" in first_line or b\"\\n\" in first_line:\\n\\t\\t\\traise ParsingError(\"Bare CR or LF found in HTTP message\")\\n\\t\\tself.first_line = first_line  \\n\\t\\tlines = get_header_lines(header)\\n\\t\\theaders = self.headers\\n\\t\\tfor line in lines:\\n\\t\\t\\theader = HEADER_FIELD.match(line)\\n\\t\\t\\tif not header:\\n\\t\\t\\t\\traise ParsingError(\"Invalid header\")\\n\\t\\t\\tkey, value = header.group(\"name\", \"value\")\\n\\t\\t\\tif b\"_\" in key:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tvalue = value.strip(b\" \\t\")\\n\\t\\t\\tkey1 = key.upper().replace(b\"-\", b\"_\").decode(\"latin-1\")\\n\\t\\t\\ttry:\\n\\t\\t\\t\\theaders[key1] += (b\", \" + value).decode(\"latin-1\")\\n\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\theaders[key1] = value.decode(\"latin-1\")\\n\\t\\tcommand, uri, version = crack_first_line(first_line)\\n\\t\\tself.request_uri = uri.decode(\"latin-1\")\\n\\t\\tversion = version.decode(\"latin-1\")\\n\\t\\tcommand = command.decode(\"latin-1\")\\n\\t\\tself.command = command\\n\\t\\tself.version = version\\n\\t\\t(\\n\\t\\t\\tself.proxy_scheme,\\n\\t\\t\\tself.proxy_netloc,\\n\\t\\t\\tself.path,\\n\\t\\t\\tself.query,\\n\\t\\t\\tself.fragment,\\n\\t\\t) = split_uri(uri)\\n\\t\\tself.url_scheme = self.adj.url_scheme\\n\\t\\tconnection = headers.get(\"CONNECTION\", \"\")\\n\\t\\tif version == \"1.0\":\\n\\t\\t\\tif connection.lower() != \"keep-alive\":\\n\\t\\t\\t\\tself.connection_close = True\\n\\t\\tif version == \"1.1\":\\n\\t\\t\\tte = headers.pop(\"TRANSFER_ENCODING\", \"\")\\n\\t\\t\\tencodings = [\\n\\t\\t\\t\\tencoding.strip(\" \\t\").lower() for encoding in te.split(\",\") if encoding\\n\\t\\t\\t]\\n\\t\\t\\tfor encoding in encodings:\\n\\t\\t\\t\\tif encoding not in {\"chunked\"}:\\n\\t\\t\\t\\t\\traise TransferEncodingNotImplemented(\\n\\t\\t\\t\\t\\t\\t\"Transfer-Encoding requested is not supported.\"\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\tif encodings and encodings[-1] == \"chunked\":\\n\\t\\t\\t\\tself.chunked = True\\n\\t\\t\\t\\tbuf = OverflowableBuffer(self.adj.inbuf_overflow)\\n\\t\\t\\t\\tself.body_rcv = ChunkedReceiver(buf)\\n\\t\\t\\telif encodings:  \\n\\t\\t\\t\\traise TransferEncodingNotImplemented(\\n\\t\\t\\t\\t\\t\"Transfer-Encoding requested is not supported.\"\\n\\t\\t\\t\\t)\\n\\t\\t\\texpect = headers.get(\"EXPECT\", \"\").lower()\\n\\t\\t\\tself.expect_continue = expect == \"100-continue\"\\n\\t\\t\\tif connection.lower() == \"close\":\\n\\t\\t\\t\\tself.connection_close = True\\n\\t\\tif not self.chunked:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcl = int(headers.get(\"CONTENT_LENGTH\", 0))\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\traise ParsingError(\"Content-Length is invalid\")\\n\\t\\t\\tself.content_length = cl\\n\\t\\t\\tif cl > 0:\\n\\t\\t\\t\\tbuf = OverflowableBuffer(self.adj.inbuf_overflow)\\n\\t\\t\\t\\tself.body_rcv = FixedStreamReceiver(cl, buf)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Be more strict in parsing Content-Length\\n\\nValidate that we are only parsing digits and nothing else. RFC7230 is\\nexplicit in that the Content-Length can only exist of 1*DIGIT and may\\nnot include any additional sign information.\\n\\nThe Python int() function parses `+10` as `10` which means we were more\\nlenient than the standard intended.",
    "fixed_code": "def parse_header(self, header_plus):\\n\\t\\tindex = header_plus.find(b\"\\r\\n\")\\n\\t\\tif index >= 0:\\n\\t\\t\\tfirst_line = header_plus[:index].rstrip()\\n\\t\\t\\theader = header_plus[index + 2 :]\\n\\t\\telse:\\n\\t\\t\\traise ParsingError(\"HTTP message header invalid\")\\n\\t\\tif b\"\\r\" in first_line or b\"\\n\" in first_line:\\n\\t\\t\\traise ParsingError(\"Bare CR or LF found in HTTP message\")\\n\\t\\tself.first_line = first_line  \\n\\t\\tlines = get_header_lines(header)\\n\\t\\theaders = self.headers\\n\\t\\tfor line in lines:\\n\\t\\t\\theader = HEADER_FIELD_RE.match(line)\\n\\t\\t\\tif not header:\\n\\t\\t\\t\\traise ParsingError(\"Invalid header\")\\n\\t\\t\\tkey, value = header.group(\"name\", \"value\")\\n\\t\\t\\tif b\"_\" in key:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tvalue = value.strip(b\" \\t\")\\n\\t\\t\\tkey1 = key.upper().replace(b\"-\", b\"_\").decode(\"latin-1\")\\n\\t\\t\\ttry:\\n\\t\\t\\t\\theaders[key1] += (b\", \" + value).decode(\"latin-1\")\\n\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\theaders[key1] = value.decode(\"latin-1\")\\n\\t\\tcommand, uri, version = crack_first_line(first_line)\\n\\t\\tself.request_uri = uri.decode(\"latin-1\")\\n\\t\\tversion = version.decode(\"latin-1\")\\n\\t\\tcommand = command.decode(\"latin-1\")\\n\\t\\tself.command = command\\n\\t\\tself.version = version\\n\\t\\t(\\n\\t\\t\\tself.proxy_scheme,\\n\\t\\t\\tself.proxy_netloc,\\n\\t\\t\\tself.path,\\n\\t\\t\\tself.query,\\n\\t\\t\\tself.fragment,\\n\\t\\t) = split_uri(uri)\\n\\t\\tself.url_scheme = self.adj.url_scheme\\n\\t\\tconnection = headers.get(\"CONNECTION\", \"\")\\n\\t\\tif version == \"1.0\":\\n\\t\\t\\tif connection.lower() != \"keep-alive\":\\n\\t\\t\\t\\tself.connection_close = True\\n\\t\\tif version == \"1.1\":\\n\\t\\t\\tte = headers.pop(\"TRANSFER_ENCODING\", \"\")\\n\\t\\t\\tencodings = [\\n\\t\\t\\t\\tencoding.strip(\" \\t\").lower() for encoding in te.split(\",\") if encoding\\n\\t\\t\\t]\\n\\t\\t\\tfor encoding in encodings:\\n\\t\\t\\t\\tif encoding not in {\"chunked\"}:\\n\\t\\t\\t\\t\\traise TransferEncodingNotImplemented(\\n\\t\\t\\t\\t\\t\\t\"Transfer-Encoding requested is not supported.\"\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\tif encodings and encodings[-1] == \"chunked\":\\n\\t\\t\\t\\tself.chunked = True\\n\\t\\t\\t\\tbuf = OverflowableBuffer(self.adj.inbuf_overflow)\\n\\t\\t\\t\\tself.body_rcv = ChunkedReceiver(buf)\\n\\t\\t\\telif encodings:  \\n\\t\\t\\t\\traise TransferEncodingNotImplemented(\\n\\t\\t\\t\\t\\t\"Transfer-Encoding requested is not supported.\"\\n\\t\\t\\t\\t)\\n\\t\\t\\texpect = headers.get(\"EXPECT\", \"\").lower()\\n\\t\\t\\tself.expect_continue = expect == \"100-continue\"\\n\\t\\t\\tif connection.lower() == \"close\":\\n\\t\\t\\t\\tself.connection_close = True\\n\\t\\tif not self.chunked:\\n\\t\\t\\tcl = headers.get(\"CONTENT_LENGTH\", \"0\")\\n\\t\\t\\tif not ONLY_DIGIT_RE.match(cl.encode(\"latin-1\")):\\n\\t\\t\\t\\traise ParsingError(\"Content-Length is invalid\")\\n\\t\\t\\tcl = int(cl)\\n\\t\\t\\tself.content_length = cl\\n\\t\\t\\tif cl > 0:\\n\\t\\t\\t\\tbuf = OverflowableBuffer(self.adj.inbuf_overflow)\\n\\t\\t\\t\\tself.body_rcv = FixedStreamReceiver(cl, buf)"
  },
  {
    "code": "def lookups(self, request, model_admin):\\n        raise NotImplementedError",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def lookups(self, request, model_admin):\\n        raise NotImplementedError"
  },
  {
    "code": "def time_lstrip(self, dtype):\\n        self.s.str.lstrip(\"A\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_lstrip(self, dtype):\\n        self.s.str.lstrip(\"A\")"
  },
  {
    "code": "def label_for_field(name, model, model_admin=None, return_attr=False):\\n    attr = None\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n        try:\\n            label = field.verbose_name\\n        except AttributeError:\\n            label = field.related_model._meta.verbose_name\\n    except FieldDoesNotExist:\\n        if name == \"__unicode__\":\\n            label = force_text(model._meta.verbose_name)\\n            attr = six.text_type\\n        elif name == \"__str__\":\\n            label = force_str(model._meta.verbose_name)\\n            attr = bytes\\n        else:\\n            if callable(name):\\n                attr = name\\n            elif model_admin is not None and hasattr(model_admin, name):\\n                attr = getattr(model_admin, name)\\n            elif hasattr(model, name):\\n                attr = getattr(model, name)\\n            else:\\n                message = \"Unable to lookup '%s' on %s\" % (name, model._meta.object_name)\\n                if model_admin:\\n                    message += \" or %s\" % (model_admin.__class__.__name__,)\\n                raise AttributeError(message)\\n            if hasattr(attr, \"short_description\"):\\n                label = attr.short_description\\n            elif (isinstance(attr, property) and\\n                  hasattr(attr, \"fget\") and\\n                  hasattr(attr.fget, \"short_description\")):\\n                label = attr.fget.short_description\\n            elif callable(attr):\\n                if attr.__name__ == \"<lambda>\":\\n                    label = \"--\"\\n                else:\\n                    label = pretty_name(attr.__name__)\\n            else:\\n                label = pretty_name(name)\\n    if return_attr:\\n        return (label, attr)\\n    else:\\n        return label",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def label_for_field(name, model, model_admin=None, return_attr=False):\\n    attr = None\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n        try:\\n            label = field.verbose_name\\n        except AttributeError:\\n            label = field.related_model._meta.verbose_name\\n    except FieldDoesNotExist:\\n        if name == \"__unicode__\":\\n            label = force_text(model._meta.verbose_name)\\n            attr = six.text_type\\n        elif name == \"__str__\":\\n            label = force_str(model._meta.verbose_name)\\n            attr = bytes\\n        else:\\n            if callable(name):\\n                attr = name\\n            elif model_admin is not None and hasattr(model_admin, name):\\n                attr = getattr(model_admin, name)\\n            elif hasattr(model, name):\\n                attr = getattr(model, name)\\n            else:\\n                message = \"Unable to lookup '%s' on %s\" % (name, model._meta.object_name)\\n                if model_admin:\\n                    message += \" or %s\" % (model_admin.__class__.__name__,)\\n                raise AttributeError(message)\\n            if hasattr(attr, \"short_description\"):\\n                label = attr.short_description\\n            elif (isinstance(attr, property) and\\n                  hasattr(attr, \"fget\") and\\n                  hasattr(attr.fget, \"short_description\")):\\n                label = attr.fget.short_description\\n            elif callable(attr):\\n                if attr.__name__ == \"<lambda>\":\\n                    label = \"--\"\\n                else:\\n                    label = pretty_name(attr.__name__)\\n            else:\\n                label = pretty_name(name)\\n    if return_attr:\\n        return (label, attr)\\n    else:\\n        return label"
  },
  {
    "code": "def findsource(object):\\n    file = getsourcefile(object) or getfile(object)\\n    module = getmodule(object, file)\\n    if module:\\n        lines = linecache.getlines(file, module.__dict__)\\n    else:\\n        lines = linecache.getlines(file)\\n    if not lines:\\n        raise IOError('could not get source code')\\n    if ismodule(object):\\n        return lines, 0\\n    if isclass(object):\\n        name = object.__name__\\n        pat = re.compile(r'^\\s*class\\s*' + name + r'\\b')\\n        for i in range(len(lines)):\\n            if pat.match(lines[i]): return lines, i\\n        else:\\n            raise IOError('could not find class definition')\\n    if ismethod(object):\\n        object = object.im_func\\n    if isfunction(object):\\n        object = object.func_code\\n    if istraceback(object):\\n        object = object.tb_frame\\n    if isframe(object):\\n        object = object.f_code\\n    if iscode(object):\\n        if not hasattr(object, 'co_firstlineno'):\\n            raise IOError('could not find function definition')\\n        lnum = object.co_firstlineno - 1\\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\\n        while lnum > 0:\\n            if pat.match(lines[lnum]): break\\n            lnum = lnum - 1\\n        return lines, lnum\\n    raise IOError('could not find code object')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def findsource(object):\\n    file = getsourcefile(object) or getfile(object)\\n    module = getmodule(object, file)\\n    if module:\\n        lines = linecache.getlines(file, module.__dict__)\\n    else:\\n        lines = linecache.getlines(file)\\n    if not lines:\\n        raise IOError('could not get source code')\\n    if ismodule(object):\\n        return lines, 0\\n    if isclass(object):\\n        name = object.__name__\\n        pat = re.compile(r'^\\s*class\\s*' + name + r'\\b')\\n        for i in range(len(lines)):\\n            if pat.match(lines[i]): return lines, i\\n        else:\\n            raise IOError('could not find class definition')\\n    if ismethod(object):\\n        object = object.im_func\\n    if isfunction(object):\\n        object = object.func_code\\n    if istraceback(object):\\n        object = object.tb_frame\\n    if isframe(object):\\n        object = object.f_code\\n    if iscode(object):\\n        if not hasattr(object, 'co_firstlineno'):\\n            raise IOError('could not find function definition')\\n        lnum = object.co_firstlineno - 1\\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\\n        while lnum > 0:\\n            if pat.match(lines[lnum]): break\\n            lnum = lnum - 1\\n        return lines, lnum\\n    raise IOError('could not find code object')"
  },
  {
    "code": "def transform_dict_like(\\n    obj: FrameOrSeries,\\n    func: Dict[Label, Union[AggFuncTypeBase, List[AggFuncTypeBase]]],\\n    *args,\\n    **kwargs,\\n):\\n    from pandas.core.reshape.concat import concat\\n    if len(func) == 0:\\n        raise ValueError(\"No transform functions were provided\")\\n    if obj.ndim != 1:\\n        cols = sorted(set(func.keys()) - set(obj.columns))\\n        if len(cols) > 0:\\n            raise SpecificationError(f\"Column(s) {cols} do not exist\")\\n    if any(is_dict_like(v) for _, v in func.items()):\\n        raise SpecificationError(\"nested renamer is not supported\")\\n    results: Dict[Label, FrameOrSeriesUnion] = {}\\n    for name, how in func.items():\\n        colg = obj._gotitem(name, ndim=1)\\n        try:\\n            results[name] = transform(colg, how, 0, *args, **kwargs)\\n        except Exception as err:\\n            if (\\n                str(err) == \"Function did not transform\"\\n                or str(err) == \"No transform functions were provided\"\\n            ):\\n                raise err\\n    if len(results) == 0:\\n        raise ValueError(\"Transform function failed\")\\n    return concat(results, axis=1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def transform_dict_like(\\n    obj: FrameOrSeries,\\n    func: Dict[Label, Union[AggFuncTypeBase, List[AggFuncTypeBase]]],\\n    *args,\\n    **kwargs,\\n):\\n    from pandas.core.reshape.concat import concat\\n    if len(func) == 0:\\n        raise ValueError(\"No transform functions were provided\")\\n    if obj.ndim != 1:\\n        cols = sorted(set(func.keys()) - set(obj.columns))\\n        if len(cols) > 0:\\n            raise SpecificationError(f\"Column(s) {cols} do not exist\")\\n    if any(is_dict_like(v) for _, v in func.items()):\\n        raise SpecificationError(\"nested renamer is not supported\")\\n    results: Dict[Label, FrameOrSeriesUnion] = {}\\n    for name, how in func.items():\\n        colg = obj._gotitem(name, ndim=1)\\n        try:\\n            results[name] = transform(colg, how, 0, *args, **kwargs)\\n        except Exception as err:\\n            if (\\n                str(err) == \"Function did not transform\"\\n                or str(err) == \"No transform functions were provided\"\\n            ):\\n                raise err\\n    if len(results) == 0:\\n        raise ValueError(\"Transform function failed\")\\n    return concat(results, axis=1)"
  },
  {
    "code": "def hashed_name(self, name, content=None):\\n        parsed_name = urlsplit(unquote(name))\\n        clean_name = parsed_name.path.strip()\\n        if content is None:\\n            if not self.exists(clean_name):\\n                raise ValueError(\"The file '%s' could not be found with %r.\" %\\n                                 (clean_name, self))\\n            try:\\n                content = self.open(clean_name)\\n            except IOError:\\n                return name\\n        path, filename = os.path.split(clean_name)\\n        root, ext = os.path.splitext(filename)\\n        md5 = hashlib.md5()\\n        for chunk in content.chunks():\\n            md5.update(chunk)\\n        md5sum = md5.hexdigest()[:12]\\n        hashed_name = os.path.join(path, u\"%s.%s%s\" %\\n                                   (root, md5sum, ext))\\n        unparsed_name = list(parsed_name)\\n        unparsed_name[2] = hashed_name\\n        if '?\\n            unparsed_name[2] += '?'\\n        return urlunsplit(unparsed_name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def hashed_name(self, name, content=None):\\n        parsed_name = urlsplit(unquote(name))\\n        clean_name = parsed_name.path.strip()\\n        if content is None:\\n            if not self.exists(clean_name):\\n                raise ValueError(\"The file '%s' could not be found with %r.\" %\\n                                 (clean_name, self))\\n            try:\\n                content = self.open(clean_name)\\n            except IOError:\\n                return name\\n        path, filename = os.path.split(clean_name)\\n        root, ext = os.path.splitext(filename)\\n        md5 = hashlib.md5()\\n        for chunk in content.chunks():\\n            md5.update(chunk)\\n        md5sum = md5.hexdigest()[:12]\\n        hashed_name = os.path.join(path, u\"%s.%s%s\" %\\n                                   (root, md5sum, ext))\\n        unparsed_name = list(parsed_name)\\n        unparsed_name[2] = hashed_name\\n        if '?\\n            unparsed_name[2] += '?'\\n        return urlunsplit(unparsed_name)"
  },
  {
    "code": "def validation_curve(estimator, X, y, param_name, param_range, cv=None,\\n                     scoring=None, n_jobs=1, pre_dispatch=\"all\", verbose=0):\\n    X, y = check_arrays(X, y, sparse_format='csr', allow_lists=True)\\n    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))\\n    scorer = check_scoring(estimator, scoring=scoring)\\n    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\\n                        verbose=verbose)\\n    out = parallel(delayed(_fit_and_score)(\\n        estimator, X, y, scorer, train, test, verbose,\\n        parameters={param_name : v}, fit_params=None, return_train_score=True)\\n        for train, test in cv for v in param_range)\\n    out = np.asarray(out)[:, :2]\\n    n_params = len(param_range)\\n    n_cv_folds = out.shape[0] / n_params\\n    out = out.reshape(n_cv_folds, n_params, 2)\\n    avg_over_cv = out.mean(axis=0).reshape(n_params, 2)\\n    return avg_over_cv[:, 0], avg_over_cv[:, 1]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validation_curve(estimator, X, y, param_name, param_range, cv=None,\\n                     scoring=None, n_jobs=1, pre_dispatch=\"all\", verbose=0):\\n    X, y = check_arrays(X, y, sparse_format='csr', allow_lists=True)\\n    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))\\n    scorer = check_scoring(estimator, scoring=scoring)\\n    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\\n                        verbose=verbose)\\n    out = parallel(delayed(_fit_and_score)(\\n        estimator, X, y, scorer, train, test, verbose,\\n        parameters={param_name : v}, fit_params=None, return_train_score=True)\\n        for train, test in cv for v in param_range)\\n    out = np.asarray(out)[:, :2]\\n    n_params = len(param_range)\\n    n_cv_folds = out.shape[0] / n_params\\n    out = out.reshape(n_cv_folds, n_params, 2)\\n    avg_over_cv = out.mean(axis=0).reshape(n_params, 2)\\n    return avg_over_cv[:, 0], avg_over_cv[:, 1]"
  },
  {
    "code": "def import_optional_dependency(\\n    name: str,\\n    extra: str = \"\",\\n    errors: str = \"raise\",\\n    min_version: str | None = None,\\n):\\n    assert errors in {\"warn\", \"raise\", \"ignore\"}\\n    package_name = INSTALL_MAPPING.get(name)\\n    install_name = package_name if package_name is not None else name\\n    msg = (\\n        f\"Missing optional dependency '{install_name}'. {extra} \"\\n        f\"Use pip or conda to install {install_name}.\"\\n    )\\n    try:\\n        module = importlib.import_module(name)\\n    except ImportError:\\n        if errors == \"raise\":\\n            raise ImportError(msg)\\n        else:\\n            return None\\n    parent = name.split(\".\")[0]\\n    if parent != name:\\n        install_name = parent\\n        module_to_get = sys.modules[install_name]\\n    else:\\n        module_to_get = module\\n    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)\\n    if minimum_version:\\n        version = get_version(module_to_get)\\n        if Version(version) < Version(minimum_version):\\n            msg = (\\n                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"\\n                f\"(version '{version}' currently installed).\"\\n            )\\n            if errors == \"warn\":\\n                warnings.warn(msg, UserWarning)\\n                return None\\n            elif errors == \"raise\":\\n                raise ImportError(msg)\\n    return module",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def import_optional_dependency(\\n    name: str,\\n    extra: str = \"\",\\n    errors: str = \"raise\",\\n    min_version: str | None = None,\\n):\\n    assert errors in {\"warn\", \"raise\", \"ignore\"}\\n    package_name = INSTALL_MAPPING.get(name)\\n    install_name = package_name if package_name is not None else name\\n    msg = (\\n        f\"Missing optional dependency '{install_name}'. {extra} \"\\n        f\"Use pip or conda to install {install_name}.\"\\n    )\\n    try:\\n        module = importlib.import_module(name)\\n    except ImportError:\\n        if errors == \"raise\":\\n            raise ImportError(msg)\\n        else:\\n            return None\\n    parent = name.split(\".\")[0]\\n    if parent != name:\\n        install_name = parent\\n        module_to_get = sys.modules[install_name]\\n    else:\\n        module_to_get = module\\n    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)\\n    if minimum_version:\\n        version = get_version(module_to_get)\\n        if Version(version) < Version(minimum_version):\\n            msg = (\\n                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"\\n                f\"(version '{version}' currently installed).\"\\n            )\\n            if errors == \"warn\":\\n                warnings.warn(msg, UserWarning)\\n                return None\\n            elif errors == \"raise\":\\n                raise ImportError(msg)\\n    return module"
  },
  {
    "code": "def _isnull_ndarraylike(obj):\\n    from pandas import Series\\n    values = np.asarray(obj)\\n    if values.dtype.kind in ('O', 'S', 'U'):\\n        shape = values.shape\\n        if values.dtype.kind in ('S', 'U'):\\n            result = np.zeros(values.shape, dtype=bool)\\n        else:\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(values.ravel())\\n            result[:] = vec.reshape(shape)\\n        if isinstance(obj, Series):\\n            result = Series(result, index=obj.index, copy=False)\\n    elif values.dtype == np.dtype('M8[ns]'):\\n        result = values.view('i8') == tslib.iNaT\\n    elif issubclass(values.dtype.type, np.timedelta64):\\n        result = np.ones(values.shape, dtype=bool)\\n    else:\\n        result = np.isnan(obj)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _isnull_ndarraylike(obj):\\n    from pandas import Series\\n    values = np.asarray(obj)\\n    if values.dtype.kind in ('O', 'S', 'U'):\\n        shape = values.shape\\n        if values.dtype.kind in ('S', 'U'):\\n            result = np.zeros(values.shape, dtype=bool)\\n        else:\\n            result = np.empty(shape, dtype=bool)\\n            vec = lib.isnullobj(values.ravel())\\n            result[:] = vec.reshape(shape)\\n        if isinstance(obj, Series):\\n            result = Series(result, index=obj.index, copy=False)\\n    elif values.dtype == np.dtype('M8[ns]'):\\n        result = values.view('i8') == tslib.iNaT\\n    elif issubclass(values.dtype.type, np.timedelta64):\\n        result = np.ones(values.shape, dtype=bool)\\n    else:\\n        result = np.isnan(obj)\\n    return result"
  },
  {
    "code": "def findCaller(self, stack_info=False):\\n        f = currentframe()\\n        if f is not None:\\n            f = f.f_back\\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\\n        while hasattr(f, \"f_code\"):\\n            co = f.f_code\\n            filename = os.path.normcase(co.co_filename)\\n            if filename == _srcfile:\\n                f = f.f_back\\n                continue\\n            sinfo = None\\n            if stack_info:\\n                sio = io.StringIO()\\n                sio.write('Stack (most recent call last):\\n')\\n                traceback.print_stack(f, file=sio)\\n                sinfo = sio.getvalue()\\n                if sinfo[-1] == '\\n':\\n                    sinfo = sinfo[:-1]\\n                sio.close()\\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\\n            break\\n        return rv",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-33165: Added stacklevel parameter to logging APIs. (GH-7424)",
    "fixed_code": "def findCaller(self, stack_info=False, stacklevel=1):\\n        f = currentframe()\\n        if f is not None:\\n            f = f.f_back\\n        orig_f = f\\n        while f and stacklevel > 1:\\n            f = f.f_back\\n            stacklevel -= 1\\n        if not f:\\n            f = orig_f\\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\\n        while hasattr(f, \"f_code\"):\\n            co = f.f_code\\n            filename = os.path.normcase(co.co_filename)\\n            if filename == _srcfile:\\n                f = f.f_back\\n                continue\\n            sinfo = None\\n            if stack_info:\\n                sio = io.StringIO()\\n                sio.write('Stack (most recent call last):\\n')\\n                traceback.print_stack(f, file=sio)\\n                sinfo = sio.getvalue()\\n                if sinfo[-1] == '\\n':\\n                    sinfo = sinfo[:-1]\\n                sio.close()\\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\\n            break\\n        return rv"
  },
  {
    "code": "def delete_net_route(self):\\n        route_obj = netapp_utils.zapi.NaElement('net-routes-destroy')\\n        route_obj.add_new_child(\"destination\", self.parameters['destination'])\\n        route_obj.add_new_child(\"gateway\", self.parameters['gateway'])\\n        try:\\n            self.server.invoke_successfully(route_obj, True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error deleting net route: %s'\\n                                  % (to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Updates to na_ontap_net_route (#49656)",
    "fixed_code": "def delete_net_route(self, params=None):\\n        route_obj = netapp_utils.zapi.NaElement('net-routes-destroy')\\n        if params is None:\\n            params = self.parameters\\n        route_obj.add_new_child(\"destination\", params['destination'])\\n        route_obj.add_new_child(\"gateway\", params['gateway'])\\n        try:\\n            self.server.invoke_successfully(route_obj, True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error deleting net route: %s'\\n                                  % (to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def download_request(self, request, spider):\\n        factory = self.httpclientfactory(request)\\n        self._connect(factory)\\n        return factory.deferred",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def download_request(self, request, spider):\\n        factory = self.httpclientfactory(request)\\n        self._connect(factory)\\n        return factory.deferred"
  },
  {
    "code": "def isnull(input):\\n    ''\\n    from pandas.core.generic import PandasObject\\n    from pandas import Series\\n    if isinstance(input, np.ndarray):\\n        if input.dtype.kind in ('O', 'S'):\\n            shape = input.shape\\n            result = np.empty(shape, dtype=bool)\\n            vec = _tseries.isnullobj(input.ravel())\\n            result[:] = vec.reshape(shape)\\n            if isinstance(input, Series):\\n                result = Series(result, index=input.index, copy=False)\\n        else:\\n            result = -np.isfinite(input)\\n    elif isinstance(input, PandasObject):\\n        return input.apply(isnull)\\n    else:\\n        result = _tseries.checknull(input)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def isnull(input):\\n    ''\\n    from pandas.core.generic import PandasObject\\n    from pandas import Series\\n    if isinstance(input, np.ndarray):\\n        if input.dtype.kind in ('O', 'S'):\\n            shape = input.shape\\n            result = np.empty(shape, dtype=bool)\\n            vec = _tseries.isnullobj(input.ravel())\\n            result[:] = vec.reshape(shape)\\n            if isinstance(input, Series):\\n                result = Series(result, index=input.index, copy=False)\\n        else:\\n            result = -np.isfinite(input)\\n    elif isinstance(input, PandasObject):\\n        return input.apply(isnull)\\n    else:\\n        result = _tseries.checknull(input)\\n    return result"
  },
  {
    "code": "def connect(self):\\n        self.sock = socket.create_connection((self.host,self.port),\\n                                             self.timeout)\\n        if self._tunnel_host:\\n            self._tunnel()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def connect(self):\\n        self.sock = socket.create_connection((self.host,self.port),\\n                                             self.timeout)\\n        if self._tunnel_host:\\n            self._tunnel()"
  },
  {
    "code": "def numeric_tester(self, calc_type, calc_value, data_type, used_locale):\\n\\t\\ttry:\\n\\t\\t\\tset_locale = setlocale(LC_NUMERIC)\\n\\t\\texcept Error:\\n\\t\\t\\tset_locale = \"<not able to determine>\"\\n\\t\\tknown_value = known_numerics.get(used_locale,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t('', ''))[data_type == 'thousands_sep']\\n\\t\\tif known_value and calc_value:\\n\\t\\t\\tself.assertEquals(calc_value, known_value,\\n\\t\\t\\t\\t\\t\\t\\t\\tself.lc_numeric_err_msg % (\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_value, known_value,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_type, data_type, set_locale,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tused_locale))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def numeric_tester(self, calc_type, calc_value, data_type, used_locale):\\n\\t\\ttry:\\n\\t\\t\\tset_locale = setlocale(LC_NUMERIC)\\n\\t\\texcept Error:\\n\\t\\t\\tset_locale = \"<not able to determine>\"\\n\\t\\tknown_value = known_numerics.get(used_locale,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t('', ''))[data_type == 'thousands_sep']\\n\\t\\tif known_value and calc_value:\\n\\t\\t\\tself.assertEquals(calc_value, known_value,\\n\\t\\t\\t\\t\\t\\t\\t\\tself.lc_numeric_err_msg % (\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_value, known_value,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_type, data_type, set_locale,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tused_locale))"
  },
  {
    "code": "def __getitem__(self, idx):\\n        if not isinstance(idx, six.integer_types + (slice,)):\\n            raise TypeError\\n        return self.subwidgets[idx]\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, idx):\\n        if not isinstance(idx, six.integer_types + (slice,)):\\n            raise TypeError\\n        return self.subwidgets[idx]\\n    @property"
  },
  {
    "code": "def load_current_config(self):\\n        self._current_config = dict()\\n        lag_types = set([lag_obj['type'] for lag_obj in self._required_config])\\n        for lag_type in lag_types:\\n            if_type = self.IF_TYPE_MAP[lag_type]\\n            lag_summary = self._get_port_channels(if_type)\\n            if lag_summary:\\n                self._parse_port_channels_summary(lag_type, lag_summary)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "issue:38167 add support for onyx version 3.6.6000 for onyx_linkagg (#38191)",
    "fixed_code": "def load_current_config(self):\\n        self._current_config = dict()\\n        self._os_version = self._get_os_version()\\n        lag_types = set([lag_obj['type'] for lag_obj in self._required_config])\\n        for lag_type in lag_types:\\n            if_type = self.IF_TYPE_MAP[lag_type]\\n            lag_summary = self._get_port_channels(if_type)\\n            if lag_summary:\\n                self._parse_port_channels_summary(lag_type, lag_summary)"
  },
  {
    "code": "def poke(self, context: dict) -> bool:\\n        self.log.info(\\n            \"Sensor check existence of the document that matches the following query: %s\", self.query\\n        )\\n        hook = MongoHook(self.mongo_conn_id)\\n        return hook.find(self.collection, self.query, mongo_db=self.mongo_db, find_one=True) is not None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def poke(self, context: dict) -> bool:\\n        self.log.info(\\n            \"Sensor check existence of the document that matches the following query: %s\", self.query\\n        )\\n        hook = MongoHook(self.mongo_conn_id)\\n        return hook.find(self.collection, self.query, mongo_db=self.mongo_db, find_one=True) is not None"
  },
  {
    "code": "def get_dataset_tables(self, dataset_id: str, project_id: Optional[str] = None,\\n                           max_results: Optional[int] = None,\\n                           page_token: Optional[str] = None) -> Dict[str, Union[str, int, List]]:\\n        optional_params = {}  \\n        if max_results:\\n            optional_params['maxResults'] = max_results\\n        if page_token:\\n            optional_params['pageToken'] = page_token\\n        dataset_project_id = project_id or self.project_id\\n        return (self.service.tables().list(\\n            projectId=dataset_project_id,\\n            datasetId=dataset_id,\\n            **optional_params).execute(num_retries=self.num_retries))\\n    @CloudBaseHook.catch_http_exception",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_dataset_tables(self, dataset_id: str, project_id: Optional[str] = None,\\n                           max_results: Optional[int] = None,\\n                           page_token: Optional[str] = None) -> Dict[str, Union[str, int, List]]:\\n        optional_params = {}  \\n        if max_results:\\n            optional_params['maxResults'] = max_results\\n        if page_token:\\n            optional_params['pageToken'] = page_token\\n        dataset_project_id = project_id or self.project_id\\n        return (self.service.tables().list(\\n            projectId=dataset_project_id,\\n            datasetId=dataset_id,\\n            **optional_params).execute(num_retries=self.num_retries))\\n    @CloudBaseHook.catch_http_exception"
  },
  {
    "code": "def ix(self):\\n        if self._ix is None:\\n            self._ix = _NDFrameIndexer(self)\\n        return self._ix\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ix(self):\\n        if self._ix is None:\\n            self._ix = _NDFrameIndexer(self)\\n        return self._ix\\n    @property"
  },
  {
    "code": "def _read(self, fp, fpname):\\n        elements_added = set()\\n        cursect = None                        \\n        sectname = None\\n        optname = None\\n        lineno = 0\\n        indent_level = 0\\n        e = None                              \\n        for lineno, line in enumerate(fp, start=1):\\n            comment_start = None\\n            for prefix in self._startonly_comment_prefixes:\\n                if line.strip().startswith(prefix):\\n                    comment_start = 0\\n                    break\\n            for prefix in self._comment_prefixes:\\n                index = line.find(prefix)\\n                if index == 0 or (index > 0 and line[index-1].isspace()):\\n                    comment_start = index\\n                    break\\n            value = line[:comment_start].strip()\\n            if not value:\\n                if self._empty_lines_in_values:\\n                    if (comment_start is None and\\n                        cursect is not None and\\n                        optname and\\n                        cursect[optname] is not None):\\n                        cursect[optname].append('') \\n                else:\\n                    indent_level = sys.maxsize\\n                continue\\n            first_nonspace = self.NONSPACECRE.search(line)\\n            cur_indent_level = first_nonspace.start() if first_nonspace else 0\\n            if (cursect is not None and optname and\\n                cur_indent_level > indent_level):\\n                cursect[optname].append(value)\\n            else:\\n                indent_level = cur_indent_level\\n                mo = self.SECTCRE.match(value)\\n                if mo:\\n                    sectname = mo.group('header')\\n                    if sectname in self._sections:\\n                        if self._strict and sectname in elements_added:\\n                            raise DuplicateSectionError(sectname, fpname,\\n                                                        lineno)\\n                        cursect = self._sections[sectname]\\n                        elements_added.add(sectname)\\n                    elif sectname == self._default_section:\\n                        cursect = self._defaults\\n                    else:\\n                        cursect = self._dict()\\n                        cursect['__name__'] = sectname\\n                        self._sections[sectname] = cursect\\n                        self._proxies[sectname] = SectionProxy(self, sectname)\\n                        elements_added.add(sectname)\\n                    optname = None\\n                elif cursect is None:\\n                    raise MissingSectionHeaderError(fpname, lineno, line)\\n                else:\\n                    mo = self._optcre.match(value)\\n                    if mo:\\n                        optname, vi, optval = mo.group('option', 'vi', 'value')\\n                        if not optname:\\n                            e = self._handle_error(e, fpname, lineno, line)\\n                        optname = self.optionxform(optname.rstrip())\\n                        if (self._strict and\\n                            (sectname, optname) in elements_added):\\n                            raise DuplicateOptionError(sectname, optname,\\n                                                       fpname, lineno)\\n                        elements_added.add((sectname, optname))\\n                        if optval is not None:\\n                            optval = optval.strip()\\n                            if optval == '\"\"':\\n                                optval = ''\\n                            cursect[optname] = [optval]\\n                        else:\\n                            cursect[optname] = optval\\n                    else:\\n                        e = self._handle_error(e, fpname, lineno, line)\\n        if e:\\n            raise e\\n        self._join_multiline_values()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read(self, fp, fpname):\\n        elements_added = set()\\n        cursect = None                        \\n        sectname = None\\n        optname = None\\n        lineno = 0\\n        indent_level = 0\\n        e = None                              \\n        for lineno, line in enumerate(fp, start=1):\\n            comment_start = None\\n            for prefix in self._startonly_comment_prefixes:\\n                if line.strip().startswith(prefix):\\n                    comment_start = 0\\n                    break\\n            for prefix in self._comment_prefixes:\\n                index = line.find(prefix)\\n                if index == 0 or (index > 0 and line[index-1].isspace()):\\n                    comment_start = index\\n                    break\\n            value = line[:comment_start].strip()\\n            if not value:\\n                if self._empty_lines_in_values:\\n                    if (comment_start is None and\\n                        cursect is not None and\\n                        optname and\\n                        cursect[optname] is not None):\\n                        cursect[optname].append('') \\n                else:\\n                    indent_level = sys.maxsize\\n                continue\\n            first_nonspace = self.NONSPACECRE.search(line)\\n            cur_indent_level = first_nonspace.start() if first_nonspace else 0\\n            if (cursect is not None and optname and\\n                cur_indent_level > indent_level):\\n                cursect[optname].append(value)\\n            else:\\n                indent_level = cur_indent_level\\n                mo = self.SECTCRE.match(value)\\n                if mo:\\n                    sectname = mo.group('header')\\n                    if sectname in self._sections:\\n                        if self._strict and sectname in elements_added:\\n                            raise DuplicateSectionError(sectname, fpname,\\n                                                        lineno)\\n                        cursect = self._sections[sectname]\\n                        elements_added.add(sectname)\\n                    elif sectname == self._default_section:\\n                        cursect = self._defaults\\n                    else:\\n                        cursect = self._dict()\\n                        cursect['__name__'] = sectname\\n                        self._sections[sectname] = cursect\\n                        self._proxies[sectname] = SectionProxy(self, sectname)\\n                        elements_added.add(sectname)\\n                    optname = None\\n                elif cursect is None:\\n                    raise MissingSectionHeaderError(fpname, lineno, line)\\n                else:\\n                    mo = self._optcre.match(value)\\n                    if mo:\\n                        optname, vi, optval = mo.group('option', 'vi', 'value')\\n                        if not optname:\\n                            e = self._handle_error(e, fpname, lineno, line)\\n                        optname = self.optionxform(optname.rstrip())\\n                        if (self._strict and\\n                            (sectname, optname) in elements_added):\\n                            raise DuplicateOptionError(sectname, optname,\\n                                                       fpname, lineno)\\n                        elements_added.add((sectname, optname))\\n                        if optval is not None:\\n                            optval = optval.strip()\\n                            if optval == '\"\"':\\n                                optval = ''\\n                            cursect[optname] = [optval]\\n                        else:\\n                            cursect[optname] = optval\\n                    else:\\n                        e = self._handle_error(e, fpname, lineno, line)\\n        if e:\\n            raise e\\n        self._join_multiline_values()"
  },
  {
    "code": "def to_sci_string(self, a):\\n        a = _convert_other(a, raiseit=True)\\n        return a.__str__(context=self)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_sci_string(self, a):\\n        a = _convert_other(a, raiseit=True)\\n        return a.__str__(context=self)"
  },
  {
    "code": "def make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\\n                            n_features=2, n_classes=3,\\n                            shuffle=True, random_state=None):\\n    if n_samples < n_classes:\\n        raise ValueError(\"n_samples must be at least n_classes\")\\n    generator = check_random_state(random_state)\\n    if mean is None:\\n        mean = np.zeros(n_features)\\n    else:\\n        mean = np.array(mean)\\n    X = generator.multivariate_normal(mean, cov * np.identity(n_features),\\n                                      (n_samples,))\\n    idx = np.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1))\\n    X = X[idx, :]\\n    step = n_samples // n_classes\\n    y = np.hstack([np.repeat(np.arange(n_classes), step),\\n                   np.repeat(n_classes - 1, n_samples - step * n_classes)])\\n    if shuffle:\\n        X, y = util_shuffle(X, y, random_state=generator)\\n    return X, y",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\\n                            n_features=2, n_classes=3,\\n                            shuffle=True, random_state=None):\\n    if n_samples < n_classes:\\n        raise ValueError(\"n_samples must be at least n_classes\")\\n    generator = check_random_state(random_state)\\n    if mean is None:\\n        mean = np.zeros(n_features)\\n    else:\\n        mean = np.array(mean)\\n    X = generator.multivariate_normal(mean, cov * np.identity(n_features),\\n                                      (n_samples,))\\n    idx = np.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1))\\n    X = X[idx, :]\\n    step = n_samples // n_classes\\n    y = np.hstack([np.repeat(np.arange(n_classes), step),\\n                   np.repeat(n_classes - 1, n_samples - step * n_classes)])\\n    if shuffle:\\n        X, y = util_shuffle(X, y, random_state=generator)\\n    return X, y"
  },
  {
    "code": "def insert_mode(page_html):\\n\\tpage_html = page_html\\n\\tmatch_found = False\\n\\tmatches = re.finditer('workerId={{ workerid }}', page_html)\\n\\tmatch = None\\n\\tfor match in matches:\\n\\t\\tmatch_found = True\\n\\tif match_found:\\n\\t\\tnew_html = page_html[:match.end()] + '&mode={{ mode }}' +\\\\n\\t\\t\\tpage_html[match.end():]\\n\\t\\treturn new_html\\n\\telse:\\n\\t\\traise ExperimentError(\"insert_mode_failed\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def insert_mode(page_html):\\n\\tpage_html = page_html\\n\\tmatch_found = False\\n\\tmatches = re.finditer('workerId={{ workerid }}', page_html)\\n\\tmatch = None\\n\\tfor match in matches:\\n\\t\\tmatch_found = True\\n\\tif match_found:\\n\\t\\tnew_html = page_html[:match.end()] + '&mode={{ mode }}' +\\\\n\\t\\t\\tpage_html[match.end():]\\n\\t\\treturn new_html\\n\\telse:\\n\\t\\traise ExperimentError(\"insert_mode_failed\")"
  },
  {
    "code": "def _build_tree(is_classification, features, labels, criterion,\\n               max_depth, min_split, F, K, random_state):\\n    n_samples, n_dims = features.shape\\n    if len(labels) != len(features):\\n        raise ValueError(\"Number of labels=%d does not match \"\\n                          \"number of features=%d\\n\"\\n                         % (len(labels), len(features)))\\n    labels = np.array(labels, dtype=np.float64, order=\"c\")\\n    feature_mask = np.ones(n_dims, dtype=np.bool)\\n    sample_dims = np.arange(n_dims)\\n    if F is not None:\\n        if F <= 0 or F > n_dims:\\n            raise ValueError(\"F=%d must be in range (0..%d].\\n\"\\n                             \"Did you mean to use None to signal no F?\"\\n                             % (F, n_dims))\\n        permutation = random_state.permutation(n_dims)\\n        sample_dims = np.sort(permutation[-F:])\\n        feature_mask[sample_dims] = False\\n        feature_mask = np.logical_not(feature_mask)\\n    features = features[:, feature_mask]\\n    if not features.flags[\"F_CONTIGUOUS\"]:\\n        features = np.array(features, order=\"F\")\\n    if min_split <= 0:\\n        raise ValueError(\"min_split must be greater than zero.\\n\"\\n                         \"min_split is %s.\" % min_split)\\n    if max_depth <= 0:\\n        raise ValueError(\"max_depth must be greater than zero.\\n\"\\n                         \"max_depth is %s.\" % max_depth)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_tree(is_classification, features, labels, criterion,\\n               max_depth, min_split, F, K, random_state):\\n    n_samples, n_dims = features.shape\\n    if len(labels) != len(features):\\n        raise ValueError(\"Number of labels=%d does not match \"\\n                          \"number of features=%d\\n\"\\n                         % (len(labels), len(features)))\\n    labels = np.array(labels, dtype=np.float64, order=\"c\")\\n    feature_mask = np.ones(n_dims, dtype=np.bool)\\n    sample_dims = np.arange(n_dims)\\n    if F is not None:\\n        if F <= 0 or F > n_dims:\\n            raise ValueError(\"F=%d must be in range (0..%d].\\n\"\\n                             \"Did you mean to use None to signal no F?\"\\n                             % (F, n_dims))\\n        permutation = random_state.permutation(n_dims)\\n        sample_dims = np.sort(permutation[-F:])\\n        feature_mask[sample_dims] = False\\n        feature_mask = np.logical_not(feature_mask)\\n    features = features[:, feature_mask]\\n    if not features.flags[\"F_CONTIGUOUS\"]:\\n        features = np.array(features, order=\"F\")\\n    if min_split <= 0:\\n        raise ValueError(\"min_split must be greater than zero.\\n\"\\n                         \"min_split is %s.\" % min_split)\\n    if max_depth <= 0:\\n        raise ValueError(\"max_depth must be greater than zero.\\n\"\\n                         \"max_depth is %s.\" % max_depth)"
  },
  {
    "code": "def worker(args):\\n    env = os.environ.copy()\\n    env['AIRFLOW_HOME'] = settings.AIRFLOW_HOME\\n    if not settings.validate_session():\\n        log = LoggingMixin().log\\n        log.error(\"Worker exiting... database connection precheck failed! \")\\n        sys.exit(1)\\n    from airflow.executors.celery_executor import app as celery_app\\n    from celery.bin import worker\\n    autoscale = args.autoscale\\n    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):\\n        autoscale = conf.get(\"celery\", \"worker_autoscale\")\\n    worker = worker.worker(app=celery_app)\\n    options = {\\n        'optimization': 'fair',\\n        'O': 'fair',\\n        'queues': args.queues,\\n        'concurrency': args.concurrency,\\n        'autoscale': autoscale,\\n        'hostname': args.celery_hostname,\\n        'loglevel': conf.get('core', 'LOGGING_LEVEL'),\\n    }\\n    if conf.has_option(\"celery\", \"pool\"):\\n        options[\"pool\"] = conf.get(\"celery\", \"pool\")\\n    if args.daemon:\\n        pid, stdout, stderr, log_file = setup_locations(\"worker\",\\n                                                        args.pid,\\n                                                        args.stdout,\\n                                                        args.stderr,\\n                                                        args.log_file)\\n        handle = setup_logging(log_file)\\n        stdout = open(stdout, 'w+')\\n        stderr = open(stderr, 'w+')\\n        ctx = daemon.DaemonContext(\\n            pidfile=TimeoutPIDLockFile(pid, -1),\\n            files_preserve=[handle],\\n            stdout=stdout,\\n            stderr=stderr,\\n        )\\n        with ctx:\\n            sp = subprocess.Popen(['airflow', 'serve_logs'], env=env, close_fds=True)\\n            worker.run(**options)\\n            sp.kill()\\n        stdout.close()\\n        stderr.close()\\n    else:\\n        signal.signal(signal.SIGINT, sigint_handler)\\n        signal.signal(signal.SIGTERM, sigint_handler)\\n        sp = subprocess.Popen(['airflow', 'serve_logs'], env=env, close_fds=True)\\n        worker.run(**options)\\n        sp.kill()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def worker(args):\\n    env = os.environ.copy()\\n    env['AIRFLOW_HOME'] = settings.AIRFLOW_HOME\\n    if not settings.validate_session():\\n        log = LoggingMixin().log\\n        log.error(\"Worker exiting... database connection precheck failed! \")\\n        sys.exit(1)\\n    from airflow.executors.celery_executor import app as celery_app\\n    from celery.bin import worker  \\n    autoscale = args.autoscale\\n    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):\\n        autoscale = conf.get(\"celery\", \"worker_autoscale\")\\n    worker = worker.worker(app=celery_app)   \\n    options = {\\n        'optimization': 'fair',\\n        'O': 'fair',\\n        'queues': args.queues,\\n        'concurrency': args.concurrency,\\n        'autoscale': autoscale,\\n        'hostname': args.celery_hostname,\\n        'loglevel': conf.get('core', 'LOGGING_LEVEL'),\\n    }\\n    if conf.has_option(\"celery\", \"pool\"):\\n        options[\"pool\"] = conf.get(\"celery\", \"pool\")\\n    if args.daemon:\\n        pid, stdout, stderr, log_file = setup_locations(\"worker\",\\n                                                        args.pid,\\n                                                        args.stdout,\\n                                                        args.stderr,\\n                                                        args.log_file)\\n        handle = setup_logging(log_file)\\n        stdout = open(stdout, 'w+')\\n        stderr = open(stderr, 'w+')\\n        ctx = daemon.DaemonContext(\\n            pidfile=TimeoutPIDLockFile(pid, -1),\\n            files_preserve=[handle],\\n            stdout=stdout,\\n            stderr=stderr,\\n        )\\n        with ctx:\\n            sub_proc = subprocess.Popen(['airflow', 'serve_logs'], env=env, close_fds=True)\\n            worker.run(**options)\\n            sub_proc.kill()\\n        stdout.close()\\n        stderr.close()\\n    else:\\n        signal.signal(signal.SIGINT, sigint_handler)\\n        signal.signal(signal.SIGTERM, sigint_handler)\\n        sub_proc = subprocess.Popen(['airflow', 'serve_logs'], env=env, close_fds=True)\\n        worker.run(**options)\\n        sub_proc.kill()"
  },
  {
    "code": "def __init__(  \\n        self,\\n        connections_path: str,\\n        url: Optional[str] = None,\\n        auth_type: str = 'token',\\n        mount_point: str = 'secret',\\n        kv_engine_version: int = 2,\\n        token: Optional[str] = None,\\n        username: Optional[str] = None,\\n        password: Optional[str] = None,\\n        role_id: Optional[str] = None,\\n        secret_id: Optional[str] = None,\\n        gcp_key_path: Optional[str] = None,\\n        gcp_scopes: Optional[str] = None,\\n        **kwargs\\n    ):\\n        super().__init__(**kwargs)\\n        self.connections_path = connections_path.rstrip('/')\\n        self.url = url\\n        self.auth_type = auth_type\\n        self.kwargs = kwargs\\n        self.token = token\\n        self.username = username\\n        self.password = password\\n        self.role_id = role_id\\n        self.secret_id = secret_id\\n        self.mount_point = mount_point\\n        self.kv_engine_version = kv_engine_version\\n        self.gcp_key_path = gcp_key_path\\n        self.gcp_scopes = gcp_scopes\\n    @cached_property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Get Airflow Variables from Hashicorp Vault (#7944)",
    "fixed_code": "def __init__(  \\n        self,\\n        connections_path: str = 'connections',\\n        variables_path: str = 'variables',\\n        url: Optional[str] = None,\\n        auth_type: str = 'token',\\n        mount_point: str = 'secret',\\n        kv_engine_version: int = 2,\\n        token: Optional[str] = None,\\n        username: Optional[str] = None,\\n        password: Optional[str] = None,\\n        role_id: Optional[str] = None,\\n        secret_id: Optional[str] = None,\\n        gcp_key_path: Optional[str] = None,\\n        gcp_scopes: Optional[str] = None,\\n        **kwargs\\n    ):\\n        super().__init__(**kwargs)\\n        self.connections_path = connections_path.rstrip('/')\\n        self.variables_path = variables_path.rstrip('/')\\n        self.url = url\\n        self.auth_type = auth_type\\n        self.kwargs = kwargs\\n        self.token = token\\n        self.username = username\\n        self.password = password\\n        self.role_id = role_id\\n        self.secret_id = secret_id\\n        self.mount_point = mount_point\\n        self.kv_engine_version = kv_engine_version\\n        self.gcp_key_path = gcp_key_path\\n        self.gcp_scopes = gcp_scopes\\n    @cached_property"
  },
  {
    "code": "def _get_na_values(col, na_values):\\n    if isinstance(na_values, dict):\\n        if col in na_values:\\n            return set(list(na_values[col]))\\n        else:\\n            return _NA_VALUES\\n    else:\\n        return na_values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: (GH3611) Fix read_csv to correctly encode identical na_values, e.g. na_values=[-999.0,-999] was failing",
    "fixed_code": "def _get_na_values(col, na_values):\\n    if isinstance(na_values, dict):\\n        if col in na_values:\\n            return set(_stringify_na_values(list(na_values[col])))\\n        else:\\n            return _NA_VALUES\\n    else:\\n        return na_values"
  },
  {
    "code": "def create_axes(self, axes_to_index, obj, validate = True, min_itemsize = None):\\n        self.index_axes     = []\\n        self.non_index_axes = []\\n        j = 0\\n        for i, a in enumerate(obj.axes):\\n            if i in axes_to_index:\\n                name = obj._AXIS_NAMES[i]\\n                self.index_axes.append(_convert_index(a).set_name(name).set_axis(i).set_pos(j))\\n                j += 1\\n            else:\\n                self.non_index_axes.append((i,list(a)))\\n        if validate:\\n            for a in self.axes:\\n                a.maybe_set_size(min_itemsize = min_itemsize)\\n        self.values_axes = []\\n        for i, b in enumerate(obj._data.blocks):\\n            values = b.values\\n            if b.dtype.name == 'object':\\n                atom  = _tables().StringCol(itemsize = values.dtype.itemsize, shape = b.shape[0])\\n                utype = 'S8'\\n            else:\\n                atom  = getattr(_tables(),\"%sCol\" % b.dtype.name.capitalize())(shape = b.shape[0])\\n                utype = atom._deftype\\n            try:\\n                values = values.astype(utype)\\n            except (Exception), detail:\\n                raise Exception(\"cannot coerce data type -> [dtype->%s]\" % b.dtype.name)\\n            dc = DataCol.create_for_block(i = i, values = list(b.items), kind = b.dtype.name, typ = atom, data = values, pos = j)\\n            j += 1\\n            self.values_axes.append(dc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: ndim tables in HDFStore      changes axes_to_index keyword in table creation to axes      allow passing of numeric or named axes (e.g. 0 or 'minor_axis') in axes      create_axes now checks for current table scheme; raises if this indexing scheme is violated      added many p4d tests for appending/selection/partial selection/and axis permuation      added addition Term tests to include p4d      add __eq__ operators to IndexCol/DataCol/Table to comparisons      updated docs with Panel4D saving & issues relating to threading      supporting non-regular indexables:        e.g. can index a Panel4D on say [labels,major_axis,minor_axis], rather             than the default of [items,major_axis,minor_axis]      support column oriented DataFrames (e.g. queryable by the columns)",
    "fixed_code": "def create_axes(self, axes, obj, validate = True, min_itemsize = None):\\n        axes = set([ obj._get_axis_number(a) for a in axes ])\\n        if self.infer_axes():\\n            existing_table = self.copy()\\n            axes = [ a.axis for a in existing_table.index_axes]\\n        else:\\n            existing_table = None\\n        if len(axes) != self.ndim-1:\\n            raise Exception(\"currenctly only support ndim-1 indexers in an AppendableTable\")\\n        self.index_axes       = []\\n        self.non_index_axes   = []\\n        j = 0\\n        for i, a in enumerate(obj.axes):\\n            if i in axes:\\n                name = obj._AXIS_NAMES[i]\\n                self.index_axes.append(_convert_index(a).set_name(name).set_axis(i).set_pos(j))\\n                j += 1\\n            else:\\n                self.non_index_axes.append((i,list(a)))\\n        if validate:\\n            for a in self.axes:\\n                a.maybe_set_size(min_itemsize = min_itemsize)"
  },
  {
    "code": "def _check_generic_foreign_key_existence(self):\\n        target = self.rel.to\\n        if isinstance(target, ModelBase):\\n            fields = target._meta.virtual_fields\\n            if any(isinstance(field, GenericForeignKey) and\\n                    field.ct_field == self.content_type_field_name and\\n                    field.fk_field == self.object_id_field_name\\n                    for field in fields):\\n                return []\\n            else:\\n                return [\\n                    checks.Warning(\\n                        ('The field defines a generic relation with the model '\\n                         '%s.%s, but the model lacks GenericForeignKey.') % (\\n                            target._meta.app_label, target._meta.object_name\\n                        ),\\n                        hint=None,\\n                        obj=self,\\n                        id='contenttypes.E004',\\n                    )\\n                ]\\n        else:\\n            return []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Cleanup of contrib.contenttypes check messages.",
    "fixed_code": "def _check_generic_foreign_key_existence(self):\\n        target = self.rel.to\\n        if isinstance(target, ModelBase):\\n            fields = target._meta.virtual_fields\\n            if any(isinstance(field, GenericForeignKey) and\\n                    field.ct_field == self.content_type_field_name and\\n                    field.fk_field == self.object_id_field_name\\n                    for field in fields):\\n                return []\\n            else:\\n                return [\\n                    checks.Warning(\\n                        (\"The GenericRelation defines a relation with the model \"\\n                         \"'%s.%s', but that model does not have a GenericForeignKey.\") % (\\n                            target._meta.app_label, target._meta.object_name\\n                        ),\\n                        hint=None,\\n                        obj=self,\\n                        id='contenttypes.E004',\\n                    )\\n                ]\\n        else:\\n            return []"
  },
  {
    "code": "def __copy__(self):\\n        result = self.__class__('', mutable=True)\\n        for key, value in dict.items(self):\\n            dict.__setitem__(result, key, value)\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #13572: copies of QueryDicts now have their encoding set correctly.",
    "fixed_code": "def __copy__(self):\\n        result = self.__class__('', mutable=True, encoding=self.encoding)\\n        for key, value in dict.items(self):\\n            dict.__setitem__(result, key, value)\\n        return result"
  },
  {
    "code": "def move(src, dst, copy_function=copy2):\\n    real_dst = dst\\n    if os.path.isdir(dst):\\n        if _samefile(src, dst):\\n            os.rename(src, dst)\\n            return\\n        real_dst = os.path.join(dst, _basename(src))\\n        if os.path.exists(real_dst):\\n            raise Error(\"Destination path '%s' already exists\" % real_dst)\\n    try:\\n        os.rename(src, real_dst)\\n    except OSError:\\n        if os.path.islink(src):\\n            linkto = os.readlink(src)\\n            os.symlink(linkto, real_dst)\\n            os.unlink(src)\\n        elif os.path.isdir(src):\\n            if _destinsrc(src, dst):\\n                raise Error(\"Cannot move a directory '%s' into itself\"\\n                            \" '%s'.\" % (src, dst))\\n            copytree(src, real_dst, copy_function=copy_function,\\n                     symlinks=True)\\n            rmtree(src)\\n        else:\\n            copy_function(src, real_dst)\\n            os.unlink(src)\\n    return real_dst",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def move(src, dst, copy_function=copy2):\\n    real_dst = dst\\n    if os.path.isdir(dst):\\n        if _samefile(src, dst):\\n            os.rename(src, dst)\\n            return\\n        real_dst = os.path.join(dst, _basename(src))\\n        if os.path.exists(real_dst):\\n            raise Error(\"Destination path '%s' already exists\" % real_dst)\\n    try:\\n        os.rename(src, real_dst)\\n    except OSError:\\n        if os.path.islink(src):\\n            linkto = os.readlink(src)\\n            os.symlink(linkto, real_dst)\\n            os.unlink(src)\\n        elif os.path.isdir(src):\\n            if _destinsrc(src, dst):\\n                raise Error(\"Cannot move a directory '%s' into itself\"\\n                            \" '%s'.\" % (src, dst))\\n            copytree(src, real_dst, copy_function=copy_function,\\n                     symlinks=True)\\n            rmtree(src)\\n        else:\\n            copy_function(src, real_dst)\\n            os.unlink(src)\\n    return real_dst"
  },
  {
    "code": "def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,\\n                    init_size=None):\\n    random_state = check_random_state(random_state)\\n    n_samples = X.shape[0]\\n    if init_size is not None and init_size < n_samples:\\n        if init_size < k:\\n            warnings.warn(\\n                \"init_size=%d should be larger than k=%d. \"\\n                \"Setting it to 3*k\" % (init_size, k),\\n                RuntimeWarning, stacklevel=2)\\n            init_size = 3 * k\\n        init_indices = random_state.random_integers(\\n            0, n_samples - 1, init_size)\\n        X = X[init_indices]\\n        x_squared_norms = x_squared_norms[init_indices]\\n        n_samples = X.shape[0]\\n    elif n_samples < k:\\n            raise ValueError(\\n                \"n_samples=%d should be larger than k=%d\" % (n_samples, k))\\n    if init == 'k-means++':\\n        centers = _k_init(X, k, random_state=random_state,\\n                          x_squared_norms=x_squared_norms)\\n    elif init == 'random':\\n        seeds = random_state.permutation(n_samples)[:k]\\n        centers = X[seeds]\\n    elif hasattr(init, '__array__'):\\n        centers = init\\n    elif callable(init):\\n        centers = init(X, k, random_state=random_state)\\n    else:\\n        raise ValueError(\"the init parameter for the k-means should \"\\n                         \"be 'k-means++' or 'random' or an ndarray, \"\\n                         \"'%s' (type '%s') was passed.\" % (init, type(init)))\\n    if sp.issparse(centers):\\n        centers = centers.toarray()\\n    return centers",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: check n_clusters == len(cluster_centers_)",
    "fixed_code": "def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,\\n                    init_size=None):\\n    random_state = check_random_state(random_state)\\n    n_samples = X.shape[0]\\n    if init_size is not None and init_size < n_samples:\\n        if init_size < k:\\n            warnings.warn(\\n                \"init_size=%d should be larger than k=%d. \"\\n                \"Setting it to 3*k\" % (init_size, k),\\n                RuntimeWarning, stacklevel=2)\\n            init_size = 3 * k\\n        init_indices = random_state.random_integers(\\n            0, n_samples - 1, init_size)\\n        X = X[init_indices]\\n        x_squared_norms = x_squared_norms[init_indices]\\n        n_samples = X.shape[0]\\n    elif n_samples < k:\\n            raise ValueError(\\n                \"n_samples=%d should be larger than k=%d\" % (n_samples, k))\\n    if init == 'k-means++':\\n        centers = _k_init(X, k, random_state=random_state,\\n                          x_squared_norms=x_squared_norms)\\n    elif init == 'random':\\n        seeds = random_state.permutation(n_samples)[:k]\\n        centers = X[seeds]\\n    elif hasattr(init, '__array__'):\\n        centers = init\\n    elif callable(init):\\n        centers = init(X, k, random_state=random_state)\\n    else:\\n        raise ValueError(\"the init parameter for the k-means should \"\\n                         \"be 'k-means++' or 'random' or an ndarray, \"\\n                         \"'%s' (type '%s') was passed.\" % (init, type(init)))\\n    if sp.issparse(centers):\\n        centers = centers.toarray()\\n    if len(centers) != k:\\n        raise ValueError('The shape of the inital centers (%s) '\\n            'does not match the number of clusters %i'\\n            % (centers.shape, k))\\n    return centers"
  },
  {
    "code": "def json_normalize(data, record_path=None, meta=None,\\n                   meta_prefix=None,\\n                   record_prefix=None,\\n                   errors='raise',\\n                   sep='.'):\\n    \"\"\"\\n    \"Normalize\" semi-structured JSON data into a flat table\\n    Parameters\\n    ----------\\n    data : dict or list of dicts\\n        Unserialized JSON objects\\n    record_path : string or list of strings, default None\\n        Path in each object to list of records. If not passed, data will be\\n        assumed to be an array of records\\n    meta : list of paths (string or list of strings), default None\\n        Fields to use as metadata for each record in resulting table\\n    record_prefix : string, default None\\n        If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\n        path to records is ['foo', 'bar']\\n    meta_prefix : string, default None\\n    errors : {'raise', 'ignore'}, default 'raise'\\n        * ignore : will ignore KeyError if keys listed in meta are not\\n        always present\\n        * raise : will raise KeyError if keys listed in meta are not\\n        always present\\n        .. versionadded:: 0.20.0\\n    sep : string, default '.'\\n        Nested records will generate names separated by sep,\\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\\n        .. versionadded:: 0.20.0",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def json_normalize(data, record_path=None, meta=None,\\n                   meta_prefix=None,\\n                   record_prefix=None,\\n                   errors='raise',\\n                   sep='.'):\\n    \"\"\"\\n    \"Normalize\" semi-structured JSON data into a flat table\\n    Parameters\\n    ----------\\n    data : dict or list of dicts\\n        Unserialized JSON objects\\n    record_path : string or list of strings, default None\\n        Path in each object to list of records. If not passed, data will be\\n        assumed to be an array of records\\n    meta : list of paths (string or list of strings), default None\\n        Fields to use as metadata for each record in resulting table\\n    record_prefix : string, default None\\n        If True, prefix records with dotted (?) path, e.g. foo.bar.field if\\n        path to records is ['foo', 'bar']\\n    meta_prefix : string, default None\\n    errors : {'raise', 'ignore'}, default 'raise'\\n        * ignore : will ignore KeyError if keys listed in meta are not\\n        always present\\n        * raise : will raise KeyError if keys listed in meta are not\\n        always present\\n        .. versionadded:: 0.20.0\\n    sep : string, default '.'\\n        Nested records will generate names separated by sep,\\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\\n        .. versionadded:: 0.20.0"
  },
  {
    "code": "def reshape(self, *args, **kwargs):\\n        if len(args) == 1 and hasattr(args[0], '__iter__'):\\n            shape = args[0]\\n        else:\\n            shape = args\\n        if tuple(shape) == self.shape:\\n            return self\\n        return self.values.reshape(shape, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reshape(self, *args, **kwargs):\\n        if len(args) == 1 and hasattr(args[0], '__iter__'):\\n            shape = args[0]\\n        else:\\n            shape = args\\n        if tuple(shape) == self.shape:\\n            return self\\n        return self.values.reshape(shape, **kwargs)"
  },
  {
    "code": "def fromkeys(cls, iterable, v=None):\\n        raise NotImplementedError(\\n            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Backport PEP 372: OrderedDict()",
    "fixed_code": "def fromkeys(cls, iterable, value=None):\\n        d = cls()\\n        for key in iterable:\\n            d[key] = value\\n        return d"
  },
  {
    "code": "def is_skipped(self):\\n        if 'results' in self._result and self._task.loop:\\n            results = self._result['results']\\n            return results and all(isinstance(res, dict) and res.get('skipped', False) for res in results)\\n        else:\\n            return self._result.get('skipped', False)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "add jimi-c's unit test for squashed skip results, tweaked is_skipped() logic to pass",
    "fixed_code": "def is_skipped(self):\\n        if 'results' in self._result and self._task.loop:\\n            results = self._result['results']\\n            if results and all(isinstance(res, dict) and res.get('skipped', False) for res in results):\\n                return True\\n        return self._result.get('skipped', False)"
  },
  {
    "code": "def _unpack_opargs(code):\\n    extended_arg = 0\\n    caches = 0\\n    for i in range(0, len(code), 2):\\n        if caches:\\n            caches -= 1\\n            continue\\n        op = code[i]\\n        deop = _deoptop(op)\\n        caches = _inline_cache_entries[deop]\\n        if deop >= HAVE_ARGUMENT:\\n            arg = code[i+1] | extended_arg\\n            extended_arg = (arg << 8) if op == EXTENDED_ARG else 0\\n            if extended_arg >= _INT_OVERFLOW:\\n                extended_arg -= 2 * _INT_OVERFLOW\\n        else:\\n            arg = None\\n            extended_arg = 0\\n        yield (i, op, arg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-92932: dis._unpack_opargs should handle EXTENDED_ARG_QUICK (gh-92945)",
    "fixed_code": "def _unpack_opargs(code):\\n    extended_arg = 0\\n    caches = 0\\n    for i in range(0, len(code), 2):\\n        if caches:\\n            caches -= 1\\n            continue\\n        op = code[i]\\n        deop = _deoptop(op)\\n        caches = _inline_cache_entries[deop]\\n        if deop >= HAVE_ARGUMENT:\\n            arg = code[i+1] | extended_arg\\n            extended_arg = (arg << 8) if deop == EXTENDED_ARG else 0\\n            if extended_arg >= _INT_OVERFLOW:\\n                extended_arg -= 2 * _INT_OVERFLOW\\n        else:\\n            arg = None\\n            extended_arg = 0\\n        yield (i, op, arg)"
  },
  {
    "code": "def I(self):\\n        \"'1' if Daylight Savings Time, '0' otherwise.\"\\n        try:\\n            if self.timezone and self.timezone.dst(self.data):\\n                return '1'\\n            else:\\n                return '0'\\n        except Exception:\\n            return ''",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def I(self):\\n        \"'1' if Daylight Savings Time, '0' otherwise.\"\\n        try:\\n            if self.timezone and self.timezone.dst(self.data):\\n                return '1'\\n            else:\\n                return '0'\\n        except Exception:\\n            return ''"
  },
  {
    "code": "def __new__(cls, data, index=None, copy=False, kind='block',\\n                sparse_value=np.NaN):\\n        if isinstance(data, SparseVector):\\n            if index is not None:\\n                assert(len(index) == data.length)\\n        elif isinstance(data, (Series, dict)):\\n            data = Series(data)\\n            if index is None:\\n                index = data.index\\n            data = make_sparse(data.values, kind=kind)\\n        elif isinstance(data, np.ndarray):\\n            data = make_sparse(data, kind=kind)\\n        if index is None:\\n            index = Index(np.arange(data.length))\\n        subarr = np.array(data.values, dtype=np.float64, copy=False)\\n        if index is None:\\n            raise Exception('Index cannot be None!')\\n        if issubclass(subarr.dtype.type, basestring):\\n            subarr = np.array(data, dtype=object, copy=copy)\\n        if index._allDates:\\n            cls = SparseTimeSeries\\n        subarr = subarr.view(cls)\\n        subarr._vector = data\\n        subarr._sparse_value = sparse_value\\n        subarr.index = index\\n        return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "refactored to remove SparseVector class",
    "fixed_code": "def __new__(cls, data, index=None, sparse_index=None, copy=False,\\n                kind='block', sparse_value=None):\\n        if isinstance(data, SparseSeries):\\n            if index is None:\\n                index = data.index\\n            if sparse_value is None:\\n                sparse_index = data.sparse_value\\n            if index is not None:\\n                assert(len(index) == data.length)\\n            values = np.asarray(data)\\n        elif isinstance(data, (Series, dict)):\\n            data = Series(data)\\n            if index is None:\\n                index = data.index\\n            values, sparse_index = make_sparse(data.values, kind=kind)\\n        elif isinstance(data, np.ndarray):\\n            if sparse_index is None:\\n                values, sparse_index = make_sparse(data, kind=kind)\\n            else:\\n                values = data\\n                assert(len(values) == sparse_index.npoints)\\n        if index is None:\\n            index = Index(np.arange(sparse_index.length))\\n        subarr = np.array(values, dtype=np.float64, copy=False)\\n        if index._allDates:\\n            cls = SparseTimeSeries\\n        subarr = subarr.view(cls)\\n        subarr.sparse_index = sparse_index\\n        subarr.sparse_value = sparse_value\\n        subarr.index = index\\n        return subarr"
  },
  {
    "code": "def check_clustering(name, Alg):\\n    X, y = make_blobs(n_samples=50, random_state=1)\\n    X, y = shuffle(X, y, random_state=7)\\n    X = StandardScaler().fit_transform(X)\\n    n_samples, n_features = X.shape\\n    with warnings.catch_warnings(record=True):\\n        alg = Alg()\\n    set_testing_parameters(alg)\\n    if hasattr(alg, \"n_clusters\"):\\n        alg.set_params(n_clusters=3)\\n    set_random_state(alg)\\n    if name == 'AffinityPropagation':\\n        alg.set_params(preference=-100)\\n        alg.set_params(max_iter=100)\\n    alg.fit(X)\\n    alg.fit(X.tolist())\\n    assert_equal(alg.labels_.shape, (n_samples,))\\n    pred = alg.labels_\\n    assert_greater(adjusted_rand_score(pred, y), 0.4)\\n    if name == 'SpectralClustering':\\n        return\\n    set_random_state(alg)\\n    with warnings.catch_warnings(record=True):\\n        pred2 = alg.fit_predict(X)\\n    assert_array_equal(pred, pred2)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_clustering(name, Alg):\\n    X, y = make_blobs(n_samples=50, random_state=1)\\n    X, y = shuffle(X, y, random_state=7)\\n    X = StandardScaler().fit_transform(X)\\n    n_samples, n_features = X.shape\\n    with ignore_warnings(category=DeprecationWarning):\\n        alg = Alg()\\n    set_testing_parameters(alg)\\n    if hasattr(alg, \"n_clusters\"):\\n        alg.set_params(n_clusters=3)\\n    set_random_state(alg)\\n    if name == 'AffinityPropagation':\\n        alg.set_params(preference=-100)\\n        alg.set_params(max_iter=100)\\n    alg.fit(X)\\n    alg.fit(X.tolist())\\n    assert_equal(alg.labels_.shape, (n_samples,))\\n    pred = alg.labels_\\n    assert_greater(adjusted_rand_score(pred, y), 0.4)\\n    if name == 'SpectralClustering':\\n        return\\n    set_random_state(alg)\\n    with warnings.catch_warnings(record=True):\\n        pred2 = alg.fit_predict(X)\\n    assert_array_equal(pred, pred2)"
  },
  {
    "code": "def __init__(self, data=None, index=None, columns=None):\\n        self._series = {}\\n        if data is not None and len(data) > 0:\\n            if index is None:\\n                s = data.values()[0]\\n                if isinstance(s, Series):\\n                    self.index = s.index\\n                else:\\n                    self.index = np.arange(len(s))\\n            else:\\n                self.index = index\\n            for k, v in data.iteritems():\\n                if isinstance(v, Series):\\n                    self._series[k] = v.reindex(self.index)\\n                else:\\n                    self._series[k] = Series(v, index=self.index)\\n        elif index is not None:\\n            self.index = index\\n        else:\\n            self.index = NULL_INDEX\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "loads of cleanup, tests, and DataFrame / DataMatrix API consistency. removed DataFrame.fromDict and fromMatrix methods, incorporated into default constructor",
    "fixed_code": "def __init__(self, data=None, index=None, columns=None, dtype=None):\\n        if isinstance(data, dict):\\n            self._series, self.index = self._initDict(data, index,\\n                                                      columns, dtype)\\n        elif isinstance(data, np.ndarray):\\n            if columns is None:\\n                raise Exception('Must pass column names with ndarray!')\\n            if index is None:\\n                raise Exception('Must pass index with ndarray!')\\n            if data.ndim == 1:\\n                data = data.reshape((len(data), 1))\\n            elif data.ndim != 2:\\n                raise Exception('Must pass 2-d input!')\\n            self._series, self.index = self._initMatrix(data, index,\\n                                                        columns, dtype)\\n        elif data is None:\\n            if index is None:\\n                index = NULL_INDEX\\n            self._series, self.index = {}, index"
  },
  {
    "code": "def _usefields(adict, align):\\n    from multiarray import dtype\\n    try:\\n        names = adict[-1]\\n    except KeyError:\\n        names = None\\n    if names is None:\\n        names, formats, offsets, titles = _makenames_list(adict)\\n    else:\\n        formats = []\\n        offsets = []\\n        titles = []\\n        for name in names:\\n            res = adict[name]\\n            formats.append(res[0])\\n            offsets.append(res[1])\\n            if (len(res) > 2):\\n                titles.append(res[2])\\n            else:\\n                titles.append(None)\\n    return dtype({\"names\" : names,\\n                  \"formats\" : formats,\\n                  \"offsets\" : offsets,\\n                  \"titles\" : titles}, align)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: dtype-struct: Ensure alignment of nested struct dtypes",
    "fixed_code": "def _usefields(adict, align):\\n    from multiarray import dtype\\n    try:\\n        names = adict[-1]\\n    except KeyError:\\n        names = None\\n    if names is None:\\n        names, formats, offsets, titles = _makenames_list(adict, align)\\n    else:\\n        formats = []\\n        offsets = []\\n        titles = []\\n        for name in names:\\n            res = adict[name]\\n            formats.append(res[0])\\n            offsets.append(res[1])\\n            if (len(res) > 2):\\n                titles.append(res[2])\\n            else:\\n                titles.append(None)\\n    return dtype({\"names\" : names,\\n                  \"formats\" : formats,\\n                  \"offsets\" : offsets,\\n                  \"titles\" : titles}, align)"
  },
  {
    "code": "def _box_col_values(self, values: SingleDataManager, loc: int) -> Series:\\n        name = self.columns[loc]\\n        klass = self._constructor_sliced\\n        return klass(values, index=self.index, name=name, fastpath=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _box_col_values(self, values: SingleDataManager, loc: int) -> Series:\\n        name = self.columns[loc]\\n        klass = self._constructor_sliced\\n        return klass(values, index=self.index, name=name, fastpath=True)"
  },
  {
    "code": "def latest(self, *fields, field_name=None):\\n        return self.reverse()._earliest_or_latest(*fields, field_name=field_name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def latest(self, *fields, field_name=None):\\n        return self.reverse()._earliest_or_latest(*fields, field_name=field_name)"
  },
  {
    "code": "def login(request):\\n\\tform = forms.PasswordLoginForm()\\n\\tmagic_form = forms.EmailLoginForm()\\n\\tif request.method == \"POST\":\\n\\t\\tif request.POST.get(\"action\") == \"login\":\\n\\t\\t\\tform = forms.PasswordLoginForm(request.POST)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\treturn _check_2fa(request, form.user)\\n\\t\\telse:\\n\\t\\t\\tmagic_form = forms.EmailLoginForm(request.POST)\\n\\t\\t\\tif magic_form.is_valid():\\n\\t\\t\\t\\tredirect_url = request.GET.get(\"next\")\\n\\t\\t\\t\\tif not _allow_redirect(redirect_url):\\n\\t\\t\\t\\t\\tredirect_url = None\\n\\t\\t\\t\\tif magic_form.user:\\n\\t\\t\\t\\t\\tprofile = Profile.objects.for_user(magic_form.user)\\n\\t\\t\\t\\t\\tprofile.send_instant_login_link(redirect_url=redirect_url)\\n\\t\\t\\t\\tresponse = redirect(\"hc-login-link-sent\")\\n\\t\\t\\t\\tresponse.set_cookie(\"auto-login\", \"1\", max_age=300, httponly=True)\\n\\t\\t\\t\\treturn response\\n\\tif request.user.is_authenticated:\\n\\t\\treturn _redirect_after_login(request)\\n\\tbad_link = request.session.pop(\"bad_link\", None)\\n\\tctx = {\\n\\t\\t\"page\": \"login\",\\n\\t\\t\"form\": form,\\n\\t\\t\"magic_form\": magic_form,\\n\\t\\t\"bad_link\": bad_link,\\n\\t\\t\"registration_open\": settings.REGISTRATION_OPEN,\\n\\t\\t\"support_email\": settings.SUPPORT_EMAIL,\\n\\t}\\n\\treturn render(request, \"accounts/login.html\", ctx)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def login(request):\\n\\tform = forms.PasswordLoginForm()\\n\\tmagic_form = forms.EmailLoginForm()\\n\\tif request.method == \"POST\":\\n\\t\\tif request.POST.get(\"action\") == \"login\":\\n\\t\\t\\tform = forms.PasswordLoginForm(request.POST)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\treturn _check_2fa(request, form.user)\\n\\t\\telse:\\n\\t\\t\\tmagic_form = forms.EmailLoginForm(request.POST)\\n\\t\\t\\tif magic_form.is_valid():\\n\\t\\t\\t\\tredirect_url = request.GET.get(\"next\")\\n\\t\\t\\t\\tif not _allow_redirect(redirect_url):\\n\\t\\t\\t\\t\\tredirect_url = None\\n\\t\\t\\t\\tif magic_form.user:\\n\\t\\t\\t\\t\\tprofile = Profile.objects.for_user(magic_form.user)\\n\\t\\t\\t\\t\\tprofile.send_instant_login_link(redirect_url=redirect_url)\\n\\t\\t\\t\\tresponse = redirect(\"hc-login-link-sent\")\\n\\t\\t\\t\\tresponse.set_cookie(\"auto-login\", \"1\", max_age=300, httponly=True)\\n\\t\\t\\t\\treturn response\\n\\tif request.user.is_authenticated:\\n\\t\\treturn _redirect_after_login(request)\\n\\tbad_link = request.session.pop(\"bad_link\", None)\\n\\tctx = {\\n\\t\\t\"page\": \"login\",\\n\\t\\t\"form\": form,\\n\\t\\t\"magic_form\": magic_form,\\n\\t\\t\"bad_link\": bad_link,\\n\\t\\t\"registration_open\": settings.REGISTRATION_OPEN,\\n\\t\\t\"support_email\": settings.SUPPORT_EMAIL,\\n\\t}\\n\\treturn render(request, \"accounts/login.html\", ctx)"
  },
  {
    "code": "def load(data, variable_manager=None, loader=None, vars=None):\\n        p = Play()\\n        if vars:\\n            p.vars = vars.copy()\\n        return p.load_data(data, variable_manager=variable_manager, loader=loader)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load(data, variable_manager=None, loader=None, vars=None):\\n        p = Play()\\n        if vars:\\n            p.vars = vars.copy()\\n        return p.load_data(data, variable_manager=variable_manager, loader=loader)"
  },
  {
    "code": "def __init__(self, conn_id: str = default_conn_name) -> None:\\n        super().__init__(sdk_client=ContainerInstanceManagementClient, conn_id=conn_id)\\n        self.connection = self.get_conn()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, conn_id: str = default_conn_name) -> None:\\n        super().__init__(sdk_client=ContainerInstanceManagementClient, conn_id=conn_id)\\n        self.connection = self.get_conn()"
  },
  {
    "code": "def _coerce_to_dtypes(result, dtypes):\\n    if len(result) != len(dtypes):\\n        raise AssertionError(\"_coerce_to_dtypes requires equal len arrays\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: refactored locations of timedeltas to core/tseries/timedeltas (from a series of functions in core/common)",
    "fixed_code": "def _coerce_to_dtypes(result, dtypes):\\n    if len(result) != len(dtypes):\\n        raise AssertionError(\"_coerce_to_dtypes requires equal len arrays\")\\n    from pandas.tseries.timedeltas import _coerce_scalar_to_timedelta_type"
  },
  {
    "code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.panel import _slow_pivot\\n        return _slow_pivot(self[index], self[columns], self[values])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests, getXS deprecation warning",
    "fixed_code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.panel import pivot\\n        return pivot(self[index], self[columns], self[values])"
  },
  {
    "code": "def datetime_cast_time_sql(self, sql, params, tzname):\\n\\t\\treturn f\"django_datetime_cast_time({sql}, %s, %s)\", (\\n\\t\\t\\t*params,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def datetime_cast_time_sql(self, sql, params, tzname):\\n\\t\\treturn f\"django_datetime_cast_time({sql}, %s, %s)\", (\\n\\t\\t\\t*params,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)"
  },
  {
    "code": "def __repr__(self):\\n        clsname = self.__class__.__name__\\n        try:\\n            name = self.name\\n        except AttributeError:\\n            return \"<_pyio.{0}>\".format(clsname)\\n        else:\\n            return \"<_pyio.{0} name={1!r}>\".format(clsname, name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #22033: Reprs of most Python implemened classes now contain actual class name instead of hardcoded one.",
    "fixed_code": "def __repr__(self):\\n        modname = self.__class__.__module__\\n        clsname = self.__class__.__qualname__\\n        try:\\n            name = self.name\\n        except AttributeError:\\n            return \"<{}.{}>\".format(modname, clsname)\\n        else:\\n            return \"<{}.{} name={!r}>\".format(modname, clsname, name)"
  },
  {
    "code": "def get_host(self):\\n\\t\\tif 'HTTP_X_FORWARDED_HOST' in self.META:\\n\\t\\t\\thost = self.META['HTTP_X_FORWARDED_HOST']\\n\\t\\telif 'HTTP_HOST' in self.META:\\n\\t\\t\\thost = self.META['HTTP_HOST']\\n\\t\\telse:\\n\\t\\t\\thost = self.META['SERVER_NAME']\\n\\t\\t\\tserver_port = str(self.META['SERVER_PORT'])\\n\\t\\t\\tif server_port != (self.is_secure() and '443' or '80'):\\n\\t\\t\\t\\thost = '%s:%s' % (host, server_port)\\n\\t\\treturn host",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Added protection against spoofing of X_FORWARDED_HOST headers. A security announcement will be made shortly.",
    "fixed_code": "def get_host(self):\\n\\t\\tif settings.USE_X_FORWARDED_HOST and (\\n\\t\\t\\t'HTTP_X_FORWARDED_HOST' in self.META):\\n\\t\\t\\thost = self.META['HTTP_X_FORWARDED_HOST']\\n\\t\\telif 'HTTP_HOST' in self.META:\\n\\t\\t\\thost = self.META['HTTP_HOST']\\n\\t\\telse:\\n\\t\\t\\thost = self.META['SERVER_NAME']\\n\\t\\t\\tserver_port = str(self.META['SERVER_PORT'])\\n\\t\\t\\tif server_port != (self.is_secure() and '443' or '80'):\\n\\t\\t\\t\\thost = '%s:%s' % (host, server_port)\\n\\t\\treturn host"
  },
  {
    "code": "def unique(self):\\n        values = self._values\\n        if not isinstance(values, np.ndarray):\\n            result = values.unique()\\n        else:\\n            result = unique1d(values)\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def unique(self):\\n        values = self._values\\n        if not isinstance(values, np.ndarray):\\n            result = values.unique()\\n        else:\\n            result = unique1d(values)\\n        return result"
  },
  {
    "code": "def _high_bit(value):\\n    return value.bit_length() - 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _high_bit(value):\\n    return value.bit_length() - 1"
  },
  {
    "code": "def _fast_dot(A, B):\\n    if B.shape[0] != A.shape[A.ndim - 1]:  \\n        raise ValueError('matrices are not aligned')\\n    if A.dtype != B.dtype or any(x.dtype not in (np.float32, np.float64)\\n        for x in [A, B]):\\n        warnings.warn('Data must be of same type. Supported types '\\n                      'are 32 and 64 bit float. '\\n                      'Falling back to np.dot.', DataConversionWarning)\\n        return np.dot(A, B)\\n    if ((A.ndim == 1 or B.ndim == 1) or\\n        (min(A.shape) == 1) or (min(B.shape) == 1) or\\n        (A.ndim != 2) or (B.ndim != 2)):\\n        warnings.warn('Data must be 2D with more than one colum / row.'\\n                      'Falling back to np.dot', DataConversionWarning)\\n        return np.dot(A, B)\\n    dot = linalg.get_blas_funcs('gemm', (A, B))\\n    A, trans_a = _impose_f_order(A)\\n    B, trans_b = _impose_f_order(B)\\n    return dot(alpha=1.0, a=A, b=B, trans_a=trans_a, trans_b=trans_b)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fast_dot(A, B):\\n    if B.shape[0] != A.shape[A.ndim - 1]:  \\n        raise ValueError('matrices are not aligned')\\n    if A.dtype != B.dtype or any(x.dtype not in (np.float32, np.float64)\\n        for x in [A, B]):\\n        warnings.warn('Data must be of same type. Supported types '\\n                      'are 32 and 64 bit float. '\\n                      'Falling back to np.dot.', DataConversionWarning)\\n        return np.dot(A, B)\\n    if ((A.ndim == 1 or B.ndim == 1) or\\n        (min(A.shape) == 1) or (min(B.shape) == 1) or\\n        (A.ndim != 2) or (B.ndim != 2)):\\n        warnings.warn('Data must be 2D with more than one colum / row.'\\n                      'Falling back to np.dot', DataConversionWarning)\\n        return np.dot(A, B)\\n    dot = linalg.get_blas_funcs('gemm', (A, B))\\n    A, trans_a = _impose_f_order(A)\\n    B, trans_b = _impose_f_order(B)\\n    return dot(alpha=1.0, a=A, b=B, trans_a=trans_a, trans_b=trans_b)"
  },
  {
    "code": "def merge_provider_server_port_param(self, result, provider):\\n        if self.validate_params('server_port', provider):\\n            result['server_port'] = provider['server_port']\\n        elif self.validate_params('F5_SERVER_PORT', os.environ):\\n            result['server_port'] = os.environ['F5_SERVER_PORT']\\n        else:\\n            result['server_port'] = 443",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def merge_provider_server_port_param(self, result, provider):\\n        if self.validate_params('server_port', provider):\\n            result['server_port'] = provider['server_port']\\n        elif self.validate_params('F5_SERVER_PORT', os.environ):\\n            result['server_port'] = os.environ['F5_SERVER_PORT']\\n        else:\\n            result['server_port'] = 443"
  },
  {
    "code": "def _encode_params(data):\\n\\t\\tif isinstance(data, (str, bytes)):\\n\\t\\t\\treturn to_native_string(data)\\n\\t\\telif hasattr(data, 'read'):\\n\\t\\t\\treturn data\\n\\t\\telif hasattr(data, '__iter__'):\\n\\t\\t\\tresult = []\\n\\t\\t\\tfor k, vs in to_key_val_list(data):\\n\\t\\t\\t\\tif isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\\n\\t\\t\\t\\t\\tvs = [vs]\\n\\t\\t\\t\\tfor v in vs:\\n\\t\\t\\t\\t\\tif v is not None:\\n\\t\\t\\t\\t\\t\\tresult.append(\\n\\t\\t\\t\\t\\t\\t\\t(k.encode('utf-8') if isinstance(k, str) else k,\\n\\t\\t\\t\\t\\t\\t\\t v.encode('utf-8') if isinstance(v, str) else v))\\n\\t\\t\\treturn urlencode(result, doseq=True)\\n\\t\\telse:\\n\\t\\t\\treturn data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _encode_params(data):\\n\\t\\tif isinstance(data, (str, bytes)):\\n\\t\\t\\treturn to_native_string(data)\\n\\t\\telif hasattr(data, 'read'):\\n\\t\\t\\treturn data\\n\\t\\telif hasattr(data, '__iter__'):\\n\\t\\t\\tresult = []\\n\\t\\t\\tfor k, vs in to_key_val_list(data):\\n\\t\\t\\t\\tif isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\\n\\t\\t\\t\\t\\tvs = [vs]\\n\\t\\t\\t\\tfor v in vs:\\n\\t\\t\\t\\t\\tif v is not None:\\n\\t\\t\\t\\t\\t\\tresult.append(\\n\\t\\t\\t\\t\\t\\t\\t(k.encode('utf-8') if isinstance(k, str) else k,\\n\\t\\t\\t\\t\\t\\t\\t v.encode('utf-8') if isinstance(v, str) else v))\\n\\t\\t\\treturn urlencode(result, doseq=True)\\n\\t\\telse:\\n\\t\\t\\treturn data"
  },
  {
    "code": "def main():\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        public_key=dict(),\\n        public_key_contents=dict(),\\n        group=dict(aliases=['role']),\\n        groups=dict(type='list', elements='dict'),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    mutually_exclusive = [('name', 'aggregate'), ('public_key', 'public_key_contents'), ('group', 'groups')]\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection'],\\n                       mutually_exclusive=mutually_exclusive),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(iosxr_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    if (module.params['public_key_contents'] or module.params['public_key']):\\n        if not HAS_B64:\\n            module.fail_json(\\n                msg='library base64 is required but does not appear to be '\\n                    'installed. It can be installed using `pip install base64`'\\n            )\\n        if not HAS_PARAMIKO:\\n            module.fail_json(\\n                msg='library paramiko is required but does not appear to be '\\n                    'installed. It can be installed using `pip install paramiko`'\\n            )\\n    result = {'changed': False, 'warnings': []}\\n    if module.params['password'] and not module.params['configured_password']:\\n        result['warnings'].append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    config_object = None\\n    if is_cliconf(module):\\n        module.deprecate(msg=\"cli support for 'iosxr_user' is deprecated. Use transport netconf instead\",\\n                         version=\"2.9\")\\n        config_object = CliConfiguration(module, result)\\n    elif is_netconf(module):\\n        config_object = NCConfiguration(module, result)\\n    if config_object:\\n        result = config_object.run()\\n    if module.params['public_key_contents'] or module.params['public_key']:\\n        pubkey_object = PublicKeyManager(module, result)\\n        result = pubkey_object.run()\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add option to enter admin configuration mode in iosxr_user (#51430)",
    "fixed_code": "def main():\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        admin=dict(type='bool', default=False),\\n        public_key=dict(),\\n        public_key_contents=dict(),\\n        group=dict(aliases=['role']),\\n        groups=dict(type='list', elements='dict'),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    mutually_exclusive = [('name', 'aggregate'), ('public_key', 'public_key_contents'), ('group', 'groups')]\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection'],\\n                       mutually_exclusive=mutually_exclusive),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(iosxr_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    if (module.params['public_key_contents'] or module.params['public_key']):\\n        if not HAS_B64:\\n            module.fail_json(\\n                msg='library base64 is required but does not appear to be '\\n                    'installed. It can be installed using `pip install base64`'\\n            )\\n        if not HAS_PARAMIKO:\\n            module.fail_json(\\n                msg='library paramiko is required but does not appear to be '\\n                    'installed. It can be installed using `pip install paramiko`'\\n            )\\n    result = {'changed': False, 'warnings': []}\\n    if module.params['password'] and not module.params['configured_password']:\\n        result['warnings'].append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    config_object = None\\n    if is_cliconf(module):\\n        module.deprecate(msg=\"cli support for 'iosxr_user' is deprecated. Use transport netconf instead\",\\n                         version=\"2.9\")\\n        config_object = CliConfiguration(module, result)\\n    elif is_netconf(module):\\n        config_object = NCConfiguration(module, result)\\n    if config_object:\\n        result = config_object.run()\\n    if module.params['public_key_contents'] or module.params['public_key']:\\n        pubkey_object = PublicKeyManager(module, result)\\n        result = pubkey_object.run()\\n    module.exit_json(**result)"
  },
  {
    "code": "def _get_level_number(self, level):\\n        if not isinstance(level, int):\\n            if level != self.name:\\n                raise AssertionError('Level %s must be same as name (%s)'\\n                                     % (level, self.name))\\n            level = 0\\n        return level\\n    @cache_readonly",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: add 'level' kwarg to 'Index.isin' method\\n\\nPERF: optimize MultiIndex.isin when level != None\\n\\nDOC: update doc bits on Index.isin and MultiIndex.isin\\n\\nDOC: merge isin sections for container & index classes\\n\\nDOC: make isin a separate section\\n\\nIt's no longer a subsection of boolean indexing.",
    "fixed_code": "def _get_level_number(self, level):\\n        self._validate_index_level(level)\\n        return 0\\n    @cache_readonly"
  },
  {
    "code": "def filesizeformat(bytes):\\n    try:\\n        bytes = float(bytes)\\n    except (TypeError,ValueError,UnicodeDecodeError):\\n        return u\"0 bytes\"\\n    if bytes < 1024:\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\\n    if bytes < 1024 * 1024:\\n        return ugettext(\"%.1f KB\") % (bytes / 1024)\\n    if bytes < 1024 * 1024 * 1024:\\n        return ugettext(\"%.1f MB\") % (bytes / (1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%.1f GB\") % (bytes / (1024 * 1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%.1f TB\") % (bytes / (1024 * 1024 * 1024 * 1024))\\n    return ugettext(\"%.1f PB\") % (bytes / (1024 * 1024 * 1024 * 1024 * 1024))\\nfilesizeformat.is_safe = True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #14240 -- Enabled localization for the filesize filter. Thanks to David Danier for the report and patch.",
    "fixed_code": "def filesizeformat(bytes):\\n    try:\\n        bytes = float(bytes)\\n    except (TypeError,ValueError,UnicodeDecodeError):\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", 0) % {'size': 0}\\n    filesize_number_format = lambda value: formats.number_format(round(value, 1), 1)\\n    if bytes < 1024:\\n        return ungettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\\n    if bytes < 1024 * 1024:\\n        return ugettext(\"%s KB\") % filesize_number_format(bytes / 1024)\\n    if bytes < 1024 * 1024 * 1024:\\n        return ugettext(\"%s MB\") % filesize_number_format(bytes / (1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%s GB\") % filesize_number_format(bytes / (1024 * 1024 * 1024))\\n    if bytes < 1024 * 1024 * 1024 * 1024 * 1024:\\n        return ugettext(\"%s TB\") % filesize_number_format(bytes / (1024 * 1024 * 1024 * 1024))\\n    return ugettext(\"%s PB\") % filesize_number_format(bytes / (1024 * 1024 * 1024 * 1024 * 1024))\\nfilesizeformat.is_safe = True"
  },
  {
    "code": "def _check_date_fields(year, month, day):\\n    year = _check_int_field(year)\\n    month = _check_int_field(month)\\n    day = _check_int_field(day)\\n    if not MINYEAR <= year <= MAXYEAR:\\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\\n    if not 1 <= month <= 12:\\n        raise ValueError('month must be in 1..12', month)\\n    dim = _days_in_month(year, month)\\n    if not 1 <= day <= dim:\\n        raise ValueError('day must be in 1..%d' % dim, day)\\n    return year, month, day",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-37999: No longer use __int__ in implicit integer conversions. (GH-15636)\\n\\nOnly __index__ should be used to make integer conversions lossless.",
    "fixed_code": "def _check_date_fields(year, month, day):\\n    year = _index(year)\\n    month = _index(month)\\n    day = _index(day)\\n    if not MINYEAR <= year <= MAXYEAR:\\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\\n    if not 1 <= month <= 12:\\n        raise ValueError('month must be in 1..12', month)\\n    dim = _days_in_month(year, month)\\n    if not 1 <= day <= dim:\\n        raise ValueError('day must be in 1..%d' % dim, day)\\n    return year, month, day"
  },
  {
    "code": "def as_sql(self, compiler, connection):\\n\\t\\tlhs, params = compiler.compile(self.lhs)\\n\\t\\treturn '(%s -> %%s)' % lhs, tuple(params) + (self.key_name,)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_sql(self, compiler, connection):\\n\\t\\tlhs, params = compiler.compile(self.lhs)\\n\\t\\treturn '(%s -> %%s)' % lhs, tuple(params) + (self.key_name,)"
  },
  {
    "code": "def _slice_blocks(blocks, slice_obj):\\n    new_blocks = []\\n    for block in blocks:\\n        newb = make_block(block.values[slice_obj], block.ref_locs,\\n                          block.ref_columns, _columns=block._columns)\\n        new_blocks.append(newb)\\n    return new_blocks",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def _slice_blocks(blocks, slice_obj):\\n    new_blocks = []\\n    for block in blocks:\\n        newb = make_block(block.values[slice_obj], block.columns,\\n                          block.ref_columns)\\n        new_blocks.append(newb)\\n    return new_blocks"
  },
  {
    "code": "def detect_task_dependencies(task: Operator) -> Optional['DagDependency']:\\n        if task.task_type == \"TriggerDagRunOperator\":\\n            return DagDependency(\\n                source=task.dag_id,\\n                target=getattr(task, \"trigger_dag_id\"),\\n                dependency_type=\"trigger\",\\n                dependency_id=task.task_id,\\n            )\\n        elif task.task_type == \"ExternalTaskSensor\":\\n            return DagDependency(\\n                source=getattr(task, \"external_dag_id\"),\\n                target=task.dag_id,\\n                dependency_type=\"sensor\",\\n                dependency_id=task.task_id,\\n            )\\n        return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "In DAG dependency detector, use class type instead of class name, 2nd attempt (#21706)",
    "fixed_code": "def detect_task_dependencies(task: Operator) -> Optional['DagDependency']:\\n        if isinstance(task, TriggerDagRunOperator):\\n            return DagDependency(\\n                source=task.dag_id,\\n                target=getattr(task, \"trigger_dag_id\"),\\n                dependency_type=\"trigger\",\\n                dependency_id=task.task_id,\\n            )\\n        elif isinstance(task, ExternalTaskSensor):\\n            return DagDependency(\\n                source=getattr(task, \"external_dag_id\"),\\n                target=task.dag_id,\\n                dependency_type=\"sensor\",\\n                dependency_id=task.task_id,\\n            )\\n        return None"
  },
  {
    "code": "def add_api_websocket_route(\\n\\t\\tself, path: str, endpoint: Callable, name: str = None\\n\\t) -> None:\\n\\t\\troute = APIWebSocketRoute(\\n\\t\\t\\tpath,\\n\\t\\t\\tendpoint=endpoint,\\n\\t\\t\\tname=name,\\n\\t\\t\\tdependency_overrides_provider=self.dependency_overrides_provider,\\n\\t\\t)\\n\\t\\tself.routes.append(route)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def add_api_websocket_route(\\n\\t\\tself, path: str, endpoint: Callable, name: str = None\\n\\t) -> None:\\n\\t\\troute = APIWebSocketRoute(\\n\\t\\t\\tpath,\\n\\t\\t\\tendpoint=endpoint,\\n\\t\\t\\tname=name,\\n\\t\\t\\tdependency_overrides_provider=self.dependency_overrides_provider,\\n\\t\\t)\\n\\t\\tself.routes.append(route)"
  },
  {
    "code": "def to_timestamp(arg, freq=None, tzinfo=None):\\n    if arg is None:\\n        return arg\\n    if type(arg) == float:\\n        raise TypeError(\"Cannot convert a float to datetime\")\\n    if isinstance(arg, basestring):\\n        try:\\n            arg = parser.parse(arg)\\n        except Exception:\\n            pass\\n    return lib.Timestamp(arg, offset=freq, tzinfo=tzinfo)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: delete accumulated datetime cruft",
    "fixed_code": "def to_timestamp(self):\\n        base, mult = _get_freq_code('S')\\n        new_val = self.resample('S', 'S')\\n        return Timestamp(lib.skts_ordinal_to_dt64(new_val, base, mult))\\n    @property"
  },
  {
    "code": "def construct_1d_ndarray_preserving_na(\\n    values: Sequence, dtype: Optional[DtypeObj] = None, copy: bool = False\\n) -> np.ndarray:\\n    if dtype is not None and dtype.kind == \"U\":\\n        subarr = lib.ensure_string_array(values, convert_na_value=False, copy=copy)\\n    else:\\n        if dtype is not None:\\n            _disallow_mismatched_datetimelike(values, dtype)\\n        subarr = np.array(values, dtype=dtype, copy=copy)\\n    return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: construct_1d_ndarray_preserving_na with dt64/td64 and object dtype (#40068)",
    "fixed_code": "def construct_1d_ndarray_preserving_na(\\n    values: Sequence, dtype: Optional[DtypeObj] = None, copy: bool = False\\n) -> np.ndarray:\\n    if dtype is not None and dtype.kind == \"U\":\\n        subarr = lib.ensure_string_array(values, convert_na_value=False, copy=copy)\\n    else:\\n        if dtype is not None:\\n            _disallow_mismatched_datetimelike(values, dtype)\\n        if (\\n            dtype == object\\n            and isinstance(values, np.ndarray)\\n            and values.dtype.kind in [\"m\", \"M\"]\\n        ):\\n            subarr = construct_1d_object_array_from_listlike(list(values))\\n        else:\\n            subarr = np.array(values, dtype=dtype, copy=copy)\\n    return subarr"
  },
  {
    "code": "def _maybe_convert_indices(indices, n):\\n    if isinstance(indices, list):\\n        indices = np.array(indices)\\n    mask = indices < 0\\n    if mask.any():\\n        indices[mask] += n\\n    mask = (indices >= n) | (indices < 0)\\n    if mask.any():\\n        raise IndexError(\"indices are out-of-bounds\")\\n    return indices",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix fancy indexing with empty list",
    "fixed_code": "def _maybe_convert_indices(indices, n):\\n    if isinstance(indices, list):\\n        indices = np.array(indices)\\n        if len(indices) == 0:\\n            return np.empty(0, dtype=np.int_)\\n    mask = indices < 0\\n    if mask.any():\\n        indices[mask] += n\\n    mask = (indices >= n) | (indices < 0)\\n    if mask.any():\\n        raise IndexError(\"indices are out-of-bounds\")\\n    return indices"
  },
  {
    "code": "def lookup_field(name, obj, model_admin=None):\\n    opts = obj._meta\\n    try:\\n        f = _get_non_gfk_field(opts, name)\\n    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):\\n        if callable(name):\\n            attr = name\\n            value = attr(obj)\\n        elif (model_admin is not None and\\n                hasattr(model_admin, name) and\\n                not name == '__str__' and\\n                not name == '__unicode__'):\\n            attr = getattr(model_admin, name)\\n            value = attr(obj)\\n        else:\\n            attr = getattr(obj, name)\\n            if callable(attr):\\n                value = attr()\\n            else:\\n                value = attr\\n        f = None\\n    else:\\n        attr = None\\n        value = getattr(obj, name)\\n    return f, attr, value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def lookup_field(name, obj, model_admin=None):\\n    opts = obj._meta\\n    try:\\n        f = _get_non_gfk_field(opts, name)\\n    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):\\n        if callable(name):\\n            attr = name\\n            value = attr(obj)\\n        elif (model_admin is not None and\\n                hasattr(model_admin, name) and\\n                not name == '__str__' and\\n                not name == '__unicode__'):\\n            attr = getattr(model_admin, name)\\n            value = attr(obj)\\n        else:\\n            attr = getattr(obj, name)\\n            if callable(attr):\\n                value = attr()\\n            else:\\n                value = attr\\n        f = None\\n    else:\\n        attr = None\\n        value = getattr(obj, name)\\n    return f, attr, value"
  },
  {
    "code": "def __init__(\\n        self,\\n        gcp_conn_id: str = \"google_cloud_default\",\\n        delegate_to: Optional[str] = None\\n    ) -> None:\\n        super().__init__(gcp_conn_id=gcp_conn_id, delegate_to=delegate_to)\\n        self._conn = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        gcp_conn_id: str = \"google_cloud_default\",\\n        delegate_to: Optional[str] = None\\n    ) -> None:\\n        super().__init__(gcp_conn_id=gcp_conn_id, delegate_to=delegate_to)\\n        self._conn = None"
  },
  {
    "code": "def _check_params(self):\\n        if self.n_topics is not None:\\n            self._n_components = self.n_topics\\n            warnings.warn(\"n_topics has been renamed to n_components in version 0.19 \"\\n                          \"and will be removed in 0.21\", DeprecationWarning)\\n        else:\\n            self._n_components = self.n_components\\n        if self._n_components <= 0:\\n            raise ValueError(\"Invalid 'n_components' parameter: %r\"\\n                             % self._n_components)\\n        if self.total_samples <= 0:\\n            raise ValueError(\"Invalid 'total_samples' parameter: %r\"\\n                             % self.total_samples)\\n        if self.learning_offset < 0:\\n            raise ValueError(\"Invalid 'learning_offset' parameter: %r\"\\n                             % self.learning_offset)\\n        if self.learning_method not in (\"batch\", \"online\", None):\\n            raise ValueError(\"Invalid 'learning_method' parameter: %r\"\\n                             % self.learning_method)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_params(self):\\n        if self.n_topics is not None:\\n            self._n_components = self.n_topics\\n            warnings.warn(\"n_topics has been renamed to n_components in version 0.19 \"\\n                          \"and will be removed in 0.21\", DeprecationWarning)\\n        else:\\n            self._n_components = self.n_components\\n        if self._n_components <= 0:\\n            raise ValueError(\"Invalid 'n_components' parameter: %r\"\\n                             % self._n_components)\\n        if self.total_samples <= 0:\\n            raise ValueError(\"Invalid 'total_samples' parameter: %r\"\\n                             % self.total_samples)\\n        if self.learning_offset < 0:\\n            raise ValueError(\"Invalid 'learning_offset' parameter: %r\"\\n                             % self.learning_offset)\\n        if self.learning_method not in (\"batch\", \"online\", None):\\n            raise ValueError(\"Invalid 'learning_method' parameter: %r\"\\n                             % self.learning_method)"
  },
  {
    "code": "def __init__(self, parent, action, current_key_sequences):\\n        super().__init__(parent)\\n        self['borderwidth'] = 2\\n        self['relief'] = 'sunken'\\n        self.parent = parent\\n        self.action = action\\n        self.current_key_sequences = current_key_sequences\\n        self.result = ''\\n        self.key_string = StringVar(self)\\n        self.key_string.set('')\\n        self.set_modifiers_for_platform()\\n        self.modifier_vars = []\\n        for modifier in self.modifiers:\\n            variable = StringVar(self)\\n            variable.set('')\\n            self.modifier_vars.append(variable)\\n        self.advanced = False\\n        self.create_widgets()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, parent, action, current_key_sequences):\\n        super().__init__(parent)\\n        self['borderwidth'] = 2\\n        self['relief'] = 'sunken'\\n        self.parent = parent\\n        self.action = action\\n        self.current_key_sequences = current_key_sequences\\n        self.result = ''\\n        self.key_string = StringVar(self)\\n        self.key_string.set('')\\n        self.set_modifiers_for_platform()\\n        self.modifier_vars = []\\n        for modifier in self.modifiers:\\n            variable = StringVar(self)\\n            variable.set('')\\n            self.modifier_vars.append(variable)\\n        self.advanced = False\\n        self.create_widgets()"
  },
  {
    "code": "def state_present(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if value is True:\\n            commands.append(key)\\n        elif value is False:\\n            if key == 'passive-interface default':\\n                if existing_commands.get(key):\\n                    commands.append('no {0}'.format(key))\\n            else:\\n                commands.append('no {0}'.format(key))\\n        elif value == 'default' or value == '':\\n            if key == 'log-adjacency-changes':\\n                commands.append('no {0}'.format(key))\\n            elif existing_commands.get(key):\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n        else:\\n            if key == 'timers throttle lsa':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_lsa_start', proposed),\\n                    get_timer_prd('timer_throttle_lsa_hold', proposed),\\n                    get_timer_prd('timer_throttle_lsa_max', proposed))\\n            elif key == 'timers throttle spf':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_spf_start', proposed),\\n                    get_timer_prd('timer_throttle_spf_hold', proposed),\\n                    get_timer_prd('timer_throttle_spf_max', proposed))\\n            elif key == 'log-adjacency-changes':\\n                if value == 'log':\\n                    command = key\\n                elif value == 'detail':\\n                    command = '{0} {1}'.format(key, value)\\n            elif key == 'auto-cost reference-bandwidth':\\n                if len(value) < 5:\\n                    command = '{0} {1} Mbps'.format(key, value)\\n                else:\\n                    value = str(int(value) / 1000)\\n                    command = '{0} {1} Gbps'.format(key, value)\\n            else:\\n                command = '{0} {1}'.format(key, value.lower())\\n            if command not in commands:\\n                commands.append(command)\\n    if commands:\\n        parents = ['router ospf {0}'.format(module.params['ospf'])]\\n        if module.params['vrf'] != 'default':\\n            parents.append('vrf {0}'.format(module.params['vrf']))\\n        candidate.add(commands, parents=parents)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_ospf_vrf: Add 'bfd' support (#57425)",
    "fixed_code": "def state_present(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if key == 'vrf':\\n            continue\\n        if value is True:\\n            commands.append(key)\\n        elif value is False:\\n            if key == 'passive-interface default':\\n                if existing_commands.get(key):\\n                    commands.append('no {0}'.format(key))\\n            else:\\n                commands.append('no {0}'.format(key))\\n        elif value == 'default' or value == '':\\n            if key == 'log-adjacency-changes':\\n                commands.append('no {0}'.format(key))\\n            elif existing_commands.get(key):\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n        else:\\n            if key == 'timers throttle lsa':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_lsa_start', proposed),\\n                    get_timer_prd('timer_throttle_lsa_hold', proposed),\\n                    get_timer_prd('timer_throttle_lsa_max', proposed))\\n            elif key == 'timers throttle spf':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_spf_start', proposed),\\n                    get_timer_prd('timer_throttle_spf_hold', proposed),\\n                    get_timer_prd('timer_throttle_spf_max', proposed))\\n            elif key == 'log-adjacency-changes':\\n                if value == 'log':\\n                    command = key\\n                elif value == 'detail':\\n                    command = '{0} {1}'.format(key, value)\\n            elif key == 'auto-cost reference-bandwidth':\\n                if len(value) < 5:\\n                    command = '{0} {1} Mbps'.format(key, value)\\n                else:\\n                    value = str(int(value) / 1000)\\n                    command = '{0} {1} Gbps'.format(key, value)\\n            elif key == 'bfd':\\n                command = 'no bfd' if value == 'disable' else 'bfd'\\n            else:\\n                command = '{0} {1}'.format(key, value.lower())\\n            if command not in commands:\\n                commands.append(command)\\n    if commands:\\n        parents = ['router ospf {0}'.format(module.params['ospf'])]\\n        if module.params['vrf'] != 'default':\\n            parents.append('vrf {0}'.format(module.params['vrf']))\\n        candidate.add(commands, parents=parents)"
  },
  {
    "code": "def users_delete(args):\\n    appbuilder = cached_appbuilder()\\n    try:\\n        user = next(u for u in appbuilder.sm.get_all_users()\\n                    if u.username == args.username)\\n    except StopIteration:\\n        raise SystemExit('{} is not a valid user.'.format(args.username))\\n    if appbuilder.sm.del_register_user(user):\\n        print('User {} deleted.'.format(args.username))\\n    else:\\n        raise SystemExit('Failed to delete user.')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def users_delete(args):\\n    appbuilder = cached_appbuilder()\\n    try:\\n        user = next(u for u in appbuilder.sm.get_all_users()\\n                    if u.username == args.username)\\n    except StopIteration:\\n        raise SystemExit('{} is not a valid user.'.format(args.username))\\n    if appbuilder.sm.del_register_user(user):\\n        print('User {} deleted.'.format(args.username))\\n    else:\\n        raise SystemExit('Failed to delete user.')"
  },
  {
    "code": "def __init__(self, query, database, output_location, aws_conn_id='aws_default', client_request_token=None,\\n                 query_execution_context=None, result_configuration=None, sleep_time=30, max_tries=None,\\n                 *args, **kwargs):\\n        super(AWSAthenaOperator, self).__init__(*args, **kwargs)\\n        self.query = query\\n        self.database = database\\n        self.output_location = output_location\\n        self.aws_conn_id = aws_conn_id\\n        self.client_request_token = client_request_token or str(uuid4())\\n        self.query_execution_context = query_execution_context or {}\\n        self.result_configuration = result_configuration or {}\\n        self.sleep_time = sleep_time\\n        self.max_tries = max_tries\\n        self.query_execution_id = None\\n        self.hook = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, query, database, output_location, aws_conn_id='aws_default', client_request_token=None,\\n                 query_execution_context=None, result_configuration=None, sleep_time=30, max_tries=None,\\n                 *args, **kwargs):\\n        super(AWSAthenaOperator, self).__init__(*args, **kwargs)\\n        self.query = query\\n        self.database = database\\n        self.output_location = output_location\\n        self.aws_conn_id = aws_conn_id\\n        self.client_request_token = client_request_token or str(uuid4())\\n        self.query_execution_context = query_execution_context or {}\\n        self.result_configuration = result_configuration or {}\\n        self.sleep_time = sleep_time\\n        self.max_tries = max_tries\\n        self.query_execution_id = None\\n        self.hook = None"
  },
  {
    "code": "def snapmirror_get_iter(self, destination=None):\\n        snapmirror_get_iter = netapp_utils.zapi.NaElement('snapmirror-get-iter')\\n        query = netapp_utils.zapi.NaElement('query')\\n        snapmirror_info = netapp_utils.zapi.NaElement('snapmirror-info')\\n        if destination is None:\\n            destination = self.parameters['destination_path']\\n        snapmirror_info.add_new_child('destination-location', destination)\\n        query.add_child_elem(snapmirror_info)\\n        snapmirror_get_iter.add_child_elem(query)\\n        return snapmirror_get_iter\\n    def snapmirror_get(self, destination=None):\\n        snapmirror_get_iter = self.snapmirror_get_iter(destination)\\n        snap_info = dict()\\n        try:\\n            result = self.server.invoke_successfully(snapmirror_get_iter, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching snapmirror info: %s' % to_native(error),\\n                                  exception=traceback.format_exc())\\n        if result.get_child_by_name('num-records') and \\\\n                int(result.get_child_content('num-records')) > 0:\\n            snapmirror_info = result.get_child_by_name('attributes-list').get_child_by_name(\\n                'snapmirror-info')\\n            snap_info['mirror_state'] = snapmirror_info.get_child_content('mirror-state')\\n            snap_info['status'] = snapmirror_info.get_child_content('relationship-status')\\n            snap_info['schedule'] = snapmirror_info.get_child_content('schedule')\\n            snap_info['policy'] = snapmirror_info.get_child_content('policy')\\n            snap_info['relationship'] = snapmirror_info.get_child_content('relationship-type')\\n            if snapmirror_info.get_child_by_name('max-transfer-rate'):\\n                snap_info['max_transfer_rate'] = int(snapmirror_info.get_child_content('max-transfer-rate'))\\n            if snap_info['schedule'] is None:\\n                snap_info['schedule'] = \"\"\\n            return snap_info\\n        return None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def snapmirror_get_iter(self, destination=None):\\n        snapmirror_get_iter = netapp_utils.zapi.NaElement('snapmirror-get-iter')\\n        query = netapp_utils.zapi.NaElement('query')\\n        snapmirror_info = netapp_utils.zapi.NaElement('snapmirror-info')\\n        if destination is None:\\n            destination = self.parameters['destination_path']\\n        snapmirror_info.add_new_child('destination-location', destination)\\n        query.add_child_elem(snapmirror_info)\\n        snapmirror_get_iter.add_child_elem(query)\\n        return snapmirror_get_iter\\n    def snapmirror_get(self, destination=None):\\n        snapmirror_get_iter = self.snapmirror_get_iter(destination)\\n        snap_info = dict()\\n        try:\\n            result = self.server.invoke_successfully(snapmirror_get_iter, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching snapmirror info: %s' % to_native(error),\\n                                  exception=traceback.format_exc())\\n        if result.get_child_by_name('num-records') and \\\\n                int(result.get_child_content('num-records')) > 0:\\n            snapmirror_info = result.get_child_by_name('attributes-list').get_child_by_name(\\n                'snapmirror-info')\\n            snap_info['mirror_state'] = snapmirror_info.get_child_content('mirror-state')\\n            snap_info['status'] = snapmirror_info.get_child_content('relationship-status')\\n            snap_info['schedule'] = snapmirror_info.get_child_content('schedule')\\n            snap_info['policy'] = snapmirror_info.get_child_content('policy')\\n            snap_info['relationship'] = snapmirror_info.get_child_content('relationship-type')\\n            if snapmirror_info.get_child_by_name('max-transfer-rate'):\\n                snap_info['max_transfer_rate'] = int(snapmirror_info.get_child_content('max-transfer-rate'))\\n            if snap_info['schedule'] is None:\\n                snap_info['schedule'] = \"\"\\n            return snap_info\\n        return None"
  },
  {
    "code": "def run(self, tmp=None, task_vars=None):\\n        socket_path = None\\n        network_os = self._get_network_os(task_vars)\\n        persistent_connection = self._play_context.connection.split('.')[-1]\\n        result = super(ActionModule, self).run(task_vars=task_vars)\\n        if persistent_connection != 'network_cli':\\n            result['failed'] = True\\n            result['msg'] = ('connection type %s is not valid for net_get module,'\\n                             ' please use fully qualified name of network_cli connection type' % self._play_context.connection)\\n            return result\\n        try:\\n            src = self._task.args['src']\\n        except KeyError as exc:\\n            return {'failed': True, 'msg': 'missing required argument: %s' % exc}\\n        dest = self._task.args.get('dest')\\n        if dest is None:\\n            dest = self._get_default_dest(src)\\n        else:\\n            dest = self._handle_dest_path(dest)\\n        proto = self._task.args.get('protocol')\\n        if proto is None:\\n            proto = 'scp'\\n        if socket_path is None:\\n            socket_path = self._connection.socket_path\\n        conn = Connection(socket_path)\\n        sock_timeout = conn.get_option('persistent_command_timeout')\\n        try:\\n            changed = self._handle_existing_file(conn, src, dest, proto, sock_timeout)\\n            if changed is False:\\n                result['changed'] = changed\\n                result['destination'] = dest\\n                return result\\n        except Exception as exc:\\n            result['msg'] = ('Warning: %s idempotency check failed. Check dest' % exc)\\n        try:\\n            conn.get_file(\\n                source=src, destination=dest,\\n                proto=proto, timeout=sock_timeout\\n            )\\n        except Exception as exc:\\n            result['failed'] = True\\n            result['msg'] = 'Exception received: %s' % exc\\n        result['changed'] = changed\\n        result['destination'] = dest\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Clean up flake8 for ansible.netcommon collection (#65842)\\n\\nWe noticed the following flake8 errors when we ran the migration script\\nfor ansible.netcommon.",
    "fixed_code": "def run(self, tmp=None, task_vars=None):\\n        socket_path = None\\n        self._get_network_os(task_vars)\\n        persistent_connection = self._play_context.connection.split('.')[-1]\\n        result = super(ActionModule, self).run(task_vars=task_vars)\\n        if persistent_connection != 'network_cli':\\n            result['failed'] = True\\n            result['msg'] = ('connection type %s is not valid for net_get module,'\\n                             ' please use fully qualified name of network_cli connection type' % self._play_context.connection)\\n            return result\\n        try:\\n            src = self._task.args['src']\\n        except KeyError as exc:\\n            return {'failed': True, 'msg': 'missing required argument: %s' % exc}\\n        dest = self._task.args.get('dest')\\n        if dest is None:\\n            dest = self._get_default_dest(src)\\n        else:\\n            dest = self._handle_dest_path(dest)\\n        proto = self._task.args.get('protocol')\\n        if proto is None:\\n            proto = 'scp'\\n        if socket_path is None:\\n            socket_path = self._connection.socket_path\\n        conn = Connection(socket_path)\\n        sock_timeout = conn.get_option('persistent_command_timeout')\\n        try:\\n            changed = self._handle_existing_file(conn, src, dest, proto, sock_timeout)\\n            if changed is False:\\n                result['changed'] = changed\\n                result['destination'] = dest\\n                return result\\n        except Exception as exc:\\n            result['msg'] = ('Warning: %s idempotency check failed. Check dest' % exc)\\n        try:\\n            conn.get_file(\\n                source=src, destination=dest,\\n                proto=proto, timeout=sock_timeout\\n            )\\n        except Exception as exc:\\n            result['failed'] = True\\n            result['msg'] = 'Exception received: %s' % exc\\n        result['changed'] = changed\\n        result['destination'] = dest\\n        return result"
  },
  {
    "code": "def _concat_date_cols(date_cols):\\n    if len(date_cols) == 1:\\n        return date_cols[0]\\n    return np.array([' '.join(x) for x in zip(*date_cols)], dtype=object)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "put date parsing after individual column inferencing so integers can be parsed better",
    "fixed_code": "def _concat_date_cols(date_cols):\\n    if len(date_cols) == 1:\\n        return date_cols[0]\\n    rs = np.array([' '.join([str(y) for y in x])\\n                   for x in zip(*date_cols)], dtype=object)\\n    return rs"
  },
  {
    "code": "def fq_list_names(partition, list_names):\\n    if list_names is None:\\n        return None\\n    return map(lambda x: fq_name(partition, x), list_names)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fq_list_names(partition, list_names):\\n    if list_names is None:\\n        return None\\n    return map(lambda x: fq_name(partition, x), list_names)"
  },
  {
    "code": "def _asof_function(direction, on_type):\\n    return getattr(_join, 'asof_join_%s_%s' % (direction, on_type), None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _asof_function(direction, on_type):\\n    return getattr(_join, 'asof_join_%s_%s' % (direction, on_type), None)"
  },
  {
    "code": "def validate_one(func_name):\\n    doc = Docstring(func_name)\\n    errs = []\\n    wrns = []\\n    if doc.start_blank_lines != 1:\\n        errs.append(error('GL01'))\\n    if doc.end_blank_lines != 1:\\n        errs.append(error('GL02'))\\n    if doc.double_blank_lines:\\n        errs.append(error('GL03'))\\n    mentioned_errs = doc.mentioned_private_classes\\n    if mentioned_errs:\\n        errs.append(error('GL04',\\n                          mentioned_private_classes=', '.join(mentioned_errs)))\\n    for line in doc.raw_doc.splitlines():\\n        if re.match(\"^ *\\t\", line):\\n            errs.append(error('GL05', line_with_tabs=line.lstrip()))\\n    unseen_sections = list(ALLOWED_SECTIONS)\\n    for section in doc.section_titles:\\n        if section not in ALLOWED_SECTIONS:\\n            errs.append(error('GL06',\\n                              section=section,\\n                              allowed_sections=', '.join(ALLOWED_SECTIONS)))\\n        else:\\n            if section in unseen_sections:\\n                section_idx = unseen_sections.index(section)\\n                unseen_sections = unseen_sections[section_idx + 1:]\\n            else:\\n                section_idx = ALLOWED_SECTIONS.index(section)\\n                goes_before = ALLOWED_SECTIONS[section_idx + 1]\\n                errs.append(error('GL07',\\n                                  sorted_sections=' > '.join(ALLOWED_SECTIONS),\\n                                  wrong_section=section,\\n                                  goes_before=goes_before))\\n                break\\n    if not doc.summary:\\n        errs.append(error('SS01'))\\n    else:\\n        if not doc.summary[0].isupper():\\n            errs.append(error('SS02'))\\n        if doc.summary[-1] != '.':\\n            errs.append(error('SS03'))\\n        if doc.summary != doc.summary.lstrip():\\n            errs.append(error('SS04'))\\n        elif (doc.is_function_or_method\\n                and doc.summary.split(' ')[0][-1] == 's'):\\n            errs.append(error('SS05'))\\n        if doc.num_summary_lines > 1:\\n            errs.append(error('SS06'))\\n    if not doc.extended_summary:\\n        wrns.append(('ES01', 'No extended summary found'))\\n    errs += doc.parameter_mismatches\\n    for param in doc.doc_parameters:\\n        if not param.startswith(\"*\"):  \\n            if not doc.parameter_type(param):\\n                if ':' in param:\\n                    errs.append(error('PR10',\\n                                      param_name=param.split(':')[0]))\\n                else:\\n                    errs.append(error('PR04', param_name=param))\\n            else:\\n                if doc.parameter_type(param)[-1] == '.':\\n                    errs.append(error('PR05', param_name=param))\\n                common_type_errors = [('integer', 'int'),\\n                                      ('boolean', 'bool'),\\n                                      ('string', 'str')]\\n                for wrong_type, right_type in common_type_errors:\\n                    if wrong_type in doc.parameter_type(param):\\n                        errs.append(error('PR06',\\n                                          param_name=param,\\n                                          right_type=right_type,\\n                                          wrong_type=wrong_type))\\n        if not doc.parameter_desc(param):\\n            errs.append(error('PR07', param_name=param))\\n        else:\\n            if not doc.parameter_desc(param)[0].isupper():\\n                errs.append(error('PR08', param_name=param))\\n            if doc.parameter_desc(param)[-1] != '.':\\n                errs.append(error('PR09', param_name=param))\\n    if doc.is_function_or_method:\\n        if not doc.returns and 'return' in doc.method_source:\\n            errs.append(error('RT01'))\\n        if not doc.yields and 'yield' in doc.method_source:\\n            errs.append(error('YD01'))\\n    if not doc.see_also:\\n        wrns.append(error('SA01'))\\n    else:\\n        for rel_name, rel_desc in doc.see_also.items():\\n            if rel_desc:\\n                if not rel_desc.endswith('.'):\\n                    errs.append(error('SA02', reference_name=rel_name))\\n                if not rel_desc[0].isupper():\\n                    errs.append(error('SA03', reference_name=rel_name))\\n            else:\\n                errs.append(error('SA04', reference_name=rel_name))\\n            if rel_name.startswith('pandas.'):\\n                errs.append(error('SA05',\\n                                  reference_name=rel_name,\\n                                  right_reference=rel_name[len('pandas.'):]))\\n    examples_errs = ''\\n    if not doc.examples:\\n        wrns.append(error('EX01'))\\n    else:\\n        examples_errs = doc.examples_errors\\n        if examples_errs:\\n            errs.append(error('EX02', doctest_log=examples_errs))\\n        for err in doc.validate_pep8():\\n            errs.append(error('EX03',\\n                              error_code=err.error_code,\\n                              error_message=err.message,\\n                              times_happening=' ({} times)'.format(err.count)\\n                                              if err.count > 1 else ''))\\n        examples_source_code = ''.join(doc.examples_source_code)\\n        for wrong_import in ('numpy', 'pandas'):\\n            if 'import {}'.format(wrong_import) in examples_source_code:\\n                errs.append(error('EX04', imported_library=wrong_import))\\n    return {'type': doc.type,\\n            'docstring': doc.clean_doc,\\n            'deprecated': doc.deprecated,\\n            'file': doc.source_file_name,\\n            'file_line': doc.source_file_def_line,\\n            'github_link': doc.github_url,\\n            'errors': errs,\\n            'warnings': wrns,\\n            'examples_errors': examples_errs}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: Improve error message to show correct order (#23652)",
    "fixed_code": "def validate_one(func_name):\\n    doc = Docstring(func_name)\\n    errs = []\\n    wrns = []\\n    if doc.start_blank_lines != 1:\\n        errs.append(error('GL01'))\\n    if doc.end_blank_lines != 1:\\n        errs.append(error('GL02'))\\n    if doc.double_blank_lines:\\n        errs.append(error('GL03'))\\n    mentioned_errs = doc.mentioned_private_classes\\n    if mentioned_errs:\\n        errs.append(error('GL04',\\n                          mentioned_private_classes=', '.join(mentioned_errs)))\\n    for line in doc.raw_doc.splitlines():\\n        if re.match(\"^ *\\t\", line):\\n            errs.append(error('GL05', line_with_tabs=line.lstrip()))\\n    unexpected_sections = [section for section in doc.section_titles\\n                           if section not in ALLOWED_SECTIONS]\\n    for section in unexpected_sections:\\n        errs.append(error('GL06',\\n                          section=section,\\n                          allowed_sections=', '.join(ALLOWED_SECTIONS)))\\n    correct_order = [section for section in ALLOWED_SECTIONS\\n                     if section in doc.section_titles]\\n    if correct_order != doc.section_titles:\\n        errs.append(error('GL07',\\n                          correct_sections=', '.join(correct_order)))\\n    if not doc.summary:\\n        errs.append(error('SS01'))\\n    else:\\n        if not doc.summary[0].isupper():\\n            errs.append(error('SS02'))\\n        if doc.summary[-1] != '.':\\n            errs.append(error('SS03'))\\n        if doc.summary != doc.summary.lstrip():\\n            errs.append(error('SS04'))\\n        elif (doc.is_function_or_method\\n                and doc.summary.split(' ')[0][-1] == 's'):\\n            errs.append(error('SS05'))\\n        if doc.num_summary_lines > 1:\\n            errs.append(error('SS06'))\\n    if not doc.extended_summary:\\n        wrns.append(('ES01', 'No extended summary found'))\\n    errs += doc.parameter_mismatches\\n    for param in doc.doc_parameters:\\n        if not param.startswith(\"*\"):  \\n            if not doc.parameter_type(param):\\n                if ':' in param:\\n                    errs.append(error('PR10',\\n                                      param_name=param.split(':')[0]))\\n                else:\\n                    errs.append(error('PR04', param_name=param))\\n            else:\\n                if doc.parameter_type(param)[-1] == '.':\\n                    errs.append(error('PR05', param_name=param))\\n                common_type_errors = [('integer', 'int'),\\n                                      ('boolean', 'bool'),\\n                                      ('string', 'str')]\\n                for wrong_type, right_type in common_type_errors:\\n                    if wrong_type in doc.parameter_type(param):\\n                        errs.append(error('PR06',\\n                                          param_name=param,\\n                                          right_type=right_type,\\n                                          wrong_type=wrong_type))\\n        if not doc.parameter_desc(param):\\n            errs.append(error('PR07', param_name=param))\\n        else:\\n            if not doc.parameter_desc(param)[0].isupper():\\n                errs.append(error('PR08', param_name=param))\\n            if doc.parameter_desc(param)[-1] != '.':\\n                errs.append(error('PR09', param_name=param))\\n    if doc.is_function_or_method:\\n        if not doc.returns and 'return' in doc.method_source:\\n            errs.append(error('RT01'))\\n        if not doc.yields and 'yield' in doc.method_source:\\n            errs.append(error('YD01'))\\n    if not doc.see_also:\\n        wrns.append(error('SA01'))\\n    else:\\n        for rel_name, rel_desc in doc.see_also.items():\\n            if rel_desc:\\n                if not rel_desc.endswith('.'):\\n                    errs.append(error('SA02', reference_name=rel_name))\\n                if not rel_desc[0].isupper():\\n                    errs.append(error('SA03', reference_name=rel_name))\\n            else:\\n                errs.append(error('SA04', reference_name=rel_name))\\n            if rel_name.startswith('pandas.'):\\n                errs.append(error('SA05',\\n                                  reference_name=rel_name,\\n                                  right_reference=rel_name[len('pandas.'):]))\\n    examples_errs = ''\\n    if not doc.examples:\\n        wrns.append(error('EX01'))\\n    else:\\n        examples_errs = doc.examples_errors\\n        if examples_errs:\\n            errs.append(error('EX02', doctest_log=examples_errs))\\n        for err in doc.validate_pep8():\\n            errs.append(error('EX03',\\n                              error_code=err.error_code,\\n                              error_message=err.message,\\n                              times_happening=' ({} times)'.format(err.count)\\n                                              if err.count > 1 else ''))\\n        examples_source_code = ''.join(doc.examples_source_code)\\n        for wrong_import in ('numpy', 'pandas'):\\n            if 'import {}'.format(wrong_import) in examples_source_code:\\n                errs.append(error('EX04', imported_library=wrong_import))\\n    return {'type': doc.type,\\n            'docstring': doc.clean_doc,\\n            'deprecated': doc.deprecated,\\n            'file': doc.source_file_name,\\n            'file_line': doc.source_file_def_line,\\n            'github_link': doc.github_url,\\n            'errors': errs,\\n            'warnings': wrns,\\n            'examples_errors': examples_errs}"
  },
  {
    "code": "def json_normalize(data, record_path=None, meta=None,\\n\\t\\t\\t\\t   meta_prefix=None,\\n\\t\\t\\t\\t   record_prefix=None,\\n\\t\\t\\t\\t   errors='raise',\\n\\t\\t\\t\\t   sep='.'):\\n\\tdef _pull_field(js, spec):\\n\\t\\tresult = js\\n\\t\\tif isinstance(spec, list):\\n\\t\\t\\tfor field in spec:\\n\\t\\t\\t\\tresult = result[field]\\n\\t\\telse:\\n\\t\\t\\tresult = result[spec]\\n\\t\\treturn result\\n\\tif isinstance(data, list) and not data:\\n\\t\\treturn DataFrame()\\n\\tif isinstance(data, dict):\\n\\t\\tdata = [data]\\n\\tif record_path is None:\\n\\t\\tif any([isinstance(x, dict)\\n\\t\\t\\t\\tfor x in compat.itervalues(y)] for y in data):\\n\\t\\t\\tdata = nested_to_record(data, sep=sep)\\n\\t\\treturn DataFrame(data)\\n\\telif not isinstance(record_path, list):\\n\\t\\trecord_path = [record_path]\\n\\tif meta is None:\\n\\t\\tmeta = []\\n\\telif not isinstance(meta, list):\\n\\t\\tmeta = [meta]\\n\\tmeta = [m if isinstance(m, list) else [m] for m in meta]\\n\\trecords = []\\n\\tlengths = []\\n\\tmeta_vals = defaultdict(list)\\n\\tif not isinstance(sep, compat.string_types):\\n\\t\\tsep = str(sep)\\n\\tmeta_keys = [sep.join(val) for val in meta]\\n\\tdef _recursive_extract(data, path, seen_meta, level=0):\\n\\t\\tif len(path) > 1:\\n\\t\\t\\tfor obj in data:\\n\\t\\t\\t\\tfor val, key in zip(meta, meta_keys):\\n\\t\\t\\t\\t\\tif level + 1 == len(val):\\n\\t\\t\\t\\t\\t\\tseen_meta[key] = _pull_field(obj, val[-1])\\n\\t\\t\\t\\t_recursive_extract(obj[path[0]], path[1:],\\n\\t\\t\\t\\t\\t\\t\\t\\t   seen_meta, level=level + 1)\\n\\t\\telse:\\n\\t\\t\\tfor obj in data:\\n\\t\\t\\t\\trecs = _pull_field(obj, path[0])\\n\\t\\t\\t\\tlengths.append(len(recs))\\n\\t\\t\\t\\tfor val, key in zip(meta, meta_keys):\\n\\t\\t\\t\\t\\tif level + 1 > len(val):\\n\\t\\t\\t\\t\\t\\tmeta_val = seen_meta[key]\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tmeta_val = _pull_field(obj, val[level:])\\n\\t\\t\\t\\t\\t\\texcept KeyError as e:\\n\\t\\t\\t\\t\\t\\t\\tif errors == 'ignore':\\n\\t\\t\\t\\t\\t\\t\\t\\tmeta_val = np.nan\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\traise KeyError(\"Try running with \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \"errors='ignore' as key \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \"{err} is not always present\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   .format(err=e))\\n\\t\\t\\t\\t\\tmeta_vals[key].append(meta_val)\\n\\t\\t\\t\\trecords.extend(recs)\\n\\t_recursive_extract(data, record_path, {}, level=0)\\n\\tresult = DataFrame(records)\\n\\tif record_prefix is not None:\\n\\t\\tresult = result.rename(\\n\\t\\t\\tcolumns=lambda x: \"{p}{c}\".format(p=record_prefix, c=x))\\n\\tfor k, v in compat.iteritems(meta_vals):\\n\\t\\tif meta_prefix is not None:\\n\\t\\t\\tk = meta_prefix + k\\n\\t\\tif k in result:\\n\\t\\t\\traise ValueError('Conflicting metadata name {name}, '\\n\\t\\t\\t\\t\\t\\t\\t 'need distinguishing prefix '.format(name=k))\\n\\t\\tresult[k] = np.array(v).repeat(lengths)\\n\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix json_normalize throwing TypeError when record_path has a sequence of dicts #22706 (#22804)",
    "fixed_code": "def json_normalize(data, record_path=None, meta=None,\\n\\t\\t\\t\\t   meta_prefix=None,\\n\\t\\t\\t\\t   record_prefix=None,\\n\\t\\t\\t\\t   errors='raise',\\n\\t\\t\\t\\t   sep='.'):\\n\\tdef _pull_field(js, spec):\\n\\t\\tresult = js\\n\\t\\tif isinstance(spec, list):\\n\\t\\t\\tfor field in spec:\\n\\t\\t\\t\\tresult = result[field]\\n\\t\\telse:\\n\\t\\t\\tresult = result[spec]\\n\\t\\treturn result\\n\\tif isinstance(data, list) and not data:\\n\\t\\treturn DataFrame()\\n\\tif isinstance(data, dict):\\n\\t\\tdata = [data]\\n\\tif record_path is None:\\n\\t\\tif any([isinstance(x, dict)\\n\\t\\t\\t\\tfor x in compat.itervalues(y)] for y in data):\\n\\t\\t\\tdata = nested_to_record(data, sep=sep)\\n\\t\\treturn DataFrame(data)\\n\\telif not isinstance(record_path, list):\\n\\t\\trecord_path = [record_path]\\n\\tif meta is None:\\n\\t\\tmeta = []\\n\\telif not isinstance(meta, list):\\n\\t\\tmeta = [meta]\\n\\tmeta = [m if isinstance(m, list) else [m] for m in meta]\\n\\trecords = []\\n\\tlengths = []\\n\\tmeta_vals = defaultdict(list)\\n\\tif not isinstance(sep, compat.string_types):\\n\\t\\tsep = str(sep)\\n\\tmeta_keys = [sep.join(val) for val in meta]\\n\\tdef _recursive_extract(data, path, seen_meta, level=0):\\n\\t\\tif isinstance(data, dict):\\n\\t\\t\\tdata = [data]\\n\\t\\tif len(path) > 1:\\n\\t\\t\\tfor obj in data:\\n\\t\\t\\t\\tfor val, key in zip(meta, meta_keys):\\n\\t\\t\\t\\t\\tif level + 1 == len(val):\\n\\t\\t\\t\\t\\t\\tseen_meta[key] = _pull_field(obj, val[-1])\\n\\t\\t\\t\\t_recursive_extract(obj[path[0]], path[1:],\\n\\t\\t\\t\\t\\t\\t\\t\\t   seen_meta, level=level + 1)\\n\\t\\telse:\\n\\t\\t\\tfor obj in data:\\n\\t\\t\\t\\trecs = _pull_field(obj, path[0])\\n\\t\\t\\t\\tlengths.append(len(recs))\\n\\t\\t\\t\\tfor val, key in zip(meta, meta_keys):\\n\\t\\t\\t\\t\\tif level + 1 > len(val):\\n\\t\\t\\t\\t\\t\\tmeta_val = seen_meta[key]\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\tmeta_val = _pull_field(obj, val[level:])\\n\\t\\t\\t\\t\\t\\texcept KeyError as e:\\n\\t\\t\\t\\t\\t\\t\\tif errors == 'ignore':\\n\\t\\t\\t\\t\\t\\t\\t\\tmeta_val = np.nan\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\traise KeyError(\"Try running with \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \"errors='ignore' as key \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \"{err} is not always present\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   .format(err=e))\\n\\t\\t\\t\\t\\tmeta_vals[key].append(meta_val)\\n\\t\\t\\t\\trecords.extend(recs)\\n\\t_recursive_extract(data, record_path, {}, level=0)\\n\\tresult = DataFrame(records)\\n\\tif record_prefix is not None:\\n\\t\\tresult = result.rename(\\n\\t\\t\\tcolumns=lambda x: \"{p}{c}\".format(p=record_prefix, c=x))\\n\\tfor k, v in compat.iteritems(meta_vals):\\n\\t\\tif meta_prefix is not None:\\n\\t\\t\\tk = meta_prefix + k\\n\\t\\tif k in result:\\n\\t\\t\\traise ValueError('Conflicting metadata name {name}, '\\n\\t\\t\\t\\t\\t\\t\\t 'need distinguishing prefix '.format(name=k))\\n\\t\\tresult[k] = np.array(v).repeat(lengths)\\n\\treturn result"
  },
  {
    "code": "def andrews_curves(data, class_column, samples=200):\\n    from math import sqrt, pi, sin, cos\\n    import matplotlib.pyplot as plt\\n    import random",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added documentation",
    "fixed_code": "def andrews_curves(data, class_column, ax=None, samples=200):\\n    from math import sqrt, pi, sin, cos\\n    import matplotlib.pyplot as plt\\n    import random"
  },
  {
    "code": "def get_offset_name(offset):\\n    name = _offset_names.get(offset)\\n    if name is not None:\\n        return name\\n    else:\\n        raise ValueError('Bad rule given: %s.' % offset)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_offset_name(offset):\\n    name = _offset_names.get(offset)\\n    if name is not None:\\n        return name\\n    else:\\n        raise ValueError('Bad rule given: %s.' % offset)"
  },
  {
    "code": "def feed_data(self, data):\\n        assert not self._eof, 'feed_data after feed_eof'\\n        if not data:\\n            return\\n        self._buffer.extend(data)\\n        waiter = self._waiter\\n        if waiter is not None:\\n            self._waiter = None\\n            if not waiter.cancelled():\\n                waiter.set_result(False)\\n        if (self._transport is not None and\\n            not self._paused and\\n            len(self._buffer) > 2*self._limit):\\n            try:\\n                self._transport.pause_reading()\\n            except NotImplementedError:\\n                self._transport = None\\n            else:\\n                self._paused = True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def feed_data(self, data):\\n        assert not self._eof, 'feed_data after feed_eof'\\n        if not data:\\n            return\\n        self._buffer.extend(data)\\n        waiter = self._waiter\\n        if waiter is not None:\\n            self._waiter = None\\n            if not waiter.cancelled():\\n                waiter.set_result(False)\\n        if (self._transport is not None and\\n            not self._paused and\\n            len(self._buffer) > 2*self._limit):\\n            try:\\n                self._transport.pause_reading()\\n            except NotImplementedError:\\n                self._transport = None\\n            else:\\n                self._paused = True"
  },
  {
    "code": "def _readmodule(module, path, inpackage=None):\\n    ''\\n    if inpackage is not None:\\n        fullmodule = \"%s.%s\" % (inpackage, module)\\n    else:\\n        fullmodule = module\\n    if fullmodule in _modules:\\n        return _modules[fullmodule]\\n    dict = {}\\n    if module in sys.builtin_module_names and inpackage is None:\\n        _modules[module] = dict\\n        return dict\\n    i = module.rfind('.')\\n    if i >= 0:\\n        package = module[:i]\\n        submodule = module[i+1:]\\n        parent = _readmodule(package, path, inpackage)\\n        if inpackage is not None:\\n            package = \"%s.%s\" % (inpackage, package)\\n        return _readmodule(submodule, parent['__path__'], package)\\n    f = None\\n    if inpackage is not None:\\n        f, fname, (_s, _m, ty) = imp.find_module(module, path)\\n    else:\\n        f, fname, (_s, _m, ty) = imp.find_module(module, path + sys.path)\\n    if ty == imp.PKG_DIRECTORY:\\n        dict['__path__'] = [fname]\\n        path = [fname] + path\\n        f, fname, (_s, _m, ty) = imp.find_module('__init__', [fname])\\n    _modules[fullmodule] = dict\\n    if ty != imp.PY_SOURCE:\\n        f.close()\\n        return dict\\n    stack = [] \\n    g = tokenize.generate_tokens(f.readline)\\n    try:\\n        for tokentype, token, start, _end, _line in g:\\n            if tokentype == DEDENT:\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n            elif token == 'def':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, meth_name, start = g.next()[0:3]\\n                if tokentype != NAME:\\n                    continue \\n                if stack:\\n                    cur_class = stack[-1][0]\\n                    if isinstance(cur_class, Class):\\n                        cur_class._addmethod(meth_name, lineno)\\n                else:\\n                    dict[meth_name] = Function(fullmodule, meth_name,\\n                                               fname, lineno)\\n                stack.append((None, thisindent)) \\n            elif token == 'class':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, class_name, start = g.next()[0:3]\\n                if tokentype != NAME:\\n                    continue \\n                tokentype, token, start = g.next()[0:3]\\n                inherit = None\\n                if token == '(':\\n                    names = [] \\n                    level = 1\\n                    super = [] \\n                    while True:\\n                        tokentype, token, start = g.next()[0:3]\\n                        if token in (')', ',') and level == 1:\\n                            n = \"\".join(super)\\n                            if n in dict:\\n                                n = dict[n]\\n                            else:\\n                                c = n.split('.')\\n                                if len(c) > 1:\\n                                    m = c[-2]\\n                                    c = c[-1]\\n                                    if m in _modules:\\n                                        d = _modules[m]\\n                                        if c in d:\\n                                            n = d[c]\\n                            names.append(n)\\n                            super = []\\n                        if token == '(':\\n                            level += 1\\n                        elif token == ')':\\n                            level -= 1\\n                            if level == 0:\\n                                break\\n                        elif token == ',' and level == 1:\\n                            pass\\n                        elif tokentype in (NAME, OP) and level == 1:\\n                            super.append(token)\\n                    inherit = names\\n                cur_class = Class(fullmodule, class_name, inherit,\\n                                  fname, lineno)\\n                if not stack:\\n                    dict[class_name] = cur_class\\n                stack.append((cur_class, thisindent))\\n            elif token == 'import' and start[1] == 0:\\n                modules = _getnamelist(g)\\n                for mod, _mod2 in modules:\\n                    try:\\n                        if inpackage is None:\\n                            _readmodule(mod, path)\\n                        else:\\n                            try:\\n                                _readmodule(mod, path, inpackage)\\n                            except ImportError:\\n                                _readmodule(mod, [])\\n                    except:\\n                        pass\\n            elif token == 'from' and start[1] == 0:\\n                mod, token = _getname(g)\\n                if not mod or token != \"import\":\\n                    continue\\n                names = _getnamelist(g)\\n                try:\\n                    d = _readmodule(mod, path, inpackage)\\n                except:\\n                    continue\\n                for n, n2 in names:\\n                    if n in d:\\n                        dict[n2 or n] = d[n]\\n                    elif n == '*':\\n                        for n in d:\\n                            if n[0] != '_':\\n                                dict[n] = d[n]\\n    except StopIteration:\\n        pass\\n    f.close()\\n    return dict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#14798: pyclbr now raises ImportError instead of KeyError for missing packages",
    "fixed_code": "def _readmodule(module, path, inpackage=None):\\n    ''\\n    if inpackage is not None:\\n        fullmodule = \"%s.%s\" % (inpackage, module)\\n    else:\\n        fullmodule = module\\n    if fullmodule in _modules:\\n        return _modules[fullmodule]\\n    dict = {}\\n    if module in sys.builtin_module_names and inpackage is None:\\n        _modules[module] = dict\\n        return dict\\n    i = module.rfind('.')\\n    if i >= 0:\\n        package = module[:i]\\n        submodule = module[i+1:]\\n        parent = _readmodule(package, path, inpackage)\\n        if inpackage is not None:\\n            package = \"%s.%s\" % (inpackage, package)\\n        if not '__path__' in parent:\\n            raise ImportError('No package named {}'.format(package))\\n        return _readmodule(submodule, parent['__path__'], package)\\n    f = None\\n    if inpackage is not None:\\n        f, fname, (_s, _m, ty) = imp.find_module(module, path)\\n    else:\\n        f, fname, (_s, _m, ty) = imp.find_module(module, path + sys.path)\\n    if ty == imp.PKG_DIRECTORY:\\n        dict['__path__'] = [fname]\\n        path = [fname] + path\\n        f, fname, (_s, _m, ty) = imp.find_module('__init__', [fname])\\n    _modules[fullmodule] = dict\\n    if ty != imp.PY_SOURCE:\\n        f.close()\\n        return dict\\n    stack = [] \\n    g = tokenize.generate_tokens(f.readline)\\n    try:\\n        for tokentype, token, start, _end, _line in g:\\n            if tokentype == DEDENT:\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n            elif token == 'def':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, meth_name, start = g.next()[0:3]\\n                if tokentype != NAME:\\n                    continue \\n                if stack:\\n                    cur_class = stack[-1][0]\\n                    if isinstance(cur_class, Class):\\n                        cur_class._addmethod(meth_name, lineno)\\n                else:\\n                    dict[meth_name] = Function(fullmodule, meth_name,\\n                                               fname, lineno)\\n                stack.append((None, thisindent)) \\n            elif token == 'class':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, class_name, start = g.next()[0:3]\\n                if tokentype != NAME:\\n                    continue \\n                tokentype, token, start = g.next()[0:3]\\n                inherit = None\\n                if token == '(':\\n                    names = [] \\n                    level = 1\\n                    super = [] \\n                    while True:\\n                        tokentype, token, start = g.next()[0:3]\\n                        if token in (')', ',') and level == 1:\\n                            n = \"\".join(super)\\n                            if n in dict:\\n                                n = dict[n]\\n                            else:\\n                                c = n.split('.')\\n                                if len(c) > 1:\\n                                    m = c[-2]\\n                                    c = c[-1]\\n                                    if m in _modules:\\n                                        d = _modules[m]\\n                                        if c in d:\\n                                            n = d[c]\\n                            names.append(n)\\n                            super = []\\n                        if token == '(':\\n                            level += 1\\n                        elif token == ')':\\n                            level -= 1\\n                            if level == 0:\\n                                break\\n                        elif token == ',' and level == 1:\\n                            pass\\n                        elif tokentype in (NAME, OP) and level == 1:\\n                            super.append(token)\\n                    inherit = names\\n                cur_class = Class(fullmodule, class_name, inherit,\\n                                  fname, lineno)\\n                if not stack:\\n                    dict[class_name] = cur_class\\n                stack.append((cur_class, thisindent))\\n            elif token == 'import' and start[1] == 0:\\n                modules = _getnamelist(g)\\n                for mod, _mod2 in modules:\\n                    try:\\n                        if inpackage is None:\\n                            _readmodule(mod, path)\\n                        else:\\n                            try:\\n                                _readmodule(mod, path, inpackage)\\n                            except ImportError:\\n                                _readmodule(mod, [])\\n                    except:\\n                        pass\\n            elif token == 'from' and start[1] == 0:\\n                mod, token = _getname(g)\\n                if not mod or token != \"import\":\\n                    continue\\n                names = _getnamelist(g)\\n                try:\\n                    d = _readmodule(mod, path, inpackage)\\n                except:\\n                    continue\\n                for n, n2 in names:\\n                    if n in d:\\n                        dict[n2 or n] = d[n]\\n                    elif n == '*':\\n                        for n in d:\\n                            if n[0] != '_':\\n                                dict[n] = d[n]\\n    except StopIteration:\\n        pass\\n    f.close()\\n    return dict"
  },
  {
    "code": "def get_form(self, request, obj=None, **kwargs):\\n        if 'fields' in kwargs:\\n            fields = kwargs.pop('fields')\\n        else:\\n            fields = flatten_fieldsets(self.get_fieldsets(request, obj))\\n        if self.exclude is None:\\n            exclude = []\\n        else:\\n            exclude = list(self.exclude)\\n        exclude.extend(self.get_readonly_fields(request, obj))\\n        if self.exclude is None and hasattr(self.form, '_meta') and self.form._meta.exclude:\\n            exclude.extend(self.form._meta.exclude)\\n        exclude = exclude or None\\n        defaults = {\\n            \"form\": self.form,\\n            \"fields\": fields,\\n            \"exclude\": exclude,\\n            \"formfield_callback\": partial(self.formfield_for_dbfield, request=request),\\n        }\\n        defaults.update(kwargs)\\n        if defaults['fields'] is None and not modelform_defines_fields(defaults['form']):\\n            defaults['fields'] = forms.ALL_FIELDS\\n        try:\\n            return modelform_factory(self.model, **defaults)\\n        except FieldError as e:\\n            raise FieldError('%s. Check fields/fieldsets/exclude attributes of class %s.'\\n                             % (e, self.__class__.__name__))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_form(self, request, obj=None, **kwargs):\\n        if 'fields' in kwargs:\\n            fields = kwargs.pop('fields')\\n        else:\\n            fields = flatten_fieldsets(self.get_fieldsets(request, obj))\\n        if self.exclude is None:\\n            exclude = []\\n        else:\\n            exclude = list(self.exclude)\\n        exclude.extend(self.get_readonly_fields(request, obj))\\n        if self.exclude is None and hasattr(self.form, '_meta') and self.form._meta.exclude:\\n            exclude.extend(self.form._meta.exclude)\\n        exclude = exclude or None\\n        defaults = {\\n            \"form\": self.form,\\n            \"fields\": fields,\\n            \"exclude\": exclude,\\n            \"formfield_callback\": partial(self.formfield_for_dbfield, request=request),\\n        }\\n        defaults.update(kwargs)\\n        if defaults['fields'] is None and not modelform_defines_fields(defaults['form']):\\n            defaults['fields'] = forms.ALL_FIELDS\\n        try:\\n            return modelform_factory(self.model, **defaults)\\n        except FieldError as e:\\n            raise FieldError('%s. Check fields/fieldsets/exclude attributes of class %s.'\\n                             % (e, self.__class__.__name__))"
  },
  {
    "code": "def datatype(self, as_string=False):\\n        dtype = capi.get_band_datatype(self._ptr)\\n        if as_string:\\n            dtype = GDAL_PIXEL_TYPES[dtype]\\n        return dtype\\n    def data(self, data=None, offset=None, size=None, as_memoryview=False):\\n        if not offset:\\n            offset = (0, 0)\\n        if not size:\\n            size = (self.width - offset[0], self.height - offset[1])\\n        if any(x <= 0 for x in size):\\n            raise ValueError('Offset too big for this raster.')\\n        if size[0] > self.width or size[1] > self.height:\\n            raise ValueError('Size is larger than raster.')\\n        ctypes_array = GDAL_TO_CTYPES[self.datatype()] * (size[0] * size[1])\\n        if data is None:\\n            access_flag = 0\\n            data_array = ctypes_array()\\n        else:\\n            access_flag = 1\\n            if isinstance(data, (bytes, six.memoryview)) or (numpy and isinstance(data, numpy.ndarray)):\\n                data_array = ctypes_array.from_buffer_copy(data)\\n            else:\\n                data_array = ctypes_array(*data)\\n        capi.band_io(self._ptr, access_flag, offset[0], offset[1],\\n                     size[0], size[1], byref(data_array), size[0],\\n                     size[1], self.datatype(), 0, 0)\\n        if data is None:\\n            if as_memoryview:\\n                return memoryview(data_array)\\n            elif numpy:\\n                return numpy.frombuffer(\\n                    data_array, dtype=numpy.dtype(data_array)).reshape(size)\\n            else:\\n                return list(data_array)\\n        else:\\n            self.source._flush()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25734 -- Made GDALBand min and max properties use GDALComputeRasterStatistics.\\n\\nThanks Sergey Fedoseev and Tim Graham for the review.",
    "fixed_code": "def datatype(self, as_string=False):\\n        dtype = capi.get_band_datatype(self._ptr)\\n        if as_string:\\n            dtype = GDAL_PIXEL_TYPES[dtype]\\n        return dtype\\n    def data(self, data=None, offset=None, size=None, as_memoryview=False):\\n        if not offset:\\n            offset = (0, 0)\\n        if not size:\\n            size = (self.width - offset[0], self.height - offset[1])\\n        if any(x <= 0 for x in size):\\n            raise ValueError('Offset too big for this raster.')\\n        if size[0] > self.width or size[1] > self.height:\\n            raise ValueError('Size is larger than raster.')\\n        ctypes_array = GDAL_TO_CTYPES[self.datatype()] * (size[0] * size[1])\\n        if data is None:\\n            access_flag = 0\\n            data_array = ctypes_array()\\n        else:\\n            access_flag = 1\\n            if isinstance(data, (bytes, six.memoryview)) or (numpy and isinstance(data, numpy.ndarray)):\\n                data_array = ctypes_array.from_buffer_copy(data)\\n            else:\\n                data_array = ctypes_array(*data)\\n        capi.band_io(self._ptr, access_flag, offset[0], offset[1],\\n                     size[0], size[1], byref(data_array), size[0],\\n                     size[1], self.datatype(), 0, 0)\\n        if data is None:\\n            if as_memoryview:\\n                return memoryview(data_array)\\n            elif numpy:\\n                return numpy.frombuffer(\\n                    data_array, dtype=numpy.dtype(data_array)).reshape(size)\\n            else:\\n                return list(data_array)\\n        else:\\n            self._flush()"
  },
  {
    "code": "def _frozen_get_del_attr(cls, fields, globals):\\n    locals = {'cls': cls,\\n              'FrozenInstanceError': FrozenInstanceError}\\n    if fields:\\n        fields_str = '(' + ','.join(repr(f.name) for f in fields) + ',)'\\n    else:\\n        fields_str = '()'\\n    return (_create_fn('__setattr__',\\n                      ('self', 'name', 'value'),\\n                      (f'if type(self) is cls or name in {fields_str}:',\\n                        ' raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\\n                       f'super(cls, self).__setattr__(name, value)'),\\n                       locals=locals,\\n                       globals=globals),\\n            _create_fn('__delattr__',\\n                      ('self', 'name'),\\n                      (f'if type(self) is cls or name in {fields_str}:',\\n                        ' raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\\n                       f'super(cls, self).__delattr__(name)'),\\n                       locals=locals,\\n                       globals=globals),\\n            )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _frozen_get_del_attr(cls, fields, globals):\\n    locals = {'cls': cls,\\n              'FrozenInstanceError': FrozenInstanceError}\\n    if fields:\\n        fields_str = '(' + ','.join(repr(f.name) for f in fields) + ',)'\\n    else:\\n        fields_str = '()'\\n    return (_create_fn('__setattr__',\\n                      ('self', 'name', 'value'),\\n                      (f'if type(self) is cls or name in {fields_str}:',\\n                        ' raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\\n                       f'super(cls, self).__setattr__(name, value)'),\\n                       locals=locals,\\n                       globals=globals),\\n            _create_fn('__delattr__',\\n                      ('self', 'name'),\\n                      (f'if type(self) is cls or name in {fields_str}:',\\n                        ' raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\\n                       f'super(cls, self).__delattr__(name)'),\\n                       locals=locals,\\n                       globals=globals),\\n            )"
  },
  {
    "code": "def sequence_to_datetimes(data, require_iso8601: bool = False) -> DatetimeArray:\\n    result, tz, freq = _sequence_to_dt64ns(\\n        data,\\n        allow_mixed=True,\\n        require_iso8601=require_iso8601,\\n    )\\n    unit = np.datetime_data(result.dtype)[0]\\n    dtype = tz_to_dtype(tz, unit)\\n    dta = DatetimeArray._simple_new(result, freq=freq, dtype=dtype)\\n    return dta",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sequence_to_datetimes(data, require_iso8601: bool = False) -> DatetimeArray:\\n    result, tz, freq = _sequence_to_dt64ns(\\n        data,\\n        allow_mixed=True,\\n        require_iso8601=require_iso8601,\\n    )\\n    unit = np.datetime_data(result.dtype)[0]\\n    dtype = tz_to_dtype(tz, unit)\\n    dta = DatetimeArray._simple_new(result, freq=freq, dtype=dtype)\\n    return dta"
  },
  {
    "code": "def validate_baseindexer_support(func_name: Optional[str]) -> None:\\n    BASEINDEXER_WHITELIST = {\\n        \"count\",\\n        \"min\",\\n        \"max\",\\n        \"mean\",\\n        \"sum\",\\n        \"median\",\\n        \"std\",\\n        \"var\",\\n        \"skew\",\\n        \"kurt\",\\n        \"quantile\",\\n    }\\n    if isinstance(func_name, str) and func_name not in BASEINDEXER_WHITELIST:\\n        raise NotImplementedError(\\n            f\"{func_name} is not supported with using a BaseIndexer \"\\n            f\"subclasses. You can use .apply() with {func_name}.\"\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate_baseindexer_support(func_name: Optional[str]) -> None:\\n    BASEINDEXER_WHITELIST = {\\n        \"count\",\\n        \"min\",\\n        \"max\",\\n        \"mean\",\\n        \"sum\",\\n        \"median\",\\n        \"std\",\\n        \"var\",\\n        \"skew\",\\n        \"kurt\",\\n        \"quantile\",\\n    }\\n    if isinstance(func_name, str) and func_name not in BASEINDEXER_WHITELIST:\\n        raise NotImplementedError(\\n            f\"{func_name} is not supported with using a BaseIndexer \"\\n            f\"subclasses. You can use .apply() with {func_name}.\"\\n        )"
  },
  {
    "code": "def is_fsspec_url(url: FilePath | BaseBuffer) -> bool:\\n    return (\\n        isinstance(url, str)\\n        and \"://\" in url\\n        and not url.startswith((\"http://\", \"https://\"))\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix issue #36271 - pd.read_json() fails for strings that look similar to fsspec_url (#44619)",
    "fixed_code": "def is_fsspec_url(url: FilePath | BaseBuffer) -> bool:\\n    return (\\n        isinstance(url, str)\\n        and bool(_RFC_3986_PATTERN.match(url))\\n        and not url.startswith((\"http://\", \"https://\"))\\n    )"
  },
  {
    "code": "def _dt_index_cmp(opname, nat_result=False):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _dt_index_cmp(opname, nat_result=False):"
  },
  {
    "code": "def uuid3(namespace, name):\\n    from hashlib import md5\\n    hash = md5(namespace.bytes + bytes(name, \"utf-8\")).digest()\\n    return UUID(bytes=bytes_(hash[:16]), version=3)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Change hashlib to return bytes from digest() instead of str8.",
    "fixed_code": "def uuid3(namespace, name):\\n    from hashlib import md5\\n    hash = md5(namespace.bytes + bytes(name, \"utf-8\")).digest()\\n    return UUID(bytes=hash[:16], version=3)"
  },
  {
    "code": "def hash(self, obj, return_digest=True):\\n        try:\\n            self.dump(obj)\\n        except pickle.PicklingError as e:\\n            warnings.warn('PicklingError while hashing %r: %r' % (obj, e))\\n        dumps = self.stream.getvalue()\\n        self._hash.update(dumps)\\n        if return_digest:\\n            return self._hash.hexdigest()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def hash(self, obj, return_digest=True):\\n        try:\\n            self.dump(obj)\\n        except pickle.PicklingError as e:\\n            warnings.warn('PicklingError while hashing %r: %r' % (obj, e))\\n        dumps = self.stream.getvalue()\\n        self._hash.update(dumps)\\n        if return_digest:\\n            return self._hash.hexdigest()"
  },
  {
    "code": "def as_sql(self, qn, connection):\\n\\t\\tlhs, lhs_params = self.process_lhs(qn, connection)\\n\\t\\trhs, rhs_params = self.process_rhs(qn, connection)\\n\\t\\tparams = tuple(lhs_params) + tuple(rhs_params)\\n\\t\\treturn '%s %s %s' % (lhs, self.operator, rhs), params",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_sql(self, qn, connection):\\n\\t\\tlhs, lhs_params = self.process_lhs(qn, connection)\\n\\t\\trhs, rhs_params = self.process_rhs(qn, connection)\\n\\t\\tparams = tuple(lhs_params) + tuple(rhs_params)\\n\\t\\treturn '%s %s %s' % (lhs, self.operator, rhs), params"
  },
  {
    "code": "def set_acls(context, location_uri, public=False, read_tenants=[],\\n\\t\\t\\t write_tenants=[]):\\n\\tloc = location.get_location_from_uri(location_uri)\\n\\tscheme = get_store_from_location(location_uri)\\n\\tstore = get_store_from_scheme(context, scheme, loc)\\n\\ttry:\\n\\t\\tstore.set_acls(loc, public=public, read_tenants=read_tenants,\\n\\t\\t\\t\\t\\t   write_tenants=write_tenants)\\n\\texcept NotImplementedError:\\n\\t\\tLOG.debug(_(\"Skipping store.set_acls... not implemented.\"))",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Uses None instead of mutables for function param defaults\\n\\nAddressing bug 1307878, changes use of mutable lists and dicts as\\ndefault arguments and defaults them within the function. Otherwise,\\nthose defaults can be unexpectedly persisted with the function between\\ninvocations and erupt into mass hysteria on the streets.\\n\\nTo my knowledge there aren't known cases of the current use causing\\nspecific issues, but needs addressing (even stylistically) to avoid\\nproblems in the future -- ones that may crop up as extremely subtle or\\nintermittent bugs...or worse, security vulnerabilities.\\n\\nIn Glance's case there are ACL-related methods using this, so\\nalthough I haven't confirmed one way or the other yet, I've marked it\\nwith SecurityImpact so that a more knowledgeable set of eyes can\\nreview it in this context as well.\\n\\nCloses-Bug: #1307878\\nSecurityImpact",
    "fixed_code": "def set_acls(context, location_uri, public=False, read_tenants=None,\\n\\t\\t\\t write_tenants=None):\\n\\tif read_tenants is None:\\n\\t\\tread_tenants = []\\n\\tif write_tenants is None:\\n\\t\\twrite_tenants = []\\n\\tloc = location.get_location_from_uri(location_uri)\\n\\tscheme = get_store_from_location(location_uri)\\n\\tstore = get_store_from_scheme(context, scheme, loc)\\n\\ttry:\\n\\t\\tstore.set_acls(loc, public=public, read_tenants=read_tenants,\\n\\t\\t\\t\\t\\t   write_tenants=write_tenants)\\n\\texcept NotImplementedError:\\n\\t\\tLOG.debug(_(\"Skipping store.set_acls... not implemented.\"))"
  },
  {
    "code": "def time_microsecond(self, tz):\\n        self.ts.microsecond",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fastpaths for Timestamp properties (#18539)",
    "fixed_code": "def time_microsecond(self, tz, freq):\\n        self.ts.microsecond"
  },
  {
    "code": "def main(opcode_py, outfile='Include/opcode.h', internaloutfile='Include/internal/pycore_opcode.h'):\\n    opcode = {}\\n    if hasattr(tokenize, 'open'):\\n        fp = tokenize.open(opcode_py)   \\n    else:\\n        fp = open(opcode_py)            \\n    with fp:\\n        code = fp.read()\\n    exec(code, opcode)\\n    opmap = opcode['opmap']\\n    opname = opcode['opname']\\n    hasconst = opcode['hasconst']\\n    hasjrel = opcode['hasjrel']\\n    hasjabs = opcode['hasjabs']\\n    used = [ False ] * 256\\n    next_op = 1\\n    for name, op in opmap.items():\\n        used[op] = True\\n    specialized_opmap = {}\\n    opname_including_specialized = opname.copy()\\n    for name in opcode['_specialized_instructions']:\\n        while used[next_op]:\\n            next_op += 1\\n        specialized_opmap[name] = next_op\\n        opname_including_specialized[next_op] = name\\n        used[next_op] = True\\n    specialized_opmap['DO_TRACING'] = 255\\n    opname_including_specialized[255] = 'DO_TRACING'\\n    used[255] = True\\n    with (open(outfile, 'w') as fobj, open(internaloutfile, 'w') as iobj):\\n        fobj.write(header)\\n        iobj.write(internal_header)\\n        for name in opname:\\n            if name in opmap:\\n                fobj.write(DEFINE.format(name, opmap[name]))\\n            if name == 'POP_EXCEPT': \\n                fobj.write(DEFINE.format(\"HAVE_ARGUMENT\", opcode[\"HAVE_ARGUMENT\"]))\\n        for name, op in specialized_opmap.items():\\n            fobj.write(DEFINE.format(name, op))\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Caches[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Deopt[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Original[256];\\n\")\\n        iobj.write(\"\\n\")\\n        write_int_array_from_ops(\"_PyOpcode_RelativeJump\", opcode['hasjrel'], iobj)\\n        write_int_array_from_ops(\"_PyOpcode_Jump\", opcode['hasjrel'] + opcode['hasjabs'], iobj)\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Caches[256] = {\\n\")\\n        for i, entries in enumerate(opcode[\"_inline_cache_entries\"]):\\n            if entries:\\n                iobj.write(f\"    [{opname[i]}] = {entries},\\n\")\\n        iobj.write(\"};\\n\")\\n        deoptcodes = {}\\n        for basic in opmap:\\n            deoptcodes[basic] = basic\\n        for basic, family in opcode[\"_specializations\"].items():\\n            for specialized in family:\\n                deoptcodes[specialized] = basic\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Deopt[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Original[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            if opt.startswith(\"EXTENDED_ARG\"):\\n                deopt = \"EXTENDED_ARG_QUICK\"\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        fobj.write(\"\\n\")\\n        fobj.write(\"\\n        for op in hasconst:\\n            fobj.write(f\"\\n    || ((op) == {op}) \\\\\")\\n        fobj.write(\"\\n    )\\n\")\\n        fobj.write(\"\\n\")\\n        for i, (op, _) in enumerate(opcode[\"_nb_ops\"]):\\n            fobj.write(DEFINE.format(op, i))\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"static const char *const _PyOpcode_OpName[256] = {\\n\")\\n        for op, name in enumerate(opname_including_specialized):\\n            if name[0] != \"<\":\\n                op = name\\n            iobj.write(f'')\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        for i, flag in enumerate(used):\\n            if not flag:\\n                iobj.write(f\"    case {i}: \\\\\\n\")\\n        iobj.write(\"        ;\\n\")\\n        fobj.write(footer)\\n        iobj.write(internal_footer)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main(opcode_py, outfile='Include/opcode.h', internaloutfile='Include/internal/pycore_opcode.h'):\\n    opcode = {}\\n    if hasattr(tokenize, 'open'):\\n        fp = tokenize.open(opcode_py)   \\n    else:\\n        fp = open(opcode_py)            \\n    with fp:\\n        code = fp.read()\\n    exec(code, opcode)\\n    opmap = opcode['opmap']\\n    opname = opcode['opname']\\n    hasconst = opcode['hasconst']\\n    hasjrel = opcode['hasjrel']\\n    hasjabs = opcode['hasjabs']\\n    used = [ False ] * 256\\n    next_op = 1\\n    for name, op in opmap.items():\\n        used[op] = True\\n    specialized_opmap = {}\\n    opname_including_specialized = opname.copy()\\n    for name in opcode['_specialized_instructions']:\\n        while used[next_op]:\\n            next_op += 1\\n        specialized_opmap[name] = next_op\\n        opname_including_specialized[next_op] = name\\n        used[next_op] = True\\n    specialized_opmap['DO_TRACING'] = 255\\n    opname_including_specialized[255] = 'DO_TRACING'\\n    used[255] = True\\n    with (open(outfile, 'w') as fobj, open(internaloutfile, 'w') as iobj):\\n        fobj.write(header)\\n        iobj.write(internal_header)\\n        for name in opname:\\n            if name in opmap:\\n                fobj.write(DEFINE.format(name, opmap[name]))\\n            if name == 'POP_EXCEPT': \\n                fobj.write(DEFINE.format(\"HAVE_ARGUMENT\", opcode[\"HAVE_ARGUMENT\"]))\\n        for name, op in specialized_opmap.items():\\n            fobj.write(DEFINE.format(name, op))\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Caches[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Deopt[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Original[256];\\n\")\\n        iobj.write(\"\\n\")\\n        write_int_array_from_ops(\"_PyOpcode_RelativeJump\", opcode['hasjrel'], iobj)\\n        write_int_array_from_ops(\"_PyOpcode_Jump\", opcode['hasjrel'] + opcode['hasjabs'], iobj)\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Caches[256] = {\\n\")\\n        for i, entries in enumerate(opcode[\"_inline_cache_entries\"]):\\n            if entries:\\n                iobj.write(f\"    [{opname[i]}] = {entries},\\n\")\\n        iobj.write(\"};\\n\")\\n        deoptcodes = {}\\n        for basic in opmap:\\n            deoptcodes[basic] = basic\\n        for basic, family in opcode[\"_specializations\"].items():\\n            for specialized in family:\\n                deoptcodes[specialized] = basic\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Deopt[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Original[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            if opt.startswith(\"EXTENDED_ARG\"):\\n                deopt = \"EXTENDED_ARG_QUICK\"\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        fobj.write(\"\\n\")\\n        fobj.write(\"\\n        for op in hasconst:\\n            fobj.write(f\"\\n    || ((op) == {op}) \\\\\")\\n        fobj.write(\"\\n    )\\n\")\\n        fobj.write(\"\\n\")\\n        for i, (op, _) in enumerate(opcode[\"_nb_ops\"]):\\n            fobj.write(DEFINE.format(op, i))\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"static const char *const _PyOpcode_OpName[256] = {\\n\")\\n        for op, name in enumerate(opname_including_specialized):\\n            if name[0] != \"<\":\\n                op = name\\n            iobj.write(f'')\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        for i, flag in enumerate(used):\\n            if not flag:\\n                iobj.write(f\"    case {i}: \\\\\\n\")\\n        iobj.write(\"        ;\\n\")\\n        fobj.write(footer)\\n        iobj.write(internal_footer)"
  },
  {
    "code": "def fmgr_get_task_status(fmg, paramgram):\\n    if paramgram[\"task_id\"] is not None:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task/{task_id}'.format(task_id=paramgram[\"task_id\"])\\n        response = fmg.get(url, datagram)\\n    else:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task'\\n        response = fmg.get(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_query (#52770)",
    "fixed_code": "def fmgr_get_task_status(fmgr, paramgram):\\n    if paramgram[\"task_id\"] is not None:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task/{task_id}'.format(task_id=paramgram[\"task_id\"])\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    else:\\n        datagram = {\\n            \"adom\": paramgram[\"adom\"]\\n        }\\n        url = '/task/task'\\n        response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    return response"
  },
  {
    "code": "def delete_operation(self, name):\\n        conn = self.get_conn()\\n        resp = conn.projects().operations().delete(name=name).execute()\\n        return resp",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def delete_operation(self, name):\\n        conn = self.get_conn()\\n        resp = conn.projects().operations().delete(name=name).execute()\\n        return resp"
  },
  {
    "code": "def ewmstd(arg, com=None, span=None, halflife=None, min_periods=0, bias=False):\\n    result = ewmvar(arg, com=com, span=span, halflife=halflife,\\n                    min_periods=min_periods, bias=bias)\\n    return _zsqrt(result)\\newmvol = ewmstd",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: ewma() weights incorrect when some values are missing (GH7543)",
    "fixed_code": "def ewmstd(arg, com=None, span=None, halflife=None, min_periods=0, bias=False,\\n           ignore_na=False):\\n    result = ewmvar(arg, com=com, span=span, halflife=halflife,\\n                    min_periods=min_periods, bias=bias, ignore_na=ignore_na)\\n    return _zsqrt(result)\\newmvol = ewmstd"
  },
  {
    "code": "def parse_from_uri(self, uri):\\n        uri_parts = urlparse(uri)\\n        conn_type = uri_parts.scheme\\n        if conn_type == 'postgresql':\\n            conn_type = 'postgres'\\n        elif '-' in conn_type:\\n            conn_type = conn_type.replace('-', '_')\\n        self.conn_type = conn_type\\n        self.host = parse_netloc_to_hostname(uri_parts)\\n        quoted_schema = uri_parts.path[1:]\\n        self.schema = unquote(quoted_schema) if quoted_schema else quoted_schema\\n        self.login = unquote(uri_parts.username) \\\\n            if uri_parts.username else uri_parts.username\\n        self.password = unquote(uri_parts.password) \\\\n            if uri_parts.password else uri_parts.password\\n        self.port = uri_parts.port\\n        if uri_parts.query:\\n            self.extra = json.dumps(dict(parse_qsl(uri_parts.query)))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_from_uri(self, uri):\\n        uri_parts = urlparse(uri)\\n        conn_type = uri_parts.scheme\\n        if conn_type == 'postgresql':\\n            conn_type = 'postgres'\\n        elif '-' in conn_type:\\n            conn_type = conn_type.replace('-', '_')\\n        self.conn_type = conn_type\\n        self.host = parse_netloc_to_hostname(uri_parts)\\n        quoted_schema = uri_parts.path[1:]\\n        self.schema = unquote(quoted_schema) if quoted_schema else quoted_schema\\n        self.login = unquote(uri_parts.username) \\\\n            if uri_parts.username else uri_parts.username\\n        self.password = unquote(uri_parts.password) \\\\n            if uri_parts.password else uri_parts.password\\n        self.port = uri_parts.port\\n        if uri_parts.query:\\n            self.extra = json.dumps(dict(parse_qsl(uri_parts.query)))"
  },
  {
    "code": "def import_ssh_public_key(\\n        self, user: str, ssh_public_key: Dict, project_id: str, retry=None, timeout=None, metadata=None\\n    ) -> ImportSshPublicKeyResponse:\\n        conn = self.get_conn()\\n        return conn.import_ssh_public_key(\\n            request=dict(\\n                parent=f\"users/{user}\",\\n                ssh_public_key=ssh_public_key,\\n                project_id=project_id,\\n            ),\\n            retry=retry,\\n            timeout=timeout,\\n            metadata=metadata or (),\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def import_ssh_public_key(\\n        self, user: str, ssh_public_key: Dict, project_id: str, retry=None, timeout=None, metadata=None\\n    ) -> ImportSshPublicKeyResponse:\\n        conn = self.get_conn()\\n        return conn.import_ssh_public_key(\\n            request=dict(\\n                parent=f\"users/{user}\",\\n                ssh_public_key=ssh_public_key,\\n                project_id=project_id,\\n            ),\\n            retry=retry,\\n            timeout=timeout,\\n            metadata=metadata or (),\\n        )"
  },
  {
    "code": "def is_valid_na_for_dtype(obj, dtype: DtypeObj) -> bool:\\n    if not lib.is_scalar(obj) or not isna(obj):\\n        return False\\n    elif dtype.kind == \"M\":\\n        if isinstance(dtype, np.dtype):\\n            return not isinstance(obj, (np.timedelta64, Decimal))\\n        return not isinstance(obj, (np.timedelta64, np.datetime64, Decimal))\\n    elif dtype.kind == \"m\":\\n        return not isinstance(obj, (np.datetime64, Decimal))\\n    elif dtype.kind in [\"i\", \"u\", \"f\", \"c\"]:\\n        return obj is not NaT and not isinstance(obj, (np.datetime64, np.timedelta64))\\n    elif dtype == np.dtype(object):\\n        return True\\n    return not isinstance(obj, (np.datetime64, np.timedelta64, Decimal))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_valid_na_for_dtype(obj, dtype: DtypeObj) -> bool:\\n    if not lib.is_scalar(obj) or not isna(obj):\\n        return False\\n    elif dtype.kind == \"M\":\\n        if isinstance(dtype, np.dtype):\\n            return not isinstance(obj, (np.timedelta64, Decimal))\\n        return not isinstance(obj, (np.timedelta64, np.datetime64, Decimal))\\n    elif dtype.kind == \"m\":\\n        return not isinstance(obj, (np.datetime64, Decimal))\\n    elif dtype.kind in [\"i\", \"u\", \"f\", \"c\"]:\\n        return obj is not NaT and not isinstance(obj, (np.datetime64, np.timedelta64))\\n    elif dtype == np.dtype(object):\\n        return True\\n    return not isinstance(obj, (np.datetime64, np.timedelta64, Decimal))"
  },
  {
    "code": "def forbid_multi_line_headers(name, val):\\n    if '\\n' in val or '\\r' in val:\\n        raise BadHeaderError(\"Header values can't contain newlines (got %r for header %r)\" % (val, name))\\n    try:\\n        val = force_unicode(val).encode('ascii')\\n    except UnicodeEncodeError:\\n        if name.lower() in ('to', 'from', 'cc'):\\n            result = []\\n            for item in val.split(', '):\\n                nm, addr = parseaddr(item)\\n                nm = str(Header(nm, settings.DEFAULT_CHARSET))\\n                result.append(formataddr((nm, str(addr))))\\n            val = ', '.join(result)\\n        else:\\n            val = Header(force_unicode(val), settings.DEFAULT_CHARSET)\\n    return name, val\\nclass SafeMIMEText(MIMEText):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #7574 -- Fixed the handling of lazy translation in email headers.",
    "fixed_code": "def forbid_multi_line_headers(name, val):\\n    val = force_unicode(val)\\n    if '\\n' in val or '\\r' in val:\\n        raise BadHeaderError(\"Header values can't contain newlines (got %r for header %r)\" % (val, name))\\n    try:\\n        val = val.encode('ascii')\\n    except UnicodeEncodeError:\\n        if name.lower() in ('to', 'from', 'cc'):\\n            result = []\\n            for item in val.split(', '):\\n                nm, addr = parseaddr(item)\\n                nm = str(Header(nm, settings.DEFAULT_CHARSET))\\n                result.append(formataddr((nm, str(addr))))\\n            val = ', '.join(result)\\n        else:\\n            val = Header(val, settings.DEFAULT_CHARSET)\\n    return name, val\\nclass SafeMIMEText(MIMEText):"
  },
  {
    "code": "def get_traceback_data(self):\\n\\t\\tif self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\\n\\t\\t\\tself.template_does_not_exist = True\\n\\t\\t\\tself.postmortem = self.exc_value.chain or [self.exc_value]\\n\\t\\tframes = self.get_traceback_frames()\\n\\t\\tfor i, frame in enumerate(frames):\\n\\t\\t\\tif 'vars' in frame:\\n\\t\\t\\t\\tframe_vars = []\\n\\t\\t\\t\\tfor k, v in frame['vars']:\\n\\t\\t\\t\\t\\tv = pprint(v)\\n\\t\\t\\t\\t\\tif len(v) > 4096:\\n\\t\\t\\t\\t\\t\\tv = '%s... <trimmed %d bytes string>' % (v[0:4096], len(v))\\n\\t\\t\\t\\t\\tframe_vars.append((k, v))\\n\\t\\t\\t\\tframe['vars'] = frame_vars\\n\\t\\t\\tframes[i] = frame\\n\\t\\tunicode_hint = ''\\n\\t\\tif self.exc_type and issubclass(self.exc_type, UnicodeError):\\n\\t\\t\\tstart = getattr(self.exc_value, 'start', None)\\n\\t\\t\\tend = getattr(self.exc_value, 'end', None)\\n\\t\\t\\tif start is not None and end is not None:\\n\\t\\t\\t\\tunicode_str = self.exc_value.args[1]\\n\\t\\t\\t\\tunicode_hint = force_text(\\n\\t\\t\\t\\t\\tunicode_str[max(start - 5, 0):min(end + 5, len(unicode_str))],\\n\\t\\t\\t\\t\\t'ascii', errors='replace'\\n\\t\\t\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_traceback_data(self):\\n\\t\\tif self.exc_type and issubclass(self.exc_type, TemplateDoesNotExist):\\n\\t\\t\\tself.template_does_not_exist = True\\n\\t\\t\\tself.postmortem = self.exc_value.chain or [self.exc_value]\\n\\t\\tframes = self.get_traceback_frames()\\n\\t\\tfor i, frame in enumerate(frames):\\n\\t\\t\\tif 'vars' in frame:\\n\\t\\t\\t\\tframe_vars = []\\n\\t\\t\\t\\tfor k, v in frame['vars']:\\n\\t\\t\\t\\t\\tv = pprint(v)\\n\\t\\t\\t\\t\\tif len(v) > 4096:\\n\\t\\t\\t\\t\\t\\tv = '%s... <trimmed %d bytes string>' % (v[0:4096], len(v))\\n\\t\\t\\t\\t\\tframe_vars.append((k, v))\\n\\t\\t\\t\\tframe['vars'] = frame_vars\\n\\t\\t\\tframes[i] = frame\\n\\t\\tunicode_hint = ''\\n\\t\\tif self.exc_type and issubclass(self.exc_type, UnicodeError):\\n\\t\\t\\tstart = getattr(self.exc_value, 'start', None)\\n\\t\\t\\tend = getattr(self.exc_value, 'end', None)\\n\\t\\t\\tif start is not None and end is not None:\\n\\t\\t\\t\\tunicode_str = self.exc_value.args[1]\\n\\t\\t\\t\\tunicode_hint = force_text(\\n\\t\\t\\t\\t\\tunicode_str[max(start - 5, 0):min(end + 5, len(unicode_str))],\\n\\t\\t\\t\\t\\t'ascii', errors='replace'\\n\\t\\t\\t\\t)"
  },
  {
    "code": "def mean(data):\\n    if iter(data) is data:\\n        data = list(data)\\n    n = len(data)\\n    if n < 1:\\n        raise StatisticsError('mean requires at least one data point')\\n    return _sum(data)/n",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed issue #25177, problems with the mean of very small and very large numbers.",
    "fixed_code": "def mean(data):\\n    if iter(data) is data:\\n        data = list(data)\\n    n = len(data)\\n    if n < 1:\\n        raise StatisticsError('mean requires at least one data point')\\n    T, total, count = _sum(data)\\n    assert count == n\\n    return _convert(total/n, T)"
  },
  {
    "code": "def trigger_dag(\\n        dag_id: str,\\n        run_id: Optional[str] = None,\\n        conf: Optional[Union[dict, str]] = None,\\n        execution_date: Optional[datetime] = None,\\n        replace_microseconds: bool = True,\\n) -> Optional[DagRun]:\\n    dag_model = DagModel.get_current(dag_id)\\n    if dag_model is None:\\n        raise DagNotFound(\"Dag id {} not found in DagModel\".format(dag_id))\\n    dagbag = DagBag(dag_folder=dag_model.fileloc)\\n    dag_run = DagRun()\\n    triggers = _trigger_dag(\\n        dag_id=dag_id,\\n        dag_run=dag_run,\\n        dag_bag=dagbag,\\n        run_id=run_id,\\n        conf=conf,\\n        execution_date=execution_date,\\n        replace_microseconds=replace_microseconds,\\n    )\\n    return triggers[0] if triggers else None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6683] REST API respects store_serialized_dag setting (#7296)\\n\\nMake REST API respect core.store_serialized_dags setting",
    "fixed_code": "def trigger_dag(\\n        dag_id: str,\\n        run_id: Optional[str] = None,\\n        conf: Optional[Union[dict, str]] = None,\\n        execution_date: Optional[datetime] = None,\\n        replace_microseconds: bool = True,\\n) -> Optional[DagRun]:\\n    dag_model = DagModel.get_current(dag_id)\\n    if dag_model is None:\\n        raise DagNotFound(\"Dag id {} not found in DagModel\".format(dag_id))"
  },
  {
    "code": "def change_view(self, request, object_id, form_url='', extra_context=None):\\n        \"The 'change' admin view for this model.\"\\n        model = self.model\\n        opts = model._meta\\n        obj = self.get_object(request, unquote(object_id))\\n        if not self.has_change_permission(request, obj):\\n            raise PermissionDenied\\n        if obj is None:\\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {'name': force_unicode(opts.verbose_name), 'key': escape(object_id)})\\n        if request.method == 'POST' and \"_saveasnew\" in request.POST:\\n            return self.add_view(request, form_url=reverse('admin:%s_%s_add' %\\n                                    (opts.app_label, opts.module_name),\\n                                    current_app=self.admin_site.name))\\n        ModelForm = self.get_form(request, obj)\\n        formsets = []\\n        inline_instances = self.get_inline_instances(request)\\n        if request.method == 'POST':\\n            form = ModelForm(request.POST, request.FILES, instance=obj)\\n            if form.is_valid():\\n                form_validated = True\\n                new_object = self.save_form(request, form, change=True)\\n            else:\\n                form_validated = False\\n                new_object = obj\\n            prefixes = {}\\n            for FormSet, inline in zip(self.get_formsets(request, new_object), inline_instances):\\n                prefix = FormSet.get_default_prefix()\\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\\n                if prefixes[prefix] != 1 or not prefix:\\n                    prefix = \"%s-%s\" % (prefix, prefixes[prefix])\\n                formset = FormSet(request.POST, request.FILES,\\n                                  instance=new_object, prefix=prefix,\\n                                  queryset=inline.queryset(request))\\n                formsets.append(formset)\\n            if all_valid(formsets) and form_validated:\\n                self.save_model(request, new_object, form, True)\\n                self.save_related(request, form, formsets, True)\\n                change_message = self.construct_change_message(request, form, formsets)\\n                self.log_change(request, new_object, change_message)\\n                return self.response_change(request, new_object)\\n        else:\\n            form = ModelForm(instance=obj)\\n            prefixes = {}\\n            for FormSet, inline in zip(self.get_formsets(request, obj), inline_instances):\\n                prefix = FormSet.get_default_prefix()\\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\\n                if prefixes[prefix] != 1 or not prefix:\\n                    prefix = \"%s-%s\" % (prefix, prefixes[prefix])\\n                formset = FormSet(instance=obj, prefix=prefix,\\n                                  queryset=inline.queryset(request))\\n                formsets.append(formset)\\n        adminForm = helpers.AdminForm(form, self.get_fieldsets(request, obj),\\n            self.get_prepopulated_fields(request, obj),\\n            self.get_readonly_fields(request, obj),\\n            model_admin=self)\\n        media = self.media + adminForm.media\\n        inline_admin_formsets = []\\n        for inline, formset in zip(inline_instances, formsets):\\n            fieldsets = list(inline.get_fieldsets(request, obj))\\n            readonly = list(inline.get_readonly_fields(request, obj))\\n            prepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n            inline_admin_formset = helpers.InlineAdminFormSet(inline, formset,\\n                fieldsets, prepopulated, readonly, model_admin=self)\\n            inline_admin_formsets.append(inline_admin_formset)\\n            media = media + inline_admin_formset.media\\n        context = {\\n            'title': _('Change %s') % force_unicode(opts.verbose_name),\\n            'adminform': adminForm,\\n            'object_id': object_id,\\n            'original': obj,\\n            'is_popup': \"_popup\" in request.REQUEST,\\n            'media': mark_safe(media),\\n            'inline_admin_formsets': inline_admin_formsets,\\n            'errors': helpers.AdminErrorList(form, formsets),\\n            'app_label': opts.app_label,\\n        }\\n        context.update(extra_context or {})\\n        return self.render_change_form(request, context, change=True, obj=obj, form_url=form_url)\\n    @csrf_protect_m",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def change_view(self, request, object_id, form_url='', extra_context=None):\\n        \"The 'change' admin view for this model.\"\\n        model = self.model\\n        opts = model._meta\\n        obj = self.get_object(request, unquote(object_id))\\n        if not self.has_change_permission(request, obj):\\n            raise PermissionDenied\\n        if obj is None:\\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {'name': force_unicode(opts.verbose_name), 'key': escape(object_id)})\\n        if request.method == 'POST' and \"_saveasnew\" in request.POST:\\n            return self.add_view(request, form_url=reverse('admin:%s_%s_add' %\\n                                    (opts.app_label, opts.module_name),\\n                                    current_app=self.admin_site.name))\\n        ModelForm = self.get_form(request, obj)\\n        formsets = []\\n        inline_instances = self.get_inline_instances(request)\\n        if request.method == 'POST':\\n            form = ModelForm(request.POST, request.FILES, instance=obj)\\n            if form.is_valid():\\n                form_validated = True\\n                new_object = self.save_form(request, form, change=True)\\n            else:\\n                form_validated = False\\n                new_object = obj\\n            prefixes = {}\\n            for FormSet, inline in zip(self.get_formsets(request, new_object), inline_instances):\\n                prefix = FormSet.get_default_prefix()\\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\\n                if prefixes[prefix] != 1 or not prefix:\\n                    prefix = \"%s-%s\" % (prefix, prefixes[prefix])\\n                formset = FormSet(request.POST, request.FILES,\\n                                  instance=new_object, prefix=prefix,\\n                                  queryset=inline.queryset(request))\\n                formsets.append(formset)\\n            if all_valid(formsets) and form_validated:\\n                self.save_model(request, new_object, form, True)\\n                self.save_related(request, form, formsets, True)\\n                change_message = self.construct_change_message(request, form, formsets)\\n                self.log_change(request, new_object, change_message)\\n                return self.response_change(request, new_object)\\n        else:\\n            form = ModelForm(instance=obj)\\n            prefixes = {}\\n            for FormSet, inline in zip(self.get_formsets(request, obj), inline_instances):\\n                prefix = FormSet.get_default_prefix()\\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\\n                if prefixes[prefix] != 1 or not prefix:\\n                    prefix = \"%s-%s\" % (prefix, prefixes[prefix])\\n                formset = FormSet(instance=obj, prefix=prefix,\\n                                  queryset=inline.queryset(request))\\n                formsets.append(formset)\\n        adminForm = helpers.AdminForm(form, self.get_fieldsets(request, obj),\\n            self.get_prepopulated_fields(request, obj),\\n            self.get_readonly_fields(request, obj),\\n            model_admin=self)\\n        media = self.media + adminForm.media\\n        inline_admin_formsets = []\\n        for inline, formset in zip(inline_instances, formsets):\\n            fieldsets = list(inline.get_fieldsets(request, obj))\\n            readonly = list(inline.get_readonly_fields(request, obj))\\n            prepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n            inline_admin_formset = helpers.InlineAdminFormSet(inline, formset,\\n                fieldsets, prepopulated, readonly, model_admin=self)\\n            inline_admin_formsets.append(inline_admin_formset)\\n            media = media + inline_admin_formset.media\\n        context = {\\n            'title': _('Change %s') % force_unicode(opts.verbose_name),\\n            'adminform': adminForm,\\n            'object_id': object_id,\\n            'original': obj,\\n            'is_popup': \"_popup\" in request.REQUEST,\\n            'media': mark_safe(media),\\n            'inline_admin_formsets': inline_admin_formsets,\\n            'errors': helpers.AdminErrorList(form, formsets),\\n            'app_label': opts.app_label,\\n        }\\n        context.update(extra_context or {})\\n        return self.render_change_form(request, context, change=True, obj=obj, form_url=form_url)\\n    @csrf_protect_m"
  },
  {
    "code": "def astype_nansafe(\\n    arr, dtype: DtypeObj, copy: bool = True, skipna: bool = False\\n) -> ArrayLike:\\n    if is_extension_array_dtype(dtype):\\n        return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\\n    if not isinstance(dtype, np.dtype):\\n        dtype = pandas_dtype(dtype)\\n    if issubclass(dtype.type, str):\\n        return lib.ensure_string_array(\\n            arr.ravel(), skipna=skipna, convert_na_value=False\\n        ).reshape(arr.shape)\\n    elif is_datetime64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return ints_to_pydatetime(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            if isna(arr).any():\\n                raise ValueError(\"Cannot convert NaT values to integer\")\\n            return arr.view(dtype)\\n        if dtype.kind == \"M\":\\n            return arr.astype(dtype)\\n        raise TypeError(f\"cannot astype a datetimelike from [{arr.dtype}] to [{dtype}]\")\\n    elif is_timedelta64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return ints_to_pytimedelta(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            if isna(arr).any():\\n                raise ValueError(\"Cannot convert NaT values to integer\")\\n            return arr.view(dtype)\\n        if dtype not in [INT64_DTYPE, TD64NS_DTYPE]:\\n            if dtype.kind == \"m\":\\n                mask = isna(arr)\\n                result = arr.astype(dtype).astype(np.float64)\\n                result[mask] = np.nan\\n                return result\\n        elif dtype == TD64NS_DTYPE:\\n            return arr.astype(TD64NS_DTYPE, copy=copy)\\n        raise TypeError(f\"cannot astype a timedelta from [{arr.dtype}] to [{dtype}]\")\\n    elif np.issubdtype(arr.dtype, np.floating) and np.issubdtype(dtype, np.integer):\\n        if not np.isfinite(arr).all():\\n            raise ValueError(\"Cannot convert non-finite values (NA or inf) to integer\")\\n    elif is_object_dtype(arr):\\n        if np.issubdtype(dtype.type, np.integer):\\n            return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\\n        elif is_datetime64_dtype(dtype):\\n            from pandas import to_datetime\\n            return astype_nansafe(to_datetime(arr).values, dtype, copy=copy)\\n        elif is_timedelta64_dtype(dtype):\\n            from pandas import to_timedelta\\n            return astype_nansafe(to_timedelta(arr)._values, dtype, copy=copy)\\n    if dtype.name in (\"datetime64\", \"timedelta64\"):\\n        msg = (\\n            f\"The '{dtype.name}' dtype has no unit. Please pass in \"\\n            f\"'{dtype.name}[ns]' instead.\"\\n        )\\n        raise ValueError(msg)\\n    if copy or is_object_dtype(arr) or is_object_dtype(dtype):\\n        return arr.astype(dtype, copy=True)\\n    return arr.view(dtype)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: astype_nansafe require dtype object (#38466)",
    "fixed_code": "def astype_nansafe(\\n    arr, dtype: DtypeObj, copy: bool = True, skipna: bool = False\\n) -> ArrayLike:\\n    if isinstance(dtype, ExtensionDtype):\\n        return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)\\n    elif not isinstance(dtype, np.dtype):\\n        raise ValueError(\"dtype must be np.dtype or ExtensionDtype\")\\n    if issubclass(dtype.type, str):\\n        return lib.ensure_string_array(\\n            arr.ravel(), skipna=skipna, convert_na_value=False\\n        ).reshape(arr.shape)\\n    elif is_datetime64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return ints_to_pydatetime(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            if isna(arr).any():\\n                raise ValueError(\"Cannot convert NaT values to integer\")\\n            return arr.view(dtype)\\n        if dtype.kind == \"M\":\\n            return arr.astype(dtype)\\n        raise TypeError(f\"cannot astype a datetimelike from [{arr.dtype}] to [{dtype}]\")\\n    elif is_timedelta64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return ints_to_pytimedelta(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            if isna(arr).any():\\n                raise ValueError(\"Cannot convert NaT values to integer\")\\n            return arr.view(dtype)\\n        if dtype not in [INT64_DTYPE, TD64NS_DTYPE]:\\n            if dtype.kind == \"m\":\\n                mask = isna(arr)\\n                result = arr.astype(dtype).astype(np.float64)\\n                result[mask] = np.nan\\n                return result\\n        elif dtype == TD64NS_DTYPE:\\n            return arr.astype(TD64NS_DTYPE, copy=copy)\\n        raise TypeError(f\"cannot astype a timedelta from [{arr.dtype}] to [{dtype}]\")\\n    elif np.issubdtype(arr.dtype, np.floating) and np.issubdtype(dtype, np.integer):\\n        if not np.isfinite(arr).all():\\n            raise ValueError(\"Cannot convert non-finite values (NA or inf) to integer\")\\n    elif is_object_dtype(arr):\\n        if np.issubdtype(dtype.type, np.integer):\\n            return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\\n        elif is_datetime64_dtype(dtype):\\n            from pandas import to_datetime\\n            return astype_nansafe(to_datetime(arr).values, dtype, copy=copy)\\n        elif is_timedelta64_dtype(dtype):\\n            from pandas import to_timedelta\\n            return astype_nansafe(to_timedelta(arr)._values, dtype, copy=copy)\\n    if dtype.name in (\"datetime64\", \"timedelta64\"):\\n        msg = (\\n            f\"The '{dtype.name}' dtype has no unit. Please pass in \"\\n            f\"'{dtype.name}[ns]' instead.\"\\n        )\\n        raise ValueError(msg)\\n    if copy or is_object_dtype(arr) or is_object_dtype(dtype):\\n        return arr.astype(dtype, copy=True)\\n    return arr.view(dtype)"
  },
  {
    "code": "def add_level(self) -> None:\\n        self.print(\"if (p->level++ == MAXSTACK) {\")\\n        with self.indent():\\n            self.print(\"p->error_indicator = 1;\")\\n            self.print(\"PyErr_NoMemory();\")\\n        self.print(\"}\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert \"bpo-46110: Add a recursion check to avoid stack overflow in the PEG parser (GH-30177)\" (GH-30363)",
    "fixed_code": "def add_level(self) -> None:\\n        self.print(\"D(p->level++);\")"
  },
  {
    "code": "def _check_backend_specific_checks(self, **kwargs):\\n\\t\\tapp_label = self.model._meta.app_label\\n\\t\\tfor db in connections:\\n\\t\\t\\tif router.allow_migrate(db, app_label, model=self.model):\\n\\t\\t\\t\\treturn connections[db].validation.check_field(self, **kwargs)\\n\\t\\treturn []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Updated allow_migrate() signature in check framework tests",
    "fixed_code": "def _check_backend_specific_checks(self, **kwargs):\\n\\t\\tapp_label = self.model._meta.app_label\\n\\t\\tfor db in connections:\\n\\t\\t\\tif router.allow_migrate(db, app_label, model_name=self.model._meta.model_name):\\n\\t\\t\\t\\treturn connections[db].validation.check_field(self, **kwargs)\\n\\t\\treturn []"
  },
  {
    "code": "def _encode_params(data):\\n\\t\\tif isinstance(data, (str, bytes)):\\n\\t\\t\\treturn data\\n\\t\\telif hasattr(data, 'read'):\\n\\t\\t\\treturn data\\n\\t\\telif hasattr(data, '__iter__'):\\n\\t\\t\\tresult = []\\n\\t\\t\\tfor k, vs in to_key_val_list(data):\\n\\t\\t\\t\\tif isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\\n\\t\\t\\t\\t\\tvs = [vs]\\n\\t\\t\\t\\tfor v in vs:\\n\\t\\t\\t\\t\\tif v is not None:\\n\\t\\t\\t\\t\\t\\tresult.append(\\n\\t\\t\\t\\t\\t\\t\\t(k.encode('utf-8') if isinstance(k, str) else k,\\n\\t\\t\\t\\t\\t\\t\\t v.encode('utf-8') if isinstance(v, str) else v))\\n\\t\\t\\treturn urlencode(result, doseq=True)\\n\\t\\telse:\\n\\t\\t\\treturn data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix regression from #2844 regarding binary bodies.",
    "fixed_code": "def _encode_params(data):\\n\\t\\tif isinstance(data, (str, bytes)):\\n\\t\\t\\treturn to_native_string(data)\\n\\t\\telif hasattr(data, 'read'):\\n\\t\\t\\treturn data\\n\\t\\telif hasattr(data, '__iter__'):\\n\\t\\t\\tresult = []\\n\\t\\t\\tfor k, vs in to_key_val_list(data):\\n\\t\\t\\t\\tif isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\\n\\t\\t\\t\\t\\tvs = [vs]\\n\\t\\t\\t\\tfor v in vs:\\n\\t\\t\\t\\t\\tif v is not None:\\n\\t\\t\\t\\t\\t\\tresult.append(\\n\\t\\t\\t\\t\\t\\t\\t(k.encode('utf-8') if isinstance(k, str) else k,\\n\\t\\t\\t\\t\\t\\t\\t v.encode('utf-8') if isinstance(v, str) else v))\\n\\t\\t\\treturn urlencode(result, doseq=True)\\n\\t\\telse:\\n\\t\\t\\treturn data"
  },
  {
    "code": "def load_string(self):\\n        data = self.readline()[:-1]\\n        if len(data) >= 2 and data[0] == data[-1] and data[0] in b'\"\\'':\\n            data = data[1:-1]\\n        else:\\n            raise UnpicklingError(\"the STRING opcode argument must be quoted\")\\n        self.append(self._decode_string(codecs.escape_decode(data)[0]))\\n    dispatch[STRING[0]] = load_string",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_string(self):\\n        data = self.readline()[:-1]\\n        if len(data) >= 2 and data[0] == data[-1] and data[0] in b'\"\\'':\\n            data = data[1:-1]\\n        else:\\n            raise UnpicklingError(\"the STRING opcode argument must be quoted\")\\n        self.append(self._decode_string(codecs.escape_decode(data)[0]))\\n    dispatch[STRING[0]] = load_string"
  },
  {
    "code": "def http_error_auth_reqed(self, authreq, host, req, headers):\\n        headers = headers.get_all(authreq)\\n        if not headers:\\n            return\\n        unsupported = None\\n        for header in headers:\\n            for scheme, realm in self._parse_realm(header):\\n                if scheme.lower() != 'basic':\\n                    unsupported = scheme\\n                    continue\\n                if realm is not None:\\n                    return self.retry_http_basic_auth(host, req, realm)\\n        if unsupported is not None:\\n            raise ValueError(\"AbstractBasicAuthHandler does not \"\\n                             \"support the following scheme: %r\"\\n                             % (scheme,))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def http_error_auth_reqed(self, authreq, host, req, headers):\\n        headers = headers.get_all(authreq)\\n        if not headers:\\n            return\\n        unsupported = None\\n        for header in headers:\\n            for scheme, realm in self._parse_realm(header):\\n                if scheme.lower() != 'basic':\\n                    unsupported = scheme\\n                    continue\\n                if realm is not None:\\n                    return self.retry_http_basic_auth(host, req, realm)\\n        if unsupported is not None:\\n            raise ValueError(\"AbstractBasicAuthHandler does not \"\\n                             \"support the following scheme: %r\"\\n                             % (scheme,))"
  },
  {
    "code": "def getattr_static(obj, attr, default=_sentinel):\\n    instance_result = _sentinel\\n    if not isinstance(obj, type):\\n        instance_result = _check_instance(obj, attr)\\n        klass = type(obj)\\n    else:\\n        klass = obj\\n    klass_result = _check_class(klass, attr)\\n    if instance_result is not _sentinel and klass_result is not _sentinel:\\n        if (_check_class(type(klass_result), '__get__') is not _sentinel and\\n            _check_class(type(klass_result), '__set__') is not _sentinel):\\n            return klass_result\\n    if instance_result is not _sentinel:\\n        return instance_result\\n    if klass_result is not _sentinel:\\n        return klass_result\\n    if obj is klass:\\n        for entry in getmro(type(klass)):\\n            try:\\n                return entry.__dict__[attr]\\n            except KeyError:\\n                pass\\n    if default is not _sentinel:\\n        return default\\n    raise AttributeError(attr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getattr_static(obj, attr, default=_sentinel):\\n    instance_result = _sentinel\\n    if not isinstance(obj, type):\\n        instance_result = _check_instance(obj, attr)\\n        klass = type(obj)\\n    else:\\n        klass = obj\\n    klass_result = _check_class(klass, attr)\\n    if instance_result is not _sentinel and klass_result is not _sentinel:\\n        if (_check_class(type(klass_result), '__get__') is not _sentinel and\\n            _check_class(type(klass_result), '__set__') is not _sentinel):\\n            return klass_result\\n    if instance_result is not _sentinel:\\n        return instance_result\\n    if klass_result is not _sentinel:\\n        return klass_result\\n    if obj is klass:\\n        for entry in getmro(type(klass)):\\n            try:\\n                return entry.__dict__[attr]\\n            except KeyError:\\n                pass\\n    if default is not _sentinel:\\n        return default\\n    raise AttributeError(attr)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\"),\\n        vdom=dict(required=False, type=\"str\"),\\n        host=dict(required=True, type=\"str\"),\\n        password=dict(fallback=(env_fallback, [\"ANSIBLE_NET_PASSWORD\"]), no_log=True),\\n        username=dict(fallback=(env_fallback, [\"ANSIBLE_NET_USERNAME\"])),\\n        state=dict(choices=[\"execute\", \"delete\", \"present\"], type=\"str\"),\\n        script_name=dict(required=True, type=\"str\"),\\n        script_type=dict(required=False, type=\"str\"),\\n        script_target=dict(required=False, type=\"str\"),\\n        script_description=dict(required=False, type=\"str\"),\\n        script_content=dict(required=False, type=\"str\"),\\n        script_scope=dict(required=False, type=\"str\"),\\n        script_package=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec, supports_check_mode=True,)\\n    if module.params[\"host\"] is None or module.params[\"username\"] is None:\\n        module.fail_json(msg=\"Host and username are required for connection\")\\n    fmg = AnsibleFortiManager(module, module.params[\"host\"], module.params[\"username\"], module.params[\"password\"])\\n    response = fmg.login()\\n    if \"FortiManager instance connnected\" not in str(response):\\n        module.fail_json(msg=\"Connection to FortiManager Failed\")\\n    else:\\n        adom = module.params[\"adom\"]\\n        if adom is None:\\n            adom = \"root\"\\n        vdom = module.params[\"vdom\"]\\n        if vdom is None:\\n            vdom = \"root\"\\n        state = module.params[\"state\"]\\n        if state is None:\\n            state = \"present\"\\n        script_name = module.params[\"script_name\"]\\n        script_type = module.params[\"script_type\"]\\n        script_target = module.params[\"script_target\"]\\n        script_description = module.params[\"script_description\"]\\n        script_content = module.params[\"script_content\"]\\n        script_scope = module.params[\"script_scope\"]\\n        script_package = module.params[\"script_package\"]\\n        if state == \"present\":\\n            results = set_script(fmg, script_name, script_type, script_content, script_description, script_target, adom)\\n            if results[0] != 0:\\n                if isinstance(results[1], list):\\n                    module.fail_json(msg=\"Adding Script Failed\", **results)\\n                else:\\n                    module.fail_json(msg=\"Adding Script Failed\")\\n        elif state == \"execute\":\\n            results = execute_script(fmg, script_name, script_scope, script_package, adom, vdom)\\n            if results[0] != 0:\\n                module.fail_json(msg=\"Script Execution Failed\", **results)\\n        elif state == \"delete\":\\n            results = delete_script(fmg, script_name, adom)\\n            if results[0] != 0:\\n                module.fail_json(msg=\"Script Deletion Failed\", **results)\\n        fmg.logout()\\n        return module.exit_json(**results[1])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_script (#52786)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        vdom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"execute\", \"set\", \"delete\"], type=\"str\", default=\"add\"),\\n        script_name=dict(required=True, type=\"str\"),\\n        script_type=dict(required=False, type=\"str\"),\\n        script_target=dict(required=False, type=\"str\"),\\n        script_description=dict(required=False, type=\"str\"),\\n        script_content=dict(required=False, type=\"str\"),\\n        script_scope=dict(required=False, type=\"str\"),\\n        script_package=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"script_name\": module.params[\"script_name\"],\\n        \"script_type\": module.params[\"script_type\"],\\n        \"script_target\": module.params[\"script_target\"],\\n        \"script_description\": module.params[\"script_description\"],\\n        \"script_content\": module.params[\"script_content\"],\\n        \"script_scope\": module.params[\"script_scope\"],\\n        \"script_package\": module.params[\"script_package\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"vdom\": module.params[\"vdom\"],\\n        \"mode\": module.params[\"mode\"],\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"mode\"] in ['add', 'set']:\\n            results = set_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"mode\"] == \"execute\":\\n            results = execute_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"mode\"] == \"delete\":\\n            results = delete_script(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results, msg=\"Operation Finished\",\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, module.params))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def _concat_blocks(self, blocks):\\n        concat_values = np.concatenate([b.values for b in blocks\\n                                        if b is not None],\\n                                       axis=self.axis)\\n        if self.axis > 0:\\n            if not _all_indexes_same([b.items for b in blocks]):\\n                raise Exception('dtypes are not consistent throughout '\\n                                'DataFrames')\\n            return make_block(concat_values, blocks[0].items, self.new_axes[0])\\n        else:\\n            all_items = [b.items for b in blocks if b is not None]\\n            if self.axis == 0 and self.keys is not None:\\n                offsets = np.r_[0, np.cumsum([len(x._data.axes[self.axis]) for\\n                                              x in self.objs])]\\n                indexer = np.concatenate([offsets[i] + b.ref_locs\\n                                          for i, b in enumerate(blocks)\\n                                          if b is not None])\\n                concat_items = self.new_axes[0].take(indexer)\\n            else:\\n                concat_items = _concat_indexes(all_items)\\n            return make_block(concat_values, concat_items, self.new_axes[0])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _concat_blocks(self, blocks):\\n        concat_values = np.concatenate([b.values for b in blocks\\n                                        if b is not None],\\n                                       axis=self.axis)\\n        if self.axis > 0:\\n            if not _all_indexes_same([b.items for b in blocks]):\\n                raise Exception('dtypes are not consistent throughout '\\n                                'DataFrames')\\n            return make_block(concat_values, blocks[0].items, self.new_axes[0])\\n        else:\\n            all_items = [b.items for b in blocks if b is not None]\\n            if self.axis == 0 and self.keys is not None:\\n                offsets = np.r_[0, np.cumsum([len(x._data.axes[self.axis]) for\\n                                              x in self.objs])]\\n                indexer = np.concatenate([offsets[i] + b.ref_locs\\n                                          for i, b in enumerate(blocks)\\n                                          if b is not None])\\n                concat_items = self.new_axes[0].take(indexer)\\n            else:\\n                concat_items = _concat_indexes(all_items)\\n            return make_block(concat_values, concat_items, self.new_axes[0])"
  },
  {
    "code": "def _EndRecData64(fpin, offset, endrec):\\n    try:\\n        fpin.seek(offset - sizeEndCentDir64Locator, 2)\\n    except IOError:\\n        return endrec\\n    data = fpin.read(sizeEndCentDir64Locator)\\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\\n    if sig != stringEndArchive64Locator:\\n        return endrec\\n    if diskno != 0 or disks != 1:\\n        raise BadZipZile(\"zipfiles that span multiple disks are not supported\")\\n    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)\\n    data = fpin.read(sizeEndCentDir64)\\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\\n            dircount, dircount2, dirsize, diroffset = \\\\n            struct.unpack(structEndArchive64, data)\\n    if sig != stringEndArchive64:\\n        return endrec\\n    endrec[_ECD_SIGNATURE] = sig\\n    endrec[_ECD_DISK_NUMBER] = disk_num\\n    endrec[_ECD_DISK_START] = disk_dir\\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\\n    endrec[_ECD_SIZE] = dirsize\\n    endrec[_ECD_OFFSET] = diroffset\\n    return endrec",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _EndRecData64(fpin, offset, endrec):\\n    try:\\n        fpin.seek(offset - sizeEndCentDir64Locator, 2)\\n    except IOError:\\n        return endrec\\n    data = fpin.read(sizeEndCentDir64Locator)\\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\\n    if sig != stringEndArchive64Locator:\\n        return endrec\\n    if diskno != 0 or disks != 1:\\n        raise BadZipZile(\"zipfiles that span multiple disks are not supported\")\\n    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)\\n    data = fpin.read(sizeEndCentDir64)\\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\\n            dircount, dircount2, dirsize, diroffset = \\\\n            struct.unpack(structEndArchive64, data)\\n    if sig != stringEndArchive64:\\n        return endrec\\n    endrec[_ECD_SIGNATURE] = sig\\n    endrec[_ECD_DISK_NUMBER] = disk_num\\n    endrec[_ECD_DISK_START] = disk_dir\\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\\n    endrec[_ECD_SIZE] = dirsize\\n    endrec[_ECD_OFFSET] = diroffset\\n    return endrec"
  },
  {
    "code": "def _perfcheck(object=None):\\n    import time\\n    if object is None:\\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\\n    p = PrettyPrinter()\\n    t1 = time.perf_counter()\\n    _safe_repr(object, {}, None, 0, True)\\n    t2 = time.perf_counter()\\n    p.pformat(object)\\n    t3 = time.perf_counter()\\n    print(\"_safe_repr:\", t2 - t1)\\n    print(\"pformat:\", t3 - t2)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-28850: Fix PrettyPrinter.format overrides ignored for contents of small containers (GH-22120)",
    "fixed_code": "def _perfcheck(object=None):\\n    import time\\n    if object is None:\\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\\n    p = PrettyPrinter()\\n    t1 = time.perf_counter()\\n    p._safe_repr(object, {}, None, 0, True)\\n    t2 = time.perf_counter()\\n    p.pformat(object)\\n    t3 = time.perf_counter()\\n    print(\"_safe_repr:\", t2 - t1)\\n    print(\"pformat:\", t3 - t2)"
  },
  {
    "code": "def str_encode(arr, encoding):\\n    f = lambda x: x.encode(encoding)\\n    return _na_map(f, arr)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add encoding/decoding error handling\\n\\nWhen encoding/decoding strings with errors allow you\\nto pass error handling strings. This works the same as\\nerror handling for other encode/decode functions.\\nDefaults to 'strict', but you can pass 'ignore', 'replace',\\netc.\\n\\nExtends work done in #1706.",
    "fixed_code": "def str_encode(arr, encoding, errors=\"strict\"):\\n    f = lambda x: x.encode(encoding, errors)\\n    return _na_map(f, arr)"
  },
  {
    "code": "def validate_pep8(self):\\n        if not self.examples:\\n            return\\n        content = ''.join(('import numpy as np  \\n',\\n                           'import pandas as pd  \\n',\\n                           *self.examples_source_code))\\n        application = flake8.main.application.Application()\\n        application.initialize([\"--quiet\"])\\n        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8') as file:\\n            file.write(content)\\n            file.flush()\\n            application.run_checks([file.name])\\n        application.formatter.write = lambda line, source: None\\n        application.report()\\n        yield from application.guide.stats.statistics_for('')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate_pep8(self):\\n        if not self.examples:\\n            return\\n        content = ''.join(('import numpy as np  \\n',\\n                           'import pandas as pd  \\n',\\n                           *self.examples_source_code))\\n        application = flake8.main.application.Application()\\n        application.initialize([\"--quiet\"])\\n        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8') as file:\\n            file.write(content)\\n            file.flush()\\n            application.run_checks([file.name])\\n        application.formatter.write = lambda line, source: None\\n        application.report()\\n        yield from application.guide.stats.statistics_for('')"
  },
  {
    "code": "def url2pathname(url):\\n    import string, urllib.parse\\n    url = url.replace(':', '|')\\n    if not '|' in url:\\n        if url[:4] == '////':\\n            url = url[2:]\\n        components = url.split('/')\\n        return urllib.parse.unquote('\\\\'.join(components))\\n    comp = url.split('|')\\n    if len(comp) != 2 or comp[0][-1] not in string.ascii_letters:\\n        error = 'Bad URL: ' + url\\n        raise IOError(error)\\n    drive = comp[0][-1].upper()\\n    components = comp[1].split('/')\\n    path = drive + ':'\\n    for comp in components:\\n        if comp:\\n            path = path + '\\\\' + urllib.parse.unquote(comp)\\n    if path.endswith(':') and url.endswith('/'):\\n        path += '\\\\'\\n    return path",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def url2pathname(url):\\n    import string, urllib.parse\\n    url = url.replace(':', '|')\\n    if not '|' in url:\\n        if url[:4] == '////':\\n            url = url[2:]\\n        components = url.split('/')\\n        return urllib.parse.unquote('\\\\'.join(components))\\n    comp = url.split('|')\\n    if len(comp) != 2 or comp[0][-1] not in string.ascii_letters:\\n        error = 'Bad URL: ' + url\\n        raise IOError(error)\\n    drive = comp[0][-1].upper()\\n    components = comp[1].split('/')\\n    path = drive + ':'\\n    for comp in components:\\n        if comp:\\n            path = path + '\\\\' + urllib.parse.unquote(comp)\\n    if path.endswith(':') and url.endswith('/'):\\n        path += '\\\\'\\n    return path"
  },
  {
    "code": "def __getstate__(self):\\n        raise TypeError(\"can not serialize a '{0}' object\"\\n                        .format(self.__class__.__name__))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getstate__(self):\\n        raise TypeError(\"can not serialize a '{0}' object\"\\n                        .format(self.__class__.__name__))"
  },
  {
    "code": "def check_dtype_object(name, Estimator):\\n    rng = np.random.RandomState(0)\\n    X = rng.rand(40, 10).astype(object)\\n    y = (X[:, 0] * 4).astype(np.int)\\n    y = multioutput_estimator_convert_y_2d(name, y)\\n    with warnings.catch_warnings():\\n        estimator = Estimator()\\n    set_testing_parameters(estimator)\\n    estimator.fit(X, y)\\n    if hasattr(estimator, \"predict\"):\\n        estimator.predict(X)\\n    if (hasattr(estimator, \"transform\") and\\n            name not in DEPRECATED_TRANSFORM):\\n        estimator.transform(X)\\n    try:\\n        estimator.fit(X, y.astype(object))\\n    except Exception as e:\\n        if \"Unknown label type\" not in str(e):\\n            raise\\n    X[0, 0] = {'foo': 'bar'}\\n    msg = \"argument must be a string or a number\"\\n    assert_raises_regex(TypeError, msg, estimator.fit, X, y)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_dtype_object(name, Estimator):\\n    rng = np.random.RandomState(0)\\n    X = rng.rand(40, 10).astype(object)\\n    y = (X[:, 0] * 4).astype(np.int)\\n    y = multioutput_estimator_convert_y_2d(name, y)\\n    estimator = Estimator()\\n    set_testing_parameters(estimator)\\n    estimator.fit(X, y)\\n    if hasattr(estimator, \"predict\"):\\n        estimator.predict(X)\\n    if (hasattr(estimator, \"transform\") and\\n            name not in DEPRECATED_TRANSFORM):\\n        estimator.transform(X)\\n    try:\\n        estimator.fit(X, y.astype(object))\\n    except Exception as e:\\n        if \"Unknown label type\" not in str(e):\\n            raise\\n    X[0, 0] = {'foo': 'bar'}\\n    msg = \"argument must be a string or a number\"\\n    assert_raises_regex(TypeError, msg, estimator.fit, X, y)"
  },
  {
    "code": "def getfile(object):\\n    if ismodule(object):\\n        if hasattr(object, '__file__'):\\n            return object.__file__\\n        raise TypeError('{!r} is a built-in module'.format(object))\\n    if isclass(object):\\n        if hasattr(object, '__module__'):\\n            object = sys.modules.get(object.__module__)\\n            if hasattr(object, '__file__'):\\n                return object.__file__\\n        raise TypeError('{!r} is a built-in class'.format(object))\\n    if ismethod(object):\\n        object = object.__func__\\n    if isfunction(object):\\n        object = object.__code__\\n    if istraceback(object):\\n        object = object.tb_frame\\n    if isframe(object):\\n        object = object.f_code\\n    if iscode(object):\\n        return object.co_filename\\n    raise TypeError('module, class, method, function, traceback, frame, or '\\n                    'code object was expected, got {}'.format(\\n                    type(object).__name__))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getfile(object):\\n    if ismodule(object):\\n        if hasattr(object, '__file__'):\\n            return object.__file__\\n        raise TypeError('{!r} is a built-in module'.format(object))\\n    if isclass(object):\\n        if hasattr(object, '__module__'):\\n            object = sys.modules.get(object.__module__)\\n            if hasattr(object, '__file__'):\\n                return object.__file__\\n        raise TypeError('{!r} is a built-in class'.format(object))\\n    if ismethod(object):\\n        object = object.__func__\\n    if isfunction(object):\\n        object = object.__code__\\n    if istraceback(object):\\n        object = object.tb_frame\\n    if isframe(object):\\n        object = object.f_code\\n    if iscode(object):\\n        return object.co_filename\\n    raise TypeError('module, class, method, function, traceback, frame, or '\\n                    'code object was expected, got {}'.format(\\n                    type(object).__name__))"
  },
  {
    "code": "def time_infer_dst(self):\\n        self.index.tz_localize('US/Eastern', ambiguous='infer')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_infer_dst(self):\\n        self.index.tz_localize('US/Eastern', ambiguous='infer')"
  },
  {
    "code": "def __setitem__(self, key, value):\\n        _, N, K = self.dims\\n        if isinstance(value, LongPanel):\\n            if len(value.items) != 1:\\n                raise ValueError('Input panel must have only one item!')\\n            value = value.to_wide()[value.items[0]]\\n        if isinstance(value, DataFrame):\\n            value = value.reindex(index=self.major_axis,\\n                                  columns=self.minor_axis)\\n            mat = value.values\\n        elif np.isscalar(value):\\n            mat = np.empty((N, K), dtype=float)\\n            mat.fill(value)\\n        if key in self.items:\\n            loc = self.items.indexMap[key]\\n            self.values[loc] = mat\\n        else:\\n            self.items = Index(list(self.items) + [key])\\n            mat = mat.reshape((1, N, K))\\n            self.values = np.row_stack((self.values, mat))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "most efficient SparseWidePanel.to_long that I can think of",
    "fixed_code": "def __setitem__(self, key, value):\\n        _, N, K = self.shape\\n        if isinstance(value, LongPanel):\\n            if len(value.items) != 1:\\n                raise ValueError('Input panel must have only one item!')\\n            value = value.to_wide()[value.items[0]]\\n        if isinstance(value, DataFrame):\\n            value = value.reindex(index=self.major_axis,\\n                                  columns=self.minor_axis)\\n            mat = value.values\\n        elif np.isscalar(value):\\n            mat = np.empty((N, K), dtype=float)\\n            mat.fill(value)\\n        if key in self.items:\\n            loc = self.items.indexMap[key]\\n            self.values[loc] = mat\\n        else:\\n            self.items = Index(list(self.items) + [key])\\n            mat = mat.reshape((1, N, K))\\n            self.values = np.row_stack((self.values, mat))"
  },
  {
    "code": "def _bootstrap(*, root=None, upgrade=False, user=False,\\n              altinstall=False, default_pip=False,\\n              verbosity=0):\\n    if altinstall and default_pip:\\n        raise ValueError(\"Cannot use altinstall and default_pip together\")\\n    sys.audit(\"ensurepip.bootstrap\", root)\\n    _disable_pip_configuration_settings()\\n    if altinstall:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"altinstall\"\\n    elif not default_pip:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"install\"\\n    with tempfile.TemporaryDirectory() as tmpdir:\\n        additional_paths = []\\n        for project, version in _PROJECTS:\\n            wheel_name = \"{}-{}-py2.py3-none-any.whl\".format(project, version)\\n            whl = resources.read_binary(\\n                _bundled,\\n                wheel_name,\\n            )\\n            with open(os.path.join(tmpdir, wheel_name), \"wb\") as fp:\\n                fp.write(whl)\\n            additional_paths.append(os.path.join(tmpdir, wheel_name))\\n        args = [\"install\", \"--no-index\", \"--find-links\", tmpdir]\\n        if root:\\n            args += [\"--root\", root]\\n        if upgrade:\\n            args += [\"--upgrade\"]\\n        if user:\\n            args += [\"--user\"]\\n        if verbosity:\\n            args += [\"-\" + \"v\" * verbosity]\\n        return _run_pip(args + [p[0] for p in _PROJECTS], additional_paths)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-40448: ensurepip: Do not use cache (GH-19812)\\n\\nensurepip optionally installs or upgrades 'pip' and 'setuptools' using\\nthe version of those modules bundled with Python.  The internal PIP\\ninstallation routine by default temporarily uses its cache, if it\\nexists.  This is undesirable as Python builds and installations may be\\nindependent of the user running the build, whilst PIP cache location\\nis dependent on the user's environment and outside of the build\\nenvironment.\\n\\nAt the same time, there's no value in using the cache while installing\\nbundled modules.\\n\\nThis change disables PIP caching when used in ensurepip.",
    "fixed_code": "def _bootstrap(*, root=None, upgrade=False, user=False,\\n              altinstall=False, default_pip=False,\\n              verbosity=0):\\n    if altinstall and default_pip:\\n        raise ValueError(\"Cannot use altinstall and default_pip together\")\\n    sys.audit(\"ensurepip.bootstrap\", root)\\n    _disable_pip_configuration_settings()\\n    if altinstall:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"altinstall\"\\n    elif not default_pip:\\n        os.environ[\"ENSUREPIP_OPTIONS\"] = \"install\"\\n    with tempfile.TemporaryDirectory() as tmpdir:\\n        additional_paths = []\\n        for project, version in _PROJECTS:\\n            wheel_name = \"{}-{}-py2.py3-none-any.whl\".format(project, version)\\n            whl = resources.read_binary(\\n                _bundled,\\n                wheel_name,\\n            )\\n            with open(os.path.join(tmpdir, wheel_name), \"wb\") as fp:\\n                fp.write(whl)\\n            additional_paths.append(os.path.join(tmpdir, wheel_name))\\n        args = [\"install\", \"--no-cache-dir\", \"--no-index\", \"--find-links\", tmpdir]\\n        if root:\\n            args += [\"--root\", root]\\n        if upgrade:\\n            args += [\"--upgrade\"]\\n        if user:\\n            args += [\"--user\"]\\n        if verbosity:\\n            args += [\"-\" + \"v\" * verbosity]\\n        return _run_pip(args + [p[0] for p in _PROJECTS], additional_paths)"
  },
  {
    "code": "def next(self):\\n        return self.statcmd('NEXT')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def next(self):\\n        return self._statcmd('NEXT')"
  },
  {
    "code": "def spectral_embedding(\\n    adjacency,\\n    *,\\n    n_components=8,\\n    eigen_solver=None,\\n    random_state=None,\\n    eigen_tol=0.0,\\n    norm_laplacian=True,\\n    drop_first=True,\\n):\\n    adjacency = check_symmetric(adjacency)\\n    try:\\n        from pyamg import smoothed_aggregation_solver\\n    except ImportError as e:\\n        if eigen_solver == \"amg\":\\n            raise ValueError(\\n                \"The eigen_solver was set to 'amg', but pyamg is not available.\"\\n            ) from e\\n    if eigen_solver is None:\\n        eigen_solver = \"arpack\"\\n    elif eigen_solver not in (\"arpack\", \"lobpcg\", \"amg\"):\\n        raise ValueError(\\n            \"Unknown value for eigen_solver: '%s'.\"\\n            \"Should be 'amg', 'arpack', or 'lobpcg'\" % eigen_solver\\n        )\\n    random_state = check_random_state(random_state)\\n    n_nodes = adjacency.shape[0]\\n    if drop_first:\\n        n_components = n_components + 1\\n    if not _graph_is_connected(adjacency):\\n        warnings.warn(\\n            \"Graph is not fully connected, spectral embedding may not work as expected.\"\\n        )\\n    laplacian, dd = csgraph_laplacian(\\n        adjacency, normed=norm_laplacian, return_diag=True\\n    )\\n    if (\\n        eigen_solver == \"arpack\"\\n        or eigen_solver != \"lobpcg\"\\n        and (not sparse.isspmatrix(laplacian) or n_nodes < 5 * n_components)\\n    ):\\n        laplacian = _set_diag(laplacian, 1, norm_laplacian)\\n        try:\\n            laplacian *= -1\\n            v0 = _init_arpack_v0(laplacian.shape[0], random_state)\\n            _, diffusion_map = eigsh(\\n                laplacian, k=n_components, sigma=1.0, which=\"LM\", tol=eigen_tol, v0=v0\\n            )\\n            embedding = diffusion_map.T[n_components::-1]\\n            if norm_laplacian:\\n                embedding = embedding / dd\\n        except RuntimeError:\\n            eigen_solver = \"lobpcg\"\\n            laplacian *= -1\\n    elif eigen_solver == \"amg\":\\n        if not sparse.issparse(laplacian):\\n            warnings.warn(\"AMG works better for sparse matrices\")\\n        laplacian = check_array(\\n            laplacian, dtype=[np.float64, np.float32], accept_sparse=True\\n        )\\n        laplacian = _set_diag(laplacian, 1, norm_laplacian)\\n        diag_shift = 1e-5 * sparse.eye(laplacian.shape[0])\\n        laplacian += diag_shift\\n        ml = smoothed_aggregation_solver(check_array(laplacian, accept_sparse=\"csr\"))\\n        laplacian -= diag_shift\\n        M = ml.aspreconditioner()\\n        X = random_state.rand(laplacian.shape[0], n_components + 1)\\n        X[:, 0] = dd.ravel()\\n        X = X.astype(laplacian.dtype)\\n        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.0e-5, largest=False)\\n        embedding = diffusion_map.T\\n        if norm_laplacian:\\n            embedding = embedding / dd\\n        if embedding.shape[0] == 1:\\n            raise ValueError\\n    if eigen_solver == \"lobpcg\":\\n        laplacian = check_array(\\n            laplacian, dtype=[np.float64, np.float32], accept_sparse=True\\n        )\\n        if n_nodes < 5 * n_components + 1:\\n            if sparse.isspmatrix(laplacian):\\n                laplacian = laplacian.toarray()\\n            _, diffusion_map = eigh(laplacian, check_finite=False)\\n            embedding = diffusion_map.T[:n_components]\\n            if norm_laplacian:\\n                embedding = embedding / dd\\n        else:\\n            laplacian = _set_diag(laplacian, 1, norm_laplacian)\\n            X = random_state.rand(laplacian.shape[0], n_components + 1)\\n            X[:, 0] = dd.ravel()\\n            X = X.astype(laplacian.dtype)\\n            _, diffusion_map = lobpcg(\\n                laplacian, X, tol=1e-5, largest=False, maxiter=2000\\n            )\\n            embedding = diffusion_map.T[:n_components]\\n            if norm_laplacian:\\n                embedding = embedding / dd\\n            if embedding.shape[0] == 1:\\n                raise ValueError\\n    embedding = _deterministic_vector_sign_flip(embedding)\\n    if drop_first:\\n        return embedding[1:n_components].T\\n    else:\\n        return embedding[:n_components].T",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def spectral_embedding(\\n    adjacency,\\n    *,\\n    n_components=8,\\n    eigen_solver=None,\\n    random_state=None,\\n    eigen_tol=0.0,\\n    norm_laplacian=True,\\n    drop_first=True,\\n):\\n    adjacency = check_symmetric(adjacency)\\n    try:\\n        from pyamg import smoothed_aggregation_solver\\n    except ImportError as e:\\n        if eigen_solver == \"amg\":\\n            raise ValueError(\\n                \"The eigen_solver was set to 'amg', but pyamg is not available.\"\\n            ) from e\\n    if eigen_solver is None:\\n        eigen_solver = \"arpack\"\\n    elif eigen_solver not in (\"arpack\", \"lobpcg\", \"amg\"):\\n        raise ValueError(\\n            \"Unknown value for eigen_solver: '%s'.\"\\n            \"Should be 'amg', 'arpack', or 'lobpcg'\" % eigen_solver\\n        )\\n    random_state = check_random_state(random_state)\\n    n_nodes = adjacency.shape[0]\\n    if drop_first:\\n        n_components = n_components + 1\\n    if not _graph_is_connected(adjacency):\\n        warnings.warn(\\n            \"Graph is not fully connected, spectral embedding may not work as expected.\"\\n        )\\n    laplacian, dd = csgraph_laplacian(\\n        adjacency, normed=norm_laplacian, return_diag=True\\n    )\\n    if (\\n        eigen_solver == \"arpack\"\\n        or eigen_solver != \"lobpcg\"\\n        and (not sparse.isspmatrix(laplacian) or n_nodes < 5 * n_components)\\n    ):\\n        laplacian = _set_diag(laplacian, 1, norm_laplacian)\\n        try:\\n            laplacian *= -1\\n            v0 = _init_arpack_v0(laplacian.shape[0], random_state)\\n            _, diffusion_map = eigsh(\\n                laplacian, k=n_components, sigma=1.0, which=\"LM\", tol=eigen_tol, v0=v0\\n            )\\n            embedding = diffusion_map.T[n_components::-1]\\n            if norm_laplacian:\\n                embedding = embedding / dd\\n        except RuntimeError:\\n            eigen_solver = \"lobpcg\"\\n            laplacian *= -1\\n    elif eigen_solver == \"amg\":\\n        if not sparse.issparse(laplacian):\\n            warnings.warn(\"AMG works better for sparse matrices\")\\n        laplacian = check_array(\\n            laplacian, dtype=[np.float64, np.float32], accept_sparse=True\\n        )\\n        laplacian = _set_diag(laplacian, 1, norm_laplacian)\\n        diag_shift = 1e-5 * sparse.eye(laplacian.shape[0])\\n        laplacian += diag_shift\\n        ml = smoothed_aggregation_solver(check_array(laplacian, accept_sparse=\"csr\"))\\n        laplacian -= diag_shift\\n        M = ml.aspreconditioner()\\n        X = random_state.rand(laplacian.shape[0], n_components + 1)\\n        X[:, 0] = dd.ravel()\\n        X = X.astype(laplacian.dtype)\\n        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.0e-5, largest=False)\\n        embedding = diffusion_map.T\\n        if norm_laplacian:\\n            embedding = embedding / dd\\n        if embedding.shape[0] == 1:\\n            raise ValueError\\n    if eigen_solver == \"lobpcg\":\\n        laplacian = check_array(\\n            laplacian, dtype=[np.float64, np.float32], accept_sparse=True\\n        )\\n        if n_nodes < 5 * n_components + 1:\\n            if sparse.isspmatrix(laplacian):\\n                laplacian = laplacian.toarray()\\n            _, diffusion_map = eigh(laplacian, check_finite=False)\\n            embedding = diffusion_map.T[:n_components]\\n            if norm_laplacian:\\n                embedding = embedding / dd\\n        else:\\n            laplacian = _set_diag(laplacian, 1, norm_laplacian)\\n            X = random_state.rand(laplacian.shape[0], n_components + 1)\\n            X[:, 0] = dd.ravel()\\n            X = X.astype(laplacian.dtype)\\n            _, diffusion_map = lobpcg(\\n                laplacian, X, tol=1e-5, largest=False, maxiter=2000\\n            )\\n            embedding = diffusion_map.T[:n_components]\\n            if norm_laplacian:\\n                embedding = embedding / dd\\n            if embedding.shape[0] == 1:\\n                raise ValueError\\n    embedding = _deterministic_vector_sign_flip(embedding)\\n    if drop_first:\\n        return embedding[1:n_components].T\\n    else:\\n        return embedding[:n_components].T"
  },
  {
    "code": "def _r2_raw(self):\\n        has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR\\n        if self._intercept:\\n            return 1 - self.sm_ols.ssr / self.sm_ols.centered_tss\\n        else:\\n            return 1 - self.sm_ols.ssr / self.sm_ols.uncentered_tss\\n    @cache_readonly",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _r2_raw(self):\\n        has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR\\n        if self._intercept:\\n            return 1 - self.sm_ols.ssr / self.sm_ols.centered_tss\\n        else:\\n            return 1 - self.sm_ols.ssr / self.sm_ols.uncentered_tss\\n    @cache_readonly"
  },
  {
    "code": "def subtract(self, a, b):\\n        return a.__sub__(b, context=self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #7633: Context method in the decimal module (with the exception of the 'canonical' and 'is_canonical' methods) now consistently accept integer arguments wherever a Decimal instance is accepted.  Thanks Juan Jos\u00e9 Conti for the patch.",
    "fixed_code": "def subtract(self, a, b):\\n        a = _convert_other(a, raiseit=True)\\n        r = a.__sub__(b, context=self)\\n        if r is NotImplemented:\\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\\n        else:\\n            return r"
  },
  {
    "code": "def column_or_1d(y, warn=False):\\n    shape = np.shape(y)\\n    if len(shape) == 1:\\n        return np.ravel(y)\\n    if len(shape) == 2 and shape[1] == 1:\\n        if warn:\\n            warnings.warn(\"A column-vector y was passed when a 1d array was\"\\n                          \" expected. Please change the shape of y to \"\\n                          \"(n_samples, ), for example using ravel().\")\\n        return np.ravel(y)\\n    raise ValueError(\"bad input shape {0}\".format(shape))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add a new DataConversionWarning\\n\\nfor implicit conversions such as raveling the y",
    "fixed_code": "def column_or_1d(y, warn=False):\\n    shape = np.shape(y)\\n    if len(shape) == 1:\\n        return np.ravel(y)\\n    if len(shape) == 2 and shape[1] == 1:\\n        if warn:\\n            warnings.warn(\"A column-vector y was passed when a 1d array was\"\\n                          \" expected. Please change the shape of y to \"\\n                          \"(n_samples, ), for example using ravel().\",\\n                          DataConversionWarning, stacklevel=2)\\n        return np.ravel(y)\\n    raise ValueError(\"bad input shape {0}\".format(shape))"
  },
  {
    "code": "def check_bool_indexer(index: Index, key) -> np.ndarray:\\n    result = key\\n    if isinstance(key, ABCSeries) and not key.index.equals(index):\\n        indexer = result.index.get_indexer_for(index)\\n        if -1 in indexer:\\n            raise IndexingError(\\n                \"Unalignable boolean Series provided as \"\\n                \"indexer (index of the boolean Series and of \"\\n                \"the indexed object do not match).\"\\n            )\\n        result = result.take(indexer)\\n        if not is_extension_array_dtype(result.dtype):\\n            return result.astype(bool)._values\\n    if is_object_dtype(key):\\n        result = np.asarray(result, dtype=bool)\\n    elif not is_array_like(result):\\n        result = pd_array(result, dtype=bool)\\n    return check_array_indexer(index, result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_bool_indexer(index: Index, key) -> np.ndarray:\\n    result = key\\n    if isinstance(key, ABCSeries) and not key.index.equals(index):\\n        indexer = result.index.get_indexer_for(index)\\n        if -1 in indexer:\\n            raise IndexingError(\\n                \"Unalignable boolean Series provided as \"\\n                \"indexer (index of the boolean Series and of \"\\n                \"the indexed object do not match).\"\\n            )\\n        result = result.take(indexer)\\n        if not is_extension_array_dtype(result.dtype):\\n            return result.astype(bool)._values\\n    if is_object_dtype(key):\\n        result = np.asarray(result, dtype=bool)\\n    elif not is_array_like(result):\\n        result = pd_array(result, dtype=bool)\\n    return check_array_indexer(index, result)"
  },
  {
    "code": "def as_table(self):\\n        \"Returns this form rendered as HTML <tr>s -- excluding the <table></table>.\"\\n        return u'\\n'.join(['<tr><td>%s:</td><td>%s</td></tr>' % (pretty_name(name), BoundField(self, field, name)) for name, field in self.fields.items()])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_table(self):\\n        \"Returns this form rendered as HTML <tr>s -- excluding the <table></table>.\"\\n        return u'\\n'.join(['<tr><td>%s:</td><td>%s</td></tr>' % (pretty_name(name), BoundField(self, field, name)) for name, field in self.fields.items()])"
  },
  {
    "code": "def get_ident():\\t\\treturn -1",
    "label": 1,
    "bug_type": "numeric",
    "bug_description": "bpo-6532: Make the thread id an unsigned integer. (#781)\\n\\n\\nFrom C API side the type of results of PyThread_start_new_thread() and\\nPyThread_get_thread_ident(), the id parameter of\\nPyThreadState_SetAsyncExc(), and the thread_id field of PyThreadState\\nchanged from \"long\" to \"unsigned long\".",
    "fixed_code": "def get_ident():\\n\\treturn 1"
  },
  {
    "code": "def _round_half_up(self, prec):\\n        if self._int[prec] in '56789':\\n            return 1\\n        elif _all_zeros(self._int, prec):\\n            return 0\\n        else:\\n            return -1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _round_half_up(self, prec):\\n        if self._int[prec] in '56789':\\n            return 1\\n        elif _all_zeros(self._int, prec):\\n            return 0\\n        else:\\n            return -1"
  },
  {
    "code": "def max(self, axis='major'):\\n        i = self._get_axis_number(axis)\\n        y = np.array(self.values)\\n        mask = np.isfinite(y)\\n        y[-mask] = -np.inf\\n        result = y.max(axis=i)\\n        result = np.where(np.isneginf(result), np.nan, result)\\n        return self._wrap_result(result, axis)\\n    _add_docs(max, 'maximum', 'maximum')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: skipna and unit testing for Panel",
    "fixed_code": "def max(self, axis='major', skipna=True):\\n        i = self._get_axis_number(axis)\\n        y = np.array(self.values)\\n        if skipna:\\n            np.putmask(y, np.isnan(y), -np.inf)\\n        result = y.max(axis=i)\\n        result = np.where(np.isneginf(result), np.nan, result)\\n        return self._wrap_result(result, axis)\\n    _add_docs(max, 'maximum', 'maximum')"
  },
  {
    "code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in seq:\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or (\"|\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 82076 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n................\\n  r82076 | jean-paul.calderone | 2010-06-18 16:03:54 -0400 (Fri, 18 Jun 2010) | 19 lines\\n\\n  Merged revisions 82075 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r82075 | jean-paul.calderone | 2010-06-18 16:00:17 -0400 (Fri, 18 Jun 2010) | 12 lines\\n\\n    Revert r60115\\n\\n    This revision introduced quoting for strings containing | based\\n    on a misunderstanding of the commonly used quoting rules used\\n    on Windows.\\n\\n    | is interpreted by cmd.exe, not by the MS C runtime argv initializer.\\n    It only needs to be quoted if it is part of an argument passed through\\n    cmd.exe.\\n\\n    See issue1300, issue7839, and issue8972.\\n  ........\\n................",
    "fixed_code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in seq:\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)"
  },
  {
    "code": "def to_feather(df, path):\\n    path = _stringify_path(path)\\n    if not isinstance(df, DataFrame):\\n        raise ValueError(\"feather only support IO with DataFrames\")\\n    feather = _try_import()[0]\\n    valid_types = {'string', 'unicode'}\\n    if not isinstance(df.index, Int64Index):\\n        raise ValueError(\"feather does not support serializing {} \"\\n                         \"for the index; you can .reset_index()\"\\n                         \"to make the index into column(s)\".format(\\n                             type(df.index)))\\n    if not df.index.equals(RangeIndex.from_range(range(len(df)))):\\n        raise ValueError(\"feather does not support serializing a \"\\n                         \"non-default index for the index; you \"\\n                         \"can .reset_index() to make the index \"\\n                         \"into column(s)\")\\n    if df.index.name is not None:\\n        raise ValueError(\"feather does not serialize index meta-data on a \"\\n                         \"default index\")\\n    if df.columns.inferred_type not in valid_types:\\n        raise ValueError(\"feather must have string column names\")\\n    feather.write_feather(df, path)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_feather(df, path):\\n    path = _stringify_path(path)\\n    if not isinstance(df, DataFrame):\\n        raise ValueError(\"feather only support IO with DataFrames\")\\n    feather = _try_import()[0]\\n    valid_types = {'string', 'unicode'}\\n    if not isinstance(df.index, Int64Index):\\n        raise ValueError(\"feather does not support serializing {} \"\\n                         \"for the index; you can .reset_index()\"\\n                         \"to make the index into column(s)\".format(\\n                             type(df.index)))\\n    if not df.index.equals(RangeIndex.from_range(range(len(df)))):\\n        raise ValueError(\"feather does not support serializing a \"\\n                         \"non-default index for the index; you \"\\n                         \"can .reset_index() to make the index \"\\n                         \"into column(s)\")\\n    if df.index.name is not None:\\n        raise ValueError(\"feather does not serialize index meta-data on a \"\\n                         \"default index\")\\n    if df.columns.inferred_type not in valid_types:\\n        raise ValueError(\"feather must have string column names\")\\n    feather.write_feather(df, path)"
  },
  {
    "code": "def _make_concat_multiindex(indexes, keys, levels=None, names=None):\\n    if ((levels is None and isinstance(keys[0], tuple)) or\\n        (levels is not None and len(levels) > 1)):\\n        zipped = zip(*keys)\\n        if names is None:\\n            names = [None] * len(zipped)\\n        if levels is None:\\n            levels = [Factor(zp).levels for zp in zipped]\\n        else:\\n            levels = [_ensure_index(x) for x in levels]\\n    else:\\n        zipped = [keys]\\n        if names is None:\\n            names = [None]\\n        if levels is None:\\n            levels = [_ensure_index(keys)]\\n        else:\\n            levels = [_ensure_index(x) for x in levels]\\n    if not _all_indexes_same(indexes):\\n        label_list = []\\n        for hlevel, level in zip(zipped, levels):\\n            to_concat = []\\n            for key, index in zip(hlevel, indexes):\\n                i = level.get_loc(key)\\n                to_concat.append(np.repeat(i, len(index)))\\n            label_list.append(np.concatenate(to_concat))\\n        concat_index = _concat_indexes(indexes)\\n        if isinstance(concat_index, MultiIndex):\\n            levels.extend(concat_index.levels)\\n            label_list.extend(concat_index.labels)\\n        else:\\n            factor = Factor(concat_index)\\n            levels.append(factor.levels)\\n            label_list.append(factor.labels)\\n        names = names + _get_consensus_names(indexes)\\n        return MultiIndex(levels=levels, labels=label_list, names=names)\\n    new_index = indexes[0]\\n    n = len(new_index)\\n    kpieces = len(indexes)\\n    new_names = list(names)\\n    new_levels = list(levels)\\n    new_labels = []\\n    for hlevel, level in zip(zipped, levels):\\n        mapped = level.get_indexer(hlevel)\\n        new_labels.append(np.repeat(mapped, n))\\n    if isinstance(new_index, MultiIndex):\\n        new_levels.extend(new_index.levels)\\n        new_labels.extend([np.tile(lab, kpieces) for lab in new_index.labels])\\n        new_names.extend(new_index.names)\\n    else:\\n        new_levels.append(new_index)\\n        new_names.append(new_index.name)\\n        new_labels.append(np.tile(np.arange(n), kpieces))\\n    return MultiIndex(levels=new_levels, labels=new_labels, names=new_names)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _make_concat_multiindex(indexes, keys, levels=None, names=None):\\n    if ((levels is None and isinstance(keys[0], tuple)) or\\n        (levels is not None and len(levels) > 1)):\\n        zipped = zip(*keys)\\n        if names is None:\\n            names = [None] * len(zipped)\\n        if levels is None:\\n            levels = [Factor(zp).levels for zp in zipped]\\n        else:\\n            levels = [_ensure_index(x) for x in levels]\\n    else:\\n        zipped = [keys]\\n        if names is None:\\n            names = [None]\\n        if levels is None:\\n            levels = [_ensure_index(keys)]\\n        else:\\n            levels = [_ensure_index(x) for x in levels]\\n    if not _all_indexes_same(indexes):\\n        label_list = []\\n        for hlevel, level in zip(zipped, levels):\\n            to_concat = []\\n            for key, index in zip(hlevel, indexes):\\n                i = level.get_loc(key)\\n                to_concat.append(np.repeat(i, len(index)))\\n            label_list.append(np.concatenate(to_concat))\\n        concat_index = _concat_indexes(indexes)\\n        if isinstance(concat_index, MultiIndex):\\n            levels.extend(concat_index.levels)\\n            label_list.extend(concat_index.labels)\\n        else:\\n            factor = Factor(concat_index)\\n            levels.append(factor.levels)\\n            label_list.append(factor.labels)\\n        names = names + _get_consensus_names(indexes)\\n        return MultiIndex(levels=levels, labels=label_list, names=names)\\n    new_index = indexes[0]\\n    n = len(new_index)\\n    kpieces = len(indexes)\\n    new_names = list(names)\\n    new_levels = list(levels)\\n    new_labels = []\\n    for hlevel, level in zip(zipped, levels):\\n        mapped = level.get_indexer(hlevel)\\n        new_labels.append(np.repeat(mapped, n))\\n    if isinstance(new_index, MultiIndex):\\n        new_levels.extend(new_index.levels)\\n        new_labels.extend([np.tile(lab, kpieces) for lab in new_index.labels])\\n        new_names.extend(new_index.names)\\n    else:\\n        new_levels.append(new_index)\\n        new_names.append(new_index.name)\\n        new_labels.append(np.tile(np.arange(n), kpieces))\\n    return MultiIndex(levels=new_levels, labels=new_labels, names=new_names)"
  },
  {
    "code": "def _convert_to_ascii(self, *values):\\n        for value in values:\\n            if isinstance(value, six.text_type):\\n                try:\\n                    if not six.PY3:\\n                        value = value.encode('us-ascii')\\n                    else:\\n                        value.encode('us-ascii')\\n                except UnicodeError as e:\\n                    e.reason += ', HTTP response headers must be in US-ASCII format'\\n                    raise\\n            else:\\n                value = str(value)\\n            if '\\n' in value or '\\r' in value:\\n                raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % (value))\\n            yield value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[py3] Ported django.http according to PEP 3333.\\n\\nPerfomed some style cleanup while I was in the area.",
    "fixed_code": "def _convert_to_ascii(self, *values):\\n        for value in values:\\n            if not isinstance(value, six.string_types):\\n                value = str(value)\\n            try:\\n                if six.PY3:\\n                    value.encode('us-ascii')\\n                else:\\n                    if isinstance(value, str):\\n                        value.decode('us-ascii')\\n                    else:\\n                        value = value.encode('us-ascii')\\n            except UnicodeError as e:\\n                e.reason += ', HTTP response headers must be in US-ASCII format'\\n                raise\\n            if '\\n' in value or '\\r' in value:\\n                raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\\n            yield value"
  },
  {
    "code": "def _aggregate_multiple_funcs(self, arg):\\n        if isinstance(arg, dict):\\n            columns = arg.keys()\\n            arg = arg.items()\\n        elif isinstance(arg[0], (tuple, list)):\\n            columns = zip(*arg)[0]\\n        else:\\n            columns = [func.__name__ for func in arg]\\n            arg = zip(columns, arg)\\n        results = {}\\n        for name, func in arg:\\n            results[name] = self.aggregate(func)\\n        return DataFrame(results, columns=columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_multiple_funcs(self, arg):\\n        if isinstance(arg, dict):\\n            columns = arg.keys()\\n            arg = arg.items()\\n        elif isinstance(arg[0], (tuple, list)):\\n            columns = zip(*arg)[0]\\n        else:\\n            columns = [func.__name__ for func in arg]\\n            arg = zip(columns, arg)\\n        results = {}\\n        for name, func in arg:\\n            results[name] = self.aggregate(func)\\n        return DataFrame(results, columns=columns)"
  },
  {
    "code": "def hlines(\\n\\t\\ty, xmin, xmax, colors=None, linestyles='solid', label='', *,\\n\\t\\tdata=None, **kwargs):\\n\\treturn gca().hlines(\\n\\t\\ty, xmin, xmax, colors=colors, linestyles=linestyles,\\n\\t\\tlabel=label, **({\"data\": data} if data is not None else {}),\\n\\t\\t**kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def hlines(\\n\\t\\ty, xmin, xmax, colors=None, linestyles='solid', label='', *,\\n\\t\\tdata=None, **kwargs):\\n\\treturn gca().hlines(\\n\\t\\ty, xmin, xmax, colors=colors, linestyles=linestyles,\\n\\t\\tlabel=label, **({\"data\": data} if data is not None else {}),\\n\\t\\t**kwargs)"
  },
  {
    "code": "def replace(self, pat, repl, n=-1, case=True, flags=0):\\n        result = str_replace(self._data, pat, repl, n=n, case=case,\\n                             flags=flags)\\n        return self._wrap_result(result)\\n    @copy(str_repeat)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: str.replace accepts a compiled expression (#15456)\\n\\n- Series.str.replace now accepts a compiled regular expression for `pat`.\\n- Signature for .str.replace changed, but remains backwards compatible.\\n\\nSee #15446",
    "fixed_code": "def replace(self, pat, repl, n=-1, case=None, flags=0):\\n        result = str_replace(self._data, pat, repl, n=n, case=case,\\n                             flags=flags)\\n        return self._wrap_result(result)\\n    @copy(str_repeat)"
  },
  {
    "code": "def __new__(cls, *args, **kwargs):\\n        raise TypeError(\"Type Annotated cannot be instantiated.\")\\n    @_tp_cache",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-46998: Allow subclassing Any at runtime (GH-31841)",
    "fixed_code": "def __new__(cls, *args, **kwargs):\\n        if cls is Any:\\n            raise TypeError(\"Any cannot be instantiated\")\\n        return super().__new__(cls, *args, **kwargs)"
  },
  {
    "code": "def get_features_used(node: Node) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n    return features",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Deprecate Python 2 formatting support (#2523)\\n\\n\\n- Use BlackRunner and .stdout in command line test\\n\\nSo the next commit won't break this test. This is in its own commit so\\nwe can just revert the depreciation commit when dropping Python 2\\nsupport completely.",
    "fixed_code": "def get_features_used(node: Node) -> Set[Feature]:  \\n    features: Set[Feature] = set()\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n        elif n.type == token.PRINT_STMT:\\n            features.add(Feature.PRINT_STMT)\\n        elif n.type == token.EXEC_STMT:\\n            features.add(Feature.EXEC_STMT)\\n    return features"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        vrfs=dict(type='list'),\\n        name=dict(),\\n        description=dict(),\\n        rd=dict(),\\n        interfaces=dict(type='list'),\\n        delay=dict(default=10, type='int'),\\n        purge=dict(type='bool', default=False),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'vrfs')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    result = {'changed': False}\\n    warnings = list()\\n    check_args(module, warnings)\\n    result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_vrfs = [x['name'] for x in want]\\n        have_vrfs = [x['name'] for x in have]\\n        for item in set(have_vrfs).difference(want_vrfs):\\n            cmd = 'no vrf definition %s' % item\\n            if cmd not in commands:\\n                commands.append(cmd)\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    if result['changed']:\\n        time.sleep(module.params['delay'])\\n    check_declarative_intent_params(want, module)\\n    module.exit_json(**result)\\nif __name__ == '__main__':\\n    main()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        vrfs=dict(type='list'),\\n        name=dict(),\\n        description=dict(),\\n        rd=dict(),\\n        interfaces=dict(type='list'),\\n        delay=dict(default=10, type='int'),\\n        purge=dict(type='bool', default=False),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'vrfs')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    result = {'changed': False}\\n    warnings = list()\\n    check_args(module, warnings)\\n    result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_vrfs = [x['name'] for x in want]\\n        have_vrfs = [x['name'] for x in have]\\n        for item in set(have_vrfs).difference(want_vrfs):\\n            cmd = 'no vrf definition %s' % item\\n            if cmd not in commands:\\n                commands.append(cmd)\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    if result['changed']:\\n        time.sleep(module.params['delay'])\\n    check_declarative_intent_params(want, module)\\n    module.exit_json(**result)\\nif __name__ == '__main__':\\n    main()"
  },
  {
    "code": "def create_unix_group(self):\\n        if self.parameters.get('id') is None:\\n            self.module.fail_json(msg='Error: Missing a required parameter for create: (id)')\\n        group_create = netapp_utils.zapi.NaElement('name-mapping-unix-group-create')\\n        group_details = {}\\n        for item in self.parameters:\\n            if item in self.na_helper.zapi_string_keys:\\n                zapi_key = self.na_helper.zapi_string_keys.get(item)\\n                group_details[zapi_key] = self.parameters[item]\\n            elif item in self.na_helper.zapi_bool_keys:\\n                zapi_key = self.na_helper.zapi_bool_keys.get(item)\\n                group_details[zapi_key] = self.na_helper.get_value_for_bool(from_zapi=False,\\n                                                                            value=self.parameters[item])\\n            elif item in self.na_helper.zapi_int_keys:\\n                zapi_key = self.na_helper.zapi_int_keys.get(item)\\n                group_details[zapi_key] = self.na_helper.get_value_for_int(from_zapi=True,\\n                                                                           value=self.parameters[item])\\n        group_create.translate_struct(group_details)\\n        try:\\n            self.server.invoke_successfully(group_create, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error creating UNIX group %s: %s' % (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        if self.parameters.get('users') is not None:\\n            self.modify_users_in_group()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_unix_group(self):\\n        if self.parameters.get('id') is None:\\n            self.module.fail_json(msg='Error: Missing a required parameter for create: (id)')\\n        group_create = netapp_utils.zapi.NaElement('name-mapping-unix-group-create')\\n        group_details = {}\\n        for item in self.parameters:\\n            if item in self.na_helper.zapi_string_keys:\\n                zapi_key = self.na_helper.zapi_string_keys.get(item)\\n                group_details[zapi_key] = self.parameters[item]\\n            elif item in self.na_helper.zapi_bool_keys:\\n                zapi_key = self.na_helper.zapi_bool_keys.get(item)\\n                group_details[zapi_key] = self.na_helper.get_value_for_bool(from_zapi=False,\\n                                                                            value=self.parameters[item])\\n            elif item in self.na_helper.zapi_int_keys:\\n                zapi_key = self.na_helper.zapi_int_keys.get(item)\\n                group_details[zapi_key] = self.na_helper.get_value_for_int(from_zapi=True,\\n                                                                           value=self.parameters[item])\\n        group_create.translate_struct(group_details)\\n        try:\\n            self.server.invoke_successfully(group_create, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error creating UNIX group %s: %s' % (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        if self.parameters.get('users') is not None:\\n            self.modify_users_in_group()"
  },
  {
    "code": "def create_changeset(module, stack_params, cfn, events_limit):\\n    if 'TemplateBody' not in stack_params and 'TemplateURL' not in stack_params:\\n        module.fail_json(msg=\"Either 'template' or 'template_url' is required.\")\\n    if module.params['changeset_name'] is not None:\\n        stack_params['ChangeSetName'] = module.params['changeset_name']\\n    stack_params.pop('ClientRequestToken', None)\\n    try:\\n        changeset_name = build_changeset_name(stack_params)\\n        stack_params['ChangeSetName'] = changeset_name\\n        pending_changesets = list_changesets(cfn, stack_params['StackName'])\\n        if changeset_name in pending_changesets:\\n            warning = 'WARNING: %d pending changeset(s) exist(s) for this stack!' % len(pending_changesets)\\n            result = dict(changed=False, output='ChangeSet %s already exists.' % changeset_name, warnings=[warning])\\n        else:\\n            cs = cfn.create_change_set(**stack_params)\\n            time_end = time.time() + 600\\n            while time.time() < time_end:\\n                try:\\n                    newcs = cfn.describe_change_set(ChangeSetName=cs['Id'])\\n                except botocore.exceptions.BotoCoreError as err:\\n                    error_msg = boto_exception(err)\\n                    module.fail_json(msg=error_msg)\\n                if newcs['Status'] == 'CREATE_PENDING' or newcs['Status'] == 'CREATE_IN_PROGRESS':\\n                    time.sleep(1)\\n                elif newcs['Status'] == 'FAILED' and \"The submitted information didn't contain changes\" in newcs['StatusReason']:\\n                    cfn.delete_change_set(ChangeSetName=cs['Id'])\\n                    result = dict(changed=False,\\n                                  output='Stack is already up-to-date, Change Set refused to create due to lack of changes.')\\n                    module.exit_json(**result)\\n                else:\\n                    break\\n                time.sleep(1)\\n            result = stack_operation(cfn, stack_params['StackName'], 'CREATE_CHANGESET', events_limit)\\n            result['warnings'] = ['Created changeset named %s for stack %s' % (changeset_name, stack_params['StackName']),\\n                                  'You can execute it using: aws cloudformation execute-change-set --change-set-name %s' % cs['Id'],\\n                                  'NOTE that dependencies on this stack might fail due to pending changes!']\\n    except Exception as err:\\n        error_msg = boto_exception(err)\\n        if 'No updates are to be performed.' in error_msg:\\n            result = dict(changed=False, output='Stack is already up-to-date.')\\n        else:\\n            module.fail_json(msg=\"Failed to create change set: {0}\".format(error_msg), exception=traceback.format_exc())\\n    if not result:\\n        module.fail_json(msg=\"empty result\")\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_changeset(module, stack_params, cfn, events_limit):\\n    if 'TemplateBody' not in stack_params and 'TemplateURL' not in stack_params:\\n        module.fail_json(msg=\"Either 'template' or 'template_url' is required.\")\\n    if module.params['changeset_name'] is not None:\\n        stack_params['ChangeSetName'] = module.params['changeset_name']\\n    stack_params.pop('ClientRequestToken', None)\\n    try:\\n        changeset_name = build_changeset_name(stack_params)\\n        stack_params['ChangeSetName'] = changeset_name\\n        pending_changesets = list_changesets(cfn, stack_params['StackName'])\\n        if changeset_name in pending_changesets:\\n            warning = 'WARNING: %d pending changeset(s) exist(s) for this stack!' % len(pending_changesets)\\n            result = dict(changed=False, output='ChangeSet %s already exists.' % changeset_name, warnings=[warning])\\n        else:\\n            cs = cfn.create_change_set(**stack_params)\\n            time_end = time.time() + 600\\n            while time.time() < time_end:\\n                try:\\n                    newcs = cfn.describe_change_set(ChangeSetName=cs['Id'])\\n                except botocore.exceptions.BotoCoreError as err:\\n                    error_msg = boto_exception(err)\\n                    module.fail_json(msg=error_msg)\\n                if newcs['Status'] == 'CREATE_PENDING' or newcs['Status'] == 'CREATE_IN_PROGRESS':\\n                    time.sleep(1)\\n                elif newcs['Status'] == 'FAILED' and \"The submitted information didn't contain changes\" in newcs['StatusReason']:\\n                    cfn.delete_change_set(ChangeSetName=cs['Id'])\\n                    result = dict(changed=False,\\n                                  output='Stack is already up-to-date, Change Set refused to create due to lack of changes.')\\n                    module.exit_json(**result)\\n                else:\\n                    break\\n                time.sleep(1)\\n            result = stack_operation(cfn, stack_params['StackName'], 'CREATE_CHANGESET', events_limit)\\n            result['warnings'] = ['Created changeset named %s for stack %s' % (changeset_name, stack_params['StackName']),\\n                                  'You can execute it using: aws cloudformation execute-change-set --change-set-name %s' % cs['Id'],\\n                                  'NOTE that dependencies on this stack might fail due to pending changes!']\\n    except Exception as err:\\n        error_msg = boto_exception(err)\\n        if 'No updates are to be performed.' in error_msg:\\n            result = dict(changed=False, output='Stack is already up-to-date.')\\n        else:\\n            module.fail_json(msg=\"Failed to create change set: {0}\".format(error_msg), exception=traceback.format_exc())\\n    if not result:\\n        module.fail_json(msg=\"empty result\")\\n    return result"
  },
  {
    "code": "def readfp(self, fp, filename=None):\\n        warnings.warn(\\n            \"This method will be removed in future versions.  \"\\n            \"Use 'parser.read_file()' instead.\",\\n            PendingDeprecationWarning, stacklevel=2\\n        )\\n        self.read_file(fp, source=filename)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def readfp(self, fp, filename=None):\\n        warnings.warn(\\n            \"This method will be removed in future versions.  \"\\n            \"Use 'parser.read_file()' instead.\",\\n            PendingDeprecationWarning, stacklevel=2\\n        )\\n        self.read_file(fp, source=filename)"
  },
  {
    "code": "def do_open(self, http_class, req):\\n        host = req.get_host()\\n        if not host:\\n            raise URLError('no host given')\\n        h = http_class(host, timeout=req.timeout) \\n        h.set_debuglevel(self._debuglevel)\\n        headers = dict(req.headers)\\n        headers.update(req.unredirected_hdrs)\\n        headers[\"Connection\"] = \"close\"\\n        headers = dict(\\n            (name.title(), val) for name, val in headers.items())\\n        if req._tunnel_host:\\n            tunnel_headers = {}\\n            proxy_auth_hdr = \"Proxy-Authorization\"\\n            if proxy_auth_hdr in headers:\\n                tunnel_headers[proxy_auth_hdr] = headers[proxy_auth_hdr]\\n                del headers[proxy_auth_hdr]\\n            h.set_tunnel(req._tunnel_host, headers=tunnel_headers)\\n        try:\\n            h.request(req.get_method(), req.get_selector(), req.data, headers)\\n            try:\\n                r = h.getresponse(buffering=True)\\n            except TypeError: \\n                r = h.getresponse()\\n        except socket.error, err: \\n            raise URLError(err)\\n        r.recv = r.read\\n        fp = socket._fileobject(r, close=True)\\n        resp = addinfourl(fp, r.msg, req.get_full_url())\\n        resp.code = r.status\\n        resp.msg = r.reason\\n        return resp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 85025 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n........\\n  r85025 | senthil.kumaran | 2010-09-27 06:56:03 +0530 (Mon, 27 Sep 2010) | 6 lines\\n\\n  Fix Issue1595365 - Adding the req.headers after the un-redirect headers have\\n  been added. This helps in accidental overwritting of User-Agent header to\\n  default value. To preserve the old behavior, only headers not in unredirected\\n  headers will be updated.\\n........",
    "fixed_code": "def do_open(self, http_class, req):\\n        host = req.get_host()\\n        if not host:\\n            raise URLError('no host given')\\n        h = http_class(host, timeout=req.timeout) \\n        h.set_debuglevel(self._debuglevel)\\n        headers = dict(req.unredirected_hdrs)\\n        headers.update(dict((k, v) for k, v in req.headers.items()\\n                            if k not in headers))\\n        headers[\"Connection\"] = \"close\"\\n        headers = dict(\\n            (name.title(), val) for name, val in headers.items())\\n        if req._tunnel_host:\\n            tunnel_headers = {}\\n            proxy_auth_hdr = \"Proxy-Authorization\"\\n            if proxy_auth_hdr in headers:\\n                tunnel_headers[proxy_auth_hdr] = headers[proxy_auth_hdr]\\n                del headers[proxy_auth_hdr]\\n            h.set_tunnel(req._tunnel_host, headers=tunnel_headers)\\n        try:\\n            h.request(req.get_method(), req.get_selector(), req.data, headers)\\n            try:\\n                r = h.getresponse(buffering=True)\\n            except TypeError: \\n                r = h.getresponse()\\n        except socket.error, err: \\n            raise URLError(err)\\n        r.recv = r.read\\n        fp = socket._fileobject(r, close=True)\\n        resp = addinfourl(fp, r.msg, req.get_full_url())\\n        resp.code = r.status\\n        resp.msg = r.reason\\n        return resp"
  },
  {
    "code": "def get_conn(self):\\n        http_authorized = self._authorize()\\n        return build('dataproc', self.api_version, http=http_authorized)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_conn(self):\\n        http_authorized = self._authorize()\\n        return build('dataproc', self.api_version, http=http_authorized)"
  },
  {
    "code": "def get_libraries(self, ext):\\n        if sys.platform == \"win32\":\\n            from distutils.msvccompiler import MSVCCompiler\\n            if not isinstance(self.compiler_obj, MSVCCompiler):\\n                template = \"python%d%d\"\\n                if self.debug:\\n                    template = template + '_d'\\n                pythonlib = (template %\\n                       (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries\\n        elif sys.platform == \"os2emx\":\\n            template = \"python%d%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"cygwin\":\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"atheos\":\\n            _sysconfig = __import__('sysconfig')\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            extra = []\\n            for lib in _sysconfig.get_config_var('SHLIBS').split():\\n                if lib.startswith('-l'):\\n                    extra.append(lib[2:])\\n                else:\\n                    extra.append(lib)\\n            return ext.libraries + [pythonlib, \"m\"] + extra\\n        elif sys.platform == 'darwin':\\n            return ext.libraries\\n        else:\\n            _sysconfig = __import__('sysconfig')\\n            if _sysconfig.get_config_var('Py_ENABLE_SHARED'):\\n                template = \"python%d.%d\"\\n                pythonlib = (template %\\n                             (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "reverting partially distutils to its 2.6.x state so 2.7a4 looks more like the 2.7b1 in this. the whole revert will occur after a4 is tagged",
    "fixed_code": "def get_libraries (self, ext):\\n        if sys.platform == \"win32\":\\n            from distutils.msvccompiler import MSVCCompiler\\n            if not isinstance(self.compiler, MSVCCompiler):\\n                template = \"python%d%d\"\\n                if self.debug:\\n                    template = template + '_d'\\n                pythonlib = (template %\\n                       (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries\\n        elif sys.platform == \"os2emx\":\\n            template = \"python%d%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"cygwin\":\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            return ext.libraries + [pythonlib]\\n        elif sys.platform[:6] == \"atheos\":\\n            from distutils import sysconfig\\n            template = \"python%d.%d\"\\n            pythonlib = (template %\\n                   (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n            extra = []\\n            for lib in sysconfig.get_config_var('SHLIBS').split():\\n                if lib.startswith('-l'):\\n                    extra.append(lib[2:])\\n                else:\\n                    extra.append(lib)\\n            return ext.libraries + [pythonlib, \"m\"] + extra\\n        elif sys.platform == 'darwin':\\n            return ext.libraries\\n        else:\\n            from distutils import sysconfig\\n            if sysconfig.get_config_var('Py_ENABLE_SHARED'):\\n                template = \"python%d.%d\"\\n                pythonlib = (template %\\n                             (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\\n                return ext.libraries + [pythonlib]\\n            else:\\n                return ext.libraries"
  },
  {
    "code": "def __setitem__(self, key, value):\\n        if _is_sunder(key):\\n            if key not in (\\n                    '_order_', '_create_pseudo_member_',\\n                    '_generate_next_value_', '_missing_', '_ignore_',\\n                    ):\\n                raise ValueError('_names_ are reserved for future Enum use')\\n            if key == '_generate_next_value_':\\n                if self._auto_called:\\n                    raise TypeError(\"_generate_next_value_ must be defined before members\")\\n                setattr(self, '_generate_next_value', value)\\n            elif key == '_ignore_':\\n                if isinstance(value, str):\\n                    value = value.replace(',',' ').split()\\n                else:\\n                    value = list(value)\\n                self._ignore = value\\n                already = set(value) & set(self._member_names)\\n                if already:\\n                    raise ValueError('_ignore_ cannot specify already set names: %r' % (already, ))\\n        elif _is_dunder(key):\\n            if key == '__order__':\\n                key = '_order_'\\n        elif key in self._member_names:\\n            raise TypeError('Attempted to reuse key: %r' % key)\\n        elif key in self._ignore:\\n            pass\\n        elif not _is_descriptor(value):\\n            if key in self:\\n                raise TypeError('%r already defined as: %r' % (key, self[key]))\\n            if isinstance(value, auto):\\n                self._auto_called = True\\n                if value.value == _auto_null:\\n                    value.value = self._generate_next_value(key, 1, len(self._member_names), self._last_values[:])\\n                value = value.value\\n            self._member_names.append(key)\\n            self._last_values.append(value)\\n        super().__setitem__(key, value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __setitem__(self, key, value):\\n        if _is_sunder(key):\\n            if key not in (\\n                    '_order_', '_create_pseudo_member_',\\n                    '_generate_next_value_', '_missing_', '_ignore_',\\n                    ):\\n                raise ValueError('_names_ are reserved for future Enum use')\\n            if key == '_generate_next_value_':\\n                if self._auto_called:\\n                    raise TypeError(\"_generate_next_value_ must be defined before members\")\\n                setattr(self, '_generate_next_value', value)\\n            elif key == '_ignore_':\\n                if isinstance(value, str):\\n                    value = value.replace(',',' ').split()\\n                else:\\n                    value = list(value)\\n                self._ignore = value\\n                already = set(value) & set(self._member_names)\\n                if already:\\n                    raise ValueError('_ignore_ cannot specify already set names: %r' % (already, ))\\n        elif _is_dunder(key):\\n            if key == '__order__':\\n                key = '_order_'\\n        elif key in self._member_names:\\n            raise TypeError('Attempted to reuse key: %r' % key)\\n        elif key in self._ignore:\\n            pass\\n        elif not _is_descriptor(value):\\n            if key in self:\\n                raise TypeError('%r already defined as: %r' % (key, self[key]))\\n            if isinstance(value, auto):\\n                self._auto_called = True\\n                if value.value == _auto_null:\\n                    value.value = self._generate_next_value(key, 1, len(self._member_names), self._last_values[:])\\n                value = value.value\\n            self._member_names.append(key)\\n            self._last_values.append(value)\\n        super().__setitem__(key, value)"
  },
  {
    "code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    with warnings.catch_warnings():\\n        warnings.simplefilter(\"ignore\", (SyntaxWarning, DeprecationWarning))\\n        try:\\n            compiler(source, filename, symbol)\\n        except SyntaxError:  \\n            try:\\n                compiler(source + \"\\n\", filename, symbol)\\n                return None\\n            except SyntaxError as e:\\n                if \"incomplete input\" in str(e):\\n                    return None\\n    return compiler(source, filename, symbol)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    with warnings.catch_warnings():\\n        warnings.simplefilter(\"ignore\", (SyntaxWarning, DeprecationWarning))\\n        try:\\n            compiler(source, filename, symbol)\\n        except SyntaxError:  \\n            try:\\n                compiler(source + \"\\n\", filename, symbol)\\n                return None\\n            except SyntaxError as e:\\n                if \"incomplete input\" in str(e):\\n                    return None\\n    return compiler(source, filename, symbol)"
  },
  {
    "code": "def read_multi(self, environ, keep_blank_values, strict_parsing):\\n\\t\\tib = self.innerboundary\\n\\t\\tif not valid_boundary(ib):\\n\\t\\t\\traise ValueError('Invalid boundary in multipart form: %r' % (ib,))\\n\\t\\tself.list = []\\n\\t\\tif self.qs_on_post:\\n\\t\\t\\tquery = urllib.parse.parse_qsl(\\n\\t\\t\\t\\tself.qs_on_post, self.keep_blank_values, self.strict_parsing,\\n\\t\\t\\t\\tencoding=self.encoding, errors=self.errors,\\n\\t\\t\\t\\tmax_num_fields=self.max_num_fields)\\n\\t\\t\\tself.list.extend(MiniFieldStorage(key, value) for key, value in query)\\n\\t\\tklass = self.FieldStorageClass or self.__class__\\n\\t\\tfirst_line = self.fp.readline() \\n\\t\\tif not isinstance(first_line, bytes):\\n\\t\\t\\traise ValueError(\"%s should return bytes, got %s\" \\\\n\\t\\t\\t\\t\\t\\t\\t % (self.fp, type(first_line).__name__))\\n\\t\\tself.bytes_read += len(first_line)\\n\\t\\twhile (first_line.strip() != (b\"--\" + self.innerboundary) and\\n\\t\\t\\t\\tfirst_line):\\n\\t\\t\\tfirst_line = self.fp.readline()\\n\\t\\t\\tself.bytes_read += len(first_line)\\n\\t\\tmax_num_fields = self.max_num_fields\\n\\t\\tif max_num_fields is not None:\\n\\t\\t\\tmax_num_fields -= len(self.list)\\n\\t\\twhile True:\\n\\t\\t\\tparser = FeedParser()\\n\\t\\t\\thdr_text = b\"\"\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\tdata = self.fp.readline()\\n\\t\\t\\t\\thdr_text += data\\n\\t\\t\\t\\tif not data.strip():\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tif not hdr_text:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tself.bytes_read += len(hdr_text)\\n\\t\\t\\tparser.feed(hdr_text.decode(self.encoding, self.errors))\\n\\t\\t\\theaders = parser.close()\\n\\t\\t\\tif 'content-length' in headers:\\n\\t\\t\\t\\tdel headers['content-length']\\n\\t\\t\\tlimit = None if self.limit is None \\\\n\\t\\t\\t\\telse self.limit - self.bytes_read\\n\\t\\t\\tpart = klass(self.fp, headers, ib, environ, keep_blank_values,\\n\\t\\t\\t\\t\\t\\t strict_parsing, limit,\\n\\t\\t\\t\\t\\t\\t self.encoding, self.errors, max_num_fields)\\n\\t\\t\\tif max_num_fields is not None:\\n\\t\\t\\t\\tmax_num_fields -= 1\\n\\t\\t\\t\\tif part.list:\\n\\t\\t\\t\\t\\tmax_num_fields -= len(part.list)\\n\\t\\t\\t\\tif max_num_fields < 0:\\n\\t\\t\\t\\t\\traise ValueError('Max number of fields exceeded')\\n\\t\\t\\tself.bytes_read += part.bytes_read\\n\\t\\t\\tself.list.append(part)\\n\\t\\t\\tif part.done or self.bytes_read >= self.length > 0:\\n\\t\\t\\t\\tbreak\\n\\t\\tself.skip_lines()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[3.9] bpo-42967: only use '&' as a query string separator (GH-24297) (#24528)\\n\\n(cherry picked from commit fcbe0cb04d35189401c0c880ebfb4311e952d776)\\n\\n\\nbpo-42967: [security] Address a web cache-poisoning issue reported in urllib.parse.parse_qsl().\\n\\nurllib.parse will only us \"&\" as query string separator by default instead of both \";\" and \"&\" as allowed in earlier versions. An optional argument seperator with default value \"&\" is added to specify the separator.",
    "fixed_code": "def read_multi(self, environ, keep_blank_values, strict_parsing):\\n\\t\\tib = self.innerboundary\\n\\t\\tif not valid_boundary(ib):\\n\\t\\t\\traise ValueError('Invalid boundary in multipart form: %r' % (ib,))\\n\\t\\tself.list = []\\n\\t\\tif self.qs_on_post:\\n\\t\\t\\tquery = urllib.parse.parse_qsl(\\n\\t\\t\\t\\tself.qs_on_post, self.keep_blank_values, self.strict_parsing,\\n\\t\\t\\t\\tencoding=self.encoding, errors=self.errors,\\n\\t\\t\\t\\tmax_num_fields=self.max_num_fields, separator=self.separator)\\n\\t\\t\\tself.list.extend(MiniFieldStorage(key, value) for key, value in query)\\n\\t\\tklass = self.FieldStorageClass or self.__class__\\n\\t\\tfirst_line = self.fp.readline() \\n\\t\\tif not isinstance(first_line, bytes):\\n\\t\\t\\traise ValueError(\"%s should return bytes, got %s\" \\\\n\\t\\t\\t\\t\\t\\t\\t % (self.fp, type(first_line).__name__))\\n\\t\\tself.bytes_read += len(first_line)\\n\\t\\twhile (first_line.strip() != (b\"--\" + self.innerboundary) and\\n\\t\\t\\t\\tfirst_line):\\n\\t\\t\\tfirst_line = self.fp.readline()\\n\\t\\t\\tself.bytes_read += len(first_line)\\n\\t\\tmax_num_fields = self.max_num_fields\\n\\t\\tif max_num_fields is not None:\\n\\t\\t\\tmax_num_fields -= len(self.list)\\n\\t\\twhile True:\\n\\t\\t\\tparser = FeedParser()\\n\\t\\t\\thdr_text = b\"\"\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\tdata = self.fp.readline()\\n\\t\\t\\t\\thdr_text += data\\n\\t\\t\\t\\tif not data.strip():\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tif not hdr_text:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tself.bytes_read += len(hdr_text)\\n\\t\\t\\tparser.feed(hdr_text.decode(self.encoding, self.errors))\\n\\t\\t\\theaders = parser.close()\\n\\t\\t\\tif 'content-length' in headers:\\n\\t\\t\\t\\tdel headers['content-length']\\n\\t\\t\\tlimit = None if self.limit is None \\\\n\\t\\t\\t\\telse self.limit - self.bytes_read\\n\\t\\t\\tpart = klass(self.fp, headers, ib, environ, keep_blank_values,\\n\\t\\t\\t\\t\\t\\t strict_parsing, limit,\\n\\t\\t\\t\\t\\t\\t self.encoding, self.errors, max_num_fields, self.separator)\\n\\t\\t\\tif max_num_fields is not None:\\n\\t\\t\\t\\tmax_num_fields -= 1\\n\\t\\t\\t\\tif part.list:\\n\\t\\t\\t\\t\\tmax_num_fields -= len(part.list)\\n\\t\\t\\t\\tif max_num_fields < 0:\\n\\t\\t\\t\\t\\traise ValueError('Max number of fields exceeded')\\n\\t\\t\\tself.bytes_read += part.bytes_read\\n\\t\\t\\tself.list.append(part)\\n\\t\\t\\tif part.done or self.bytes_read >= self.length > 0:\\n\\t\\t\\t\\tbreak\\n\\t\\tself.skip_lines()"
  },
  {
    "code": "def move(src, dst):\\n    real_dst = dst\\n    if os.path.isdir(dst):\\n        real_dst = os.path.join(dst, _basename(src))\\n        if os.path.exists(real_dst):\\n            raise Error, \"Destination path '%s' already exists\" % real_dst\\n    try:\\n        os.rename(src, real_dst)\\n    except OSError:\\n        if os.path.isdir(src):\\n            if _destinsrc(src, dst):\\n                raise Error, \"Cannot move a directory '%s' into itself '%s'.\" % (src, dst)\\n            copytree(src, real_dst, symlinks=True)\\n            rmtree(src)\\n        else:\\n            copy2(src, real_dst)\\n            os.unlink(src)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Backport fix for issue #10684 from 3.x",
    "fixed_code": "def move(src, dst):\\n    real_dst = dst\\n    if os.path.isdir(dst):\\n        if _samefile(src, dst):\\n            os.rename(src, dst)\\n            return\\n        real_dst = os.path.join(dst, _basename(src))\\n        if os.path.exists(real_dst):\\n            raise Error, \"Destination path '%s' already exists\" % real_dst\\n    try:\\n        os.rename(src, real_dst)\\n    except OSError:\\n        if os.path.isdir(src):\\n            if _destinsrc(src, dst):\\n                raise Error, \"Cannot move a directory '%s' into itself '%s'.\" % (src, dst)\\n            copytree(src, real_dst, symlinks=True)\\n            rmtree(src)\\n        else:\\n            copy2(src, real_dst)\\n            os.unlink(src)"
  },
  {
    "code": "def _ymask(length):\\n\\t  return _math_ops.cast(1 - 2 * (_math_ops.range(length) % 2),\\n\\t\\t\\t\\t\\t\\t\\tcomplex_dtype)\\n\\ty0 = grad[..., 0:1]\\n\\tif rank == 1:\\n\\t  ym = grad[..., -1:]\\n\\t  extra_terms = y0 + is_even * ym * _ymask(input_shape[-1])\\n\\telif rank == 2:\\n\\t  base_mask = _mask_matrix(input_shape[-2])\\n\\t  tiled_mask = _tile_for_broadcasting(base_mask, y0)\\n\\t  y0_term = _math_ops.matmul(tiled_mask, _math_ops.conj(y0))\\n\\t  extra_terms = y0_term\\n\\t  ym = grad[..., -1:]\\n\\t  ym_term = _math_ops.matmul(tiled_mask, _math_ops.conj(ym))\\n\\t  inner_dim = input_shape[-1]\\n\\t  ym_term = _array_ops.tile(\\n\\t\\t  ym_term,\\n\\t\\t  _array_ops.concat([\\n\\t\\t\\t  _array_ops.ones([_array_ops.rank(grad) - 1], _dtypes.int32),\\n\\t\\t\\t  [inner_dim]\\n\\t\\t  ], 0)) * _ymask(inner_dim)\\n\\t  extra_terms += is_even * ym_term\\n\\tinput_size = _math_ops.cast(\\n\\t\\t_fft_size_for_grad(op.inputs[0], rank), real_dtype)\\n\\tthe_irfft = irfft_fn(grad, fft_length)\\n\\treturn 0.5 * (the_irfft * input_size + _math_ops.real(extra_terms)), None\\n  return _grad",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ymask(length):\\n\\t  return _math_ops.cast(1 - 2 * (_math_ops.range(length) % 2),\\n\\t\\t\\t\\t\\t\\t\\tcomplex_dtype)\\n\\ty0 = grad[..., 0:1]\\n\\tif rank == 1:\\n\\t  ym = grad[..., -1:]\\n\\t  extra_terms = y0 + is_even * ym * _ymask(input_shape[-1])\\n\\telif rank == 2:\\n\\t  base_mask = _mask_matrix(input_shape[-2])\\n\\t  tiled_mask = _tile_for_broadcasting(base_mask, y0)\\n\\t  y0_term = _math_ops.matmul(tiled_mask, _math_ops.conj(y0))\\n\\t  extra_terms = y0_term\\n\\t  ym = grad[..., -1:]\\n\\t  ym_term = _math_ops.matmul(tiled_mask, _math_ops.conj(ym))\\n\\t  inner_dim = input_shape[-1]\\n\\t  ym_term = _array_ops.tile(\\n\\t\\t  ym_term,\\n\\t\\t  _array_ops.concat([\\n\\t\\t\\t  _array_ops.ones([_array_ops.rank(grad) - 1], _dtypes.int32),\\n\\t\\t\\t  [inner_dim]\\n\\t\\t  ], 0)) * _ymask(inner_dim)\\n\\t  extra_terms += is_even * ym_term\\n\\tinput_size = _math_ops.cast(\\n\\t\\t_fft_size_for_grad(op.inputs[0], rank), real_dtype)\\n\\tthe_irfft = irfft_fn(grad, fft_length)\\n\\treturn 0.5 * (the_irfft * input_size + _math_ops.real(extra_terms)), None\\n  return _grad"
  },
  {
    "code": "def time_days_in_month(self, tz, freq):\\n        self.ts.days_in_month",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_days_in_month(self, tz, freq):\\n        self.ts.days_in_month"
  },
  {
    "code": "def icol(self, i):\\n        return self._ixs(i, axis=1)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: deprecate irow,icol,iget_value,iget in Series/DataFrame, #10711",
    "fixed_code": "def icol(self, i):\\n        warnings.warn(\"icol(i) is deprecated. Please use .iloc[:,i]\",\\n                      FutureWarning, stacklevel=2)\\n        return self._ixs(i, axis=1)"
  },
  {
    "code": "def _build_find_pod_label_selector(self, context: Optional[dict] = None, *, exclude_checked=True) -> str:\\n        labels = self._get_ti_pod_labels(context, include_try_number=False)\\n        label_strings = [f'{label_id}={label}' for label_id, label in sorted(labels.items())]\\n        labels_value = ','.join(label_strings)\\n        if exclude_checked:\\n            labels_value += f',{self.POD_CHECKED_KEY}!=True'\\n        labels_value += ',!airflow-worker'\\n        return labels_value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_find_pod_label_selector(self, context: Optional[dict] = None, *, exclude_checked=True) -> str:\\n        labels = self._get_ti_pod_labels(context, include_try_number=False)\\n        label_strings = [f'{label_id}={label}' for label_id, label in sorted(labels.items())]\\n        labels_value = ','.join(label_strings)\\n        if exclude_checked:\\n            labels_value += f',{self.POD_CHECKED_KEY}!=True'\\n        labels_value += ',!airflow-worker'\\n        return labels_value"
  },
  {
    "code": "def populate_facts(self, connection, ansible_facts, data=None):\\n        if not data:\\n            data = connection.get('show running-config | section ^interface')\\n        config = data.split('interface ')\\n        objs = []\\n        for conf in config:\\n            if conf:\\n                obj = self.render_config(self.generated_spec, conf)\\n                if obj:\\n                    objs.append(obj)\\n        facts = {'interfaces': []}\\n        if objs:\\n            params = utils.validate_config(self.argument_spec, {'config': objs})\\n            for cfg in params['config']:\\n                facts['interfaces'].append(utils.remove_empties(cfg))\\n        ansible_facts['ansible_network_resources'].update(facts)\\n        return ansible_facts",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "eos_interfaces: Added unit testcases for eos_interfaces (#63813)",
    "fixed_code": "def populate_facts(self, connection, ansible_facts, data=None):\\n        if not data:\\n            data = self.get_device_data(connection)\\n        config = data.split('interface ')\\n        objs = []\\n        for conf in config:\\n            if conf:\\n                obj = self.render_config(self.generated_spec, conf)\\n                if obj:\\n                    objs.append(obj)\\n        facts = {'interfaces': []}\\n        if objs:\\n            params = utils.validate_config(self.argument_spec, {'config': objs})\\n            for cfg in params['config']:\\n                facts['interfaces'].append(utils.remove_empties(cfg))\\n        ansible_facts['ansible_network_resources'].update(facts)\\n        return ansible_facts"
  },
  {
    "code": "def OnDoubleClick(self):\\n        \"Open a module in an editor window when double clicked.\"\\n        if os.path.normcase(self.file[-3:]) != \".py\":\\n            return\\n        if not os.path.exists(self.file):\\n            return\\n        file_open(self.file)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-95411: IDLE - Enable using the module browser with .pyw files (#95397)",
    "fixed_code": "def OnDoubleClick(self):\\n        \"Open a module in an editor window when double clicked.\"\\n        if not is_browseable_extension(self.file):\\n            return\\n        if not os.path.exists(self.file):\\n            return\\n        file_open(self.file)"
  },
  {
    "code": "def _get_base_actions(self):\\n        actions = []\\n        base_actions = (self.get_action(action) for action in self.actions or [])\\n        base_actions = [action for action in base_actions if action]\\n        base_action_names = {name for _, name, _ in base_actions}\\n        for (name, func) in self.admin_site.actions:\\n            if name in base_action_names:\\n                continue\\n            description = getattr(func, 'short_description', name.replace('_', ' '))\\n            actions.append((func, name, description))\\n        actions.extend(base_actions)\\n        return actions",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_base_actions(self):\\n        actions = []\\n        base_actions = (self.get_action(action) for action in self.actions or [])\\n        base_actions = [action for action in base_actions if action]\\n        base_action_names = {name for _, name, _ in base_actions}\\n        for (name, func) in self.admin_site.actions:\\n            if name in base_action_names:\\n                continue\\n            description = getattr(func, 'short_description', name.replace('_', ' '))\\n            actions.append((func, name, description))\\n        actions.extend(base_actions)\\n        return actions"
  },
  {
    "code": "def run(self):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tDocServer.base = http.server.HTTPServer\\n\\t\\t\\t\\tDocServer.handler = DocHandler\\n\\t\\t\\t\\tDocHandler.MessageClass = email.message.Message\\n\\t\\t\\t\\tDocHandler.urlhandler = staticmethod(self.urlhandler)\\n\\t\\t\\t\\tdocsvr = DocServer(self.host, self.port, self.ready)\\n\\t\\t\\t\\tself.docserver = docsvr\\n\\t\\t\\t\\tdocsvr.serve_until_quit()\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tself.error = e",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run(self):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tDocServer.base = http.server.HTTPServer\\n\\t\\t\\t\\tDocServer.handler = DocHandler\\n\\t\\t\\t\\tDocHandler.MessageClass = email.message.Message\\n\\t\\t\\t\\tDocHandler.urlhandler = staticmethod(self.urlhandler)\\n\\t\\t\\t\\tdocsvr = DocServer(self.host, self.port, self.ready)\\n\\t\\t\\t\\tself.docserver = docsvr\\n\\t\\t\\t\\tdocsvr.serve_until_quit()\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tself.error = e"
  },
  {
    "code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        ip=dict(type='str'),\\n        servername=dict(type='str'),\\n        servicetype=dict(\\n            type='str',\\n            choices=[\\n                'HTTP',\\n                'FTP',\\n                'TCP',\\n                'UDP',\\n                'SSL',\\n                'SSL_BRIDGE',\\n                'SSL_TCP',\\n                'DTLS',\\n                'NNTP',\\n                'RPCSVR',\\n                'DNS',\\n                'ADNS',\\n                'SNMP',\\n                'RTSP',\\n                'DHCPRA',\\n                'ANY',\\n                'SIP_UDP',\\n                'SIP_TCP',\\n                'SIP_SSL',\\n                'DNS_TCP',\\n                'ADNS_TCP',\\n                'MYSQL',\\n                'MSSQL',\\n                'ORACLE',\\n                'RADIUS',\\n                'RADIUSListener',\\n                'RDP',\\n                'DIAMETER',\\n                'SSL_DIAMETER',\\n                'TFTP',\\n                'SMPP',\\n                'PPTP',\\n                'GRE',\\n                'SYSLOGTCP',\\n                'SYSLOGUDP',\\n                'FIX',\\n                'SSL_FIX'\\n            ]\\n        ),\\n        port=dict(type='int'),\\n        cleartextport=dict(type='int'),\\n        cachetype=dict(\\n            type='str',\\n            choices=[\\n                'TRANSPARENT',\\n                'REVERSE',\\n                'FORWARD',\\n            ]\\n        ),\\n        maxclient=dict(type='float'),\\n        healthmonitor=dict(\\n            type='bool',\\n            default=True,\\n        ),\\n        maxreq=dict(type='float'),\\n        cacheable=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        cip=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ]\\n        ),\\n        cipheader=dict(type='str'),\\n        usip=dict(type='bool'),\\n        useproxyport=dict(type='bool'),\\n        sp=dict(type='bool'),\\n        rtspsessionidremap=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        clttimeout=dict(type='float'),\\n        svrtimeout=dict(type='float'),\\n        customserverid=dict(\\n            type='str',\\n            default='None',\\n        ),\\n        cka=dict(type='bool'),\\n        tcpb=dict(type='bool'),\\n        cmp=dict(type='bool'),\\n        maxbandwidth=dict(type='float'),\\n        accessdown=dict(\\n            type='bool',\\n            default=False\\n        ),\\n        monthreshold=dict(type='float'),\\n        downstateflush=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        tcpprofilename=dict(type='str'),\\n        httpprofilename=dict(type='str'),\\n        hashid=dict(type='float'),\\n        comment=dict(type='str'),\\n        appflowlog=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        netprofile=dict(type='str'),\\n        processlocal=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='DISABLED',\\n        ),\\n        dnsprofilename=dict(type='str'),\\n        ipaddress=dict(type='str'),\\n        graceful=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    hand_inserted_arguments = dict(\\n        monitor_bindings=dict(type='list'),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    argument_spec.update(hand_inserted_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines,\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'maxclient',\\n        'healthmonitor',\\n        'maxreq',\\n        'cacheable',\\n        'cip',\\n        'cipheader',\\n        'usip',\\n        'useproxyport',\\n        'sp',\\n        'rtspsessionidremap',\\n        'clttimeout',\\n        'svrtimeout',\\n        'customserverid',\\n        'cka',\\n        'tcpb',\\n        'cmp',\\n        'maxbandwidth',\\n        'accessdown',\\n        'monthreshold',\\n        'downstateflush',\\n        'tcpprofilename',\\n        'httpprofilename',\\n        'hashid',\\n        'comment',\\n        'appflowlog',\\n        'netprofile',\\n        'processlocal',\\n        'dnsprofilename',\\n        'ipaddress',\\n        'graceful',\\n    ]\\n    readonly_attrs = [\\n        'numofconnections',\\n        'policyname',\\n        'serviceconftype',\\n        'serviceconftype2',\\n        'value',\\n        'gslb',\\n        'dup_state',\\n        'publicip',\\n        'publicport',\\n        'svrstate',\\n        'monitor_state',\\n        'monstatcode',\\n        'lastresponse',\\n        'responsetime',\\n        'riseapbrstatsmsgcode2',\\n        'monstatparam1',\\n        'monstatparam2',\\n        'monstatparam3',\\n        'statechangetimesec',\\n        'statechangetimemsec',\\n        'tickssincelaststatechange',\\n        'stateupdatereason',\\n        'clmonowner',\\n        'clmonview',\\n        'serviceipstr',\\n        'oracleserverversion',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'cipheader',\\n        'serverid',\\n        'state',\\n        'td',\\n        'monitor_name_svc',\\n        'riseapbrstatsmsgcode',\\n        'graceful',\\n        'all',\\n        'Internal',\\n        'newname',\\n    ]\\n    transforms = {\\n        'pathmonitorindv': ['bool_yes_no'],\\n        'cacheable': ['bool_yes_no'],\\n        'cka': ['bool_yes_no'],\\n        'pathmonitor': ['bool_yes_no'],\\n        'tcpb': ['bool_yes_no'],\\n        'sp': ['bool_on_off'],\\n        'graceful': ['bool_yes_no'],\\n        'usip': ['bool_yes_no'],\\n        'healthmonitor': ['bool_yes_no'],\\n        'useproxyport': ['bool_yes_no'],\\n        'rtspsessionidremap': ['bool_on_off'],\\n        'accessdown': ['bool_yes_no'],\\n        'cmp': ['bool_yes_no'],\\n    }\\n    monitor_bindings_rw_attrs = [\\n        'servicename',\\n        'servicegroupname',\\n        'dup_state',\\n        'dup_weight',\\n        'monitorname',\\n        'weight',\\n    ]\\n    if module.params['ip'] is None:\\n        module.params['ip'] = module.params['ipaddress']\\n    service_proxy = ConfigProxy(\\n        actual=service(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.add()\\n                    sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not all_identical(client, module, service_proxy, monitor_bindings_rw_attrs):\\n                diff_dict = diff(client, module, service_proxy)\\n                immutables_changed = get_immutables_intersection(service_proxy, diff_dict.keys())\\n                if immutables_changed != []:\\n                    msg = 'Cannot update immutable attributes %s. Must delete and recreate entity.' % (immutables_changed,)\\n                    module.fail_json(msg=msg, diff=diff_dict, **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    if not module.check_mode:\\n                        service_proxy.update()\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    if not module.check_mode:\\n                        sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                module_result['changed'] = True\\n                if not module.check_mode:\\n                    if module.params['save_config']:\\n                        client.save_config()\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state present')\\n                if not service_exists(client, module):\\n                    module.fail_json(msg='Service does not exist', **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    module.fail_json(msg='Service differs from configured', diff=diff(client, module, service_proxy), **module_result)\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    module.fail_json(msg='Monitor bindings are not identical', **module_result)\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if service_exists(client, module):\\n                    module.fail_json(msg='Service still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add enable, disable operation in netscaler_service (#28321)",
    "fixed_code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        ip=dict(type='str'),\\n        servername=dict(type='str'),\\n        servicetype=dict(\\n            type='str',\\n            choices=[\\n                'HTTP',\\n                'FTP',\\n                'TCP',\\n                'UDP',\\n                'SSL',\\n                'SSL_BRIDGE',\\n                'SSL_TCP',\\n                'DTLS',\\n                'NNTP',\\n                'RPCSVR',\\n                'DNS',\\n                'ADNS',\\n                'SNMP',\\n                'RTSP',\\n                'DHCPRA',\\n                'ANY',\\n                'SIP_UDP',\\n                'SIP_TCP',\\n                'SIP_SSL',\\n                'DNS_TCP',\\n                'ADNS_TCP',\\n                'MYSQL',\\n                'MSSQL',\\n                'ORACLE',\\n                'RADIUS',\\n                'RADIUSListener',\\n                'RDP',\\n                'DIAMETER',\\n                'SSL_DIAMETER',\\n                'TFTP',\\n                'SMPP',\\n                'PPTP',\\n                'GRE',\\n                'SYSLOGTCP',\\n                'SYSLOGUDP',\\n                'FIX',\\n                'SSL_FIX'\\n            ]\\n        ),\\n        port=dict(type='int'),\\n        cleartextport=dict(type='int'),\\n        cachetype=dict(\\n            type='str',\\n            choices=[\\n                'TRANSPARENT',\\n                'REVERSE',\\n                'FORWARD',\\n            ]\\n        ),\\n        maxclient=dict(type='float'),\\n        healthmonitor=dict(\\n            type='bool',\\n            default=True,\\n        ),\\n        maxreq=dict(type='float'),\\n        cacheable=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        cip=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ]\\n        ),\\n        cipheader=dict(type='str'),\\n        usip=dict(type='bool'),\\n        useproxyport=dict(type='bool'),\\n        sp=dict(type='bool'),\\n        rtspsessionidremap=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n        clttimeout=dict(type='float'),\\n        svrtimeout=dict(type='float'),\\n        customserverid=dict(\\n            type='str',\\n            default='None',\\n        ),\\n        cka=dict(type='bool'),\\n        tcpb=dict(type='bool'),\\n        cmp=dict(type='bool'),\\n        maxbandwidth=dict(type='float'),\\n        accessdown=dict(\\n            type='bool',\\n            default=False\\n        ),\\n        monthreshold=dict(type='float'),\\n        downstateflush=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        tcpprofilename=dict(type='str'),\\n        httpprofilename=dict(type='str'),\\n        hashid=dict(type='float'),\\n        comment=dict(type='str'),\\n        appflowlog=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='ENABLED',\\n        ),\\n        netprofile=dict(type='str'),\\n        processlocal=dict(\\n            type='str',\\n            choices=[\\n                'ENABLED',\\n                'DISABLED',\\n            ],\\n            default='DISABLED',\\n        ),\\n        dnsprofilename=dict(type='str'),\\n        ipaddress=dict(type='str'),\\n        graceful=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    hand_inserted_arguments = dict(\\n        monitor_bindings=dict(type='list'),\\n        disabled=dict(\\n            type='bool',\\n            default=False,\\n        ),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    argument_spec.update(hand_inserted_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines,\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'maxclient',\\n        'healthmonitor',\\n        'maxreq',\\n        'cacheable',\\n        'cip',\\n        'cipheader',\\n        'usip',\\n        'useproxyport',\\n        'sp',\\n        'rtspsessionidremap',\\n        'clttimeout',\\n        'svrtimeout',\\n        'customserverid',\\n        'cka',\\n        'tcpb',\\n        'cmp',\\n        'maxbandwidth',\\n        'accessdown',\\n        'monthreshold',\\n        'downstateflush',\\n        'tcpprofilename',\\n        'httpprofilename',\\n        'hashid',\\n        'comment',\\n        'appflowlog',\\n        'netprofile',\\n        'processlocal',\\n        'dnsprofilename',\\n        'ipaddress',\\n        'graceful',\\n    ]\\n    readonly_attrs = [\\n        'numofconnections',\\n        'policyname',\\n        'serviceconftype',\\n        'serviceconftype2',\\n        'value',\\n        'gslb',\\n        'dup_state',\\n        'publicip',\\n        'publicport',\\n        'svrstate',\\n        'monitor_state',\\n        'monstatcode',\\n        'lastresponse',\\n        'responsetime',\\n        'riseapbrstatsmsgcode2',\\n        'monstatparam1',\\n        'monstatparam2',\\n        'monstatparam3',\\n        'statechangetimesec',\\n        'statechangetimemsec',\\n        'tickssincelaststatechange',\\n        'stateupdatereason',\\n        'clmonowner',\\n        'clmonview',\\n        'serviceipstr',\\n        'oracleserverversion',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'ip',\\n        'servername',\\n        'servicetype',\\n        'port',\\n        'cleartextport',\\n        'cachetype',\\n        'cipheader',\\n        'serverid',\\n        'state',\\n        'td',\\n        'monitor_name_svc',\\n        'riseapbrstatsmsgcode',\\n        'graceful',\\n        'all',\\n        'Internal',\\n        'newname',\\n    ]\\n    transforms = {\\n        'pathmonitorindv': ['bool_yes_no'],\\n        'cacheable': ['bool_yes_no'],\\n        'cka': ['bool_yes_no'],\\n        'pathmonitor': ['bool_yes_no'],\\n        'tcpb': ['bool_yes_no'],\\n        'sp': ['bool_on_off'],\\n        'graceful': ['bool_yes_no'],\\n        'usip': ['bool_yes_no'],\\n        'healthmonitor': ['bool_yes_no'],\\n        'useproxyport': ['bool_yes_no'],\\n        'rtspsessionidremap': ['bool_on_off'],\\n        'accessdown': ['bool_yes_no'],\\n        'cmp': ['bool_yes_no'],\\n    }\\n    monitor_bindings_rw_attrs = [\\n        'servicename',\\n        'servicegroupname',\\n        'dup_state',\\n        'dup_weight',\\n        'monitorname',\\n        'weight',\\n    ]\\n    if module.params['ip'] is None:\\n        module.params['ip'] = module.params['ipaddress']\\n    service_proxy = ConfigProxy(\\n        actual=service(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.add()\\n                    sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not all_identical(client, module, service_proxy, monitor_bindings_rw_attrs):\\n                diff_dict = diff(client, module, service_proxy)\\n                immutables_changed = get_immutables_intersection(service_proxy, diff_dict.keys())\\n                if immutables_changed != []:\\n                    msg = 'Cannot update immutable attributes %s. Must delete and recreate entity.' % (immutables_changed,)\\n                    module.fail_json(msg=msg, diff=diff_dict, **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    if not module.check_mode:\\n                        service_proxy.update()\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    if not module.check_mode:\\n                        sync_monitor_bindings(client, module, monitor_bindings_rw_attrs)\\n                module_result['changed'] = True\\n                if not module.check_mode:\\n                    if module.params['save_config']:\\n                        client.save_config()\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                res = do_state_change(client, module, service_proxy)\\n                if res.errorcode != 0:\\n                    msg = 'Error when setting disabled state. errorcode: %s message: %s' % (res.errorcode, res.message)\\n                    module.fail_json(msg=msg, **module_result)\\n            if not module.check_mode:\\n                log('Sanity checks for state present')\\n                if not service_exists(client, module):\\n                    module.fail_json(msg='Service does not exist', **module_result)\\n                if not service_identical(client, module, service_proxy):\\n                    module.fail_json(msg='Service differs from configured', diff=diff(client, module, service_proxy), **module_result)\\n                if not monitor_bindings_identical(client, module, monitor_bindings_rw_attrs):\\n                    module.fail_json(msg='Monitor bindings are not identical', **module_result)\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if service_exists(client, module):\\n                if not module.check_mode:\\n                    service_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if service_exists(client, module):\\n                    module.fail_json(msg='Service still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)"
  },
  {
    "code": "def __init__(self, loader=DataLoader):\\n        self._role_path = None\\n        super(Role, self).__init__(loader=loader)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "More v2 roles class work\\n\\n  use in detecting when roles have already been run)\\n  instead of a dictionary blob",
    "fixed_code": "def __init__(self, loader=DataLoader):\\n        self._role_path = None\\n        self._parents   = []\\n        super(Role, self).__init__(loader=loader)"
  },
  {
    "code": "def wait(fs, timeout=None, return_when=ALL_COMPLETED):\\n    fs = set(fs)\\n    with _AcquireFutures(fs):\\n        done = {f for f in fs\\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}\\n        not_done = fs - done\\n        if (return_when == FIRST_COMPLETED) and done:\\n            return DoneAndNotDoneFutures(done, not_done)\\n        elif (return_when == FIRST_EXCEPTION) and done:\\n            if any(f for f in done\\n                   if not f.cancelled() and f.exception() is not None):\\n                return DoneAndNotDoneFutures(done, not_done)\\n        if len(done) == len(fs):\\n            return DoneAndNotDoneFutures(done, not_done)\\n        waiter = _create_and_install_waiters(fs, return_when)\\n    waiter.event.wait(timeout)\\n    for f in fs:\\n        with f._condition:\\n            f._waiters.remove(waiter)\\n    done.update(waiter.finished_futures)\\n    return DoneAndNotDoneFutures(done, fs - done)\\nclass Future(object):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def wait(fs, timeout=None, return_when=ALL_COMPLETED):\\n    fs = set(fs)\\n    with _AcquireFutures(fs):\\n        done = {f for f in fs\\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}\\n        not_done = fs - done\\n        if (return_when == FIRST_COMPLETED) and done:\\n            return DoneAndNotDoneFutures(done, not_done)\\n        elif (return_when == FIRST_EXCEPTION) and done:\\n            if any(f for f in done\\n                   if not f.cancelled() and f.exception() is not None):\\n                return DoneAndNotDoneFutures(done, not_done)\\n        if len(done) == len(fs):\\n            return DoneAndNotDoneFutures(done, not_done)\\n        waiter = _create_and_install_waiters(fs, return_when)\\n    waiter.event.wait(timeout)\\n    for f in fs:\\n        with f._condition:\\n            f._waiters.remove(waiter)\\n    done.update(waiter.finished_futures)\\n    return DoneAndNotDoneFutures(done, fs - done)\\nclass Future(object):"
  },
  {
    "code": "def build_analyzer(self):\\n        if hasattr(self.analyzer, '__call__'):\\n            return self.analyzer\\n        preprocess = self.build_preprocessor()\\n        if self.analyzer == 'char':\\n            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))\\n        elif self.analyzer == 'word':\\n            stop_words = self.get_stop_words()\\n            tokenize = self.build_tokenizer()\\n            return lambda doc: self._word_ngrams(\\n                tokenize(preprocess(self.decode(doc))), stop_words)\\n        else:\\n            raise ValueError('%s is not a valid tokenization scheme' %\\n                             self.tokenize)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "feature_extraction.text.CountVectorizer analyzer 'char_nospace' creates character n-grams but only inside word boundaries, e.g. 'this is it' to 3-grams: ['thi', 'his', 'is', 'it']",
    "fixed_code": "def build_analyzer(self):\\n        if hasattr(self.analyzer, '__call__'):\\n            return self.analyzer\\n        preprocess = self.build_preprocessor()\\n        if self.analyzer == 'char':\\n            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))\\n        elif self.analyzer == 'char_nospace':\\n            return lambda doc: self._char_nospace_ngrams(\\n                preprocess(self.decode(doc)))\\n        elif self.analyzer == 'word':\\n            stop_words = self.get_stop_words()\\n            tokenize = self.build_tokenizer()\\n            return lambda doc: self._word_ngrams(\\n                tokenize(preprocess(self.decode(doc))), stop_words)\\n        else:\\n            raise ValueError('%s is not a valid tokenization scheme' %\\n                             self.tokenize)"
  },
  {
    "code": "def execute(self, context) -> dict:\\n        self.preprocess_config()\\n        if self.check_if_job_exists:\\n            self._check_if_job_exists()\\n        self.log.info(\"Creating SageMaker training job %s.\", self.config[\"TrainingJobName\"])\\n        response = self.hook.create_training_job(\\n            self.config,\\n            wait_for_completion=self.wait_for_completion,\\n            print_log=self.print_log,\\n            check_interval=self.check_interval,\\n            max_ingestion_time=self.max_ingestion_time,\\n        )\\n        if response['ResponseMetadata']['HTTPStatusCode'] != 200:\\n            raise AirflowException(f'Sagemaker Training Job creation failed: {response}')\\n        else:\\n            return {'Training': self.hook.describe_training_job(self.config['TrainingJobName'])}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(self, context) -> dict:\\n        self.preprocess_config()\\n        if self.check_if_job_exists:\\n            self._check_if_job_exists()\\n        self.log.info(\"Creating SageMaker training job %s.\", self.config[\"TrainingJobName\"])\\n        response = self.hook.create_training_job(\\n            self.config,\\n            wait_for_completion=self.wait_for_completion,\\n            print_log=self.print_log,\\n            check_interval=self.check_interval,\\n            max_ingestion_time=self.max_ingestion_time,\\n        )\\n        if response['ResponseMetadata']['HTTPStatusCode'] != 200:\\n            raise AirflowException(f'Sagemaker Training Job creation failed: {response}')\\n        else:\\n            return {'Training': self.hook.describe_training_job(self.config['TrainingJobName'])}"
  },
  {
    "code": "def _update_magp_data(self, magp_data):\\n        for magp_item in magp_data:\\n            magp_id = self.get_magp_id(magp_item)\\n            inst_data = self._create_magp_instance_data(magp_id, magp_item)\\n            self._current_config[magp_id] = inst_data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue: 46475 Fix onyx magp module for supporting new json format (#49417)",
    "fixed_code": "def _update_magp_data(self, magp_data):\\n        if self._os_version >= self.ONYX_API_VERSION:\\n            for magp_config in magp_data:\\n                for magp_name, data in iteritems(magp_config):\\n                    magp_id = int(magp_name.replace('MAGP ', ''))\\n                    self._current_config[magp_id] = \\\\n                        self._create_magp_instance_data(magp_id, data[0])\\n        else:\\n            for magp_item in magp_data:\\n                magp_id = self.get_magp_id(magp_item)\\n                inst_data = self._create_magp_instance_data(magp_id, magp_item)\\n                self._current_config[magp_id] = inst_data"
  },
  {
    "code": "def decorated(*args, **kwargs):\\n\\t\\t\\t__tracebackhide__ = True  \\n\\t\\t\\tappbuilder = current_app.appbuilder\\n\\t\\t\\tdag_id = (\\n\\t\\t\\t\\trequest.args.get(\"dag_id\")\\n\\t\\t\\t\\tor request.form.get(\"dag_id\")\\n\\t\\t\\t\\tor (request.is_json and request.json.get(\"dag_id\"))\\n\\t\\t\\t\\tor None\\n\\t\\t\\t)\\n\\t\\t\\tif appbuilder.sm.check_authorization(permissions, dag_id):\\n\\t\\t\\t\\treturn func(*args, **kwargs)\\n\\t\\t\\telif not g.user.is_anonymous and not g.user.perms:\\n\\t\\t\\t\\treturn (\\n\\t\\t\\t\\t\\trender_template(\\n\\t\\t\\t\\t\\t\\t\"airflow/no_roles_permissions.html\",\\n\\t\\t\\t\\t\\t\\thostname=get_hostname()\\n\\t\\t\\t\\t\\t\\tif conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\\n\\t\\t\\t\\t\\t\\telse \"redact\",\\n\\t\\t\\t\\t\\t\\tlogout_url=appbuilder.get_url_for_logout,\\n\\t\\t\\t\\t\\t),\\n\\t\\t\\t\\t\\t403,\\n\\t\\t\\t\\t)\\n\\t\\t\\telse:\\n\\t\\t\\t\\taccess_denied = \"Access is Denied\"\\n\\t\\t\\t\\tflash(access_denied, \"danger\")\\n\\t\\t\\treturn redirect(\\n\\t\\t\\t\\turl_for(\\n\\t\\t\\t\\t\\tappbuilder.sm.auth_view.__class__.__name__ + \".login\",\\n\\t\\t\\t\\t\\tnext=request.url,\\n\\t\\t\\t\\t)\\n\\t\\t\\t)\\n\\t\\treturn cast(T, decorated)\\n\\treturn requires_access_decorator",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Check for DAG ID in query param from url as well as kwargs (#32014)\\n\\nPreviously the dag id was only being checked in request args and form\\nbut not kwargs, so it was possible for the id when passed as kwargs\\nto be None. This can allow auth for a user who does not have the\\npermissions to view a particular DAG.",
    "fixed_code": "def decorated(*args, **kwargs):\\n\\t\\t\\t__tracebackhide__ = True  \\n\\t\\t\\tappbuilder = current_app.appbuilder\\n\\t\\t\\tdag_id = (\\n\\t\\t\\t\\tkwargs.get(\"dag_id\")\\n\\t\\t\\t\\tor request.args.get(\"dag_id\")\\n\\t\\t\\t\\tor request.form.get(\"dag_id\")\\n\\t\\t\\t\\tor (request.is_json and request.json.get(\"dag_id\"))\\n\\t\\t\\t\\tor None\\n\\t\\t\\t)\\n\\t\\t\\tif appbuilder.sm.check_authorization(permissions, dag_id):\\n\\t\\t\\t\\treturn func(*args, **kwargs)\\n\\t\\t\\telif not g.user.is_anonymous and not g.user.perms:\\n\\t\\t\\t\\treturn (\\n\\t\\t\\t\\t\\trender_template(\\n\\t\\t\\t\\t\\t\\t\"airflow/no_roles_permissions.html\",\\n\\t\\t\\t\\t\\t\\thostname=get_hostname()\\n\\t\\t\\t\\t\\t\\tif conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")\\n\\t\\t\\t\\t\\t\\telse \"redact\",\\n\\t\\t\\t\\t\\t\\tlogout_url=appbuilder.get_url_for_logout,\\n\\t\\t\\t\\t\\t),\\n\\t\\t\\t\\t\\t403,\\n\\t\\t\\t\\t)\\n\\t\\t\\telse:\\n\\t\\t\\t\\taccess_denied = \"Access is Denied\"\\n\\t\\t\\t\\tflash(access_denied, \"danger\")\\n\\t\\t\\treturn redirect(\\n\\t\\t\\t\\turl_for(\\n\\t\\t\\t\\t\\tappbuilder.sm.auth_view.__class__.__name__ + \".login\",\\n\\t\\t\\t\\t\\tnext=request.url,\\n\\t\\t\\t\\t)\\n\\t\\t\\t)\\n\\t\\treturn cast(T, decorated)\\n\\treturn requires_access_decorator"
  },
  {
    "code": "def execute(self, context):\\n        if self.bq_cursor is None:\\n            self.log.info('Executing: %s', self.sql)\\n            hook = BigQueryHook(\\n                bigquery_conn_id=self.bigquery_conn_id,\\n                use_legacy_sql=self.use_legacy_sql,\\n                delegate_to=self.delegate_to)\\n            conn = hook.get_conn()\\n            self.bq_cursor = conn.cursor()\\n        self.bq_cursor.run_query(\\n            self.sql,\\n            destination_dataset_table=self.destination_dataset_table,\\n            write_disposition=self.write_disposition,\\n            allow_large_results=self.allow_large_results,\\n            flatten_results=self.flatten_results,\\n            udf_config=self.udf_config,\\n            maximum_billing_tier=self.maximum_billing_tier,\\n            maximum_bytes_billed=self.maximum_bytes_billed,\\n            create_disposition=self.create_disposition,\\n            query_params=self.query_params,\\n            labels=self.labels,\\n            schema_update_options=self.schema_update_options,\\n            priority=self.priority,\\n            time_partitioning=self.time_partitioning,\\n            api_resource_configs=self.api_resource_configs,\\n            cluster_fields=self.cluster_fields,\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3438] Fix default values in BigQuery Hook & BigQueryOperator (#4274)",
    "fixed_code": "def execute(self, context):\\n        if self.bq_cursor is None:\\n            self.log.info('Executing: %s', self.sql)\\n            hook = BigQueryHook(\\n                bigquery_conn_id=self.bigquery_conn_id,\\n                use_legacy_sql=self.use_legacy_sql,\\n                delegate_to=self.delegate_to)\\n            conn = hook.get_conn()\\n            self.bq_cursor = conn.cursor()\\n        self.bq_cursor.run_query(\\n            sql=self.sql,\\n            destination_dataset_table=self.destination_dataset_table,\\n            write_disposition=self.write_disposition,\\n            allow_large_results=self.allow_large_results,\\n            flatten_results=self.flatten_results,\\n            udf_config=self.udf_config,\\n            maximum_billing_tier=self.maximum_billing_tier,\\n            maximum_bytes_billed=self.maximum_bytes_billed,\\n            create_disposition=self.create_disposition,\\n            query_params=self.query_params,\\n            labels=self.labels,\\n            schema_update_options=self.schema_update_options,\\n            priority=self.priority,\\n            time_partitioning=self.time_partitioning,\\n            api_resource_configs=self.api_resource_configs,\\n            cluster_fields=self.cluster_fields,\\n        )"
  },
  {
    "code": "def _handle_options(self, argument_spec=None, params=None):\\n        ''\\n        if argument_spec is None:\\n            argument_spec = self.argument_spec\\n        if params is None:\\n            params = self.params\\n        for (k, v) in argument_spec.items():\\n            wanted = v.get('type', None)\\n            if wanted == 'dict' or (wanted == 'list' and v.get('elements', '') == 'dict'):\\n                spec = v.get('options', None)\\n                if v.get('apply_defaults', False):\\n                    if spec is not None:\\n                        if params.get(k) is None:\\n                            params[k] = {}\\n                    else:\\n                        continue\\n                elif spec is None or k not in params or params[k] is None:\\n                    continue\\n                self._options_context.append(k)\\n                if isinstance(params[k], dict):\\n                    elements = [params[k]]\\n                else:\\n                    elements = params[k]\\n                for param in elements:\\n                    if not isinstance(param, dict):\\n                        self.fail_json(msg=\"value of %s must be of type dict or list of dict\" % k)\\n                    self._set_fallbacks(spec, param)\\n                    options_aliases = self._handle_aliases(spec, param)\\n                    self._handle_no_log_values(spec, param)\\n                    options_legal_inputs = list(spec.keys()) + list(options_aliases.keys())\\n                    self._check_arguments(self.check_invalid_arguments, spec, param, options_legal_inputs)\\n                    if not self.bypass_checks:\\n                        self._check_mutually_exclusive(v.get('mutually_exclusive', None), param)\\n                    self._set_defaults(pre=True, spec=spec, param=param)\\n                    if not self.bypass_checks:\\n                        self._check_required_arguments(spec, param)\\n                        self._check_argument_types(spec, param)\\n                        self._check_argument_values(spec, param)\\n                        self._check_required_together(v.get('required_together', None), param)\\n                        self._check_required_one_of(v.get('required_one_of', None), param)\\n                        self._check_required_if(v.get('required_if', None), param)\\n                        self._check_required_by(v.get('required_by', None), param)\\n                    self._set_defaults(pre=False, spec=spec, param=param)\\n                    self._handle_options(spec, param)\\n                self._options_context.pop()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Option parsing: warn if both an option and its alias are specified for a module (#53698)",
    "fixed_code": "def _handle_options(self, argument_spec=None, params=None, prefix=''):\\n        ''\\n        if argument_spec is None:\\n            argument_spec = self.argument_spec\\n        if params is None:\\n            params = self.params\\n        for (k, v) in argument_spec.items():\\n            wanted = v.get('type', None)\\n            if wanted == 'dict' or (wanted == 'list' and v.get('elements', '') == 'dict'):\\n                spec = v.get('options', None)\\n                if v.get('apply_defaults', False):\\n                    if spec is not None:\\n                        if params.get(k) is None:\\n                            params[k] = {}\\n                    else:\\n                        continue\\n                elif spec is None or k not in params or params[k] is None:\\n                    continue\\n                self._options_context.append(k)\\n                if isinstance(params[k], dict):\\n                    elements = [params[k]]\\n                else:\\n                    elements = params[k]\\n                for idx, param in enumerate(elements):\\n                    if not isinstance(param, dict):\\n                        self.fail_json(msg=\"value of %s must be of type dict or list of dict\" % k)\\n                    new_prefix = prefix + k\\n                    if wanted == 'list':\\n                        new_prefix += '[%d]' % idx\\n                    new_prefix += '.'\\n                    self._set_fallbacks(spec, param)\\n                    options_aliases = self._handle_aliases(spec, param, option_prefix=new_prefix)\\n                    self._handle_no_log_values(spec, param)\\n                    options_legal_inputs = list(spec.keys()) + list(options_aliases.keys())\\n                    self._check_arguments(self.check_invalid_arguments, spec, param, options_legal_inputs)\\n                    if not self.bypass_checks:\\n                        self._check_mutually_exclusive(v.get('mutually_exclusive', None), param)\\n                    self._set_defaults(pre=True, spec=spec, param=param)\\n                    if not self.bypass_checks:\\n                        self._check_required_arguments(spec, param)\\n                        self._check_argument_types(spec, param)\\n                        self._check_argument_values(spec, param)\\n                        self._check_required_together(v.get('required_together', None), param)\\n                        self._check_required_one_of(v.get('required_one_of', None), param)\\n                        self._check_required_if(v.get('required_if', None), param)\\n                        self._check_required_by(v.get('required_by', None), param)\\n                    self._set_defaults(pre=False, spec=spec, param=param)\\n                    self._handle_options(spec, param, new_prefix)\\n                self._options_context.pop()"
  },
  {
    "code": "def validate_base(cls, model):\\n    opts = model._meta\\n    if hasattr(cls, 'raw_id_fields'):\\n        check_isseq(cls, 'raw_id_fields', cls.raw_id_fields)\\n        for idx, field in enumerate(cls.raw_id_fields):\\n            f = get_field(cls, model, opts, 'raw_id_fields', field)\\n            if not isinstance(f, (models.ForeignKey, models.ManyToManyField)):\\n                raise ImproperlyConfigured(\"'%s.raw_id_fields[%d]', '%s' must \"\\n                        \"be either a ForeignKey or ManyToManyField.\"\\n                        % (cls.__name__, idx, field))\\n    if cls.fields: \\n        check_isseq(cls, 'fields', cls.fields)\\n        for field in cls.fields:\\n            if field in cls.readonly_fields:\\n                continue\\n            check_formfield(cls, model, opts, 'fields', field)\\n            try:\\n                f = opts.get_field(field)\\n            except models.FieldDoesNotExist:\\n                continue\\n            if isinstance(f, models.ManyToManyField) and not f.rel.through._meta.auto_created:\\n                raise ImproperlyConfigured(\"'%s.fields' can't include the ManyToManyField \"\\n                    \"field '%s' because '%s' manually specifies \"\\n                    \"a 'through' model.\" % (cls.__name__, field, field))\\n        if cls.fieldsets:\\n            raise ImproperlyConfigured('Both fieldsets and fields are specified in %s.' % cls.__name__)\\n        if len(cls.fields) > len(set(cls.fields)):\\n            raise ImproperlyConfigured('There are duplicate field(s) in %s.fields' % cls.__name__)\\n    if cls.fieldsets: \\n        check_isseq(cls, 'fieldsets', cls.fieldsets)\\n        for idx, fieldset in enumerate(cls.fieldsets):\\n            check_isseq(cls, 'fieldsets[%d]' % idx, fieldset)\\n            if len(fieldset) != 2:\\n                raise ImproperlyConfigured(\"'%s.fieldsets[%d]' does not \"\\n                        \"have exactly two elements.\" % (cls.__name__, idx))\\n            check_isdict(cls, 'fieldsets[%d][1]' % idx, fieldset[1])\\n            if 'fields' not in fieldset[1]:\\n                raise ImproperlyConfigured(\"'fields' key is required in \"\\n                        \"%s.fieldsets[%d][1] field options dict.\"\\n                        % (cls.__name__, idx))\\n            for fields in fieldset[1]['fields']:\\n                if type(fields) != tuple:\\n                    fields = (fields,)\\n                for field in fields:\\n                    if field in cls.readonly_fields:\\n                        continue\\n                    check_formfield(cls, model, opts, \"fieldsets[%d][1]['fields']\" % idx, field)\\n                    try:\\n                        f = opts.get_field(field)\\n                        if isinstance(f, models.ManyToManyField) and not f.rel.through._meta.auto_created:\\n                            raise ImproperlyConfigured(\"'%s.fieldsets[%d][1]['fields']' \"\\n                                \"can't include the ManyToManyField field '%s' because \"\\n                                \"'%s' manually specifies a 'through' model.\" % (\\n                                    cls.__name__, idx, field, field))\\n                    except models.FieldDoesNotExist:\\n                        pass\\n        flattened_fieldsets = flatten_fieldsets(cls.fieldsets)\\n        if len(flattened_fieldsets) > len(set(flattened_fieldsets)):\\n            raise ImproperlyConfigured('There are duplicate field(s) in %s.fieldsets' % cls.__name__)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate_base(cls, model):\\n    opts = model._meta\\n    if hasattr(cls, 'raw_id_fields'):\\n        check_isseq(cls, 'raw_id_fields', cls.raw_id_fields)\\n        for idx, field in enumerate(cls.raw_id_fields):\\n            f = get_field(cls, model, opts, 'raw_id_fields', field)\\n            if not isinstance(f, (models.ForeignKey, models.ManyToManyField)):\\n                raise ImproperlyConfigured(\"'%s.raw_id_fields[%d]', '%s' must \"\\n                        \"be either a ForeignKey or ManyToManyField.\"\\n                        % (cls.__name__, idx, field))\\n    if cls.fields: \\n        check_isseq(cls, 'fields', cls.fields)\\n        for field in cls.fields:\\n            if field in cls.readonly_fields:\\n                continue\\n            check_formfield(cls, model, opts, 'fields', field)\\n            try:\\n                f = opts.get_field(field)\\n            except models.FieldDoesNotExist:\\n                continue\\n            if isinstance(f, models.ManyToManyField) and not f.rel.through._meta.auto_created:\\n                raise ImproperlyConfigured(\"'%s.fields' can't include the ManyToManyField \"\\n                    \"field '%s' because '%s' manually specifies \"\\n                    \"a 'through' model.\" % (cls.__name__, field, field))\\n        if cls.fieldsets:\\n            raise ImproperlyConfigured('Both fieldsets and fields are specified in %s.' % cls.__name__)\\n        if len(cls.fields) > len(set(cls.fields)):\\n            raise ImproperlyConfigured('There are duplicate field(s) in %s.fields' % cls.__name__)\\n    if cls.fieldsets: \\n        check_isseq(cls, 'fieldsets', cls.fieldsets)\\n        for idx, fieldset in enumerate(cls.fieldsets):\\n            check_isseq(cls, 'fieldsets[%d]' % idx, fieldset)\\n            if len(fieldset) != 2:\\n                raise ImproperlyConfigured(\"'%s.fieldsets[%d]' does not \"\\n                        \"have exactly two elements.\" % (cls.__name__, idx))\\n            check_isdict(cls, 'fieldsets[%d][1]' % idx, fieldset[1])\\n            if 'fields' not in fieldset[1]:\\n                raise ImproperlyConfigured(\"'fields' key is required in \"\\n                        \"%s.fieldsets[%d][1] field options dict.\"\\n                        % (cls.__name__, idx))\\n            for fields in fieldset[1]['fields']:\\n                if type(fields) != tuple:\\n                    fields = (fields,)\\n                for field in fields:\\n                    if field in cls.readonly_fields:\\n                        continue\\n                    check_formfield(cls, model, opts, \"fieldsets[%d][1]['fields']\" % idx, field)\\n                    try:\\n                        f = opts.get_field(field)\\n                        if isinstance(f, models.ManyToManyField) and not f.rel.through._meta.auto_created:\\n                            raise ImproperlyConfigured(\"'%s.fieldsets[%d][1]['fields']' \"\\n                                \"can't include the ManyToManyField field '%s' because \"\\n                                \"'%s' manually specifies a 'through' model.\" % (\\n                                    cls.__name__, idx, field, field))\\n                    except models.FieldDoesNotExist:\\n                        pass\\n        flattened_fieldsets = flatten_fieldsets(cls.fieldsets)\\n        if len(flattened_fieldsets) > len(set(flattened_fieldsets)):\\n            raise ImproperlyConfigured('There are duplicate field(s) in %s.fieldsets' % cls.__name__)"
  },
  {
    "code": "def iteritems(self):\\n        if not self.db:\\n            return\\n        self._kill_iteration = False\\n        self._in_iter += 1\\n        try:\\n            try:\\n                cur = self._make_iter_cursor()\\n                kv = _DeadlockWrap(cur.first)\\n                key = kv[0]\\n                yield kv\\n                next = cur.next\\n                while 1:\\n                    try:\\n                        kv = _DeadlockWrap(next)\\n                        key = kv[0]\\n                        yield kv\\n                    except _bsddb.DBCursorClosedError:\\n                        if self._kill_iteration:\\n                            raise RuntimeError('Database changed size '\\n                                               'during iteration.')\\n                        cur = self._make_iter_cursor()\\n                        _DeadlockWrap(cur.set, key,0,0,0)\\n                        next = cur.next\\n            except _bsddb.DBNotFoundError:\\n                pass\\n            except _bsddb.DBCursorClosedError:\\n                pass\\n        except :\\n            self._in_iter -= 1\\n            raise\\n        self._in_iter -= 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def iteritems(self):\\n        if not self.db:\\n            return\\n        self._kill_iteration = False\\n        self._in_iter += 1\\n        try:\\n            try:\\n                cur = self._make_iter_cursor()\\n                kv = _DeadlockWrap(cur.first)\\n                key = kv[0]\\n                yield kv\\n                next = cur.next\\n                while 1:\\n                    try:\\n                        kv = _DeadlockWrap(next)\\n                        key = kv[0]\\n                        yield kv\\n                    except _bsddb.DBCursorClosedError:\\n                        if self._kill_iteration:\\n                            raise RuntimeError('Database changed size '\\n                                               'during iteration.')\\n                        cur = self._make_iter_cursor()\\n                        _DeadlockWrap(cur.set, key,0,0,0)\\n                        next = cur.next\\n            except _bsddb.DBNotFoundError:\\n                pass\\n            except _bsddb.DBCursorClosedError:\\n                pass\\n        except :\\n            self._in_iter -= 1\\n            raise\\n        self._in_iter -= 1"
  },
  {
    "code": "def __init__(self, *, filepath, fs_conn_id='fs_default', recursive=False, **kwargs):\\n        super().__init__(**kwargs)\\n        self.filepath = filepath\\n        self.fs_conn_id = fs_conn_id\\n        self.recursive = recursive",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, *, filepath, fs_conn_id='fs_default', recursive=False, **kwargs):\\n        super().__init__(**kwargs)\\n        self.filepath = filepath\\n        self.fs_conn_id = fs_conn_id\\n        self.recursive = recursive"
  },
  {
    "code": "def melt(\\n\\tframe: DataFrame,\\n\\tid_vars=None,\\n\\tvalue_vars=None,\\n\\tvar_name=None,\\n\\tvalue_name=\"value\",\\n\\tcol_level=None,\\n) -> DataFrame:\\n\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\tcols = [x for c in frame.columns for x in c]\\n\\telse:\\n\\t\\tcols = list(frame.columns)\\n\\tif id_vars is not None:\\n\\t\\tif not is_list_like(id_vars):\\n\\t\\t\\tid_vars = [id_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(id_vars, list):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"id_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tid_vars = list(id_vars)\\n\\t\\t\\tmissing = Index(com.flatten(id_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'id_vars' are not present\"\\n\\t\\t\\t\\t\\t\" in the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\telse:\\n\\t\\tid_vars = []\\n\\tif value_vars is not None:\\n\\t\\tif not is_list_like(value_vars):\\n\\t\\t\\tvalue_vars = [value_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(\\n\\t\\t\\tvalue_vars, list\\n\\t\\t):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"value_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tvalue_vars = list(value_vars)\\n\\t\\t\\tmissing = Index(com.flatten(value_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'value_vars' are not present in\"\\n\\t\\t\\t\\t\\t\" the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\t\\tframe = frame.loc[:, id_vars + value_vars]\\n\\telse:\\n\\t\\tframe = frame.copy()\\n\\tif col_level is not None:  \\n\\t\\tframe.columns = frame.columns.get_level_values(col_level)\\n\\tif var_name is None:\\n\\t\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\t\\tif len(frame.columns.names) == len(set(frame.columns.names)):\\n\\t\\t\\t\\tvar_name = frame.columns.names\\n\\t\\t\\telse:\\n\\t\\t\\t\\tvar_name = [\\n\\t\\t\\t\\t\\t\"variable_{i}\".format(i=i) for i in range(len(frame.columns.names))\\n\\t\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tvar_name = [\\n\\t\\t\\t\\tframe.columns.name if frame.columns.name is not None else \"variable\"\\n\\t\\t\\t]\\n\\tif isinstance(var_name, str):\\n\\t\\tvar_name = [var_name]\\n\\tN, K = frame.shape\\n\\tK -= len(id_vars)\\n\\tmdata = {}\\n\\tfor col in id_vars:\\n\\t\\tid_data = frame.pop(col)\\n\\t\\tif is_extension_array_dtype(id_data):\\n\\t\\t\\tid_data = concat([id_data] * K, ignore_index=True)\\n\\t\\telse:\\n\\t\\t\\tid_data = np.tile(id_data.values, K)\\n\\t\\tmdata[col] = id_data\\n\\tmcolumns = id_vars + var_name + [value_name]\\n\\tmdata[value_name] = frame.values.ravel(\"F\")\\n\\tfor i, col in enumerate(var_name):\\n\\t\\tmdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)\\n\\treturn frame._constructor(mdata, columns=mcolumns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def melt(\\n\\tframe: DataFrame,\\n\\tid_vars=None,\\n\\tvalue_vars=None,\\n\\tvar_name=None,\\n\\tvalue_name=\"value\",\\n\\tcol_level=None,\\n) -> DataFrame:\\n\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\tcols = [x for c in frame.columns for x in c]\\n\\telse:\\n\\t\\tcols = list(frame.columns)\\n\\tif id_vars is not None:\\n\\t\\tif not is_list_like(id_vars):\\n\\t\\t\\tid_vars = [id_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(id_vars, list):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"id_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tid_vars = list(id_vars)\\n\\t\\t\\tmissing = Index(com.flatten(id_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'id_vars' are not present\"\\n\\t\\t\\t\\t\\t\" in the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\telse:\\n\\t\\tid_vars = []\\n\\tif value_vars is not None:\\n\\t\\tif not is_list_like(value_vars):\\n\\t\\t\\tvalue_vars = [value_vars]\\n\\t\\telif isinstance(frame.columns, ABCMultiIndex) and not isinstance(\\n\\t\\t\\tvalue_vars, list\\n\\t\\t):\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"value_vars must be a list of tuples when columns are a MultiIndex\"\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tvalue_vars = list(value_vars)\\n\\t\\t\\tmissing = Index(com.flatten(value_vars)).difference(cols)\\n\\t\\t\\tif not missing.empty:\\n\\t\\t\\t\\traise KeyError(\\n\\t\\t\\t\\t\\t\"The following 'value_vars' are not present in\"\\n\\t\\t\\t\\t\\t\" the DataFrame: {missing}\"\\n\\t\\t\\t\\t\\t\"\".format(missing=list(missing))\\n\\t\\t\\t\\t)\\n\\t\\tframe = frame.loc[:, id_vars + value_vars]\\n\\telse:\\n\\t\\tframe = frame.copy()\\n\\tif col_level is not None:  \\n\\t\\tframe.columns = frame.columns.get_level_values(col_level)\\n\\tif var_name is None:\\n\\t\\tif isinstance(frame.columns, ABCMultiIndex):\\n\\t\\t\\tif len(frame.columns.names) == len(set(frame.columns.names)):\\n\\t\\t\\t\\tvar_name = frame.columns.names\\n\\t\\t\\telse:\\n\\t\\t\\t\\tvar_name = [\\n\\t\\t\\t\\t\\t\"variable_{i}\".format(i=i) for i in range(len(frame.columns.names))\\n\\t\\t\\t\\t]\\n\\t\\telse:\\n\\t\\t\\tvar_name = [\\n\\t\\t\\t\\tframe.columns.name if frame.columns.name is not None else \"variable\"\\n\\t\\t\\t]\\n\\tif isinstance(var_name, str):\\n\\t\\tvar_name = [var_name]\\n\\tN, K = frame.shape\\n\\tK -= len(id_vars)\\n\\tmdata = {}\\n\\tfor col in id_vars:\\n\\t\\tid_data = frame.pop(col)\\n\\t\\tif is_extension_array_dtype(id_data):\\n\\t\\t\\tid_data = concat([id_data] * K, ignore_index=True)\\n\\t\\telse:\\n\\t\\t\\tid_data = np.tile(id_data.values, K)\\n\\t\\tmdata[col] = id_data\\n\\tmcolumns = id_vars + var_name + [value_name]\\n\\tmdata[value_name] = frame.values.ravel(\"F\")\\n\\tfor i, col in enumerate(var_name):\\n\\t\\tmdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)\\n\\treturn frame._constructor(mdata, columns=mcolumns)"
  },
  {
    "code": "def has_requests(self):\\n        return self.request_cpu is not None or self.request_memory is not None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5659] Add support for ephemeral storage on KubernetesPodOperator (#6337)",
    "fixed_code": "def has_requests(self):\\n        return self.request_cpu is not None or \\\\n            self.request_memory is not None or \\\\n            self.request_ephemeral_storage is not None"
  },
  {
    "code": "def format_exception_only(etype, value):\\n    list = []\\n    if (type(etype) == types.ClassType\\n        or (isinstance(etype, type) and issubclass(etype, Exception))):\\n        stype = etype.__name__\\n    else:\\n        stype = etype\\n    if value is None:\\n        list.append(str(stype) + '\\n')\\n    else:\\n        if etype is SyntaxError:\\n            try:\\n                msg, (filename, lineno, offset, line) = value\\n            except:\\n                pass\\n            else:\\n                if not filename: filename = \"<string>\"\\n                list.append('  File \"%s\", line %d\\n' %\\n                            (filename, lineno))\\n                if line is not None:\\n                    i = 0\\n                    while i < len(line) and line[i].isspace():\\n                        i = i+1\\n                    list.append('    %s\\n' % line.strip())\\n                    if offset is not None:\\n                        s = '    '\\n                        for c in line[i:offset-1]:\\n                            if c.isspace():\\n                                s = s + c\\n                            else:\\n                                s = s + ' '\\n                        list.append('%s^\\n' % s)\\n                    value = msg\\n        s = _some_str(value)\\n        if s:\\n            list.append('%s: %s\\n' % (str(stype), s))\\n        else:\\n            list.append('%s\\n' % str(stype))\\n    return list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_exception_only(etype, value):\\n    list = []\\n    if (type(etype) == types.ClassType\\n        or (isinstance(etype, type) and issubclass(etype, Exception))):\\n        stype = etype.__name__\\n    else:\\n        stype = etype\\n    if value is None:\\n        list.append(str(stype) + '\\n')\\n    else:\\n        if etype is SyntaxError:\\n            try:\\n                msg, (filename, lineno, offset, line) = value\\n            except:\\n                pass\\n            else:\\n                if not filename: filename = \"<string>\"\\n                list.append('  File \"%s\", line %d\\n' %\\n                            (filename, lineno))\\n                if line is not None:\\n                    i = 0\\n                    while i < len(line) and line[i].isspace():\\n                        i = i+1\\n                    list.append('    %s\\n' % line.strip())\\n                    if offset is not None:\\n                        s = '    '\\n                        for c in line[i:offset-1]:\\n                            if c.isspace():\\n                                s = s + c\\n                            else:\\n                                s = s + ' '\\n                        list.append('%s^\\n' % s)\\n                    value = msg\\n        s = _some_str(value)\\n        if s:\\n            list.append('%s: %s\\n' % (str(stype), s))\\n        else:\\n            list.append('%s\\n' % str(stype))\\n    return list"
  },
  {
    "code": "def setlist(self, key, list_):\\n        self._assert_mutable()\\n        key = bytes_to_text(key, self.encoding)\\n        list_ = [bytes_to_text(elt, self.encoding) for elt in list_]\\n        super(QueryDict, self).setlist(key, list_)\\n    def setlistdefault(self, key, default_list=None):\\n        self._assert_mutable()\\n        return super(QueryDict, self).setlistdefault(key, default_list)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setlist(self, key, list_):\\n        self._assert_mutable()\\n        key = bytes_to_text(key, self.encoding)\\n        list_ = [bytes_to_text(elt, self.encoding) for elt in list_]\\n        super(QueryDict, self).setlist(key, list_)\\n    def setlistdefault(self, key, default_list=None):\\n        self._assert_mutable()\\n        return super(QueryDict, self).setlistdefault(key, default_list)"
  },
  {
    "code": "def get_value(arg, config):\\n    command = PARAM_TO_COMMAND_KEYMAP.get(arg)\\n    if command.split()[0] == 'event-history':\\n        has_size = re.search(r'^\\s+{0} size\\s(?P<value>.*)$'.format(command), config, re.M)\\n        if command == 'event-history detail':\\n            value = False\\n        else:\\n            value = 'size_small'\\n        if has_size:\\n            value = 'size_%s' % has_size.group('value')\\n    elif arg in ['enforce_first_as', 'fast_external_fallover']:\\n        no_command_re = re.compile(r'no\\s+{0}\\s*'.format(command), re.M)\\n        value = True\\n        if no_command_re.search(config):\\n            value = False\\n    elif arg in BOOL_PARAMS:\\n        has_command = re.search(r'^\\s+{0}\\s*$'.format(command), config, re.M)\\n        value = False\\n        if has_command:\\n            value = True\\n    else:\\n        command_val_re = re.compile(r'(?:{0}\\s)(?P<value>.*)'.format(command), re.M)\\n        value = ''\\n        has_command = command_val_re.search(config)\\n        if has_command:\\n            found_value = has_command.group('value')\\n            if arg == 'confederation_peers':\\n                value = found_value.split()\\n            elif arg == 'timer_bgp_keepalive':\\n                value = found_value.split()[0]\\n            elif arg == 'timer_bgp_hold':\\n                split_values = found_value.split()\\n                if len(split_values) == 2:\\n                    value = split_values[1]\\n            elif found_value:\\n                value = found_value\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_value(arg, config):\\n    command = PARAM_TO_COMMAND_KEYMAP.get(arg)\\n    if command.split()[0] == 'event-history':\\n        has_size = re.search(r'^\\s+{0} size\\s(?P<value>.*)$'.format(command), config, re.M)\\n        if command == 'event-history detail':\\n            value = False\\n        else:\\n            value = 'size_small'\\n        if has_size:\\n            value = 'size_%s' % has_size.group('value')\\n    elif arg in ['enforce_first_as', 'fast_external_fallover']:\\n        no_command_re = re.compile(r'no\\s+{0}\\s*'.format(command), re.M)\\n        value = True\\n        if no_command_re.search(config):\\n            value = False\\n    elif arg in BOOL_PARAMS:\\n        has_command = re.search(r'^\\s+{0}\\s*$'.format(command), config, re.M)\\n        value = False\\n        if has_command:\\n            value = True\\n    else:\\n        command_val_re = re.compile(r'(?:{0}\\s)(?P<value>.*)'.format(command), re.M)\\n        value = ''\\n        has_command = command_val_re.search(config)\\n        if has_command:\\n            found_value = has_command.group('value')\\n            if arg == 'confederation_peers':\\n                value = found_value.split()\\n            elif arg == 'timer_bgp_keepalive':\\n                value = found_value.split()[0]\\n            elif arg == 'timer_bgp_hold':\\n                split_values = found_value.split()\\n                if len(split_values) == 2:\\n                    value = split_values[1]\\n            elif found_value:\\n                value = found_value\\n    return value"
  },
  {
    "code": "def _fancy_getitem_axis(self, key, axis=0):\\n        if isinstance(key, slice):\\n            return self._get_slice_axis(key, axis=axis)\\n        elif _is_list_like(key):\\n            return self._fancy_getitem(key, axis=axis)\\n        elif axis == 0:\\n            idx = key\\n            if isinstance(key, int):\\n                idx = self.index[key]\\n            if self._is_mixed_type:\\n                return self.xs(idx)\\n            else:\\n                return self.xs(idx, copy=False)\\n        else:\\n            col = key\\n            if isinstance(key, int):\\n                col = self.columns[key]\\n            return self[col]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fancy_getitem_axis(self, key, axis=0):\\n        if isinstance(key, slice):\\n            return self._get_slice_axis(key, axis=axis)\\n        elif _is_list_like(key):\\n            return self._fancy_getitem(key, axis=axis)\\n        elif axis == 0:\\n            idx = key\\n            if isinstance(key, int):\\n                idx = self.index[key]\\n            if self._is_mixed_type:\\n                return self.xs(idx)\\n            else:\\n                return self.xs(idx, copy=False)\\n        else:\\n            col = key\\n            if isinstance(key, int):\\n                col = self.columns[key]\\n            return self[col]"
  },
  {
    "code": "def _bar(\\n    data: FrameOrSeries,\\n    align: str | float | int | Callable,\\n    colors: list[str],\\n    width: float,\\n    height: float,\\n    vmin: float | None,\\n    vmax: float | None,\\n    base_css: str,\\n):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _bar(\\n    data: FrameOrSeries,\\n    align: str | float | int | Callable,\\n    colors: list[str],\\n    width: float,\\n    height: float,\\n    vmin: float | None,\\n    vmax: float | None,\\n    base_css: str,\\n):"
  },
  {
    "code": "def as_pod(self) -> k8s.V1Pod:\\n        if self.kube_config.pod_template_file:\\n            return PodGenerator(pod_template_file=self.kube_config.pod_template_file).gen_pod()\\n        pod = PodGenerator(\\n            image=self.kube_config.kube_image,\\n            image_pull_policy=self.kube_config.kube_image_pull_policy or 'IfNotPresent',\\n            image_pull_secrets=self.kube_config.image_pull_secrets,\\n            volumes=self._get_volumes(),\\n            volume_mounts=self._get_volume_mounts(),\\n            init_containers=self._get_init_containers(),\\n            annotations=self.kube_config.kube_annotations,\\n            affinity=self.kube_config.kube_affinity,\\n            tolerations=self.kube_config.kube_tolerations,\\n            envs=self._get_environment(),\\n            node_selectors=self.kube_config.kube_node_selectors,\\n            service_account_name=self.kube_config.worker_service_account_name or 'default',\\n            restart_policy='Never'\\n        ).gen_pod()\\n        pod.spec.containers[0].env_from = pod.spec.containers[0].env_from or []\\n        pod.spec.containers[0].env_from.extend(self._get_env_from())\\n        pod.spec.security_context = self._get_security_context()\\n        return append_to_pod(pod, self._get_secrets())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6561] Add possibility to specify default resources for airflow k8s workers (#7168)",
    "fixed_code": "def as_pod(self) -> k8s.V1Pod:\\n        if self.kube_config.pod_template_file:\\n            return PodGenerator(pod_template_file=self.kube_config.pod_template_file).gen_pod()\\n        pod = PodGenerator(\\n            image=self.kube_config.kube_image,\\n            image_pull_policy=self.kube_config.kube_image_pull_policy or 'IfNotPresent',\\n            image_pull_secrets=self.kube_config.image_pull_secrets,\\n            volumes=self._get_volumes(),\\n            volume_mounts=self._get_volume_mounts(),\\n            init_containers=self._get_init_containers(),\\n            annotations=self.kube_config.kube_annotations,\\n            affinity=self.kube_config.kube_affinity,\\n            tolerations=self.kube_config.kube_tolerations,\\n            envs=self._get_environment(),\\n            node_selectors=self.kube_config.kube_node_selectors,\\n            service_account_name=self.kube_config.worker_service_account_name or 'default',\\n            restart_policy='Never',\\n            resources=self._get_resources(),\\n        ).gen_pod()\\n        pod.spec.containers[0].env_from = pod.spec.containers[0].env_from or []\\n        pod.spec.containers[0].env_from.extend(self._get_env_from())\\n        pod.spec.security_context = self._get_security_context()\\n        return append_to_pod(pod, self._get_secrets())"
  },
  {
    "code": "def regex(self):\\n\\t\\treturn re.compile(re.escape(self.language_prefix))\\n\\t@property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def regex(self):\\n\\t\\treturn re.compile(re.escape(self.language_prefix))\\n\\t@property"
  },
  {
    "code": "def start_template_dataflow(self, job_name, variables, parameters, dataflow_template,\\n                                append_job_name=True):\\n        variables = self._set_variables(variables)\\n        name = self._build_dataflow_job_name(job_name, append_job_name)\\n        self._start_template_dataflow(\\n            name, variables, parameters, dataflow_template)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def start_template_dataflow(self, job_name, variables, parameters, dataflow_template,\\n                                append_job_name=True):\\n        variables = self._set_variables(variables)\\n        name = self._build_dataflow_job_name(job_name, append_job_name)\\n        self._start_template_dataflow(\\n            name, variables, parameters, dataflow_template)"
  },
  {
    "code": "def is_safe_url(url, host=None):\\n\\tif url is not None:\\n\\t\\turl = url.strip()\\n\\tif not url:\\n\\t\\treturn False\\n\\treturn _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_safe_url(url, host=None):\\n\\tif url is not None:\\n\\t\\turl = url.strip()\\n\\tif not url:\\n\\t\\treturn False\\n\\treturn _is_safe_url(url, host) and _is_safe_url(url.replace('\\\\', '/'), host)"
  },
  {
    "code": "def set_atom(self, block, existing_col, min_itemsize, nan_rep, **kwargs):\\n        self.values   = list(block.items)\\n        dtype         = block.dtype.name\\n        if dtype == 'object':\\n            inferred_type = lib.infer_dtype(block.values.flatten())\\n            if inferred_type == 'unicode':\\n                raise NotImplementedError(\"unicode is not implemented as a table column\")\\n            elif inferred_type == 'date':\\n                raise NotImplementedError(\"date is not implemented as a table column\")\\n            self.set_atom_object(block, existing_col, min_itemsize, nan_rep)\\n        elif dtype == 'datetime64[ns]':\\n            raise NotImplementedError(\"datetime64[ns] is not implemented as a table column\")\\n        else:\\n            self.set_atom_data(block)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_atom(self, block, existing_col, min_itemsize, nan_rep, **kwargs):\\n        self.values   = list(block.items)\\n        dtype         = block.dtype.name\\n        if dtype == 'object':\\n            inferred_type = lib.infer_dtype(block.values.flatten())\\n            if inferred_type == 'unicode':\\n                raise NotImplementedError(\"unicode is not implemented as a table column\")\\n            elif inferred_type == 'date':\\n                raise NotImplementedError(\"date is not implemented as a table column\")\\n            self.set_atom_object(block, existing_col, min_itemsize, nan_rep)\\n        elif dtype == 'datetime64[ns]':\\n            raise NotImplementedError(\"datetime64[ns] is not implemented as a table column\")\\n        else:\\n            self.set_atom_data(block)\\n        return self"
  },
  {
    "code": "def copy(self):\\n        values = np.asarray(self).copy()\\n        return SparseSeries(values, index=self.index,\\n                            sparse_index=self.sparse_index)\\nclass SparseTimeSeries(SparseSeries, TimeSeries):\\n    pass\\nclass SparseDataFrame(DataFrame):\\n    _columns = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy(self):\\n        values = np.asarray(self).copy()\\n        return SparseSeries(values, index=self.index,\\n                            sparse_index=self.sparse_index)\\nclass SparseTimeSeries(SparseSeries, TimeSeries):\\n    pass\\nclass SparseDataFrame(DataFrame):\\n    _columns = None"
  },
  {
    "code": "def _log_multivariate_normal_density_full(X, means, covars, min_covar=1.e-7):\\n    n_samples, n_dim = X.shape\\n    nmix = len(means)\\n    log_prob = np.empty((n_samples, nmix))\\n    for c, (mu, cv) in enumerate(zip(means, covars)):\\n        try:\\n            cv_chol = linalg.cholesky(cv, lower=True)\\n        except linalg.LinAlgError:\\n            try:\\n                cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),\\n                                          lower=True)\\n            except linalg.LinAlgError:\\n                raise ValueError(\"'covars' must be symmetric, positive-definite\")\\n        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\\n        cv_sol = linalg.solve_triangular(cv_chol, (X - mu).T, lower=True).T\\n        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) +\\n                                 n_dim * np.log(2 * np.pi) + cv_log_det)\\n    return log_prob",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _log_multivariate_normal_density_full(X, means, covars, min_covar=1.e-7):\\n    n_samples, n_dim = X.shape\\n    nmix = len(means)\\n    log_prob = np.empty((n_samples, nmix))\\n    for c, (mu, cv) in enumerate(zip(means, covars)):\\n        try:\\n            cv_chol = linalg.cholesky(cv, lower=True)\\n        except linalg.LinAlgError:\\n            try:\\n                cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),\\n                                          lower=True)\\n            except linalg.LinAlgError:\\n                raise ValueError(\"'covars' must be symmetric, positive-definite\")\\n        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\\n        cv_sol = linalg.solve_triangular(cv_chol, (X - mu).T, lower=True).T\\n        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) +\\n                                 n_dim * np.log(2 * np.pi) + cv_log_det)\\n    return log_prob"
  },
  {
    "code": "def __bootstrap_inner(self):\\n        try:\\n            self._set_ident()\\n            self.__started.set()\\n            _active_limbo_lock.acquire()\\n            _active[self.__ident] = self\\n            del _limbo[self]\\n            _active_limbo_lock.release()\\n            if __debug__:\\n                self._note(\"%s.__bootstrap(): thread started\", self)\\n            if _trace_hook:\\n                self._note(\"%s.__bootstrap(): registering trace hook\", self)\\n                _sys.settrace(_trace_hook)\\n            if _profile_hook:\\n                self._note(\"%s.__bootstrap(): registering profile hook\", self)\\n                _sys.setprofile(_profile_hook)\\n            try:\\n                self.run()\\n            except SystemExit:\\n                if __debug__:\\n                    self._note(\"%s.__bootstrap(): raised SystemExit\", self)\\n            except:\\n                if __debug__:\\n                    self._note(\"%s.__bootstrap(): unhandled exception\", self)\\n                if _sys:\\n                    _sys.stderr.write(\"Exception in thread %s:\\n%s\\n\" %\\n                                      (self.name, _format_exc()))\\n                else:\\n                    exc_type, exc_value, exc_tb = self.__exc_info()\\n                    try:\\n                        print>>self.__stderr, (\\n                            \"Exception in thread \" + self.name +\\n                            \" (most likely raised during interpreter shutdown):\")\\n                        print>>self.__stderr, (\\n                            \"Traceback (most recent call last):\")\\n                        while exc_tb:\\n                            print>>self.__stderr, (\\n                                '  File \"%s\", line %s, in %s' %\\n                                (exc_tb.tb_frame.f_code.co_filename,\\n                                    exc_tb.tb_lineno,\\n                                    exc_tb.tb_frame.f_code.co_name))\\n                            exc_tb = exc_tb.tb_next\\n                        print>>self.__stderr, (\"%s: %s\" % (exc_type, exc_value))\\n                    finally:\\n                        del exc_type, exc_value, exc_tb\\n            else:\\n                if __debug__:\\n                    self._note(\"%s.__bootstrap(): normal return\", self)\\n            finally:\\n                self.__exc_clear()\\n        finally:\\n            with _active_limbo_lock:\\n                self.__stop()\\n                try:\\n                    del _active[_get_ident()]\\n                except:\\n                    pass",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __bootstrap_inner(self):\\n        try:\\n            self._set_ident()\\n            self.__started.set()\\n            _active_limbo_lock.acquire()\\n            _active[self.__ident] = self\\n            del _limbo[self]\\n            _active_limbo_lock.release()\\n            if __debug__:\\n                self._note(\"%s.__bootstrap(): thread started\", self)\\n            if _trace_hook:\\n                self._note(\"%s.__bootstrap(): registering trace hook\", self)\\n                _sys.settrace(_trace_hook)\\n            if _profile_hook:\\n                self._note(\"%s.__bootstrap(): registering profile hook\", self)\\n                _sys.setprofile(_profile_hook)\\n            try:\\n                self.run()\\n            except SystemExit:\\n                if __debug__:\\n                    self._note(\"%s.__bootstrap(): raised SystemExit\", self)\\n            except:\\n                if __debug__:\\n                    self._note(\"%s.__bootstrap(): unhandled exception\", self)\\n                if _sys:\\n                    _sys.stderr.write(\"Exception in thread %s:\\n%s\\n\" %\\n                                      (self.name, _format_exc()))\\n                else:\\n                    exc_type, exc_value, exc_tb = self.__exc_info()\\n                    try:\\n                        print>>self.__stderr, (\\n                            \"Exception in thread \" + self.name +\\n                            \" (most likely raised during interpreter shutdown):\")\\n                        print>>self.__stderr, (\\n                            \"Traceback (most recent call last):\")\\n                        while exc_tb:\\n                            print>>self.__stderr, (\\n                                '  File \"%s\", line %s, in %s' %\\n                                (exc_tb.tb_frame.f_code.co_filename,\\n                                    exc_tb.tb_lineno,\\n                                    exc_tb.tb_frame.f_code.co_name))\\n                            exc_tb = exc_tb.tb_next\\n                        print>>self.__stderr, (\"%s: %s\" % (exc_type, exc_value))\\n                    finally:\\n                        del exc_type, exc_value, exc_tb\\n            else:\\n                if __debug__:\\n                    self._note(\"%s.__bootstrap(): normal return\", self)\\n            finally:\\n                self.__exc_clear()\\n        finally:\\n            with _active_limbo_lock:\\n                self.__stop()\\n                try:\\n                    del _active[_get_ident()]\\n                except:\\n                    pass"
  },
  {
    "code": "def drop_duplicates(self, subset=None, take_last=False, inplace=False):\\n        duplicated = self.duplicated(subset, take_last=take_last)\\n        if inplace:\\n            inds, = (-duplicated).nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]\\n    @deprecate_kwarg(old_arg_name='cols', new_arg_name='subset')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def drop_duplicates(self, subset=None, take_last=False, inplace=False):\\n        duplicated = self.duplicated(subset, take_last=take_last)\\n        if inplace:\\n            inds, = (-duplicated).nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]\\n    @deprecate_kwarg(old_arg_name='cols', new_arg_name='subset')"
  },
  {
    "code": "def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\\n    ''\\n    from ansible.playbook.block import Block\\n    from ansible.playbook.handler import Handler\\n    from ansible.playbook.task import Task\\n    from ansible.playbook.task_include import TaskInclude\\n    from ansible.playbook.role_include import IncludeRole\\n    from ansible.playbook.handler_task_include import HandlerTaskInclude\\n    from ansible.template import Templar\\n    from ansible.utils.plugin_docs import get_versioned_doclink\\n    if not isinstance(ds, list):\\n        raise AnsibleAssertionError('The ds (%s) should be a list but was a %s' % (ds, type(ds)))\\n    task_list = []\\n    for task_ds in ds:\\n        if not isinstance(task_ds, dict):\\n            raise AnsibleAssertionError('The ds (%s) should be a dict but was a %s' % (ds, type(ds)))\\n        if 'block' in task_ds:\\n            t = Block.load(\\n                task_ds,\\n                play=play,\\n                parent_block=block,\\n                role=role,\\n                task_include=task_include,\\n                use_handlers=use_handlers,\\n                variable_manager=variable_manager,\\n                loader=loader,\\n            )\\n            task_list.append(t)\\n        else:\\n            args_parser = ModuleArgsParser(task_ds)\\n            try:\\n                (action, args, delegate_to) = args_parser.parse(skip_action_validation=True)\\n            except AnsibleParserError as e:\\n                if e.obj:\\n                    raise\\n                raise AnsibleParserError(to_native(e), obj=task_ds, orig_exc=e)\\n            if action in C._ACTION_ALL_INCLUDE_IMPORT_TASKS:\\n                if use_handlers:\\n                    include_class = HandlerTaskInclude\\n                else:\\n                    include_class = TaskInclude\\n                t = include_class.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader\\n                )\\n                all_vars = variable_manager.get_vars(play=play, task=t)\\n                templar = Templar(loader=loader, variables=all_vars)\\n                if action in C._ACTION_INCLUDE_TASKS:\\n                    is_static = False\\n                elif action in C._ACTION_IMPORT_TASKS:\\n                    is_static = True\\n                else:\\n                    include_link = get_versioned_doclink('user_guide/playbooks_reuse_includes.html')\\n                    display.deprecated('\"include\" is deprecated, use include_tasks/import_tasks instead. See %s for details' % include_link, \"2.16\")\\n                    is_static = not templar.is_template(t.args['_raw_params']) and t.all_parents_static() and not t.loop\\n                if is_static:\\n                    if t.loop is not None:\\n                        if action in C._ACTION_IMPORT_TASKS:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_tasks' statements. You should use 'include_tasks' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include with a loop\", obj=task_ds)\\n                    t.statically_loaded = True\\n                    parent_include = block\\n                    cumulative_path = None\\n                    found = False\\n                    subdir = 'tasks'\\n                    if use_handlers:\\n                        subdir = 'handlers'\\n                    while parent_include is not None:\\n                        if not isinstance(parent_include, TaskInclude):\\n                            parent_include = parent_include._parent\\n                            continue\\n                        try:\\n                            parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\\n                        except AnsibleUndefinedVariable as e:\\n                            if not parent_include.statically_loaded:\\n                                raise AnsibleParserError(\\n                                    \"Error when evaluating variable in dynamic parent include path: %s. \"\\n                                    \"When using static imports, the parent dynamic include cannot utilize host facts \"\\n                                    \"or variables from inventory\" % parent_include.args.get('_raw_params'),\\n                                    obj=task_ds,\\n                                    suppress_extended_error=True,\\n                                    orig_exc=e\\n                                )\\n                            raise\\n                        if cumulative_path is None:\\n                            cumulative_path = parent_include_dir\\n                        elif not os.path.isabs(cumulative_path):\\n                            cumulative_path = os.path.join(parent_include_dir, cumulative_path)\\n                        include_target = templar.template(t.args['_raw_params'])\\n                        if t._role:\\n                            new_basedir = os.path.join(t._role._role_path, subdir, cumulative_path)\\n                            include_file = loader.path_dwim_relative(new_basedir, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim_relative(loader.get_basedir(), cumulative_path, include_target)\\n                        if os.path.exists(include_file):\\n                            found = True\\n                            break\\n                        else:\\n                            parent_include = parent_include._parent\\n                    if not found:\\n                        try:\\n                            include_target = templar.template(t.args['_raw_params'])\\n                        except AnsibleUndefinedVariable as e:\\n                            raise AnsibleParserError(\\n                                \"Error when evaluating variable in import path: %s.\\n\"\\n                                \"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\\n                                \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\\n                                \"sources like group or host vars.\" % t.args['_raw_params'],\\n                                obj=task_ds,\\n                                suppress_extended_error=True,\\n                                orig_exc=e)\\n                        if t._role:\\n                            include_file = loader.path_dwim_relative(t._role._role_path, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim(include_target)\\n                    data = loader.load_from_file(include_file)\\n                    if not data:\\n                        display.warning('file %s is empty and had no tasks to include' % include_file)\\n                        continue\\n                    elif not isinstance(data, list):\\n                        raise AnsibleParserError(\"included task files must contain a list of tasks\", obj=data)\\n                    display.vv(\"statically imported: %s\" % include_file)\\n                    ti_copy = t.copy(exclude_parent=True)\\n                    ti_copy._parent = block\\n                    included_blocks = load_list_of_blocks(\\n                        data,\\n                        play=play,\\n                        parent_block=None,\\n                        task_include=ti_copy,\\n                        role=role,\\n                        use_handlers=use_handlers,\\n                        loader=loader,\\n                        variable_manager=variable_manager,\\n                    )\\n                    tags = ti_copy.tags[:]\\n                    for b in included_blocks:\\n                        b.tags = list(set(b.tags).union(tags))\\n                    if use_handlers:\\n                        for b in included_blocks:\\n                            task_list.extend(b.block)\\n                    else:\\n                        task_list.extend(included_blocks)\\n                else:\\n                    t.is_static = False\\n                    task_list.append(t)\\n            elif action in C._ACTION_ALL_PROPER_INCLUDE_IMPORT_ROLES:\\n                ir = IncludeRole.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader,\\n                )\\n                is_static = False\\n                if action in C._ACTION_IMPORT_ROLE:\\n                    is_static = True\\n                if is_static:\\n                    if ir.loop is not None:\\n                        if action in C._ACTION_IMPORT_ROLE:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_role' statements. You should use 'include_role' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include_role with a loop\", obj=task_ds)\\n                    ir.statically_loaded = True\\n                    all_vars = variable_manager.get_vars(play=play, task=ir)\\n                    templar = Templar(loader=loader, variables=all_vars)\\n                    ir._role_name = templar.template(ir._role_name)\\n                    blocks, _ = ir.get_block_list(variable_manager=variable_manager, loader=loader)\\n                    task_list.extend(blocks)\\n                else:\\n                    task_list.append(ir)\\n            else:\\n                if use_handlers:\\n                    t = Handler.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                else:\\n                    t = Task.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                task_list.append(t)\\n    return task_list",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Raise a proper error when include/import_role is used as a handler (#77807)",
    "fixed_code": "def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\\n    ''\\n    from ansible.playbook.block import Block\\n    from ansible.playbook.handler import Handler\\n    from ansible.playbook.task import Task\\n    from ansible.playbook.task_include import TaskInclude\\n    from ansible.playbook.role_include import IncludeRole\\n    from ansible.playbook.handler_task_include import HandlerTaskInclude\\n    from ansible.template import Templar\\n    from ansible.utils.plugin_docs import get_versioned_doclink\\n    if not isinstance(ds, list):\\n        raise AnsibleAssertionError('The ds (%s) should be a list but was a %s' % (ds, type(ds)))\\n    task_list = []\\n    for task_ds in ds:\\n        if not isinstance(task_ds, dict):\\n            raise AnsibleAssertionError('The ds (%s) should be a dict but was a %s' % (ds, type(ds)))\\n        if 'block' in task_ds:\\n            t = Block.load(\\n                task_ds,\\n                play=play,\\n                parent_block=block,\\n                role=role,\\n                task_include=task_include,\\n                use_handlers=use_handlers,\\n                variable_manager=variable_manager,\\n                loader=loader,\\n            )\\n            task_list.append(t)\\n        else:\\n            args_parser = ModuleArgsParser(task_ds)\\n            try:\\n                (action, args, delegate_to) = args_parser.parse(skip_action_validation=True)\\n            except AnsibleParserError as e:\\n                if e.obj:\\n                    raise\\n                raise AnsibleParserError(to_native(e), obj=task_ds, orig_exc=e)\\n            if action in C._ACTION_ALL_INCLUDE_IMPORT_TASKS:\\n                if use_handlers:\\n                    include_class = HandlerTaskInclude\\n                else:\\n                    include_class = TaskInclude\\n                t = include_class.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader\\n                )\\n                all_vars = variable_manager.get_vars(play=play, task=t)\\n                templar = Templar(loader=loader, variables=all_vars)\\n                if action in C._ACTION_INCLUDE_TASKS:\\n                    is_static = False\\n                elif action in C._ACTION_IMPORT_TASKS:\\n                    is_static = True\\n                else:\\n                    include_link = get_versioned_doclink('user_guide/playbooks_reuse_includes.html')\\n                    display.deprecated('\"include\" is deprecated, use include_tasks/import_tasks instead. See %s for details' % include_link, \"2.16\")\\n                    is_static = not templar.is_template(t.args['_raw_params']) and t.all_parents_static() and not t.loop\\n                if is_static:\\n                    if t.loop is not None:\\n                        if action in C._ACTION_IMPORT_TASKS:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_tasks' statements. You should use 'include_tasks' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include with a loop\", obj=task_ds)\\n                    t.statically_loaded = True\\n                    parent_include = block\\n                    cumulative_path = None\\n                    found = False\\n                    subdir = 'tasks'\\n                    if use_handlers:\\n                        subdir = 'handlers'\\n                    while parent_include is not None:\\n                        if not isinstance(parent_include, TaskInclude):\\n                            parent_include = parent_include._parent\\n                            continue\\n                        try:\\n                            parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\\n                        except AnsibleUndefinedVariable as e:\\n                            if not parent_include.statically_loaded:\\n                                raise AnsibleParserError(\\n                                    \"Error when evaluating variable in dynamic parent include path: %s. \"\\n                                    \"When using static imports, the parent dynamic include cannot utilize host facts \"\\n                                    \"or variables from inventory\" % parent_include.args.get('_raw_params'),\\n                                    obj=task_ds,\\n                                    suppress_extended_error=True,\\n                                    orig_exc=e\\n                                )\\n                            raise\\n                        if cumulative_path is None:\\n                            cumulative_path = parent_include_dir\\n                        elif not os.path.isabs(cumulative_path):\\n                            cumulative_path = os.path.join(parent_include_dir, cumulative_path)\\n                        include_target = templar.template(t.args['_raw_params'])\\n                        if t._role:\\n                            new_basedir = os.path.join(t._role._role_path, subdir, cumulative_path)\\n                            include_file = loader.path_dwim_relative(new_basedir, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim_relative(loader.get_basedir(), cumulative_path, include_target)\\n                        if os.path.exists(include_file):\\n                            found = True\\n                            break\\n                        else:\\n                            parent_include = parent_include._parent\\n                    if not found:\\n                        try:\\n                            include_target = templar.template(t.args['_raw_params'])\\n                        except AnsibleUndefinedVariable as e:\\n                            raise AnsibleParserError(\\n                                \"Error when evaluating variable in import path: %s.\\n\"\\n                                \"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\\n                                \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\\n                                \"sources like group or host vars.\" % t.args['_raw_params'],\\n                                obj=task_ds,\\n                                suppress_extended_error=True,\\n                                orig_exc=e)\\n                        if t._role:\\n                            include_file = loader.path_dwim_relative(t._role._role_path, subdir, include_target)\\n                        else:\\n                            include_file = loader.path_dwim(include_target)\\n                    data = loader.load_from_file(include_file)\\n                    if not data:\\n                        display.warning('file %s is empty and had no tasks to include' % include_file)\\n                        continue\\n                    elif not isinstance(data, list):\\n                        raise AnsibleParserError(\"included task files must contain a list of tasks\", obj=data)\\n                    display.vv(\"statically imported: %s\" % include_file)\\n                    ti_copy = t.copy(exclude_parent=True)\\n                    ti_copy._parent = block\\n                    included_blocks = load_list_of_blocks(\\n                        data,\\n                        play=play,\\n                        parent_block=None,\\n                        task_include=ti_copy,\\n                        role=role,\\n                        use_handlers=use_handlers,\\n                        loader=loader,\\n                        variable_manager=variable_manager,\\n                    )\\n                    tags = ti_copy.tags[:]\\n                    for b in included_blocks:\\n                        b.tags = list(set(b.tags).union(tags))\\n                    if use_handlers:\\n                        for b in included_blocks:\\n                            task_list.extend(b.block)\\n                    else:\\n                        task_list.extend(included_blocks)\\n                else:\\n                    t.is_static = False\\n                    task_list.append(t)\\n            elif action in C._ACTION_ALL_PROPER_INCLUDE_IMPORT_ROLES:\\n                if use_handlers:\\n                    raise AnsibleParserError(f\"Using '{action}' as a handler is not supported.\", obj=task_ds)\\n                ir = IncludeRole.load(\\n                    task_ds,\\n                    block=block,\\n                    role=role,\\n                    task_include=None,\\n                    variable_manager=variable_manager,\\n                    loader=loader,\\n                )\\n                is_static = False\\n                if action in C._ACTION_IMPORT_ROLE:\\n                    is_static = True\\n                if is_static:\\n                    if ir.loop is not None:\\n                        if action in C._ACTION_IMPORT_ROLE:\\n                            raise AnsibleParserError(\"You cannot use loops on 'import_role' statements. You should use 'include_role' instead.\", obj=task_ds)\\n                        else:\\n                            raise AnsibleParserError(\"You cannot use 'static' on an include_role with a loop\", obj=task_ds)\\n                    ir.statically_loaded = True\\n                    all_vars = variable_manager.get_vars(play=play, task=ir)\\n                    templar = Templar(loader=loader, variables=all_vars)\\n                    ir._role_name = templar.template(ir._role_name)\\n                    blocks, _ = ir.get_block_list(variable_manager=variable_manager, loader=loader)\\n                    task_list.extend(blocks)\\n                else:\\n                    task_list.append(ir)\\n            else:\\n                if use_handlers:\\n                    t = Handler.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                else:\\n                    t = Task.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\\n                task_list.append(t)\\n    return task_list"
  },
  {
    "code": "def mean(self, axis=None):\\n        if self.axis == 0:\\n            return self._cython_agg_general('mean')\\n        else:\\n            return self.aggregate(lambda x: x.mean(axis=self.axis))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mean(self, axis=None):\\n        if self.axis == 0:\\n            return self._cython_agg_general('mean')\\n        else:\\n            return self.aggregate(lambda x: x.mean(axis=self.axis))"
  },
  {
    "code": "def swaplevel(self, i=-2, j=-1, axis=0):\\n        result = self.copy()\\n        axis = self._get_axis_number(axis)\\n        if axis == 0:\\n            result.index = result.index.swaplevel(i, j)\\n        else:\\n            result.columns = result.columns.swaplevel(i, j)\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def swaplevel(self, i=-2, j=-1, axis=0):\\n        result = self.copy()\\n        axis = self._get_axis_number(axis)\\n        if axis == 0:\\n            result.index = result.index.swaplevel(i, j)\\n        else:\\n            result.columns = result.columns.swaplevel(i, j)\\n        return result"
  },
  {
    "code": "def configuration(parent_package='',top_path=None):\\n    from numpy.distutils.misc_util import Configuration\\n    from numpy.distutils.system_info import get_info, get_standard_file, BlasNotFoundError\\n    config = Configuration('learn',parent_package,top_path)\\n    site_cfg  = ConfigParser()\\n    site_cfg.read(get_standard_file('site.cfg'))\\n    config.add_subpackage('datasets')\\n    config.add_subpackage('features')\\n    config.add_subpackage('feature_selection')\\n    config.add_subpackage('utils')\\n    libsvm_includes = [numpy.get_include()]\\n    libsvm_libraries = []\\n    libsvm_library_dirs = []\\n    libsvm_sources = [join('src', 'libsvm', '_libsvm.c')]\\n    if site_cfg.has_section('libsvm'):\\n        libsvm_includes.append(site_cfg.get('libsvm', 'include_dirs'))\\n        libsvm_libraries.append(site_cfg.get('libsvm', 'libraries'))\\n        libsvm_library_dirs.append(site_cfg.get('libsvm', 'library_dirs'))\\n    else:\\n        libsvm_sources.append(join('src', 'libsvm', 'svm.cpp'))\\n    config.add_extension('_libsvm',\\n                         sources=libsvm_sources,\\n                         include_dirs=libsvm_includes,\\n                         libraries=libsvm_libraries,\\n                         library_dirs=libsvm_library_dirs,\\n                         depends=[join('src', 'libsvm', 'svm.h'),\\n                                  join('src', 'libsvm', 'libsvm_helper.c'),\\n                                  ])\\n    blas_sources = [join('src', 'blas', 'daxpy.c'),\\n                    join('src', 'blas', 'ddot.c'),\\n                    join('src', 'blas', 'dnrm2.c'),\\n                    join('src', 'blas', 'dscal.c')]\\n    liblinear_sources = [join('src', 'linear.cpp'),\\n                         join('src', '_liblinear.c'),\\n                         join('src', 'tron.cpp')]\\n    blas_info = get_info('blas_opt', 0)\\n    if not blas_info:\\n        config.add_library('blas', blas_sources)\\n        warnings.warn(BlasNotFoundError.__doc__)\\n    config.add_extension('_liblinear',\\n                         sources=liblinear_sources,\\n                         libraries = blas_info.pop('libraries', ['blas']),\\n                         include_dirs=['src',\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         depends=[join('src', 'linear.h'),\\n                                  join('src', 'tron.h'),\\n                                  join('src', 'blas', 'blas.h'),\\n                                  join('src', 'blas', 'blasp.h')],\\n                         **blas_info)\\n    blas_info = get_info('blas_opt', 0)\\n    if (not blas_info) or (\\n        ('NO_ATLAS_INFO', 1) in blas_info.get('define_macros', [])) :\\n        config.add_library('cblas',\\n                           sources=[\\n                               join('src', 'cblas', '*.c'),\\n                               ]\\n                           )\\n        cblas_libs = ['cblas']\\n        blas_info.pop('libraries', None)\\n    else:\\n        cblas_libs = blas_info.pop('libraries', [])\\n    minilearn_sources = [\\n        join('src', 'minilearn', 'lars.c'),\\n        join('src', 'minilearn', '_minilearn.c')]\\n    config.add_extension('_minilearn',\\n                         sources = minilearn_sources,\\n                         libraries = cblas_libs,\\n                         include_dirs=[join('src', 'minilearn'),\\n                                       join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )\\n    config.add_extension('ball_tree',\\n                         sources=[join('src', 'BallTree.cpp')],\\n                         include_dirs=[numpy.get_include()]\\n                         )\\n    config.add_extension('cd_fast',\\n                         sources=[join('src', 'cd_fast.c')],\\n                         libraries=cblas_libs,\\n                         include_dirs=[join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Initial support for sparse matrices in SVMs (scikits.learn.sparse.svm)",
    "fixed_code": "def configuration(parent_package='',top_path=None):\\n    from numpy.distutils.misc_util import Configuration\\n    from numpy.distutils.system_info import get_info, get_standard_file, BlasNotFoundError\\n    config = Configuration('learn',parent_package,top_path)\\n    site_cfg  = ConfigParser()\\n    site_cfg.read(get_standard_file('site.cfg'))\\n    config.add_subpackage('datasets')\\n    config.add_subpackage('features')\\n    config.add_subpackage('feature_selection')\\n    config.add_subpackage('utils')\\n    libsvm_includes = [numpy.get_include()]\\n    libsvm_libraries = []\\n    libsvm_library_dirs = []\\n    libsvm_sources = [join('src', 'libsvm', '_libsvm.c')]\\n    if site_cfg.has_section('libsvm'):\\n        libsvm_includes.append(site_cfg.get('libsvm', 'include_dirs'))\\n        libsvm_libraries.append(site_cfg.get('libsvm', 'libraries'))\\n        libsvm_library_dirs.append(site_cfg.get('libsvm', 'library_dirs'))\\n    else:\\n        libsvm_sources.append(join('src', 'libsvm', 'svm.cpp'))\\n    config.add_extension('_libsvm',\\n                         sources=libsvm_sources,\\n                         include_dirs=libsvm_includes,\\n                         libraries=libsvm_libraries,\\n                         library_dirs=libsvm_library_dirs,\\n                         depends=[join('src', 'libsvm', 'svm.h'),\\n                                  join('src', 'libsvm', 'libsvm_helper.c')]\\n                                  )\\n    blas_sources = [join('src', 'blas', 'daxpy.c'),\\n                    join('src', 'blas', 'ddot.c'),\\n                    join('src', 'blas', 'dnrm2.c'),\\n                    join('src', 'blas', 'dscal.c')]\\n    liblinear_sources = [join('src', 'linear.cpp'),\\n                         join('src', '_liblinear.c'),\\n                         join('src', 'tron.cpp')]\\n    blas_info = get_info('blas_opt', 0)\\n    if not blas_info:\\n        config.add_library('blas', blas_sources)\\n        warnings.warn(BlasNotFoundError.__doc__)\\n    config.add_extension('_liblinear',\\n                         sources=liblinear_sources,\\n                         libraries = blas_info.pop('libraries', ['blas']),\\n                         include_dirs=['src',\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         depends=[join('src', 'linear.h'),\\n                                  join('src', 'tron.h'),\\n                                  join('src', 'blas', 'blas.h'),\\n                                  join('src', 'blas', 'blasp.h')],\\n                         **blas_info)\\n    blas_info = get_info('blas_opt', 0)\\n    if (not blas_info) or (\\n        ('NO_ATLAS_INFO', 1) in blas_info.get('define_macros', [])) :\\n        config.add_library('cblas',\\n                           sources=[\\n                               join('src', 'cblas', '*.c'),\\n                               ]\\n                           )\\n        cblas_libs = ['cblas']\\n        blas_info.pop('libraries', None)\\n    else:\\n        cblas_libs = blas_info.pop('libraries', [])\\n    minilearn_sources = [\\n        join('src', 'minilearn', 'lars.c'),\\n        join('src', 'minilearn', '_minilearn.c')]\\n    config.add_extension('_minilearn',\\n                         sources = minilearn_sources,\\n                         libraries = cblas_libs,\\n                         include_dirs=[join('src', 'minilearn'),\\n                                       join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )\\n    config.add_extension('ball_tree',\\n                         sources=[join('src', 'BallTree.cpp')],\\n                         include_dirs=[numpy.get_include()]\\n                         )\\n    config.add_extension('cd_fast',\\n                         sources=[join('src', 'cd_fast.c')],\\n                         libraries=cblas_libs,\\n                         include_dirs=[join('src', 'cblas'),\\n                                       numpy.get_include(),\\n                                       blas_info.pop('include_dirs', [])],\\n                         extra_compile_args=['-std=c99'] + \\\\n                                             blas_info.pop('extra_compile_args', []),\\n                         **blas_info\\n                         )"
  },
  {
    "code": "def process_rhs(self, compiler, connection):\\n        rhs, rhs_params = super().process_rhs(compiler, connection)\\n        cast_type = self.lhs.output_field.cast_db_type(connection)\\n        return '%s::%s' % (rhs, cast_type), rhs_params",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_rhs(self, compiler, connection):\\n        rhs, rhs_params = super().process_rhs(compiler, connection)\\n        cast_type = self.lhs.output_field.cast_db_type(connection)\\n        return '%s::%s' % (rhs, cast_type), rhs_params"
  },
  {
    "code": "def decr_version(self, key, delta=1, version=None):\\n        return self.incr_version(key, -delta, version)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decr_version(self, key, delta=1, version=None):\\n        return self.incr_version(key, -delta, version)"
  },
  {
    "code": "def _create_methods(cls, arith_method, comp_method, bool_method, special):\\n    have_divmod = issubclass(cls, ABCSeries)\\n    new_methods = dict(\\n        add=arith_method(cls, operator.add, special),\\n        radd=arith_method(cls, radd, special),\\n        sub=arith_method(cls, operator.sub, special),\\n        mul=arith_method(cls, operator.mul, special),\\n        truediv=arith_method(cls, operator.truediv, special),\\n        floordiv=arith_method(cls, operator.floordiv, special),\\n        mod=arith_method(cls, operator.mod, special),\\n        pow=arith_method(cls, operator.pow, special),\\n        rmul=arith_method(cls, rmul, special),\\n        rsub=arith_method(cls, rsub, special),\\n        rtruediv=arith_method(cls, rtruediv, special),\\n        rfloordiv=arith_method(cls, rfloordiv, special),\\n        rpow=arith_method(cls, rpow, special),\\n        rmod=arith_method(cls, rmod, special))\\n    new_methods['div'] = new_methods['truediv']\\n    new_methods['rdiv'] = new_methods['rtruediv']\\n    if have_divmod:\\n        new_methods['divmod'] = arith_method(cls, divmod, special)\\n        new_methods['rdivmod'] = arith_method(cls, rdivmod, special)\\n    new_methods.update(dict(\\n        eq=comp_method(cls, operator.eq, special),\\n        ne=comp_method(cls, operator.ne, special),\\n        lt=comp_method(cls, operator.lt, special),\\n        gt=comp_method(cls, operator.gt, special),\\n        le=comp_method(cls, operator.le, special),\\n        ge=comp_method(cls, operator.ge, special)))\\n    if bool_method:\\n        new_methods.update(\\n            dict(and_=bool_method(cls, operator.and_, special),\\n                 or_=bool_method(cls, operator.or_, special),\\n                 xor=bool_method(cls, operator.xor, special),\\n                 rand_=bool_method(cls, rand_, special),\\n                 ror_=bool_method(cls, ror_, special),\\n                 rxor=bool_method(cls, rxor, special)))\\n    if special:\\n        dunderize = lambda x: '__{name}__'.format(name=x.strip('_'))\\n    else:\\n        dunderize = lambda x: x\\n    new_methods = {dunderize(k): v for k, v in new_methods.items()}\\n    return new_methods",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_methods(cls, arith_method, comp_method, bool_method, special):\\n    have_divmod = issubclass(cls, ABCSeries)\\n    new_methods = dict(\\n        add=arith_method(cls, operator.add, special),\\n        radd=arith_method(cls, radd, special),\\n        sub=arith_method(cls, operator.sub, special),\\n        mul=arith_method(cls, operator.mul, special),\\n        truediv=arith_method(cls, operator.truediv, special),\\n        floordiv=arith_method(cls, operator.floordiv, special),\\n        mod=arith_method(cls, operator.mod, special),\\n        pow=arith_method(cls, operator.pow, special),\\n        rmul=arith_method(cls, rmul, special),\\n        rsub=arith_method(cls, rsub, special),\\n        rtruediv=arith_method(cls, rtruediv, special),\\n        rfloordiv=arith_method(cls, rfloordiv, special),\\n        rpow=arith_method(cls, rpow, special),\\n        rmod=arith_method(cls, rmod, special))\\n    new_methods['div'] = new_methods['truediv']\\n    new_methods['rdiv'] = new_methods['rtruediv']\\n    if have_divmod:\\n        new_methods['divmod'] = arith_method(cls, divmod, special)\\n        new_methods['rdivmod'] = arith_method(cls, rdivmod, special)\\n    new_methods.update(dict(\\n        eq=comp_method(cls, operator.eq, special),\\n        ne=comp_method(cls, operator.ne, special),\\n        lt=comp_method(cls, operator.lt, special),\\n        gt=comp_method(cls, operator.gt, special),\\n        le=comp_method(cls, operator.le, special),\\n        ge=comp_method(cls, operator.ge, special)))\\n    if bool_method:\\n        new_methods.update(\\n            dict(and_=bool_method(cls, operator.and_, special),\\n                 or_=bool_method(cls, operator.or_, special),\\n                 xor=bool_method(cls, operator.xor, special),\\n                 rand_=bool_method(cls, rand_, special),\\n                 ror_=bool_method(cls, ror_, special),\\n                 rxor=bool_method(cls, rxor, special)))\\n    if special:\\n        dunderize = lambda x: '__{name}__'.format(name=x.strip('_'))\\n    else:\\n        dunderize = lambda x: x\\n    new_methods = {dunderize(k): v for k, v in new_methods.items()}\\n    return new_methods"
  },
  {
    "code": "def load(self):\\n\\t\\ttry:\\n\\t\\t\\tsession_data = self._cache.get(self.cache_key)\\n\\t\\texcept Exception:\\n\\t\\t\\tsession_data = None\\n\\t\\tif session_data is not None:\\n\\t\\t\\treturn session_data\\n\\t\\tself._session_key = None\\n\\t\\treturn {}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load(self):\\n\\t\\ttry:\\n\\t\\t\\tsession_data = self._cache.get(self.cache_key)\\n\\t\\texcept Exception:\\n\\t\\t\\tsession_data = None\\n\\t\\tif session_data is not None:\\n\\t\\t\\treturn session_data\\n\\t\\tself._session_key = None\\n\\t\\treturn {}"
  },
  {
    "code": "def apply_key_map(key_map, table):\\n    new_dict = {}\\n    for key, value in table.items():\\n        new_key = key_map.get(key)\\n        if value is not None:\\n            new_dict[new_key] = value\\n    return new_dict",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def apply_key_map(key_map, table):\\n    new_dict = {}\\n    for key, value in table.items():\\n        new_key = key_map.get(key)\\n        if value is not None:\\n            new_dict[new_key] = value\\n    return new_dict"
  },
  {
    "code": "def fit(self, X, y):\\n        X, y = check_arrays(X, y, sparse_format='dense')\\n        y = column_or_1d(y, warn=True)\\n        n_samples, n_features = X.shape\\n        self.classes_ = unique_y = np.unique(y)\\n        n_classes = unique_y.shape[0]\\n        self.theta_ = np.zeros((n_classes, n_features))\\n        self.sigma_ = np.zeros((n_classes, n_features))\\n        self.class_prior_ = np.zeros(n_classes)\\n        epsilon = 1e-9\\n        for i, y_i in enumerate(unique_y):\\n            Xi = X[y == y_i, :]\\n            self.theta_[i, :] = np.mean(Xi, axis=0)\\n            self.sigma_[i, :] = np.var(Xi, axis=0) + epsilon\\n            self.class_prior_[i] = np.float(Xi.shape[0]) / n_samples\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add partial_fit to GaussianNB\\n\\nUses Chan/Golub/LeVeque update for model parameters. Does not implement\\nsample weighting.",
    "fixed_code": "def fit(self, X, y):\\n        X, y = check_arrays(X, y, sparse_format='dense')\\n        y = column_or_1d(y, warn=True)\\n        n_samples, n_features = X.shape\\n        self.classes_ = unique_y = np.unique(y)\\n        n_classes = unique_y.shape[0]\\n        self.theta_ = np.zeros((n_classes, n_features))\\n        self.sigma_ = np.zeros((n_classes, n_features))\\n        self.class_prior_ = np.zeros(n_classes)\\n        self.class_count_ = np.zeros(n_classes)\\n        epsilon = 1e-9\\n        for i, y_i in enumerate(unique_y):\\n            Xi = X[y == y_i, :]\\n            self.theta_[i, :] = np.mean(Xi, axis=0)\\n            self.sigma_[i, :] = np.var(Xi, axis=0) + epsilon\\n            self.class_count_[i] = Xi.shape[0]\\n        self.class_prior_[:] = self.class_count_ / n_samples\\n        return self\\n    @staticmethod"
  },
  {
    "code": "def run_load(self,\\n                 destination_project_dataset_table,\\n                 schema_fields,\\n                 source_uris,\\n                 source_format='CSV',\\n                 create_disposition='CREATE_IF_NEEDED',\\n                 skip_leading_rows=0,\\n                 write_disposition='WRITE_EMPTY',\\n                 field_delimiter=',',\\n                 max_bad_records=0,\\n                 quote_character=None,\\n                 ignore_unknown_values=False,\\n                 allow_quoted_newlines=False,\\n                 allow_jagged_rows=False,\\n                 schema_update_options=(),\\n                 src_fmt_configs=None,\\n                 time_partitioning=None,\\n                 cluster_fields=None):\\n        if src_fmt_configs is None:\\n            src_fmt_configs = {}\\n        source_format = source_format.upper()\\n        allowed_formats = [\\n            \"CSV\", \"NEWLINE_DELIMITED_JSON\", \"AVRO\", \"GOOGLE_SHEETS\",\\n            \"DATASTORE_BACKUP\", \"PARQUET\"\\n        ]\\n        if source_format not in allowed_formats:\\n            raise ValueError(\"{0} is not a valid source format. \"\\n                             \"Please use one of the following types: {1}\"\\n                             .format(source_format, allowed_formats))\\n        allowed_schema_update_options = [\\n            'ALLOW_FIELD_ADDITION', \"ALLOW_FIELD_RELAXATION\"\\n        ]\\n        if not set(allowed_schema_update_options).issuperset(\\n                set(schema_update_options)):\\n            raise ValueError(\\n                \"{0} contains invalid schema update options.\"\\n                \"Please only use one or more of the following options: {1}\"\\n                .format(schema_update_options, allowed_schema_update_options))\\n        destination_project, destination_dataset, destination_table = \\\\n            _split_tablename(table_input=destination_project_dataset_table,\\n                             default_project_id=self.project_id,\\n                             var_name='destination_project_dataset_table')\\n        configuration = {\\n            'load': {\\n                'createDisposition': create_disposition,\\n                'destinationTable': {\\n                    'projectId': destination_project,\\n                    'datasetId': destination_dataset,\\n                    'tableId': destination_table,\\n                },\\n                'sourceFormat': source_format,\\n                'sourceUris': source_uris,\\n                'writeDisposition': write_disposition,\\n                'ignoreUnknownValues': ignore_unknown_values\\n            }\\n        }\\n        time_partitioning = _cleanse_time_partitioning(\\n            destination_project_dataset_table,\\n            time_partitioning\\n        )\\n        if time_partitioning:\\n            configuration['load'].update({\\n                'timePartitioning': time_partitioning\\n            })\\n        if cluster_fields:\\n            configuration['load'].update({'clustering': {'fields': cluster_fields}})\\n        if schema_fields:\\n            configuration['load']['schema'] = {'fields': schema_fields}\\n        if schema_update_options:\\n            if write_disposition not in [\"WRITE_APPEND\", \"WRITE_TRUNCATE\"]:\\n                raise ValueError(\"schema_update_options is only \"\\n                                 \"allowed if write_disposition is \"\\n                                 \"'WRITE_APPEND' or 'WRITE_TRUNCATE'.\")\\n            else:\\n                self.log.info(\\n                    \"Adding experimental \"\\n                    \"'schemaUpdateOptions': {0}\".format(schema_update_options))\\n                configuration['load'][\\n                    'schemaUpdateOptions'] = schema_update_options\\n        if max_bad_records:\\n            configuration['load']['maxBadRecords'] = max_bad_records\\n        if 'skipLeadingRows' not in src_fmt_configs:\\n            src_fmt_configs['skipLeadingRows'] = skip_leading_rows\\n        if 'fieldDelimiter' not in src_fmt_configs:\\n            src_fmt_configs['fieldDelimiter'] = field_delimiter\\n        if 'ignoreUnknownValues' not in src_fmt_configs:\\n            src_fmt_configs['ignoreUnknownValues'] = ignore_unknown_values\\n        if quote_character is not None:\\n            src_fmt_configs['quote'] = quote_character\\n        if allow_quoted_newlines:\\n            src_fmt_configs['allowQuotedNewlines'] = allow_quoted_newlines\\n        src_fmt_to_configs_mapping = {\\n            'CSV': [\\n                'allowJaggedRows', 'allowQuotedNewlines', 'autodetect',\\n                'fieldDelimiter', 'skipLeadingRows', 'ignoreUnknownValues',\\n                'nullMarker', 'quote'\\n            ],\\n            'DATASTORE_BACKUP': ['projectionFields'],\\n            'NEWLINE_DELIMITED_JSON': ['autodetect', 'ignoreUnknownValues'],\\n            'PARQUET': ['autodetect', 'ignoreUnknownValues'],\\n            'AVRO': [],\\n        }\\n        valid_configs = src_fmt_to_configs_mapping[source_format]\\n        src_fmt_configs = {\\n            k: v\\n            for k, v in src_fmt_configs.items() if k in valid_configs\\n        }\\n        configuration['load'].update(src_fmt_configs)\\n        if allow_jagged_rows:\\n            configuration['load']['allowJaggedRows'] = allow_jagged_rows\\n        return self.run_with_configuration(configuration)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-461]  Support autodetected schemas in BigQuery run_load (#3880)",
    "fixed_code": "def run_load(self,\\n                 destination_project_dataset_table,\\n                 source_uris,\\n                 schema_fields=None,\\n                 source_format='CSV',\\n                 create_disposition='CREATE_IF_NEEDED',\\n                 skip_leading_rows=0,\\n                 write_disposition='WRITE_EMPTY',\\n                 field_delimiter=',',\\n                 max_bad_records=0,\\n                 quote_character=None,\\n                 ignore_unknown_values=False,\\n                 allow_quoted_newlines=False,\\n                 allow_jagged_rows=False,\\n                 schema_update_options=(),\\n                 src_fmt_configs=None,\\n                 time_partitioning=None,\\n                 cluster_fields=None,\\n                 autodetect=False):\\n        if schema_fields is None and not autodetect:\\n            raise ValueError(\\n                'You must either pass a schema or autodetect=True.')\\n        if src_fmt_configs is None:\\n            src_fmt_configs = {}\\n        source_format = source_format.upper()\\n        allowed_formats = [\\n            \"CSV\", \"NEWLINE_DELIMITED_JSON\", \"AVRO\", \"GOOGLE_SHEETS\",\\n            \"DATASTORE_BACKUP\", \"PARQUET\"\\n        ]\\n        if source_format not in allowed_formats:\\n            raise ValueError(\"{0} is not a valid source format. \"\\n                             \"Please use one of the following types: {1}\"\\n                             .format(source_format, allowed_formats))\\n        allowed_schema_update_options = [\\n            'ALLOW_FIELD_ADDITION', \"ALLOW_FIELD_RELAXATION\"\\n        ]\\n        if not set(allowed_schema_update_options).issuperset(\\n                set(schema_update_options)):\\n            raise ValueError(\\n                \"{0} contains invalid schema update options.\"\\n                \"Please only use one or more of the following options: {1}\"\\n                .format(schema_update_options, allowed_schema_update_options))\\n        destination_project, destination_dataset, destination_table = \\\\n            _split_tablename(table_input=destination_project_dataset_table,\\n                             default_project_id=self.project_id,\\n                             var_name='destination_project_dataset_table')\\n        configuration = {\\n            'load': {\\n                'autodetect': autodetect,\\n                'createDisposition': create_disposition,\\n                'destinationTable': {\\n                    'projectId': destination_project,\\n                    'datasetId': destination_dataset,\\n                    'tableId': destination_table,\\n                },\\n                'sourceFormat': source_format,\\n                'sourceUris': source_uris,\\n                'writeDisposition': write_disposition,\\n                'ignoreUnknownValues': ignore_unknown_values\\n            }\\n        }\\n        time_partitioning = _cleanse_time_partitioning(\\n            destination_project_dataset_table,\\n            time_partitioning\\n        )\\n        if time_partitioning:\\n            configuration['load'].update({\\n                'timePartitioning': time_partitioning\\n            })\\n        if cluster_fields:\\n            configuration['load'].update({'clustering': {'fields': cluster_fields}})\\n        if schema_fields:\\n            configuration['load']['schema'] = {'fields': schema_fields}\\n        if schema_update_options:\\n            if write_disposition not in [\"WRITE_APPEND\", \"WRITE_TRUNCATE\"]:\\n                raise ValueError(\"schema_update_options is only \"\\n                                 \"allowed if write_disposition is \"\\n                                 \"'WRITE_APPEND' or 'WRITE_TRUNCATE'.\")\\n            else:\\n                self.log.info(\\n                    \"Adding experimental \"\\n                    \"'schemaUpdateOptions': {0}\".format(schema_update_options))\\n                configuration['load'][\\n                    'schemaUpdateOptions'] = schema_update_options\\n        if max_bad_records:\\n            configuration['load']['maxBadRecords'] = max_bad_records\\n        if 'skipLeadingRows' not in src_fmt_configs:\\n            src_fmt_configs['skipLeadingRows'] = skip_leading_rows\\n        if 'fieldDelimiter' not in src_fmt_configs:\\n            src_fmt_configs['fieldDelimiter'] = field_delimiter\\n        if 'ignoreUnknownValues' not in src_fmt_configs:\\n            src_fmt_configs['ignoreUnknownValues'] = ignore_unknown_values\\n        if quote_character is not None:\\n            src_fmt_configs['quote'] = quote_character\\n        if allow_quoted_newlines:\\n            src_fmt_configs['allowQuotedNewlines'] = allow_quoted_newlines\\n        src_fmt_to_configs_mapping = {\\n            'CSV': [\\n                'allowJaggedRows', 'allowQuotedNewlines', 'autodetect',\\n                'fieldDelimiter', 'skipLeadingRows', 'ignoreUnknownValues',\\n                'nullMarker', 'quote'\\n            ],\\n            'DATASTORE_BACKUP': ['projectionFields'],\\n            'NEWLINE_DELIMITED_JSON': ['autodetect', 'ignoreUnknownValues'],\\n            'PARQUET': ['autodetect', 'ignoreUnknownValues'],\\n            'AVRO': [],\\n        }\\n        valid_configs = src_fmt_to_configs_mapping[source_format]\\n        src_fmt_configs = {\\n            k: v\\n            for k, v in src_fmt_configs.items() if k in valid_configs\\n        }\\n        configuration['load'].update(src_fmt_configs)\\n        if allow_jagged_rows:\\n            configuration['load']['allowJaggedRows'] = allow_jagged_rows\\n        return self.run_with_configuration(configuration)"
  },
  {
    "code": "def time_pad(self):\\n        self.s.str.pad(100, side=\"both\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_pad(self, dtype):\\n        self.s.str.pad(100, side=\"both\")"
  },
  {
    "code": "def _unstack_multiple(data, clocs, fill_value=None):\\n\\tif len(clocs) == 0:\\n\\t\\treturn data\\n\\tindex = data.index\\n\\tif clocs in index.names:\\n\\t\\tclocs = [clocs]\\n\\tclocs = [index._get_level_number(i) for i in clocs]\\n\\trlocs = [i for i in range(index.nlevels) if i not in clocs]\\n\\tclevels = [index.levels[i] for i in clocs]\\n\\tccodes = [index.codes[i] for i in clocs]\\n\\tcnames = [index.names[i] for i in clocs]\\n\\trlevels = [index.levels[i] for i in rlocs]\\n\\trcodes = [index.codes[i] for i in rlocs]\\n\\trnames = [index.names[i] for i in rlocs]\\n\\tshape = [len(x) for x in clevels]\\n\\tgroup_index = get_group_index(ccodes, shape, sort=False, xnull=False)\\n\\tcomp_ids, obs_ids = compress_group_index(group_index, sort=False)\\n\\trecons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\\n\\tif rlocs == []:\\n\\t\\tdummy_index = Index(obs_ids, name=\"__placeholder__\")\\n\\telse:\\n\\t\\tdummy_index = MultiIndex(\\n\\t\\t\\tlevels=rlevels + [obs_ids],\\n\\t\\t\\tcodes=rcodes + [comp_ids],\\n\\t\\t\\tnames=rnames + [\"__placeholder__\"],\\n\\t\\t\\tverify_integrity=False,\\n\\t\\t)\\n\\tif isinstance(data, Series):\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tnew_levels = clevels\\n\\t\\tnew_names = cnames\\n\\t\\tnew_codes = recons_codes\\n\\telse:\\n\\t\\tif isinstance(data.columns, MultiIndex):\\n\\t\\t\\tresult = data\\n\\t\\t\\tfor i in range(len(clocs)):\\n\\t\\t\\t\\tval = clocs[i]\\n\\t\\t\\t\\tresult = result.unstack(val, fill_value=fill_value)\\n\\t\\t\\t\\tclocs = [v if i > v else v - 1 for v in clocs]\\n\\t\\t\\treturn result\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tif isinstance(unstacked, Series):\\n\\t\\t\\tunstcols = unstacked.index\\n\\t\\telse:\\n\\t\\t\\tunstcols = unstacked.columns\\n\\t\\tassert isinstance(unstcols, MultiIndex)  \\n\\t\\tnew_levels = [unstcols.levels[0]] + clevels\\n\\t\\tnew_names = [data.columns.name] + cnames\\n\\t\\tnew_codes = [unstcols.codes[0]]\\n\\t\\tfor rec in recons_codes:\\n\\t\\t\\tnew_codes.append(rec.take(unstcols.codes[-1]))\\n\\tnew_columns = MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)\\n\\tif isinstance(unstacked, Series):\\n\\t\\tunstacked.index = new_columns\\n\\telse:\\n\\t\\tunstacked.columns = new_columns\\n\\treturn unstacked",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Multiple unstack using row index level labels and multi level columns DataFrame (#32990)",
    "fixed_code": "def _unstack_multiple(data, clocs, fill_value=None):\\n\\tif len(clocs) == 0:\\n\\t\\treturn data\\n\\tindex = data.index\\n\\tif clocs in index.names:\\n\\t\\tclocs = [clocs]\\n\\tclocs = [index._get_level_number(i) for i in clocs]\\n\\trlocs = [i for i in range(index.nlevels) if i not in clocs]\\n\\tclevels = [index.levels[i] for i in clocs]\\n\\tccodes = [index.codes[i] for i in clocs]\\n\\tcnames = [index.names[i] for i in clocs]\\n\\trlevels = [index.levels[i] for i in rlocs]\\n\\trcodes = [index.codes[i] for i in rlocs]\\n\\trnames = [index.names[i] for i in rlocs]\\n\\tshape = [len(x) for x in clevels]\\n\\tgroup_index = get_group_index(ccodes, shape, sort=False, xnull=False)\\n\\tcomp_ids, obs_ids = compress_group_index(group_index, sort=False)\\n\\trecons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\\n\\tif not rlocs:\\n\\t\\tdummy_index = Index(obs_ids, name=\"__placeholder__\")\\n\\telse:\\n\\t\\tdummy_index = MultiIndex(\\n\\t\\t\\tlevels=rlevels + [obs_ids],\\n\\t\\t\\tcodes=rcodes + [comp_ids],\\n\\t\\t\\tnames=rnames + [\"__placeholder__\"],\\n\\t\\t\\tverify_integrity=False,\\n\\t\\t)\\n\\tif isinstance(data, Series):\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tnew_levels = clevels\\n\\t\\tnew_names = cnames\\n\\t\\tnew_codes = recons_codes\\n\\telse:\\n\\t\\tif isinstance(data.columns, MultiIndex):\\n\\t\\t\\tresult = data\\n\\t\\t\\tfor i in range(len(clocs)):\\n\\t\\t\\t\\tval = clocs[i]\\n\\t\\t\\t\\tresult = result.unstack(val, fill_value=fill_value)\\n\\t\\t\\t\\tclocs = [v if v < val else v - 1 for v in clocs]\\n\\t\\t\\treturn result\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tif isinstance(unstacked, Series):\\n\\t\\t\\tunstcols = unstacked.index\\n\\t\\telse:\\n\\t\\t\\tunstcols = unstacked.columns\\n\\t\\tassert isinstance(unstcols, MultiIndex)  \\n\\t\\tnew_levels = [unstcols.levels[0]] + clevels\\n\\t\\tnew_names = [data.columns.name] + cnames\\n\\t\\tnew_codes = [unstcols.codes[0]]\\n\\t\\tfor rec in recons_codes:\\n\\t\\t\\tnew_codes.append(rec.take(unstcols.codes[-1]))\\n\\tnew_columns = MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)\\n\\tif isinstance(unstacked, Series):\\n\\t\\tunstacked.index = new_columns\\n\\telse:\\n\\t\\tunstacked.columns = new_columns\\n\\treturn unstacked"
  },
  {
    "code": "def make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0,\\n                 dry_run=0, owner=None, group=None):\\n    save_cwd = os.getcwd()\\n    if root_dir is not None:\\n        log.debug(\"changing into '%s'\", root_dir)\\n        base_name = os.path.abspath(base_name)\\n        if not dry_run:\\n            os.chdir(root_dir)\\n    if base_dir is None:\\n        base_dir = os.curdir\\n    kwargs = {'dry_run': dry_run}\\n    try:\\n        format_info = ARCHIVE_FORMATS[format]\\n    except KeyError:\\n        raise ValueError(\"unknown archive format '%s'\" % format)\\n    func = format_info[0]\\n    for arg, val in format_info[1]:\\n        kwargs[arg] = val\\n    if format != 'zip':\\n        kwargs['owner'] = owner\\n        kwargs['group'] = group\\n    filename = func(base_name, base_dir, **kwargs)\\n    if root_dir is not None:\\n        log.debug(\"changing back to '%s'\", save_cwd)\\n        os.chdir(save_cwd)\\n    return filename",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0,\\n                 dry_run=0, owner=None, group=None):\\n    save_cwd = os.getcwd()\\n    if root_dir is not None:\\n        log.debug(\"changing into '%s'\", root_dir)\\n        base_name = os.path.abspath(base_name)\\n        if not dry_run:\\n            os.chdir(root_dir)\\n    if base_dir is None:\\n        base_dir = os.curdir\\n    kwargs = {'dry_run': dry_run}\\n    try:\\n        format_info = ARCHIVE_FORMATS[format]\\n    except KeyError:\\n        raise ValueError(\"unknown archive format '%s'\" % format)\\n    func = format_info[0]\\n    for arg, val in format_info[1]:\\n        kwargs[arg] = val\\n    if format != 'zip':\\n        kwargs['owner'] = owner\\n        kwargs['group'] = group\\n    filename = func(base_name, base_dir, **kwargs)\\n    if root_dir is not None:\\n        log.debug(\"changing back to '%s'\", save_cwd)\\n        os.chdir(save_cwd)\\n    return filename"
  },
  {
    "code": "def getinnerframes(tb, context=1):\\n    framelist = []\\n    while tb:\\n        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\\n        framelist.append(FrameInfo(*frameinfo))\\n        tb = tb.tb_next\\n    return framelist",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-88116: Enhance the inspect frame APIs to use the extended position information (GH-91531)",
    "fixed_code": "def getinnerframes(tb, context=1):\\n    framelist = []\\n    while tb:\\n        traceback_info = getframeinfo(tb, context)\\n        frameinfo = (tb.tb_frame,) + traceback_info\\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\\n        tb = tb.tb_next\\n    return framelist"
  },
  {
    "code": "def infer_axes(self):\\n        table = self.table\\n        if table is None:\\n            return False\\n        self.index_axes, self.values_axes = [ a.infer(self.table) for a in self.indexables if a.is_indexable ], [ a.infer(self.table) for a in self.indexables if not a.is_indexable ]\\n        self.non_index_axes   = getattr(self.attrs,'non_index_axes',None) or []\\n        return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def infer_axes(self):\\n        table = self.table\\n        if table is None:\\n            return False\\n        self.index_axes, self.values_axes = [ a.infer(self.table) for a in self.indexables if a.is_indexable ], [ a.infer(self.table) for a in self.indexables if not a.is_indexable ]\\n        self.non_index_axes   = getattr(self.attrs,'non_index_axes',None) or []\\n        return True"
  },
  {
    "code": "def _parse_parameters(self, term):\\n        first_split = term.split(' ', 1)\\n        if len(first_split) <= 1:\\n            relpath = term\\n            params = dict()\\n        else:\\n            relpath = first_split[0]\\n            params = parse_kv(first_split[1])\\n            if '_raw_params' in params:\\n                relpath = u' '.join((relpath, params['_raw_params']))\\n                del params['_raw_params']\\n                if not term.startswith(relpath):\\n                    raise AnsibleError('Unrecognized value after key=value parameters given to password lookup')\\n        invalid_params = frozenset(params.keys()).difference(VALID_PARAMS)\\n        if invalid_params:\\n            raise AnsibleError('Unrecognized parameter(s) given to password lookup: %s' % ', '.join(invalid_params))\\n        params['length'] = int(params.get('length', self.get_option('length')))\\n        params['encrypt'] = params.get('encrypt', self.get_option('encrypt'))\\n        params['ident'] = params.get('ident', self.get_option('ident'))\\n        params['seed'] = params.get('seed', self.get_option('seed'))\\n        params['chars'] = params.get('chars', self.get_option('chars'))\\n        if params['chars'] and isinstance(params['chars'], string_types):\\n            tmp_chars = []\\n            if u',,' in params['chars']:\\n                tmp_chars.append(u',')\\n            tmp_chars.extend(c for c in params['chars'].replace(u',,', u',').split(u',') if c)\\n            params['chars'] = tmp_chars\\n        return relpath, params",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_parameters(self, term):\\n        first_split = term.split(' ', 1)\\n        if len(first_split) <= 1:\\n            relpath = term\\n            params = dict()\\n        else:\\n            relpath = first_split[0]\\n            params = parse_kv(first_split[1])\\n            if '_raw_params' in params:\\n                relpath = u' '.join((relpath, params['_raw_params']))\\n                del params['_raw_params']\\n                if not term.startswith(relpath):\\n                    raise AnsibleError('Unrecognized value after key=value parameters given to password lookup')\\n        invalid_params = frozenset(params.keys()).difference(VALID_PARAMS)\\n        if invalid_params:\\n            raise AnsibleError('Unrecognized parameter(s) given to password lookup: %s' % ', '.join(invalid_params))\\n        params['length'] = int(params.get('length', self.get_option('length')))\\n        params['encrypt'] = params.get('encrypt', self.get_option('encrypt'))\\n        params['ident'] = params.get('ident', self.get_option('ident'))\\n        params['seed'] = params.get('seed', self.get_option('seed'))\\n        params['chars'] = params.get('chars', self.get_option('chars'))\\n        if params['chars'] and isinstance(params['chars'], string_types):\\n            tmp_chars = []\\n            if u',,' in params['chars']:\\n                tmp_chars.append(u',')\\n            tmp_chars.extend(c for c in params['chars'].replace(u',,', u',').split(u',') if c)\\n            params['chars'] = tmp_chars\\n        return relpath, params"
  },
  {
    "code": "def _add_to_cache(self, using, ct):\\n        model = ct.model_class()\\n        key = (model._meta.app_label, model._meta.model_name)\\n        self.__class__._cache.setdefault(using, {})[key] = ct\\n        self.__class__._cache.setdefault(using, {})[ct.id] = ct\\n@python_2_unicode_compatible\\nclass ContentType(models.Model):\\n    name = models.CharField(max_length=100)\\n    app_label = models.CharField(max_length=100)\\n    model = models.CharField(_('python model class name'), max_length=100)\\n    objects = ContentTypeManager()\\n    class Meta:\\n        verbose_name = _('content type')\\n        verbose_name_plural = _('content types')\\n        db_table = 'django_content_type'\\n        ordering = ('name',)\\n        unique_together = (('app_label', 'model'),)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix NoneType error when fetching a stale ContentType with get_for_id\\n\\nWhen a stale ContentType is fetched, the _add_to_cache() function\\ndidn't detect that `model_class()` returns `None` (which it does by\\ndesign). However, the `app_label` + `model` fields can be used instead\\nto as local cache key. Third party apps can detect stale models by\\nchecking whether `model_class()` returns `None`.",
    "fixed_code": "def _add_to_cache(self, using, ct):\\n        key = (ct.app_label, ct.model)\\n        self.__class__._cache.setdefault(using, {})[key] = ct\\n        self.__class__._cache.setdefault(using, {})[ct.id] = ct"
  },
  {
    "code": "def mad(self, axis=0):\\n        if axis == 0:\\n            demeaned = self - self.mean(axis=axis)\\n        else:\\n            demeaned = (self.T - self.mean(axis=axis)).T\\n        y = np.array(demeaned.values, subok=True)\\n        if not issubclass(y.dtype.type, np.int_):\\n            y[np.isnan(y)] = 0\\n        result = np.abs(y).mean(axis=axis)\\n        if axis == 0:\\n            return Series(result, demeaned.columns)\\n        else:\\n            return Series(result, demeaned.index)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: skipna in DataFrame stat functions",
    "fixed_code": "def mad(self, axis=0, skipna=True):\\n        if axis == 0:\\n            demeaned = self - self.mean(axis=0)\\n        else:\\n            demeaned = self.sub(self.mean(axis=1), axis=0)\\n        return np.abs(demeaned).mean(axis=axis, skipna=skipna)"
  },
  {
    "code": "def __getattr__(self, attr):\\n        if attr in self._internal_names_set:\\n            return object.__getattribute__(self, attr)\\n        if attr in self._attributes:\\n            return getattr(self.groupby, attr)\\n        if attr in self.obj:\\n            return self[attr]\\n        if attr in self._deprecated_invalids:\\n            raise ValueError(\".resample() is now a deferred operation\\n\"\\n                             \"\\tuse .resample(...).mean() instead of \"\\n                             \".resample(...)\")\\n        matches_pattern = any(attr.startswith(x) for x\\n                              in self._deprecated_valid_patterns)\\n        if not matches_pattern and attr not in self._deprecated_valids:\\n            self = self._deprecated(attr)\\n        return object.__getattribute__(self, attr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getattr__(self, attr):\\n        if attr in self._internal_names_set:\\n            return object.__getattribute__(self, attr)\\n        if attr in self._attributes:\\n            return getattr(self.groupby, attr)\\n        if attr in self.obj:\\n            return self[attr]\\n        if attr in self._deprecated_invalids:\\n            raise ValueError(\".resample() is now a deferred operation\\n\"\\n                             \"\\tuse .resample(...).mean() instead of \"\\n                             \".resample(...)\")\\n        matches_pattern = any(attr.startswith(x) for x\\n                              in self._deprecated_valid_patterns)\\n        if not matches_pattern and attr not in self._deprecated_valids:\\n            self = self._deprecated(attr)\\n        return object.__getattribute__(self, attr)"
  },
  {
    "code": "def __init__(self, base_field, size=None, **kwargs):\\n        self.base_field = base_field\\n        self.size = size\\n        if self.size:\\n            self.default_validators = self.default_validators[:]\\n            self.default_validators.append(ArrayMaxLengthValidator(self.size))\\n        super(ArrayField, self).__init__(**kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, base_field, size=None, **kwargs):\\n        self.base_field = base_field\\n        self.size = size\\n        if self.size:\\n            self.default_validators = self.default_validators[:]\\n            self.default_validators.append(ArrayMaxLengthValidator(self.size))\\n        super(ArrayField, self).__init__(**kwargs)"
  },
  {
    "code": "def from_string(self, template_code):\\n        return Template(self.env.from_string(template_code))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24694 -- Added support for context_processors to Jinja2 backend.",
    "fixed_code": "def from_string(self, template_code):\\n        return Template(self.env.from_string(template_code), self)"
  },
  {
    "code": "def install_collections(collections, output_path, apis, validate_certs, ignore_errors, no_deps, force, force_deps,\\n                        allow_pre_release=False):\\n    existing_collections = find_existing_collections(output_path, fallback_metadata=True)\\n    with _tempdir() as b_temp_path:\\n        display.display(\"Process install dependency map\")\\n        with _display_progress():\\n            dependency_map = _build_dependency_map(collections, existing_collections, b_temp_path, apis,\\n                                                   validate_certs, force, force_deps, no_deps,\\n                                                   allow_pre_release=allow_pre_release)\\n        display.display(\"Starting collection install process\")\\n        with _display_progress():\\n            for collection in dependency_map.values():\\n                try:\\n                    collection.install(output_path, b_temp_path)\\n                except AnsibleError as err:\\n                    if ignore_errors:\\n                        display.warning(\"Failed to install collection %s but skipping due to --ignore-errors being set. \"\\n                                        \"Error: %s\" % (to_text(collection), to_text(err)))\\n                    else:\\n                        raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def install_collections(collections, output_path, apis, validate_certs, ignore_errors, no_deps, force, force_deps,\\n                        allow_pre_release=False):\\n    existing_collections = find_existing_collections(output_path, fallback_metadata=True)\\n    with _tempdir() as b_temp_path:\\n        display.display(\"Process install dependency map\")\\n        with _display_progress():\\n            dependency_map = _build_dependency_map(collections, existing_collections, b_temp_path, apis,\\n                                                   validate_certs, force, force_deps, no_deps,\\n                                                   allow_pre_release=allow_pre_release)\\n        display.display(\"Starting collection install process\")\\n        with _display_progress():\\n            for collection in dependency_map.values():\\n                try:\\n                    collection.install(output_path, b_temp_path)\\n                except AnsibleError as err:\\n                    if ignore_errors:\\n                        display.warning(\"Failed to install collection %s but skipping due to --ignore-errors being set. \"\\n                                        \"Error: %s\" % (to_text(collection), to_text(err)))\\n                    else:\\n                        raise"
  },
  {
    "code": "def select(condlist, choicelist, default=0):\\n    n = len(condlist)\\n    n2 = len(choicelist)\\n    if n2 != n:\\n        raise ValueError(\\n            \"list of cases must be same length as list of conditions\")\\n    choicelist = [default] + choicelist\\n    S = 0\\n    pfac = 1\\n    for k in range(1, n+1):\\n        S += k * pfac * asarray(condlist[k-1])\\n        if k < n:\\n            pfac *= (1-asarray(condlist[k-1]))\\n    if type(S) in ScalarType or max(asarray(S).shape) == 1:\\n        pfac = asarray(1)\\n        for k in range(n2+1):\\n            pfac = pfac + asarray(choicelist[k])\\n        if type(S) in ScalarType:\\n            S = S*ones(asarray(pfac).shape, type(S))\\n        else:\\n            S = S*ones(asarray(pfac).shape, S.dtype)\\n    return choose(S, tuple(choicelist))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Speed improvements and deprecations for np.select\\n\\nThe idea for this (and some of the code) originally comes from\\nGraeme B Bell (gh-3537).\\nChoose is not as fast and pretty limited, so an iterative\\ncopyto is used instead.\\n\\nCloses gh-3259, gh-3537, gh-3551, and gh-3254",
    "fixed_code": "def select(condlist, choicelist, default=0):\\n    if len(condlist) != len(choicelist):\\n        raise ValueError(\\n            'list of cases must be same length as list of conditions')\\n    if len(condlist) == 0:\\n        warnings.warn(\"select with an empty condition list is not possible\"\\n                      \"and will be deprecated\",\\n                      DeprecationWarning)\\n        return np.asarray(default)[()]\\n    choicelist = [np.asarray(choice) for choice in choicelist]\\n    choicelist.append(np.asarray(default))\\n    dtype = np.result_type(*choicelist)\\n    condlist = np.broadcast_arrays(*condlist)\\n    choicelist = np.broadcast_arrays(*choicelist)\\n    deprecated_ints = False\\n    for i in range(len(condlist)):\\n        cond = condlist[i]\\n        if cond.dtype.type is not np.bool_:\\n            if np.issubdtype(cond.dtype, np.integer):\\n                condlist[i] = condlist[i].astype(bool)\\n                deprecated_ints = True\\n            else:\\n                raise ValueError(\\n                    'invalid entry in choicelist: should be boolean ndarray')\\n    if deprecated_ints:\\n        msg = \"select condlists containing integer ndarrays is deprecated \" \\\\n            \"and will be removed in the future. Use `.astype(bool)` to \" \\\\n            \"convert to bools.\"\\n        warnings.warn(msg, DeprecationWarning)\\n    if choicelist[0].ndim == 0:\\n        result_shape = condlist[0].shape\\n    else:\\n        result_shape = np.broadcast_arrays(condlist[0], choicelist[0])[0].shape\\n    result = np.full(result_shape, choicelist[-1], dtype)\\n    choicelist = choicelist[-2::-1]\\n    condlist = condlist[::-1]\\n    for choice, cond in zip(choicelist, condlist):\\n        np.copyto(result, choice, where=cond)\\n    return result"
  },
  {
    "code": "def __init__(self, query, database, output_location, aws_conn_id='aws_default', client_request_token=None,\\n                 query_execution_context=None, result_configuration=None, sleep_time=30, *args, **kwargs):\\n        super(AWSAthenaOperator, self).__init__(*args, **kwargs)\\n        self.query = query\\n        self.database = database\\n        self.output_location = output_location\\n        self.aws_conn_id = aws_conn_id\\n        self.client_request_token = client_request_token or str(uuid4())\\n        self.query_execution_context = query_execution_context or {}\\n        self.result_configuration = result_configuration or {}\\n        self.sleep_time = sleep_time\\n        self.query_execution_id = None\\n        self.hook = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4093] AWSAthenaOperator-Throw exception if job failed/cancelled/reach max retries (#4919)",
    "fixed_code": "def __init__(self, query, database, output_location, aws_conn_id='aws_default', client_request_token=None,\\n                 query_execution_context=None, result_configuration=None, sleep_time=30, max_tries=None,\\n                 *args, **kwargs):\\n        super(AWSAthenaOperator, self).__init__(*args, **kwargs)\\n        self.query = query\\n        self.database = database\\n        self.output_location = output_location\\n        self.aws_conn_id = aws_conn_id\\n        self.client_request_token = client_request_token or str(uuid4())\\n        self.query_execution_context = query_execution_context or {}\\n        self.result_configuration = result_configuration or {}\\n        self.sleep_time = sleep_time\\n        self.max_tries = max_tries\\n        self.query_execution_id = None\\n        self.hook = None"
  },
  {
    "code": "def __call__(self, request, *args, **kwargs):\\n        if 'extra_context' in kwargs:\\n            self.extra_context.update(kwargs['extra_context'])\\n        current_step = self.determine_step(request, *args, **kwargs)\\n        self.parse_params(request, *args, **kwargs)\\n        if current_step >= self.num_steps():\\n            raise Http404('Step %s does not exist' % current_step)\\n        for i in range(current_step):\\n            form = self.get_form(i, request.POST)\\n            if request.POST.get(\"hash_%d\" % i, '') != self.security_hash(request, form):\\n                return self.render_hash_failure(request, i)\\n            self.process_step(request, form, i)\\n        if request.method == 'POST':\\n            form = self.get_form(current_step, request.POST)\\n        else:\\n            form = self.get_form(current_step)\\n        if form.is_valid():\\n            self.process_step(request, form, current_step)\\n            next_step = current_step + 1\\n            num = self.num_steps()\\n            if next_step == num:\\n                final_form_list = [self.get_form(i, request.POST) for i in range(num)]\\n                for i, f in enumerate(final_form_list):\\n                    if not f.is_valid():\\n                        return self.render_revalidation_failure(request, i, f)\\n                return self.done(request, final_form_list)\\n            else:\\n                form = self.get_form(next_step)\\n                current_step = next_step\\n        return self.render(form, request, current_step)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #6893: `FormWizard` now properly updates its `step` value. Thanks, siddhi and wamberg.",
    "fixed_code": "def __call__(self, request, *args, **kwargs):\\n        if 'extra_context' in kwargs:\\n            self.extra_context.update(kwargs['extra_context'])\\n        current_step = self.determine_step(request, *args, **kwargs)\\n        self.parse_params(request, *args, **kwargs)\\n        if current_step >= self.num_steps():\\n            raise Http404('Step %s does not exist' % current_step)\\n        for i in range(current_step):\\n            form = self.get_form(i, request.POST)\\n            if request.POST.get(\"hash_%d\" % i, '') != self.security_hash(request, form):\\n                return self.render_hash_failure(request, i)\\n            self.process_step(request, form, i)\\n        if request.method == 'POST':\\n            form = self.get_form(current_step, request.POST)\\n        else:\\n            form = self.get_form(current_step)\\n        if form.is_valid():\\n            self.process_step(request, form, current_step)\\n            next_step = current_step + 1\\n            num = self.num_steps()\\n            if next_step == num:\\n                final_form_list = [self.get_form(i, request.POST) for i in range(num)]\\n                for i, f in enumerate(final_form_list):\\n                    if not f.is_valid():\\n                        return self.render_revalidation_failure(request, i, f)\\n                return self.done(request, final_form_list)\\n            else:\\n                form = self.get_form(next_step)\\n                self.step = current_step = next_step\\n        return self.render(form, request, current_step)"
  },
  {
    "code": "def _all_indexes_same(indexes):\\n    first = indexes[0]\\n    for index in indexes[1:]:\\n        if not first.equals(index):\\n            return False\\n    return True\\nclass WidePanelGroupBy(GroupBy):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _all_indexes_same(indexes):\\n    first = indexes[0]\\n    for index in indexes[1:]:\\n        if not first.equals(index):\\n            return False\\n    return True\\nclass WidePanelGroupBy(GroupBy):"
  },
  {
    "code": "def fetch_20newsgroups_tfidf(subset=\"train\", data_home=None):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fetch_20newsgroups_tfidf(subset=\"train\", data_home=None):"
  },
  {
    "code": "def find_hook(hook_name, hooks_dir='hooks'):\\n\\tlogger.debug('hooks_dir is %s', os.path.abspath(hooks_dir))\\n\\tif not os.path.isdir(hooks_dir):\\n\\t\\tlogger.debug('No hooks/dir in template_dir')\\n\\t\\treturn None\\n\\tfor hook_file in os.listdir(hooks_dir):\\n\\t\\tif valid_hook(hook_file, hook_name):\\n\\t\\t\\treturn os.path.abspath(os.path.join(hooks_dir, hook_file))\\n\\treturn None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Changed: Generated projects can use multiple type hooks at same time. (sh + py) (#974)",
    "fixed_code": "def find_hook(hook_name, hooks_dir='hooks'):\\n\\tlogger.debug('hooks_dir is %s', os.path.abspath(hooks_dir))\\n\\tif not os.path.isdir(hooks_dir):\\n\\t\\tlogger.debug('No hooks/dir in template_dir')\\n\\t\\treturn None\\n\\tscripts = []\\n\\tfor hook_file in os.listdir(hooks_dir):\\n\\t\\tif valid_hook(hook_file, hook_name):\\n\\t\\t\\tscripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\\n\\tif len(scripts) == 0:\\n\\t\\treturn None\\n\\treturn scripts"
  },
  {
    "code": "def to_json(\\n    path_or_buf: FilePath | WriteBuffer[str] | WriteBuffer[bytes],\\n    obj: NDFrame,\\n    orient: str | None = ...,\\n    date_format: str = ...,\\n    double_precision: int = ...,\\n    force_ascii: bool = ...,\\n    date_unit: str = ...,\\n    default_handler: Callable[[Any], JSONSerializable] | None = ...,\\n    lines: bool = ...,\\n    compression: CompressionOptions = ...,\\n    index: bool = ...,\\n    indent: int = ...,\\n    storage_options: StorageOptions = ...,\\n) -> None:\\n    ...",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Added append functionality for DataFrame.to_json (#48540)",
    "fixed_code": "def to_json(\\n    path_or_buf: FilePath | WriteBuffer[str] | WriteBuffer[bytes],\\n    obj: NDFrame,\\n    orient: str | None = ...,\\n    date_format: str = ...,\\n    double_precision: int = ...,\\n    force_ascii: bool = ...,\\n    date_unit: str = ...,\\n    default_handler: Callable[[Any], JSONSerializable] | None = ...,\\n    lines: bool = ...,\\n    compression: CompressionOptions = ...,\\n    index: bool = ...,\\n    indent: int = ...,\\n    storage_options: StorageOptions = ...,\\n    mode: Literal[\"a\", \"w\"] = ...,\\n) -> None:\\n    ..."
  },
  {
    "code": "def __init__(self, name, obj):\\n        warnings.warn(self._ix_deprecation_warning,\\n                      FutureWarning, stacklevel=2)\\n        super().__init__(name, obj)\\n    @Appender(_NDFrameIndexer._validate_key.__doc__)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, name, obj):\\n        warnings.warn(self._ix_deprecation_warning,\\n                      FutureWarning, stacklevel=2)\\n        super().__init__(name, obj)\\n    @Appender(_NDFrameIndexer._validate_key.__doc__)"
  },
  {
    "code": "def to_lines(stdout):\\n    for item in stdout:\\n        if isinstance(item, string_types):\\n            item = str(item).split('\\n')\\n        yield item",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix issue #60237 when non-ascii is returned from the WLC (#60243)",
    "fixed_code": "def to_lines(stdout):\\n    for item in stdout:\\n        if isinstance(item, string_types):\\n            item = to_text(item, errors='surrogate_then_replace').split('\\n')\\n        yield item"
  },
  {
    "code": "def can_hold_element(arr: ArrayLike, element: Any) -> bool:\\n    dtype = arr.dtype\\n    if not isinstance(dtype, np.dtype) or dtype.kind in [\"m\", \"M\"]:\\n        if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):\\n            arr = cast(\\n                \"PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray\", arr\\n            )\\n            try:\\n                arr._validate_setitem_value(element)\\n                return True\\n            except (ValueError, TypeError):\\n                return False\\n        return True\\n    if dtype == object:  \\n        return True\\n    tipo = maybe_infer_dtype_type(element)\\n    if dtype.kind in [\"i\", \"u\"]:\\n        if tipo is not None:\\n            if tipo.kind not in [\"i\", \"u\"]:\\n                if is_float(element) and element.is_integer():\\n                    return True\\n                return False\\n            elif dtype.itemsize < tipo.itemsize:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return is_integer(element) or (is_float(element) and element.is_integer())\\n    elif dtype.kind == \"f\":\\n        if tipo is not None:\\n            if tipo.kind not in [\"f\", \"i\", \"u\"]:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return lib.is_integer(element) or lib.is_float(element)\\n    elif dtype.kind == \"c\":\\n        if tipo is not None:\\n            return tipo.kind in [\"c\", \"f\", \"i\", \"u\"]\\n        return (\\n            lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element)\\n        )\\n    elif dtype.kind == \"b\":\\n        if tipo is not None:\\n            return tipo.kind == \"b\"\\n        return lib.is_bool(element)\\n    elif dtype.kind == \"S\":\\n        if tipo is not None:\\n            return tipo.kind == \"S\" and tipo.itemsize <= dtype.itemsize\\n        return isinstance(element, bytes) and len(element) <= dtype.itemsize\\n    raise NotImplementedError(dtype)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def can_hold_element(arr: ArrayLike, element: Any) -> bool:\\n    dtype = arr.dtype\\n    if not isinstance(dtype, np.dtype) or dtype.kind in [\"m\", \"M\"]:\\n        if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):\\n            arr = cast(\\n                \"PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray\", arr\\n            )\\n            try:\\n                arr._validate_setitem_value(element)\\n                return True\\n            except (ValueError, TypeError):\\n                return False\\n        return True\\n    if dtype == object:  \\n        return True\\n    tipo = maybe_infer_dtype_type(element)\\n    if dtype.kind in [\"i\", \"u\"]:\\n        if tipo is not None:\\n            if tipo.kind not in [\"i\", \"u\"]:\\n                if is_float(element) and element.is_integer():\\n                    return True\\n                return False\\n            elif dtype.itemsize < tipo.itemsize:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return is_integer(element) or (is_float(element) and element.is_integer())\\n    elif dtype.kind == \"f\":\\n        if tipo is not None:\\n            if tipo.kind not in [\"f\", \"i\", \"u\"]:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return lib.is_integer(element) or lib.is_float(element)\\n    elif dtype.kind == \"c\":\\n        if tipo is not None:\\n            return tipo.kind in [\"c\", \"f\", \"i\", \"u\"]\\n        return (\\n            lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element)\\n        )\\n    elif dtype.kind == \"b\":\\n        if tipo is not None:\\n            return tipo.kind == \"b\"\\n        return lib.is_bool(element)\\n    elif dtype.kind == \"S\":\\n        if tipo is not None:\\n            return tipo.kind == \"S\" and tipo.itemsize <= dtype.itemsize\\n        return isinstance(element, bytes) and len(element) <= dtype.itemsize\\n    raise NotImplementedError(dtype)"
  },
  {
    "code": "def nested_to_record(ds, prefix=\"\", level=0):\\n\\tsingleton = False\\n\\tif isinstance(ds, dict):\\n\\t\\tds = [ds]\\n\\t\\tsingleton = True\\n\\tnew_ds = []\\n\\tfor d in ds:\\n\\t\\tnew_d = copy.deepcopy(d)\\n\\t\\tfor k, v in d.items():\\n\\t\\t\\tif level == 0:\\n\\t\\t\\t\\tnewkey = str(k)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tnewkey = prefix + '.' + str(k)\\n\\t\\t\\tif not isinstance(v, dict):\\n\\t\\t\\t\\tif level != 0:  \\n\\t\\t\\t\\t\\tv = new_d.pop(k)\\n\\t\\t\\t\\t\\tnew_d[newkey] = v\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telse:\\n\\t\\t\\t\\tv = new_d.pop(k)\\n\\t\\t\\t\\tnew_d.update(nested_to_record(v, newkey, level + 1))\\n\\t\\tnew_ds.append(new_d)\\n\\tif singleton:\\n\\t\\treturn new_ds[0]\\n\\treturn new_ds",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix #13213 json_normalize() and non-ascii characters in keys\\n\\ncloses #13213\\n\\nAuthor: Felix Marczinowski <felix.marczinowski@blue-yonder.com>\\n\\nCloses #13214 from fmarczin/13213-unicode-json_normalize and squashes the following commits:\\n\\n22e01b2 [Felix Marczinowski] fix linter warnings\\n44745ca [Felix Marczinowski] fix tests for py3\\n25fd0f8 [Felix Marczinowski] move test, fix py3 issue\\n7a38110 [Felix Marczinowski] add whatsnew note\\ndd7302c [Felix Marczinowski] remove encoding signature from test\\n4dcd2c5 [Felix Marczinowski] fix for #13213\\nb9751e9 [Felix Marczinowski] add test for #13213",
    "fixed_code": "def nested_to_record(ds, prefix=\"\", level=0):\\n\\tsingleton = False\\n\\tif isinstance(ds, dict):\\n\\t\\tds = [ds]\\n\\t\\tsingleton = True\\n\\tnew_ds = []\\n\\tfor d in ds:\\n\\t\\tnew_d = copy.deepcopy(d)\\n\\t\\tfor k, v in d.items():\\n\\t\\t\\tif not isinstance(k, compat.string_types):\\n\\t\\t\\t\\tk = str(k)\\n\\t\\t\\tif level == 0:\\n\\t\\t\\t\\tnewkey = k\\n\\t\\t\\telse:\\n\\t\\t\\t\\tnewkey = prefix + '.' + k\\n\\t\\t\\tif not isinstance(v, dict):\\n\\t\\t\\t\\tif level != 0:  \\n\\t\\t\\t\\t\\tv = new_d.pop(k)\\n\\t\\t\\t\\t\\tnew_d[newkey] = v\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telse:\\n\\t\\t\\t\\tv = new_d.pop(k)\\n\\t\\t\\t\\tnew_d.update(nested_to_record(v, newkey, level + 1))\\n\\t\\tnew_ds.append(new_d)\\n\\tif singleton:\\n\\t\\treturn new_ds[0]\\n\\treturn new_ds"
  },
  {
    "code": "def _convert_index(index):\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif isinstance(values[0], datetime):\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                            v.microsecond / 1E6) for v in values],\\n                            dtype=np.float64)\\n        return converted, 'datetime', _tables().Time64Col()\\n    elif isinstance(values[0], date):\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                            dtype=np.int32)\\n        return converted, 'date', _tables().Time32Col()\\n    elif isinstance(values[0], basestring):\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return converted, 'string', _tables().StringCol(itemsize)\\n    elif com.is_integer(values[0]):\\n        atom = _tables().Int64Col()\\n        return np.asarray(values, dtype=np.int64), 'integer', atom\\n    elif com.is_float(values[0]):\\n        atom = _tables().Float64Col()\\n        return np.asarray(values, dtype=np.float64), 'float', atom\\n    else: \\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: writing heterogenous Index to HDFStore. GH #1078",
    "fixed_code": "def _convert_index(index):\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                            v.microsecond / 1E6) for v in values],\\n                            dtype=np.float64)\\n        return converted, 'datetime', _tables().Time64Col()\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                            dtype=np.int32)\\n        return converted, 'date', _tables().Time32Col()\\n    elif inferred_type =='string':\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return converted, 'string', _tables().StringCol(itemsize)\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return np.asarray(values, dtype=np.int64), 'integer', atom\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return np.asarray(values, dtype=np.float64), 'float', atom\\n    else: \\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom"
  },
  {
    "code": "def _run_command(self, cur, sql_statement, parameters):\\n        if self.log_sql:\\n            self.log.info(\"Running statement: %s, parameters: %s\", sql_statement, parameters)\\n        if parameters:\\n            cur.execute(sql_statement, parameters)\\n        else:\\n            cur.execute(sql_statement)\\n        if cur.rowcount >= 0:\\n            self.log.info(\"Rows affected: %s\", cur.rowcount)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _run_command(self, cur, sql_statement, parameters):\\n        if self.log_sql:\\n            self.log.info(\"Running statement: %s, parameters: %s\", sql_statement, parameters)\\n        if parameters:\\n            cur.execute(sql_statement, parameters)\\n        else:\\n            cur.execute(sql_statement)\\n        if cur.rowcount >= 0:\\n            self.log.info(\"Rows affected: %s\", cur.rowcount)"
  },
  {
    "code": "def isroutine(object):\\n    return (isbuiltin(object)\\n            or isfunction(object)\\n            or ismethod(object)\\n            or ismethoddescriptor(object))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-29418: Implement inspect.ismethodwrapper and fix inspect.isroutine for cases where methodwrapper is given (GH-19261)\\n\\nAutomerge-Triggered-By: GH:isidentical",
    "fixed_code": "def isroutine(object):\\n    return (isbuiltin(object)\\n            or isfunction(object)\\n            or ismethod(object)\\n            or ismethoddescriptor(object)\\n            or ismethodwrapper(object))"
  },
  {
    "code": "def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\\n\\t\\tinline_admin_formsets = []\\n\\t\\tfor inline, formset in zip(inline_instances, formsets):\\n\\t\\t\\tfieldsets = list(inline.get_fieldsets(request, obj))\\n\\t\\t\\treadonly = list(inline.get_readonly_fields(request, obj))\\n\\t\\t\\thas_add_permission = inline._has_add_permission(request, obj)\\n\\t\\t\\thas_change_permission = inline.has_change_permission(request, obj)\\n\\t\\t\\thas_delete_permission = inline.has_delete_permission(request, obj)\\n\\t\\t\\thas_view_permission = inline.has_view_permission(request, obj)\\n\\t\\t\\tprepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n\\t\\t\\tinline_admin_formset = helpers.InlineAdminFormSet(\\n\\t\\t\\t\\tinline, formset, fieldsets, prepopulated, readonly, model_admin=self,\\n\\t\\t\\t\\thas_add_permission=has_add_permission, has_change_permission=has_change_permission,\\n\\t\\t\\t\\thas_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\\n\\t\\t\\t)\\n\\t\\t\\tinline_admin_formsets.append(inline_admin_formset)\\n\\t\\treturn inline_admin_formsets",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed CVE-2019-19118 -- Required edit permissions on parent model for editable inlines in admin.\\n\\nThank you to Shen Ying for reporting this issue.",
    "fixed_code": "def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\\n\\t\\tcan_edit_parent = self.has_change_permission(request, obj) if obj else self.has_add_permission(request)\\n\\t\\tinline_admin_formsets = []\\n\\t\\tfor inline, formset in zip(inline_instances, formsets):\\n\\t\\t\\tfieldsets = list(inline.get_fieldsets(request, obj))\\n\\t\\t\\treadonly = list(inline.get_readonly_fields(request, obj))\\n\\t\\t\\tif can_edit_parent:\\n\\t\\t\\t\\thas_add_permission = inline._has_add_permission(request, obj)\\n\\t\\t\\t\\thas_change_permission = inline.has_change_permission(request, obj)\\n\\t\\t\\t\\thas_delete_permission = inline.has_delete_permission(request, obj)\\n\\t\\t\\telse:\\n\\t\\t\\t\\thas_add_permission = has_change_permission = has_delete_permission = False\\n\\t\\t\\t\\tformset.extra = formset.max_num = 0\\n\\t\\t\\thas_view_permission = inline.has_view_permission(request, obj)\\n\\t\\t\\tprepopulated = dict(inline.get_prepopulated_fields(request, obj))\\n\\t\\t\\tinline_admin_formset = helpers.InlineAdminFormSet(\\n\\t\\t\\t\\tinline, formset, fieldsets, prepopulated, readonly, model_admin=self,\\n\\t\\t\\t\\thas_add_permission=has_add_permission, has_change_permission=has_change_permission,\\n\\t\\t\\t\\thas_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\\n\\t\\t\\t)\\n\\t\\t\\tinline_admin_formsets.append(inline_admin_formset)\\n\\t\\treturn inline_admin_formsets"
  },
  {
    "code": "def test():\\n    ''\\n    if len(sys.argv) < 2:\\n        print(test.__doc__)\\n        sys.exit(0)\\n    import netrc\\n    debugging = 0\\n    rcfile = None\\n    while sys.argv[1] == '-d':\\n        debugging = debugging+1\\n        del sys.argv[1]\\n    if sys.argv[1][:2] == '-r':\\n        rcfile = sys.argv[1][2:]\\n        del sys.argv[1]\\n    host = sys.argv[1]\\n    ftp = FTP(host)\\n    ftp.set_debuglevel(debugging)\\n    userid = passwd = acct = ''\\n    try:\\n        netrcobj = netrc.netrc(rcfile)\\n    except OSError:\\n        if rcfile is not None:\\n            sys.stderr.write(\"Could not open account file\"\\n                             \" -- using anonymous login.\")\\n    else:\\n        try:\\n            userid, acct, passwd = netrcobj.authenticators(host)\\n        except KeyError:\\n            sys.stderr.write(\\n                    \"No account -- using anonymous login.\")\\n    ftp.login(userid, passwd, acct)\\n    for file in sys.argv[2:]:\\n        if file[:2] == '-l':\\n            ftp.dir(file[2:])\\n        elif file[:2] == '-d':\\n            cmd = 'CWD'\\n            if file[2:]: cmd = cmd + ' ' + file[2:]\\n            resp = ftp.sendcmd(cmd)\\n        elif file == '-p':\\n            ftp.set_pasv(not ftp.passiveserver)\\n        else:\\n            ftp.retrbinary('RETR ' + file, \\\\n                           sys.stdout.write, 1024)\\n    ftp.quit()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test():\\n    ''\\n    if len(sys.argv) < 2:\\n        print(test.__doc__)\\n        sys.exit(0)\\n    import netrc\\n    debugging = 0\\n    rcfile = None\\n    while sys.argv[1] == '-d':\\n        debugging = debugging+1\\n        del sys.argv[1]\\n    if sys.argv[1][:2] == '-r':\\n        rcfile = sys.argv[1][2:]\\n        del sys.argv[1]\\n    host = sys.argv[1]\\n    ftp = FTP(host)\\n    ftp.set_debuglevel(debugging)\\n    userid = passwd = acct = ''\\n    try:\\n        netrcobj = netrc.netrc(rcfile)\\n    except OSError:\\n        if rcfile is not None:\\n            sys.stderr.write(\"Could not open account file\"\\n                             \" -- using anonymous login.\")\\n    else:\\n        try:\\n            userid, acct, passwd = netrcobj.authenticators(host)\\n        except KeyError:\\n            sys.stderr.write(\\n                    \"No account -- using anonymous login.\")\\n    ftp.login(userid, passwd, acct)\\n    for file in sys.argv[2:]:\\n        if file[:2] == '-l':\\n            ftp.dir(file[2:])\\n        elif file[:2] == '-d':\\n            cmd = 'CWD'\\n            if file[2:]: cmd = cmd + ' ' + file[2:]\\n            resp = ftp.sendcmd(cmd)\\n        elif file == '-p':\\n            ftp.set_pasv(not ftp.passiveserver)\\n        else:\\n            ftp.retrbinary('RETR ' + file, \\\\n                           sys.stdout.write, 1024)\\n    ftp.quit()"
  },
  {
    "code": "def _build_vectors_and_vocab(self, raw_documents):\\n        term_counts_per_doc = []\\n        term_counts = {}\\n        document_counts = {}\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_dict = {} \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n                term_counts[term] = term_counts.get(term, 0) + 1\\n            if max_df is not None:\\n                for term in term_count_dict.iterkeys():\\n                    document_counts[term] = document_counts.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        n_doc = len(term_counts_per_doc)\\n        stop_words = set()\\n        if max_df is not None:\\n            max_document_count = max_df * n_doc\\n            for t, dc in sorted(document_counts.iteritems(),\\n                                key=itemgetter(1), reverse=True):\\n                if dc < max_document_count:\\n                    break\\n                stop_words.add(t)\\n        if max_features is not None:\\n            terms = set()\\n            for t, tc in sorted(term_counts.iteritems(),\\n                                key=itemgetter(1), reverse=True):\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        else:\\n            terms = set(term_counts.iteritems())\\n            terms -= stop_words\\n        vocabulary = dict(((t, i) for i, t in enumerate(terms))) \\n        matrix = self._init_matrix((n_doc, len(vocabulary)))\\n        for i, term_count_dict in enumerate(term_counts_per_doc):\\n            for term, count in term_count_dict.iteritems():\\n                idx = vocabulary.get(term)\\n                if idx is not None:\\n                    matrix[i, idx] = count\\n        return matrix, vocabulary",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "small fixes + updated the tests",
    "fixed_code": "def _build_vectors_and_vocab(self, raw_documents):\\n        term_counts_per_doc = []\\n        term_counts = {}\\n        document_counts = {}\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_dict = {} \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n                term_counts[term] = term_counts.get(term, 0) + 1\\n            if max_df is not None:\\n                for term in term_count_dict.iterkeys():\\n                    document_counts[term] = document_counts.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        n_doc = len(term_counts_per_doc)\\n        stop_words = set()\\n        if max_df is not None:\\n            max_document_count = max_df * n_doc\\n            for t, dc in sorted(document_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if dc < max_document_count:\\n                    break\\n                stop_words.add(t)\\n        if max_features is not None:\\n            terms = set()\\n            for t, tc in sorted(term_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        else:\\n            terms = set(term_counts.keys())\\n            terms -= stop_words\\n        vocabulary = dict(((t, i) for i, t in enumerate(terms))) \\n        matrix = self._init_matrix((n_doc, len(vocabulary)))\\n        for i, term_count_dict in enumerate(term_counts_per_doc):\\n            for term, count in term_count_dict.iteritems():\\n                idx = vocabulary.get(term)\\n                if idx is not None:\\n                    matrix[i, idx] = count\\n        return matrix, vocabulary"
  },
  {
    "code": "def run(self, terms, variables, **kwargs):\\n\\t\\tret = []\\n\\t\\tfor term in terms:\\n\\t\\t\\tvar = term.split()[0]\\n\\t\\t\\tret.append(py3compat.environ.get(var, ''))\\n\\t\\treturn ret",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run(self, terms, variables, **kwargs):\\n\\t\\tret = []\\n\\t\\tfor term in terms:\\n\\t\\t\\tvar = term.split()[0]\\n\\t\\t\\tret.append(py3compat.environ.get(var, ''))\\n\\t\\treturn ret"
  },
  {
    "code": "def boxplot(data, column=None, by=None, ax=None, fontsize=None,\\n            rot=0, grid=True, figsize=None):\\n    from pandas import Series, DataFrame\\n    if isinstance(data, Series):\\n        data = DataFrame({'x' : data})\\n        column = 'x'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Pass on keywords for DataFrame.boxplot to matplotlib boxplot, close #1493.",
    "fixed_code": "def boxplot(data, column=None, by=None, ax=None, fontsize=None,\\n            rot=0, grid=True, figsize=None, **kwds):\\n    from pandas import Series, DataFrame\\n    if isinstance(data, Series):\\n        data = DataFrame({'x' : data})\\n        column = 'x'"
  },
  {
    "code": "def copy_decimal(self, a):\\n        return Decimal(a)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #7633: Context method in the decimal module (with the exception of the 'canonical' and 'is_canonical' methods) now consistently accept integer arguments wherever a Decimal instance is accepted.  Thanks Juan Jos\u00e9 Conti for the patch.",
    "fixed_code": "def copy_decimal(self, a):\\n        a = _convert_other(a, raiseit=True)\\n        return Decimal(a)"
  },
  {
    "code": "def encode_7or8bit(msg):\\n    orig = msg.get_payload()\\n    if orig is None:\\n        msg['Content-Transfer-Encoding'] = '7bit'\\n        return\\n    try:\\n        if isinstance(orig, str):\\n            orig.encode('ascii')\\n        else:\\n            orig.decode('ascii')\\n    except UnicodeError:\\n        charset = msg.get_charset()\\n        output_cset = charset and charset.output_charset\\n        if output_cset and output_cset.lower().startswith('iso-2022-'):\\n            msg['Content-Transfer-Encoding'] = '7bit'\\n        else:\\n            msg['Content-Transfer-Encoding'] = '8bit'\\n    else:\\n        msg['Content-Transfer-Encoding'] = '7bit'\\n    if not isinstance(orig, str):\\n        msg.set_payload(orig.decode('ascii', 'surrogateescape'))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#18324: set_payload now correctly handles binary input.\\n\\nThis also backs out the previous fixes for for #14360, #1717, and #16564.\\nThose bugs were actually caused by the fact that set_payload didn't decode to\\nstr, thus rendering the model inconsistent.  This fix does mean the data\\nprocessed by the encoder functions goes through an extra encode/decode cycle,\\nbut it means the model is always consistent.  Future API updates will provide\\na better way to encode payloads, which will bypass this minor de-optimization.\\n\\nTests by Vajrasky Kok.",
    "fixed_code": "def encode_7or8bit(msg):\\n    orig = msg.get_payload(decode=True)\\n    if orig is None:\\n        msg['Content-Transfer-Encoding'] = '7bit'\\n        return\\n    try:\\n        if isinstance(orig, str):\\n            orig.encode('ascii')\\n        else:\\n            orig.decode('ascii')\\n    except UnicodeError:\\n        charset = msg.get_charset()\\n        output_cset = charset and charset.output_charset\\n        if output_cset and output_cset.lower().startswith('iso-2022-'):\\n            msg['Content-Transfer-Encoding'] = '7bit'\\n        else:\\n            msg['Content-Transfer-Encoding'] = '8bit'\\n    else:\\n        msg['Content-Transfer-Encoding'] = '7bit'"
  },
  {
    "code": "def __unicode__(self):\\n        temp = 'Categorical: %s\\n%s\\n%s'\\n        values = com.pprint_thing(np.asarray(self))\\n        levheader = 'Levels (%d): ' % len(self.levels)\\n        levstring = np.array_repr(self.levels,\\n                                  max_line_width=60)\\n        indent = ' ' * (levstring.find('[') + len(levheader) + 1)\\n        lines = levstring.split('\\n')\\n        levstring = '\\n'.join([lines[0]] +\\n                              [indent + x.lstrip() for x in lines[1:]])\\n        name = '' if self.name is None else self.name\\n        return temp % (name, values, levheader + levstring)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Make categorical repr nicer.\\n\\nENH: Support printing empty categorical.\\n\\nTST: Add tests for categorical printing.\\n\\nREF: Remove unnecessary unicode calls.\\n\\nTST: Hack so tests pass with numpy < 1.7.x\\n\\nCLN: fix DataFrame import in core/categorical.py",
    "fixed_code": "def __unicode__(self):\\n        width, height = get_terminal_size()\\n        max_rows = (height if get_option(\"display.max_rows\") == 0\\n                    else get_option(\"display.max_rows\"))\\n        if len(self.labels) > (max_rows or 1000):\\n            result = self._tidy_repr(min(30, max_rows) - 4)\\n        elif len(self.labels) > 0:\\n            result = self._get_repr(length=len(self) > 50,\\n                                    name=True)\\n        else:\\n            result = u'Categorical([], %s' % self._get_repr(name=True,\\n                                                            length=False,\\n                                                            footer=True,\\n                                                            )\\n        return result"
  },
  {
    "code": "def pool_link(attr):\\n        pool_id = attr.get('pool')\\n        if pool_id is not None:\\n            url = '/taskinstance/list/?_flt_3_pool=' + str(pool_id)\\n            pool_id = escape(pool_id)\\n            return Markup(\"<a href='{url}'>{pool_id}</a>\".format(**locals()))\\n        else:\\n            return Markup('Invalid",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pool_link(attr):\\n        pool_id = attr.get('pool')\\n        if pool_id is not None:\\n            url = '/taskinstance/list/?_flt_3_pool=' + str(pool_id)\\n            pool_id = escape(pool_id)\\n            return Markup(\"<a href='{url}'>{pool_id}</a>\".format(**locals()))\\n        else:\\n            return Markup('Invalid"
  },
  {
    "code": "def diff(arr, n, axis=0):\\n    n = int(n)\\n    dtype = arr.dtype\\n    if issubclass(dtype.type, np.integer):\\n        dtype = np.float64\\n    elif issubclass(dtype.type, np.bool_):\\n        dtype = np.object_\\n    out_arr = np.empty(arr.shape, dtype=dtype)\\n    na_indexer = [slice(None)] * arr.ndim\\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n    out_arr[tuple(na_indexer)] = np.nan\\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\\n        f = _diff_special[arr.dtype.name]\\n        f(arr, out_arr, n, axis)\\n    else:\\n        res_indexer = [slice(None)] * arr.ndim\\n        res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n        res_indexer = tuple(res_indexer)\\n        lag_indexer = [slice(None)] * arr.ndim\\n        lag_indexer[axis] = slice(None, -n) if n >= 0 else slice(-n, None)\\n        lag_indexer = tuple(lag_indexer)\\n        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n    return out_arr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Series diff(0) fails #3257",
    "fixed_code": "def diff(arr, n, axis=0):\\n    n = int(n)\\n    dtype = arr.dtype\\n    if issubclass(dtype.type, np.integer):\\n        dtype = np.float64\\n    elif issubclass(dtype.type, np.bool_):\\n        dtype = np.object_\\n    out_arr = np.empty(arr.shape, dtype=dtype)\\n    na_indexer = [slice(None)] * arr.ndim\\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n    out_arr[tuple(na_indexer)] = np.nan\\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\\n        f = _diff_special[arr.dtype.name]\\n        f(arr, out_arr, n, axis)\\n    else:\\n        res_indexer = [slice(None)] * arr.ndim\\n        res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n        res_indexer = tuple(res_indexer)\\n        lag_indexer = [slice(None)] * arr.ndim\\n        lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n        lag_indexer = tuple(lag_indexer)\\n        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n    return out_arr"
  },
  {
    "code": "def create(self, req, body):\\n\\t\\tif not self.is_valid_body(body, 'server'):\\n\\t\\t\\traise exc.HTTPUnprocessableEntity()\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tserver_dict = body['server']\\n\\t\\tpassword = self._get_server_admin_password(server_dict)\\n\\t\\tif 'name' not in server_dict:\\n\\t\\t\\tmsg = _(\"Server name is not defined\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\tname = server_dict['name']\\n\\t\\tself._validate_server_name(name)\\n\\t\\tname = name.strip()\\n\\t\\timage_uuid = self._image_from_req_data(body)\\n\\t\\tpersonality = server_dict.get('personality')\\n\\t\\tconfig_drive = None\\n\\t\\tif self.ext_mgr.is_loaded('os-config-drive'):\\n\\t\\t\\tconfig_drive = server_dict.get('config_drive')\\n\\t\\tinjected_files = []\\n\\t\\tif personality:\\n\\t\\t\\tinjected_files = self._get_injected_files(personality)\\n\\t\\tsg_names = []\\n\\t\\tif self.ext_mgr.is_loaded('os-security-groups'):\\n\\t\\t\\tsecurity_groups = server_dict.get('security_groups')\\n\\t\\t\\tif security_groups is not None:\\n\\t\\t\\t\\tsg_names = [sg['name'] for sg in security_groups\\n\\t\\t\\t\\t\\t\\t\\tif sg.get('name')]\\n\\t\\tif not sg_names:\\n\\t\\t\\tsg_names.append('default')\\n\\t\\tsg_names = list(set(sg_names))\\n\\t\\trequested_networks = None\\n\\t\\tif (self.ext_mgr.is_loaded('os-networks')\\n\\t\\t\\t\\tor self._is_neutron_v2()):\\n\\t\\t\\trequested_networks = server_dict.get('networks')\\n\\t\\tif requested_networks is not None:\\n\\t\\t\\tif not isinstance(requested_networks, list):\\n\\t\\t\\t\\texpl = _('Bad networks format')\\n\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=expl)\\n\\t\\t\\trequested_networks = self._get_requested_networks(\\n\\t\\t\\t\\trequested_networks)\\n\\t\\t(access_ip_v4, ) = server_dict.get('accessIPv4'),\\n\\t\\tif access_ip_v4 is not None:\\n\\t\\t\\tself._validate_access_ipv4(access_ip_v4)\\n\\t\\t(access_ip_v6, ) = server_dict.get('accessIPv6'),\\n\\t\\tif access_ip_v6 is not None:\\n\\t\\t\\tself._validate_access_ipv6(access_ip_v6)\\n\\t\\ttry:\\n\\t\\t\\tflavor_id = self._flavor_id_from_req_data(body)\\n\\t\\texcept ValueError as error:\\n\\t\\t\\tmsg = _(\"Invalid flavorRef provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\tkey_name = None\\n\\t\\tif self.ext_mgr.is_loaded('os-keypairs'):\\n\\t\\t\\tkey_name = server_dict.get('key_name')\\n\\t\\tuser_data = None\\n\\t\\tif self.ext_mgr.is_loaded('os-user-data'):\\n\\t\\t\\tuser_data = server_dict.get('user_data')\\n\\t\\tself._validate_user_data(user_data)\\n\\t\\tavailability_zone = None\\n\\t\\tif self.ext_mgr.is_loaded('os-availability-zone'):\\n\\t\\t\\tavailability_zone = server_dict.get('availability_zone')\\n\\t\\tblock_device_mapping = None\\n\\t\\tblock_device_mapping_v2 = None\\n\\t\\tlegacy_bdm = True\\n\\t\\tif self.ext_mgr.is_loaded('os-volumes'):\\n\\t\\t\\tblock_device_mapping = server_dict.get('block_device_mapping', [])\\n\\t\\t\\tfor bdm in block_device_mapping:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tblock_device.validate_device_name(bdm.get(\"device_name\"))\\n\\t\\t\\t\\t\\tblock_device.validate_and_default_volume_size(bdm)\\n\\t\\t\\t\\texcept exception.InvalidBDMFormat as e:\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=e.format_message())\\n\\t\\t\\t\\tif 'delete_on_termination' in bdm:\\n\\t\\t\\t\\t\\tbdm['delete_on_termination'] = strutils.bool_from_string(\\n\\t\\t\\t\\t\\t\\tbdm['delete_on_termination'])\\n\\t\\t\\tif self.ext_mgr.is_loaded('os-block-device-mapping-v2-boot'):\\n\\t\\t\\t\\tblock_device_mapping_v2 = server_dict.get(\\n\\t\\t\\t\\t\\t'block_device_mapping_v2', [])\\n\\t\\t\\t\\tif block_device_mapping and block_device_mapping_v2:\\n\\t\\t\\t\\t\\texpl = _('Using different block_device_mapping syntaxes '\\n\\t\\t\\t\\t\\t\\t\\t 'is not allowed in the same request.')\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=expl)\\n\\t\\t\\t\\tlegacy_bdm = not bool(block_device_mapping_v2)\\n\\t\\t\\t\\tif any('device_name' not in bdm\\n\\t\\t\\t\\t\\t   for bdm in block_device_mapping_v2):\\n\\t\\t\\t\\t\\texpl = _('Missing device_name in some block devices.')\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=expl)\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tblock_device_mapping_v2 = [\\n\\t\\t\\t\\t\\t\\tblock_device.BlockDeviceDict.from_api(bdm_dict)\\n\\t\\t\\t\\t\\t\\tfor bdm_dict in block_device_mapping_v2]\\n\\t\\t\\t\\texcept exception.InvalidBDMFormat as e:\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=e.format_message())\\n\\t\\tblock_device_mapping = (block_device_mapping or\\n\\t\\t\\t\\t\\t\\t\\t\\tblock_device_mapping_v2)\\n\\t\\tret_resv_id = False\\n\\t\\tmin_count = 1\\n\\t\\tmax_count = 1\\n\\t\\tif self.ext_mgr.is_loaded('os-multiple-create'):\\n\\t\\t\\tret_resv_id = server_dict.get('return_reservation_id', False)\\n\\t\\t\\tmin_count = server_dict.get('min_count', 1)\\n\\t\\t\\tmax_count = server_dict.get('max_count', min_count)\\n\\t\\ttry:\\n\\t\\t\\tmin_count = utils.validate_integer(\\n\\t\\t\\t\\tmin_count, \"min_count\", min_value=1)\\n\\t\\t\\tmax_count = utils.validate_integer(\\n\\t\\t\\t\\tmax_count, \"max_count\", min_value=1)\\n\\t\\texcept exception.InvalidInput as e:\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=e.format_message())\\n\\t\\tif min_count > max_count:\\n\\t\\t\\tmsg = _('min_count must be <= max_count')\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\tauto_disk_config = False\\n\\t\\tif self.ext_mgr.is_loaded('OS-DCF'):\\n\\t\\t\\tauto_disk_config = server_dict.get('auto_disk_config')\\n\\t\\tscheduler_hints = {}\\n\\t\\tif self.ext_mgr.is_loaded('OS-SCH-HNT'):\\n\\t\\t\\tscheduler_hints = server_dict.get('scheduler_hints', {})\\n\\t\\ttry:\\n\\t\\t\\t_get_inst_type = flavors.get_flavor_by_flavor_id\\n\\t\\t\\tinst_type = _get_inst_type(flavor_id, read_deleted=\"no\")\\n\\t\\t\\t(instances, resv_id) = self.compute_api.create(context,\\n\\t\\t\\t\\t\\t\\t\\tinst_type,\\n\\t\\t\\t\\t\\t\\t\\timage_uuid,\\n\\t\\t\\t\\t\\t\\t\\tdisplay_name=name,\\n\\t\\t\\t\\t\\t\\t\\tdisplay_description=name,\\n\\t\\t\\t\\t\\t\\t\\tkey_name=key_name,\\n\\t\\t\\t\\t\\t\\t\\tmetadata=server_dict.get('metadata', {}),\\n\\t\\t\\t\\t\\t\\t\\taccess_ip_v4=access_ip_v4,\\n\\t\\t\\t\\t\\t\\t\\taccess_ip_v6=access_ip_v6,\\n\\t\\t\\t\\t\\t\\t\\tinjected_files=injected_files,\\n\\t\\t\\t\\t\\t\\t\\tadmin_password=password,\\n\\t\\t\\t\\t\\t\\t\\tmin_count=min_count,\\n\\t\\t\\t\\t\\t\\t\\tmax_count=max_count,\\n\\t\\t\\t\\t\\t\\t\\trequested_networks=requested_networks,\\n\\t\\t\\t\\t\\t\\t\\tsecurity_group=sg_names,\\n\\t\\t\\t\\t\\t\\t\\tuser_data=user_data,\\n\\t\\t\\t\\t\\t\\t\\tavailability_zone=availability_zone,\\n\\t\\t\\t\\t\\t\\t\\tconfig_drive=config_drive,\\n\\t\\t\\t\\t\\t\\t\\tblock_device_mapping=block_device_mapping,\\n\\t\\t\\t\\t\\t\\t\\tauto_disk_config=auto_disk_config,\\n\\t\\t\\t\\t\\t\\t\\tscheduler_hints=scheduler_hints,\\n\\t\\t\\t\\t\\t\\t\\tlegacy_bdm=legacy_bdm)\\n\\t\\texcept exception.QuotaError as error:\\n\\t\\t\\traise exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=error.format_message(),\\n\\t\\t\\t\\theaders={'Retry-After': 0})\\n\\t\\texcept exception.InvalidMetadataSize as error:\\n\\t\\t\\traise exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=error.format_message())\\n\\t\\texcept exception.ImageNotFound as error:\\n\\t\\t\\tmsg = _(\"Can not find requested image\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept exception.FlavorNotFound as error:\\n\\t\\t\\tmsg = _(\"Invalid flavorRef provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept exception.KeypairNotFound as error:\\n\\t\\t\\tmsg = _(\"Invalid key_name provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept exception.ConfigDriveInvalidValue:\\n\\t\\t\\tmsg = _(\"Invalid config_drive provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept rpc_common.RemoteError as err:\\n\\t\\t\\tmsg = \"%(err_type)s: %(err_msg)s\" % {'err_type': err.exc_type,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t 'err_msg': err.value}\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept UnicodeDecodeError as error:\\n\\t\\t\\tmsg = \"UnicodeError: %s\" % unicode(error)\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept (exception.ImageNotActive,\\n\\t\\t\\t\\texception.InstanceTypeDiskTooSmall,\\n\\t\\t\\t\\texception.InstanceTypeMemoryTooSmall,\\n\\t\\t\\t\\texception.InstanceTypeNotFound,\\n\\t\\t\\t\\texception.InvalidMetadata,\\n\\t\\t\\t\\texception.InvalidRequest,\\n\\t\\t\\t\\texception.PortNotFound,\\n\\t\\t\\t\\texception.SecurityGroupNotFound) as error:\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=error.format_message())\\n\\t\\texcept exception.PortInUse as error:\\n\\t\\t\\traise exc.HTTPConflict(explanation=error.format_message())\\n\\t\\tif ret_resv_id:\\n\\t\\t\\treturn wsgi.ResponseObject({'reservation_id': resv_id},\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   xml=ServerMultipleCreateTemplate)\\n\\t\\treq.cache_db_instances(instances)\\n\\t\\tserver = self._view_builder.create(req, instances[0])\\n\\t\\tif CONF.enable_instance_password:\\n\\t\\t\\tserver['server']['adminPass'] = password\\n\\t\\trobj = wsgi.ResponseObject(server)\\n\\t\\treturn self._add_location(robj)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Enforce flavor access during instance boot\\n\\nThe code in the servers API did not pass the context when retrieving\\nflavor details.  That means it would use an admin context instead,\\nbypassing all flavor access control checks.\\n\\nThis patch includes the fix, and the corresponding unit test, for both\\nthe v2 and v3 APIs.\\n\\nCloses-bug: #1212179",
    "fixed_code": "def create(self, req, body):\\n\\t\\tif not self.is_valid_body(body, 'server'):\\n\\t\\t\\traise exc.HTTPUnprocessableEntity()\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tserver_dict = body['server']\\n\\t\\tpassword = self._get_server_admin_password(server_dict)\\n\\t\\tif 'name' not in server_dict:\\n\\t\\t\\tmsg = _(\"Server name is not defined\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\tname = server_dict['name']\\n\\t\\tself._validate_server_name(name)\\n\\t\\tname = name.strip()\\n\\t\\timage_uuid = self._image_from_req_data(body)\\n\\t\\tpersonality = server_dict.get('personality')\\n\\t\\tconfig_drive = None\\n\\t\\tif self.ext_mgr.is_loaded('os-config-drive'):\\n\\t\\t\\tconfig_drive = server_dict.get('config_drive')\\n\\t\\tinjected_files = []\\n\\t\\tif personality:\\n\\t\\t\\tinjected_files = self._get_injected_files(personality)\\n\\t\\tsg_names = []\\n\\t\\tif self.ext_mgr.is_loaded('os-security-groups'):\\n\\t\\t\\tsecurity_groups = server_dict.get('security_groups')\\n\\t\\t\\tif security_groups is not None:\\n\\t\\t\\t\\tsg_names = [sg['name'] for sg in security_groups\\n\\t\\t\\t\\t\\t\\t\\tif sg.get('name')]\\n\\t\\tif not sg_names:\\n\\t\\t\\tsg_names.append('default')\\n\\t\\tsg_names = list(set(sg_names))\\n\\t\\trequested_networks = None\\n\\t\\tif (self.ext_mgr.is_loaded('os-networks')\\n\\t\\t\\t\\tor self._is_neutron_v2()):\\n\\t\\t\\trequested_networks = server_dict.get('networks')\\n\\t\\tif requested_networks is not None:\\n\\t\\t\\tif not isinstance(requested_networks, list):\\n\\t\\t\\t\\texpl = _('Bad networks format')\\n\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=expl)\\n\\t\\t\\trequested_networks = self._get_requested_networks(\\n\\t\\t\\t\\trequested_networks)\\n\\t\\t(access_ip_v4, ) = server_dict.get('accessIPv4'),\\n\\t\\tif access_ip_v4 is not None:\\n\\t\\t\\tself._validate_access_ipv4(access_ip_v4)\\n\\t\\t(access_ip_v6, ) = server_dict.get('accessIPv6'),\\n\\t\\tif access_ip_v6 is not None:\\n\\t\\t\\tself._validate_access_ipv6(access_ip_v6)\\n\\t\\ttry:\\n\\t\\t\\tflavor_id = self._flavor_id_from_req_data(body)\\n\\t\\texcept ValueError as error:\\n\\t\\t\\tmsg = _(\"Invalid flavorRef provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\tkey_name = None\\n\\t\\tif self.ext_mgr.is_loaded('os-keypairs'):\\n\\t\\t\\tkey_name = server_dict.get('key_name')\\n\\t\\tuser_data = None\\n\\t\\tif self.ext_mgr.is_loaded('os-user-data'):\\n\\t\\t\\tuser_data = server_dict.get('user_data')\\n\\t\\tself._validate_user_data(user_data)\\n\\t\\tavailability_zone = None\\n\\t\\tif self.ext_mgr.is_loaded('os-availability-zone'):\\n\\t\\t\\tavailability_zone = server_dict.get('availability_zone')\\n\\t\\tblock_device_mapping = None\\n\\t\\tblock_device_mapping_v2 = None\\n\\t\\tlegacy_bdm = True\\n\\t\\tif self.ext_mgr.is_loaded('os-volumes'):\\n\\t\\t\\tblock_device_mapping = server_dict.get('block_device_mapping', [])\\n\\t\\t\\tfor bdm in block_device_mapping:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tblock_device.validate_device_name(bdm.get(\"device_name\"))\\n\\t\\t\\t\\t\\tblock_device.validate_and_default_volume_size(bdm)\\n\\t\\t\\t\\texcept exception.InvalidBDMFormat as e:\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=e.format_message())\\n\\t\\t\\t\\tif 'delete_on_termination' in bdm:\\n\\t\\t\\t\\t\\tbdm['delete_on_termination'] = strutils.bool_from_string(\\n\\t\\t\\t\\t\\t\\tbdm['delete_on_termination'])\\n\\t\\t\\tif self.ext_mgr.is_loaded('os-block-device-mapping-v2-boot'):\\n\\t\\t\\t\\tblock_device_mapping_v2 = server_dict.get(\\n\\t\\t\\t\\t\\t'block_device_mapping_v2', [])\\n\\t\\t\\t\\tif block_device_mapping and block_device_mapping_v2:\\n\\t\\t\\t\\t\\texpl = _('Using different block_device_mapping syntaxes '\\n\\t\\t\\t\\t\\t\\t\\t 'is not allowed in the same request.')\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=expl)\\n\\t\\t\\t\\tlegacy_bdm = not bool(block_device_mapping_v2)\\n\\t\\t\\t\\tif any('device_name' not in bdm\\n\\t\\t\\t\\t\\t   for bdm in block_device_mapping_v2):\\n\\t\\t\\t\\t\\texpl = _('Missing device_name in some block devices.')\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=expl)\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tblock_device_mapping_v2 = [\\n\\t\\t\\t\\t\\t\\tblock_device.BlockDeviceDict.from_api(bdm_dict)\\n\\t\\t\\t\\t\\t\\tfor bdm_dict in block_device_mapping_v2]\\n\\t\\t\\t\\texcept exception.InvalidBDMFormat as e:\\n\\t\\t\\t\\t\\traise exc.HTTPBadRequest(explanation=e.format_message())\\n\\t\\tblock_device_mapping = (block_device_mapping or\\n\\t\\t\\t\\t\\t\\t\\t\\tblock_device_mapping_v2)\\n\\t\\tret_resv_id = False\\n\\t\\tmin_count = 1\\n\\t\\tmax_count = 1\\n\\t\\tif self.ext_mgr.is_loaded('os-multiple-create'):\\n\\t\\t\\tret_resv_id = server_dict.get('return_reservation_id', False)\\n\\t\\t\\tmin_count = server_dict.get('min_count', 1)\\n\\t\\t\\tmax_count = server_dict.get('max_count', min_count)\\n\\t\\ttry:\\n\\t\\t\\tmin_count = utils.validate_integer(\\n\\t\\t\\t\\tmin_count, \"min_count\", min_value=1)\\n\\t\\t\\tmax_count = utils.validate_integer(\\n\\t\\t\\t\\tmax_count, \"max_count\", min_value=1)\\n\\t\\texcept exception.InvalidInput as e:\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=e.format_message())\\n\\t\\tif min_count > max_count:\\n\\t\\t\\tmsg = _('min_count must be <= max_count')\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\tauto_disk_config = False\\n\\t\\tif self.ext_mgr.is_loaded('OS-DCF'):\\n\\t\\t\\tauto_disk_config = server_dict.get('auto_disk_config')\\n\\t\\tscheduler_hints = {}\\n\\t\\tif self.ext_mgr.is_loaded('OS-SCH-HNT'):\\n\\t\\t\\tscheduler_hints = server_dict.get('scheduler_hints', {})\\n\\t\\ttry:\\n\\t\\t\\t_get_inst_type = flavors.get_flavor_by_flavor_id\\n\\t\\t\\tinst_type = _get_inst_type(flavor_id, ctxt=context,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   read_deleted=\"no\")\\n\\t\\t\\t(instances, resv_id) = self.compute_api.create(context,\\n\\t\\t\\t\\t\\t\\t\\tinst_type,\\n\\t\\t\\t\\t\\t\\t\\timage_uuid,\\n\\t\\t\\t\\t\\t\\t\\tdisplay_name=name,\\n\\t\\t\\t\\t\\t\\t\\tdisplay_description=name,\\n\\t\\t\\t\\t\\t\\t\\tkey_name=key_name,\\n\\t\\t\\t\\t\\t\\t\\tmetadata=server_dict.get('metadata', {}),\\n\\t\\t\\t\\t\\t\\t\\taccess_ip_v4=access_ip_v4,\\n\\t\\t\\t\\t\\t\\t\\taccess_ip_v6=access_ip_v6,\\n\\t\\t\\t\\t\\t\\t\\tinjected_files=injected_files,\\n\\t\\t\\t\\t\\t\\t\\tadmin_password=password,\\n\\t\\t\\t\\t\\t\\t\\tmin_count=min_count,\\n\\t\\t\\t\\t\\t\\t\\tmax_count=max_count,\\n\\t\\t\\t\\t\\t\\t\\trequested_networks=requested_networks,\\n\\t\\t\\t\\t\\t\\t\\tsecurity_group=sg_names,\\n\\t\\t\\t\\t\\t\\t\\tuser_data=user_data,\\n\\t\\t\\t\\t\\t\\t\\tavailability_zone=availability_zone,\\n\\t\\t\\t\\t\\t\\t\\tconfig_drive=config_drive,\\n\\t\\t\\t\\t\\t\\t\\tblock_device_mapping=block_device_mapping,\\n\\t\\t\\t\\t\\t\\t\\tauto_disk_config=auto_disk_config,\\n\\t\\t\\t\\t\\t\\t\\tscheduler_hints=scheduler_hints,\\n\\t\\t\\t\\t\\t\\t\\tlegacy_bdm=legacy_bdm)\\n\\t\\texcept exception.QuotaError as error:\\n\\t\\t\\traise exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=error.format_message(),\\n\\t\\t\\t\\theaders={'Retry-After': 0})\\n\\t\\texcept exception.InvalidMetadataSize as error:\\n\\t\\t\\traise exc.HTTPRequestEntityTooLarge(\\n\\t\\t\\t\\texplanation=error.format_message())\\n\\t\\texcept exception.ImageNotFound as error:\\n\\t\\t\\tmsg = _(\"Can not find requested image\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept exception.FlavorNotFound as error:\\n\\t\\t\\tmsg = _(\"Invalid flavorRef provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept exception.KeypairNotFound as error:\\n\\t\\t\\tmsg = _(\"Invalid key_name provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept exception.ConfigDriveInvalidValue:\\n\\t\\t\\tmsg = _(\"Invalid config_drive provided.\")\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept rpc_common.RemoteError as err:\\n\\t\\t\\tmsg = \"%(err_type)s: %(err_msg)s\" % {'err_type': err.exc_type,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t 'err_msg': err.value}\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept UnicodeDecodeError as error:\\n\\t\\t\\tmsg = \"UnicodeError: %s\" % unicode(error)\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=msg)\\n\\t\\texcept (exception.ImageNotActive,\\n\\t\\t\\t\\texception.InstanceTypeDiskTooSmall,\\n\\t\\t\\t\\texception.InstanceTypeMemoryTooSmall,\\n\\t\\t\\t\\texception.InstanceTypeNotFound,\\n\\t\\t\\t\\texception.InvalidMetadata,\\n\\t\\t\\t\\texception.InvalidRequest,\\n\\t\\t\\t\\texception.PortNotFound,\\n\\t\\t\\t\\texception.SecurityGroupNotFound) as error:\\n\\t\\t\\traise exc.HTTPBadRequest(explanation=error.format_message())\\n\\t\\texcept exception.PortInUse as error:\\n\\t\\t\\traise exc.HTTPConflict(explanation=error.format_message())\\n\\t\\tif ret_resv_id:\\n\\t\\t\\treturn wsgi.ResponseObject({'reservation_id': resv_id},\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   xml=ServerMultipleCreateTemplate)\\n\\t\\treq.cache_db_instances(instances)\\n\\t\\tserver = self._view_builder.create(req, instances[0])\\n\\t\\tif CONF.enable_instance_password:\\n\\t\\t\\tserver['server']['adminPass'] = password\\n\\t\\trobj = wsgi.ResponseObject(server)\\n\\t\\treturn self._add_location(robj)"
  },
  {
    "code": "def __repr__(self):\\n        return '<factory>'\\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\\nclass _MISSING_TYPE:\\n    pass\\nMISSING = _MISSING_TYPE()\\n_EMPTY_METADATA = types.MappingProxyType({})\\n_FIELD = object()                 \\n_FIELD_CLASSVAR = object()        \\n_FIELD_INITVAR = object()         \\n_MARKER = '__dataclass_fields__'\\n_POST_INIT_NAME = '__post_init__'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-32953: Dataclasses: frozen should not be inherited for non-dataclass derived classes (#6147)\\n\\nIf a non-dataclass derives from a frozen dataclass, allow attributes to be set.\\nRequire either all of the dataclasses in a class hierarchy to be frozen, or all non-frozen.\\nStore `@dataclass` parameters on the class object under `__dataclass_params__`. This is needed to detect frozen base classes.",
    "fixed_code": "def __repr__(self):\\n        return '<factory>'\\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\\nclass _MISSING_TYPE:\\n    pass\\nMISSING = _MISSING_TYPE()\\n_EMPTY_METADATA = types.MappingProxyType({})\\n_FIELD = object()                 \\n_FIELD_CLASSVAR = object()        \\n_FIELD_INITVAR = object()         \\n_FIELDS = '__dataclass_fields__'\\n_PARAMS = '__dataclass_params__'\\n_POST_INIT_NAME = '__post_init__'"
  },
  {
    "code": "def clean_ipv6_address(ip_str, unpack_ipv4=False,\\n        error_message=_(\"This is not a valid IPv6 address.\")):\\n    best_doublecolon_start = -1\\n    best_doublecolon_len = 0\\n    doublecolon_start = -1\\n    doublecolon_len = 0\\n    if not is_valid_ipv6_address(ip_str):\\n        raise ValidationError(error_message, code='invalid')\\n    ip_str = _explode_shorthand_ip_string(ip_str)\\n    ip_str = _sanitize_ipv4_mapping(ip_str)\\n    if unpack_ipv4:\\n        ipv4_unpacked = _unpack_ipv4(ip_str)\\n        if ipv4_unpacked:\\n            return ipv4_unpacked\\n    hextets = ip_str.split(\":\")\\n    for index in range(len(hextets)):\\n        if '.' not in hextets[index]:\\n            hextets[index] = hextets[index].lstrip('0')\\n        if not hextets[index]:\\n            hextets[index] = '0'\\n        if hextets[index] == '0':\\n            doublecolon_len += 1\\n            if doublecolon_start == -1:\\n                doublecolon_start = index\\n            if doublecolon_len > best_doublecolon_len:\\n                best_doublecolon_len = doublecolon_len\\n                best_doublecolon_start = doublecolon_start\\n        else:\\n            doublecolon_len = 0\\n            doublecolon_start = -1\\n    if best_doublecolon_len > 1:\\n        best_doublecolon_end = (best_doublecolon_start +\\n                                best_doublecolon_len)\\n        if best_doublecolon_end == len(hextets):\\n            hextets += ['']\\n        hextets[best_doublecolon_start:best_doublecolon_end] = ['']\\n        if best_doublecolon_start == 0:\\n            hextets = [''] + hextets\\n    result = \":\".join(hextets)\\n    return result.lower()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def clean_ipv6_address(ip_str, unpack_ipv4=False,\\n        error_message=_(\"This is not a valid IPv6 address.\")):\\n    best_doublecolon_start = -1\\n    best_doublecolon_len = 0\\n    doublecolon_start = -1\\n    doublecolon_len = 0\\n    if not is_valid_ipv6_address(ip_str):\\n        raise ValidationError(error_message, code='invalid')\\n    ip_str = _explode_shorthand_ip_string(ip_str)\\n    ip_str = _sanitize_ipv4_mapping(ip_str)\\n    if unpack_ipv4:\\n        ipv4_unpacked = _unpack_ipv4(ip_str)\\n        if ipv4_unpacked:\\n            return ipv4_unpacked\\n    hextets = ip_str.split(\":\")\\n    for index in range(len(hextets)):\\n        if '.' not in hextets[index]:\\n            hextets[index] = hextets[index].lstrip('0')\\n        if not hextets[index]:\\n            hextets[index] = '0'\\n        if hextets[index] == '0':\\n            doublecolon_len += 1\\n            if doublecolon_start == -1:\\n                doublecolon_start = index\\n            if doublecolon_len > best_doublecolon_len:\\n                best_doublecolon_len = doublecolon_len\\n                best_doublecolon_start = doublecolon_start\\n        else:\\n            doublecolon_len = 0\\n            doublecolon_start = -1\\n    if best_doublecolon_len > 1:\\n        best_doublecolon_end = (best_doublecolon_start +\\n                                best_doublecolon_len)\\n        if best_doublecolon_end == len(hextets):\\n            hextets += ['']\\n        hextets[best_doublecolon_start:best_doublecolon_end] = ['']\\n        if best_doublecolon_start == 0:\\n            hextets = [''] + hextets\\n    result = \":\".join(hextets)\\n    return result.lower()"
  },
  {
    "code": "def plot(self, label=None, kind='line', rot=30, axes=None, style='-',\\n             **kwds): \\n        import matplotlib.pyplot as plt\\n        if label is not None:\\n            kwds = kwds.copy()\\n            kwds['label'] = label\\n        N = len(self)\\n        if axes is None:\\n            axes = plt.gca()\\n        if kind == 'line':\\n            axes.plot(self.index, self.values, style, **kwds)\\n        elif kind == 'bar':\\n            xinds = np.arange(N) + 0.25\\n            axes.bar(xinds, self.values, 0.5, bottom=np.zeros(N), linewidth=1)\\n            if N < 10:\\n                fontsize = 12\\n            else:\\n                fontsize = 10\\n            axes.set_xticks(xinds + 0.25)\\n            axes.set_xticklabels(self.index, rotation=rot, fontsize=fontsize)\\n        plt.draw_if_interactive()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def plot(self, label=None, kind='line', rot=30, axes=None, style='-',\\n             **kwds): \\n        import matplotlib.pyplot as plt\\n        if label is not None:\\n            kwds = kwds.copy()\\n            kwds['label'] = label\\n        N = len(self)\\n        if axes is None:\\n            axes = plt.gca()\\n        if kind == 'line':\\n            axes.plot(self.index, self.values, style, **kwds)\\n        elif kind == 'bar':\\n            xinds = np.arange(N) + 0.25\\n            axes.bar(xinds, self.values, 0.5, bottom=np.zeros(N), linewidth=1)\\n            if N < 10:\\n                fontsize = 12\\n            else:\\n                fontsize = 10\\n            axes.set_xticks(xinds + 0.25)\\n            axes.set_xticklabels(self.index, rotation=rot, fontsize=fontsize)\\n        plt.draw_if_interactive()"
  },
  {
    "code": "def fastica(X, n_components=None, algorithm=\"parallel\", whiten=True,\\n            fun=\"logcosh\", fun_args={}, max_iter=200, tol=1e-04, w_init=None,\\n            random_state=None, return_X_mean=False):\\n    \"\"\"Perform Fast Independent Component Analysis.\\n    Parameters\\n    ----------\\n    X : array-like, shape = [n_samples, n_features]\\n        Training vector, where n_samples is the number of samples and\\n        n_features is the number of features.\\n    n_components : int, optional\\n        Number of components to extract. If None no dimension reduction\\n        is performed.\\n    algorithm : {'parallel', 'deflation'}, optional\\n        Apply a parallel or deflational FASTICA algorithm.\\n    whiten : boolean, optional\\n        If True perform an initial whitening of the data.\\n        If False, the data is assumed to have already been\\n        preprocessed: it should be centered, normed and white.\\n        Otherwise you will get incorrect results.\\n        In this case the parameter n_components will be ignored.\\n    fun : string or function, optional. Default: 'logcosh'\\n        The functional form of the G function used in the\\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\\n        or 'cube'.\\n        You can also provide your own function. It should return a tuple\\n        containing the value of the function, and of its derivative, in the\\n        point. Example:",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: improve ICA memory profile by 40%",
    "fixed_code": "def fastica(X, n_components=None, algorithm=\"parallel\", whiten=True,\\n            fun=\"logcosh\", fun_args={}, max_iter=200, tol=1e-04, w_init=None,\\n            random_state=None, return_X_mean=False, compute_sources=True):\\n    \"\"\"Perform Fast Independent Component Analysis.\\n    Parameters\\n    ----------\\n    X : array-like, shape = [n_samples, n_features]\\n        Training vector, where n_samples is the number of samples and\\n        n_features is the number of features.\\n    n_components : int, optional\\n        Number of components to extract. If None no dimension reduction\\n        is performed.\\n    algorithm : {'parallel', 'deflation'}, optional\\n        Apply a parallel or deflational FASTICA algorithm.\\n    whiten : boolean, optional\\n        If True perform an initial whitening of the data.\\n        If False, the data is assumed to have already been\\n        preprocessed: it should be centered, normed and white.\\n        Otherwise you will get incorrect results.\\n        In this case the parameter n_components will be ignored.\\n    fun : string or function, optional. Default: 'logcosh'\\n        The functional form of the G function used in the\\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\\n        or 'cube'.\\n        You can also provide your own function. It should return a tuple\\n        containing the value of the function, and of its derivative, in the\\n        point. Example:"
  },
  {
    "code": "def _queue_management_worker(executor_reference,\\n                             processes,\\n                             pending_work_items,\\n                             work_ids_queue,\\n                             call_queue,\\n                             result_queue):\\n    executor = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _queue_management_worker(executor_reference,\\n                             processes,\\n                             pending_work_items,\\n                             work_ids_queue,\\n                             call_queue,\\n                             result_queue):\\n    executor = None"
  },
  {
    "code": "def __init__(self, sql, using, params=()):\\n        self.params = params\\n        self.sql = sql\\n        self.using = using\\n        self.cursor = None\\n        self.low_mark, self.high_mark = 0, None  \\n        self.extra_select = {}\\n        self.annotation_select = {}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, sql, using, params=()):\\n        self.params = params\\n        self.sql = sql\\n        self.using = using\\n        self.cursor = None\\n        self.low_mark, self.high_mark = 0, None  \\n        self.extra_select = {}\\n        self.annotation_select = {}"
  },
  {
    "code": "def scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False,\\n                   diagonal='hist', marker='.', density_kwds=None,\\n                   hist_kwds=None, **kwds):\\n    import matplotlib.pyplot as plt\\n    from matplotlib.artist import setp\\n    df = frame._get_numeric_data()\\n    n = df.columns.size\\n    fig, axes = _subplots(nrows=n, ncols=n, figsize=figsize, ax=ax,\\n                          squeeze=False)\\n    fig.subplots_adjust(wspace=0, hspace=0)\\n    mask = com.notnull(df)\\n    marker = _get_marker_compat(marker)\\n    hist_kwds = hist_kwds or {}\\n    density_kwds = density_kwds or {}\\n    kwds.setdefault('c', plt.rcParams['patch.facecolor'])\\n    for i, a in zip(lrange(n), df.columns):\\n        for j, b in zip(lrange(n), df.columns):\\n            ax = axes[i, j]\\n            if i == j:\\n                values = df[a].values[mask[a].values]\\n                if diagonal == 'hist':\\n                    ax.hist(values, **hist_kwds)\\n                elif diagonal in ('kde', 'density'):\\n                    from scipy.stats import gaussian_kde\\n                    y = values\\n                    gkde = gaussian_kde(y)\\n                    ind = np.linspace(y.min(), y.max(), 1000)\\n                    ax.plot(ind, gkde.evaluate(ind), **density_kwds)\\n            else:\\n                common = (mask[a] & mask[b]).values\\n                ax.scatter(df[b][common], df[a][common],\\n                           marker=marker, alpha=alpha, **kwds)\\n            ax.set_xlabel('')\\n            ax.set_ylabel('')\\n            _label_axis(ax, kind='x', label=b, position='bottom', rotate=True)\\n            _label_axis(ax, kind='y', label=a, position='left')\\n            if j!= 0:\\n                ax.yaxis.set_visible(False)\\n            if i != n-1:\\n                ax.xaxis.set_visible(False)\\n    for ax in axes.flat:\\n        setp(ax.get_xticklabels(), fontsize=8)\\n        setp(ax.get_yticklabels(), fontsize=8)\\n    return axes",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: scatter_plot ranges are unaligned among subframes GH5497",
    "fixed_code": "def scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False,\\n                   diagonal='hist', marker='.', density_kwds=None,\\n                   hist_kwds=None, range_padding=0.05, **kwds):\\n    import matplotlib.pyplot as plt\\n    from matplotlib.artist import setp\\n    df = frame._get_numeric_data()\\n    n = df.columns.size\\n    fig, axes = _subplots(nrows=n, ncols=n, figsize=figsize, ax=ax,\\n                          squeeze=False)\\n    fig.subplots_adjust(wspace=0, hspace=0)\\n    mask = com.notnull(df)\\n    marker = _get_marker_compat(marker)\\n    hist_kwds = hist_kwds or {}\\n    density_kwds = density_kwds or {}\\n    kwds.setdefault('c', plt.rcParams['patch.facecolor'])\\n    boundaries_list = []\\n    for a in df.columns:\\n        values = df[a].values[mask[a].values]\\n        rmin_, rmax_ = np.min(values), np.max(values)\\n        rdelta_ext = (rmax_ - rmin_) * range_padding / 2.\\n        boundaries_list.append((rmin_ - rdelta_ext, rmax_+ rdelta_ext))\\n    for i, a in zip(lrange(n), df.columns):\\n        for j, b in zip(lrange(n), df.columns):\\n            ax = axes[i, j]\\n            if i == j:\\n                values = df[a].values[mask[a].values]\\n                if diagonal == 'hist':\\n                    ax.hist(values, **hist_kwds)\\n                elif diagonal in ('kde', 'density'):\\n                    from scipy.stats import gaussian_kde\\n                    y = values\\n                    gkde = gaussian_kde(y)\\n                    ind = np.linspace(y.min(), y.max(), 1000)\\n                    ax.plot(ind, gkde.evaluate(ind), **density_kwds)\\n                ax.set_xlim(boundaries_list[i])\\n            else:\\n                common = (mask[a] & mask[b]).values\\n                ax.scatter(df[b][common], df[a][common],\\n                           marker=marker, alpha=alpha, **kwds)\\n                ax.set_xlim(boundaries_list[j])\\n                ax.set_ylim(boundaries_list[i])\\n            ax.set_xlabel('')\\n            ax.set_ylabel('')\\n            _label_axis(ax, kind='x', label=b, position='bottom', rotate=True)\\n            _label_axis(ax, kind='y', label=a, position='left')\\n            if j!= 0:\\n                ax.yaxis.set_visible(False)\\n            if i != n-1:\\n                ax.xaxis.set_visible(False)\\n    for ax in axes.flat:\\n        setp(ax.get_xticklabels(), fontsize=8)\\n        setp(ax.get_yticklabels(), fontsize=8)\\n    return axes"
  },
  {
    "code": "def _is_owned(self):\\n        return self._owner is current_thread()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 76138,76173 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n................\\n  r76138 | antoine.pitrou | 2009-11-06 23:41:14 +0100 (ven., 06 nov. 2009) | 10 lines\\n\\n  Merged revisions 76137 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r76137 | antoine.pitrou | 2009-11-06 23:34:35 +0100 (ven., 06 nov. 2009) | 4 lines\\n\\n    Issue #7270: Add some dedicated unit tests for multi-thread synchronization\\n    primitives such as Lock, RLock, Condition, Event and Semaphore.\\n  ........\\n................\\n  r76173 | antoine.pitrou | 2009-11-09 17:08:16 +0100 (lun., 09 nov. 2009) | 11 lines\\n\\n  Merged revisions 76172 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r76172 | antoine.pitrou | 2009-11-09 17:00:11 +0100 (lun., 09 nov. 2009) | 5 lines\\n\\n    Issue #7282: Fix a memory leak when an RLock was used in a thread other\\n    than those started through `threading.Thread` (for example, using\\n    `thread.start_new_thread()`.\\n  ........\\n................",
    "fixed_code": "def _is_owned(self):\\n        return self._owner == _get_ident()"
  },
  {
    "code": "def __repr__(self):\\n        if self.__class__ == SymbolTable:\\n            kind = \"\"\\n        else:\\n            kind = \"%s \" % self.__class__.__name__\\n        if self._table.name == \"global\":\\n            return \"<{0}SymbolTable for module {1}>\".format(kind, self._filename)\\n        else:\\n            return \"<{0}SymbolTable for {1} in {2}>\".format(kind,\\n                                                            self._table.name,\\n                                                            self._filename)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n        if self.__class__ == SymbolTable:\\n            kind = \"\"\\n        else:\\n            kind = \"%s \" % self.__class__.__name__\\n        if self._table.name == \"global\":\\n            return \"<{0}SymbolTable for module {1}>\".format(kind, self._filename)\\n        else:\\n            return \"<{0}SymbolTable for {1} in {2}>\".format(kind,\\n                                                            self._table.name,\\n                                                            self._filename)"
  },
  {
    "code": "def signature_parameters(self):\\n        if inspect.isclass(self.method_obj):\\n            if hasattr(self.method_obj, '_accessors') and (\\n                    self.method_name.split('.')[-1] in\\n                    self.method_obj._accessors):\\n                return tuple()\\n        try:\\n            sig = signature(self.method_obj)\\n        except (TypeError, ValueError):\\n            return tuple()\\n        params = sig.args\\n        if sig.varargs:\\n            params.append(\"*\" + sig.varargs)\\n        if sig.keywords:\\n            params.append(\"**\" + sig.keywords)\\n        params = tuple(params)\\n        if params and params[0] in ('self', 'cls'):\\n            return params[1:]\\n        return params\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Changes to validate_docstring script to be able to check all docstrings at once (#22408)",
    "fixed_code": "def signature_parameters(self):\\n        if inspect.isclass(self.obj):\\n            if hasattr(self.obj, '_accessors') and (\\n                    self.name.split('.')[-1] in\\n                    self.obj._accessors):\\n                return tuple()\\n        try:\\n            sig = signature(self.obj)\\n        except (TypeError, ValueError):\\n            return tuple()\\n        params = sig.args\\n        if sig.varargs:\\n            params.append(\"*\" + sig.varargs)\\n        if sig.keywords:\\n            params.append(\"**\" + sig.keywords)\\n        params = tuple(params)\\n        if params and params[0] in ('self', 'cls'):\\n            return params[1:]\\n        return params\\n    @property"
  },
  {
    "code": "def _get_global_configs(self):\\n        config = self.config.get_configuration_definitions(ignore_private=True).copy()\\n        for setting in config.keys():\\n            v, o = C.config.get_config_value_and_origin(setting, cfile=self.config_file, variables=get_constants())\\n            config[setting] = Setting(setting, v, o, None)\\n        return self._render_settings(config)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_global_configs(self):\\n        config = self.config.get_configuration_definitions(ignore_private=True).copy()\\n        for setting in config.keys():\\n            v, o = C.config.get_config_value_and_origin(setting, cfile=self.config_file, variables=get_constants())\\n            config[setting] = Setting(setting, v, o, None)\\n        return self._render_settings(config)"
  },
  {
    "code": "def _checkflag(flag, file):\\n    if flag == 'r':\\n        flags = db.DB_RDONLY\\n    elif flag == 'rw':\\n        flags = 0\\n    elif flag == 'w':\\n        flags =  db.DB_CREATE\\n    elif flag == 'c':\\n        flags =  db.DB_CREATE\\n    elif flag == 'n':\\n        flags = db.DB_CREATE\\n        if os.path.isfile(file):\\n            os.unlink(file)\\n    else:\\n        raise error, \"flags should be one of 'r', 'w', 'c' or 'n'\"\\n    return flags | db.DB_THREAD",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Closes bug #1149413\\n\\nUsing None for a filename with the 'n' flag when calling bsddb.btopen\\nwould cause an error while checking if the file None existed.  error\\nnot likely to be seen as anyone using None for a filename would likely\\nuse the 'c' flag in the first place.",
    "fixed_code": "def _checkflag(flag, file):\\n    if flag == 'r':\\n        flags = db.DB_RDONLY\\n    elif flag == 'rw':\\n        flags = 0\\n    elif flag == 'w':\\n        flags =  db.DB_CREATE\\n    elif flag == 'c':\\n        flags =  db.DB_CREATE\\n    elif flag == 'n':\\n        flags = db.DB_CREATE\\n        if file is not None and os.path.isfile(file):\\n            os.unlink(file)\\n    else:\\n        raise error, \"flags should be one of 'r', 'w', 'c' or 'n'\"\\n    return flags | db.DB_THREAD"
  },
  {
    "code": "def __repr__(self):\\n        base, mult = _gfc(self.freq)\\n        formatted = lib.period_ordinal_to_string(self.ordinal, base)\\n        freqstr = _freq_mod._reverse_period_code_map[base]\\n        return \"Period('%s', '%s')\" % (formatted, freqstr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n        base, mult = _gfc(self.freq)\\n        formatted = lib.period_ordinal_to_string(self.ordinal, base)\\n        freqstr = _freq_mod._reverse_period_code_map[base]\\n        return \"Period('%s', '%s')\" % (formatted, freqstr)"
  },
  {
    "code": "def sortlevel(self, level=0, axis=0, ascending=True):\\n        the_axis = self._get_axis(axis)\\n        if not isinstance(the_axis, MultiIndex):\\n            raise Exception('can only sort by level with a hierarchical index')\\n        new_axis, indexer = the_axis.sortlevel(level, ascending=ascending)\\n        if self._data.is_mixed_dtype():\\n            if axis == 0:\\n                return self.reindex(index=new_axis)\\n            else:\\n                return self.reindex(columns=new_axis)\\n        if axis == 0:\\n            index = new_axis\\n            columns = self.columns\\n        else:\\n            index = self.index\\n            columns = new_axis\\n        new_values = self.values.take(indexer, axis=axis)\\n        return self._constructor(new_values, index=index, columns=columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: inplace for DataFrame.sortlevel #1873",
    "fixed_code": "def sortlevel(self, level=0, axis=0, ascending=True, inplace=False):\\n        the_axis = self._get_axis(axis)\\n        if not isinstance(the_axis, MultiIndex):\\n            raise Exception('can only sort by level with a hierarchical index')\\n        new_axis, indexer = the_axis.sortlevel(level, ascending=ascending)\\n        if self._data.is_mixed_dtype() and not inplace:\\n            if axis == 0:\\n                return self.reindex(index=new_axis)\\n            else:\\n                return self.reindex(columns=new_axis)\\n        if inplace:\\n            if axis == 1:\\n                self._data = self._data.reindex_items(self._data.items[indexer],\\n                                                      copy=False)\\n            elif axis == 0:\\n                self._data = self._data.take(indexer)\\n            self._clear_item_cache()\\n            return self\\n        else:\\n            return self.take(indexer, axis=axis)"
  },
  {
    "code": "def spectral_clustering(affinity, n_clusters=8, n_components=None,\\n                        eigen_solver=None, random_state=None, n_init=10,\\n                        eigen_tol=0.0, assign_labels='kmeans'):\\n    if assign_labels not in ('kmeans', 'discretize'):\\n        raise ValueError(\"The 'assign_labels' parameter should be \"\\n                         \"'kmeans' or 'discretize', but '%s' was given\"\\n                         % assign_labels)\\n    random_state = check_random_state(random_state)\\n    n_components = n_clusters if n_components is None else n_components\\n    maps = spectral_embedding(affinity, n_components=n_components,\\n                              eigen_solver=eigen_solver,\\n                              random_state=random_state,\\n                              eigen_tol=eigen_tol, drop_first=False)\\n    if assign_labels == 'kmeans':\\n        _, labels, _ = k_means(maps, n_clusters, random_state=random_state,\\n                               n_init=n_init)\\n    else:\\n        labels = discretize(maps, random_state=random_state)\\n    return labels",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def spectral_clustering(affinity, n_clusters=8, n_components=None,\\n                        eigen_solver=None, random_state=None, n_init=10,\\n                        eigen_tol=0.0, assign_labels='kmeans'):\\n    if assign_labels not in ('kmeans', 'discretize'):\\n        raise ValueError(\"The 'assign_labels' parameter should be \"\\n                         \"'kmeans' or 'discretize', but '%s' was given\"\\n                         % assign_labels)\\n    random_state = check_random_state(random_state)\\n    n_components = n_clusters if n_components is None else n_components\\n    maps = spectral_embedding(affinity, n_components=n_components,\\n                              eigen_solver=eigen_solver,\\n                              random_state=random_state,\\n                              eigen_tol=eigen_tol, drop_first=False)\\n    if assign_labels == 'kmeans':\\n        _, labels, _ = k_means(maps, n_clusters, random_state=random_state,\\n                               n_init=n_init)\\n    else:\\n        labels = discretize(maps, random_state=random_state)\\n    return labels"
  },
  {
    "code": "def isin(comps: AnyArrayLike, values: AnyArrayLike) -> np.ndarray:\\n    if not is_list_like(comps):\\n        raise TypeError(\\n            \"only list-like objects are allowed to be passed \"\\n            f\"to isin(), you passed a [{type(comps).__name__}]\"\\n        )\\n    if not is_list_like(values):\\n        raise TypeError(\\n            \"only list-like objects are allowed to be passed \"\\n            f\"to isin(), you passed a [{type(values).__name__}]\"\\n        )\\n    if not isinstance(values, (ABCIndex, ABCSeries, ABCExtensionArray, np.ndarray)):\\n        values = _ensure_arraylike(list(values))\\n    elif isinstance(values, ABCMultiIndex):\\n        values = np.array(values)\\n    else:\\n        values = extract_array(values, extract_numpy=True)\\n    comps = _ensure_arraylike(comps)\\n    comps = extract_array(comps, extract_numpy=True)\\n    if is_extension_array_dtype(comps.dtype):\\n        return comps.isin(values)  \\n    elif needs_i8_conversion(comps.dtype):\\n        return pd_array(comps).isin(values)\\n    elif needs_i8_conversion(values.dtype) and not is_object_dtype(comps.dtype):\\n        return np.zeros(comps.shape, dtype=bool)\\n    elif needs_i8_conversion(values.dtype):\\n        return isin(comps, values.astype(object))\\n    elif is_extension_array_dtype(values.dtype):\\n        return isin(np.asarray(comps), np.asarray(values))\\n    if len(comps) > 1_000_000 and len(values) <= 26 and not is_object_dtype(comps):\\n        if isna(values).any():",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: isin raising TypeError for RangeIndex (#41155)",
    "fixed_code": "def isin(comps: AnyArrayLike, values: AnyArrayLike) -> np.ndarray:\\n    if not is_list_like(comps):\\n        raise TypeError(\\n            \"only list-like objects are allowed to be passed \"\\n            f\"to isin(), you passed a [{type(comps).__name__}]\"\\n        )\\n    if not is_list_like(values):\\n        raise TypeError(\\n            \"only list-like objects are allowed to be passed \"\\n            f\"to isin(), you passed a [{type(values).__name__}]\"\\n        )\\n    if not isinstance(values, (ABCIndex, ABCSeries, ABCExtensionArray, np.ndarray)):\\n        values = _ensure_arraylike(list(values))\\n    elif isinstance(values, ABCMultiIndex):\\n        values = np.array(values)\\n    else:\\n        values = extract_array(values, extract_numpy=True, extract_range=True)\\n    comps = _ensure_arraylike(comps)\\n    comps = extract_array(comps, extract_numpy=True)\\n    if is_extension_array_dtype(comps.dtype):\\n        return comps.isin(values)  \\n    elif needs_i8_conversion(comps.dtype):\\n        return pd_array(comps).isin(values)\\n    elif needs_i8_conversion(values.dtype) and not is_object_dtype(comps.dtype):\\n        return np.zeros(comps.shape, dtype=bool)\\n    elif needs_i8_conversion(values.dtype):\\n        return isin(comps, values.astype(object))\\n    elif is_extension_array_dtype(values.dtype):\\n        return isin(np.asarray(comps), np.asarray(values))\\n    if len(comps) > 1_000_000 and len(values) <= 26 and not is_object_dtype(comps):\\n        if isna(values).any():"
  },
  {
    "code": "def get_config(module, flags=None):\\n    flags = [] if flags is None else flags\\n    cmd = 'show running-config '\\n    cmd += ' '.join(flags)\\n    cmd = cmd.strip()\\n    try:\\n        return _DEVICE_CONFIGS[cmd]\\n    except KeyError:\\n        rc, out, err = exec_command(module, cmd)\\n        if rc != 0:\\n            module.fail_json(msg='unable to retrieve current config', stderr=to_text(err, errors='surrogate_then_replace'))\\n        cfg = sanitize(to_text(out, errors='surrogate_then_replace').strip())\\n        _DEVICE_CONFIGS[cmd] = cfg\\n        return cfg",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_config(module, flags=None):\\n    flags = [] if flags is None else flags\\n    cmd = 'show running-config '\\n    cmd += ' '.join(flags)\\n    cmd = cmd.strip()\\n    try:\\n        return _DEVICE_CONFIGS[cmd]\\n    except KeyError:\\n        rc, out, err = exec_command(module, cmd)\\n        if rc != 0:\\n            module.fail_json(msg='unable to retrieve current config', stderr=to_text(err, errors='surrogate_then_replace'))\\n        cfg = sanitize(to_text(out, errors='surrogate_then_replace').strip())\\n        _DEVICE_CONFIGS[cmd] = cfg\\n        return cfg"
  },
  {
    "code": "def lag_plot(series, ax=None, **kwds):\\n    import matplotlib.pyplot as plt\\n    data = series.values\\n    y1 = data[:-1]\\n    y2 = data[1:]\\n    if ax is None:\\n        ax = plt.gca()\\n    ax.set_xlabel(\"y(t)\")\\n    ax.set_ylabel(\"y(t + 1)\")\\n    ax.scatter(y1, y2, **kwds)\\n    return ax",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "lag_plot() accepts variable lags\\n\\nthe default is still 1",
    "fixed_code": "def lag_plot(series, lag=1, ax=None, **kwds):\\n    import matplotlib.pyplot as plt\\n    data = series.values\\n    y1 = data[:-lag]\\n    y2 = data[lag:]\\n    if ax is None:\\n        ax = plt.gca()\\n    ax.set_xlabel(\"y(t)\")\\n    ax.set_ylabel(\"y(t + %s)\" % lag)\\n    ax.scatter(y1, y2, **kwds)\\n    return ax"
  },
  {
    "code": "def __call__(self, *args, **kwargs):\\n        if self.func is None:\\n            self.func = self.get_func(*self.args, **self.kwargs)\\n        return self.func(self.thread_context.ptr, *args)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __call__(self, *args, **kwargs):\\n        if self.func is None:\\n            self.func = self.get_func(*self.args, **self.kwargs)\\n        return self.func(self.thread_context.ptr, *args)"
  },
  {
    "code": "def _solve_svd_design_matrix(\\n            self, alpha, y, sqrt_sw, X_mean, singvals_sq, U, UT_y):\\n        w = ((singvals_sq + alpha) ** -1) - (alpha ** -1)\\n        if self.fit_intercept:\\n            normalized_sw = sqrt_sw / np.linalg.norm(sqrt_sw)\\n            intercept_dim = _find_smallest_angle(normalized_sw, U)\\n            w[intercept_dim] = - (alpha ** -1)\\n        c = np.dot(U, self._diag_dot(w, UT_y)) + (alpha ** -1) * y\\n        G_inverse_diag = self._decomp_diag(w, U) + (alpha ** -1)\\n        if len(y.shape) != 1:\\n            G_inverse_diag = G_inverse_diag[:, np.newaxis]\\n        return G_inverse_diag, c",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _solve_svd_design_matrix(\\n            self, alpha, y, sqrt_sw, X_mean, singvals_sq, U, UT_y):\\n        w = ((singvals_sq + alpha) ** -1) - (alpha ** -1)\\n        if self.fit_intercept:\\n            normalized_sw = sqrt_sw / np.linalg.norm(sqrt_sw)\\n            intercept_dim = _find_smallest_angle(normalized_sw, U)\\n            w[intercept_dim] = - (alpha ** -1)\\n        c = np.dot(U, self._diag_dot(w, UT_y)) + (alpha ** -1) * y\\n        G_inverse_diag = self._decomp_diag(w, U) + (alpha ** -1)\\n        if len(y.shape) != 1:\\n            G_inverse_diag = G_inverse_diag[:, np.newaxis]\\n        return G_inverse_diag, c"
  },
  {
    "code": "def advertisement():\\n\\tuser_agent_string = request.user_agent.string\\n\\tuser_agent_obj = user_agents.parse(user_agent_string)\\n\\tbrowser_ok = True\\n\\tbrowser_exclude_rule = CONFIG.get('Task Parameters', 'browser_exclude_rule')\\n\\tfor rule in browser_exclude_rule.split(','):\\n\\t\\tmyrule = rule.strip()\\n\\t\\tif myrule in [\"mobile\", \"tablet\", \"touchcapable\", \"pc\", \"bot\"]:\\n\\t\\t\\tif (myrule == \"mobile\" and user_agent_obj.is_mobile) or\\\\n\\t\\t\\t   (myrule == \"tablet\" and user_agent_obj.is_tablet) or\\\\n\\t\\t\\t   (myrule == \"touchcapable\" and user_agent_obj.is_touch_capable) or\\\\n\\t\\t\\t   (myrule == \"pc\" and user_agent_obj.is_pc) or\\\\n\\t\\t\\t   (myrule == \"bot\" and user_agent_obj.is_bot):\\n\\t\\t\\t\\tbrowser_ok = False\\n\\t\\telif myrule == \"Safari\" or myrule == \"safari\":\\n\\t\\t\\tif \"Chrome\" in user_agent_string and \"Safari\" in user_agent_string:\\n\\t\\t\\t\\tpass\\n\\t\\t\\telif \"Safari\" in user_agent_string:\\n\\t\\t\\t\\tbrowser_ok = False\\n\\t\\telif myrule in user_agent_string:\\n\\t\\t\\tbrowser_ok = False\\n\\tif not browser_ok:\\n\\t\\traise ExperimentError('browser_type_not_allowed')\\n\\tif not ('hitId' in request.args and 'assignmentId' in request.args):\\n\\t\\traise ExperimentError('hit_assign_worker_id_not_set_in_mturk')\\n\\thit_id = request.args['hitId']\\n\\tassignment_id = request.args['assignmentId']\\n\\tmode = request.args['mode']\\n\\tif hit_id[:5] == \"debug\":\\n\\t\\tdebug_mode = True\\n\\telse:\\n\\t\\tdebug_mode = False\\n\\talready_in_db = False\\n\\tif 'workerId' in request.args:\\n\\t\\tworker_id = request.args['workerId']\\n\\t\\tnrecords = Participant.query.\\\\n\\t\\t\\tfilter(Participant.assignmentid != assignment_id).\\\\n\\t\\t\\tfilter(Participant.workerid == worker_id).\\\\n\\t\\t\\tcount()\\n\\t\\tif nrecords > 0:  \\n\\t\\t\\talready_in_db = True\\n\\telse:  \\n\\t\\tworker_id = None\\n\\ttry:\\n\\t\\tpart = Participant.query.\\\\n\\t\\t\\tfilter(Participant.hitid == hit_id).\\\\n\\t\\t\\tfilter(Participant.assignmentid == assignment_id).\\\\n\\t\\t\\tfilter(Participant.workerid == worker_id).\\\\n\\t\\t\\tone()\\n\\t\\tstatus = part.status\\n\\texcept exc.SQLAlchemyError:\\n\\t\\tstatus = None\\n\\tallow_repeats = CONFIG.getboolean('Task Parameters', 'allow_repeats')\\n\\tif (status == STARTED or status == QUITEARLY) and not debug_mode:\\n\\t\\traise ExperimentError('already_started_exp_mturk')\\n\\telif status == COMPLETED or (status == SUBMITTED and not already_in_db):\\n\\t\\treturn render_template(\\n\\t\\t\\t'thanks-mturksubmit.html',\\n\\t\\t\\tusing_sandbox=(mode == \"sandbox\"),\\n\\t\\t\\thitid=hit_id,\\n\\t\\t\\tassignmentid=assignment_id,\\n\\t\\t\\tworkerid=worker_id\\n\\t\\t)\\n\\telif already_in_db and not (debug_mode or allow_repeats):\\n\\t\\traise ExperimentError('already_did_exp_hit')\\n\\telif status == ALLOCATED or not status or debug_mode:\\n\\t\\twith open('templates/ad.html', 'r') as temp_file:\\n\\t\\t\\tad_string = temp_file.read()\\n\\t\\tad_string = insert_mode(ad_string)\\n\\t\\treturn render_template_string(\\n\\t\\t\\tad_string,\\n\\t\\t\\tmode=mode,\\n\\t\\t\\thitid=hit_id,\\n\\t\\t\\tassignmentid=assignment_id,\\n\\t\\t\\tworkerid=worker_id\\n\\t\\t)\\n\\telse:\\n\\t\\traise ExperimentError('status_incorrectly_set')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def advertisement():\\n\\tuser_agent_string = request.user_agent.string\\n\\tuser_agent_obj = user_agents.parse(user_agent_string)\\n\\tbrowser_ok = True\\n\\tbrowser_exclude_rule = CONFIG.get('Task Parameters', 'browser_exclude_rule')\\n\\tfor rule in browser_exclude_rule.split(','):\\n\\t\\tmyrule = rule.strip()\\n\\t\\tif myrule in [\"mobile\", \"tablet\", \"touchcapable\", \"pc\", \"bot\"]:\\n\\t\\t\\tif (myrule == \"mobile\" and user_agent_obj.is_mobile) or\\\\n\\t\\t\\t   (myrule == \"tablet\" and user_agent_obj.is_tablet) or\\\\n\\t\\t\\t   (myrule == \"touchcapable\" and user_agent_obj.is_touch_capable) or\\\\n\\t\\t\\t   (myrule == \"pc\" and user_agent_obj.is_pc) or\\\\n\\t\\t\\t   (myrule == \"bot\" and user_agent_obj.is_bot):\\n\\t\\t\\t\\tbrowser_ok = False\\n\\t\\telif myrule == \"Safari\" or myrule == \"safari\":\\n\\t\\t\\tif \"Chrome\" in user_agent_string and \"Safari\" in user_agent_string:\\n\\t\\t\\t\\tpass\\n\\t\\t\\telif \"Safari\" in user_agent_string:\\n\\t\\t\\t\\tbrowser_ok = False\\n\\t\\telif myrule in user_agent_string:\\n\\t\\t\\tbrowser_ok = False\\n\\tif not browser_ok:\\n\\t\\traise ExperimentError('browser_type_not_allowed')\\n\\tif not ('hitId' in request.args and 'assignmentId' in request.args):\\n\\t\\traise ExperimentError('hit_assign_worker_id_not_set_in_mturk')\\n\\thit_id = request.args['hitId']\\n\\tassignment_id = request.args['assignmentId']\\n\\tmode = request.args['mode']\\n\\tif hit_id[:5] == \"debug\":\\n\\t\\tdebug_mode = True\\n\\telse:\\n\\t\\tdebug_mode = False\\n\\talready_in_db = False\\n\\tif 'workerId' in request.args:\\n\\t\\tworker_id = request.args['workerId']\\n\\t\\tnrecords = Participant.query.\\\\n\\t\\t\\tfilter(Participant.assignmentid != assignment_id).\\\\n\\t\\t\\tfilter(Participant.workerid == worker_id).\\\\n\\t\\t\\tcount()\\n\\t\\tif nrecords > 0:  \\n\\t\\t\\talready_in_db = True\\n\\telse:  \\n\\t\\tworker_id = None\\n\\ttry:\\n\\t\\tpart = Participant.query.\\\\n\\t\\t\\tfilter(Participant.hitid == hit_id).\\\\n\\t\\t\\tfilter(Participant.assignmentid == assignment_id).\\\\n\\t\\t\\tfilter(Participant.workerid == worker_id).\\\\n\\t\\t\\tone()\\n\\t\\tstatus = part.status\\n\\texcept exc.SQLAlchemyError:\\n\\t\\tstatus = None\\n\\tallow_repeats = CONFIG.getboolean('Task Parameters', 'allow_repeats')\\n\\tif (status == STARTED or status == QUITEARLY) and not debug_mode:\\n\\t\\traise ExperimentError('already_started_exp_mturk')\\n\\telif status == COMPLETED or (status == SUBMITTED and not already_in_db):\\n\\t\\treturn render_template(\\n\\t\\t\\t'thanks-mturksubmit.html',\\n\\t\\t\\tusing_sandbox=(mode == \"sandbox\"),\\n\\t\\t\\thitid=hit_id,\\n\\t\\t\\tassignmentid=assignment_id,\\n\\t\\t\\tworkerid=worker_id\\n\\t\\t)\\n\\telif already_in_db and not (debug_mode or allow_repeats):\\n\\t\\traise ExperimentError('already_did_exp_hit')\\n\\telif status == ALLOCATED or not status or debug_mode:\\n\\t\\twith open('templates/ad.html', 'r') as temp_file:\\n\\t\\t\\tad_string = temp_file.read()\\n\\t\\tad_string = insert_mode(ad_string)\\n\\t\\treturn render_template_string(\\n\\t\\t\\tad_string,\\n\\t\\t\\tmode=mode,\\n\\t\\t\\thitid=hit_id,\\n\\t\\t\\tassignmentid=assignment_id,\\n\\t\\t\\tworkerid=worker_id\\n\\t\\t)\\n\\telse:\\n\\t\\traise ExperimentError('status_incorrectly_set')"
  },
  {
    "code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None):\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.filename = filename\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        if self.mode == WRITE:\\n            self._write_gzip_header()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "force gzip module to open files using 'b'inary mode. closes patch #536278.",
    "fixed_code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.filename = filename\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        if self.mode == WRITE:\\n            self._write_gzip_header()"
  },
  {
    "code": "def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\\n\\t\\t   force_grouping=False, use_l10n=None):\\n\\tuse_grouping = (use_l10n or (use_l10n is None and settings.USE_L10N)) and settings.USE_THOUSAND_SEPARATOR\\n\\tuse_grouping = use_grouping or force_grouping\\n\\tuse_grouping = use_grouping and grouping != 0\\n\\tif isinstance(number, int) and not use_grouping and not decimal_pos:\\n\\t\\treturn mark_safe(number)\\n\\tsign = ''\\n\\tif isinstance(number, Decimal):\\n\\t\\tstr_number = '{:f}'.format(number)\\n\\telse:\\n\\t\\tstr_number = str(number)\\n\\tif str_number[0] == '-':\\n\\t\\tsign = '-'\\n\\t\\tstr_number = str_number[1:]\\n\\tif '.' in str_number:\\n\\t\\tint_part, dec_part = str_number.split('.')\\n\\t\\tif decimal_pos is not None:\\n\\t\\t\\tdec_part = dec_part[:decimal_pos]\\n\\telse:\\n\\t\\tint_part, dec_part = str_number, ''\\n\\tif decimal_pos is not None:\\n\\t\\tdec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\\n\\tdec_part = dec_part and decimal_sep + dec_part\\n\\tif use_grouping:\\n\\t\\ttry:\\n\\t\\t\\tintervals = list(grouping)\\n\\t\\texcept TypeError:\\n\\t\\t\\tintervals = [grouping, 0]\\n\\t\\tactive_interval = intervals.pop(0)\\n\\t\\tint_part_gd = ''\\n\\t\\tcnt = 0\\n\\t\\tfor digit in int_part[::-1]:\\n\\t\\t\\tif cnt and cnt == active_interval:\\n\\t\\t\\t\\tif intervals:\\n\\t\\t\\t\\t\\tactive_interval = intervals.pop(0) or active_interval\\n\\t\\t\\t\\tint_part_gd += thousand_sep[::-1]\\n\\t\\t\\t\\tcnt = 0\\n\\t\\t\\tint_part_gd += digit\\n\\t\\t\\tcnt += 1\\n\\t\\tint_part = int_part_gd[::-1]\\n\\treturn sign + int_part + dec_part",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed CVE-2019-6975 -- Fixed memory exhaustion in utils.numberformat.format().\\n\\nThanks Sjoerd Job Postmus for the report and initial patch.\\nThanks Michael Manfre, Tim Graham, and Florian Apolloner for review.",
    "fixed_code": "def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\\n\\t\\t   force_grouping=False, use_l10n=None):\\n\\tuse_grouping = (use_l10n or (use_l10n is None and settings.USE_L10N)) and settings.USE_THOUSAND_SEPARATOR\\n\\tuse_grouping = use_grouping or force_grouping\\n\\tuse_grouping = use_grouping and grouping != 0\\n\\tif isinstance(number, int) and not use_grouping and not decimal_pos:\\n\\t\\treturn mark_safe(number)\\n\\tsign = ''\\n\\tif isinstance(number, Decimal):\\n\\t\\t_, digits, exponent = number.as_tuple()\\n\\t\\tif abs(exponent) + len(digits) > 200:\\n\\t\\t\\tnumber = '{:e}'.format(number)\\n\\t\\t\\tcoefficient, exponent = number.split('e')\\n\\t\\t\\tcoefficient = format(\\n\\t\\t\\t\\tcoefficient, decimal_sep, decimal_pos, grouping,\\n\\t\\t\\t\\tthousand_sep, force_grouping, use_l10n,\\n\\t\\t\\t)\\n\\t\\t\\treturn '{}e{}'.format(coefficient, exponent)\\n\\t\\telse:\\n\\t\\t\\tstr_number = '{:f}'.format(number)\\n\\telse:\\n\\t\\tstr_number = str(number)\\n\\tif str_number[0] == '-':\\n\\t\\tsign = '-'\\n\\t\\tstr_number = str_number[1:]\\n\\tif '.' in str_number:\\n\\t\\tint_part, dec_part = str_number.split('.')\\n\\t\\tif decimal_pos is not None:\\n\\t\\t\\tdec_part = dec_part[:decimal_pos]\\n\\telse:\\n\\t\\tint_part, dec_part = str_number, ''\\n\\tif decimal_pos is not None:\\n\\t\\tdec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\\n\\tdec_part = dec_part and decimal_sep + dec_part\\n\\tif use_grouping:\\n\\t\\ttry:\\n\\t\\t\\tintervals = list(grouping)\\n\\t\\texcept TypeError:\\n\\t\\t\\tintervals = [grouping, 0]\\n\\t\\tactive_interval = intervals.pop(0)\\n\\t\\tint_part_gd = ''\\n\\t\\tcnt = 0\\n\\t\\tfor digit in int_part[::-1]:\\n\\t\\t\\tif cnt and cnt == active_interval:\\n\\t\\t\\t\\tif intervals:\\n\\t\\t\\t\\t\\tactive_interval = intervals.pop(0) or active_interval\\n\\t\\t\\t\\tint_part_gd += thousand_sep[::-1]\\n\\t\\t\\t\\tcnt = 0\\n\\t\\t\\tint_part_gd += digit\\n\\t\\t\\tcnt += 1\\n\\t\\tint_part = int_part_gd[::-1]\\n\\treturn sign + int_part + dec_part"
  },
  {
    "code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    from pandas.tseries.period import PeriodIndex\\n    is_period = com.is_period_arraylike(values)\\n    values = Series(values).values\\n    is_category = com.is_categorical_dtype(values.dtype)\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    elif is_category:\\n        bins = values.categories\\n        cat = values\\n        values = cat.codes\\n    dtype = values.dtype\\n    if issubclass(values.dtype.type, (np.datetime64, np.timedelta64)) or is_period:\\n        if is_period:\\n            values = PeriodIndex(values)\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        if dropna:\\n            from pandas.tslib import iNaT\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n        keys = keys.astype(dtype)\\n    elif com.is_integer_dtype(dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    else:\\n        values = com._ensure_object(values)\\n        mask = com.isnull(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n        if not dropna:\\n            keys = np.insert(keys, 0, np.NaN)\\n            counts = np.insert(counts, 0, mask.sum())\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.categories)), fill_value=0)\\n        if not is_category:\\n            result.index = bins[:-1]\\n        else:\\n            result.index = cat.categories\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG,TST,CLN: improve handling of Series.value_counts's argument 'dropna' (GH9443)\\n\\n- Fixed bug in Series.values_counts with excluding NaN for categorical\\n  type Series with dropna=True.\\n- Series.values_counts and Series.describe for categorical type will now\\n  put NaN entries at the end.\\n- Series.describe for categorical type will now give counts and frequencies\\n  of 0, not NA, for unused categories.\\n- Added Categorical.value_counts and Categorical.dropna for internal use.",
    "fixed_code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    from pandas.tseries.period import PeriodIndex\\n    values = Series(values).values\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    if com.is_categorical_dtype(values.dtype):\\n        result = values.value_counts(dropna)\\n    else:\\n        dtype = values.dtype\\n        is_period = com.is_period_arraylike(values)\\n        if com.is_datetime_or_timedelta_dtype(dtype) or is_period:\\n            if is_period:\\n                values = PeriodIndex(values)\\n            values = values.view(np.int64)\\n            keys, counts = htable.value_count_int64(values)\\n            if dropna:\\n                from pandas.tslib import iNaT\\n                msk = keys != iNaT\\n                keys, counts = keys[msk], counts[msk]\\n            keys = keys.astype(dtype)\\n        elif com.is_integer_dtype(dtype):\\n            values = com._ensure_int64(values)\\n            keys, counts = htable.value_count_int64(values)\\n        else:\\n            values = com._ensure_object(values)\\n            mask = com.isnull(values)\\n            keys, counts = htable.value_count_object(values, mask)\\n            if not dropna and mask.any():\\n                keys = np.insert(keys, 0, np.NaN)\\n                counts = np.insert(counts, 0, mask.sum())\\n        result = Series(counts, index=com._values_from_object(keys))\\n        if bins is not None:\\n            result = result.reindex(np.arange(len(cat.categories)), fill_value=0)\\n            result.index = bins[:-1]\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result"
  },
  {
    "code": "def build_extensions(self):\\n        missing = self.detect_modules()\\n        extensions = [ext for ext in self.extensions\\n                      if ext.name not in disabled_module_list]\\n        ext_map = dict((ext.name, i) for i, ext in enumerate(extensions))\\n        if \"_ctypes\" in ext_map:\\n            ctypes = extensions.pop(ext_map[\"_ctypes\"])\\n            extensions.append(ctypes)\\n        self.extensions = extensions\\n        srcdir = sysconfig.get_config_var('srcdir')\\n        if not srcdir:\\n            raise ValueError(\"No source directory; cannot proceed.\")\\n        srcdir = os.path.abspath(srcdir)\\n        moddirlist = [os.path.join(srcdir, 'Modules')]\\n        platform = self.get_platform()\\n        self.distribution.scripts = [os.path.join(srcdir, filename)\\n                                     for filename in self.distribution.scripts]\\n        headers = [sysconfig.get_config_h_filename()]\\n        headers += glob(os.path.join(sysconfig.get_python_inc(), \"*.h\"))\\n        for ext in self.extensions[:]:\\n            ext.sources = [ find_module_file(filename, moddirlist)\\n                            for filename in ext.sources ]\\n            if ext.depends is not None:\\n                ext.depends = [find_module_file(filename, moddirlist)\\n                               for filename in ext.depends]\\n            else:\\n                ext.depends = []\\n            ext.depends.extend(headers)\\n            if ext.name in sys.builtin_module_names:\\n                self.extensions.remove(ext)\\n        if platform != 'mac':\\n            remove_modules = []\\n            for filename in ('Modules/Setup', 'Modules/Setup.local'):\\n                input = text_file.TextFile(filename, join_lines=1)\\n                while 1:\\n                    line = input.readline()\\n                    if not line: break\\n                    line = line.split()\\n                    remove_modules.append(line[0])\\n                input.close()\\n            for ext in self.extensions[:]:\\n                if ext.name in remove_modules:\\n                    self.extensions.remove(ext)\\n        compiler = os.environ.get('CC')\\n        args = {}\\n        if compiler is not None:\\n            (ccshared,cflags) = sysconfig.get_config_vars('CCSHARED','CFLAGS')\\n            args['compiler_so'] = compiler + ' ' + ccshared + ' ' + cflags\\n        self.compiler_obj.set_executables(**args)\\n        build_ext.build_extensions(self)\\n        longest = max([len(e.name) for e in self.extensions])\\n        if self.failed:\\n            longest = max(longest, max([len(name) for name in self.failed]))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def build_extensions(self):\\n        missing = self.detect_modules()\\n        extensions = [ext for ext in self.extensions\\n                      if ext.name not in disabled_module_list]\\n        ext_map = dict((ext.name, i) for i, ext in enumerate(extensions))\\n        if \"_ctypes\" in ext_map:\\n            ctypes = extensions.pop(ext_map[\"_ctypes\"])\\n            extensions.append(ctypes)\\n        self.extensions = extensions\\n        srcdir = sysconfig.get_config_var('srcdir')\\n        if not srcdir:\\n            raise ValueError(\"No source directory; cannot proceed.\")\\n        srcdir = os.path.abspath(srcdir)\\n        moddirlist = [os.path.join(srcdir, 'Modules')]\\n        platform = self.get_platform()\\n        self.distribution.scripts = [os.path.join(srcdir, filename)\\n                                     for filename in self.distribution.scripts]\\n        headers = [sysconfig.get_config_h_filename()]\\n        headers += glob(os.path.join(sysconfig.get_python_inc(), \"*.h\"))\\n        for ext in self.extensions[:]:\\n            ext.sources = [ find_module_file(filename, moddirlist)\\n                            for filename in ext.sources ]\\n            if ext.depends is not None:\\n                ext.depends = [find_module_file(filename, moddirlist)\\n                               for filename in ext.depends]\\n            else:\\n                ext.depends = []\\n            ext.depends.extend(headers)\\n            if ext.name in sys.builtin_module_names:\\n                self.extensions.remove(ext)\\n        if platform != 'mac':\\n            remove_modules = []\\n            for filename in ('Modules/Setup', 'Modules/Setup.local'):\\n                input = text_file.TextFile(filename, join_lines=1)\\n                while 1:\\n                    line = input.readline()\\n                    if not line: break\\n                    line = line.split()\\n                    remove_modules.append(line[0])\\n                input.close()\\n            for ext in self.extensions[:]:\\n                if ext.name in remove_modules:\\n                    self.extensions.remove(ext)\\n        compiler = os.environ.get('CC')\\n        args = {}\\n        if compiler is not None:\\n            (ccshared,cflags) = sysconfig.get_config_vars('CCSHARED','CFLAGS')\\n            args['compiler_so'] = compiler + ' ' + ccshared + ' ' + cflags\\n        self.compiler_obj.set_executables(**args)\\n        build_ext.build_extensions(self)\\n        longest = max([len(e.name) for e in self.extensions])\\n        if self.failed:\\n            longest = max(longest, max([len(name) for name in self.failed]))"
  },
  {
    "code": "def _aggregate_named(self, applyfunc):\\n        result = {}\\n        for name in self.primary:\\n            grp = self.get_group(name)\\n            grp.groupName = name\\n            output = applyfunc(grp)\\n            if isinstance(output, Series):\\n                raise Exception('Given applyfunc did not return a '\\n                                'value from the subseries as expected!')\\n            result[name] = output\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_named(self, applyfunc):\\n        result = {}\\n        for name in self.primary:\\n            grp = self.get_group(name)\\n            grp.groupName = name\\n            output = applyfunc(grp)\\n            if isinstance(output, Series):\\n                raise Exception('Given applyfunc did not return a '\\n                                'value from the subseries as expected!')\\n            result[name] = output\\n        return result"
  },
  {
    "code": "def generate_take_cython_file():\\n    directory = os.path.dirname(os.path.realpath(__file__))\\n    filename = 'generated.pyx'\\n    path = os.path.join(directory, filename)\\n    with open(path, 'w') as f:\\n        print(warning_to_new_contributors, file=f)\\n        print(header, file=f)\\n        print(generate_ensure_dtypes(), file=f)\\n        for template in templates_1d:\\n            print(generate_from_template(template), file=f)\\n        for template in take_templates:\\n            print(generate_take_template(template), file=f)\\n        for template in put_2d:\\n            print(generate_put_template(template), file=f)\\n        for template in groupbys:\\n            print(generate_put_template(template, use_ints=False), file=f)\\n        for template in groupby_selection:\\n            print(generate_put_selection_template(template, use_ints=True),\\n                  file=f)\\n        for template in groupby_min_max:\\n            print(generate_put_min_max_template(template, use_ints=True),\\n                  file=f)\\n        for template in groupby_count:\\n            print(generate_put_selection_template(template, use_ints=True,\\n                                                  use_datelikes=True,\\n                                                  use_objects=True),\\n                                                  file=f)\\n        for template in nobool_1d_templates:\\n            print(generate_from_template(template, exclude=['bool']), file=f)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: removes cython implementation of groupby count",
    "fixed_code": "def generate_take_cython_file():\\n    directory = os.path.dirname(os.path.realpath(__file__))\\n    filename = 'generated.pyx'\\n    path = os.path.join(directory, filename)\\n    with open(path, 'w') as f:\\n        print(warning_to_new_contributors, file=f)\\n        print(header, file=f)\\n        print(generate_ensure_dtypes(), file=f)\\n        for template in templates_1d:\\n            print(generate_from_template(template), file=f)\\n        for template in take_templates:\\n            print(generate_take_template(template), file=f)\\n        for template in put_2d:\\n            print(generate_put_template(template), file=f)\\n        for template in groupbys:\\n            print(generate_put_template(template, use_ints=False), file=f)\\n        for template in groupby_selection:\\n            print(generate_put_selection_template(template, use_ints=True),\\n                  file=f)\\n        for template in groupby_min_max:\\n            print(generate_put_min_max_template(template, use_ints=True),\\n                  file=f)\\n        for template in nobool_1d_templates:\\n            print(generate_from_template(template, exclude=['bool']), file=f)"
  },
  {
    "code": "def get_nowait(self):\\n        self._consume_done_putters()\\n        if self._putters:\\n            assert self.full(), 'queue not full, why are putters waiting?'\\n            item, putter = self._putters.popleft()\\n            self._put(item)\\n            putter.set_result(None)\\n            return self._get()\\n        elif self.qsize():\\n            return self._get()\\n        else:\\n            raise Empty",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio: Rename {Empty,Full} to {QueueEmpty,QueueFull} and no longer get them from queue.py.",
    "fixed_code": "def get_nowait(self):\\n        self._consume_done_putters()\\n        if self._putters:\\n            assert self.full(), 'queue not full, why are putters waiting?'\\n            item, putter = self._putters.popleft()\\n            self._put(item)\\n            putter.set_result(None)\\n            return self._get()\\n        elif self.qsize():\\n            return self._get()\\n        else:\\n            raise QueueEmpty"
  },
  {
    "code": "def _set_new_attribute(cls, name, value):\\n    if name in cls.__dict__:\\n        return True\\n    _set_qualname(cls, value)\\n    setattr(cls, name, value)\\n    return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _set_new_attribute(cls, name, value):\\n    if name in cls.__dict__:\\n        return True\\n    _set_qualname(cls, value)\\n    setattr(cls, name, value)\\n    return False"
  },
  {
    "code": "def patterns(prefix, *tuples):\\n    pattern_list = []\\n    for t in tuples:\\n        regex, view_or_include = t[:2]\\n        default_kwargs = t[2:]\\n        if type(view_or_include) == list:\\n            pattern_list.append(RegexURLResolver(regex, view_or_include[0], *default_kwargs))\\n        else:\\n            pattern_list.append(RegexURLPattern(regex, prefix and (prefix + '.' + view_or_include) or view_or_include, *default_kwargs))\\n    return pattern_list",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added the ability to name URL patterns. Helps with disambiguity reverse matches.",
    "fixed_code": "def patterns(prefix, *args):\\n    pattern_list = []\\n    for t in args:\\n        if isinstance(t, (list, tuple)):\\n            pattern_list.append(url(prefix=prefix, *t))\\n        else:\\n            pattern_list.append(t)\\n    return pattern_list"
  },
  {
    "code": "def __setitem__(self, key, value):\\n        if isinstance(key, DataFrame):\\n            if not (key.index.equals(self.index) and\\n                    key.columns.equals(self.columns)):\\n                raise PandasError('Can only index with like-indexed '\\n                                  'DataFrame objects')\\n            self._boolean_set(key, value)\\n        else:\\n            self._insert_item(key, value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __setitem__(self, key, value):\\n        if isinstance(key, DataFrame):\\n            if not (key.index.equals(self.index) and\\n                    key.columns.equals(self.columns)):\\n                raise PandasError('Can only index with like-indexed '\\n                                  'DataFrame objects')\\n            self._boolean_set(key, value)\\n        else:\\n            self._insert_item(key, value)"
  },
  {
    "code": "def __init__(self,\\n                 datastore_conn_id='google_cloud_default',\\n                 delegate_to=None,\\n                 api_version='v1'):\\n        super(DatastoreHook, self).__init__(datastore_conn_id, delegate_to)\\n        self.connection = None\\n        self.api_version = api_version",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self,\\n                 datastore_conn_id='google_cloud_default',\\n                 delegate_to=None,\\n                 api_version='v1'):\\n        super(DatastoreHook, self).__init__(datastore_conn_id, delegate_to)\\n        self.connection = None\\n        self.api_version = api_version"
  },
  {
    "code": "def parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None):\\n    default_vault_id = default_vault_id or C.DEFAULT_VAULT_IDENTITY\\n    b_tmpdata = b_vaulttext_envelope.splitlines()\\n    b_tmpheader = b_tmpdata[0].strip().split(b';')\\n    b_version = b_tmpheader[1].strip()\\n    cipher_name = to_text(b_tmpheader[2].strip())\\n    vault_id = default_vault_id\\n    if len(b_tmpheader) >= 4:\\n        vault_id = to_text(b_tmpheader[3].strip())\\n    b_ciphertext = b''.join(b_tmpdata[1:])\\n    return b_ciphertext, b_version, cipher_name, vault_id",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None):\\n    default_vault_id = default_vault_id or C.DEFAULT_VAULT_IDENTITY\\n    b_tmpdata = b_vaulttext_envelope.splitlines()\\n    b_tmpheader = b_tmpdata[0].strip().split(b';')\\n    b_version = b_tmpheader[1].strip()\\n    cipher_name = to_text(b_tmpheader[2].strip())\\n    vault_id = default_vault_id\\n    if len(b_tmpheader) >= 4:\\n        vault_id = to_text(b_tmpheader[3].strip())\\n    b_ciphertext = b''.join(b_tmpdata[1:])\\n    return b_ciphertext, b_version, cipher_name, vault_id"
  },
  {
    "code": "def nested_to_record(ds, prefix=\"\", level=0):\\n    singleton = False\\n    if isinstance(ds, dict):\\n        ds = [ds]\\n        singleton = True\\n    new_ds = []\\n    for d in ds:\\n        new_d = copy.deepcopy(d)\\n        for k, v in d.items():\\n            if not isinstance(k, compat.string_types):\\n                k = str(k)\\n            if level == 0:\\n                newkey = k\\n            else:\\n                newkey = prefix + '.' + k\\n            if not isinstance(v, dict):\\n                if level != 0:  \\n                    v = new_d.pop(k)\\n                    new_d[newkey] = v\\n                continue\\n            else:\\n                v = new_d.pop(k)\\n                new_d.update(nested_to_record(v, newkey, level + 1))\\n        new_ds.append(new_d)\\n    if singleton:\\n        return new_ds[0]\\n    return new_ds",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: GH14883: json_normalize now takes a user-specified separator\\n\\ncloses #14883\\n\\nAuthor: Jeff Reback <jeff@reback.net>\\nAuthor: John Owens <jowens@ece.ucdavis.edu>\\n\\nCloses #14950 from jowens/json_normalize-separator and squashes the following commits:\\n\\n0327dd1 [Jeff Reback] compare sorted columns\\nbc5aae8 [Jeff Reback] CLN: fixup json_normalize with sep\\n8edc40e [John Owens] ENH: json_normalize now takes a user-specified separator",
    "fixed_code": "def nested_to_record(ds, prefix=\"\", sep=\".\", level=0):\\n    singleton = False\\n    if isinstance(ds, dict):\\n        ds = [ds]\\n        singleton = True\\n    new_ds = []\\n    for d in ds:\\n        new_d = copy.deepcopy(d)\\n        for k, v in d.items():\\n            if not isinstance(k, compat.string_types):\\n                k = str(k)\\n            if level == 0:\\n                newkey = k\\n            else:\\n                newkey = prefix + sep + k\\n            if not isinstance(v, dict):\\n                if level != 0:  \\n                    v = new_d.pop(k)\\n                    new_d[newkey] = v\\n                continue\\n            else:\\n                v = new_d.pop(k)\\n                new_d.update(nested_to_record(v, newkey, sep, level + 1))\\n        new_ds.append(new_d)\\n    if singleton:\\n        return new_ds[0]\\n    return new_ds"
  },
  {
    "code": "def get_array_facts(self):\\n        facts = dict(facts_from_proxy=(not self.is_embedded()), ssid=self.ssid)\\n        controller_reference_label = self.get_controllers()\\n        array_facts = None\\n        try:\\n            rc, array_facts = self.request(\"storage-systems/%s/graph\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to obtain facts from storage array with id [%s]. Error [%s]\" % (self.ssid, str(error)))\\n        facts['netapp_storage_array'] = dict(\\n            name=array_facts['sa']['saData']['storageArrayLabel'],\\n            chassis_serial=array_facts['sa']['saData']['chassisSerialNumber'],\\n            firmware=array_facts['sa']['saData']['fwVersion'],\\n            wwn=array_facts['sa']['saData']['saId']['worldWideName'],\\n            segment_sizes=array_facts['sa']['featureParameters']['supportedSegSizes'],\\n            cache_block_sizes=array_facts['sa']['featureParameters']['cacheBlockSizes'])\\n        facts['netapp_controllers'] = [\\n            dict(\\n                name=controller_reference_label[controller['controllerRef']],\\n                serial=controller['serialNumber'].strip(),\\n                status=controller['status'],\\n            ) for controller in array_facts['controller']]\\n        facts['netapp_host_groups'] = [\\n            dict(\\n                id=group['id'],\\n                name=group['name']\\n            ) for group in array_facts['storagePoolBundle']['cluster']]\\n        facts['netapp_hosts'] = [\\n            dict(\\n                group_id=host['clusterRef'],\\n                hosts_reference=host['hostRef'],\\n                id=host['id'],\\n                name=host['name'],\\n                host_type_index=host['hostTypeIndex'],\\n                posts=host['hostSidePorts']\\n            ) for host in array_facts['storagePoolBundle']['host']]\\n        facts['netapp_host_types'] = [\\n            dict(\\n                type=host_type['hostType'],\\n                index=host_type['index']\\n            ) for host_type in array_facts['sa']['hostSpecificVals']\\n            if 'hostType' in host_type.keys() and host_type['hostType']\\n        ]\\n        facts['snapshot_images'] = [\\n            dict(\\n                id=snapshot['id'],\\n                status=snapshot['status'],\\n                pit_capacity=snapshot['pitCapacity'],\\n                creation_method=snapshot['creationMethod'],\\n                reposity_cap_utilization=snapshot['repositoryCapacityUtilization'],\\n                active_cow=snapshot['activeCOW'],\\n                rollback_source=snapshot['isRollbackSource']\\n            ) for snapshot in array_facts['highLevelVolBundle']['pit']]\\n        facts['netapp_disks'] = [\\n            dict(\\n                id=disk['id'],\\n                available=disk['available'],\\n                media_type=disk['driveMediaType'],\\n                status=disk['status'],\\n                usable_bytes=disk['usableCapacity'],\\n                tray_ref=disk['physicalLocation']['trayRef'],\\n                product_id=disk['productID'],\\n                firmware_version=disk['firmwareVersion'],\\n                serial_number=disk['serialNumber'].lstrip()\\n            ) for disk in array_facts['drive']]\\n        facts['netapp_management_interfaces'] = [\\n            dict(controller=controller_reference_label[controller['controllerRef']],\\n                 name=iface['ethernet']['interfaceName'],\\n                 alias=iface['ethernet']['alias'],\\n                 channel=iface['ethernet']['channel'],\\n                 mac_address=iface['ethernet']['macAddr'],\\n                 remote_ssh_access=iface['ethernet']['rloginEnabled'],\\n                 link_status=iface['ethernet']['linkStatus'],\\n                 ipv4_enabled=iface['ethernet']['ipv4Enabled'],\\n                 ipv4_address_config_method=iface['ethernet']['ipv4AddressConfigMethod'].lower().replace(\"config\", \"\"),\\n                 ipv4_address=iface['ethernet']['ipv4Address'],\\n                 ipv4_subnet_mask=iface['ethernet']['ipv4SubnetMask'],\\n                 ipv4_gateway=iface['ethernet']['ipv4GatewayAddress'],\\n                 ipv6_enabled=iface['ethernet']['ipv6Enabled'],\\n                 dns_config_method=iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsAcquisitionType'],\\n                 dns_servers=(iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers']\\n                              if iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers'] else []),\\n                 ntp_config_method=iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpAcquisitionType'],\\n                 ntp_servers=(iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers']\\n                              if iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers'] else [])\\n                 ) for controller in array_facts['controller'] for iface in controller['netInterfaces']]\\n        facts['netapp_hostside_interfaces'] = [\\n            dict(\\n                fc=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['fibre']['channel'],\\n                         link_status=iface['fibre']['linkStatus'],\\n                         current_interface_speed=strip_interface_speed(iface['fibre']['currentInterfaceSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['fibre']['maximumInterfaceSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'fc'],\\n                ib=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['ib']['channel'],\\n                         link_status=iface['ib']['linkState'],\\n                         mtu=iface['ib']['maximumTransmissionUnit'],\\n                         current_interface_speed=strip_interface_speed(iface['ib']['currentSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['ib']['supportedSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'ib'],\\n                iscsi=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                            iqn=iface['iscsi']['iqn'],\\n                            link_status=iface['iscsi']['interfaceData']['ethernetData']['linkStatus'],\\n                            ipv4_enabled=iface['iscsi']['ipv4Enabled'],\\n                            ipv4_address=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4Address'],\\n                            ipv4_subnet_mask=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4SubnetMask'],\\n                            ipv4_gateway=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4GatewayAddress'],\\n                            ipv6_enabled=iface['iscsi']['ipv6Enabled'],\\n                            mtu=iface['iscsi']['interfaceData']['ethernetData']['maximumFramePayloadSize'],\\n                            current_interface_speed=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                          ['ethernetData']['currentInterfaceSpeed']),\\n                            supported_interface_speeds=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                             ['ethernetData']\\n                                                                             ['supportedInterfaceSpeeds']))\\n                       for controller in array_facts['controller']\\n                       for iface in controller['hostInterfaces']\\n                       if iface['interfaceType'] == 'iscsi'],\\n                sas=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                          channel=iface['sas']['channel'],\\n                          current_interface_speed=strip_interface_speed(iface['sas']['currentInterfaceSpeed']),\\n                          maximum_interface_speed=strip_interface_speed(iface['sas']['maximumInterfaceSpeed']),\\n                          link_status=iface['sas']['iocPort']['state'])\\n                     for controller in array_facts['controller']\\n                     for iface in controller['hostInterfaces']\\n                     if iface['interfaceType'] == 'sas'])]\\n        facts['netapp_driveside_interfaces'] = [\\n            dict(\\n                controller=controller_reference_label[controller['controllerRef']],\\n                interface_type=interface['interfaceType'],\\n                interface_speed=strip_interface_speed(\\n                    interface[interface['interfaceType']]['maximumInterfaceSpeed']\\n                    if (interface['interfaceType'] == 'sata' or\\n                        interface['interfaceType'] == 'sas' or\\n                        interface['interfaceType'] == 'fibre')\\n                    else (\\n                        interface[interface['interfaceType']]['currentSpeed']\\n                        if interface['interfaceType'] == 'ib'\\n                        else (\\n                            interface[interface['interfaceType']]['interfaceData']['maximumInterfaceSpeed']\\n                            if interface['interfaceType'] == 'iscsi' else 'unknown'\\n                        ))),\\n            )\\n            for controller in array_facts['controller']\\n            for interface in controller['driveInterfaces']]\\n        facts['netapp_storage_pools'] = [\\n            dict(\\n                id=storage_pool['id'],\\n                name=storage_pool['name'],\\n                available_capacity=storage_pool['freeSpace'],\\n                total_capacity=storage_pool['totalRaidedSpace'],\\n                used_capacity=storage_pool['usedSpace']\\n            ) for storage_pool in array_facts['volumeGroup']]\\n        all_volumes = list(array_facts['volume'])\\n        facts['netapp_volumes'] = [\\n            dict(\\n                id=v['id'],\\n                name=v['name'],\\n                parent_storage_pool_id=v['volumeGroupRef'],\\n                capacity=v['capacity'],\\n                is_thin_provisioned=v['thinProvisioned'],\\n                workload=v['metadata'],\\n            ) for v in all_volumes]\\n        workload_tags = None\\n        try:\\n            rc, workload_tags = self.request(\"storage-systems/%s/workloads\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to retrieve workload tags. Array [%s].\" % self.ssid)\\n        facts['netapp_workload_tags'] = [\\n            dict(\\n                id=workload_tag['id'],\\n                name=workload_tag['name'],\\n                attributes=workload_tag['workloadAttributes']\\n            ) for workload_tag in workload_tags]\\n        facts['netapp_volumes_by_initiators'] = dict()\\n        for mapping in array_facts['storagePoolBundle']['lunMapping']:\\n            for host in facts['netapp_hosts']:\\n                if mapping['mapRef'] == host['hosts_reference'] or mapping['mapRef'] == host['group_id']:\\n                    if host['name'] not in facts['netapp_volumes_by_initiators'].keys():\\n                        facts['netapp_volumes_by_initiators'].update({host['name']: []})\\n                    for volume in all_volumes:\\n                        if mapping['id'] in [volume_mapping['id'] for volume_mapping in volume['listOfMappings']]:\\n                            workload_name = \"\"\\n                            metadata = dict()\\n                            for volume_tag in volume['metadata']:\\n                                if volume_tag['key'] == 'workloadId':\\n                                    for workload_tag in facts['netapp_workload_tags']:\\n                                        if volume_tag['value'] == workload_tag['id']:\\n                                            workload_name = workload_tag['name']\\n                                            metadata = dict((entry['key'], entry['value'])\\n                                                            for entry in workload_tag['attributes']\\n                                                            if entry['key'] != 'profileId')\\n                            facts['netapp_volumes_by_initiators'][host['name']].append(\\n                                dict(name=volume['name'],\\n                                     id=volume['id'],\\n                                     wwn=volume['wwn'],\\n                                     workload_name=workload_name,\\n                                     meta_data=metadata))\\n        features = [feature for feature in array_facts['sa']['capabilities']]\\n        features.extend([feature['capability'] for feature in array_facts['sa']['premiumFeatures']\\n                         if feature['isEnabled']])\\n        features = list(set(features))  \\n        features.sort()\\n        facts['netapp_enabled_features'] = features\\n        return facts",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_array_facts(self):\\n        facts = dict(facts_from_proxy=(not self.is_embedded()), ssid=self.ssid)\\n        controller_reference_label = self.get_controllers()\\n        array_facts = None\\n        try:\\n            rc, array_facts = self.request(\"storage-systems/%s/graph\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to obtain facts from storage array with id [%s]. Error [%s]\" % (self.ssid, str(error)))\\n        facts['netapp_storage_array'] = dict(\\n            name=array_facts['sa']['saData']['storageArrayLabel'],\\n            chassis_serial=array_facts['sa']['saData']['chassisSerialNumber'],\\n            firmware=array_facts['sa']['saData']['fwVersion'],\\n            wwn=array_facts['sa']['saData']['saId']['worldWideName'],\\n            segment_sizes=array_facts['sa']['featureParameters']['supportedSegSizes'],\\n            cache_block_sizes=array_facts['sa']['featureParameters']['cacheBlockSizes'])\\n        facts['netapp_controllers'] = [\\n            dict(\\n                name=controller_reference_label[controller['controllerRef']],\\n                serial=controller['serialNumber'].strip(),\\n                status=controller['status'],\\n            ) for controller in array_facts['controller']]\\n        facts['netapp_host_groups'] = [\\n            dict(\\n                id=group['id'],\\n                name=group['name']\\n            ) for group in array_facts['storagePoolBundle']['cluster']]\\n        facts['netapp_hosts'] = [\\n            dict(\\n                group_id=host['clusterRef'],\\n                hosts_reference=host['hostRef'],\\n                id=host['id'],\\n                name=host['name'],\\n                host_type_index=host['hostTypeIndex'],\\n                posts=host['hostSidePorts']\\n            ) for host in array_facts['storagePoolBundle']['host']]\\n        facts['netapp_host_types'] = [\\n            dict(\\n                type=host_type['hostType'],\\n                index=host_type['index']\\n            ) for host_type in array_facts['sa']['hostSpecificVals']\\n            if 'hostType' in host_type.keys() and host_type['hostType']\\n        ]\\n        facts['snapshot_images'] = [\\n            dict(\\n                id=snapshot['id'],\\n                status=snapshot['status'],\\n                pit_capacity=snapshot['pitCapacity'],\\n                creation_method=snapshot['creationMethod'],\\n                reposity_cap_utilization=snapshot['repositoryCapacityUtilization'],\\n                active_cow=snapshot['activeCOW'],\\n                rollback_source=snapshot['isRollbackSource']\\n            ) for snapshot in array_facts['highLevelVolBundle']['pit']]\\n        facts['netapp_disks'] = [\\n            dict(\\n                id=disk['id'],\\n                available=disk['available'],\\n                media_type=disk['driveMediaType'],\\n                status=disk['status'],\\n                usable_bytes=disk['usableCapacity'],\\n                tray_ref=disk['physicalLocation']['trayRef'],\\n                product_id=disk['productID'],\\n                firmware_version=disk['firmwareVersion'],\\n                serial_number=disk['serialNumber'].lstrip()\\n            ) for disk in array_facts['drive']]\\n        facts['netapp_management_interfaces'] = [\\n            dict(controller=controller_reference_label[controller['controllerRef']],\\n                 name=iface['ethernet']['interfaceName'],\\n                 alias=iface['ethernet']['alias'],\\n                 channel=iface['ethernet']['channel'],\\n                 mac_address=iface['ethernet']['macAddr'],\\n                 remote_ssh_access=iface['ethernet']['rloginEnabled'],\\n                 link_status=iface['ethernet']['linkStatus'],\\n                 ipv4_enabled=iface['ethernet']['ipv4Enabled'],\\n                 ipv4_address_config_method=iface['ethernet']['ipv4AddressConfigMethod'].lower().replace(\"config\", \"\"),\\n                 ipv4_address=iface['ethernet']['ipv4Address'],\\n                 ipv4_subnet_mask=iface['ethernet']['ipv4SubnetMask'],\\n                 ipv4_gateway=iface['ethernet']['ipv4GatewayAddress'],\\n                 ipv6_enabled=iface['ethernet']['ipv6Enabled'],\\n                 dns_config_method=iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsAcquisitionType'],\\n                 dns_servers=(iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers']\\n                              if iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers'] else []),\\n                 ntp_config_method=iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpAcquisitionType'],\\n                 ntp_servers=(iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers']\\n                              if iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers'] else [])\\n                 ) for controller in array_facts['controller'] for iface in controller['netInterfaces']]\\n        facts['netapp_hostside_interfaces'] = [\\n            dict(\\n                fc=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['fibre']['channel'],\\n                         link_status=iface['fibre']['linkStatus'],\\n                         current_interface_speed=strip_interface_speed(iface['fibre']['currentInterfaceSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['fibre']['maximumInterfaceSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'fc'],\\n                ib=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['ib']['channel'],\\n                         link_status=iface['ib']['linkState'],\\n                         mtu=iface['ib']['maximumTransmissionUnit'],\\n                         current_interface_speed=strip_interface_speed(iface['ib']['currentSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['ib']['supportedSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'ib'],\\n                iscsi=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                            iqn=iface['iscsi']['iqn'],\\n                            link_status=iface['iscsi']['interfaceData']['ethernetData']['linkStatus'],\\n                            ipv4_enabled=iface['iscsi']['ipv4Enabled'],\\n                            ipv4_address=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4Address'],\\n                            ipv4_subnet_mask=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4SubnetMask'],\\n                            ipv4_gateway=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4GatewayAddress'],\\n                            ipv6_enabled=iface['iscsi']['ipv6Enabled'],\\n                            mtu=iface['iscsi']['interfaceData']['ethernetData']['maximumFramePayloadSize'],\\n                            current_interface_speed=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                          ['ethernetData']['currentInterfaceSpeed']),\\n                            supported_interface_speeds=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                             ['ethernetData']\\n                                                                             ['supportedInterfaceSpeeds']))\\n                       for controller in array_facts['controller']\\n                       for iface in controller['hostInterfaces']\\n                       if iface['interfaceType'] == 'iscsi'],\\n                sas=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                          channel=iface['sas']['channel'],\\n                          current_interface_speed=strip_interface_speed(iface['sas']['currentInterfaceSpeed']),\\n                          maximum_interface_speed=strip_interface_speed(iface['sas']['maximumInterfaceSpeed']),\\n                          link_status=iface['sas']['iocPort']['state'])\\n                     for controller in array_facts['controller']\\n                     for iface in controller['hostInterfaces']\\n                     if iface['interfaceType'] == 'sas'])]\\n        facts['netapp_driveside_interfaces'] = [\\n            dict(\\n                controller=controller_reference_label[controller['controllerRef']],\\n                interface_type=interface['interfaceType'],\\n                interface_speed=strip_interface_speed(\\n                    interface[interface['interfaceType']]['maximumInterfaceSpeed']\\n                    if (interface['interfaceType'] == 'sata' or\\n                        interface['interfaceType'] == 'sas' or\\n                        interface['interfaceType'] == 'fibre')\\n                    else (\\n                        interface[interface['interfaceType']]['currentSpeed']\\n                        if interface['interfaceType'] == 'ib'\\n                        else (\\n                            interface[interface['interfaceType']]['interfaceData']['maximumInterfaceSpeed']\\n                            if interface['interfaceType'] == 'iscsi' else 'unknown'\\n                        ))),\\n            )\\n            for controller in array_facts['controller']\\n            for interface in controller['driveInterfaces']]\\n        facts['netapp_storage_pools'] = [\\n            dict(\\n                id=storage_pool['id'],\\n                name=storage_pool['name'],\\n                available_capacity=storage_pool['freeSpace'],\\n                total_capacity=storage_pool['totalRaidedSpace'],\\n                used_capacity=storage_pool['usedSpace']\\n            ) for storage_pool in array_facts['volumeGroup']]\\n        all_volumes = list(array_facts['volume'])\\n        facts['netapp_volumes'] = [\\n            dict(\\n                id=v['id'],\\n                name=v['name'],\\n                parent_storage_pool_id=v['volumeGroupRef'],\\n                capacity=v['capacity'],\\n                is_thin_provisioned=v['thinProvisioned'],\\n                workload=v['metadata'],\\n            ) for v in all_volumes]\\n        workload_tags = None\\n        try:\\n            rc, workload_tags = self.request(\"storage-systems/%s/workloads\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to retrieve workload tags. Array [%s].\" % self.ssid)\\n        facts['netapp_workload_tags'] = [\\n            dict(\\n                id=workload_tag['id'],\\n                name=workload_tag['name'],\\n                attributes=workload_tag['workloadAttributes']\\n            ) for workload_tag in workload_tags]\\n        facts['netapp_volumes_by_initiators'] = dict()\\n        for mapping in array_facts['storagePoolBundle']['lunMapping']:\\n            for host in facts['netapp_hosts']:\\n                if mapping['mapRef'] == host['hosts_reference'] or mapping['mapRef'] == host['group_id']:\\n                    if host['name'] not in facts['netapp_volumes_by_initiators'].keys():\\n                        facts['netapp_volumes_by_initiators'].update({host['name']: []})\\n                    for volume in all_volumes:\\n                        if mapping['id'] in [volume_mapping['id'] for volume_mapping in volume['listOfMappings']]:\\n                            workload_name = \"\"\\n                            metadata = dict()\\n                            for volume_tag in volume['metadata']:\\n                                if volume_tag['key'] == 'workloadId':\\n                                    for workload_tag in facts['netapp_workload_tags']:\\n                                        if volume_tag['value'] == workload_tag['id']:\\n                                            workload_name = workload_tag['name']\\n                                            metadata = dict((entry['key'], entry['value'])\\n                                                            for entry in workload_tag['attributes']\\n                                                            if entry['key'] != 'profileId')\\n                            facts['netapp_volumes_by_initiators'][host['name']].append(\\n                                dict(name=volume['name'],\\n                                     id=volume['id'],\\n                                     wwn=volume['wwn'],\\n                                     workload_name=workload_name,\\n                                     meta_data=metadata))\\n        features = [feature for feature in array_facts['sa']['capabilities']]\\n        features.extend([feature['capability'] for feature in array_facts['sa']['premiumFeatures']\\n                         if feature['isEnabled']])\\n        features = list(set(features))  \\n        features.sort()\\n        facts['netapp_enabled_features'] = features\\n        return facts"
  },
  {
    "code": "def active_member_count(self):\\n        return int(self._values['stats']['activeMemberCnt'])\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes issue with sub collection for pool members (#53955)\\n\\nRefactors main() function and module manager in multiple modules in line with recent changes\\nAdds variable types to docs\\nRefactors unit tests to remove deprecated parameters",
    "fixed_code": "def active_member_count(self):\\n        if 'availableMemberCnt' in self._values['stats']:\\n            return int(self._values['stats']['activeMemberCnt'])\\n        return None\\n    @property"
  },
  {
    "code": "def _evaluate_compare(self, other, op):\\n        if not isinstance(other, type(self)):\\n            if not is_list_like(other):\\n                other = [other]\\n            elif is_scalar(lib.item_from_zerodim(other)):\\n                other = [other.item()]\\n            other = type(self)(other)\\n        result = op(self.asi8, other.asi8)\\n        mask = (self._isnan) | (other._isnan)\\n        if is_bool_dtype(result):\\n            result[mask] = False\\n            return result\\n        result[mask] = iNaT\\n        try:\\n            return Index(result)\\n        except TypeError:\\n            return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[REF] Move comparison methods to EAMixins, share code (#21872)",
    "fixed_code": "def _evaluate_compare(self, other, op):\\n        result = DatetimeLikeArrayMixin._evaluate_compare(self, other, op)\\n        if is_bool_dtype(result):\\n            return result\\n        try:\\n            return Index(result)\\n        except TypeError:\\n            return result"
  },
  {
    "code": "def check_archive_formats(formats):\\n    for format in formats:\\n        if format not in ARCHIVE_FORMATS:\\n            return format\\n    return None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_archive_formats(formats):\\n    for format in formats:\\n        if format not in ARCHIVE_FORMATS:\\n            return format\\n    return None"
  },
  {
    "code": "def as_sql(self, qn, connection):\\n\\t\\tlhs, lhs_params = self.process_lhs(qn, connection)\\n\\t\\trhs, rhs_params = self.process_rhs(qn, connection)\\n\\t\\tparams = lhs_params + rhs_params\\n\\t\\treturn '%s %s %s' % (lhs, self.operator, rhs), params",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #30826 -- Fixed crash of many JSONField lookups when one hand side is key transform.\\n\\nRegression in 6c3dfba89215fc56fc27ef61829a6fff88be4abb.",
    "fixed_code": "def as_sql(self, qn, connection):\\n\\t\\tlhs, lhs_params = self.process_lhs(qn, connection)\\n\\t\\trhs, rhs_params = self.process_rhs(qn, connection)\\n\\t\\tparams = tuple(lhs_params) + tuple(rhs_params)\\n\\t\\treturn '%s %s %s' % (lhs, self.operator, rhs), params"
  },
  {
    "code": "def read_parquet(path, engine: str = \"auto\", columns=None, **kwargs):\\n    impl = get_engine(engine)\\n    return impl.read(path, columns=columns, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add use_nullable_dtypes option in read_parquet (#31242)",
    "fixed_code": "def read_parquet(\\n    path,\\n    engine: str = \"auto\",\\n    columns=None,\\n    use_nullable_dtypes: bool = False,\\n    **kwargs,\\n):\\n    impl = get_engine(engine)\\n    return impl.read(\\n        path, columns=columns, use_nullable_dtypes=use_nullable_dtypes, **kwargs\\n    )"
  },
  {
    "code": "def quantize(self, exp, rounding=None, context=None, watchexp=True):\\n        exp = _convert_other(exp, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if rounding is None:\\n            rounding = context.rounding\\n        if self._is_special or exp._is_special:\\n            ans = self._check_nans(exp, context)\\n            if ans:\\n                return ans\\n            if exp._isinfinity() or self._isinfinity():\\n                if exp._isinfinity() and self._isinfinity():\\n                    return Decimal(self)  \\n                return context._raise_error(InvalidOperation,\\n                                        'quantize with one INF')\\n        if not watchexp:\\n            ans = self._rescale(exp._exp, rounding)\\n            if ans._exp > self._exp:\\n                context._raise_error(Rounded)\\n                if ans != self:\\n                    context._raise_error(Inexact)\\n            return ans\\n        if not (context.Etiny() <= exp._exp <= context.Emax):\\n            return context._raise_error(InvalidOperation,\\n                   'target exponent out of bounds in quantize')\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', exp._exp)\\n            return ans._fix(context)\\n        self_adjusted = self.adjusted()\\n        if self_adjusted > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if self_adjusted - exp._exp + 1 > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        ans = self._rescale(exp._exp, rounding)\\n        if ans.adjusted() > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if len(ans._int) > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        if ans._exp > self._exp:\\n            context._raise_error(Rounded)\\n            if ans != self:\\n                context._raise_error(Inexact)\\n        if ans and ans.adjusted() < context.Emin:\\n            context._raise_error(Subnormal)\\n        ans = ans._fix(context)\\n        return ans",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quantize(self, exp, rounding=None, context=None, watchexp=True):\\n        exp = _convert_other(exp, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if rounding is None:\\n            rounding = context.rounding\\n        if self._is_special or exp._is_special:\\n            ans = self._check_nans(exp, context)\\n            if ans:\\n                return ans\\n            if exp._isinfinity() or self._isinfinity():\\n                if exp._isinfinity() and self._isinfinity():\\n                    return Decimal(self)  \\n                return context._raise_error(InvalidOperation,\\n                                        'quantize with one INF')\\n        if not watchexp:\\n            ans = self._rescale(exp._exp, rounding)\\n            if ans._exp > self._exp:\\n                context._raise_error(Rounded)\\n                if ans != self:\\n                    context._raise_error(Inexact)\\n            return ans\\n        if not (context.Etiny() <= exp._exp <= context.Emax):\\n            return context._raise_error(InvalidOperation,\\n                   'target exponent out of bounds in quantize')\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', exp._exp)\\n            return ans._fix(context)\\n        self_adjusted = self.adjusted()\\n        if self_adjusted > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if self_adjusted - exp._exp + 1 > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        ans = self._rescale(exp._exp, rounding)\\n        if ans.adjusted() > context.Emax:\\n            return context._raise_error(InvalidOperation,\\n                                        'exponent of quantize result too large for current context')\\n        if len(ans._int) > context.prec:\\n            return context._raise_error(InvalidOperation,\\n                                        'quantize result has too many digits for current context')\\n        if ans._exp > self._exp:\\n            context._raise_error(Rounded)\\n            if ans != self:\\n                context._raise_error(Inexact)\\n        if ans and ans.adjusted() < context.Emin:\\n            context._raise_error(Subnormal)\\n        ans = ans._fix(context)\\n        return ans"
  },
  {
    "code": "def diff(arr, n, axis=0):\\n\\tn = int(n)\\n\\tdtype = arr.dtype\\n\\tif issubclass(dtype.type, np.integer):\\n\\t\\tdtype = np.float64\\n\\telif issubclass(dtype.type, np.bool_):\\n\\t\\tdtype = np.object_\\n\\tout_arr = np.empty(arr.shape, dtype=dtype)\\n\\tna_indexer = [slice(None)] * arr.ndim\\n\\tna_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n\\tout_arr[tuple(na_indexer)] = np.nan\\n\\tif arr.ndim == 2 and arr.dtype.name in _diff_special:\\n\\t\\tf = _diff_special[arr.dtype.name]\\n\\t\\tf(arr, out_arr, n, axis)\\n\\telse:\\n\\t\\tres_indexer = [slice(None)] * arr.ndim\\n\\t\\tres_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n\\t\\tres_indexer = tuple(res_indexer)\\n\\t\\tlag_indexer = [slice(None)] * arr.ndim\\n\\t\\tlag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n\\t\\tlag_indexer = tuple(lag_indexer)\\n\\t\\tout_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n\\treturn out_arr",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def diff(arr, n, axis=0):\\n\\tn = int(n)\\n\\tdtype = arr.dtype\\n\\tif issubclass(dtype.type, np.integer):\\n\\t\\tdtype = np.float64\\n\\telif issubclass(dtype.type, np.bool_):\\n\\t\\tdtype = np.object_\\n\\tout_arr = np.empty(arr.shape, dtype=dtype)\\n\\tna_indexer = [slice(None)] * arr.ndim\\n\\tna_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n\\tout_arr[tuple(na_indexer)] = np.nan\\n\\tif arr.ndim == 2 and arr.dtype.name in _diff_special:\\n\\t\\tf = _diff_special[arr.dtype.name]\\n\\t\\tf(arr, out_arr, n, axis)\\n\\telse:\\n\\t\\tres_indexer = [slice(None)] * arr.ndim\\n\\t\\tres_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n\\t\\tres_indexer = tuple(res_indexer)\\n\\t\\tlag_indexer = [slice(None)] * arr.ndim\\n\\t\\tlag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n\\t\\tlag_indexer = tuple(lag_indexer)\\n\\t\\tout_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n\\treturn out_arr"
  },
  {
    "code": "def __init__(self, n_components='auto', density='auto', eps=0.1,\\n                 random_state=None):\\n        self.n_components = n_components\\n        self.density = density\\n        self.eps = eps\\n        self.random_state = random_state",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "change API to enforce dense_output representation by default",
    "fixed_code": "def __init__(self, n_components='auto', density='auto', eps=0.1,\\n                 dense_output=True, random_state=None):\\n        self.n_components = n_components\\n        self.density = density\\n        self.eps = eps\\n        self.dense_output = dense_output\\n        self.random_state = random_state"
  },
  {
    "code": "def populate_facts(self, connection, ansible_facts, data=None):\\n        if not data:\\n            data = self.get_device_data(connection)\\n        resource_delim = 'lacp'\\n        find_pattern = r'(?:^|\\n)%s.*?(?=(?:^|\\n)%s|$)' % (resource_delim,\\n                                                           resource_delim)\\n        resources = [p.strip() for p in re.findall(find_pattern, data, re.DOTALL)]\\n        objs = {}\\n        for resource in resources:\\n            if resource:\\n                obj = self.render_config(self.generated_spec, resource)\\n                if obj:\\n                    objs.update(obj)\\n        ansible_facts['ansible_network_resources'].pop('lacp', None)\\n        facts = {'lacp': {}}\\n        if objs:\\n            params = utils.validate_config(self.argument_spec, {'config': objs})\\n            facts['lacp'] = utils.remove_empties(params['config'])\\n        ansible_facts['ansible_network_resources'].update(facts)\\n        return ansible_facts",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def populate_facts(self, connection, ansible_facts, data=None):\\n        if not data:\\n            data = self.get_device_data(connection)\\n        resource_delim = 'lacp'\\n        find_pattern = r'(?:^|\\n)%s.*?(?=(?:^|\\n)%s|$)' % (resource_delim,\\n                                                           resource_delim)\\n        resources = [p.strip() for p in re.findall(find_pattern, data, re.DOTALL)]\\n        objs = {}\\n        for resource in resources:\\n            if resource:\\n                obj = self.render_config(self.generated_spec, resource)\\n                if obj:\\n                    objs.update(obj)\\n        ansible_facts['ansible_network_resources'].pop('lacp', None)\\n        facts = {'lacp': {}}\\n        if objs:\\n            params = utils.validate_config(self.argument_spec, {'config': objs})\\n            facts['lacp'] = utils.remove_empties(params['config'])\\n        ansible_facts['ansible_network_resources'].update(facts)\\n        return ansible_facts"
  },
  {
    "code": "def _python_agg_general(self, arg):\\n        group_shape = self._group_shape\\n        counts = np.zeros(group_shape, dtype=int)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: multi-key groupby now works with multiple functions. starting to add *args, **kwargs. address GH #133",
    "fixed_code": "def _python_agg_general(self, func, *args, **kwargs):\\n        group_shape = self._group_shape\\n        counts = np.zeros(group_shape, dtype=int)"
  },
  {
    "code": "def __init__(self, bandwidth=None, seeds=None, bin_seeding=False,\\n                 cluster_all=True):\\n        self.bandwidth = bandwidth\\n        self.seeds = seeds\\n        self.bin_seeding = bin_seeding\\n        self.cluster_all = cluster_all\\n        self.cluster_centers_ = None\\n        self.labels_ = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX/ENH mean shift clustering",
    "fixed_code": "def __init__(self, bandwidth=None, seeds=None, bin_seeding=False,\\n                 min_bin_freq=1, cluster_all=True):\\n        self.bandwidth = bandwidth\\n        self.seeds = seeds\\n        self.bin_seeding = bin_seeding\\n        self.cluster_all = cluster_all\\n        self.min_bin_freq = min_bin_freq"
  },
  {
    "code": "def to_dense(self):\\n        pass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "broke something. sigh",
    "fixed_code": "def to_dense(self):\\n        return Series(self.values, index=self.index)"
  },
  {
    "code": "def _normalize(table, normalize, margins, margins_name=\"All\"):\\n\\tif not isinstance(normalize, (bool, str)):\\n\\t\\taxis_subs = {0: \"index\", 1: \"columns\"}\\n\\t\\ttry:\\n\\t\\t\\tnormalize = axis_subs[normalize]\\n\\t\\texcept KeyError:\\n\\t\\t\\traise ValueError(\"Not a valid normalize argument\")\\n\\tif margins is False:\\n\\t\\tnormalizers = {\\n\\t\\t\\t\"all\": lambda x: x / x.sum(axis=1).sum(axis=0),\\n\\t\\t\\t\"columns\": lambda x: x / x.sum(),\\n\\t\\t\\t\"index\": lambda x: x.div(x.sum(axis=1), axis=0),\\n\\t\\t}\\n\\t\\tnormalizers[True] = normalizers[\"all\"]\\n\\t\\ttry:\\n\\t\\t\\tf = normalizers[normalize]\\n\\t\\texcept KeyError:\\n\\t\\t\\traise ValueError(\"Not a valid normalize argument\")\\n\\t\\ttable = f(table)\\n\\t\\ttable = table.fillna(0)\\n\\telif margins is True:\\n\\t\\tcolumn_margin = table.loc[:, margins_name].drop(margins_name)\\n\\t\\tindex_margin = table.loc[margins_name, :].drop(margins_name)\\n\\t\\ttable = table.drop(margins_name, axis=1).drop(margins_name)\\n\\t\\ttable_index_names = table.index.names\\n\\t\\ttable_columns_names = table.columns.names\\n\\t\\ttable = _normalize(table, normalize=normalize, margins=False)\\n\\t\\tif normalize == \"columns\":\\n\\t\\t\\tcolumn_margin = column_margin / column_margin.sum()\\n\\t\\t\\ttable = concat([table, column_margin], axis=1)\\n\\t\\t\\ttable = table.fillna(0)\\n\\t\\telif normalize == \"index\":\\n\\t\\t\\tindex_margin = index_margin / index_margin.sum()\\n\\t\\t\\ttable = table.append(index_margin)\\n\\t\\t\\ttable = table.fillna(0)\\n\\t\\telif normalize == \"all\" or normalize is True:\\n\\t\\t\\tcolumn_margin = column_margin / column_margin.sum()\\n\\t\\t\\tindex_margin = index_margin / index_margin.sum()\\n\\t\\t\\tindex_margin.loc[margins_name] = 1\\n\\t\\t\\ttable = concat([table, column_margin], axis=1)\\n\\t\\t\\ttable = table.append(index_margin)\\n\\t\\t\\ttable = table.fillna(0)\\n\\t\\telse:\\n\\t\\t\\traise ValueError(\"Not a valid normalize argument\")\\n\\t\\ttable.index.names = table_index_names\\n\\t\\ttable.columns.names = table_columns_names\\n\\telse:\\n\\t\\traise ValueError(\"Not a valid margins argument\")\\n\\treturn table",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pd.crosstab not working when margin and normalize are set together (#27663)",
    "fixed_code": "def _normalize(table, normalize, margins, margins_name=\"All\"):\\n\\tif not isinstance(normalize, (bool, str)):\\n\\t\\taxis_subs = {0: \"index\", 1: \"columns\"}\\n\\t\\ttry:\\n\\t\\t\\tnormalize = axis_subs[normalize]\\n\\t\\texcept KeyError:\\n\\t\\t\\traise ValueError(\"Not a valid normalize argument\")\\n\\tif margins is False:\\n\\t\\tnormalizers = {\\n\\t\\t\\t\"all\": lambda x: x / x.sum(axis=1).sum(axis=0),\\n\\t\\t\\t\"columns\": lambda x: x / x.sum(),\\n\\t\\t\\t\"index\": lambda x: x.div(x.sum(axis=1), axis=0),\\n\\t\\t}\\n\\t\\tnormalizers[True] = normalizers[\"all\"]\\n\\t\\ttry:\\n\\t\\t\\tf = normalizers[normalize]\\n\\t\\texcept KeyError:\\n\\t\\t\\traise ValueError(\"Not a valid normalize argument\")\\n\\t\\ttable = f(table)\\n\\t\\ttable = table.fillna(0)\\n\\telif margins is True:\\n\\t\\ttable_index = table.index\\n\\t\\ttable_columns = table.columns\\n\\t\\tif (margins_name not in table.iloc[-1, :].name) | (\\n\\t\\t\\tmargins_name != table.iloc[:, -1].name\\n\\t\\t):\\n\\t\\t\\traise ValueError(\"{} not in pivoted DataFrame\".format(margins_name))\\n\\t\\tcolumn_margin = table.iloc[:-1, -1]\\n\\t\\tindex_margin = table.iloc[-1, :-1]\\n\\t\\ttable = table.iloc[:-1, :-1]\\n\\t\\ttable = _normalize(table, normalize=normalize, margins=False)\\n\\t\\tif normalize == \"columns\":\\n\\t\\t\\tcolumn_margin = column_margin / column_margin.sum()\\n\\t\\t\\ttable = concat([table, column_margin], axis=1)\\n\\t\\t\\ttable = table.fillna(0)\\n\\t\\t\\ttable.columns = table_columns\\n\\t\\telif normalize == \"index\":\\n\\t\\t\\tindex_margin = index_margin / index_margin.sum()\\n\\t\\t\\ttable = table.append(index_margin)\\n\\t\\t\\ttable = table.fillna(0)\\n\\t\\t\\ttable.index = table_index\\n\\t\\telif normalize == \"all\" or normalize is True:\\n\\t\\t\\tcolumn_margin = column_margin / column_margin.sum()\\n\\t\\t\\tindex_margin = index_margin / index_margin.sum()\\n\\t\\t\\tindex_margin.loc[margins_name] = 1\\n\\t\\t\\ttable = concat([table, column_margin], axis=1)\\n\\t\\t\\ttable = table.append(index_margin)\\n\\t\\t\\ttable = table.fillna(0)\\n\\t\\t\\ttable.index = table_index\\n\\t\\t\\ttable.columns = table_columns\\n\\t\\telse:\\n\\t\\t\\traise ValueError(\"Not a valid normalize argument\")\\n\\telse:\\n\\t\\traise ValueError(\"Not a valid margins argument\")\\n\\treturn table"
  },
  {
    "code": "def _retry_read_url(url, retry_count, pause, name):\\n    for _ in range(retry_count):\\n        time.sleep(pause)\\n        try:\\n            with urlopen(url) as resp:\\n                lines = resp.read()\\n        except _network_error_classes:\\n            pass\\n        else:\\n            rs = read_csv(StringIO(bytes_to_str(lines)), index_col=0,\\n                          parse_dates=True, na_values='-')[::-1]\\n            if len(rs) > 2 and rs.index[-1] == rs.index[-2]:  \\n                rs = rs[:-1]\\n    try:\\n        rs.index.name = rs.index.name.decode('unicode_escape').encode('ascii', 'ignore')\\n    except AttributeError:\\n        rs.index.name = rs.index.name.encode('ascii', 'ignore').decode()\\n    return rs\\n    raise IOError(\"after %d tries, %s did not \"\\n                  \"return a 200 for url %r\" % (retry_count, name, url))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Bring pandas up to date with pandas_datareader",
    "fixed_code": "def _retry_read_url(url, retry_count, pause, name):\\n    for _ in range(retry_count):\\n        time.sleep(pause)\\n        try:\\n            with urlopen(url) as resp:\\n                lines = resp.read()\\n        except _network_error_classes:\\n            pass\\n        else:\\n            rs = read_csv(StringIO(bytes_to_str(lines)), index_col=0,\\n                          parse_dates=True, na_values='-')[::-1]\\n            if len(rs) > 2 and rs.index[-1] == rs.index[-2]:  \\n                rs = rs[:-1]\\n            try:\\n                rs.index.name = rs.index.name.decode('unicode_escape').encode('ascii', 'ignore')\\n            except AttributeError:\\n                rs.index.name = rs.index.name.encode('ascii', 'ignore').decode()\\n            return rs\\n    raise IOError(\"after %d tries, %s did not \"\\n                  \"return a 200 for url %r\" % (retry_count, name, url))"
  },
  {
    "code": "def _infer_types(\\n        self, values, na_values, no_dtype_specified, try_num_bool: bool = True\\n    ):\\n        na_count = 0\\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\\n            na_values = np.array([val for val in na_values if not isinstance(val, str)])\\n            mask = algorithms.isin(values, na_values)\\n            na_count = mask.astype(\"uint8\", copy=False).sum()\\n            if na_count > 0:\\n                if is_integer_dtype(values):\\n                    values = values.astype(np.float64)\\n                np.putmask(values, mask, np.nan)\\n            return values, na_count\\n        use_nullable_dtypes: Literal[True] | Literal[False] = (\\n            self.use_nullable_dtypes and no_dtype_specified\\n        )\\n        result: ArrayLike\\n        if try_num_bool and is_object_dtype(values.dtype):\\n            try:\\n                result, result_mask = lib.maybe_convert_numeric(\\n                    values,\\n                    na_values,\\n                    False,\\n                    convert_to_masked_nullable=use_nullable_dtypes,\\n                )\\n            except (ValueError, TypeError):\\n                na_count = parsers.sanitize_objects(values, na_values)\\n                result = values\\n            else:\\n                if use_nullable_dtypes:\\n                    if result_mask is None:\\n                        result_mask = np.zeros(result.shape, dtype=np.bool_)\\n                    if result_mask.all():\\n                        result = IntegerArray(\\n                            np.ones(result_mask.shape, dtype=np.int64), result_mask\\n                        )\\n                    elif is_integer_dtype(result):\\n                        result = IntegerArray(result, result_mask)\\n                    elif is_bool_dtype(result):\\n                        result = BooleanArray(result, result_mask)\\n                    elif is_float_dtype(result):\\n                        result = FloatingArray(result, result_mask)\\n                    na_count = result_mask.sum()\\n                else:\\n                    na_count = isna(result).sum()\\n        else:\\n            result = values\\n            if values.dtype == np.object_:\\n                na_count = parsers.sanitize_objects(values, na_values)\\n        if result.dtype == np.object_ and try_num_bool:\\n            result, bool_mask = libops.maybe_convert_bool(\\n                np.asarray(values),\\n                true_values=self.true_values,\\n                false_values=self.false_values,\\n                convert_to_masked_nullable=use_nullable_dtypes,\\n            )\\n            if result.dtype == np.bool_ and use_nullable_dtypes:\\n                if bool_mask is None:\\n                    bool_mask = np.zeros(result.shape, dtype=np.bool_)\\n                result = BooleanArray(result, bool_mask)\\n            elif result.dtype == np.object_ and use_nullable_dtypes:\\n                result = StringDtype().construct_array_type()._from_sequence(values)\\n        return result, na_count",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _infer_types(\\n        self, values, na_values, no_dtype_specified, try_num_bool: bool = True\\n    ):\\n        na_count = 0\\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\\n            na_values = np.array([val for val in na_values if not isinstance(val, str)])\\n            mask = algorithms.isin(values, na_values)\\n            na_count = mask.astype(\"uint8\", copy=False).sum()\\n            if na_count > 0:\\n                if is_integer_dtype(values):\\n                    values = values.astype(np.float64)\\n                np.putmask(values, mask, np.nan)\\n            return values, na_count\\n        use_nullable_dtypes: Literal[True] | Literal[False] = (\\n            self.use_nullable_dtypes and no_dtype_specified\\n        )\\n        result: ArrayLike\\n        if try_num_bool and is_object_dtype(values.dtype):\\n            try:\\n                result, result_mask = lib.maybe_convert_numeric(\\n                    values,\\n                    na_values,\\n                    False,\\n                    convert_to_masked_nullable=use_nullable_dtypes,\\n                )\\n            except (ValueError, TypeError):\\n                na_count = parsers.sanitize_objects(values, na_values)\\n                result = values\\n            else:\\n                if use_nullable_dtypes:\\n                    if result_mask is None:\\n                        result_mask = np.zeros(result.shape, dtype=np.bool_)\\n                    if result_mask.all():\\n                        result = IntegerArray(\\n                            np.ones(result_mask.shape, dtype=np.int64), result_mask\\n                        )\\n                    elif is_integer_dtype(result):\\n                        result = IntegerArray(result, result_mask)\\n                    elif is_bool_dtype(result):\\n                        result = BooleanArray(result, result_mask)\\n                    elif is_float_dtype(result):\\n                        result = FloatingArray(result, result_mask)\\n                    na_count = result_mask.sum()\\n                else:\\n                    na_count = isna(result).sum()\\n        else:\\n            result = values\\n            if values.dtype == np.object_:\\n                na_count = parsers.sanitize_objects(values, na_values)\\n        if result.dtype == np.object_ and try_num_bool:\\n            result, bool_mask = libops.maybe_convert_bool(\\n                np.asarray(values),\\n                true_values=self.true_values,\\n                false_values=self.false_values,\\n                convert_to_masked_nullable=use_nullable_dtypes,\\n            )\\n            if result.dtype == np.bool_ and use_nullable_dtypes:\\n                if bool_mask is None:\\n                    bool_mask = np.zeros(result.shape, dtype=np.bool_)\\n                result = BooleanArray(result, bool_mask)\\n            elif result.dtype == np.object_ and use_nullable_dtypes:\\n                result = StringDtype().construct_array_type()._from_sequence(values)\\n        return result, na_count"
  },
  {
    "code": "def serve(request, path, document_root=None, show_indexes=False):\\n\\tpath = posixpath.normpath(unquote(path))\\n\\tpath = path.lstrip('/')\\n\\tnewpath = ''\\n\\tfor part in path.split('/'):\\n\\t\\tif not part:\\n\\t\\t\\tcontinue\\n\\t\\tdrive, part = os.path.splitdrive(part)\\n\\t\\thead, part = os.path.split(part)\\n\\t\\tif part in (os.curdir, os.pardir):\\n\\t\\t\\tcontinue\\n\\t\\tnewpath = os.path.join(newpath, part).replace('\\\\', '/')\\n\\tif newpath and path != newpath:\\n\\t\\treturn HttpResponseRedirect(newpath)\\n\\tfullpath = os.path.join(document_root, newpath)\\n\\tif os.path.isdir(fullpath):\\n\\t\\tif show_indexes:\\n\\t\\t\\treturn directory_index(newpath, fullpath)\\n\\t\\traise Http404(_(\"Directory indexes are not allowed here.\"))\\n\\tif not os.path.exists(fullpath):\\n\\t\\traise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\\n\\tstatobj = os.stat(fullpath)\\n\\tif not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\\n\\t\\t\\t\\t\\t\\t\\t  statobj.st_mtime, statobj.st_size):\\n\\t\\treturn HttpResponseNotModified()\\n\\tcontent_type, encoding = mimetypes.guess_type(fullpath)\\n\\tcontent_type = content_type or 'application/octet-stream'\\n\\tf = open(fullpath, 'rb')\\n\\tresponse = CompatibleStreamingHttpResponse(iter(lambda: f.read(STREAM_CHUNK_SIZE), b''),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   content_type=content_type)\\n\\tresponse[\"Last-Modified\"] = http_date(statobj.st_mtime)\\n\\tif stat.S_ISREG(statobj.st_mode):\\n\\t\\tresponse[\"Content-Length\"] = statobj.st_size\\n\\tif encoding:\\n\\t\\tresponse[\"Content-Encoding\"] = encoding\\n\\treturn response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def serve(request, path, document_root=None, show_indexes=False):\\n\\tpath = posixpath.normpath(unquote(path))\\n\\tpath = path.lstrip('/')\\n\\tnewpath = ''\\n\\tfor part in path.split('/'):\\n\\t\\tif not part:\\n\\t\\t\\tcontinue\\n\\t\\tdrive, part = os.path.splitdrive(part)\\n\\t\\thead, part = os.path.split(part)\\n\\t\\tif part in (os.curdir, os.pardir):\\n\\t\\t\\tcontinue\\n\\t\\tnewpath = os.path.join(newpath, part).replace('\\\\', '/')\\n\\tif newpath and path != newpath:\\n\\t\\treturn HttpResponseRedirect(newpath)\\n\\tfullpath = os.path.join(document_root, newpath)\\n\\tif os.path.isdir(fullpath):\\n\\t\\tif show_indexes:\\n\\t\\t\\treturn directory_index(newpath, fullpath)\\n\\t\\traise Http404(_(\"Directory indexes are not allowed here.\"))\\n\\tif not os.path.exists(fullpath):\\n\\t\\traise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\\n\\tstatobj = os.stat(fullpath)\\n\\tif not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\\n\\t\\t\\t\\t\\t\\t\\t  statobj.st_mtime, statobj.st_size):\\n\\t\\treturn HttpResponseNotModified()\\n\\tcontent_type, encoding = mimetypes.guess_type(fullpath)\\n\\tcontent_type = content_type or 'application/octet-stream'\\n\\tf = open(fullpath, 'rb')\\n\\tresponse = CompatibleStreamingHttpResponse(iter(lambda: f.read(STREAM_CHUNK_SIZE), b''),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   content_type=content_type)\\n\\tresponse[\"Last-Modified\"] = http_date(statobj.st_mtime)\\n\\tif stat.S_ISREG(statobj.st_mode):\\n\\t\\tresponse[\"Content-Length\"] = statobj.st_size\\n\\tif encoding:\\n\\t\\tresponse[\"Content-Encoding\"] = encoding\\n\\treturn response"
  },
  {
    "code": "def main(\\n    ctx: click.Context,\\n    line_length: int,\\n    check: bool,\\n    diff: bool,\\n    fast: bool,\\n    pyi: bool,\\n    py36: bool,\\n    skip_string_normalization: bool,\\n    skip_numeric_underscore_normalization: bool,\\n    quiet: bool,\\n    verbose: bool,\\n    include: str,\\n    exclude: str,\\n    src: Tuple[str],\\n    config: Optional[str],\\n) -> None:\\n    write_back = WriteBack.from_configuration(check=check, diff=diff)\\n    mode = FileMode.from_configuration(\\n        py36=py36,\\n        pyi=pyi,\\n        skip_string_normalization=skip_string_normalization,\\n        skip_numeric_underscore_normalization=skip_numeric_underscore_normalization,\\n    )\\n    if config and verbose:\\n        out(f\"Using configuration from {config}.\", bold=False, fg=\"blue\")\\n    try:\\n        include_regex = re_compile_maybe_verbose(include)\\n    except re.error:\\n        err(f\"Invalid regular expression for include given: {include!r}\")\\n        ctx.exit(2)\\n    try:\\n        exclude_regex = re_compile_maybe_verbose(exclude)\\n    except re.error:\\n        err(f\"Invalid regular expression for exclude given: {exclude!r}\")\\n        ctx.exit(2)\\n    report = Report(check=check, quiet=quiet, verbose=verbose)\\n    root = find_project_root(src)\\n    sources: Set[Path] = set()\\n    for s in src:\\n        p = Path(s)\\n        if p.is_dir():\\n            sources.update(\\n                gen_python_files_in_dir(p, root, include_regex, exclude_regex, report)\\n            )\\n        elif p.is_file() or s == \"-\":\\n            sources.add(p)\\n        else:\\n            err(f\"invalid path: {s}\")\\n    if len(sources) == 0:\\n        if verbose or not quiet:\\n            out(\"No paths given. Nothing to do \ud83d\ude34\")\\n        ctx.exit(0)\\n    if len(sources) == 1:\\n        reformat_one(\\n            src=sources.pop(),\\n            line_length=line_length,\\n            fast=fast,\\n            write_back=write_back,\\n            mode=mode,\\n            report=report,\\n        )\\n    else:\\n        loop = asyncio.get_event_loop()\\n        executor = ProcessPoolExecutor(max_workers=os.cpu_count())\\n        try:\\n            loop.run_until_complete(\\n                schedule_formatting(\\n                    sources=sources,\\n                    line_length=line_length,\\n                    fast=fast,\\n                    write_back=write_back,\\n                    mode=mode,\\n                    report=report,\\n                    loop=loop,\\n                    executor=executor,\\n                )\\n            )\\n        finally:\\n            shutdown(loop)\\n    if verbose or not quiet:\\n        bang = \"\ud83d\udca5 \ud83d\udc94 \ud83d\udca5\" if report.return_code else \"\u2728 \ud83c\udf70 \u2728\"\\n        out(f\"All done! {bang}\")\\n        click.secho(str(report), err=True)\\n    ctx.exit(report.return_code)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main(\\n    ctx: click.Context,\\n    line_length: int,\\n    check: bool,\\n    diff: bool,\\n    fast: bool,\\n    pyi: bool,\\n    py36: bool,\\n    skip_string_normalization: bool,\\n    skip_numeric_underscore_normalization: bool,\\n    quiet: bool,\\n    verbose: bool,\\n    include: str,\\n    exclude: str,\\n    src: Tuple[str],\\n    config: Optional[str],\\n) -> None:\\n    write_back = WriteBack.from_configuration(check=check, diff=diff)\\n    mode = FileMode.from_configuration(\\n        py36=py36,\\n        pyi=pyi,\\n        skip_string_normalization=skip_string_normalization,\\n        skip_numeric_underscore_normalization=skip_numeric_underscore_normalization,\\n    )\\n    if config and verbose:\\n        out(f\"Using configuration from {config}.\", bold=False, fg=\"blue\")\\n    try:\\n        include_regex = re_compile_maybe_verbose(include)\\n    except re.error:\\n        err(f\"Invalid regular expression for include given: {include!r}\")\\n        ctx.exit(2)\\n    try:\\n        exclude_regex = re_compile_maybe_verbose(exclude)\\n    except re.error:\\n        err(f\"Invalid regular expression for exclude given: {exclude!r}\")\\n        ctx.exit(2)\\n    report = Report(check=check, quiet=quiet, verbose=verbose)\\n    root = find_project_root(src)\\n    sources: Set[Path] = set()\\n    for s in src:\\n        p = Path(s)\\n        if p.is_dir():\\n            sources.update(\\n                gen_python_files_in_dir(p, root, include_regex, exclude_regex, report)\\n            )\\n        elif p.is_file() or s == \"-\":\\n            sources.add(p)\\n        else:\\n            err(f\"invalid path: {s}\")\\n    if len(sources) == 0:\\n        if verbose or not quiet:\\n            out(\"No paths given. Nothing to do \ud83d\ude34\")\\n        ctx.exit(0)\\n    if len(sources) == 1:\\n        reformat_one(\\n            src=sources.pop(),\\n            line_length=line_length,\\n            fast=fast,\\n            write_back=write_back,\\n            mode=mode,\\n            report=report,\\n        )\\n    else:\\n        loop = asyncio.get_event_loop()\\n        executor = ProcessPoolExecutor(max_workers=os.cpu_count())\\n        try:\\n            loop.run_until_complete(\\n                schedule_formatting(\\n                    sources=sources,\\n                    line_length=line_length,\\n                    fast=fast,\\n                    write_back=write_back,\\n                    mode=mode,\\n                    report=report,\\n                    loop=loop,\\n                    executor=executor,\\n                )\\n            )\\n        finally:\\n            shutdown(loop)\\n    if verbose or not quiet:\\n        bang = \"\ud83d\udca5 \ud83d\udc94 \ud83d\udca5\" if report.return_code else \"\u2728 \ud83c\udf70 \u2728\"\\n        out(f\"All done! {bang}\")\\n        click.secho(str(report), err=True)\\n    ctx.exit(report.return_code)"
  },
  {
    "code": "def get_pending_tasks(self, state):\\n\\t\\tif len(self.tasks) < state.num_pending_tasks():\\n\\t\\t\\treturn six.moves.filter(lambda task: task.status in [PENDING, RUNNING],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tself.tasks)\\n\\t\\telse:\\n\\t\\t\\treturn state.get_pending_tasks()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Filters tasks in second branch of Worker.get_pending_tasks (#1849)\\n\\nWhen a worker has many DONE tasks, get_pending_tasks may switch to using\\nstate.get_pending_tasks in order to speed up the process. This can include\\npending tasks not owned by the worker, invalidating the result and causing\\nfunctions like is_trivial_worker to return erroneous results.\\n\\nTo fix this, we simply filter the results of state.get_pending_tasks to\\nremove any tasks that don't include this worker.",
    "fixed_code": "def get_pending_tasks(self, state):\\n\\t\\tif len(self.tasks) < state.num_pending_tasks():\\n\\t\\t\\treturn six.moves.filter(lambda task: task.status in [PENDING, RUNNING],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tself.tasks)\\n\\t\\telse:\\n\\t\\t\\treturn six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())"
  }
]