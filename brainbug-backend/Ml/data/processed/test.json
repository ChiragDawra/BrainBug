[
  {
    "code": "def _ica_def(X, tol, g, gprime, fun_args, maxit, w_init):\\n    n_comp = w_init.shape[0]\\n    W = np.zeros((n_comp, n_comp), dtype=float)\\n    for j in range(n_comp):\\n        w = w_init[j, :].copy()\\n        w /= np.sqrt((w**2).sum())\\n        n_iterations = 0\\n        lim = tol + 1\\n        while ((lim > tol) & (n_iterations < (maxit-1))):\\n            wtx = np.dot(w.T, X)\\n            gwtx = g(wtx, fun_args)\\n            g_wtx = gprime(wtx, fun_args)\\n            w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\\n            _gs_decorrelation(w1, W, j)\\n            w1 /= np.sqrt((w1**2).sum())\\n            lim = np.abs(np.abs((w1 * w).sum()) - 1)\\n            w = w1\\n            n_iterations = n_iterations + 1\\n        W[j, :] = w\\n    return W",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "consistently rename n_comp to n_components",
    "fixed_code": "def _ica_def(X, tol, g, gprime, fun_args, maxit, w_init):\\n    n_components = w_init.shape[0]\\n    W = np.zeros((n_components, n_components), dtype=float)\\n    for j in range(n_components):\\n        w = w_init[j, :].copy()\\n        w /= np.sqrt((w**2).sum())\\n        n_iterations = 0\\n        lim = tol + 1\\n        while ((lim > tol) & (n_iterations < (maxit-1))):\\n            wtx = np.dot(w.T, X)\\n            gwtx = g(wtx, fun_args)\\n            g_wtx = gprime(wtx, fun_args)\\n            w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\\n            _gs_decorrelation(w1, W, j)\\n            w1 /= np.sqrt((w1**2).sum())\\n            lim = np.abs(np.abs((w1 * w).sum()) - 1)\\n            w = w1\\n            n_iterations = n_iterations + 1\\n        W[j, :] = w\\n    return W"
  },
  {
    "code": "def _generate_regular_range(start, end, periods, offset):\\n    if isinstance(offset, Tick):\\n        stride = offset.nanos\\n        if periods is None:\\n            b = Timestamp(start).value\\n            e = Timestamp(end).value\\n            e += stride - e % stride\\n            tz = start.tz\\n        elif start is not None:\\n            b = Timestamp(start).value\\n            e = b + periods * stride\\n            tz = start.tz\\n        elif end is not None:\\n            e = Timestamp(end).value + stride\\n            b = e - periods * stride\\n            tz = end.tz\\n        else:\\n            raise NotImplementedError\\n        data = np.arange(b, e, stride, dtype=np.int64)\\n        data = DatetimeIndex._simple_new(data, None, tz=tz)\\n    else:\\n        if isinstance(start, Timestamp):\\n            start = start.to_pydatetime()\\n        if isinstance(end, Timestamp):\\n            end = end.to_pydatetime()\\n        xdr = generate_range(start=start, end=end,\\n                             periods=periods, offset=offset)\\n        dates = list(xdr)\\n        data = tools.to_datetime(dates)\\n    return data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Closes #8943 COMPAT: periods needs coercion to np.int64\\n\\nIn _generate_regular_range, we explicitly cast `periods` to np.int64 to prevent\\noverflow in the function tseries.index._generate_regular_range.\\nNote: we don't bother casting it for the function call generate_range since\\nthere's no danger of overflow in that function.",
    "fixed_code": "def _generate_regular_range(start, end, periods, offset):\\n    if isinstance(offset, Tick):\\n        stride = offset.nanos\\n        if periods is None:\\n            b = Timestamp(start).value\\n            e = Timestamp(end).value\\n            e += stride - e % stride\\n            tz = start.tz\\n        elif start is not None:\\n            b = Timestamp(start).value\\n            e = b + np.int64(periods) * stride\\n            tz = start.tz\\n        elif end is not None:\\n            e = Timestamp(end).value + stride\\n            b = e - np.int64(periods) * stride\\n            tz = end.tz\\n        else:\\n            raise NotImplementedError\\n        data = np.arange(b, e, stride, dtype=np.int64)\\n        data = DatetimeIndex._simple_new(data, None, tz=tz)\\n    else:\\n        if isinstance(start, Timestamp):\\n            start = start.to_pydatetime()\\n        if isinstance(end, Timestamp):\\n            end = end.to_pydatetime()\\n        xdr = generate_range(start=start, end=end,\\n                             periods=periods, offset=offset)\\n        dates = list(xdr)\\n        data = tools.to_datetime(dates)\\n    return data"
  },
  {
    "code": "class EventCollection(LineCollection):\\n\\t_edge_default = True\\n\\tdef __init__(self,\\n\\t\\t\\t\\t positions,\\t \\n\\t\\t\\t\\t orientation=None,\\n\\t\\t\\t\\t lineoffset=0,\\n\\t\\t\\t\\t linelength=1,\\n\\t\\t\\t\\t linewidth=None,\\n\\t\\t\\t\\t color=None,\\n\\t\\t\\t\\t linestyle='solid',\\n\\t\\t\\t\\t antialiased=None,\\n\\t\\t\\t\\t **kwargs\\n\\t\\t\\t\\t ):\\n\\t\\tsegment = (lineoffset + linelength / 2.,\\n\\t\\t\\t\\t   lineoffset - linelength / 2.)\\n\\t\\tif positions is None or len(positions) == 0:\\n\\t\\t\\tsegments = []\\n\\t\\telif hasattr(positions, 'ndim') and positions.ndim > 1:\\n\\t\\t\\traise ValueError('positions cannot be an array with more than '\\n\\t\\t\\t\\t\\t\\t\\t 'one dimension.')\\n\\t\\telif (orientation is None or orientation.lower() == 'none' or\\n\\t\\t\\t  orientation.lower() == 'horizontal'):\\n\\t\\t\\tpositions.sort()\\n\\t\\t\\tsegments = [[(coord1, coord2) for coord2 in segment] for\\n\\t\\t\\t\\t\\t\\tcoord1 in positions]\\n\\t\\t\\tself._is_horizontal = True\\n\\t\\telif orientation.lower() == 'vertical':\\n\\t\\t\\tpositions.sort()\\n\\t\\t\\tsegments = [[(coord2, coord1) for coord2 in segment] for\\n\\t\\t\\t\\t\\t\\tcoord1 in positions]\\n\\t\\t\\tself._is_horizontal = False\\n\\t\\telse:\\n\\t\\t\\tcbook._check_in_list(['horizontal', 'vertical'],\\n\\t\\t\\t\\t\\t\\t\\t\\t orientation=orientation)\\n\\t\\tLineCollection.__init__(self,\\n\\t\\t\\t\\t\\t\\t\\t\\tsegments,\\n\\t\\t\\t\\t\\t\\t\\t\\tlinewidths=linewidth,\\n\\t\\t\\t\\t\\t\\t\\t\\tcolors=color,\\n\\t\\t\\t\\t\\t\\t\\t\\tantialiaseds=antialiased,\\n\\t\\t\\t\\t\\t\\t\\t\\tlinestyles=linestyle,\\n\\t\\t\\t\\t\\t\\t\\t\\t**kwargs)\\n\\t\\tself._linelength = linelength\\n\\t\\tself._lineoffset = lineoffset",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Backport PR #14488: Make sure EventCollection doesn't modify input in-place",
    "fixed_code": "class EventCollection(LineCollection):\\n\\t_edge_default = True\\n\\tdef __init__(self,\\n\\t\\t\\t\\t positions,\\t \\n\\t\\t\\t\\t orientation=None,\\n\\t\\t\\t\\t lineoffset=0,\\n\\t\\t\\t\\t linelength=1,\\n\\t\\t\\t\\t linewidth=None,\\n\\t\\t\\t\\t color=None,\\n\\t\\t\\t\\t linestyle='solid',\\n\\t\\t\\t\\t antialiased=None,\\n\\t\\t\\t\\t **kwargs\\n\\t\\t\\t\\t ):\\n\\t\\tif positions is None:\\n\\t\\t\\traise ValueError('positions must be an array-like object')\\n\\t\\tpositions = np.array(positions, copy=True)\\n\\t\\tsegment = (lineoffset + linelength / 2.,\\n\\t\\t\\t\\t   lineoffset - linelength / 2.)\\n\\t\\tif positions.size == 0:\\n\\t\\t\\tsegments = []\\n\\t\\telif positions.ndim > 1:\\n\\t\\t\\traise ValueError('positions cannot be an array with more than '\\n\\t\\t\\t\\t\\t\\t\\t 'one dimension.')\\n\\t\\telif (orientation is None or orientation.lower() == 'none' or\\n\\t\\t\\t  orientation.lower() == 'horizontal'):\\n\\t\\t\\tpositions.sort()\\n\\t\\t\\tsegments = [[(coord1, coord2) for coord2 in segment] for\\n\\t\\t\\t\\t\\t\\tcoord1 in positions]\\n\\t\\t\\tself._is_horizontal = True\\n\\t\\telif orientation.lower() == 'vertical':\\n\\t\\t\\tpositions.sort()\\n\\t\\t\\tsegments = [[(coord2, coord1) for coord2 in segment] for\\n\\t\\t\\t\\t\\t\\tcoord1 in positions]\\n\\t\\t\\tself._is_horizontal = False\\n\\t\\telse:\\n\\t\\t\\tcbook._check_in_list(['horizontal', 'vertical'],\\n\\t\\t\\t\\t\\t\\t\\t\\t orientation=orientation)\\n\\t\\tLineCollection.__init__(self,\\n\\t\\t\\t\\t\\t\\t\\t\\tsegments,\\n\\t\\t\\t\\t\\t\\t\\t\\tlinewidths=linewidth,\\n\\t\\t\\t\\t\\t\\t\\t\\tcolors=color,\\n\\t\\t\\t\\t\\t\\t\\t\\tantialiaseds=antialiased,\\n\\t\\t\\t\\t\\t\\t\\t\\tlinestyles=linestyle,\\n\\t\\t\\t\\t\\t\\t\\t\\t**kwargs)\\n\\t\\tself._linelength = linelength\\n\\t\\tself._lineoffset = lineoffset"
  },
  {
    "code": "def getouterframes(frame, context=1):\\n    framelist = []\\n    while frame:\\n        frameinfo = (frame,) + getframeinfo(frame, context)\\n        framelist.append(FrameInfo(*frameinfo))\\n        frame = frame.f_back\\n    return framelist",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-88116: Enhance the inspect frame APIs to use the extended position information (GH-91531)",
    "fixed_code": "def getouterframes(frame, context=1):\\n    framelist = []\\n    while frame:\\n        traceback_info = getframeinfo(frame, context)\\n        frameinfo = (frame,) + traceback_info\\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\\n        frame = frame.f_back\\n    return framelist"
  },
  {
    "code": "def join(value, arg, autoescape=True):\\n\\tvalue = map(force_text, value)\\n\\tif autoescape:\\n\\t\\tvalue = [conditional_escape(v) for v in value]\\n\\ttry:\\n\\t\\tdata = conditional_escape(arg).join(value)\\n\\texcept AttributeError:  \\n\\t\\treturn value\\n\\treturn mark_safe(data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def join(value, arg, autoescape=True):\\n\\tvalue = map(force_text, value)\\n\\tif autoescape:\\n\\t\\tvalue = [conditional_escape(v) for v in value]\\n\\ttry:\\n\\t\\tdata = conditional_escape(arg).join(value)\\n\\texcept AttributeError:  \\n\\t\\treturn value\\n\\treturn mark_safe(data)"
  },
  {
    "code": "def __init__(  \\n        self,\\n        connections_path: str = 'connections',\\n        variables_path: str = 'variables',\\n        url: Optional[str] = None,\\n        auth_type: str = 'token',\\n        mount_point: str = 'secret',\\n        kv_engine_version: int = 2,\\n        token: Optional[str] = None,\\n        username: Optional[str] = None,\\n        password: Optional[str] = None,\\n        role_id: Optional[str] = None,\\n        secret_id: Optional[str] = None,\\n        gcp_key_path: Optional[str] = None,\\n        gcp_scopes: Optional[str] = None,\\n        **kwargs\\n    ):\\n        super().__init__(**kwargs)\\n        self.connections_path = connections_path.rstrip('/')\\n        self.variables_path = variables_path.rstrip('/')\\n        self.url = url\\n        self.auth_type = auth_type\\n        self.kwargs = kwargs\\n        self.token = token\\n        self.username = username\\n        self.password = password\\n        self.role_id = role_id\\n        self.secret_id = secret_id\\n        self.mount_point = mount_point\\n        self.kv_engine_version = kv_engine_version\\n        self.gcp_key_path = gcp_key_path\\n        self.gcp_scopes = gcp_scopes\\n    @cached_property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(  \\n        self,\\n        connections_path: str = 'connections',\\n        variables_path: str = 'variables',\\n        url: Optional[str] = None,\\n        auth_type: str = 'token',\\n        mount_point: str = 'secret',\\n        kv_engine_version: int = 2,\\n        token: Optional[str] = None,\\n        username: Optional[str] = None,\\n        password: Optional[str] = None,\\n        role_id: Optional[str] = None,\\n        secret_id: Optional[str] = None,\\n        gcp_key_path: Optional[str] = None,\\n        gcp_scopes: Optional[str] = None,\\n        **kwargs\\n    ):\\n        super().__init__(**kwargs)\\n        self.connections_path = connections_path.rstrip('/')\\n        self.variables_path = variables_path.rstrip('/')\\n        self.url = url\\n        self.auth_type = auth_type\\n        self.kwargs = kwargs\\n        self.token = token\\n        self.username = username\\n        self.password = password\\n        self.role_id = role_id\\n        self.secret_id = secret_id\\n        self.mount_point = mount_point\\n        self.kv_engine_version = kv_engine_version\\n        self.gcp_key_path = gcp_key_path\\n        self.gcp_scopes = gcp_scopes\\n    @cached_property"
  },
  {
    "code": "def read_stata(filepath_or_buffer, convert_dates=True,\\n               convert_categoricals=True, encoding=None, index=None,\\n               convert_missing=False, preserve_dtypes=True, columns=None,\\n               order_categoricals=True, chunksize=None, iterator=False):\\n    reader = StataReader(filepath_or_buffer,\\n                         convert_dates=convert_dates,\\n                         convert_categoricals=convert_categoricals,\\n                         index=index, convert_missing=convert_missing,\\n                         preserve_dtypes=preserve_dtypes,\\n                         columns=columns,\\n                         order_categoricals=order_categoricals,\\n                         chunksize=chunksize, encoding=encoding)\\n    if iterator or chunksize:\\n        data = reader\\n    else:\\n        try:\\n            data = reader.read()\\n        finally:\\n            reader.close()\\n    return data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Set index when reading Stata file (#17328)\\n\\nEnsures index is set when requested during reading of a Stata dta file\\nDeprecates and renames index to index_col for API consistence\\n\\ncloses #16342",
    "fixed_code": "def read_stata(filepath_or_buffer, convert_dates=True,\\n               convert_categoricals=True, encoding=None, index_col=None,\\n               convert_missing=False, preserve_dtypes=True, columns=None,\\n               order_categoricals=True, chunksize=None, iterator=False):\\n    reader = StataReader(filepath_or_buffer,\\n                         convert_dates=convert_dates,\\n                         convert_categoricals=convert_categoricals,\\n                         index_col=index_col, convert_missing=convert_missing,\\n                         preserve_dtypes=preserve_dtypes,\\n                         columns=columns,\\n                         order_categoricals=order_categoricals,\\n                         chunksize=chunksize, encoding=encoding)\\n    if iterator or chunksize:\\n        data = reader\\n    else:\\n        try:\\n            data = reader.read()\\n        finally:\\n            reader.close()\\n    return data"
  },
  {
    "code": "def form_blocks(data, index, columns):\\n    from pandas.core.internals import add_na_columns\\n    if columns is None:\\n        columns = Index(_try_sort(data.keys()))\\n        extra_columns = NULL_INDEX\\n    else:\\n        columns = _ensure_index(columns)\\n        extra_columns = columns - Index(data.keys())\\n    num_dict = {}\\n    object_dict = {}\\n    for k, v in data.iteritems():\\n        if issubclass(v.dtype.type, (np.floating, np.integer)):\\n            num_dict[k] = v\\n        else:\\n            object_dict[k] = v\\n    blocks = []\\n    if len(num_dict) > 0:\\n        num_dtypes = set(v.dtype for v in num_dict.values())\\n        if len(num_dtypes) > 1:\\n            num_dtype = np.float_\\n        else:\\n            num_dtype = list(num_dtypes)[0]\\n        num_block = _simple_blockify(num_dict, columns, num_dtype)\\n        blocks.append(num_block)\\n    if len(object_dict) > 0:\\n        object_block = _simple_blockify(object_dict, columns, np.object_)\\n        blocks.append(object_block)\\n    if len(extra_columns):\\n        na_block = add_na_columns(extra_columns, index, new_columns)\\n        blocks.append(na_block)\\n        blocks = _consolidate(blocks)\\n    return blocks, columns",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def form_blocks(data, index, columns):\\n    from pandas.core.internals import add_na_columns\\n    if columns is None:\\n        columns = Index(_try_sort(data.keys()))\\n        extra_columns = NULL_INDEX\\n    else:\\n        columns = _ensure_index(columns)\\n        extra_columns = columns - Index(data.keys())\\n    num_dict = {}\\n    object_dict = {}\\n    for k, v in data.iteritems():\\n        if issubclass(v.dtype.type, (np.floating, np.integer)):\\n            num_dict[k] = v\\n        else:\\n            object_dict[k] = v\\n    blocks = []\\n    if len(num_dict) > 0:\\n        num_dtypes = set(v.dtype for v in num_dict.values())\\n        if len(num_dtypes) > 1:\\n            num_dtype = np.float_\\n        else:\\n            num_dtype = list(num_dtypes)[0]\\n        num_block = _simple_blockify(num_dict, columns, num_dtype)\\n        blocks.append(num_block)\\n    if len(object_dict) > 0:\\n        object_block = _simple_blockify(object_dict, columns, np.object_)\\n        blocks.append(object_block)\\n    if len(extra_columns):\\n        na_block = add_na_columns(extra_columns, index, columns)\\n        blocks.append(na_block)\\n        blocks = _consolidate(blocks)\\n    return blocks, columns"
  },
  {
    "code": "def plot_frame(frame=None, subplots=False, sharex=True, sharey=False,\\n               use_index=True,\\n               figsize=None, grid=True, legend=True, rot=None,\\n               ax=None, title=None,\\n               xlim=None, ylim=None, logy=False,\\n               xticks=None, yticks=None,\\n               kind='line',\\n               sort_columns=False, fontsize=None, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logy=logy,\\n                     sort_columns=sort_columns, **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def plot_frame(frame=None, subplots=False, sharex=True, sharey=False,\\n               use_index=True,\\n               figsize=None, grid=True, legend=True, rot=None,\\n               ax=None, title=None,\\n               xlim=None, ylim=None, logy=False,\\n               xticks=None, yticks=None,\\n               kind='line',\\n               sort_columns=False, fontsize=None, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logy=logy,\\n                     sort_columns=sort_columns, **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]"
  },
  {
    "code": "def temppath():\\n\\tpath = tempfile.mktemp()\\n\\ttry:\\n\\t\\tyield path\\n\\tfinally:\\n\\t\\tif os.path.exists(path):\\n\\t\\t\\tif os.path.isfile(path):\\n\\t\\t\\t\\tos.remove(path)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tshutil.rmtree(path)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Replaced use of tempfile.mktemp() with tempfile.mkstemp() (#3358)",
    "fixed_code": "def temppath():\\n\\tdir_path = tempfile.TemporaryDirectory()\\n\\tpath = os.path.join(dir_path.name,'temp_test_file')\\n\\ttry:\\n\\t\\tyield path\\n\\tfinally:\\n\\t\\tdir_path.cleanup()"
  },
  {
    "code": "def main():\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        nopassword=dict(type='bool'),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        privilege=dict(type='int'),\\n        view=dict(aliases=['role']),\\n        sshkey=dict(),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection']),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'aggregate')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    if module.params['password'] and not module.params['configured_password']:\\n        warnings.append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_users = [x['name'] for x in want]\\n        have_users = [x['name'] for x in have]\\n        for item in set(have_users).difference(want_users):\\n            if item != 'admin':\\n                commands.append(user_del_cmd(item))\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        nopassword=dict(type='bool'),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        privilege=dict(type='int'),\\n        view=dict(aliases=['role']),\\n        sshkey=dict(),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection']),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'aggregate')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    if module.params['password'] and not module.params['configured_password']:\\n        warnings.append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_users = [x['name'] for x in want]\\n        have_users = [x['name'] for x in have]\\n        for item in set(have_users).difference(want_users):\\n            if item != 'admin':\\n                commands.append(user_del_cmd(item))\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    module.exit_json(**result)"
  },
  {
    "code": "def pool_link(attr):\\n        pool_id = attr.get('pool')\\n        if pool_id is not None:\\n            url = url_for('TaskInstanceModelView.list', _flt_3_pool=pool_id)\\n            pool_id = escape(pool_id)\\n            return Markup(\"<a href='{url}'>{pool_id}</a>\".format(**locals()))\\n        else:\\n            return Markup('Invalid",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pool_link(attr):\\n        pool_id = attr.get('pool')\\n        if pool_id is not None:\\n            url = url_for('TaskInstanceModelView.list', _flt_3_pool=pool_id)\\n            pool_id = escape(pool_id)\\n            return Markup(\"<a href='{url}'>{pool_id}</a>\".format(**locals()))\\n        else:\\n            return Markup('Invalid"
  },
  {
    "code": "def generate_files(repo_dir, context=None, output_dir='.',\\n\\t\\t\\t\\t   overwrite_if_exists=False):\\n\\ttemplate_dir = find_template(repo_dir)\\n\\tlogging.debug('Generating project from {0}...'.format(template_dir))\\n\\tcontext = context or {}\\n\\tunrendered_dir = os.path.split(template_dir)[1]\\n\\tensure_dir_is_templated(unrendered_dir)\\n\\tproject_dir = render_and_create_dir(unrendered_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcontext,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toutput_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toverwrite_if_exists)\\n\\tproject_dir = os.path.abspath(project_dir)\\n\\tlogging.debug('project_dir is {0}'.format(project_dir))\\n\\twith work_in(repo_dir):\\n\\t\\ttry:\\n\\t\\t\\trun_hook('pre_gen_project', project_dir, context)\\n\\t\\texcept FailedHookException:\\n\\t\\t\\tshutil.rmtree(project_dir, ignore_errors=True)\\n\\t\\t\\tlogging.error(\"Stopping generation because pre_gen_project\"\\n\\t\\t\\t\\t\\t\\t  \" hook script didn't exit sucessfully\")\\n\\t\\t\\treturn\\n\\twith work_in(template_dir):\\n\\t\\tenv = Environment(keep_trailing_newline=True)\\n\\t\\tenv.loader = FileSystemLoader('.')\\n\\t\\tfor root, dirs, files in os.walk('.'):\\n\\t\\t\\tcopy_dirs = []\\n\\t\\t\\trender_dirs = []\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\td_ = os.path.normpath(os.path.join(root, d))\\n\\t\\t\\t\\tif copy_without_render(d_, context):\\n\\t\\t\\t\\t\\tcopy_dirs.append(d)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\trender_dirs.append(d)\\n\\t\\t\\tfor copy_dir in copy_dirs:\\n\\t\\t\\t\\tindir = os.path.normpath(os.path.join(root, copy_dir))\\n\\t\\t\\t\\toutdir = os.path.normpath(os.path.join(project_dir, indir))\\n\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t'Copying dir {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t''.format(indir, outdir)\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tshutil.copytree(indir, outdir)\\n\\t\\t\\tdirs[:] = render_dirs\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\tunrendered_dir = os.path.join(project_dir, root, d)\\n\\t\\t\\t\\trender_and_create_dir(unrendered_dir, context, output_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  overwrite_if_exists)\\n\\t\\t\\tfor f in files:\\n\\t\\t\\t\\tinfile = os.path.normpath(os.path.join(root, f))\\n\\t\\t\\t\\tif copy_without_render(infile, context):\\n\\t\\t\\t\\t\\toutfile_tmpl = Template(infile)\\n\\t\\t\\t\\t\\toutfile_rendered = outfile_tmpl.render(**context)\\n\\t\\t\\t\\t\\toutfile = os.path.join(project_dir, outfile_rendered)\\n\\t\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t\\t'Copying file {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t\\t''.format(infile, outfile)\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tshutil.copyfile(infile, outfile)\\n\\t\\t\\t\\t\\tshutil.copymode(infile, outfile)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tlogging.debug('f is {0}'.format(f))\\n\\t\\t\\t\\tgenerate_file(project_dir, infile, context, env)\\n\\twith work_in(repo_dir):\\n\\t\\trun_hook('post_gen_project', project_dir, context)\\n\\treturn project_dir",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_files(repo_dir, context=None, output_dir='.',\\n\\t\\t\\t\\t   overwrite_if_exists=False):\\n\\ttemplate_dir = find_template(repo_dir)\\n\\tlogging.debug('Generating project from {0}...'.format(template_dir))\\n\\tcontext = context or {}\\n\\tunrendered_dir = os.path.split(template_dir)[1]\\n\\tensure_dir_is_templated(unrendered_dir)\\n\\tproject_dir = render_and_create_dir(unrendered_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcontext,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toutput_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\toverwrite_if_exists)\\n\\tproject_dir = os.path.abspath(project_dir)\\n\\tlogging.debug('project_dir is {0}'.format(project_dir))\\n\\twith work_in(repo_dir):\\n\\t\\ttry:\\n\\t\\t\\trun_hook('pre_gen_project', project_dir, context)\\n\\t\\texcept FailedHookException:\\n\\t\\t\\tshutil.rmtree(project_dir, ignore_errors=True)\\n\\t\\t\\tlogging.error(\"Stopping generation because pre_gen_project\"\\n\\t\\t\\t\\t\\t\\t  \" hook script didn't exit sucessfully\")\\n\\t\\t\\treturn\\n\\twith work_in(template_dir):\\n\\t\\tenv = Environment(keep_trailing_newline=True)\\n\\t\\tenv.loader = FileSystemLoader('.')\\n\\t\\tfor root, dirs, files in os.walk('.'):\\n\\t\\t\\tcopy_dirs = []\\n\\t\\t\\trender_dirs = []\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\td_ = os.path.normpath(os.path.join(root, d))\\n\\t\\t\\t\\tif copy_without_render(d_, context):\\n\\t\\t\\t\\t\\tcopy_dirs.append(d)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\trender_dirs.append(d)\\n\\t\\t\\tfor copy_dir in copy_dirs:\\n\\t\\t\\t\\tindir = os.path.normpath(os.path.join(root, copy_dir))\\n\\t\\t\\t\\toutdir = os.path.normpath(os.path.join(project_dir, indir))\\n\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t'Copying dir {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t''.format(indir, outdir)\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tshutil.copytree(indir, outdir)\\n\\t\\t\\tdirs[:] = render_dirs\\n\\t\\t\\tfor d in dirs:\\n\\t\\t\\t\\tunrendered_dir = os.path.join(project_dir, root, d)\\n\\t\\t\\t\\trender_and_create_dir(unrendered_dir, context, output_dir,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  overwrite_if_exists)\\n\\t\\t\\tfor f in files:\\n\\t\\t\\t\\tinfile = os.path.normpath(os.path.join(root, f))\\n\\t\\t\\t\\tif copy_without_render(infile, context):\\n\\t\\t\\t\\t\\toutfile_tmpl = Template(infile)\\n\\t\\t\\t\\t\\toutfile_rendered = outfile_tmpl.render(**context)\\n\\t\\t\\t\\t\\toutfile = os.path.join(project_dir, outfile_rendered)\\n\\t\\t\\t\\t\\tlogging.debug(\\n\\t\\t\\t\\t\\t\\t'Copying file {0} to {1} without rendering'\\n\\t\\t\\t\\t\\t\\t''.format(infile, outfile)\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tshutil.copyfile(infile, outfile)\\n\\t\\t\\t\\t\\tshutil.copymode(infile, outfile)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tlogging.debug('f is {0}'.format(f))\\n\\t\\t\\t\\tgenerate_file(project_dir, infile, context, env)\\n\\twith work_in(repo_dir):\\n\\t\\trun_hook('post_gen_project', project_dir, context)\\n\\treturn project_dir"
  },
  {
    "code": "def evaluate(self,\\n\\t\\t\\t   x=None,\\n\\t\\t\\t   y=None,\\n\\t\\t\\t   batch_size=None,\\n\\t\\t\\t   verbose=1,\\n\\t\\t\\t   sample_weight=None,\\n\\t\\t\\t   steps=None,\\n\\t\\t\\t   callbacks=None,\\n\\t\\t\\t   max_queue_size=10,\\n\\t\\t\\t   workers=1,\\n\\t\\t\\t   use_multiprocessing=False,\\n\\t\\t\\t   return_dict=False):\\n\\t\"\"\"Returns the loss value & metrics values for the model in test mode.\\n\\tComputation is done in batches (see the `batch_size` arg.)\\n\\tArgs:\\n\\t\\tx: Input data. It could be:\\n\\t\\t  - A Numpy array (or array-like), or a list of arrays\\n\\t\\t\\t(in case the model has multiple inputs).\\n\\t\\t  - A TensorFlow tensor, or a list of tensors\\n\\t\\t\\t(in case the model has multiple inputs).\\n\\t\\t  - A dict mapping input names to the corresponding array/tensors,\\n\\t\\t\\tif the model has named inputs.\\n\\t\\t  - A `tf.data` dataset. Should return a tuple\\n\\t\\t\\tof either `(inputs, targets)` or\\n\\t\\t\\t`(inputs, targets, sample_weights)`.\\n\\t\\t  - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\\n\\t\\t\\tor `(inputs, targets, sample_weights)`.\\n\\t\\t  A more detailed description of unpacking behavior for iterator types\\n\\t\\t  (Dataset, generator, Sequence) is given in the `Unpacking behavior\\n\\t\\t  for iterator-like inputs` section of `Model.fit`.\\n\\t\\ty: Target data. Like the input data `x`, it could be either Numpy\\n\\t\\t  array(s) or TensorFlow tensor(s). It should be consistent with `x`\\n\\t\\t  (you cannot have Numpy inputs and tensor targets, or inversely). If\\n\\t\\t  `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\\n\\t\\t  should not be specified (since targets will be obtained from the\\n\\t\\t  iterator/dataset).\\n\\t\\tbatch_size: Integer or `None`. Number of samples per batch of\\n\\t\\t  computation. If unspecified, `batch_size` will default to 32. Do not\\n\\t\\t  specify the `batch_size` if your data is in the form of a dataset,\\n\\t\\t  generators, or `keras.utils.Sequence` instances (since they generate\\n\\t\\t  batches).\\n\\t\\tverbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\\n\\t\\tsample_weight: Optional Numpy array of weights for the test samples,\\n\\t\\t  used for weighting the loss function. You can either pass a flat (1D)\\n\\t\\t  Numpy array with the same length as the input samples\\n\\t\\t\\t(1:1 mapping between weights and samples), or in the case of\\n\\t\\t\\t  temporal data, you can pass a 2D array with shape `(samples,\\n\\t\\t\\t  sequence_length)`, to apply a different weight to every timestep\\n\\t\\t\\t  of every sample. This argument is not supported when `x` is a\\n\\t\\t\\t  dataset, instead pass sample weights as the third element of `x`.\\n\\t\\tsteps: Integer or `None`. Total number of steps (batches of samples)\\n\\t\\t  before declaring the evaluation round finished. Ignored with the",
    "label": 1,
    "bug_type": "concurrency - deadlock",
    "bug_description": "Fix bug that caused calls to model.evaluate inside a fit callback to hit a cached evaluation dataset.",
    "fixed_code": "def evaluate(self,\\n\\t\\t\\t   x=None,\\n\\t\\t\\t   y=None,\\n\\t\\t\\t   batch_size=None,\\n\\t\\t\\t   verbose=1,\\n\\t\\t\\t   sample_weight=None,\\n\\t\\t\\t   steps=None,\\n\\t\\t\\t   callbacks=None,\\n\\t\\t\\t   max_queue_size=10,\\n\\t\\t\\t   workers=1,\\n\\t\\t\\t   use_multiprocessing=False,\\n\\t\\t\\t   return_dict=False,\\n\\t\\t\\t   **kwargs):\\n\\t\"\"\"Returns the loss value & metrics values for the model in test mode.\\n\\tComputation is done in batches (see the `batch_size` arg.)\\n\\tArgs:\\n\\t\\tx: Input data. It could be:\\n\\t\\t  - A Numpy array (or array-like), or a list of arrays\\n\\t\\t\\t(in case the model has multiple inputs).\\n\\t\\t  - A TensorFlow tensor, or a list of tensors\\n\\t\\t\\t(in case the model has multiple inputs).\\n\\t\\t  - A dict mapping input names to the corresponding array/tensors,\\n\\t\\t\\tif the model has named inputs.\\n\\t\\t  - A `tf.data` dataset. Should return a tuple\\n\\t\\t\\tof either `(inputs, targets)` or\\n\\t\\t\\t`(inputs, targets, sample_weights)`.\\n\\t\\t  - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\\n\\t\\t\\tor `(inputs, targets, sample_weights)`.\\n\\t\\t  A more detailed description of unpacking behavior for iterator types\\n\\t\\t  (Dataset, generator, Sequence) is given in the `Unpacking behavior\\n\\t\\t  for iterator-like inputs` section of `Model.fit`.\\n\\t\\ty: Target data. Like the input data `x`, it could be either Numpy\\n\\t\\t  array(s) or TensorFlow tensor(s). It should be consistent with `x`\\n\\t\\t  (you cannot have Numpy inputs and tensor targets, or inversely). If\\n\\t\\t  `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\\n\\t\\t  should not be specified (since targets will be obtained from the\\n\\t\\t  iterator/dataset).\\n\\t\\tbatch_size: Integer or `None`. Number of samples per batch of\\n\\t\\t  computation. If unspecified, `batch_size` will default to 32. Do not\\n\\t\\t  specify the `batch_size` if your data is in the form of a dataset,\\n\\t\\t  generators, or `keras.utils.Sequence` instances (since they generate\\n\\t\\t  batches).\\n\\t\\tverbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\\n\\t\\tsample_weight: Optional Numpy array of weights for the test samples,\\n\\t\\t  used for weighting the loss function. You can either pass a flat (1D)\\n\\t\\t  Numpy array with the same length as the input samples\\n\\t\\t\\t(1:1 mapping between weights and samples), or in the case of\\n\\t\\t\\t  temporal data, you can pass a 2D array with shape `(samples,\\n\\t\\t\\t  sequence_length)`, to apply a different weight to every timestep\\n\\t\\t\\t  of every sample. This argument is not supported when `x` is a\\n\\t\\t\\t  dataset, instead pass sample weights as the third element of `x`.\\n\\t\\tsteps: Integer or `None`. Total number of steps (batches of samples)\\n\\t\\t  before declaring the evaluation round finished. Ignored with the"
  },
  {
    "code": "def load_grammar(gt=\"Grammar.txt\", gp=None,\\n                 save=True, force=False, logger=None):\\n    if logger is None:\\n        logger = logging.getLogger(__name__)\\n    gp = _generate_pickle_name(gt) if gp is None else gp\\n    if force or not _newer(gp, gt):\\n        logger.info(\"Generating grammar tables from %s\", gt)\\n        g = pgen.generate_grammar(gt)\\n        if save:\\n            logger.info(\"Writing grammar tables to %s\", gp)\\n            try:\\n                g.dump(gp)\\n            except OSError as e:\\n                logger.info(\"Writing failed: %s\", e)\\n    else:\\n        g = grammar.Grammar()\\n        g.load(gp)\\n    return g",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_grammar(gt=\"Grammar.txt\", gp=None,\\n                 save=True, force=False, logger=None):\\n    if logger is None:\\n        logger = logging.getLogger(__name__)\\n    gp = _generate_pickle_name(gt) if gp is None else gp\\n    if force or not _newer(gp, gt):\\n        logger.info(\"Generating grammar tables from %s\", gt)\\n        g = pgen.generate_grammar(gt)\\n        if save:\\n            logger.info(\"Writing grammar tables to %s\", gp)\\n            try:\\n                g.dump(gp)\\n            except OSError as e:\\n                logger.info(\"Writing failed: %s\", e)\\n    else:\\n        g = grammar.Grammar()\\n        g.load(gp)\\n    return g"
  },
  {
    "code": "def _should_parse_dates(self, i):\\n        if isinstance(self.parse_dates, bool):\\n            return self.parse_dates\\n        else:\\n            name = self.index_name[i]\\n            j = self.index_col[i]\\n            if np.isscalar(self.parse_dates):\\n                return (j == self.parse_dates) or (name == self.parse_dates)\\n            else:\\n                return (j in self.parse_dates) or (name in self.parse_dates)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _should_parse_dates(self, i):\\n        if isinstance(self.parse_dates, bool):\\n            return self.parse_dates\\n        else:\\n            name = self.index_name[i]\\n            j = self.index_col[i]\\n            if np.isscalar(self.parse_dates):\\n                return (j == self.parse_dates) or (name == self.parse_dates)\\n            else:\\n                return (j in self.parse_dates) or (name in self.parse_dates)"
  },
  {
    "code": "def _write_constant(self, value):\\n        if isinstance(value, (float, complex)):\\n            self.write(repr(value).replace(\"inf\", _INFSTR))\\n        else:\\n            self.write(repr(value))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-28002: Roundtrip f-strings with ast.unparse better (#19612)\\n\\nBy attempting to avoid backslashes in f-string expressions.\\nWe also now proactively raise errors for some backslashes we can't\\navoid while unparsing FormattedValues",
    "fixed_code": "def _write_constant(self, value):\\n        if isinstance(value, (float, complex)):\\n            self.write(repr(value).replace(\"inf\", _INFSTR))\\n        elif self._avoid_backslashes and isinstance(value, str):\\n            self._write_str_avoiding_backslashes(value)\\n        else:\\n            self.write(repr(value))"
  },
  {
    "code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        targetlbvserver=dict(type='str'),\\n        targetvserverexpr=dict(type='str'),\\n        comment=dict(type='str'),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'targetlbvserver',\\n        'targetvserverexpr',\\n        'comment',\\n    ]\\n    readonly_attrs = [\\n        'hits',\\n        'referencecount',\\n        'undefhits',\\n        'builtin',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'targetvserverexpr',\\n    ]\\n    transforms = {\\n    }\\n    csaction_proxy = ConfigProxy(\\n        actual=csaction(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        ensure_feature_is_enabled(client, 'CS')\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.add()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not action_identical(client, module, csaction_proxy):\\n                immutables_changed = get_immutables_intersection(csaction_proxy, diff_list(client, module, csaction_proxy).keys())\\n                if immutables_changed != []:\\n                    module.fail_json(\\n                        msg='Cannot update immutable attributes %s' % (immutables_changed,),\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n                if not module.check_mode:\\n                    csaction_proxy.update()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            log('Sanity checks for state present')\\n            if not module.check_mode:\\n                if not action_exists(client, module):\\n                    module.fail_json(msg='Content switching action does not exist', **module_result)\\n                if not action_identical(client, module, csaction_proxy):\\n                    module.fail_json(\\n                        msg='Content switching action differs from configured',\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if action_exists(client, module):\\n                    module.fail_json(msg='Content switching action still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        targetlbvserver=dict(type='str'),\\n        targetvserverexpr=dict(type='str'),\\n        comment=dict(type='str'),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'targetlbvserver',\\n        'targetvserverexpr',\\n        'comment',\\n    ]\\n    readonly_attrs = [\\n        'hits',\\n        'referencecount',\\n        'undefhits',\\n        'builtin',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'targetvserverexpr',\\n    ]\\n    transforms = {\\n    }\\n    csaction_proxy = ConfigProxy(\\n        actual=csaction(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        ensure_feature_is_enabled(client, 'CS')\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.add()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not action_identical(client, module, csaction_proxy):\\n                immutables_changed = get_immutables_intersection(csaction_proxy, diff_list(client, module, csaction_proxy).keys())\\n                if immutables_changed != []:\\n                    module.fail_json(\\n                        msg='Cannot update immutable attributes %s' % (immutables_changed,),\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n                if not module.check_mode:\\n                    csaction_proxy.update()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            log('Sanity checks for state present')\\n            if not module.check_mode:\\n                if not action_exists(client, module):\\n                    module.fail_json(msg='Content switching action does not exist', **module_result)\\n                if not action_identical(client, module, csaction_proxy):\\n                    module.fail_json(\\n                        msg='Content switching action differs from configured',\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if action_exists(client, module):\\n                    module.fail_json(msg='Content switching action still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)"
  },
  {
    "code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.panel import pivot\\n        return pivot(self[index], self[columns], self[values])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.panel import pivot\\n        return pivot(self[index], self[columns], self[values])"
  },
  {
    "code": "def datetime_cast_time_sql(self, field_name, tzname):\\n\\t\\treturn \"django_datetime_cast_time(%s, %s, %s)\" % (\\n\\t\\t\\tfield_name,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Refs CVE-2022-34265 -- Properly escaped Extract() and Trunc() parameters.",
    "fixed_code": "def datetime_cast_time_sql(self, sql, params, tzname):\\n\\t\\treturn f\"django_datetime_cast_time({sql}, %s, %s)\", (\\n\\t\\t\\t*params,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)"
  },
  {
    "code": "class FieldStorage:\\n\\tdef __init__(self, fp=None, headers=None, outerboundary=b'',\\n\\t\\t\\t\\t environ=os.environ, keep_blank_values=0, strict_parsing=0,\\n\\t\\t\\t\\t limit=None, encoding='utf-8', errors='replace',\\n\\t\\t\\t\\t max_num_fields=None, separator='&'):\\n\\t\\tmethod = 'GET'\\n\\t\\tself.keep_blank_values = keep_blank_values\\n\\t\\tself.strict_parsing = strict_parsing\\n\\t\\tself.max_num_fields = max_num_fields\\n\\t\\tself.separator = separator\\n\\t\\tif 'REQUEST_METHOD' in environ:\\n\\t\\t\\tmethod = environ['REQUEST_METHOD'].upper()\\n\\t\\tself.qs_on_post = None\\n\\t\\tif method == 'GET' or method == 'HEAD':\\n\\t\\t\\tif 'QUERY_STRING' in environ:\\n\\t\\t\\t\\tqs = environ['QUERY_STRING']\\n\\t\\t\\telif sys.argv[1:]:\\n\\t\\t\\t\\tqs = sys.argv[1]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tqs = \"\"\\n\\t\\t\\tqs = qs.encode(locale.getpreferredencoding(), 'surrogateescape')\\n\\t\\t\\tfp = BytesIO(qs)\\n\\t\\t\\tif headers is None:\\n\\t\\t\\t\\theaders = {'content-type':\\n\\t\\t\\t\\t\\t\\t   \"application/x-www-form-urlencoded\"}\\n\\t\\tif headers is None:\\n\\t\\t\\theaders = {}\\n\\t\\t\\tif method == 'POST':\\n\\t\\t\\t\\theaders['content-type'] = \"application/x-www-form-urlencoded\"\\n\\t\\t\\tif 'CONTENT_TYPE' in environ:\\n\\t\\t\\t\\theaders['content-type'] = environ['CONTENT_TYPE']\\n\\t\\t\\tif 'QUERY_STRING' in environ:\\n\\t\\t\\t\\tself.qs_on_post = environ['QUERY_STRING']\\n\\t\\t\\tif 'CONTENT_LENGTH' in environ:\\n\\t\\t\\t\\theaders['content-length'] = environ['CONTENT_LENGTH']\\n\\t\\telse:\\n\\t\\t\\tif not (isinstance(headers, (Mapping, Message))):\\n\\t\\t\\t\\traise TypeError(\"headers must be mapping or an instance of \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"email.message.Message\")\\n\\t\\tself.headers = headers\\n\\t\\tif fp is None:\\n\\t\\t\\tself.fp = sys.stdin.buffer\\n\\t\\telif isinstance(fp, TextIOWrapper):\\n\\t\\t\\tself.fp = fp.buffer\\n\\t\\telse:\\n\\t\\t\\tif not (hasattr(fp, 'read') and hasattr(fp, 'readline')):\\n\\t\\t\\t\\traise TypeError(\"fp must be file pointer\")\\n\\t\\t\\tself.fp = fp\\n\\t\\tself.encoding = encoding\\n\\t\\tself.errors = errors\\n\\t\\tif not isinstance(outerboundary, bytes):\\n\\t\\t\\traise TypeError('outerboundary must be bytes, not %s'\\n\\t\\t\\t\\t\\t\\t\\t% type(outerboundary).__name__)\\n\\t\\tself.outerboundary = outerboundary\\n\\t\\tself.bytes_read = 0\\n\\t\\tself.limit = limit\\n\\t\\tcdisp, pdict = \"\", {}\\n\\t\\tif 'content-disposition' in self.headers:\\n\\t\\t\\tcdisp, pdict = parse_header(self.headers['content-disposition'])\\n\\t\\tself.disposition = cdisp\\n\\t\\tself.disposition_options = pdict\\n\\t\\tself.name = None\\n\\t\\tif 'name' in pdict:\\n\\t\\t\\tself.name = pdict['name']\\n\\t\\tself.filename = None\\n\\t\\tif 'filename' in pdict:\\n\\t\\t\\tself.filename = pdict['filename']\\n\\t\\tself._binary_file = self.filename is not None\\n\\t\\tif 'content-type' in self.headers:\\n\\t\\t\\tctype, pdict = parse_header(self.headers['content-type'])\\n\\t\\telif self.outerboundary or method != 'POST':\\n\\t\\t\\tctype, pdict = \"text/plain\", {}\\n\\t\\telse:\\n\\t\\t\\tctype, pdict = 'application/x-www-form-urlencoded', {}\\n\\t\\tself.type = ctype\\n\\t\\tself.type_options = pdict\\n\\t\\tif 'boundary' in pdict:\\n\\t\\t\\tself.innerboundary = pdict['boundary'].encode(self.encoding,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  self.errors)\\n\\t\\telse:\\n\\t\\t\\tself.innerboundary = b\"\"\\n\\t\\tclen = -1\\n\\t\\tif 'content-length' in self.headers:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tclen = int(self.headers['content-length'])\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass\\n\\t\\t\\tif maxlen and clen > maxlen:\\n\\t\\t\\t\\traise ValueError('Maximum content length exceeded')\\n\\t\\tself.length = clen\\n\\t\\tif self.limit is None and clen >= 0:\\n\\t\\t\\tself.limit = clen\\n\\t\\tself.list = self.file = None\\n\\t\\tself.done = 0\\n\\t\\tif ctype == 'application/x-www-form-urlencoded':\\n\\t\\t\\tself.read_urlencoded()\\n\\t\\telif ctype[:10] == 'multipart/':\\n\\t\\t\\tself.read_multi(environ, keep_blank_values, strict_parsing)\\n\\t\\telse:\\n\\t\\t\\tself.read_single()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class FieldStorage:\\n\\tdef __init__(self, fp=None, headers=None, outerboundary=b'',\\n\\t\\t\\t\\t environ=os.environ, keep_blank_values=0, strict_parsing=0,\\n\\t\\t\\t\\t limit=None, encoding='utf-8', errors='replace',\\n\\t\\t\\t\\t max_num_fields=None, separator='&'):\\n\\t\\tmethod = 'GET'\\n\\t\\tself.keep_blank_values = keep_blank_values\\n\\t\\tself.strict_parsing = strict_parsing\\n\\t\\tself.max_num_fields = max_num_fields\\n\\t\\tself.separator = separator\\n\\t\\tif 'REQUEST_METHOD' in environ:\\n\\t\\t\\tmethod = environ['REQUEST_METHOD'].upper()\\n\\t\\tself.qs_on_post = None\\n\\t\\tif method == 'GET' or method == 'HEAD':\\n\\t\\t\\tif 'QUERY_STRING' in environ:\\n\\t\\t\\t\\tqs = environ['QUERY_STRING']\\n\\t\\t\\telif sys.argv[1:]:\\n\\t\\t\\t\\tqs = sys.argv[1]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tqs = \"\"\\n\\t\\t\\tqs = qs.encode(locale.getpreferredencoding(), 'surrogateescape')\\n\\t\\t\\tfp = BytesIO(qs)\\n\\t\\t\\tif headers is None:\\n\\t\\t\\t\\theaders = {'content-type':\\n\\t\\t\\t\\t\\t\\t   \"application/x-www-form-urlencoded\"}\\n\\t\\tif headers is None:\\n\\t\\t\\theaders = {}\\n\\t\\t\\tif method == 'POST':\\n\\t\\t\\t\\theaders['content-type'] = \"application/x-www-form-urlencoded\"\\n\\t\\t\\tif 'CONTENT_TYPE' in environ:\\n\\t\\t\\t\\theaders['content-type'] = environ['CONTENT_TYPE']\\n\\t\\t\\tif 'QUERY_STRING' in environ:\\n\\t\\t\\t\\tself.qs_on_post = environ['QUERY_STRING']\\n\\t\\t\\tif 'CONTENT_LENGTH' in environ:\\n\\t\\t\\t\\theaders['content-length'] = environ['CONTENT_LENGTH']\\n\\t\\telse:\\n\\t\\t\\tif not (isinstance(headers, (Mapping, Message))):\\n\\t\\t\\t\\traise TypeError(\"headers must be mapping or an instance of \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"email.message.Message\")\\n\\t\\tself.headers = headers\\n\\t\\tif fp is None:\\n\\t\\t\\tself.fp = sys.stdin.buffer\\n\\t\\telif isinstance(fp, TextIOWrapper):\\n\\t\\t\\tself.fp = fp.buffer\\n\\t\\telse:\\n\\t\\t\\tif not (hasattr(fp, 'read') and hasattr(fp, 'readline')):\\n\\t\\t\\t\\traise TypeError(\"fp must be file pointer\")\\n\\t\\t\\tself.fp = fp\\n\\t\\tself.encoding = encoding\\n\\t\\tself.errors = errors\\n\\t\\tif not isinstance(outerboundary, bytes):\\n\\t\\t\\traise TypeError('outerboundary must be bytes, not %s'\\n\\t\\t\\t\\t\\t\\t\\t% type(outerboundary).__name__)\\n\\t\\tself.outerboundary = outerboundary\\n\\t\\tself.bytes_read = 0\\n\\t\\tself.limit = limit\\n\\t\\tcdisp, pdict = \"\", {}\\n\\t\\tif 'content-disposition' in self.headers:\\n\\t\\t\\tcdisp, pdict = parse_header(self.headers['content-disposition'])\\n\\t\\tself.disposition = cdisp\\n\\t\\tself.disposition_options = pdict\\n\\t\\tself.name = None\\n\\t\\tif 'name' in pdict:\\n\\t\\t\\tself.name = pdict['name']\\n\\t\\tself.filename = None\\n\\t\\tif 'filename' in pdict:\\n\\t\\t\\tself.filename = pdict['filename']\\n\\t\\tself._binary_file = self.filename is not None\\n\\t\\tif 'content-type' in self.headers:\\n\\t\\t\\tctype, pdict = parse_header(self.headers['content-type'])\\n\\t\\telif self.outerboundary or method != 'POST':\\n\\t\\t\\tctype, pdict = \"text/plain\", {}\\n\\t\\telse:\\n\\t\\t\\tctype, pdict = 'application/x-www-form-urlencoded', {}\\n\\t\\tself.type = ctype\\n\\t\\tself.type_options = pdict\\n\\t\\tif 'boundary' in pdict:\\n\\t\\t\\tself.innerboundary = pdict['boundary'].encode(self.encoding,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  self.errors)\\n\\t\\telse:\\n\\t\\t\\tself.innerboundary = b\"\"\\n\\t\\tclen = -1\\n\\t\\tif 'content-length' in self.headers:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tclen = int(self.headers['content-length'])\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass\\n\\t\\t\\tif maxlen and clen > maxlen:\\n\\t\\t\\t\\traise ValueError('Maximum content length exceeded')\\n\\t\\tself.length = clen\\n\\t\\tif self.limit is None and clen >= 0:\\n\\t\\t\\tself.limit = clen\\n\\t\\tself.list = self.file = None\\n\\t\\tself.done = 0\\n\\t\\tif ctype == 'application/x-www-form-urlencoded':\\n\\t\\t\\tself.read_urlencoded()\\n\\t\\telif ctype[:10] == 'multipart/':\\n\\t\\t\\tself.read_multi(environ, keep_blank_values, strict_parsing)\\n\\t\\telse:\\n\\t\\t\\tself.read_single()"
  },
  {
    "code": "def read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,\\n                   parse_dates=None):\\n    pandas_sql = pandasSQL_builder(con)\\n    return pandas_sql.read_query(\\n        sql, index_col=index_col, params=params, coerce_float=coerce_float,\\n        parse_dates=parse_dates)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,\\n                   parse_dates=None):\\n    pandas_sql = pandasSQL_builder(con)\\n    return pandas_sql.read_query(\\n        sql, index_col=index_col, params=params, coerce_float=coerce_float,\\n        parse_dates=parse_dates)"
  },
  {
    "code": "def update_device_hostname(fmgr, paramgram):\\n    datagram = {\\n        \"hostname\": paramgram[\"device_hostname\"]\\n    }\\n    url = \"pm/config/device/{device_name}/global/system/global\".format(device_name=paramgram[\"device_unique_name\"])\\n    response = fmgr.process_request(url, datagram, FMGRMethods.UPDATE)\\n    return response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_device_hostname(fmgr, paramgram):\\n    datagram = {\\n        \"hostname\": paramgram[\"device_hostname\"]\\n    }\\n    url = \"pm/config/device/{device_name}/global/system/global\".format(device_name=paramgram[\"device_unique_name\"])\\n    response = fmgr.process_request(url, datagram, FMGRMethods.UPDATE)\\n    return response"
  },
  {
    "code": "def _reconstruct_object(typ, obj, axes, dtype):\\n    try:\\n        typ = typ.type\\n    except AttributeError:\\n        pass\\n    try:\\n        res_t = np.result_type(obj.dtype, dtype)\\n    except AttributeError:\\n        res_t = dtype\\n    if (not isinstance(typ, partial) and\\n        issubclass(typ, pd.core.generic.PandasObject)):\\n        return typ(obj, dtype=res_t, **axes)\\n    ret_value = typ(obj).astype(res_t)\\n    try:\\n        ret = ret_value.item()\\n    except ValueError:\\n        ret = ret_value\\n    return ret",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _reconstruct_object(typ, obj, axes, dtype):\\n    try:\\n        typ = typ.type\\n    except AttributeError:\\n        pass\\n    try:\\n        res_t = np.result_type(obj.dtype, dtype)\\n    except AttributeError:\\n        res_t = dtype\\n    if (not isinstance(typ, partial) and\\n        issubclass(typ, pd.core.generic.PandasObject)):\\n        return typ(obj, dtype=res_t, **axes)\\n    ret_value = typ(obj).astype(res_t)\\n    try:\\n        ret = ret_value.item()\\n    except ValueError:\\n        ret = ret_value\\n    return ret"
  },
  {
    "code": "def freqstr(self):\\n        try:\\n            code = self.rule_code\\n        except NotImplementedError:\\n            return repr(self)\\n        if self.n != 1:\\n            return '%d%s' % (self.n, code)\\n        else:\\n            return code",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: freqstr with offset #1184",
    "fixed_code": "def freqstr(self):\\n        try:\\n            code = self.rule_code\\n        except NotImplementedError:\\n            return repr(self)\\n        if self.n != 1:\\n            fstr = '%d%s' % (self.n, code)\\n        else:\\n            fstr = code\\n        return fstr\\nclass BusinessDay(CacheableOffset, DateOffset):"
  },
  {
    "code": "def build_payload_for_slack(module, text, channel, username, icon_url, icon_emoji, link_names, parse, color, attachments):\\n    payload = {}\\n    if color == \"normal\" and text is not None:\\n        payload = dict(text=escape_quotes(text))\\n    elif text is not None:\\n        payload = dict(attachments=[dict(text=escape_quotes(text), color=color, mrkdwn_in=[\"text\"])])\\n    if channel is not None:\\n        if (channel[0] == '\\n            payload['channel'] = channel\\n        else:\\n            payload['channel'] = '\\n    if username is not None:\\n        payload['username'] = username\\n    if icon_emoji is not None:\\n        payload['icon_emoji'] = icon_emoji\\n    else:\\n        payload['icon_url'] = icon_url\\n    if link_names is not None:\\n        payload['link_names'] = link_names\\n    if parse is not None:\\n        payload['parse'] = parse\\n    if attachments is not None:\\n        if 'attachments' not in payload:\\n            payload['attachments'] = []\\n    if attachments is not None:\\n        keys_to_escape = [\\n            'title',\\n            'text',\\n            'author_name',\\n            'pretext',\\n            'fallback',\\n        ]\\n        for attachment in attachments:\\n            for key in keys_to_escape:\\n                if key in attachment:\\n                    attachment[key] = escape_quotes(attachment[key])\\n            if 'fallback' not in attachment:\\n                attachment['fallback'] = attachment['text']\\n            payload['attachments'].append(attachment)\\n    payload = module.jsonify(payload)\\n    return payload",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add threading to slack notification module (#47333)",
    "fixed_code": "def build_payload_for_slack(module, text, channel, thread_id, username, icon_url, icon_emoji, link_names,\\n                            parse, color, attachments):\\n    payload = {}\\n    if color == \"normal\" and text is not None:\\n        payload = dict(text=escape_quotes(text))\\n    elif text is not None:\\n        payload = dict(attachments=[dict(text=escape_quotes(text), color=color, mrkdwn_in=[\"text\"])])\\n    if channel is not None:\\n        if (channel[0] == '\\n            payload['channel'] = channel\\n        else:\\n            payload['channel'] = '\\n    if thread_id is not None:\\n        payload['thread_ts'] = thread_id\\n    if username is not None:\\n        payload['username'] = username\\n    if icon_emoji is not None:\\n        payload['icon_emoji'] = icon_emoji\\n    else:\\n        payload['icon_url'] = icon_url\\n    if link_names is not None:\\n        payload['link_names'] = link_names\\n    if parse is not None:\\n        payload['parse'] = parse\\n    if attachments is not None:\\n        if 'attachments' not in payload:\\n            payload['attachments'] = []\\n    if attachments is not None:\\n        keys_to_escape = [\\n            'title',\\n            'text',\\n            'author_name',\\n            'pretext',\\n            'fallback',\\n        ]\\n        for attachment in attachments:\\n            for key in keys_to_escape:\\n                if key in attachment:\\n                    attachment[key] = escape_quotes(attachment[key])\\n            if 'fallback' not in attachment:\\n                attachment['fallback'] = attachment['text']\\n            payload['attachments'].append(attachment)\\n    payload = module.jsonify(payload)\\n    return payload"
  },
  {
    "code": "def _Num(self, t):\\n        strnum = repr(t.n)\\n        if strnum.startswith(\"-\"):\\n            self.write(\"(\")\\n            self.write(strnum)\\n            self.write(\")\")\\n        else:\\n            self.write(strnum)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add parentheses around numeric literals, to avoid turning 3 .bit_length() into 3.bit_length().",
    "fixed_code": "def _Num(self, t):\\n        self.write(\"(\")\\n        self.write(repr(t.n))\\n        self.write(\")\")"
  },
  {
    "code": "def _convert_types(values, na_values):\\n    if issubclass(values.dtype.type, (np.number, np.bool_)):\\n        return values\\n    try:\\n        values = lib.maybe_convert_numeric(values, na_values)\\n    except Exception:\\n        lib.sanitize_objects(values, na_values)\\n    if values.dtype == np.object_:\\n        return lib.maybe_convert_bool(values)\\n    return values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_types(values, na_values):\\n    if issubclass(values.dtype.type, (np.number, np.bool_)):\\n        return values\\n    try:\\n        values = lib.maybe_convert_numeric(values, na_values)\\n    except Exception:\\n        lib.sanitize_objects(values, na_values)\\n    if values.dtype == np.object_:\\n        return lib.maybe_convert_bool(values)\\n    return values"
  },
  {
    "code": "def copy(self):\\n        return make_block(self.values.copy(), self.ref_locs,\\n                          self.ref_columns, _columns=self._columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def copy(self):\\n        return make_block(self.values.copy(), self.columns,\\n                          self.ref_columns)"
  },
  {
    "code": "def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\\n        max_allowed = 1\\n        if current_line.depth == 0:\\n            max_allowed = 1 if self.is_pyi else 2\\n        if current_line.leaves:\\n            first_leaf = current_line.leaves[0]\\n            before = first_leaf.prefix.count(\"\\n\")\\n            before = min(before, max_allowed)\\n            first_leaf.prefix = \"\"\\n        else:\\n            before = 0\\n        depth = current_line.depth\\n        while self.previous_defs and self.previous_defs[-1] >= depth:\\n            self.previous_defs.pop()\\n            if self.is_pyi:\\n                before = 0 if depth else 1\\n            else:\\n                before = 1 if depth else 2\\n        is_decorator = current_line.is_decorator\\n        if is_decorator or current_line.is_def or current_line.is_class:\\n            if not is_decorator:\\n                self.previous_defs.append(depth)\\n            if self.previous_line is None:\\n                return 0, 0\\n            if self.previous_line.is_decorator:\\n                return 0, 0\\n            if (\\n                self.previous_line.is_comment\\n                and self.previous_line.depth == current_line.depth\\n                and before == 0\\n            ):\\n                return 0, 0\\n            if self.is_pyi:\\n                if self.previous_line.depth > current_line.depth:\\n                    newlines = 1\\n                elif current_line.is_class or self.previous_line.is_class:\\n                    if (\\n                        current_line.is_trivial_class\\n                        and self.previous_line.is_trivial_class\\n                    ):\\n                        newlines = 0\\n                    else:\\n                        newlines = 1\\n                else:\\n                    newlines = 0\\n            else:\\n                newlines = 2\\n            if current_line.depth and newlines:\\n                newlines -= 1\\n            return newlines, 0\\n        if (\\n            self.previous_line\\n            and self.previous_line.is_import\\n            and not current_line.is_import\\n            and depth == self.previous_line.depth\\n        ):\\n            return (before or 1), 0\\n        return before, 0",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\\n        max_allowed = 1\\n        if current_line.depth == 0:\\n            max_allowed = 1 if self.is_pyi else 2\\n        if current_line.leaves:\\n            first_leaf = current_line.leaves[0]\\n            before = first_leaf.prefix.count(\"\\n\")\\n            before = min(before, max_allowed)\\n            first_leaf.prefix = \"\"\\n        else:\\n            before = 0\\n        depth = current_line.depth\\n        while self.previous_defs and self.previous_defs[-1] >= depth:\\n            self.previous_defs.pop()\\n            if self.is_pyi:\\n                before = 0 if depth else 1\\n            else:\\n                before = 1 if depth else 2\\n        is_decorator = current_line.is_decorator\\n        if is_decorator or current_line.is_def or current_line.is_class:\\n            if not is_decorator:\\n                self.previous_defs.append(depth)\\n            if self.previous_line is None:\\n                return 0, 0\\n            if self.previous_line.is_decorator:\\n                return 0, 0\\n            if (\\n                self.previous_line.is_comment\\n                and self.previous_line.depth == current_line.depth\\n                and before == 0\\n            ):\\n                return 0, 0\\n            if self.is_pyi:\\n                if self.previous_line.depth > current_line.depth:\\n                    newlines = 1\\n                elif current_line.is_class or self.previous_line.is_class:\\n                    if (\\n                        current_line.is_trivial_class\\n                        and self.previous_line.is_trivial_class\\n                    ):\\n                        newlines = 0\\n                    else:\\n                        newlines = 1\\n                else:\\n                    newlines = 0\\n            else:\\n                newlines = 2\\n            if current_line.depth and newlines:\\n                newlines -= 1\\n            return newlines, 0\\n        if (\\n            self.previous_line\\n            and self.previous_line.is_import\\n            and not current_line.is_import\\n            and depth == self.previous_line.depth\\n        ):\\n            return (before or 1), 0\\n        return before, 0"
  },
  {
    "code": "def parse_bits(parser, bits, params, varargs, varkw, defaults,\\n               kwonly, kwonly_defaults, takes_context, name):\\n    if takes_context:\\n        if params[0] == 'context':\\n            params = params[1:]\\n        else:\\n            raise TemplateSyntaxError(\\n                \"'%s' is decorated with takes_context=True so it must \"\\n                \"have a first argument of 'context'\" % name)\\n    args = []\\n    kwargs = {}\\n    unhandled_params = list(params)\\n    unhandled_kwargs = [\\n        kwarg for kwarg in kwonly\\n        if not kwonly_defaults or kwarg not in kwonly_defaults\\n    ]\\n    for bit in bits:\\n        kwarg = token_kwargs([bit], parser)\\n        if kwarg:\\n            param, value = kwarg.popitem()\\n            if param not in params and param not in kwonly and varkw is None:\\n                raise TemplateSyntaxError(\\n                    \"'%s' received unexpected keyword argument '%s'\" %\\n                    (name, param))\\n            elif param in kwargs:\\n                raise TemplateSyntaxError(\\n                    \"'%s' received multiple values for keyword argument '%s'\" %\\n                    (name, param))\\n            else:\\n                kwargs[str(param)] = value\\n                if param in unhandled_params:\\n                    unhandled_params.remove(param)\\n                elif param in unhandled_kwargs:\\n                    unhandled_kwargs.remove(param)\\n        else:\\n            if kwargs:\\n                raise TemplateSyntaxError(\\n                    \"'%s' received some positional argument(s) after some \"\\n                    \"keyword argument(s)\" % name)\\n            else:\\n                args.append(parser.compile_filter(bit))\\n                try:\\n                    unhandled_params.pop(0)\\n                except IndexError:\\n                    if varargs is None:\\n                        raise TemplateSyntaxError(\\n                            \"'%s' received too many positional arguments\" %\\n                            name)\\n    if defaults is not None:\\n        unhandled_params = unhandled_params[:-len(defaults)]\\n    if unhandled_params or unhandled_kwargs:\\n        raise TemplateSyntaxError(\\n            \"'%s' did not receive value(s) for the argument(s): %s\" %\\n            (name, \", \".join(\"'%s'\" % p for p in unhandled_params + unhandled_kwargs)))\\n    return args, kwargs",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #33036 -- Made simple_tag()/inclusion_tag() with takes_context raise TemplateSyntaxError when function has no parameters.",
    "fixed_code": "def parse_bits(parser, bits, params, varargs, varkw, defaults,\\n               kwonly, kwonly_defaults, takes_context, name):\\n    if takes_context:\\n        if params and params[0] == 'context':\\n            params = params[1:]\\n        else:\\n            raise TemplateSyntaxError(\\n                \"'%s' is decorated with takes_context=True so it must \"\\n                \"have a first argument of 'context'\" % name)\\n    args = []\\n    kwargs = {}\\n    unhandled_params = list(params)\\n    unhandled_kwargs = [\\n        kwarg for kwarg in kwonly\\n        if not kwonly_defaults or kwarg not in kwonly_defaults\\n    ]\\n    for bit in bits:\\n        kwarg = token_kwargs([bit], parser)\\n        if kwarg:\\n            param, value = kwarg.popitem()\\n            if param not in params and param not in kwonly and varkw is None:\\n                raise TemplateSyntaxError(\\n                    \"'%s' received unexpected keyword argument '%s'\" %\\n                    (name, param))\\n            elif param in kwargs:\\n                raise TemplateSyntaxError(\\n                    \"'%s' received multiple values for keyword argument '%s'\" %\\n                    (name, param))\\n            else:\\n                kwargs[str(param)] = value\\n                if param in unhandled_params:\\n                    unhandled_params.remove(param)\\n                elif param in unhandled_kwargs:\\n                    unhandled_kwargs.remove(param)\\n        else:\\n            if kwargs:\\n                raise TemplateSyntaxError(\\n                    \"'%s' received some positional argument(s) after some \"\\n                    \"keyword argument(s)\" % name)\\n            else:\\n                args.append(parser.compile_filter(bit))\\n                try:\\n                    unhandled_params.pop(0)\\n                except IndexError:\\n                    if varargs is None:\\n                        raise TemplateSyntaxError(\\n                            \"'%s' received too many positional arguments\" %\\n                            name)\\n    if defaults is not None:\\n        unhandled_params = unhandled_params[:-len(defaults)]\\n    if unhandled_params or unhandled_kwargs:\\n        raise TemplateSyntaxError(\\n            \"'%s' did not receive value(s) for the argument(s): %s\" %\\n            (name, \", \".join(\"'%s'\" % p for p in unhandled_params + unhandled_kwargs)))\\n    return args, kwargs"
  },
  {
    "code": "def snapmirror_break(self, destination=None):\\n        if destination is None:\\n            destination = self.parameters['destination_path']\\n        options = {'destination-location': destination}\\n        snapmirror_break = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'snapmirror-break', **options)\\n        try:\\n            self.server.invoke_successfully(snapmirror_break,\\n                                            enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error breaking SnapMirror relationship : %s'\\n                                      % (to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def snapmirror_break(self, destination=None):\\n        if destination is None:\\n            destination = self.parameters['destination_path']\\n        options = {'destination-location': destination}\\n        snapmirror_break = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'snapmirror-break', **options)\\n        try:\\n            self.server.invoke_successfully(snapmirror_break,\\n                                            enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error breaking SnapMirror relationship : %s'\\n                                      % (to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def getboolean(self, section, key, **kwargs):\\n        val = str(self.get(section, key, **kwargs)).lower().strip()\\n        if '\\n            val = val.split('\\n        if val in ('t', 'true', '1'):\\n            return True\\n        elif val in ('f', 'false', '0'):\\n            return False\\n        else:\\n            raise AirflowConfigException(\\n                'The value for configuration option \"{}:{}\" is not a '\\n                'boolean (received \"{}\").'.format(section, key, val))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getboolean(self, section, key, **kwargs):\\n        val = str(self.get(section, key, **kwargs)).lower().strip()\\n        if '\\n            val = val.split('\\n        if val in ('t', 'true', '1'):\\n            return True\\n        elif val in ('f', 'false', '0'):\\n            return False\\n        else:\\n            raise AirflowConfigException(\\n                'The value for configuration option \"{}:{}\" is not a '\\n                'boolean (received \"{}\").'.format(section, key, val))"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        domain=dict(required=True, type='str'),\\n        role_priority=dict(required=False, type='str'),\\n        system_priority=dict(required=False, type='str'),\\n        pkl_src=dict(required=False),\\n        pkl_dest=dict(required=False),\\n        pkl_vrf=dict(required=False),\\n        peer_gw=dict(required=False, type='bool'),\\n        auto_recovery=dict(required=False, type='bool'),\\n        auto_recovery_reload_delay=dict(required=False, type='str'),\\n        delay_restore=dict(required=False, type='str'),\\n        delay_restore_interface_vlan=dict(required=False, type='str'),\\n        delay_restore_orphan_port=dict(required=False, type='str'),\\n        state=dict(choices=['absent', 'present'], default='present'),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    mutually_exclusive = [('auto_recovery', 'auto_recovery_reload_delay')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    results = {'changed': False, 'warnings': warnings}\\n    domain = module.params['domain']\\n    role_priority = module.params['role_priority']\\n    system_priority = module.params['system_priority']\\n    pkl_src = module.params['pkl_src']\\n    pkl_dest = module.params['pkl_dest']\\n    pkl_vrf = module.params['pkl_vrf']\\n    peer_gw = module.params['peer_gw']\\n    auto_recovery = module.params['auto_recovery']\\n    auto_recovery_reload_delay = module.params['auto_recovery_reload_delay']\\n    delay_restore = module.params['delay_restore']\\n    delay_restore_interface_vlan = module.params['delay_restore_interface_vlan']\\n    delay_restore_orphan_port = module.params['delay_restore_orphan_port']\\n    state = module.params['state']\\n    args = dict(domain=domain, role_priority=role_priority,\\n                system_priority=system_priority, pkl_src=pkl_src,\\n                pkl_dest=pkl_dest, pkl_vrf=pkl_vrf, peer_gw=peer_gw,\\n                auto_recovery=auto_recovery,\\n                auto_recovery_reload_delay=auto_recovery_reload_delay,\\n                delay_restore=delay_restore,\\n                delay_restore_interface_vlan=delay_restore_interface_vlan,\\n                delay_restore_orphan_port=delay_restore_orphan_port,\\n                )\\n    if not pkl_dest:\\n        if pkl_src:\\n            module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                 ' when src IP is present')\\n        elif pkl_vrf:\\n            if pkl_vrf != 'management':\\n                module.fail_json(msg='dest and src IP for peer-keepalive are required'\\n                                     ' when vrf is present')\\n            else:\\n                module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                     ' when vrf is present')\\n    if pkl_vrf:\\n        if pkl_vrf.lower() not in get_vrf_list(module):\\n            module.fail_json(msg='The VRF you are trying to use for the peer '\\n                                 'keepalive link is not on device yet. Add it'\\n                                 ' first, please.')\\n    proposed = dict((k, v) for k, v in args.items() if v is not None)\\n    existing = get_vpc(module)\\n    commands = []\\n    if state == 'present':\\n        delta = {}\\n        for key, value in proposed.items():\\n            if str(value).lower() == 'default' and key != 'pkl_vrf':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n            if existing.get(key) != value:\\n                delta[key] = value\\n        if delta:\\n            pkl_dependencies(module, delta, existing)\\n            command = get_commands_to_config_vpc(module, delta, domain, existing)\\n            commands.append(command)\\n    elif state == 'absent':\\n        if existing:\\n            if domain != existing['domain']:\\n                module.fail_json(msg=\"You are trying to remove a domain that \"\\n                                     \"does not exist on the device\")\\n            else:\\n                commands.append('terminal dont-ask')\\n                commands.append('no vpc domain {0}'.format(domain))\\n    cmds = flatten_list(commands)\\n    results['commands'] = cmds\\n    if cmds:\\n        results['changed'] = True\\n        if not module.check_mode:\\n            load_config(module, cmds)\\n            if 'configure' in cmds:\\n                cmds.pop(0)\\n    module.exit_json(**results)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        domain=dict(required=True, type='str'),\\n        role_priority=dict(required=False, type='str'),\\n        system_priority=dict(required=False, type='str'),\\n        pkl_src=dict(required=False),\\n        pkl_dest=dict(required=False),\\n        pkl_vrf=dict(required=False),\\n        peer_gw=dict(required=False, type='bool'),\\n        auto_recovery=dict(required=False, type='bool'),\\n        auto_recovery_reload_delay=dict(required=False, type='str'),\\n        delay_restore=dict(required=False, type='str'),\\n        delay_restore_interface_vlan=dict(required=False, type='str'),\\n        delay_restore_orphan_port=dict(required=False, type='str'),\\n        state=dict(choices=['absent', 'present'], default='present'),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    mutually_exclusive = [('auto_recovery', 'auto_recovery_reload_delay')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    results = {'changed': False, 'warnings': warnings}\\n    domain = module.params['domain']\\n    role_priority = module.params['role_priority']\\n    system_priority = module.params['system_priority']\\n    pkl_src = module.params['pkl_src']\\n    pkl_dest = module.params['pkl_dest']\\n    pkl_vrf = module.params['pkl_vrf']\\n    peer_gw = module.params['peer_gw']\\n    auto_recovery = module.params['auto_recovery']\\n    auto_recovery_reload_delay = module.params['auto_recovery_reload_delay']\\n    delay_restore = module.params['delay_restore']\\n    delay_restore_interface_vlan = module.params['delay_restore_interface_vlan']\\n    delay_restore_orphan_port = module.params['delay_restore_orphan_port']\\n    state = module.params['state']\\n    args = dict(domain=domain, role_priority=role_priority,\\n                system_priority=system_priority, pkl_src=pkl_src,\\n                pkl_dest=pkl_dest, pkl_vrf=pkl_vrf, peer_gw=peer_gw,\\n                auto_recovery=auto_recovery,\\n                auto_recovery_reload_delay=auto_recovery_reload_delay,\\n                delay_restore=delay_restore,\\n                delay_restore_interface_vlan=delay_restore_interface_vlan,\\n                delay_restore_orphan_port=delay_restore_orphan_port,\\n                )\\n    if not pkl_dest:\\n        if pkl_src:\\n            module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                 ' when src IP is present')\\n        elif pkl_vrf:\\n            if pkl_vrf != 'management':\\n                module.fail_json(msg='dest and src IP for peer-keepalive are required'\\n                                     ' when vrf is present')\\n            else:\\n                module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                     ' when vrf is present')\\n    if pkl_vrf:\\n        if pkl_vrf.lower() not in get_vrf_list(module):\\n            module.fail_json(msg='The VRF you are trying to use for the peer '\\n                                 'keepalive link is not on device yet. Add it'\\n                                 ' first, please.')\\n    proposed = dict((k, v) for k, v in args.items() if v is not None)\\n    existing = get_vpc(module)\\n    commands = []\\n    if state == 'present':\\n        delta = {}\\n        for key, value in proposed.items():\\n            if str(value).lower() == 'default' and key != 'pkl_vrf':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n            if existing.get(key) != value:\\n                delta[key] = value\\n        if delta:\\n            pkl_dependencies(module, delta, existing)\\n            command = get_commands_to_config_vpc(module, delta, domain, existing)\\n            commands.append(command)\\n    elif state == 'absent':\\n        if existing:\\n            if domain != existing['domain']:\\n                module.fail_json(msg=\"You are trying to remove a domain that \"\\n                                     \"does not exist on the device\")\\n            else:\\n                commands.append('terminal dont-ask')\\n                commands.append('no vpc domain {0}'.format(domain))\\n    cmds = flatten_list(commands)\\n    results['commands'] = cmds\\n    if cmds:\\n        results['changed'] = True\\n        if not module.check_mode:\\n            load_config(module, cmds)\\n            if 'configure' in cmds:\\n                cmds.pop(0)\\n    module.exit_json(**results)"
  },
  {
    "code": "def remove_pipe(self, name):\\n\\t\\tif name not in self.pipe_names:\\n\\t\\t\\traise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\\n\\t\\tremoved = self.pipeline.pop(self.pipe_names.index(name))\\n\\t\\tif ENABLE_PIPELINE_ANALYSIS:\\n\\t\\t\\tanalyze_all_pipes(self.pipeline)\\n\\t\\treturn removed",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def remove_pipe(self, name):\\n\\t\\tif name not in self.pipe_names:\\n\\t\\t\\traise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\\n\\t\\tremoved = self.pipeline.pop(self.pipe_names.index(name))\\n\\t\\tif ENABLE_PIPELINE_ANALYSIS:\\n\\t\\t\\tanalyze_all_pipes(self.pipeline)\\n\\t\\treturn removed"
  },
  {
    "code": "def __init__(\\n        self,\\n        base_estimator=None,\\n        n_estimators=10,\\n        *,\\n        max_samples=1.0,\\n        max_features=1.0,\\n        bootstrap=True,\\n        bootstrap_features=False,\\n        oob_score=False,\\n        warm_start=False,\\n        n_jobs=None,\\n        random_state=None,\\n        verbose=0,\\n    ):\\n        super().__init__(base_estimator=base_estimator, n_estimators=n_estimators)\\n        self.max_samples = max_samples\\n        self.max_features = max_features\\n        self.bootstrap = bootstrap\\n        self.bootstrap_features = bootstrap_features\\n        self.oob_score = oob_score\\n        self.warm_start = warm_start\\n        self.n_jobs = n_jobs\\n        self.random_state = random_state\\n        self.verbose = verbose",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT rename and deprecate `base_estimator` in favor of `estimator` in ensemble classes (#23819)",
    "fixed_code": "def __init__(\\n        self,\\n        estimator=None,\\n        n_estimators=10,\\n        *,\\n        max_samples=1.0,\\n        max_features=1.0,\\n        bootstrap=True,\\n        bootstrap_features=False,\\n        oob_score=False,\\n        warm_start=False,\\n        n_jobs=None,\\n        random_state=None,\\n        verbose=0,\\n        base_estimator=\"deprecated\",\\n    ):\\n        super().__init__(\\n            estimator=estimator,\\n            n_estimators=n_estimators,\\n            base_estimator=base_estimator,\\n        )\\n        self.max_samples = max_samples\\n        self.max_features = max_features\\n        self.bootstrap = bootstrap\\n        self.bootstrap_features = bootstrap_features\\n        self.oob_score = oob_score\\n        self.warm_start = warm_start\\n        self.n_jobs = n_jobs\\n        self.random_state = random_state\\n        self.verbose = verbose"
  },
  {
    "code": "def get_all_permissions(self, user_obj, obj=None):\\n        if not user_obj.is_active or obj is not None:\\n            return set()\\n        if not hasattr(user_obj, '_perm_cache'):\\n            user_obj._perm_cache = self.get_user_permissions(user_obj)\\n            user_obj._perm_cache.update(self.get_group_permissions(user_obj))\\n        return user_obj._perm_cache",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Restored is_anonymous() check in ModelBackend permission checking removed in refs #17903.\\n\\nThanks Florian Apolloner for raising the issue.",
    "fixed_code": "def get_all_permissions(self, user_obj, obj=None):\\n        if not user_obj.is_active or user_obj.is_anonymous() or obj is not None:\\n            return set()\\n        if not hasattr(user_obj, '_perm_cache'):\\n            user_obj._perm_cache = self.get_user_permissions(user_obj)\\n            user_obj._perm_cache.update(self.get_group_permissions(user_obj))\\n        return user_obj._perm_cache"
  },
  {
    "code": "def _generate_marginal_results(\\n    table,\\n    data,\\n    values,\\n    rows,\\n    cols,\\n    aggfunc,\\n    observed,\\n    grand_margin,\\n    margins_name: str = \"All\",\\n):\\n    if len(cols) > 0:\\n        table_pieces = []\\n        margin_keys = []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: df.pivot_table fails when margin is True and only columns is defined (#31088)",
    "fixed_code": "def _generate_marginal_results(\\n    table, data, values, rows, cols, aggfunc, observed, margins_name: str = \"All\",\\n):\\n    if len(cols) > 0:\\n        table_pieces = []\\n        margin_keys = []"
  },
  {
    "code": "def _repr_footer(self):\\n        return u('Length: %d\\n%s') % (len(self), self._repr_categories_info())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _repr_footer(self):\\n        return u('Length: %d\\n%s') % (len(self), self._repr_categories_info())"
  },
  {
    "code": "def create_model(self, project_name, model):\\n        assert model['name'] is not None and model['name'] is not ''\\n        project = 'projects/{}'.format(project_name)\\n        request = self._cloudml.projects().models().create(\\n            parent=project, body=model)\\n        return request.execute()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-1401] Standardize cloud ml operator arguments\\n\\nStandardize on project_id, to be consistent with\\nother cloud operators,\\nbetter-supporting default arguments.\\n\\nThis is one of multiple commits that will be\\nrequired to resolve\\nAIRFLOW-1401.\\n\\nCloses #2439 from peterjdolan/cloudml_project_id",
    "fixed_code": "def create_model(self, project_id, model):\\n        assert model['name'] is not None and model['name'] is not ''\\n        project = 'projects/{}'.format(project_id)\\n        request = self._cloudml.projects().models().create(\\n            parent=project, body=model)\\n        return request.execute()"
  },
  {
    "code": "def predict_proba(self, X):\\n        Xt = X\\n        for name, transform in self.steps[:-1]:\\n            Xt = transform.transform(Xt)\\n        return self.steps[-1][-1].predict_proba(Xt)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH add ``decision_function`` to ``Pipeline``",
    "fixed_code": "def predict_proba(self, X):\\n        Xt = X\\n        for name, transform in self.steps[:-1]:\\n            Xt = transform.transform(Xt)\\n        return self.steps[-1][-1].predict_proba(Xt)"
  },
  {
    "code": "def _create_sql_schema(self, frame, table_name, keys=None):\\n        table = SQLTable(table_name, self, frame=frame, index=False, keys=keys)\\n        return str(table.sql_schema())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_sql_schema(self, frame, table_name, keys=None):\\n        table = SQLTable(table_name, self, frame=frame, index=False, keys=keys)\\n        return str(table.sql_schema())"
  },
  {
    "code": "def mktemp(suffix=\"\", prefix=template, dir=None):\\n    if dir is None:\\n        dir = gettempdir()\\n    names = _get_candidate_names()\\n    for seq in range(TMP_MAX):\\n        name = next(names)\\n        file = _os.path.join(dir, prefix + name + suffix)\\n        if not _exists(file):\\n            return file\\n    raise FileExistsError(\"No usable temporary filename found\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #17169: Restore errno in tempfile exceptions.",
    "fixed_code": "def mktemp(suffix=\"\", prefix=template, dir=None):\\n    if dir is None:\\n        dir = gettempdir()\\n    names = _get_candidate_names()\\n    for seq in range(TMP_MAX):\\n        name = next(names)\\n        file = _os.path.join(dir, prefix + name + suffix)\\n        if not _exists(file):\\n            return file\\n    raise FileExistsError(_errno.EEXIST,\\n                          \"No usable temporary filename found\")"
  },
  {
    "code": "def to_sql(self, name, con, schema=None, if_exists='fail', index=True,\\n               index_label=None, chunksize=None, dtype=None, method=None):\\n        from pandas.io import sql\\n        sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\\n                   index=index, index_label=index_label, chunksize=chunksize,\\n                   dtype=dtype, method=method)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_sql(self, name, con, schema=None, if_exists='fail', index=True,\\n               index_label=None, chunksize=None, dtype=None, method=None):\\n        from pandas.io import sql\\n        sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\\n                   index=index, index_label=index_label, chunksize=chunksize,\\n                   dtype=dtype, method=method)"
  },
  {
    "code": "def final_key_selected(self, event):\\n        \"Handler for clicking on key in basic settings list.\"\\n        self.build_key_string()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-35598: IDLE: Increase test coverage for config_key.py (#11360)",
    "fixed_code": "def final_key_selected(self, event=None):\\n        \"Handler for clicking on key in basic settings list.\"\\n        self.build_key_string()"
  },
  {
    "code": "def set_core_affinity(cores: List[int]) -> None:\\n  if not PC:\\n    os.sched_setaffinity(0, cores)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_core_affinity(cores: List[int]) -> None:\\n  if not PC:\\n    os.sched_setaffinity(0, cores)"
  },
  {
    "code": "def diff(arr, n: int, axis: int = 0, stacklevel=3):\\n    from pandas.core.arrays import PandasDtype\\n    n = int(n)\\n    na = np.nan\\n    dtype = arr.dtype\\n    if dtype.kind == \"b\":\\n        op = operator.xor\\n    else:\\n        op = operator.sub\\n    if isinstance(dtype, PandasDtype):\\n        arr = np.asarray(arr)\\n        dtype = arr.dtype\\n    if is_extension_array_dtype(dtype):\\n        if hasattr(arr, f\"__{op.__name__}__\"):\\n            return op(arr, arr.shift(n))\\n        else:\\n            warn(\\n                \"dtype lost in 'diff()'. In the future this will raise a \"\\n                \"TypeError. Convert to a suitable dtype prior to calling 'diff'.\",\\n                FutureWarning,\\n                stacklevel=stacklevel,\\n            )\\n            arr = np.asarray(arr)\\n            dtype = arr.dtype\\n    is_timedelta = False\\n    is_bool = False\\n    if needs_i8_conversion(arr):\\n        dtype = np.float64\\n        arr = arr.view(\"i8\")\\n        na = iNaT\\n        is_timedelta = True\\n    elif is_bool_dtype(dtype):\\n        dtype = np.object_\\n        is_bool = True\\n    elif is_integer_dtype(dtype):\\n        dtype = np.float64\\n    dtype = np.dtype(dtype)\\n    out_arr = np.empty(arr.shape, dtype=dtype)\\n    na_indexer = [slice(None)] * arr.ndim\\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n    out_arr[tuple(na_indexer)] = na\\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\\n        algos.diff_2d(arr, out_arr, n, axis)\\n    else:\\n        _res_indexer = [slice(None)] * arr.ndim\\n        _res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n        res_indexer = tuple(_res_indexer)\\n        _lag_indexer = [slice(None)] * arr.ndim\\n        _lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n        lag_indexer = tuple(_lag_indexer)\\n        if is_timedelta:\\n            res = arr[res_indexer]\\n            lag = arr[lag_indexer]\\n            mask = (arr[res_indexer] == na) | (arr[lag_indexer] == na)\\n            if mask.any():\\n                res = res.copy()\\n                res[mask] = 0\\n                lag = lag.copy()\\n                lag[mask] = 0\\n            result = res - lag\\n            result[mask] = na\\n            out_arr[res_indexer] = result\\n        elif is_bool:\\n            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]\\n        else:\\n            out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n    if is_timedelta:\\n        out_arr = out_arr.astype(\"int64\").view(\"timedelta64[ns]\")\\n    return out_arr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PERF: use fastpaths for is_period_dtype checks (#33937)",
    "fixed_code": "def diff(arr, n: int, axis: int = 0, stacklevel=3):\\n    from pandas.core.arrays import PandasDtype\\n    n = int(n)\\n    na = np.nan\\n    dtype = arr.dtype\\n    if dtype.kind == \"b\":\\n        op = operator.xor\\n    else:\\n        op = operator.sub\\n    if isinstance(dtype, PandasDtype):\\n        arr = np.asarray(arr)\\n        dtype = arr.dtype\\n    if is_extension_array_dtype(dtype):\\n        if hasattr(arr, f\"__{op.__name__}__\"):\\n            return op(arr, arr.shift(n))\\n        else:\\n            warn(\\n                \"dtype lost in 'diff()'. In the future this will raise a \"\\n                \"TypeError. Convert to a suitable dtype prior to calling 'diff'.\",\\n                FutureWarning,\\n                stacklevel=stacklevel,\\n            )\\n            arr = np.asarray(arr)\\n            dtype = arr.dtype\\n    is_timedelta = False\\n    is_bool = False\\n    if needs_i8_conversion(arr.dtype):\\n        dtype = np.float64\\n        arr = arr.view(\"i8\")\\n        na = iNaT\\n        is_timedelta = True\\n    elif is_bool_dtype(dtype):\\n        dtype = np.object_\\n        is_bool = True\\n    elif is_integer_dtype(dtype):\\n        dtype = np.float64\\n    dtype = np.dtype(dtype)\\n    out_arr = np.empty(arr.shape, dtype=dtype)\\n    na_indexer = [slice(None)] * arr.ndim\\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n    out_arr[tuple(na_indexer)] = na\\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\\n        algos.diff_2d(arr, out_arr, n, axis)\\n    else:\\n        _res_indexer = [slice(None)] * arr.ndim\\n        _res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n        res_indexer = tuple(_res_indexer)\\n        _lag_indexer = [slice(None)] * arr.ndim\\n        _lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n        lag_indexer = tuple(_lag_indexer)\\n        if is_timedelta:\\n            res = arr[res_indexer]\\n            lag = arr[lag_indexer]\\n            mask = (arr[res_indexer] == na) | (arr[lag_indexer] == na)\\n            if mask.any():\\n                res = res.copy()\\n                res[mask] = 0\\n                lag = lag.copy()\\n                lag[mask] = 0\\n            result = res - lag\\n            result[mask] = na\\n            out_arr[res_indexer] = result\\n        elif is_bool:\\n            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]\\n        else:\\n            out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n    if is_timedelta:\\n        out_arr = out_arr.astype(\"int64\").view(\"timedelta64[ns]\")\\n    return out_arr"
  },
  {
    "code": "def format_int_string(\\n    text: str, allow_underscores: bool, count_from_end: bool = True\\n) -> str:\\n    if not allow_underscores:\\n        return text\\n    text = text.replace(\"_\", \"\")\\n    if len(text) <= 6:\\n        return text\\n    if count_from_end:\\n        return format(int(\"1\" + text), \"3_\")[1:].lstrip(\"_\")\\n    else:\\n        return \"_\".join(text[i : i + 3] for i in range(0, len(text), 3))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add underscores to numeric literals with more than six digits (#529)",
    "fixed_code": "def format_int_string(\\n    text: str, allow_underscores: bool, count_from_end: bool = True\\n) -> str:\\n    if not allow_underscores:\\n        return text\\n    text = text.replace(\"_\", \"\")\\n    if len(text) <= 5:\\n        return text\\n    if count_from_end:\\n        return format(int(\"1\" + text), \"3_\")[1:].lstrip(\"_\")\\n    else:\\n        return \"_\".join(text[i : i + 3] for i in range(0, len(text), 3))"
  },
  {
    "code": "def copy(self, names=None, name=None, dtype=None, deep=False):\\n        if names is not None and name is not None:\\n            raise TypeError(\"Can only provide one of `names` and `name`\")\\n        if deep:\\n            from copy import deepcopy\\n            new_index = np.ndarray.__deepcopy__(self, {}).view(self.__class__)\\n            name = name or deepcopy(self.name)\\n        else:\\n            new_index = super(Index, self).copy()\\n        if name is not None:\\n            names = [name]\\n        if names:\\n            new_index = new_index.set_names(names)\\n        if dtype:\\n            new_index = new_index.astype(dtype)\\n        return new_index",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy(self, names=None, name=None, dtype=None, deep=False):\\n        if names is not None and name is not None:\\n            raise TypeError(\"Can only provide one of `names` and `name`\")\\n        if deep:\\n            from copy import deepcopy\\n            new_index = np.ndarray.__deepcopy__(self, {}).view(self.__class__)\\n            name = name or deepcopy(self.name)\\n        else:\\n            new_index = super(Index, self).copy()\\n        if name is not None:\\n            names = [name]\\n        if names:\\n            new_index = new_index.set_names(names)\\n        if dtype:\\n            new_index = new_index.astype(dtype)\\n        return new_index"
  },
  {
    "code": "def check_scalar(\\n    x,\\n    name,\\n    target_type,\\n    *,\\n    min_val=None,\\n    max_val=None,\\n    include_boundaries=\"both\",\\n):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_scalar(\\n    x,\\n    name,\\n    target_type,\\n    *,\\n    min_val=None,\\n    max_val=None,\\n    include_boundaries=\"both\",\\n):"
  },
  {
    "code": "def get_model(self, project_name, model_name):\\n        assert model_name is not None and model_name is not ''\\n        full_model_name = 'projects/{}/models/{}'.format(\\n            project_name, model_name)\\n        request = self._cloudml.projects().models().get(name=full_model_name)\\n        try:\\n            return request.execute()\\n        except errors.HttpError as e:\\n            if e.resp.status == 404:\\n                logging.error('Model was not found: {}'.format(e))\\n                return None\\n            raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-1401] Standardize cloud ml operator arguments\\n\\nStandardize on project_id, to be consistent with\\nother cloud operators,\\nbetter-supporting default arguments.\\n\\nThis is one of multiple commits that will be\\nrequired to resolve\\nAIRFLOW-1401.\\n\\nCloses #2439 from peterjdolan/cloudml_project_id",
    "fixed_code": "def get_model(self, project_id, model_name):\\n        assert model_name is not None and model_name is not ''\\n        full_model_name = 'projects/{}/models/{}'.format(\\n            project_id, model_name)\\n        request = self._cloudml.projects().models().get(name=full_model_name)\\n        try:\\n            return request.execute()\\n        except errors.HttpError as e:\\n            if e.resp.status == 404:\\n                logging.error('Model was not found: {}'.format(e))\\n                return None\\n            raise"
  },
  {
    "code": "def _get_entries(keys):\\n        dk_nentries = int(keys['dk_nentries'])\\n        dk_size = 1<<int(keys['dk_log2_size'])\\n        try:\\n            return keys['dk_entries'], dk_size\\n        except RuntimeError:\\n            pass\\n        if dk_size <= 0xFF:\\n            offset = dk_size\\n        elif dk_size <= 0xFFFF:\\n            offset = 2 * dk_size\\n        elif dk_size <= 0xFFFFFFFF:\\n            offset = 4 * dk_size\\n        else:\\n            offset = 8 * dk_size\\n        ent_addr = keys['dk_indices'].address\\n        ent_addr = ent_addr.cast(_type_unsigned_char_ptr()) + offset\\n        ent_ptr_t = gdb.lookup_type('PyDictKeyEntry').pointer()\\n        ent_addr = ent_addr.cast(ent_ptr_t)\\n        return ent_addr, dk_nentries",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-46845: Reduce dict size when all keys are Unicode (GH-31564)",
    "fixed_code": "def _get_entries(keys):\\n        dk_nentries = int(keys['dk_nentries'])\\n        dk_size = 1<<int(keys['dk_log2_size'])\\n        if dk_size <= 0xFF:\\n            offset = dk_size\\n        elif dk_size <= 0xFFFF:\\n            offset = 2 * dk_size\\n        elif dk_size <= 0xFFFFFFFF:\\n            offset = 4 * dk_size\\n        else:\\n            offset = 8 * dk_size\\n        ent_addr = keys['dk_indices'].address\\n        ent_addr = ent_addr.cast(_type_unsigned_char_ptr()) + offset\\n        if int(keys['dk_kind']) == 0:  \\n            ent_ptr_t = gdb.lookup_type('PyDictKeyEntry').pointer()\\n        else:\\n            ent_ptr_t = gdb.lookup_type('PyDictUnicodeEntry').pointer()\\n        ent_addr = ent_addr.cast(ent_ptr_t)\\n        return ent_addr, dk_nentries"
  },
  {
    "code": "def _get_groupings(obj, grouper=None, axis=0, level=None):\\n    group_axis = obj._get_axis(axis)\\n    if level is not None and not isinstance(group_axis, MultiIndex):\\n        raise ValueError('can only specify level with multi-level index')\\n    groupings = []\\n    exclusions = []\\n    if isinstance(grouper, (tuple, list)):\\n        for i, arg in enumerate(grouper):\\n            name = 'key_%d' % i\\n            if isinstance(arg, basestring):\\n                exclusions.append(arg)\\n                name = arg\\n                arg = obj[arg]\\n            ping = Grouping(group_axis, arg, name=name, level=level)\\n            groupings.append(ping)\\n    else:\\n        name = 'key'\\n        if isinstance(grouper, basestring):\\n            exclusions.append(grouper)\\n            name = grouper\\n            grouper = obj[grouper]\\n        ping = Grouping(group_axis, grouper, name=name, level=level)\\n        groupings.append(ping)\\n    return groupings, exclusions",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: preserve index name when grouping by level",
    "fixed_code": "def _get_groupings(obj, grouper=None, axis=0, level=None):\\n    group_axis = obj._get_axis(axis)\\n    if level is not None and not isinstance(group_axis, MultiIndex):\\n        raise ValueError('can only specify level with multi-level index')\\n    groupings = []\\n    exclusions = []\\n    if isinstance(grouper, (tuple, list)):\\n        for i, arg in enumerate(grouper):\\n            name = 'key_%d' % i\\n            if isinstance(arg, basestring):\\n                exclusions.append(arg)\\n                name = arg\\n                arg = obj[arg]\\n            ping = Grouping(group_axis, arg, name=name, level=level)\\n            groupings.append(ping)\\n    else:\\n        name = None\\n        if isinstance(grouper, basestring):\\n            exclusions.append(grouper)\\n            name = grouper\\n            grouper = obj[grouper]\\n        ping = Grouping(group_axis, grouper, name=name, level=level)\\n        groupings.append(ping)\\n    return groupings, exclusions"
  },
  {
    "code": "def spawn_subprocess(self):\\n\\t\\tif self.subprocess_arglist is None:\\n\\t\\t\\tself.subprocess_arglist = self.build_subprocess_arglist()\\n\\t\\targs = self.subprocess_arglist\\n\\t\\tself.rpcpid = os.spawnv(os.P_NOWAIT, sys.executable, args)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def spawn_subprocess(self):\\n\\t\\tif self.subprocess_arglist is None:\\n\\t\\t\\tself.subprocess_arglist = self.build_subprocess_arglist()\\n\\t\\targs = self.subprocess_arglist\\n\\t\\tself.rpcpid = os.spawnv(os.P_NOWAIT, sys.executable, args)"
  },
  {
    "code": "def undefine(self, entryid):\\n        if not self.module.check_mode:\\n            return self.find_entry(entryid).undefine()\\n        else:\\n            if not self.find_entry(entryid):\\n                return self.module.exit_json(changed=True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "virt_net: idempotency of create/stop actions (#53276)\\n\\nCurrently, if we try to stop or start a network two time in a row, the\\nsecond call will fail. With this patch:\\n\\n- we don't recreate a network, if it exists\\n- we only stop a network if it's active, and so we avoid an exception\\n  saying the network is not active",
    "fixed_code": "def undefine(self, entryid):\\n        entry = None\\n        try:\\n            entry = self.find_entry(entryid)\\n            found = True\\n        except EntryNotFound:\\n            found = False\\n        if found:\\n            return self.find_entry(entryid).undefine()\\n        if self.module.check_mode:\\n            return self.module.exit_json(changed=found)"
  },
  {
    "code": "def apply(self):\\n        changed = False\\n        net_port_exists = False\\n        results = netapp_utils.get_cserver(self.server)\\n        cserver = netapp_utils.setup_na_ontap_zapi(\\n            module=self.module, vserver=results)\\n        netapp_utils.ems_log_event(\"na_ontap_net_port\", cserver)\\n        net_port_details = self.get_net_port()\\n        if net_port_details:\\n            net_port_exists = True\\n            if self.state == 'present':\\n                if (self.mtu and self.mtu != net_port_details['mtu']) or \\\\n                   (self.autonegotiate_admin and\\n                    self.autonegotiate_admin != net_port_details['autonegotiate_admin']) or \\\\n                   (self.duplex_admin and\\n                    self.duplex_admin != net_port_details['duplex_admin']) or \\\\n                   (self.speed_admin and\\n                    self.speed_admin != net_port_details['speed_admin']) or \\\\n                   (self.flowcontrol_admin and\\n                        self.flowcontrol_admin != net_port_details['flowcontrol_admin']):\\n                    changed = True\\n        if changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if self.state == 'present':\\n                    if net_port_exists:\\n                        self.modify_net_port()\\n        self.module.exit_json(changed=changed)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Modify na_ontap_net_port NetAppModule module format, allow for multiple ports, unit tests (#52286)",
    "fixed_code": "def apply(self):\\n        self.autosupport_log()\\n        for port in self.parameters['ports']:\\n            current = self.get_net_port(port)\\n            modify = self.na_helper.get_modified_attributes(current, self.parameters)\\n            if self.na_helper.changed:\\n                if self.module.check_mode:\\n                    pass\\n                else:\\n                    if modify:\\n                        self.modify_net_port(port, modify)\\n        self.module.exit_json(changed=self.na_helper.changed)"
  },
  {
    "code": "def __contains__(self, key):\\n        if is_scalar(key) and isna(key):\\n            return self.hasnans\\n        return contains(self, key, container=self._engine)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __contains__(self, key):\\n        if is_scalar(key) and isna(key):\\n            return self.hasnans\\n        return contains(self, key, container=self._engine)"
  },
  {
    "code": "def visit(self, node):\\n        if not (isinstance(node, ast.AST) or isinstance(node, basestring)):\\n            raise TypeError('\"node\" must be an AST node or a string, you'\\n                            ' passed a(n) {0}'.format(node.__class__))\\n        if isinstance(node, basestring):\\n            node = ast.fix_missing_locations(ast.parse(preparse(node)))\\n        return super(ExprVisitor, self).visit(node)\\n    def visit_Module(self, node):\\n        if len(node.body) != 1:\\n            raise ExprParserError('only a single expression is allowed')\\n        expr = node.body[0]\\n        if not isinstance(expr, (ast.Expr, ast.Assign)):\\n            raise SyntaxError('only expressions are allowed')\\n        return self.visit(expr)\\n    def visit_Expr(self, node):\\n        return self.visit(node.value)\\n    def visit_BinOp(self, node):\\n        op = self.visit(node.op)\\n        left = self.visit(node.left)\\n        right = self.visit(node.right)\\n        return op(left, right)\\n    def visit_UnaryOp(self, node):\\n        if isinstance(node.op, ast.Not):\\n            raise NotImplementedError(\"not operator not yet supported\")\\n        op = self.visit(node.op)\\n        return op(self.visit(node.operand))\\n    def visit_Name(self, node):\\n        return Term(node.id, self.env)\\n    def visit_Num(self, node):\\n        return Constant(node.n, self.env)\\n    def visit_Compare(self, node):\\n        ops = node.ops\\n        comps = node.comparators\\n        if len(ops) != 1:\\n            raise ExprParserError('chained comparisons not supported')\\n        return self.visit(ops[0])(self.visit(node.left), self.visit(comps[0]))\\n    def visit_Assign(self, node):\\n        cmpr = ast.copy_location(ast.Compare(ops=[ast.Eq()],\\n                                             left=node.targets[0],\\n                                             comparators=[node.value]), node)\\n        return self.visit(cmpr)\\n    def visit_Call(self, node):\\n        if not isinstance(node.func, ast.Name):\\n            raise TypeError(\"Only named functions are supported\")\\n        valid_ops = _reductions + _mathops\\n        if node.func.id not in valid_ops:\\n            raise ValueError(\"Only {0} are supported\".format(valid_ops))\\n        raise NotImplementedError(\"function calls not yet supported\")\\n    def visit_Attribute(self, node):\\n        raise NotImplementedError(\"attribute access is not yet supported\")\\n    def visit_BoolOp(self, node):\\n        raise NotImplementedError(\"boolean operators are not yet supported\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def visit(self, node):\\n        if not (isinstance(node, ast.AST) or isinstance(node, basestring)):\\n            raise TypeError('\"node\" must be an AST node or a string, you'\\n                            ' passed a(n) {0}'.format(node.__class__))\\n        if isinstance(node, basestring):\\n            node = ast.fix_missing_locations(ast.parse(preparse(node)))\\n        return super(ExprVisitor, self).visit(node)\\n    def visit_Module(self, node):\\n        if len(node.body) != 1:\\n            raise ExprParserError('only a single expression is allowed')\\n        expr = node.body[0]\\n        if not isinstance(expr, (ast.Expr, ast.Assign)):\\n            raise SyntaxError('only expressions are allowed')\\n        return self.visit(expr)\\n    def visit_Expr(self, node):\\n        return self.visit(node.value)\\n    def visit_BinOp(self, node):\\n        op = self.visit(node.op)\\n        left = self.visit(node.left)\\n        right = self.visit(node.right)\\n        return op(left, right)\\n    def visit_UnaryOp(self, node):\\n        if isinstance(node.op, ast.Not):\\n            raise NotImplementedError(\"not operator not yet supported\")\\n        op = self.visit(node.op)\\n        return op(self.visit(node.operand))\\n    def visit_Name(self, node):\\n        return Term(node.id, self.env)\\n    def visit_Num(self, node):\\n        return Constant(node.n, self.env)\\n    def visit_Compare(self, node):\\n        ops = node.ops\\n        comps = node.comparators\\n        if len(ops) != 1:\\n            raise ExprParserError('chained comparisons not supported')\\n        return self.visit(ops[0])(self.visit(node.left), self.visit(comps[0]))\\n    def visit_Assign(self, node):\\n        cmpr = ast.copy_location(ast.Compare(ops=[ast.Eq()],\\n                                             left=node.targets[0],\\n                                             comparators=[node.value]), node)\\n        return self.visit(cmpr)\\n    def visit_Call(self, node):\\n        if not isinstance(node.func, ast.Name):\\n            raise TypeError(\"Only named functions are supported\")\\n        valid_ops = _reductions + _mathops\\n        if node.func.id not in valid_ops:\\n            raise ValueError(\"Only {0} are supported\".format(valid_ops))\\n        raise NotImplementedError(\"function calls not yet supported\")\\n    def visit_Attribute(self, node):\\n        raise NotImplementedError(\"attribute access is not yet supported\")\\n    def visit_BoolOp(self, node):\\n        raise NotImplementedError(\"boolean operators are not yet supported\")"
  },
  {
    "code": "def __init__(self, sql, using, params=None):\\n        self.validate_sql(sql)\\n        self.params = params or ()\\n        self.sql = sql\\n        self.using = using\\n        self.cursor = None\\n        self.low_mark, self.high_mark = 0, None  \\n        self.extra_select = {}\\n        self.aggregate_select = {}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #14733: no longer \"validate\" .raw() queries.\\n\\nTurns out that a lot more than just SELECT can return data, and this list is\\nvery hard to define up front in a cross-database manner. So let's just assume\\nthat anyone using raw() is at least halfway competant and can deal with\\nthe error messages if they don't use a data-returning query.\\n\\nThanks to Christophe Pettus for the patch.",
    "fixed_code": "def __init__(self, sql, using, params=None):\\n        self.params = params or ()\\n        self.sql = sql\\n        self.using = using\\n        self.cursor = None\\n        self.low_mark, self.high_mark = 0, None  \\n        self.extra_select = {}\\n        self.aggregate_select = {}"
  },
  {
    "code": "def seek(self, pos: int, whence: int = 0) -> int:\\n        self._unsupported(\"seek\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def seek(self, pos: int, whence: int = 0) -> int:\\n        self._unsupported(\"seek\")"
  },
  {
    "code": "def _get_base_knot_positions(X, n_knots=10, knots=\"uniform\", sample_weight=None):\\n        if knots == \"quantile\":\\n            percentiles = 100 * np.linspace(\\n                start=0, stop=1, num=n_knots, dtype=np.float64\\n            )\\n            if sample_weight is None:\\n                knots = np.percentile(X, percentiles, axis=0)\\n            else:\\n                knots = np.array(\\n                    [\\n                        _weighted_percentile(X, sample_weight, percentile)\\n                        for percentile in percentiles\\n                    ]\\n                )\\n        else:\\n            mask = slice(None, None, 1) if sample_weight is None else sample_weight > 0\\n            x_min = np.amin(X[mask], axis=0)\\n            x_max = np.amax(X[mask], axis=0)\\n            knots = linspace(\\n                start=x_min,\\n                stop=x_max,\\n                num=n_knots,\\n                endpoint=True,\\n                dtype=np.float64,\\n            )\\n        return knots",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_base_knot_positions(X, n_knots=10, knots=\"uniform\", sample_weight=None):\\n        if knots == \"quantile\":\\n            percentiles = 100 * np.linspace(\\n                start=0, stop=1, num=n_knots, dtype=np.float64\\n            )\\n            if sample_weight is None:\\n                knots = np.percentile(X, percentiles, axis=0)\\n            else:\\n                knots = np.array(\\n                    [\\n                        _weighted_percentile(X, sample_weight, percentile)\\n                        for percentile in percentiles\\n                    ]\\n                )\\n        else:\\n            mask = slice(None, None, 1) if sample_weight is None else sample_weight > 0\\n            x_min = np.amin(X[mask], axis=0)\\n            x_max = np.amax(X[mask], axis=0)\\n            knots = linspace(\\n                start=x_min,\\n                stop=x_max,\\n                num=n_knots,\\n                endpoint=True,\\n                dtype=np.float64,\\n            )\\n        return knots"
  },
  {
    "code": "def gen_python_files_in_dir(\\n\\tpath: Path,\\n\\troot: Path,\\n\\tinclude: Pattern[str],\\n\\texclude: Pattern[str],\\n\\treport: \"Report\",\\n) -> Iterator[Path]:\\n\\tassert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\\n\\tfor child in path.iterdir():\\n\\t\\ttry:\\n\\t\\t\\tnormalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\\n\\t\\texcept ValueError:\\n\\t\\t\\tif child.is_symlink():\\n\\t\\t\\t\\treport.path_ignored(\\n\\t\\t\\t\\t\\tchild,\\n\\t\\t\\t\\t\\t\"is a symbolic link that points outside of the root directory\",\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\traise\\n\\t\\tif child.is_dir():\\n\\t\\t\\tnormalized_path += \"/\"\\n\\t\\texclude_match = exclude.search(normalized_path)\\n\\t\\tif exclude_match and exclude_match.group(0):\\n\\t\\t\\treport.path_ignored(child, f\"matches the --exclude regular expression\")\\n\\t\\t\\tcontinue\\n\\t\\tif child.is_dir():\\n\\t\\t\\tyield from gen_python_files_in_dir(child, root, include, exclude, report)\\n\\t\\telif child.is_file():\\n\\t\\t\\tinclude_match = include.search(normalized_path)\\n\\t\\t\\tif include_match:\\n\\t\\t\\t\\tyield child",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def gen_python_files_in_dir(\\n\\tpath: Path,\\n\\troot: Path,\\n\\tinclude: Pattern[str],\\n\\texclude: Pattern[str],\\n\\treport: \"Report\",\\n) -> Iterator[Path]:\\n\\tassert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\\n\\tfor child in path.iterdir():\\n\\t\\ttry:\\n\\t\\t\\tnormalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\\n\\t\\texcept ValueError:\\n\\t\\t\\tif child.is_symlink():\\n\\t\\t\\t\\treport.path_ignored(\\n\\t\\t\\t\\t\\tchild,\\n\\t\\t\\t\\t\\t\"is a symbolic link that points outside of the root directory\",\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\traise\\n\\t\\tif child.is_dir():\\n\\t\\t\\tnormalized_path += \"/\"\\n\\t\\texclude_match = exclude.search(normalized_path)\\n\\t\\tif exclude_match and exclude_match.group(0):\\n\\t\\t\\treport.path_ignored(child, f\"matches the --exclude regular expression\")\\n\\t\\t\\tcontinue\\n\\t\\tif child.is_dir():\\n\\t\\t\\tyield from gen_python_files_in_dir(child, root, include, exclude, report)\\n\\t\\telif child.is_file():\\n\\t\\t\\tinclude_match = include.search(normalized_path)\\n\\t\\t\\tif include_match:\\n\\t\\t\\t\\tyield child"
  },
  {
    "code": "def main():\\n  le_handler = get_le_handler()\\n  le_level = 20  \\n  ctx = zmq.Context().instance()\\n  sock = ctx.socket(zmq.PULL)\\n  sock.bind(\"ipc:///tmp/logmessage\")\\n  pub_sock = messaging.pub_sock('logMessage')\\n  while True:\\n    dat = b''.join(sock.recv_multipart())\\n    dat = dat.decode('utf8')\\n    levelnum = ord(dat[0])\\n    dat = dat[1:]\\n    if levelnum >= le_level:\\n      le_handler.emit_raw(dat)\\n    msg = messaging.new_message()\\n    msg.logMessage = dat\\n    pub_sock.send(msg.to_bytes())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "loggerd tests (#19671)",
    "fixed_code": "def main():\\n  le_handler = get_le_handler()\\n  le_level = 20  \\n  ctx = zmq.Context().instance()\\n  sock = ctx.socket(zmq.PULL)\\n  sock.bind(\"ipc:///tmp/logmessage\")\\n  pub_sock = messaging.pub_sock('logMessage')\\n  while True:\\n    dat = b''.join(sock.recv_multipart())\\n    dat = dat.decode('utf8')\\n    levelnum = ord(dat[0])\\n    dat = dat[1:]\\n    if levelnum >= le_level:\\n      le_handler.emit_raw(dat)\\n    msg = messaging.new_message()\\n    msg.logMessage = dat\\n    pub_sock.send(msg.to_bytes())"
  },
  {
    "code": "def round(self, decimals=0, *args):\\n        from pandas.tools.merge import concat",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def round(self, decimals=0, *args):\\n        from pandas.tools.merge import concat"
  },
  {
    "code": "def _is_indexed_like(obj, other):\\n    if isinstance(obj, Series):\\n        if not isinstance(other, Series):\\n            return False\\n        return obj.index.equals(other.index)\\n    elif isinstance(obj, DataFrame):\\n        if isinstance(other, Series):\\n            return obj.index.equals(other.index)\\n        assert(isinstance(other, DataFrame))\\n        return obj._indexed_same(other)\\n    return False\\nclass Grouping(object):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _is_indexed_like(obj, other):\\n    if isinstance(obj, Series):\\n        if not isinstance(other, Series):\\n            return False\\n        return obj.index.equals(other.index)\\n    elif isinstance(obj, DataFrame):\\n        if isinstance(other, Series):\\n            return obj.index.equals(other.index)\\n        assert(isinstance(other, DataFrame))\\n        return obj._indexed_same(other)\\n    return False\\nclass Grouping(object):"
  },
  {
    "code": "def get_conn(self):\\n        http_authorized = self._authorize()\\n        return build('dataproc', 'v1', http=http_authorized)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2016] Add support for Dataproc Workflow Templates\\n\\nCloses #2958 from DanSedov/master",
    "fixed_code": "def get_conn(self):\\n        http_authorized = self._authorize()\\n        return build('dataproc', self.api_version, http=http_authorized)"
  },
  {
    "code": "def nodata_value(self):\\n        nodata_exists = c_int()\\n        value = capi.get_band_nodata_value(self._ptr, nodata_exists)\\n        if not nodata_exists:\\n            value = None\\n        elif self.datatype() in GDAL_INTEGER_TYPES:\\n            value = int(value)\\n        return value\\n    @nodata_value.setter\\n    def nodata_value(self, value):\\n        if not isinstance(value, (int, float)):\\n            raise ValueError('Nodata value must be numeric.')\\n        capi.set_band_nodata_value(self._ptr, value)\\n        self.source._flush()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25734 -- Made GDALBand min and max properties use GDALComputeRasterStatistics.\\n\\nThanks Sergey Fedoseev and Tim Graham for the review.",
    "fixed_code": "def nodata_value(self):\\n        nodata_exists = c_int()\\n        value = capi.get_band_nodata_value(self._ptr, nodata_exists)\\n        if not nodata_exists:\\n            value = None\\n        elif self.datatype() in GDAL_INTEGER_TYPES:\\n            value = int(value)\\n        return value\\n    @nodata_value.setter\\n    def nodata_value(self, value):\\n        if not isinstance(value, (int, float)):\\n            raise ValueError('Nodata value must be numeric.')\\n        capi.set_band_nodata_value(self._ptr, value)\\n        self._flush()"
  },
  {
    "code": "def delete_vserver(self):\\n        vserver_delete = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'vserver-destroy', **{'vserver-name': self.parameters['name']})\\n        try:\\n            self.server.invoke_successfully(vserver_delete,\\n                                            enable_tunneling=False)\\n        except netapp_utils.zapi.NaApiError as e:\\n            self.module.fail_json(msg='Error deleting SVM %s: %s'\\n                                      % (self.parameters['name'], to_native(e)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def delete_vserver(self):\\n        vserver_delete = netapp_utils.zapi.NaElement.create_node_with_children(\\n            'vserver-destroy', **{'vserver-name': self.parameters['name']})\\n        try:\\n            self.server.invoke_successfully(vserver_delete,\\n                                            enable_tunneling=False)\\n        except netapp_utils.zapi.NaApiError as e:\\n            self.module.fail_json(msg='Error deleting SVM %s: %s'\\n                                      % (self.parameters['name'], to_native(e)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def __init__(self, estimator, threshold=None, prefit=False,\\n                 max_features=None):\\n        self.estimator = estimator\\n        self.threshold = threshold\\n        self.prefit = prefit\\n        self.max_features = max_features",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX Revert unintentionally committed code",
    "fixed_code": "def __init__(self, estimator, threshold=None, prefit=False):\\n        self.estimator = estimator\\n        self.threshold = threshold\\n        self.prefit = prefit"
  },
  {
    "code": "def parse_cookie(cookie):\\n\\tif cookie == '':\\n\\t\\treturn {}\\n\\tif not isinstance(cookie, http_cookies.BaseCookie):\\n\\t\\ttry:\\n\\t\\t\\tc = SimpleCookie()\\n\\t\\t\\tc.load(cookie)\\n\\t\\texcept http_cookies.CookieError:\\n\\t\\t\\treturn {}\\n\\telse:\\n\\t\\tc = cookie\\n\\tcookiedict = {}\\n\\tfor key in c.keys():\\n\\t\\tcookiedict[key] = c.get(key).value\\n\\treturn cookiedict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[1.9.x] Fixed CVE-2016-7401 -- Fixed CSRF protection bypass on a site with Google Analytics.\\n\\nThis is a security fix.\\n\\nBackport of \"refs #26158 -- rewrote http.parse_cookie() to better match\\nbrowsers.\" 93a135d111c2569d88d65a3f4ad9e6d9ad291452 from master",
    "fixed_code": "def parse_cookie(cookie):\\n\\tcookiedict = {}\\n\\tif six.PY2:\\n\\t\\tcookie = force_str(cookie)\\n\\tfor chunk in cookie.split(str(';')):\\n\\t\\tif str('=') in chunk:\\n\\t\\t\\tkey, val = chunk.split(str('='), 1)\\n\\t\\telse:\\n\\t\\t\\tkey, val = str(''), chunk\\n\\t\\tkey, val = key.strip(), val.strip()\\n\\t\\tif key or val:\\n\\t\\t\\tcookiedict[key] = http_cookies._unquote(val)\\n\\treturn cookiedict"
  },
  {
    "code": "def missing_required_lib(library, reason=None, url=None):\\n    hostname = platform.node()\\n    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\\n    if reason:\\n        msg += \" This is required %s.\" % reason\\n    if url:\\n        msg += \" See %s for more info.\" % url\\n    return msg + \" Please read module documentation and install in the appropriate location\"",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "missing_required_lib: add information how to change Python interpreter (#60736)",
    "fixed_code": "def missing_required_lib(library, reason=None, url=None):\\n    hostname = platform.node()\\n    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\\n    if reason:\\n        msg += \" This is required %s.\" % reason\\n    if url:\\n        msg += \" See %s for more info.\" % url\\n    msg += (\" Please read module documentation and install in the appropriate location.\"\\n            \" If the required library is installed, but Ansible is using the wrong Python interpreter,\"\\n            \" please consult the documentation on ansible_python_interpreter\")\\n    return msg"
  },
  {
    "code": "def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiIndex:\\n\\tif (levels is None and isinstance(keys[0], tuple)) or (\\n\\t\\tlevels is not None and len(levels) > 1\\n\\t):\\n\\t\\tzipped = list(zip(*keys))\\n\\t\\tif names is None:\\n\\t\\t\\tnames = [None] * len(zipped)\\n\\t\\tif levels is None:\\n\\t\\t\\t_, levels = factorize_from_iterables(zipped)\\n\\t\\telse:\\n\\t\\t\\tlevels = [ensure_index(x) for x in levels]\\n\\telse:\\n\\t\\tzipped = [keys]\\n\\t\\tif names is None:\\n\\t\\t\\tnames = [None]\\n\\t\\tif levels is None:\\n\\t\\t\\tlevels = [ensure_index(keys)]\\n\\t\\telse:\\n\\t\\t\\tlevels = [ensure_index(x) for x in levels]\\n\\tif not all_indexes_same(indexes):\\n\\t\\tcodes_list = []\\n\\t\\tfor hlevel, level in zip(zipped, levels):\\n\\t\\t\\tto_concat = []\\n\\t\\t\\tfor key, index in zip(hlevel, indexes):\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ti = level.get_loc(key)\\n\\t\\t\\t\\texcept KeyError as err:\\n\\t\\t\\t\\t\\traise ValueError(f\"Key {key} not in level {level}\") from err\\n\\t\\t\\t\\tto_concat.append(np.repeat(i, len(index)))\\n\\t\\t\\tcodes_list.append(np.concatenate(to_concat))\\n\\t\\tconcat_index = _concat_indexes(indexes)\\n\\t\\tif isinstance(concat_index, MultiIndex):\\n\\t\\t\\tlevels.extend(concat_index.levels)\\n\\t\\t\\tcodes_list.extend(concat_index.codes)\\n\\t\\telse:\\n\\t\\t\\tcodes, categories = factorize_from_iterable(concat_index)\\n\\t\\t\\tlevels.append(categories)\\n\\t\\t\\tcodes_list.append(codes)\\n\\t\\tif len(names) == len(levels):\\n\\t\\t\\tnames = list(names)\\n\\t\\telse:\\n\\t\\t\\tif not len({idx.nlevels for idx in indexes}) == 1:\\n\\t\\t\\t\\traise AssertionError(\\n\\t\\t\\t\\t\\t\"Cannot concat indices that do not have the same number of levels\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tnames = names + get_consensus_names(indexes)\\n\\t\\treturn MultiIndex(\\n\\t\\t\\tlevels=levels, codes=codes_list, names=names, verify_integrity=False\\n\\t\\t)\\n\\tnew_index = indexes[0]\\n\\tn = len(new_index)\\n\\tkpieces = len(indexes)\\n\\tnew_names = list(names)\\n\\tnew_levels = list(levels)\\n\\tnew_codes = []\\n\\tfor hlevel, level in zip(zipped, levels):\\n\\t\\thlevel = ensure_index(hlevel)\\n\\t\\tmapped = level.get_indexer(hlevel)\\n\\t\\tmask = mapped == -1\\n\\t\\tif mask.any():\\n\\t\\t\\traise ValueError(f\"Values not found in passed level: {hlevel[mask]!s}\")\\n\\t\\tnew_codes.append(np.repeat(mapped, n))\\n\\tif isinstance(new_index, MultiIndex):\\n\\t\\tnew_levels.extend(new_index.levels)\\n\\t\\tnew_codes.extend([np.tile(lab, kpieces) for lab in new_index.codes])\\n\\telse:\\n\\t\\tnew_levels.append(new_index)\\n\\t\\tnew_codes.append(np.tile(np.arange(n), kpieces))\\n\\tif len(new_names) < len(new_levels):\\n\\t\\tnew_names.extend(new_index.names)\\n\\treturn MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: can't concatenate DataFrame with Series with duplicate keys (#33805)",
    "fixed_code": "def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiIndex:\\n\\tif (levels is None and isinstance(keys[0], tuple)) or (\\n\\t\\tlevels is not None and len(levels) > 1\\n\\t):\\n\\t\\tzipped = list(zip(*keys))\\n\\t\\tif names is None:\\n\\t\\t\\tnames = [None] * len(zipped)\\n\\t\\tif levels is None:\\n\\t\\t\\t_, levels = factorize_from_iterables(zipped)\\n\\t\\telse:\\n\\t\\t\\tlevels = [ensure_index(x) for x in levels]\\n\\telse:\\n\\t\\tzipped = [keys]\\n\\t\\tif names is None:\\n\\t\\t\\tnames = [None]\\n\\t\\tif levels is None:\\n\\t\\t\\tlevels = [ensure_index(keys)]\\n\\t\\telse:\\n\\t\\t\\tlevels = [ensure_index(x) for x in levels]\\n\\tif not all_indexes_same(indexes):\\n\\t\\tcodes_list = []\\n\\t\\tfor hlevel, level in zip(zipped, levels):\\n\\t\\t\\tto_concat = []\\n\\t\\t\\tfor key, index in zip(hlevel, indexes):\\n\\t\\t\\t\\tmask = level == key\\n\\t\\t\\t\\tif not mask.any():\\n\\t\\t\\t\\t\\traise ValueError(f\"Key {key} not in level {level}\")\\n\\t\\t\\t\\ti = np.nonzero(level == key)[0][0]\\n\\t\\t\\t\\tto_concat.append(np.repeat(i, len(index)))\\n\\t\\t\\tcodes_list.append(np.concatenate(to_concat))\\n\\t\\tconcat_index = _concat_indexes(indexes)\\n\\t\\tif isinstance(concat_index, MultiIndex):\\n\\t\\t\\tlevels.extend(concat_index.levels)\\n\\t\\t\\tcodes_list.extend(concat_index.codes)\\n\\t\\telse:\\n\\t\\t\\tcodes, categories = factorize_from_iterable(concat_index)\\n\\t\\t\\tlevels.append(categories)\\n\\t\\t\\tcodes_list.append(codes)\\n\\t\\tif len(names) == len(levels):\\n\\t\\t\\tnames = list(names)\\n\\t\\telse:\\n\\t\\t\\tif not len({idx.nlevels for idx in indexes}) == 1:\\n\\t\\t\\t\\traise AssertionError(\\n\\t\\t\\t\\t\\t\"Cannot concat indices that do not have the same number of levels\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tnames = names + get_consensus_names(indexes)\\n\\t\\treturn MultiIndex(\\n\\t\\t\\tlevels=levels, codes=codes_list, names=names, verify_integrity=False\\n\\t\\t)\\n\\tnew_index = indexes[0]\\n\\tn = len(new_index)\\n\\tkpieces = len(indexes)\\n\\tnew_names = list(names)\\n\\tnew_levels = list(levels)\\n\\tnew_codes = []\\n\\tfor hlevel, level in zip(zipped, levels):\\n\\t\\thlevel = ensure_index(hlevel)\\n\\t\\tmapped = level.get_indexer(hlevel)\\n\\t\\tmask = mapped == -1\\n\\t\\tif mask.any():\\n\\t\\t\\traise ValueError(f\"Values not found in passed level: {hlevel[mask]!s}\")\\n\\t\\tnew_codes.append(np.repeat(mapped, n))\\n\\tif isinstance(new_index, MultiIndex):\\n\\t\\tnew_levels.extend(new_index.levels)\\n\\t\\tnew_codes.extend([np.tile(lab, kpieces) for lab in new_index.codes])\\n\\telse:\\n\\t\\tnew_levels.append(new_index)\\n\\t\\tnew_codes.append(np.tile(np.arange(n), kpieces))\\n\\tif len(new_names) < len(new_levels):\\n\\t\\tnew_names.extend(new_index.names)\\n\\treturn MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)"
  },
  {
    "code": "def __getitem__(self, indx):\\n\\t\\tdout = self.data[indx]\\n\\t\\t_mask = self._mask\\n\\t\\tif not getattr(dout, 'ndim', False):\\n\\t\\t\\tif isinstance(dout, np.void):\\n\\t\\t\\t\\tmask = _mask[indx]\\n\\t\\t\\t\\tdout = mvoid(dout, mask=mask, hardmask=self._hardmask)\\n\\t\\t\\telif _mask is not nomask and _mask[indx]:\\n\\t\\t\\t\\treturn masked\\n\\t\\telif self.dtype.type is np.object_ and self.dtype is not dout.dtype:\\n\\t\\t\\tif _mask is not nomask and _mask[indx]:\\n\\t\\t\\t\\treturn MaskedArray(dout, mask=True)\\n\\t\\telse:\\n\\t\\t\\tdout = dout.view(type(self))\\n\\t\\t\\tdout._update_from(self)\\n\\t\\t\\tif isinstance(indx, basestring):\\n\\t\\t\\t\\tif self._fill_value is not None:\\n\\t\\t\\t\\t\\tdout._fill_value = self._fill_value[indx]\\n\\t\\t\\t\\tdout._isfield = True\\n\\t\\t\\tif _mask is not nomask:\\n\\t\\t\\t\\tdout._mask = _mask[indx]\\n\\t\\t\\t\\tdout._mask.shape = dout.shape\\n\\t\\t\\t\\tdout._sharedmask = True\\n\\t\\treturn dout",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/TST: Fix for #6723 including test: force fill_value.ndim==0\\n\\nFix issue #6723.  Given an exotic masked structured array, where one of\\nthe fields has a multidimensional dtype, make sure that, when accessing\\nthis field, the fill_value still makes sense.  As it stands prior to this\\ncommit, the fill_value will end up being multidimensional, possibly with\\na shape incompatible with the mother array, which leads to broadcasting\\nerrors in methods such as .filled().  This commit uses the first element\\nof this multidimensional fill value as the new fill value.  When more\\nthan one unique value existed in fill_value, a warning is issued.\\n\\nAlso add a test to verify that fill_value.ndim remains 0 after indexing.",
    "fixed_code": "def __getitem__(self, indx):\\n\\t\\tdout = self.data[indx]\\n\\t\\t_mask = self._mask\\n\\t\\tif not getattr(dout, 'ndim', False):\\n\\t\\t\\tif isinstance(dout, np.void):\\n\\t\\t\\t\\tmask = _mask[indx]\\n\\t\\t\\t\\tdout = mvoid(dout, mask=mask, hardmask=self._hardmask)\\n\\t\\t\\telif _mask is not nomask and _mask[indx]:\\n\\t\\t\\t\\treturn masked\\n\\t\\telif self.dtype.type is np.object_ and self.dtype is not dout.dtype:\\n\\t\\t\\tif _mask is not nomask and _mask[indx]:\\n\\t\\t\\t\\treturn MaskedArray(dout, mask=True)\\n\\t\\telse:\\n\\t\\t\\tdout = dout.view(type(self))\\n\\t\\t\\tdout._update_from(self)\\n\\t\\t\\tif isinstance(indx, basestring):\\n\\t\\t\\t\\tif self._fill_value is not None:\\n\\t\\t\\t\\t\\tdout._fill_value = self._fill_value[indx]\\n\\t\\t\\t\\t\\tif dout._fill_value.ndim > 0:\\n\\t\\t\\t\\t\\t\\tif not (dout._fill_value ==\\n\\t\\t\\t\\t\\t\\t\\t\\tdout._fill_value.flat[0]).all():\\n\\t\\t\\t\\t\\t\\t\\twarnings.warn(\\n\\t\\t\\t\\t\\t\\t\\t\\t\"Upon accessing multidimensional field \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"{indx:s}, need to keep dimensionality \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"of fill_value at 0. Discarding \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"heterogeneous fill_value and setting \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"all to {fv!s}.\".format(indx=indx,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tfv=dout._fill_value[0]))\\n\\t\\t\\t\\t\\t\\tdout._fill_value = dout._fill_value.flat[0]\\n\\t\\t\\t\\tdout._isfield = True\\n\\t\\t\\tif _mask is not nomask:\\n\\t\\t\\t\\tdout._mask = _mask[indx]\\n\\t\\t\\t\\tdout._mask.shape = dout.shape\\n\\t\\t\\t\\tdout._sharedmask = True\\n\\t\\treturn dout"
  },
  {
    "code": "def _refold_parse_tree(parse_tree, *, policy):\\n    maxlen = policy.max_line_length or float(\"+inf\")\\n    encoding = 'utf-8' if policy.utf8 else 'us-ascii'\\n    lines = ['']\\n    last_ew = None\\n    wrap_as_ew_blocked = 0\\n    want_encoding = False\\n    end_ew_not_allowed = Terminal('', 'wrap_as_ew_blocked')\\n    parts = list(parse_tree)\\n    while parts:\\n        part = parts.pop(0)\\n        if part is end_ew_not_allowed:\\n            wrap_as_ew_blocked -= 1\\n            continue\\n        tstr = str(part)\\n        try:\\n            tstr.encode(encoding)\\n            charset = encoding\\n        except UnicodeEncodeError:\\n            if any(isinstance(x, errors.UndecodableBytesDefect)\\n                   for x in part.all_defects):\\n                charset = 'unknown-8bit'\\n            else:\\n                charset = 'utf-8'\\n            want_encoding = True\\n        if part.token_type == 'mime-parameters':\\n            _fold_mime_parameters(part, lines, maxlen, encoding)\\n            continue\\n        if want_encoding and not wrap_as_ew_blocked:\\n            if not part.as_ew_allowed:\\n                want_encoding = False\\n                last_ew = None\\n                if part.syntactic_break:\\n                    encoded_part = part.fold(policy=policy)[:-len(policy.linesep)]\\n                    if policy.linesep not in encoded_part:\\n                        if len(encoded_part) > maxlen - len(lines[-1]):\\n                            newline = _steal_trailing_WSP_if_exists(lines)\\n                            lines.append(newline)\\n                        lines[-1] += encoded_part\\n                        continue\\n            if not hasattr(part, 'encode'):\\n                parts = list(part) + parts\\n            else:\\n                last_ew = _fold_as_ew(tstr, lines, maxlen, last_ew,\\n                                      part.ew_combine_allowed, charset)\\n            want_encoding = False\\n            continue\\n        if len(tstr) <= maxlen - len(lines[-1]):\\n            lines[-1] += tstr\\n            continue\\n        if (part.syntactic_break and\\n                len(tstr) + 1 <= maxlen):\\n            newline = _steal_trailing_WSP_if_exists(lines)\\n            if newline or part.startswith_fws():\\n                lines.append(newline + tstr)\\n                continue\\n        if not hasattr(part, 'encode'):\\n            newparts = list(part)\\n            if not part.as_ew_allowed:\\n                wrap_as_ew_blocked += 1\\n                newparts.append(end_ew_not_allowed)\\n            parts = newparts + parts\\n            continue\\n        if part.as_ew_allowed and not wrap_as_ew_blocked:\\n            parts.insert(0, part)\\n            want_encoding = True\\n            continue\\n        newline = _steal_trailing_WSP_if_exists(lines)\\n        if newline or part.startswith_fws():\\n            lines.append(newline + tstr)\\n        else:\\n            lines[-1] += tstr\\n    return policy.linesep.join(lines) + policy.linesep",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _refold_parse_tree(parse_tree, *, policy):\\n    maxlen = policy.max_line_length or float(\"+inf\")\\n    encoding = 'utf-8' if policy.utf8 else 'us-ascii'\\n    lines = ['']\\n    last_ew = None\\n    wrap_as_ew_blocked = 0\\n    want_encoding = False\\n    end_ew_not_allowed = Terminal('', 'wrap_as_ew_blocked')\\n    parts = list(parse_tree)\\n    while parts:\\n        part = parts.pop(0)\\n        if part is end_ew_not_allowed:\\n            wrap_as_ew_blocked -= 1\\n            continue\\n        tstr = str(part)\\n        try:\\n            tstr.encode(encoding)\\n            charset = encoding\\n        except UnicodeEncodeError:\\n            if any(isinstance(x, errors.UndecodableBytesDefect)\\n                   for x in part.all_defects):\\n                charset = 'unknown-8bit'\\n            else:\\n                charset = 'utf-8'\\n            want_encoding = True\\n        if part.token_type == 'mime-parameters':\\n            _fold_mime_parameters(part, lines, maxlen, encoding)\\n            continue\\n        if want_encoding and not wrap_as_ew_blocked:\\n            if not part.as_ew_allowed:\\n                want_encoding = False\\n                last_ew = None\\n                if part.syntactic_break:\\n                    encoded_part = part.fold(policy=policy)[:-len(policy.linesep)]\\n                    if policy.linesep not in encoded_part:\\n                        if len(encoded_part) > maxlen - len(lines[-1]):\\n                            newline = _steal_trailing_WSP_if_exists(lines)\\n                            lines.append(newline)\\n                        lines[-1] += encoded_part\\n                        continue\\n            if not hasattr(part, 'encode'):\\n                parts = list(part) + parts\\n            else:\\n                last_ew = _fold_as_ew(tstr, lines, maxlen, last_ew,\\n                                      part.ew_combine_allowed, charset)\\n            want_encoding = False\\n            continue\\n        if len(tstr) <= maxlen - len(lines[-1]):\\n            lines[-1] += tstr\\n            continue\\n        if (part.syntactic_break and\\n                len(tstr) + 1 <= maxlen):\\n            newline = _steal_trailing_WSP_if_exists(lines)\\n            if newline or part.startswith_fws():\\n                lines.append(newline + tstr)\\n                continue\\n        if not hasattr(part, 'encode'):\\n            newparts = list(part)\\n            if not part.as_ew_allowed:\\n                wrap_as_ew_blocked += 1\\n                newparts.append(end_ew_not_allowed)\\n            parts = newparts + parts\\n            continue\\n        if part.as_ew_allowed and not wrap_as_ew_blocked:\\n            parts.insert(0, part)\\n            want_encoding = True\\n            continue\\n        newline = _steal_trailing_WSP_if_exists(lines)\\n        if newline or part.startswith_fws():\\n            lines.append(newline + tstr)\\n        else:\\n            lines[-1] += tstr\\n    return policy.linesep.join(lines) + policy.linesep"
  },
  {
    "code": "def __init__(self, obj, grouper=None, axis=0, groupings=None,\\n                 name=None):\\n        self.name = name\\n        self.obj = obj\\n        self.axis = axis\\n        if groupings is None:\\n            groupings = _get_groupings(obj, grouper, axis=axis)\\n        self.groupings = groupings\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: multi-groupby prototype working with DataFrame. nice",
    "fixed_code": "def __init__(self, obj, grouper=None, axis=0, groupings=None,\\n                 exclusions=None, name=None):\\n        self.name = name\\n        self.obj = obj\\n        self.axis = axis\\n        if groupings is None:\\n            groupings, exclusions = _get_groupings(obj, grouper, axis=axis)\\n        self.groupings = groupings\\n        self.exclusions = set(exclusions)"
  },
  {
    "code": "def delete_dag(dag_id: str, keep_records_in_log: bool = True, session=None) -> int:\\n    dag = session.query(DagModel).filter(DagModel.dag_id == dag_id).first()\\n    if dag is None:\\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\\n    count = 0\\n    for model in models.base.Base._decl_class_registry.values():  \\n        if hasattr(model, \"dag_id\"):\\n            if keep_records_in_log and model.__name__ == 'Log':\\n                continue\\n            cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))\\n            count += session.query(model).filter(cond).delete(synchronize_session='fetch')\\n    if dag.is_subdag:\\n        parent_dag_id, task_id = dag_id.rsplit(\".\", 1)\\n        for model in models.DagRun, TaskFail, models.TaskInstance:\\n            count += session.query(model).filter(model.dag_id == parent_dag_id,\\n                                                 model.task_id == task_id).delete()\\n    return count",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def delete_dag(dag_id: str, keep_records_in_log: bool = True, session=None) -> int:\\n    dag = session.query(DagModel).filter(DagModel.dag_id == dag_id).first()\\n    if dag is None:\\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\\n    count = 0\\n    for model in models.base.Base._decl_class_registry.values():  \\n        if hasattr(model, \"dag_id\"):\\n            if keep_records_in_log and model.__name__ == 'Log':\\n                continue\\n            cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))\\n            count += session.query(model).filter(cond).delete(synchronize_session='fetch')\\n    if dag.is_subdag:\\n        parent_dag_id, task_id = dag_id.rsplit(\".\", 1)\\n        for model in models.DagRun, TaskFail, models.TaskInstance:\\n            count += session.query(model).filter(model.dag_id == parent_dag_id,\\n                                                 model.task_id == task_id).delete()\\n    return count"
  },
  {
    "code": "def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\\n                             max_iter=100, tol=1e-4, verbose=0,\\n                             solver='lbfgs', coef=None,\\n                             class_weight=None, dual=False, penalty='l2',\\n                             intercept_scaling=1., multi_class='warn',\\n                             random_state=None, check_input=True,\\n                             max_squared_sum=None, sample_weight=None,\\n                             l1_ratio=None):\\n    return _logistic_regression_path(\\n        X, y, pos_class=None, Cs=10, fit_intercept=True, max_iter=100,\\n        tol=1e-4, verbose=0, solver='lbfgs', coef=None, class_weight=None,\\n        dual=False, penalty='l2', intercept_scaling=1., multi_class='warn',\\n        random_state=None, check_input=True, max_squared_sum=None,\\n        sample_weight=None, l1_ratio=None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEP change the default of multi_class to 'auto' in logistic regression (#13807)",
    "fixed_code": "def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\\n                             max_iter=100, tol=1e-4, verbose=0,\\n                             solver='lbfgs', coef=None,\\n                             class_weight=None, dual=False, penalty='l2',\\n                             intercept_scaling=1., multi_class='auto',\\n                             random_state=None, check_input=True,\\n                             max_squared_sum=None, sample_weight=None,\\n                             l1_ratio=None):\\n    return _logistic_regression_path(\\n        X, y, pos_class=None, Cs=10, fit_intercept=True, max_iter=100,\\n        tol=1e-4, verbose=0, solver='lbfgs', coef=None, class_weight=None,\\n        dual=False, penalty='l2', intercept_scaling=1., multi_class='auto',\\n        random_state=None, check_input=True, max_squared_sum=None,\\n        sample_weight=None, l1_ratio=None)"
  },
  {
    "code": "def static_checks(\\n    verbose: bool,\\n    dry_run: bool,\\n    github_repository: str,\\n    all_files: bool,\\n    show_diff_on_failure: bool,\\n    last_commit: bool,\\n    type: Tuple[str],\\n    files: bool,\\n    precommit_args: Tuple,\\n):\\n    if check_pre_commit_installed(verbose=verbose):\\n        command_to_execute = ['pre-commit', 'run']\\n        for single_check in type:\\n            command_to_execute.append(single_check)\\n        if all_files:\\n            command_to_execute.append(\"--all-files\")\\n        if show_diff_on_failure:\\n            command_to_execute.append(\"--show-diff-on-failure\")\\n        if last_commit:\\n            command_to_execute.extend([\"--from-ref\", \"HEAD^\", \"--to-ref\", \"HEAD\"])\\n        if files:\\n            command_to_execute.append(\"--files\")\\n        if verbose or dry_run:\\n            command_to_execute.append(\"--verbose\")\\n        if precommit_args:\\n            command_to_execute.extend(precommit_args)\\n        env = os.environ.copy()\\n        env['GITHUB_REPOSITORY'] = github_repository\\n        static_checks_result = run_command(\\n            command_to_execute,\\n            verbose=verbose,\\n            dry_run=dry_run,\\n            check=False,\\n            no_output_dump_on_exception=True,\\n            text=True,\\n            env=env,\\n        )\\n        if static_checks_result.returncode != 0:\\n            console.print(\"[red]There were errors during pre-commit check. They should be fixed[/n]\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use the new breeze in CI static checks (#23187)\\n\\n  breeze-complete when new static check is added.\\n  of static checks.\\n\\nFixes: #21099",
    "fixed_code": "def static_checks(\\n    verbose: bool,\\n    dry_run: bool,\\n    github_repository: str,\\n    all_files: bool,\\n    show_diff_on_failure: bool,\\n    last_commit: bool,\\n    commit_ref: str,\\n    type: Tuple[str],\\n    files: bool,\\n    precommit_args: Tuple,\\n):\\n    if check_pre_commit_installed(verbose=verbose):\\n        command_to_execute = ['pre-commit', 'run']\\n        if last_commit and commit_ref:\\n            console.print(\"\\n[red]You cannot specify both --last-commit and --commit-ref[/]\\n\")\\n            sys.exit(1)\\n        for single_check in type:\\n            command_to_execute.append(single_check)\\n        if all_files:\\n            command_to_execute.append(\"--all-files\")\\n        if show_diff_on_failure:\\n            command_to_execute.append(\"--show-diff-on-failure\")\\n        if last_commit:\\n            command_to_execute.extend([\"--from-ref\", \"HEAD^\", \"--to-ref\", \"HEAD\"])\\n        if commit_ref:\\n            command_to_execute.extend([\"--from-ref\", f\"{commit_ref}^\", \"--to-ref\", f\"{commit_ref}\"])\\n        if files:\\n            command_to_execute.append(\"--files\")\\n        if verbose or dry_run:\\n            command_to_execute.append(\"--verbose\")\\n        if precommit_args:\\n            command_to_execute.extend(precommit_args)\\n        env = os.environ.copy()\\n        env['GITHUB_REPOSITORY'] = github_repository\\n        static_checks_result = run_command(\\n            command_to_execute,\\n            verbose=verbose,\\n            dry_run=dry_run,\\n            check=False,\\n            no_output_dump_on_exception=True,\\n            text=True,\\n            env=env,\\n        )\\n        if static_checks_result.returncode != 0:\\n            console.print(\"[red]There were errors during pre-commit check. They should be fixed[/n]\")"
  },
  {
    "code": "def get_header(self, name, default=None):\\n\\t\\treturn to_native_str(self.request.headers.get(name, default))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not break cookie parsing on non-utf8 headers",
    "fixed_code": "def get_header(self, name, default=None):\\n\\t\\treturn to_native_str(self.request.headers.get(name, default),\\n\\t\\t\\t\\t\\t\\t\\t errors='replace')"
  },
  {
    "code": "def truncate(self, before=None, after=None):\\n        beg_slice, end_slice = self._getIndices(before, after)\\n        return self[beg_slice:end_slice]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def truncate(self, before=None, after=None):\\n        beg_slice, end_slice = self._getIndices(before, after)\\n        return self[beg_slice:end_slice]"
  },
  {
    "code": "def _python_agg_general(self, func, *args, **kwargs):\\n        group_shape = self._group_shape\\n        counts = np.zeros(group_shape, dtype=int)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _python_agg_general(self, func, *args, **kwargs):\\n        group_shape = self._group_shape\\n        counts = np.zeros(group_shape, dtype=int)"
  },
  {
    "code": "def _round_half_down(self, prec, expdiff, context):\\n        tmp = Decimal( (self._sign, self._int[:prec], self._exp - expdiff))\\n        half = (self._int[prec] == 5)\\n        if half:\\n            for digit in self._int[prec+1:]:\\n                if digit != 0:\\n                    half = 0\\n                    break\\n        if half:\\n            return tmp\\n        return self._round_half_up(prec, expdiff, context, tmp)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _round_half_down(self, prec):\\n        if _exact_half(self._int, prec):\\n            return -1\\n        else:\\n            return self._round_half_up(prec)"
  },
  {
    "code": "def __array_finalize__(self, obj):\\n        self._index = getattr(obj, '_index', None)\\n        self._vector = getattr(obj, '_vector', None)\\n        self._sparse_value = getattr(obj, '_sparse_value', None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "refactored to remove SparseVector class",
    "fixed_code": "def __array_finalize__(self, obj):\\n        self._index = getattr(obj, '_index', None)\\n        self.sparse_index = getattr(obj, 'sparse_index', None)\\n        self.sparse_value = getattr(obj, 'sparse_value', None)"
  },
  {
    "code": "def connect(self, host='localhost', port=0, source_address=None):\\n        if source_address:\\n            self.source_address = source_address\\n        if not port and (host.find(':') == host.rfind(':')):\\n            i = host.rfind(':')\\n            if i >= 0:\\n                host, port = host[:i], host[i + 1:]\\n                try:\\n                    port = int(port)\\n                except ValueError:\\n                    raise socket.error(\"nonnumeric port\")\\n        if not port:\\n            port = self.default_port\\n        if self.debuglevel > 0:\\n            print('connect:', (host, port), file=stderr)\\n        self.sock = self._get_socket(host, port, self.timeout)\\n        self.file = None\\n        (code, msg) = self.getreply()\\n        if self.debuglevel > 0:\\n            print(\"connect:\", msg, file=stderr)\\n        return (code, msg)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def connect(self, host='localhost', port=0, source_address=None):\\n        if source_address:\\n            self.source_address = source_address\\n        if not port and (host.find(':') == host.rfind(':')):\\n            i = host.rfind(':')\\n            if i >= 0:\\n                host, port = host[:i], host[i + 1:]\\n                try:\\n                    port = int(port)\\n                except ValueError:\\n                    raise socket.error(\"nonnumeric port\")\\n        if not port:\\n            port = self.default_port\\n        if self.debuglevel > 0:\\n            print('connect:', (host, port), file=stderr)\\n        self.sock = self._get_socket(host, port, self.timeout)\\n        self.file = None\\n        (code, msg) = self.getreply()\\n        if self.debuglevel > 0:\\n            print(\"connect:\", msg, file=stderr)\\n        return (code, msg)"
  },
  {
    "code": "def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\\n        if not self.current_line:\\n            if self.current_line.__class__ == type:\\n                self.current_line.depth += indent\\n            else:\\n                self.current_line = type(depth=self.current_line.depth + indent)\\n            return  \\n        complete_line = self.current_line\\n        self.current_line = type(depth=complete_line.depth + indent)\\n        yield complete_line",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\\n        if not self.current_line:\\n            if self.current_line.__class__ == type:\\n                self.current_line.depth += indent\\n            else:\\n                self.current_line = type(depth=self.current_line.depth + indent)\\n            return  \\n        complete_line = self.current_line\\n        self.current_line = type(depth=complete_line.depth + indent)\\n        yield complete_line"
  },
  {
    "code": "def request(self, path, data=None, method='GET', ignore_errors=False):\\n        if self._is_web_services_valid():\\n            url = list(urlparse(path.strip(\"/\")))\\n            if url[2] == \"\":\\n                self.module.fail_json(msg=\"Web services rest api endpoint path must be specified. Path [%s].\" % path)\\n            if url[0] == \"\" or url[1] == \"\":\\n                url[0], url[1] = list(urlparse(self.url))[:2]\\n            if not all([word in url[2].split(\"/\")[:2] for word in self.DEFAULT_REST_API_PATH.split(\"/\")]):\\n                if not url[2].startswith(\"/\"):\\n                    url[2] = \"/\" + url[2]\\n                url[2] = self.DEFAULT_REST_API_PATH + url[2]\\n            if not isinstance(data, str):\\n                data = json.dumps(data)\\n            if self.log_requests:\\n                self.module.log(pformat(dict(url=urlunparse(url), data=data, method=method)))\\n            return request(url=urlunparse(url), data=data, method=method, headers=self.DEFAULT_HEADERS, use_proxy=True,\\n                           force=False, last_mod_time=None, timeout=self.DEFAULT_TIMEOUT, http_agent=self.HTTP_AGENT,\\n                           force_basic_auth=True, ignore_errors=ignore_errors, **self.creds)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Improved netapp module utility for E-Series. (#59527)\\n\\nAdd header option to request method in NetAppESeriesModule\\nAdd multipart formdata builder function\\nFix issue with url port change",
    "fixed_code": "def request(self, path, data=None, method='GET', headers=None, ignore_errors=False):\\n        self._check_web_services_version()\\n        if headers is None:\\n            headers = self.DEFAULT_HEADERS\\n        if not isinstance(data, str) and headers[\"Content-Type\"] == \"application/json\":\\n            data = json.dumps(data)\\n        if path.startswith(\"/\"):\\n            path = path[1:]\\n        request_url = self.url + self.DEFAULT_REST_API_PATH + path\\n        if self.log_requests or True:\\n            self.module.log(pformat(dict(url=request_url, data=data, method=method)))\\n        return request(url=request_url, data=data, method=method, headers=headers, use_proxy=True, force=False, last_mod_time=None,\\n                       timeout=self.DEFAULT_TIMEOUT, http_agent=self.HTTP_AGENT, force_basic_auth=True, ignore_errors=ignore_errors, **self.creds)"
  },
  {
    "code": "def wait_for_volume_action(self, timeout=None):\\n        action = None\\n        percent_complete = None\\n        while action != 'none':\\n            time.sleep(5)\\n            try:\\n                rc, expansion = self.request(\"storage-systems/%s/volumes/%s/expand\"\\n                                             % (self.ssid, self.volume_detail[\"id\"]))\\n                action = expansion[\"action\"]\\n                percent_complete = expansion[\"percentComplete\"]\\n            except Exception as err:\\n                self.module.fail_json(msg=\"Failed to get volume expansion progress. Volume [%s]. Array Id [%s].\"\\n                                          \" Error[%s].\" % (self.name, self.ssid, to_native(err)))\\n            if timeout <= 0:\\n                self.module.warn(\"Expansion action, %s, failed to complete during the allotted time. Time remaining\"\\n                                 \" [%s]. Array Id [%s].\" % (action, percent_complete, self.ssid))\\n                self.module.fail_json(msg=\"Expansion action failed to complete. Time remaining [%s]. Array Id [%s].\"\\n                                          % (percent_complete, self.ssid))\\n            if timeout:\\n                timeout -= 5\\n            self.module.log(\"Expansion action, %s, is %s complete.\" % (action, percent_complete))\\n        self.module.log(\"Expansion action is complete.\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix netapp_e_volume wait-for-initialization to complete issue. (#58304)",
    "fixed_code": "def wait_for_volume_action(self, timeout=None):\\n        action = \"unknown\"\\n        percent_complete = None\\n        while action != \"complete\":\\n            sleep(5)\\n            try:\\n                rc, operations = self.request(\"storage-systems/%s/symbol/getLongLivedOpsProgress\" % self.ssid)\\n                action = \"complete\"\\n                for operation in operations[\"longLivedOpsProgress\"]:\\n                    if operation[\"volAction\"] is not None:\\n                        for key in operation.keys():\\n                            if (operation[key] is not None and \"volumeRef\" in operation[key] and\\n                                    (operation[key][\"volumeRef\"] == self.volume_detail[\"id\"] or\\n                                     (\"storageVolumeRef\" in self.volume_detail and operation[key][\"volumeRef\"] == self.volume_detail[\"storageVolumeRef\"]))):\\n                                action = operation[\"volAction\"]\\n                                percent_complete = operation[\"init\"][\"percentComplete\"]\\n            except Exception as err:\\n                self.module.fail_json(msg=\"Failed to get volume expansion progress. Volume [%s]. Array Id [%s].\"\\n                                          \" Error[%s].\" % (self.name, self.ssid, to_native(err)))\\n            if timeout is not None:\\n                if timeout <= 0:\\n                    self.module.warn(\"Expansion action, %s, failed to complete during the allotted time. Time remaining\"\\n                                     \" [%s]. Array Id [%s].\" % (action, percent_complete, self.ssid))\\n                    self.module.fail_json(msg=\"Expansion action failed to complete. Time remaining [%s]. Array Id [%s].\" % (percent_complete, self.ssid))\\n                if timeout:\\n                    timeout -= 5\\n            self.module.log(\"Expansion action, %s, is %s complete.\" % (action, percent_complete))\\n        self.module.log(\"Expansion action is complete.\")"
  },
  {
    "code": "def load_pkcs12(buffer, passphrase=None):\\n\\tpassphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\\n\\tif isinstance(buffer, _text_type):\\n\\t\\tbuffer = buffer.encode(\"ascii\")\\n\\tbio = _new_mem_buf(buffer)\\n\\tif not passphrase:\\n\\t\\tpassphrase = _ffi.NULL\\n\\tp12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\\n\\tif p12 == _ffi.NULL:\\n\\t\\t_raise_current_error()\\n\\tp12 = _ffi.gc(p12, _lib.PKCS12_free)\\n\\tpkey = _ffi.new(\"EVP_PKEY**\")\\n\\tcert = _ffi.new(\"X509**\")\\n\\tcacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\\n\\tparse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\\n\\tif not parse_result:\\n\\t\\t_raise_current_error()\\n\\tcacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\\n\\ttry:\\n\\t\\t_raise_current_error()\\n\\texcept Error:\\n\\t\\tpass\\n\\tif pkey[0] == _ffi.NULL:\\n\\t\\tpykey = None\\n\\telse:\\n\\t\\tpykey = PKey.__new__(PKey)\\n\\t\\tpykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\\n\\tif cert[0] == _ffi.NULL:\\n\\t\\tpycert = None\\n\\t\\tfriendlyname = None\\n\\telse:\\n\\t\\tpycert = X509.__new__(X509)\\n\\t\\tpycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\\n\\t\\tfriendlyname_length = _ffi.new(\"int*\")\\n\\t\\tfriendlyname_buffer = _lib.X509_alias_get0(\\n\\t\\t\\tcert[0], friendlyname_length\\n\\t\\t)\\n\\t\\tfriendlyname = _ffi.buffer(\\n\\t\\t\\tfriendlyname_buffer, friendlyname_length[0]\\n\\t\\t)[:]\\n\\t\\tif friendlyname_buffer == _ffi.NULL:\\n\\t\\t\\tfriendlyname = None\\n\\tpycacerts = []\\n\\tfor i in range(_lib.sk_X509_num(cacerts)):\\n\\t\\tpycacert = X509.__new__(X509)\\n\\t\\tpycacert._x509 = _lib.sk_X509_value(cacerts, i)\\n\\t\\tpycacerts.append(pycacert)\\n\\tif not pycacerts:\\n\\t\\tpycacerts = None\\n\\tpkcs12 = PKCS12.__new__(PKCS12)\\n\\tpkcs12._pkey = pykey\\n\\tpkcs12._cert = pycert\\n\\tpkcs12._cacerts = pycacerts\\n\\tpkcs12._friendlyname = friendlyname\\n\\treturn pkcs12",
    "label": 1,
    "bug_type": "security",
    "bug_description": "fix a memory leak and a potential UAF and also #722 (#723)",
    "fixed_code": "def load_pkcs12(buffer, passphrase=None):\\n\\tpassphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\\n\\tif isinstance(buffer, _text_type):\\n\\t\\tbuffer = buffer.encode(\"ascii\")\\n\\tbio = _new_mem_buf(buffer)\\n\\tif not passphrase:\\n\\t\\tpassphrase = _ffi.NULL\\n\\tp12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\\n\\tif p12 == _ffi.NULL:\\n\\t\\t_raise_current_error()\\n\\tp12 = _ffi.gc(p12, _lib.PKCS12_free)\\n\\tpkey = _ffi.new(\"EVP_PKEY**\")\\n\\tcert = _ffi.new(\"X509**\")\\n\\tcacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\\n\\tparse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\\n\\tif not parse_result:\\n\\t\\t_raise_current_error()\\n\\tcacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\\n\\ttry:\\n\\t\\t_raise_current_error()\\n\\texcept Error:\\n\\t\\tpass\\n\\tif pkey[0] == _ffi.NULL:\\n\\t\\tpykey = None\\n\\telse:\\n\\t\\tpykey = PKey.__new__(PKey)\\n\\t\\tpykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\\n\\tif cert[0] == _ffi.NULL:\\n\\t\\tpycert = None\\n\\t\\tfriendlyname = None\\n\\telse:\\n\\t\\tpycert = X509._from_raw_x509_ptr(cert[0])\\n\\t\\tfriendlyname_length = _ffi.new(\"int*\")\\n\\t\\tfriendlyname_buffer = _lib.X509_alias_get0(\\n\\t\\t\\tcert[0], friendlyname_length\\n\\t\\t)\\n\\t\\tfriendlyname = _ffi.buffer(\\n\\t\\t\\tfriendlyname_buffer, friendlyname_length[0]\\n\\t\\t)[:]\\n\\t\\tif friendlyname_buffer == _ffi.NULL:\\n\\t\\t\\tfriendlyname = None\\n\\tpycacerts = []\\n\\tfor i in range(_lib.sk_X509_num(cacerts)):\\n\\t\\tx509 = _lib.sk_X509_value(cacerts, i)\\n\\t\\tpycacert = X509._from_raw_x509_ptr(x509)\\n\\t\\tpycacerts.append(pycacert)\\n\\tif not pycacerts:\\n\\t\\tpycacerts = None\\n\\tpkcs12 = PKCS12.__new__(PKCS12)\\n\\tpkcs12._pkey = pykey\\n\\tpkcs12._cert = pycert\\n\\tpkcs12._cacerts = pycacerts\\n\\tpkcs12._friendlyname = friendlyname\\n\\treturn pkcs12"
  },
  {
    "code": "def get_scorer(scoring):\\n    if isinstance(scoring, six.string_types):\\n        if scoring in ('mean_squared_error', 'mean_absolute_error',\\n                       'median_absolute_error', 'log_loss'):\\n            warnings.warn('Scoring method %s was renamed to '\\n                          'neg_%s in version 0.18 and will be '\\n                          'removed in 0.20.' % (scoring, scoring),\\n                          category=DeprecationWarning)\\n            scoring = 'neg_' + scoring\\n        try:\\n            scorer = SCORERS[scoring]\\n        except KeyError:\\n            raise ValueError('%r is not a valid scoring value. '\\n                             'Valid options are %s'\\n                             % (scoring, sorted(SCORERS.keys())))\\n    else:\\n        scorer = scoring\\n    return scorer",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Keep old metric names for deprecation period\\n\\nMaintain old names during the deprecation period and update\\ntests to use better variables names.",
    "fixed_code": "def get_scorer(scoring):\\n    if isinstance(scoring, six.string_types):\\n        try:\\n            scorer = SCORERS[scoring]\\n        except KeyError:\\n            raise ValueError('%r is not a valid scoring value. '\\n                             'Valid options are %s'\\n                             % (scoring, sorted(SCORERS.keys())))\\n    else:\\n        scorer = scoring\\n    return scorer"
  },
  {
    "code": "def process_lhs(self, compiler, connection, lhs=None):\\n        lhs = lhs or self.lhs\\n        return compiler.compile(lhs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25605 -- Made GIS DB functions accept geometric expressions, not only values, in all positions.",
    "fixed_code": "def process_lhs(self, compiler, connection, lhs=None):\\n        lhs = lhs or self.lhs\\n        if hasattr(lhs, 'resolve_expression'):\\n            lhs = lhs.resolve_expression(compiler.query)\\n        return compiler.compile(lhs)"
  },
  {
    "code": "def webserver(args):\\n    print(settings.HEADER)\\n    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')\\n    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')\\n    access_logformat = args.access_logformat or conf.get('webserver', 'access_logformat')\\n    num_workers = args.workers or conf.get('webserver', 'workers')\\n    worker_timeout = args.worker_timeout or conf.get('webserver', 'web_server_worker_timeout')\\n    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')\\n    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')\\n    if not ssl_cert and ssl_key:\\n        raise AirflowException('An SSL certificate must also be provided for use with ' + ssl_key)\\n    if ssl_cert and not ssl_key:\\n        raise AirflowException('An SSL key must also be provided for use with ' + ssl_cert)\\n    if args.debug:\\n        print(f\"Starting the web server on port {args.port} and host {args.hostname}.\")\\n        app = create_app(testing=conf.getboolean('core', 'unit_test_mode'))\\n        app.run(\\n            debug=True,\\n            use_reloader=not app.config['TESTING'],\\n            port=args.port,\\n            host=args.hostname,\\n            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,\\n        )\\n    else:\\n        os.environ['SKIP_DAGS_PARSING'] = 'True'\\n        app = cached_app(None)\\n        os.environ.pop('SKIP_DAGS_PARSING')\\n        pid_file, stdout, stderr, log_file = setup_locations(\\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file\\n        )\\n        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\\n        print(\\n            textwrap.dedent(\\n                ''.format(\\n                    num_workers=num_workers,\\n                    workerclass=args.workerclass,\\n                    hostname=args.hostname,\\n                    port=args.port,\\n                    worker_timeout=worker_timeout,\\n                    access_logfile=access_logfile,\\n                    error_logfile=error_logfile,\\n                    access_logformat=access_logformat,\\n                )\\n            )\\n        )\\n        run_args = [\\n            'gunicorn',\\n            '--workers',\\n            str(num_workers),\\n            '--worker-class',\\n            str(args.workerclass),\\n            '--timeout',\\n            str(worker_timeout),\\n            '--bind',\\n            args.hostname + ':' + str(args.port),\\n            '--name',\\n            'airflow-webserver',\\n            '--pid',\\n            pid_file,\\n            '--config',\\n            'python:airflow.www.gunicorn_config',\\n        ]\\n        if args.access_logfile:\\n            run_args += ['--access-logfile', str(args.access_logfile)]\\n        if args.error_logfile:\\n            run_args += ['--error-logfile', str(args.error_logfile)]\\n        if args.access_logformat and args.access_logformat.strip():\\n            run_args += ['--access-logformat', str(args.access_logformat)]\\n        if args.daemon:\\n            run_args += ['--daemon']\\n        if ssl_cert:\\n            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]\\n        run_args += [\"airflow.www.app:cached_app()\"]\\n        gunicorn_master_proc = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def webserver(args):\\n    print(settings.HEADER)\\n    access_logfile = args.access_logfile or conf.get('webserver', 'access_logfile')\\n    error_logfile = args.error_logfile or conf.get('webserver', 'error_logfile')\\n    access_logformat = args.access_logformat or conf.get('webserver', 'access_logformat')\\n    num_workers = args.workers or conf.get('webserver', 'workers')\\n    worker_timeout = args.worker_timeout or conf.get('webserver', 'web_server_worker_timeout')\\n    ssl_cert = args.ssl_cert or conf.get('webserver', 'web_server_ssl_cert')\\n    ssl_key = args.ssl_key or conf.get('webserver', 'web_server_ssl_key')\\n    if not ssl_cert and ssl_key:\\n        raise AirflowException('An SSL certificate must also be provided for use with ' + ssl_key)\\n    if ssl_cert and not ssl_key:\\n        raise AirflowException('An SSL key must also be provided for use with ' + ssl_cert)\\n    if args.debug:\\n        print(f\"Starting the web server on port {args.port} and host {args.hostname}.\")\\n        app = create_app(testing=conf.getboolean('core', 'unit_test_mode'))\\n        app.run(\\n            debug=True,\\n            use_reloader=not app.config['TESTING'],\\n            port=args.port,\\n            host=args.hostname,\\n            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,\\n        )\\n    else:\\n        os.environ['SKIP_DAGS_PARSING'] = 'True'\\n        app = cached_app(None)\\n        os.environ.pop('SKIP_DAGS_PARSING')\\n        pid_file, stdout, stderr, log_file = setup_locations(\\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file\\n        )\\n        check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\\n        print(\\n            textwrap.dedent(\\n                ''.format(\\n                    num_workers=num_workers,\\n                    workerclass=args.workerclass,\\n                    hostname=args.hostname,\\n                    port=args.port,\\n                    worker_timeout=worker_timeout,\\n                    access_logfile=access_logfile,\\n                    error_logfile=error_logfile,\\n                    access_logformat=access_logformat,\\n                )\\n            )\\n        )\\n        run_args = [\\n            'gunicorn',\\n            '--workers',\\n            str(num_workers),\\n            '--worker-class',\\n            str(args.workerclass),\\n            '--timeout',\\n            str(worker_timeout),\\n            '--bind',\\n            args.hostname + ':' + str(args.port),\\n            '--name',\\n            'airflow-webserver',\\n            '--pid',\\n            pid_file,\\n            '--config',\\n            'python:airflow.www.gunicorn_config',\\n        ]\\n        if args.access_logfile:\\n            run_args += ['--access-logfile', str(args.access_logfile)]\\n        if args.error_logfile:\\n            run_args += ['--error-logfile', str(args.error_logfile)]\\n        if args.access_logformat and args.access_logformat.strip():\\n            run_args += ['--access-logformat', str(args.access_logformat)]\\n        if args.daemon:\\n            run_args += ['--daemon']\\n        if ssl_cert:\\n            run_args += ['--certfile', ssl_cert, '--keyfile', ssl_key]\\n        run_args += [\"airflow.www.app:cached_app()\"]\\n        gunicorn_master_proc = None"
  },
  {
    "code": "def __init__(self, index, warn=True):\\n        self.index = index\\n        self.values = np.asarray(index).view('i8')\\n        self.warn = warn\\n        if len(index) < 3:\\n            raise ValueError('Need at least 3 dates to infer frequency')\\n        self.deltas = lib.unique_deltas(self.values)\\n        self.is_unique = len(self.deltas) == 1",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: handle non-DatetimeIndex to support statsmodels",
    "fixed_code": "def __init__(self, index, warn=True):\\n        from pandas.tseries.index import DatetimeIndex\\n        if not isinstance(index, DatetimeIndex):\\n            index = DatetimeIndex(index)\\n        self.index = index\\n        self.values = np.asarray(index).view('i8')\\n        self.warn = warn\\n        if len(index) < 3:\\n            raise ValueError('Need at least 3 dates to infer frequency')\\n        self.deltas = lib.unique_deltas(self.values)\\n        self.is_unique = len(self.deltas) == 1"
  },
  {
    "code": "def _convert_to_float_if_possible(s: str) -> float | str:\\n    try:\\n        return float(s)\\n    except (ValueError, TypeError):\\n        return s",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_to_float_if_possible(s: str) -> float | str:\\n    try:\\n        return float(s)\\n    except (ValueError, TypeError):\\n        return s"
  },
  {
    "code": "def __new__(cls, freq=None):\\n        if isinstance(freq, PeriodDtype):\\n            return freq\\n        elif freq is None:\\n            u = dtypes.PeriodDtypeBase.__new__(cls, -10_000)\\n            u._freq = None\\n            return u\\n        if not isinstance(freq, BaseOffset):\\n            freq = cls._parse_dtype_strict(freq)\\n        try:\\n            return cls._cache_dtypes[freq.freqstr]\\n        except KeyError:\\n            dtype_code = freq._period_dtype_code\\n            u = dtypes.PeriodDtypeBase.__new__(cls, dtype_code)\\n            u._freq = freq\\n            cls._cache_dtypes[freq.freqstr] = u\\n            return u",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __new__(cls, freq=None):\\n        if isinstance(freq, PeriodDtype):\\n            return freq\\n        elif freq is None:\\n            u = dtypes.PeriodDtypeBase.__new__(cls, -10_000)\\n            u._freq = None\\n            return u\\n        if not isinstance(freq, BaseOffset):\\n            freq = cls._parse_dtype_strict(freq)\\n        try:\\n            return cls._cache_dtypes[freq.freqstr]\\n        except KeyError:\\n            dtype_code = freq._period_dtype_code\\n            u = dtypes.PeriodDtypeBase.__new__(cls, dtype_code)\\n            u._freq = freq\\n            cls._cache_dtypes[freq.freqstr] = u\\n            return u"
  },
  {
    "code": "def body(self, id, file=None):\\n        return self.artcmd('BODY {0}'.format(id), file)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def body(self, message_spec=None, *, file=None):\\n        if message_spec is not None:\\n            cmd = 'BODY {0}'.format(message_spec)\\n        else:\\n            cmd = 'BODY'\\n        return self._artcmd(cmd, file)"
  },
  {
    "code": "def __init__(self, editwin=None):\\n        self.editwin = editwin\\n        if editwin is not None:   \\n            self.text = editwin.text\\n            self.autocompletewindow = None\\n            self._delayed_completion_id = None\\n            self._delayed_completion_index = None\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, editwin=None):\\n        self.editwin = editwin\\n        if editwin is not None:   \\n            self.text = editwin.text\\n            self.autocompletewindow = None\\n            self._delayed_completion_id = None\\n            self._delayed_completion_index = None\\n    @classmethod"
  },
  {
    "code": "def time_partition(self):\\n        self.s.str.partition(\"A\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_partition(self, dtype):\\n        self.s.str.partition(\"A\")"
  },
  {
    "code": "def get_datevalue(date, freq):\\n    if isinstance(date, Period):\\n        return date.asfreq(freq).ordinal\\n    elif isinstance(date, (compat.string_types, datetime,\\n                           pydt.date, pydt.time, np.datetime64)):\\n        return Period(date, freq).ordinal\\n    elif (is_integer(date) or is_float(date) or\\n          (isinstance(date, (np.ndarray, Index)) and (date.size == 1))):\\n        return date\\n    elif date is None:\\n        return None\\n    raise ValueError(\"Unrecognizable date '{date}'\".format(date=date))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_datevalue(date, freq):\\n    if isinstance(date, Period):\\n        return date.asfreq(freq).ordinal\\n    elif isinstance(date, (compat.string_types, datetime,\\n                           pydt.date, pydt.time, np.datetime64)):\\n        return Period(date, freq).ordinal\\n    elif (is_integer(date) or is_float(date) or\\n          (isinstance(date, (np.ndarray, Index)) and (date.size == 1))):\\n        return date\\n    elif date is None:\\n        return None\\n    raise ValueError(\"Unrecognizable date '{date}'\".format(date=date))"
  },
  {
    "code": "def run(self):\\n\\t\\twhile True:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif self._stopped:\\n\\t\\t\\t\\t\\tLOG.info(_(\"Stopping TaskManager\"))\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\ttask = self._get_task()\\n\\t\\t\\t\\tif task.resource_id in self._tasks:\\n\\t\\t\\t\\t\\tself._enqueue(task)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tself._main_thread_exec_task = task\\n\\t\\t\\t\\t\\tself._execute(task)\\n\\t\\t\\t\\tfinally:\\n\\t\\t\\t\\t\\tself._main_thread_exec_task = None\\n\\t\\t\\t\\t\\tif task.status is None:\\n\\t\\t\\t\\t\\t\\tself._enqueue(task)\\n\\t\\t\\t\\t\\telif task.status != TaskStatus.PENDING:\\n\\t\\t\\t\\t\\t\\tself._result(task)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tself._enqueue(task)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\tLOG.exception(_(\"TaskManager terminating because \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"of an exception\"))\\n\\t\\t\\t\\tbreak",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run(self):\\n\\t\\twhile True:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif self._stopped:\\n\\t\\t\\t\\t\\tLOG.info(_(\"Stopping TaskManager\"))\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\ttask = self._get_task()\\n\\t\\t\\t\\tif task.resource_id in self._tasks:\\n\\t\\t\\t\\t\\tself._enqueue(task)\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tself._main_thread_exec_task = task\\n\\t\\t\\t\\t\\tself._execute(task)\\n\\t\\t\\t\\tfinally:\\n\\t\\t\\t\\t\\tself._main_thread_exec_task = None\\n\\t\\t\\t\\t\\tif task.status is None:\\n\\t\\t\\t\\t\\t\\tself._enqueue(task)\\n\\t\\t\\t\\t\\telif task.status != TaskStatus.PENDING:\\n\\t\\t\\t\\t\\t\\tself._result(task)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tself._enqueue(task)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\tLOG.exception(_(\"TaskManager terminating because \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"of an exception\"))\\n\\t\\t\\t\\tbreak"
  },
  {
    "code": "def time_wrap(self):\\n        self.s.str.wrap(10)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_wrap(self, dtype):\\n        self.s.str.wrap(10)"
  },
  {
    "code": "def uuid5(namespace, name):\\n    from hashlib import sha1\\n    hash = sha1(namespace.bytes + name).digest()\\n    return UUID(bytes=hash[:16], version=5)\\nNAMESPACE_DNS = UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')\\nNAMESPACE_URL = UUID('6ba7b811-9dad-11d1-80b4-00c04fd430c8')\\nNAMESPACE_OID = UUID('6ba7b812-9dad-11d1-80b4-00c04fd430c8')\\nNAMESPACE_X500 = UUID('6ba7b814-9dad-11d1-80b4-00c04fd430c8')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make test_uuid passed. Change the UUID properties to use @property. Change the UUID API so that .bytes and .bytes_le return, and the corresponding arguments to __init__() require, values of type 'bytes'.",
    "fixed_code": "def uuid5(namespace, name):\\n    from hashlib import sha1\\n    hash = sha1(namespace.bytes + bytes(name, \"utf-8\")).digest()\\n    return UUID(bytes=bytes_(hash[:16]), version=5)\\nNAMESPACE_DNS = UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')\\nNAMESPACE_URL = UUID('6ba7b811-9dad-11d1-80b4-00c04fd430c8')\\nNAMESPACE_OID = UUID('6ba7b812-9dad-11d1-80b4-00c04fd430c8')\\nNAMESPACE_X500 = UUID('6ba7b814-9dad-11d1-80b4-00c04fd430c8')"
  },
  {
    "code": "def get_client_uri(self, protocol, host, port, proxied_path):\\n\\t\\tif self.absolute_url:\\n\\t\\t\\tcontext_path = self._get_context_path(host, port)\\n\\t\\t\\tclient_path = url_path_join(context_path, proxied_path)\\n\\t\\telse:\\n\\t\\t\\tclient_path = proxied_path\\n\\t\\tif not client_path.startswith(\"/\"):\\n\\t\\t\\tclient_path = \"/\" + client_path\\n\\t\\tclient_path = quote(client_path, safe=\":/?\\n\\t\\tclient_uri = '{protocol}://{host}:{port}{path}'.format(\\n\\t\\t\\tprotocol=protocol,\\n\\t\\t\\thost=host,\\n\\t\\t\\tport=port,\\n\\t\\t\\tpath=client_path,\\n\\t\\t)\\n\\t\\tif self.request.query:\\n\\t\\t\\tclient_uri += '?' + self.request.query\\n\\t\\treturn client_uri",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_client_uri(self, protocol, host, port, proxied_path):\\n\\t\\tif self.absolute_url:\\n\\t\\t\\tcontext_path = self._get_context_path(host, port)\\n\\t\\t\\tclient_path = url_path_join(context_path, proxied_path)\\n\\t\\telse:\\n\\t\\t\\tclient_path = proxied_path\\n\\t\\tif not client_path.startswith(\"/\"):\\n\\t\\t\\tclient_path = \"/\" + client_path\\n\\t\\tclient_path = quote(client_path, safe=\":/?\\n\\t\\tclient_uri = '{protocol}://{host}:{port}{path}'.format(\\n\\t\\t\\tprotocol=protocol,\\n\\t\\t\\thost=host,\\n\\t\\t\\tport=port,\\n\\t\\t\\tpath=client_path,\\n\\t\\t)\\n\\t\\tif self.request.query:\\n\\t\\t\\tclient_uri += '?' + self.request.query\\n\\t\\treturn client_uri"
  },
  {
    "code": "def load_binstring(self):\\n        len, = unpack('<i', self.read(4))\\n        if len < 0:\\n            raise UnpicklingError(\"BINSTRING pickle has negative byte count\")\\n        data = self.read(len)\\n        self.append(self._decode_string(data))\\n    dispatch[BINSTRING[0]] = load_binstring",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_binstring(self):\\n        len, = unpack('<i', self.read(4))\\n        if len < 0:\\n            raise UnpicklingError(\"BINSTRING pickle has negative byte count\")\\n        data = self.read(len)\\n        self.append(self._decode_string(data))\\n    dispatch[BINSTRING[0]] = load_binstring"
  },
  {
    "code": "def setitem(self, indexer, value):\\n        transpose = self.ndim == 2\\n        if value is None:\\n            if self.is_numeric:\\n                value = np.nan\\n        values = self.values\\n        if self._can_hold_element(value):\\n            if lib.is_scalar(value):\\n                value = convert_scalar(values, value)\\n        else:\\n            find_dtype = False\\n            if hasattr(value, \"dtype\"):\\n                dtype = value.dtype\\n                find_dtype = True\\n            elif lib.is_scalar(value) and not isna(value):\\n                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)\\n                find_dtype = True\\n            if find_dtype:\\n                dtype = find_common_type([values.dtype, dtype])\\n                if not is_dtype_equal(self.dtype, dtype):\\n                    b = self.astype(dtype)\\n                    return b.setitem(indexer, value)\\n        if is_extension_array_dtype(getattr(value, \"dtype\", None)):\\n            arr_value = value\\n        else:\\n            arr_value = np.array(value)\\n        if not self._can_hold_element(value):\\n            dtype, _ = maybe_promote(arr_value.dtype)\\n            values = values.astype(dtype)\\n        if transpose:\\n            values = values.T\\n        check_setitem_lengths(indexer, value, values)\\n        if is_empty_indexer(indexer, arr_value):\\n            pass\\n        elif is_scalar_indexer(indexer, arr_value):\\n            values[indexer] = value\\n        elif (\\n            len(arr_value.shape)\\n            and arr_value.shape[0] == values.shape[0]\\n            and arr_value.size == values.size\\n        ):\\n            values[indexer] = value\\n            try:\\n                values = values.astype(arr_value.dtype)\\n            except ValueError:\\n                pass\\n        else:\\n            values[indexer] = value\\n        if transpose:\\n            values = values.T\\n        block = self.make_block(values)\\n        return block",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setitem(self, indexer, value):\\n        transpose = self.ndim == 2\\n        if value is None:\\n            if self.is_numeric:\\n                value = np.nan\\n        values = self.values\\n        if self._can_hold_element(value):\\n            if lib.is_scalar(value):\\n                value = convert_scalar(values, value)\\n        else:\\n            find_dtype = False\\n            if hasattr(value, \"dtype\"):\\n                dtype = value.dtype\\n                find_dtype = True\\n            elif lib.is_scalar(value) and not isna(value):\\n                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)\\n                find_dtype = True\\n            if find_dtype:\\n                dtype = find_common_type([values.dtype, dtype])\\n                if not is_dtype_equal(self.dtype, dtype):\\n                    b = self.astype(dtype)\\n                    return b.setitem(indexer, value)\\n        if is_extension_array_dtype(getattr(value, \"dtype\", None)):\\n            arr_value = value\\n        else:\\n            arr_value = np.array(value)\\n        if not self._can_hold_element(value):\\n            dtype, _ = maybe_promote(arr_value.dtype)\\n            values = values.astype(dtype)\\n        if transpose:\\n            values = values.T\\n        check_setitem_lengths(indexer, value, values)\\n        if is_empty_indexer(indexer, arr_value):\\n            pass\\n        elif is_scalar_indexer(indexer, arr_value):\\n            values[indexer] = value\\n        elif (\\n            len(arr_value.shape)\\n            and arr_value.shape[0] == values.shape[0]\\n            and arr_value.size == values.size\\n        ):\\n            values[indexer] = value\\n            try:\\n                values = values.astype(arr_value.dtype)\\n            except ValueError:\\n                pass\\n        else:\\n            values[indexer] = value\\n        if transpose:\\n            values = values.T\\n        block = self.make_block(values)\\n        return block"
  },
  {
    "code": "def srs(self):\\n        \"Returns the OSR SpatialReference for SRID of this Geometry.\"\\n        if gdal.HAS_GDAL:\\n            if self.srid:\\n                return gdal.SpatialReference(self.srid)\\n            else:\\n                return None\\n        else:\\n            raise GEOSException('GDAL required to return a SpatialReference object.')\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19171 -- Allowed coordinate transforms with custom SRIDs\\n\\nThanks reidpr at lanl.gov for the report.",
    "fixed_code": "def srs(self):\\n        \"Returns the OSR SpatialReference for SRID of this Geometry.\"\\n        if not gdal.HAS_GDAL:\\n            raise GEOSException('GDAL required to return a SpatialReference object.')\\n        if self.srid:\\n            try:\\n                return gdal.SpatialReference(self.srid)\\n            except SRSException:\\n                pass\\n        return None\\n    @property"
  },
  {
    "code": "def tolist(self):\\n        return list(self.asobject)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def tolist(self):\\n        return list(self.asobject)"
  },
  {
    "code": "def fit(self, X, y=None):\\n        if not sp.issparse(X):\\n            X = np.atleast_2d(X)\\n        self.random_state = check_random_state(self.random_state)\\n        n_samples, n_features = X.shape\\n        if self.n_components == 'auto':\\n            self.n_components_ = johnson_lindenstrauss_min_dim(\\n                n_samples, eps=self.eps)\\n            if self.n_components_ <= 0:\\n                raise ValueError(\\n                    'eps=%f and n_samples=%d lead to a target dimension of '\\n                    '%d which is invalid' % (\\n                        self.eps, n_samples, self.n_components_))\\n            elif self.n_components_ > n_features:\\n                raise ValueError(\\n                    'eps=%f and n_samples=%d lead to a target dimension of '\\n                    '%d which is larger than the original space with '\\n                    'n_features=%d' % (self.eps, n_samples, self.n_components_,\\n                                       n_features))\\n        else:\\n            self.n_components_ = self.n_components\\n        self.density_ = _check_density(self.density, n_features)\\n        self.components_ = sparse_random_matrix(\\n            self.n_components_, n_features, density=self.density,\\n            random_state=self.random_state)\\n        assert_equal(\\n            self.components_.shape,\\n            (self.n_components_, n_features),\\n            msg='An error has occured the produced projection matrix has'\\n                    'not the proper shape.')\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add gaussian projeciton + refactor sparse random matrix to reuse code",
    "fixed_code": "def fit(self, X, y=None):\\n        if not sp.issparse(X):\\n            X = np.atleast_2d(X)\\n        self.random_state = check_random_state(self.random_state)\\n        n_samples, n_features = X.shape\\n        if self.n_components == 'auto':\\n            self.n_components_ = johnson_lindenstrauss_min_dim(\\n                n_samples, eps=self.eps)\\n            if self.n_components_ <= 0:\\n                raise ValueError(\\n                    'eps=%f and n_samples=%d lead to a target dimension of '\\n                    '%d which is invalid' % (\\n                        self.eps, n_samples, self.n_components_))\\n            elif self.n_components_ > n_features:\\n                raise ValueError(\\n                    'eps=%f and n_samples=%d lead to a target dimension of '\\n                    '%d which is larger than the original space with '\\n                    'n_features=%d' % (self.eps, n_samples, self.n_components_,\\n                                       n_features))\\n        else:\\n            if self.n_components <= 0:\\n                raise ValueError(\"n_components must be greater than 0, got %s\"\\n                                 % self.n_components_)\\n            self.n_components_ = self.n_components\\n        if self.distribution == \"bernouilli\":\\n            self.density_ = _check_density(self.density, n_features)\\n            self.components_ = bernouilli_random_matrix(\\n                self.n_components_, n_features, density=self.density,\\n                random_state=self.random_state)\\n        elif self.distribution == \"gaussian\":\\n            if self.density != \"auto\" and self.density != 1.0:\\n                raise ValueError(\"Density should be equal to one for Gaussian\"\\n                                 \" random projection.\")\\n            self.components_ = gaussian_random_matrix(self.n_components_,\\n                n_features, random_state=self.random_state)\\n        else:\\n            raise ValueError('Unknown specified distribution.')\\n        assert_equal(\\n            self.components_.shape,\\n            (self.n_components_, n_features),\\n            err_msg=('An error has occured the projection matrix has not '\\n                 'the proper shape.'))\\n        return self"
  },
  {
    "code": "def register(show_spinner=False) -> str:\\n  params = Params()\\n  params.put(\"SubscriberInfo\", HARDWARE.get_subscriber_info())\\n  IMEI = params.get(\"IMEI\", encoding='utf8')\\n  HardwareSerial = params.get(\"HardwareSerial\", encoding='utf8')\\n  dongle_id = params.get(\"DongleId\", encoding='utf8')\\n  needs_registration = None in (IMEI, HardwareSerial, dongle_id)\\n  pubkey = Path(PERSIST+\"/comma/id_rsa.pub\")\\n  if not pubkey.is_file():\\n    dongle_id = UNREGISTERED_DONGLE_ID\\n    cloudlog.warning(f\"missing public key: {pubkey}\")\\n  elif needs_registration:\\n    if show_spinner:\\n      spinner = Spinner()\\n      spinner.update(\"registering device\")\\n    with open(PERSIST+\"/comma/id_rsa.pub\") as f1, open(PERSIST+\"/comma/id_rsa\") as f2:\\n      public_key = f1.read()\\n      private_key = f2.read()\\n    serial = HARDWARE.get_serial()\\n    start_time = time.monotonic()\\n    imei1, imei2 = None, None\\n    while imei1 is None and imei2 is None:\\n      try:\\n        imei1, imei2 = HARDWARE.get_imei(0), HARDWARE.get_imei(1)\\n      except Exception:\\n        cloudlog.exception(\"Error getting imei, trying again...\")\\n        time.sleep(1)\\n      if time.monotonic() - start_time > 60 and show_spinner:\\n        spinner.update(f\"registering device - serial: {serial}, IMEI: ({imei1}, {imei2})\")\\n    params.put(\"IMEI\", imei1)\\n    params.put(\"HardwareSerial\", serial)\\n    backoff = 0\\n    start_time = time.monotonic()\\n    while True:\\n      try:\\n        register_token = jwt.encode({'register': True, 'exp': datetime.utcnow() + timedelta(hours=1)}, private_key, algorithm='RS256')\\n        cloudlog.info(\"getting pilotauth\")\\n        resp = api_get(\"v2/pilotauth/\", method='POST', timeout=15,\\n                       imei=imei1, imei2=imei2, serial=serial, public_key=public_key, register_token=register_token)\\n        if resp.status_code in (402, 403):\\n          cloudlog.info(f\"Unable to register device, got {resp.status_code}\")\\n          dongle_id = UNREGISTERED_DONGLE_ID\\n        else:\\n          dongleauth = json.loads(resp.text)\\n          dongle_id = dongleauth[\"dongle_id\"]\\n        break\\n      except Exception:\\n        cloudlog.exception(\"failed to authenticate\")\\n        backoff = min(backoff + 1, 15)\\n        time.sleep(backoff)\\n      if time.monotonic() - start_time > 60 and show_spinner:\\n        spinner.update(f\"registering device - serial: {serial}, IMEI: ({imei1}, {imei2})\")\\n    if show_spinner:\\n      spinner.close()\\n  if dongle_id:\\n    params.put(\"DongleId\", dongle_id)\\n    set_offroad_alert(\"Offroad_UnofficialHardware\", dongle_id == UNREGISTERED_DONGLE_ID)\\n  return dongle_id",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def register(show_spinner=False) -> str:\\n  params = Params()\\n  params.put(\"SubscriberInfo\", HARDWARE.get_subscriber_info())\\n  IMEI = params.get(\"IMEI\", encoding='utf8')\\n  HardwareSerial = params.get(\"HardwareSerial\", encoding='utf8')\\n  dongle_id = params.get(\"DongleId\", encoding='utf8')\\n  needs_registration = None in (IMEI, HardwareSerial, dongle_id)\\n  pubkey = Path(PERSIST+\"/comma/id_rsa.pub\")\\n  if not pubkey.is_file():\\n    dongle_id = UNREGISTERED_DONGLE_ID\\n    cloudlog.warning(f\"missing public key: {pubkey}\")\\n  elif needs_registration:\\n    if show_spinner:\\n      spinner = Spinner()\\n      spinner.update(\"registering device\")\\n    with open(PERSIST+\"/comma/id_rsa.pub\") as f1, open(PERSIST+\"/comma/id_rsa\") as f2:\\n      public_key = f1.read()\\n      private_key = f2.read()\\n    serial = HARDWARE.get_serial()\\n    start_time = time.monotonic()\\n    imei1, imei2 = None, None\\n    while imei1 is None and imei2 is None:\\n      try:\\n        imei1, imei2 = HARDWARE.get_imei(0), HARDWARE.get_imei(1)\\n      except Exception:\\n        cloudlog.exception(\"Error getting imei, trying again...\")\\n        time.sleep(1)\\n      if time.monotonic() - start_time > 60 and show_spinner:\\n        spinner.update(f\"registering device - serial: {serial}, IMEI: ({imei1}, {imei2})\")\\n    params.put(\"IMEI\", imei1)\\n    params.put(\"HardwareSerial\", serial)\\n    backoff = 0\\n    start_time = time.monotonic()\\n    while True:\\n      try:\\n        register_token = jwt.encode({'register': True, 'exp': datetime.utcnow() + timedelta(hours=1)}, private_key, algorithm='RS256')\\n        cloudlog.info(\"getting pilotauth\")\\n        resp = api_get(\"v2/pilotauth/\", method='POST', timeout=15,\\n                       imei=imei1, imei2=imei2, serial=serial, public_key=public_key, register_token=register_token)\\n        if resp.status_code in (402, 403):\\n          cloudlog.info(f\"Unable to register device, got {resp.status_code}\")\\n          dongle_id = UNREGISTERED_DONGLE_ID\\n        else:\\n          dongleauth = json.loads(resp.text)\\n          dongle_id = dongleauth[\"dongle_id\"]\\n        break\\n      except Exception:\\n        cloudlog.exception(\"failed to authenticate\")\\n        backoff = min(backoff + 1, 15)\\n        time.sleep(backoff)\\n      if time.monotonic() - start_time > 60 and show_spinner:\\n        spinner.update(f\"registering device - serial: {serial}, IMEI: ({imei1}, {imei2})\")\\n    if show_spinner:\\n      spinner.close()\\n  if dongle_id:\\n    params.put(\"DongleId\", dongle_id)\\n    set_offroad_alert(\"Offroad_UnofficialHardware\", dongle_id == UNREGISTERED_DONGLE_ID)\\n  return dongle_id"
  },
  {
    "code": "def get_put_data(self, month=None, year=None, expiry=None):\\n        return self._get_option_data(month, year, expiry, 'puts').sortlevel()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix pandas.io.data.Options for change in format of Yahoo Option page\\n\\nENH: Automatically choose next expiry if expiry date isn't valid\\n\\nCOMPAT: Remove dictionary comprehension to pass 2.6 test\\n\\nBUG: Add check that tables were downloaded.\\n\\nENH: Replace third_saturday function with Pandas offset\\n\\nBUG: Check to make sure enough option tables are downloaded.\\n\\nTST: Add sample options page during market open.\\n\\nDOC: Add bug fix report to v0.15.1.txt for data.Options\\n\\nBUG: Ensure that the value used for chopping is within the range of strikes.\\n\\nTST: Add test for requesting out of range chop\\n\\nBUG: Fix missing underlying price and quote time in first process data\\n\\nBUG: Fix AM/PM on quote time\\n\\nENH: Refactor to expose available expiry dates\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nBUG: Undo accidental deletion of privat emethods\\n\\nENH: Add ability to use list as expiry parameter.\\n\\nAlso has year & month return all data for the specified year and month.\\n\\nTST: Remove tests for warnings that are no longer in use.\\n\\nDOC: Update docstrings of Options class.",
    "fixed_code": "def get_put_data(self, month=None, year=None, expiry=None):\\n        expiry = self._try_parse_dates(year, month, expiry)\\n        return self._get_data_in_date_range(expiry, put=True, call=False)"
  },
  {
    "code": "def _get_dtype_type(arr_or_dtype):\\n    if isinstance(arr_or_dtype, np.dtype):\\n        return arr_or_dtype.type\\n    elif isinstance(arr_or_dtype, type):\\n        return np.dtype(arr_or_dtype).type\\n    elif isinstance(arr_or_dtype, CategoricalDtype):\\n        return CategoricalDtypeType\\n    return arr_or_dtype.dtype.type",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fixing == __eq__ operator for MultiIndex ... closes (GH9785)",
    "fixed_code": "def _get_dtype_type(arr_or_dtype):\\n    if isinstance(arr_or_dtype, np.dtype):\\n        return arr_or_dtype.type\\n    elif isinstance(arr_or_dtype, type):\\n        return np.dtype(arr_or_dtype).type\\n    elif isinstance(arr_or_dtype, CategoricalDtype):\\n        return CategoricalDtypeType\\n    try:\\n        return arr_or_dtype.dtype.type\\n    except AttributeError:\\n        raise ValueError('%r is not a dtype' % arr_or_dtype)"
  },
  {
    "code": "def readable(self):\\n\\t\\tnow = time.time()\\n\\t\\tif now >= self.next_channel_cleanup:\\n\\t\\t\\tself.next_channel_cleanup = now + self.adj.cleanup_interval\\n\\t\\t\\tself.maintenance(now)\\n\\t\\tif (\\n\\t\\t\\tnot self.in_connection_overflow\\n\\t\\t\\tand len(self._map) >= self.adj.connection_limit\\n\\t\\t):\\n\\t\\t\\tself.in_connection_overflow = True\\n\\t\\t\\tself.logger.warning(\\n\\t\\t\\t\\t'server active connections reached the connection limit, '\\n\\t\\t\\t\\t'no longer accepting new connections'\\n\\t\\t\\t)\\n\\t\\telif (\\n\\t\\t\\tself.in_connection_overflow\\n\\t\\t\\tand len(self._map) < self.adj.connection_limit\\n\\t\\t):\\n\\t\\t\\tself.in_connection_overflow = False\\n\\t\\t\\tself.logger.info(\\n\\t\\t\\t\\t'server active connections dropped below the connection limit, '\\n\\t\\t\\t\\t'listening again'\\n\\t\\t\\t)\\n\\t\\treturn self.accepting and not self.in_connection_overflow",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def readable(self):\\n\\t\\tnow = time.time()\\n\\t\\tif now >= self.next_channel_cleanup:\\n\\t\\t\\tself.next_channel_cleanup = now + self.adj.cleanup_interval\\n\\t\\t\\tself.maintenance(now)\\n\\t\\tif (\\n\\t\\t\\tnot self.in_connection_overflow\\n\\t\\t\\tand len(self._map) >= self.adj.connection_limit\\n\\t\\t):\\n\\t\\t\\tself.in_connection_overflow = True\\n\\t\\t\\tself.logger.warning(\\n\\t\\t\\t\\t'server active connections reached the connection limit, '\\n\\t\\t\\t\\t'no longer accepting new connections'\\n\\t\\t\\t)\\n\\t\\telif (\\n\\t\\t\\tself.in_connection_overflow\\n\\t\\t\\tand len(self._map) < self.adj.connection_limit\\n\\t\\t):\\n\\t\\t\\tself.in_connection_overflow = False\\n\\t\\t\\tself.logger.info(\\n\\t\\t\\t\\t'server active connections dropped below the connection limit, '\\n\\t\\t\\t\\t'listening again'\\n\\t\\t\\t)\\n\\t\\treturn self.accepting and not self.in_connection_overflow"
  },
  {
    "code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(sorted(terms))))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(sorted(terms))))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)"
  },
  {
    "code": "def _set_names(self, values):\\n        if len(values) != 1:\\n            raise AssertionError('Length of new names must be 1, got %d'\\n                                 % len(values))\\n        self.name = values[0]\\n    names = property(fset=_set_names, fget=_get_names)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Make core/index exceptions more descriptive\\n\\n  produce copies that are not the same object. (uses `assert_almost_equal`\\n  under the hood).\\n  after iterable check)",
    "fixed_code": "def _set_names(self, values):\\n        if len(values) != 1:\\n            raise ValueError('Length of new names must be 1, got %d'\\n                                 % len(values))\\n        self.name = values[0]\\n    names = property(fset=_set_names, fget=_get_names)\\n    @property"
  },
  {
    "code": "def strip_entities(value):\\n    return re.sub(r'&(?:\\w+|\\nstrip_entities = allow_lazy(strip_entities, six.text_type)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #23269 -- Deprecated django.utils.remove_tags() and removetags filter.\\n\\nAlso the unused, undocumented django.utils.html.strip_entities() function.",
    "fixed_code": "def strip_entities(value):\\n    warnings.warn(\\n        \"django.utils.html.strip_entities() is deprecated.\",\\n        RemovedInDjango20Warning, stacklevel=2\\n    )\\n    return re.sub(r'&(?:\\w+|\\nstrip_entities = allow_lazy(strip_entities, six.text_type)"
  },
  {
    "code": "def detect_encoding(readline):\\n    bom_found = False\\n    encoding = None\\n    default = 'utf-8'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def detect_encoding(readline):\\n    bom_found = False\\n    encoding = None\\n    default = 'utf-8'"
  },
  {
    "code": "def adjusted_rand_score(labels_true, labels_pred):\\n    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\\n    if fn == 0 and fp == 0:\\n        return 1.0\\n    return 2.0 * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX avoid overflow in adjusted_rand_score with large amount data (#20312)",
    "fixed_code": "def adjusted_rand_score(labels_true, labels_pred):\\n    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\\n    tn, fp, fn, tp = int(tn), int(fp), int(fn), int(tp)\\n    if fn == 0 and fp == 0:\\n        return 1.0\\n    return 2.0 * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn))"
  },
  {
    "code": "def do_longs(opts, opt, longopts, args):\\n    try:\\n        i = opt.index('=')\\n    except ValueError:\\n        optarg = None\\n    else:\\n        opt, optarg = opt[:i], opt[i+1:]\\n    has_arg, opt = long_has_args(opt, longopts)\\n    if has_arg:\\n        if optarg is None:\\n            if not args:\\n                raise GetoptError('option --%s requires argument' % opt, opt)\\n            optarg, args = args[0], args[1:]\\n    elif optarg is not None:\\n        raise GetoptError('option --%s must not have an argument' % opt, opt)\\n    opts.append(('--' + opt, optarg or ''))\\n    return opts, args",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_longs(opts, opt, longopts, args):\\n    try:\\n        i = opt.index('=')\\n    except ValueError:\\n        optarg = None\\n    else:\\n        opt, optarg = opt[:i], opt[i+1:]\\n    has_arg, opt = long_has_args(opt, longopts)\\n    if has_arg:\\n        if optarg is None:\\n            if not args:\\n                raise GetoptError('option --%s requires argument' % opt, opt)\\n            optarg, args = args[0], args[1:]\\n    elif optarg is not None:\\n        raise GetoptError('option --%s must not have an argument' % opt, opt)\\n    opts.append(('--' + opt, optarg or ''))\\n    return opts, args"
  },
  {
    "code": "def csv_encode(value):\\n    if py3compat.PY3 or not isinstance(value, unicode):\\n        return value\\n    return value.encode('UTF-8', 'replace')\\nclass UTF8Recoder:",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: made encoding optional on csv read/write, addresses #717",
    "fixed_code": "def csv_encode(value, encoding='UTF-8'):\\n    if py3compat.PY3 or not isinstance(value, unicode):\\n        return value\\n    return value.encode(encoding, 'replace')\\nclass UTF8Recoder:"
  },
  {
    "code": "def get_kube_client(\\n    in_cluster: bool = conf.getboolean('kubernetes_executor', 'in_cluster'),\\n    cluster_context: str | None = None,\\n    config_file: str | None = None,\\n) -> client.CoreV1Api:\\n    if not has_kubernetes:\\n        raise _import_err\\n    if conf.getboolean('kubernetes_executor', 'enable_tcp_keepalive'):\\n        _enable_tcp_keepalive()\\n    if in_cluster:\\n        config.load_incluster_config()\\n    else:\\n        if cluster_context is None:\\n            cluster_context = conf.get('kubernetes_executor', 'cluster_context', fallback=None)\\n        if config_file is None:\\n            config_file = conf.get('kubernetes_executor', 'config_file', fallback=None)\\n        config.load_kube_config(config_file=config_file, context=cluster_context)\\n    if not conf.getboolean('kubernetes_executor', 'verify_ssl'):\\n        _disable_verify_ssl()\\n    return client.CoreV1Api()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_kube_client(\\n    in_cluster: bool = conf.getboolean('kubernetes_executor', 'in_cluster'),\\n    cluster_context: str | None = None,\\n    config_file: str | None = None,\\n) -> client.CoreV1Api:\\n    if not has_kubernetes:\\n        raise _import_err\\n    if conf.getboolean('kubernetes_executor', 'enable_tcp_keepalive'):\\n        _enable_tcp_keepalive()\\n    if in_cluster:\\n        config.load_incluster_config()\\n    else:\\n        if cluster_context is None:\\n            cluster_context = conf.get('kubernetes_executor', 'cluster_context', fallback=None)\\n        if config_file is None:\\n            config_file = conf.get('kubernetes_executor', 'config_file', fallback=None)\\n        config.load_kube_config(config_file=config_file, context=cluster_context)\\n    if not conf.getboolean('kubernetes_executor', 'verify_ssl'):\\n        _disable_verify_ssl()\\n    return client.CoreV1Api()"
  },
  {
    "code": "def opt_jinja_env_helper(opts, optname):\\n\\t\\tfor k, v in opts.items():\\n\\t\\t\\tk = k.lower()\\n\\t\\t\\tif hasattr(jinja2.defaults, k.upper()):\\n\\t\\t\\t\\tlog.debug(\"Jinja2 environment %s was set to %s by %s\", k, v, optname)\\n\\t\\t\\t\\tenv_args[k] = v\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlog.warning(\"Jinja2 environment %s is not recognized\", k)\\n\\tif \"sls\" in context and context[\"sls\"] != \"\":\\n\\t\\topt_jinja_env_helper(opt_jinja_sls_env, \"jinja_sls_env\")\\n\\telse:\\n\\t\\topt_jinja_env_helper(opt_jinja_env, \"jinja_env\")\\n\\tif opts.get(\"allow_undefined\", False):\\n\\t\\tjinja_env = jinja2.sandbox.SandboxedEnvironment(**env_args)\\n\\telse:\\n\\t\\tjinja_env = jinja2.sandbox.SandboxedEnvironment(\\n\\t\\t\\tundefined=jinja2.StrictUndefined, **env_args\\n\\t\\t)\\n\\ttojson_filter = jinja_env.filters.get(\"tojson\")\\n\\tindent_filter = jinja_env.filters.get(\"indent\")\\n\\tjinja_env.tests.update(JinjaTest.salt_jinja_tests)\\n\\tjinja_env.filters.update(JinjaFilter.salt_jinja_filters)\\n\\tif tojson_filter is not None:\\n\\t\\tjinja_env.filters[\"tojson\"] = tojson_filter\\n\\tif salt.utils.jinja.JINJA_VERSION >= LooseVersion(\"2.11\"):\\n\\t\\tjinja_env.filters[\"indent\"] = indent_filter\\n\\tjinja_env.globals.update(JinjaGlobal.salt_jinja_globals)\\n\\tjinja_env.globals[\"odict\"] = OrderedDict\\n\\tjinja_env.globals[\"show_full_context\"] = salt.utils.jinja.show_full_context\\n\\tjinja_env.tests[\"list\"] = salt.utils.data.is_list\\n\\tdecoded_context = {}\\n\\tfor key, value in context.items():\\n\\t\\tif not isinstance(value, str):\\n\\t\\t\\tdecoded_context[key] = value\\n\\t\\t\\tcontinue\\n\\t\\ttry:\\n\\t\\t\\tdecoded_context[key] = salt.utils.stringutils.to_unicode(\\n\\t\\t\\t\\tvalue, encoding=SLS_ENCODING\\n\\t\\t\\t)\\n\\t\\texcept UnicodeDecodeError as ex:\\n\\t\\t\\tlog.debug(\\n\\t\\t\\t\\t\"Failed to decode using default encoding (%s), trying system encoding\",\\n\\t\\t\\t\\tSLS_ENCODING,\\n\\t\\t\\t)\\n\\t\\t\\tdecoded_context[key] = salt.utils.data.decode(value)\\n\\ttry:\\n\\t\\ttemplate = jinja_env.from_string(tmplstr)\\n\\t\\ttemplate.globals.update(decoded_context)\\n\\t\\toutput = template.render(**decoded_context)\\n\\texcept jinja2.exceptions.UndefinedError as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tout = _get_jinja_error(trace, context=decoded_context)[1]\\n\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\"Jinja variable {}{}\".format(exc, out), buf=tmplstr)\\n\\texcept (\\n\\t\\tjinja2.exceptions.TemplateRuntimeError,\\n\\t\\tjinja2.exceptions.TemplateSyntaxError,\\n\\t\\tjinja2.exceptions.SecurityError,\\n\\t) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja syntax error: {}{}\".format(exc, out), line, tmplstr\\n\\t\\t)\\n\\texcept (SaltInvocationError, CommandExecutionError) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Problem running salt function in Jinja template: {}{}\".format(exc, out),\\n\\t\\t\\tline,\\n\\t\\t\\ttmplstr,\\n\\t\\t)\\n\\texcept Exception as exc:  \\n\\t\\ttracestr = traceback.format_exc()\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\telse:\\n\\t\\t\\ttmplstr += \"\\n{}\".format(tracestr)\\n\\t\\tlog.debug(\"Jinja Error\")\\n\\t\\tlog.debug(\"Exception:\", exc_info=True)\\n\\t\\tlog.debug(\"Out: %s\", out)\\n\\t\\tlog.debug(\"Line: %s\", line)\\n\\t\\tlog.debug(\"TmplStr: %s\", tmplstr)\\n\\t\\tlog.debug(\"TraceStr: %s\", tracestr)\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja error: {}{}\".format(exc, out), line, tmplstr, trace=tracestr\\n\\t\\t)\\n\\tif newline:\\n\\t\\toutput += newline\\n\\treturn output",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def opt_jinja_env_helper(opts, optname):\\n\\t\\tfor k, v in opts.items():\\n\\t\\t\\tk = k.lower()\\n\\t\\t\\tif hasattr(jinja2.defaults, k.upper()):\\n\\t\\t\\t\\tlog.debug(\"Jinja2 environment %s was set to %s by %s\", k, v, optname)\\n\\t\\t\\t\\tenv_args[k] = v\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlog.warning(\"Jinja2 environment %s is not recognized\", k)\\n\\tif \"sls\" in context and context[\"sls\"] != \"\":\\n\\t\\topt_jinja_env_helper(opt_jinja_sls_env, \"jinja_sls_env\")\\n\\telse:\\n\\t\\topt_jinja_env_helper(opt_jinja_env, \"jinja_env\")\\n\\tif opts.get(\"allow_undefined\", False):\\n\\t\\tjinja_env = jinja2.sandbox.SandboxedEnvironment(**env_args)\\n\\telse:\\n\\t\\tjinja_env = jinja2.sandbox.SandboxedEnvironment(\\n\\t\\t\\tundefined=jinja2.StrictUndefined, **env_args\\n\\t\\t)\\n\\ttojson_filter = jinja_env.filters.get(\"tojson\")\\n\\tindent_filter = jinja_env.filters.get(\"indent\")\\n\\tjinja_env.tests.update(JinjaTest.salt_jinja_tests)\\n\\tjinja_env.filters.update(JinjaFilter.salt_jinja_filters)\\n\\tif tojson_filter is not None:\\n\\t\\tjinja_env.filters[\"tojson\"] = tojson_filter\\n\\tif salt.utils.jinja.JINJA_VERSION >= LooseVersion(\"2.11\"):\\n\\t\\tjinja_env.filters[\"indent\"] = indent_filter\\n\\tjinja_env.globals.update(JinjaGlobal.salt_jinja_globals)\\n\\tjinja_env.globals[\"odict\"] = OrderedDict\\n\\tjinja_env.globals[\"show_full_context\"] = salt.utils.jinja.show_full_context\\n\\tjinja_env.tests[\"list\"] = salt.utils.data.is_list\\n\\tdecoded_context = {}\\n\\tfor key, value in context.items():\\n\\t\\tif not isinstance(value, str):\\n\\t\\t\\tdecoded_context[key] = value\\n\\t\\t\\tcontinue\\n\\t\\ttry:\\n\\t\\t\\tdecoded_context[key] = salt.utils.stringutils.to_unicode(\\n\\t\\t\\t\\tvalue, encoding=SLS_ENCODING\\n\\t\\t\\t)\\n\\t\\texcept UnicodeDecodeError as ex:\\n\\t\\t\\tlog.debug(\\n\\t\\t\\t\\t\"Failed to decode using default encoding (%s), trying system encoding\",\\n\\t\\t\\t\\tSLS_ENCODING,\\n\\t\\t\\t)\\n\\t\\t\\tdecoded_context[key] = salt.utils.data.decode(value)\\n\\ttry:\\n\\t\\ttemplate = jinja_env.from_string(tmplstr)\\n\\t\\ttemplate.globals.update(decoded_context)\\n\\t\\toutput = template.render(**decoded_context)\\n\\texcept jinja2.exceptions.UndefinedError as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tout = _get_jinja_error(trace, context=decoded_context)[1]\\n\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\"Jinja variable {}{}\".format(exc, out), buf=tmplstr)\\n\\texcept (\\n\\t\\tjinja2.exceptions.TemplateRuntimeError,\\n\\t\\tjinja2.exceptions.TemplateSyntaxError,\\n\\t\\tjinja2.exceptions.SecurityError,\\n\\t) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja syntax error: {}{}\".format(exc, out), line, tmplstr\\n\\t\\t)\\n\\texcept (SaltInvocationError, CommandExecutionError) as exc:\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Problem running salt function in Jinja template: {}{}\".format(exc, out),\\n\\t\\t\\tline,\\n\\t\\t\\ttmplstr,\\n\\t\\t)\\n\\texcept Exception as exc:  \\n\\t\\ttracestr = traceback.format_exc()\\n\\t\\ttrace = traceback.extract_tb(sys.exc_info()[2])\\n\\t\\tline, out = _get_jinja_error(trace, context=decoded_context)\\n\\t\\tif not line:\\n\\t\\t\\ttmplstr = \"\"\\n\\t\\telse:\\n\\t\\t\\ttmplstr += \"\\n{}\".format(tracestr)\\n\\t\\tlog.debug(\"Jinja Error\")\\n\\t\\tlog.debug(\"Exception:\", exc_info=True)\\n\\t\\tlog.debug(\"Out: %s\", out)\\n\\t\\tlog.debug(\"Line: %s\", line)\\n\\t\\tlog.debug(\"TmplStr: %s\", tmplstr)\\n\\t\\tlog.debug(\"TraceStr: %s\", tracestr)\\n\\t\\traise SaltRenderError(\\n\\t\\t\\t\"Jinja error: {}{}\".format(exc, out), line, tmplstr, trace=tracestr\\n\\t\\t)\\n\\tif newline:\\n\\t\\toutput += newline\\n\\treturn output"
  },
  {
    "code": "def __init__(\\n        self,\\n        database: str,\\n        sql: Union[str, List],\\n        cluster_identifier: Optional[str] = None,\\n        db_user: Optional[str] = None,\\n        parameters: Optional[list] = None,\\n        secret_arn: Optional[str] = None,\\n        statement_name: Optional[str] = None,\\n        with_event: bool = False,\\n        await_result: bool = True,\\n        poll_interval: int = 10,\\n        aws_conn_id: str = 'aws_default',\\n        region: Optional[str] = None,\\n        **kwargs,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.database = database\\n        self.sql = sql\\n        self.cluster_identifier = cluster_identifier\\n        self.db_user = db_user\\n        self.parameters = parameters\\n        self.secret_arn = secret_arn\\n        self.statement_name = statement_name\\n        self.with_event = with_event\\n        self.await_result = await_result\\n        if poll_interval > 0:\\n            self.poll_interval = poll_interval\\n        else:\\n            self.log.warning(\\n                \"Invalid poll_interval:\",\\n                poll_interval,\\n            )\\n        self.aws_conn_id = aws_conn_id\\n        self.region = region\\n        self.statement_id = None\\n    @cached_property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        database: str,\\n        sql: Union[str, List],\\n        cluster_identifier: Optional[str] = None,\\n        db_user: Optional[str] = None,\\n        parameters: Optional[list] = None,\\n        secret_arn: Optional[str] = None,\\n        statement_name: Optional[str] = None,\\n        with_event: bool = False,\\n        await_result: bool = True,\\n        poll_interval: int = 10,\\n        aws_conn_id: str = 'aws_default',\\n        region: Optional[str] = None,\\n        **kwargs,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.database = database\\n        self.sql = sql\\n        self.cluster_identifier = cluster_identifier\\n        self.db_user = db_user\\n        self.parameters = parameters\\n        self.secret_arn = secret_arn\\n        self.statement_name = statement_name\\n        self.with_event = with_event\\n        self.await_result = await_result\\n        if poll_interval > 0:\\n            self.poll_interval = poll_interval\\n        else:\\n            self.log.warning(\\n                \"Invalid poll_interval:\",\\n                poll_interval,\\n            )\\n        self.aws_conn_id = aws_conn_id\\n        self.region = region\\n        self.statement_id = None\\n    @cached_property"
  },
  {
    "code": "def save(self):\\n        dispatcher.send(signal=signals.pre_save, sender=self.__class__, instance=self)\\n        non_pks = [f for f in self._meta.fields if not f.primary_key]\\n        cursor = connection.cursor()\\n        pk_val = self._get_pk_val()\\n        pk_set = bool(pk_val)\\n        record_exists = True\\n        if pk_set:\\n            cursor.execute(\"SELECT COUNT(*) FROM %s WHERE %s=%%s\" % \\\\n                (backend.quote_name(self._meta.db_table), backend.quote_name(self._meta.pk.column)),\\n                self._meta.pk.get_db_prep_lookup('exact', pk_val))\\n            if cursor.fetchone()[0] > 0:\\n                db_values = [f.get_db_prep_save(f.pre_save(self, False)) for f in non_pks]\\n                if db_values:\\n                    cursor.execute(\"UPDATE %s SET %s WHERE %s=%%s\" % \\\\n                        (backend.quote_name(self._meta.db_table),\\n                        ','.join(['%s=%%s' % backend.quote_name(f.column) for f in non_pks]),\\n                        backend.quote_name(self._meta.pk.column)),\\n                        db_values + self._meta.pk.get_db_prep_lookup('exact', pk_val))\\n            else:\\n                record_exists = False\\n        if not pk_set or not record_exists:\\n            field_names = [backend.quote_name(f.column) for f in self._meta.fields if not isinstance(f, AutoField)]\\n            db_values = [f.get_db_prep_save(f.pre_save(self, True)) for f in self._meta.fields if not isinstance(f, AutoField)]\\n            if pk_set:\\n                field_names += [f.column for f in self._meta.fields if isinstance(f, AutoField)]\\n                db_values += [f.get_db_prep_save(f.pre_save(self, True)) for f in self._meta.fields if isinstance(f, AutoField)]\\n            placeholders = ['%s'] * len(field_names)\\n            if self._meta.order_with_respect_to:\\n                field_names.append(backend.quote_name('_order'))\\n                placeholders.append('(SELECT COUNT(*) FROM %s WHERE %s = %%s)' % \\\\n                    (backend.quote_name(self._meta.db_table), backend.quote_name(self._meta.order_with_respect_to.column)))\\n                db_values.append(getattr(self, self._meta.order_with_respect_to.attname))\\n            if db_values:\\n                cursor.execute(\"INSERT INTO %s (%s) VALUES (%s)\" % \\\\n                    (backend.quote_name(self._meta.db_table), ','.join(field_names),\\n                    ','.join(placeholders)), db_values)\\n            else:\\n                cursor.execute(\"INSERT INTO %s (%s) VALUES (%s)\" %\\n                    (backend.quote_name(self._meta.db_table),\\n                     backend.quote_name(self._meta.pk.column),\\n                     backend.get_pk_default_value()))\\n            if self._meta.has_auto_field and not pk_set:\\n                setattr(self, self._meta.pk.attname, backend.get_last_insert_id(cursor, self._meta.db_table, self._meta.pk.column))\\n        transaction.commit_unless_managed()\\n        dispatcher.send(signal=signals.post_save, sender=self.__class__, instance=self)\\n    save.alters_data = True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #4459 -- Added 'raw' argument to save method, to override any pre-save processing, and modified serializers to use a raw-save. This enables serialization of DateFields with auto_now/auto_now_add. Also modified serializers to invoke save() directly on the model baseclass, to avoid any (potentially order-dependent, data modifying) behavior in a custom save() method.",
    "fixed_code": "def save(self, raw=False):\\n        dispatcher.send(signal=signals.pre_save, sender=self.__class__, instance=self)\\n        non_pks = [f for f in self._meta.fields if not f.primary_key]\\n        cursor = connection.cursor()\\n        pk_val = self._get_pk_val()\\n        pk_set = bool(pk_val)\\n        record_exists = True\\n        if pk_set:\\n            cursor.execute(\"SELECT COUNT(*) FROM %s WHERE %s=%%s\" % \\\\n                (backend.quote_name(self._meta.db_table), backend.quote_name(self._meta.pk.column)),\\n                self._meta.pk.get_db_prep_lookup('exact', pk_val))\\n            if cursor.fetchone()[0] > 0:\\n                db_values = [f.get_db_prep_save(raw and getattr(self, f.attname) or f.pre_save(self, False)) for f in non_pks]\\n                if db_values:\\n                    cursor.execute(\"UPDATE %s SET %s WHERE %s=%%s\" % \\\\n                        (backend.quote_name(self._meta.db_table),\\n                        ','.join(['%s=%%s' % backend.quote_name(f.column) for f in non_pks]),\\n                        backend.quote_name(self._meta.pk.column)),\\n                        db_values + self._meta.pk.get_db_prep_lookup('exact', pk_val))\\n            else:\\n                record_exists = False\\n        if not pk_set or not record_exists:\\n            field_names = [backend.quote_name(f.column) for f in self._meta.fields if not isinstance(f, AutoField)]\\n            db_values = [f.get_db_prep_save(raw and getattr(self, f.attname) or f.pre_save(self, True)) for f in self._meta.fields if not isinstance(f, AutoField)]\\n            if pk_set:\\n                field_names += [f.column for f in self._meta.fields if isinstance(f, AutoField)]\\n                db_values += [f.get_db_prep_save(raw and getattr(self, f.attname) or f.pre_save(self, True)) for f in self._meta.fields if isinstance(f, AutoField)]\\n            placeholders = ['%s'] * len(field_names)\\n            if self._meta.order_with_respect_to:\\n                field_names.append(backend.quote_name('_order'))\\n                placeholders.append('(SELECT COUNT(*) FROM %s WHERE %s = %%s)' % \\\\n                    (backend.quote_name(self._meta.db_table), backend.quote_name(self._meta.order_with_respect_to.column)))\\n                db_values.append(getattr(self, self._meta.order_with_respect_to.attname))\\n            if db_values:\\n                cursor.execute(\"INSERT INTO %s (%s) VALUES (%s)\" % \\\\n                    (backend.quote_name(self._meta.db_table), ','.join(field_names),\\n                    ','.join(placeholders)), db_values)\\n            else:\\n                cursor.execute(\"INSERT INTO %s (%s) VALUES (%s)\" %\\n                    (backend.quote_name(self._meta.db_table),\\n                     backend.quote_name(self._meta.pk.column),\\n                     backend.get_pk_default_value()))\\n            if self._meta.has_auto_field and not pk_set:\\n                setattr(self, self._meta.pk.attname, backend.get_last_insert_id(cursor, self._meta.db_table, self._meta.pk.column))\\n        transaction.commit_unless_managed()\\n        dispatcher.send(signal=signals.post_save, sender=self.__class__, instance=self)\\n    save.alters_data = True"
  },
  {
    "code": "def get_loc(self, key, method=None, tolerance=None):\\n\\t\\tif method is None:\\n\\t\\t\\tif tolerance is not None:\\n\\t\\t\\t\\traise ValueError('tolerance argument only valid if using pad, '\\n\\t\\t\\t\\t\\t\\t\\t\\t 'backfill or nearest lookups')\\n\\t\\t\\tkey = _values_from_object(key)\\n\\t\\t\\treturn self._engine.get_loc(key)\\n\\t\\tindexer = self.get_indexer([key], method=method,\\n\\t\\t\\t\\t\\t\\t\\t\\t   tolerance=tolerance)\\n\\t\\tif indexer.ndim > 1 or indexer.size > 1:\\n\\t\\t\\traise TypeError('get_loc requires scalar valued input')\\n\\t\\tloc = indexer.item()\\n\\t\\tif loc == -1:\\n\\t\\t\\traise KeyError(key)\\n\\t\\treturn loc",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_loc(self, key, method=None, tolerance=None):\\n\\t\\tif method is None:\\n\\t\\t\\tif tolerance is not None:\\n\\t\\t\\t\\traise ValueError('tolerance argument only valid if using pad, '\\n\\t\\t\\t\\t\\t\\t\\t\\t 'backfill or nearest lookups')\\n\\t\\t\\tkey = _values_from_object(key)\\n\\t\\t\\treturn self._engine.get_loc(key)\\n\\t\\tindexer = self.get_indexer([key], method=method,\\n\\t\\t\\t\\t\\t\\t\\t\\t   tolerance=tolerance)\\n\\t\\tif indexer.ndim > 1 or indexer.size > 1:\\n\\t\\t\\traise TypeError('get_loc requires scalar valued input')\\n\\t\\tloc = indexer.item()\\n\\t\\tif loc == -1:\\n\\t\\t\\traise KeyError(key)\\n\\t\\treturn loc"
  },
  {
    "code": "def guess_indent(self):\\n        opener, indented = IndentSearcher(self.text, self.tabwidth).run()\\n        if opener and indented:\\n            raw, indentsmall = classifyws(opener, self.tabwidth)\\n            raw, indentlarge = classifyws(indented, self.tabwidth)\\n        else:\\n            indentsmall = indentlarge = 0\\n        return indentlarge - indentsmall",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-36390: simplify classifyws(), rename it and add unit tests (GH-14500)",
    "fixed_code": "def guess_indent(self):\\n        opener, indented = IndentSearcher(self.text, self.tabwidth).run()\\n        if opener and indented:\\n            raw, indentsmall = get_line_indent(opener, self.tabwidth)\\n            raw, indentlarge = get_line_indent(indented, self.tabwidth)\\n        else:\\n            indentsmall = indentlarge = 0\\n        return indentlarge - indentsmall"
  },
  {
    "code": "def get_groupby(\\n    obj: NDFrame,\\n    by: Optional[_KeysArgType] = None,\\n    axis: int = 0,\\n    level=None,\\n    grouper: \"Optional[ops.BaseGrouper]\" = None,\\n    exclusions=None,\\n    selection=None,\\n    as_index: bool = True,\\n    sort: bool = True,\\n    group_keys: bool = True,\\n    squeeze: bool = False,\\n    observed: bool = False,\\n    mutated: bool = False,\\n):\\n    klass: Union[Type[\"SeriesGroupBy\"], Type[\"DataFrameGroupBy\"]]\\n    if isinstance(obj, Series):\\n        from pandas.core.groupby.generic import SeriesGroupBy\\n        klass = SeriesGroupBy\\n    elif isinstance(obj, DataFrame):\\n        from pandas.core.groupby.generic import DataFrameGroupBy\\n        klass = DataFrameGroupBy\\n    else:\\n        raise TypeError(f\"invalid type: {obj}\")\\n    return klass(\\n        obj=obj,\\n        keys=by,\\n        axis=axis,\\n        level=level,\\n        grouper=grouper,\\n        exclusions=exclusions,\\n        selection=selection,\\n        as_index=as_index,\\n        sort=sort,\\n        group_keys=group_keys,\\n        squeeze=squeeze,\\n        observed=observed,\\n        mutated=mutated,\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_groupby(\\n    obj: NDFrame,\\n    by: Optional[_KeysArgType] = None,\\n    axis: int = 0,\\n    level=None,\\n    grouper: \"Optional[ops.BaseGrouper]\" = None,\\n    exclusions=None,\\n    selection=None,\\n    as_index: bool = True,\\n    sort: bool = True,\\n    group_keys: bool = True,\\n    squeeze: bool = False,\\n    observed: bool = False,\\n    mutated: bool = False,\\n):\\n    klass: Union[Type[\"SeriesGroupBy\"], Type[\"DataFrameGroupBy\"]]\\n    if isinstance(obj, Series):\\n        from pandas.core.groupby.generic import SeriesGroupBy\\n        klass = SeriesGroupBy\\n    elif isinstance(obj, DataFrame):\\n        from pandas.core.groupby.generic import DataFrameGroupBy\\n        klass = DataFrameGroupBy\\n    else:\\n        raise TypeError(f\"invalid type: {obj}\")\\n    return klass(\\n        obj=obj,\\n        keys=by,\\n        axis=axis,\\n        level=level,\\n        grouper=grouper,\\n        exclusions=exclusions,\\n        selection=selection,\\n        as_index=as_index,\\n        sort=sort,\\n        group_keys=group_keys,\\n        squeeze=squeeze,\\n        observed=observed,\\n        mutated=mutated,\\n    )"
  },
  {
    "code": "def __init__(self,\\n                 bigquery_conn_id='google_cloud_default',\\n                 delegate_to=None,\\n                 use_legacy_sql=True,\\n                 location=None):\\n        super(BigQueryHook, self).__init__(\\n            gcp_conn_id=bigquery_conn_id, delegate_to=delegate_to)\\n        self.use_legacy_sql = use_legacy_sql\\n        self.location = location\\n        self.num_retries = self._get_field('num_retries', 5)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self,\\n                 bigquery_conn_id='google_cloud_default',\\n                 delegate_to=None,\\n                 use_legacy_sql=True,\\n                 location=None):\\n        super(BigQueryHook, self).__init__(\\n            gcp_conn_id=bigquery_conn_id, delegate_to=delegate_to)\\n        self.use_legacy_sql = use_legacy_sql\\n        self.location = location\\n        self.num_retries = self._get_field('num_retries', 5)"
  },
  {
    "code": "def ogr(self):\\n        \"Returns the OGR Geometry for this Geometry.\"\\n        if gdal.HAS_GDAL:\\n            if self.srid:\\n                return gdal.OGRGeometry(self.wkb, self.srid)\\n            else:\\n                return gdal.OGRGeometry(self.wkb)\\n        else:\\n            raise GEOSException('GDAL required to convert to an OGRGeometry.')\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19171 -- Allowed coordinate transforms with custom SRIDs\\n\\nThanks reidpr at lanl.gov for the report.",
    "fixed_code": "def ogr(self):\\n        \"Returns the OGR Geometry for this Geometry.\"\\n        if not gdal.HAS_GDAL:\\n            raise GEOSException('GDAL required to convert to an OGRGeometry.')\\n        if self.srid:\\n            try:\\n                return gdal.OGRGeometry(self.wkb, self.srid)\\n            except SRSException:\\n                pass\\n        return gdal.OGRGeometry(self.wkb)\\n    @property"
  },
  {
    "code": "def build(self, input_shape):\\n\\t\\tfor cell in self.cells:\\n\\t\\t\\tif isinstance(cell, Layer):\\n\\t\\t\\t\\tcell.build(input_shape)\\n\\t\\t\\tif hasattr(cell.state_size, '__len__'):\\n\\t\\t\\t\\toutput_dim = cell.state_size[0]\\n\\t\\t\\telse:\\n\\t\\t\\t\\toutput_dim = cell.state_size\\n\\t\\t\\tinput_shape = (input_shape[0], output_dim)\\n\\t\\tself.built = True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def build(self, input_shape):\\n\\t\\tfor cell in self.cells:\\n\\t\\t\\tif isinstance(cell, Layer):\\n\\t\\t\\t\\tcell.build(input_shape)\\n\\t\\t\\tif hasattr(cell.state_size, '__len__'):\\n\\t\\t\\t\\toutput_dim = cell.state_size[0]\\n\\t\\t\\telse:\\n\\t\\t\\t\\toutput_dim = cell.state_size\\n\\t\\t\\tinput_shape = (input_shape[0], output_dim)\\n\\t\\tself.built = True"
  },
  {
    "code": "def SetPointerType(pointer, cls):\\n    if _pointer_type_cache.get(cls, None) is not None:\\n        raise RuntimeError, \\\\n              \"This type already exists in the cache\"\\n    if id(pointer) not in _pointer_type_cache:\\n        raise RuntimeError, \\\\n              \"What's this???\"\\n    pointer.set_type(cls)\\n    _pointer_type_cache[cls] = pointer\\n    del _pointer_type_cache[id(pointer)]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Raise statement normalization in Lib/ctypes/.",
    "fixed_code": "def SetPointerType(pointer, cls):\\n    if _pointer_type_cache.get(cls, None) is not None:\\n        raise RuntimeError(\"This type already exists in the cache\")\\n    if id(pointer) not in _pointer_type_cache:\\n        raise RuntimeError(\"What's this???\")\\n    pointer.set_type(cls)\\n    _pointer_type_cache[cls] = pointer\\n    del _pointer_type_cache[id(pointer)]"
  },
  {
    "code": "def _infer_multiple_outputs(self):\\n        try:\\n            return_type = typing_extensions.get_type_hints(self.function).get(\"return\", Any)\\n        except Exception:  \\n            return False\\n        if sys.version_info < (3, 7):\\n            ttype = getattr(return_type, \"__extra__\", return_type)\\n        else:\\n            ttype = getattr(return_type, \"__origin__\", return_type)\\n        return ttype == dict or ttype == Dict",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _infer_multiple_outputs(self):\\n        try:\\n            return_type = typing_extensions.get_type_hints(self.function).get(\"return\", Any)\\n        except Exception:  \\n            return False\\n        if sys.version_info < (3, 7):\\n            ttype = getattr(return_type, \"__extra__\", return_type)\\n        else:\\n            ttype = getattr(return_type, \"__origin__\", return_type)\\n        return ttype == dict or ttype == Dict"
  },
  {
    "code": "def _create_unique_sql(\\n        self, model, fields, name=None, condition=None, deferrable=None,\\n        include=None, opclasses=None, expressions=None,\\n    ):\\n        if (\\n            (\\n                deferrable and\\n                not self.connection.features.supports_deferrable_unique_constraints\\n            ) or\\n            (condition and not self.connection.features.supports_partial_indexes) or\\n            (include and not self.connection.features.supports_covering_indexes) or\\n            (expressions and not self.connection.features.supports_expression_indexes)\\n        ):\\n            return None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_unique_sql(\\n        self, model, fields, name=None, condition=None, deferrable=None,\\n        include=None, opclasses=None, expressions=None,\\n    ):\\n        if (\\n            (\\n                deferrable and\\n                not self.connection.features.supports_deferrable_unique_constraints\\n            ) or\\n            (condition and not self.connection.features.supports_partial_indexes) or\\n            (include and not self.connection.features.supports_covering_indexes) or\\n            (expressions and not self.connection.features.supports_expression_indexes)\\n        ):\\n            return None"
  },
  {
    "code": "def main():\\n\\trhsm = Rhsm(None)\\n\\tmodule = AnsibleModule(\\n\\t\\targument_spec={\\n\\t\\t\\t'state': {'default': 'present', 'choices': ['present', 'absent']},\\n\\t\\t\\t'username': {},\\n\\t\\t\\t'password': {'no_log': True},\\n\\t\\t\\t'server_hostname': {},\\n\\t\\t\\t'server_insecure': {},\\n\\t\\t\\t'rhsm_baseurl': {},\\n\\t\\t\\t'rhsm_repo_ca_cert': {},\\n\\t\\t\\t'auto_attach': {'aliases': ['autosubscribe'], 'type': 'bool'},\\n\\t\\t\\t'activationkey': {'no_log': True},\\n\\t\\t\\t'org_id': {},\\n\\t\\t\\t'environment': {},\\n\\t\\t\\t'pool': {'default': '^$'},\\n\\t\\t\\t'pool_ids': {'default': [], 'type': 'list'},\\n\\t\\t\\t'consumer_type': {},\\n\\t\\t\\t'consumer_name': {},\\n\\t\\t\\t'consumer_id': {},\\n\\t\\t\\t'force_register': {'default': False, 'type': 'bool'},\\n\\t\\t\\t'server_proxy_hostname': {},\\n\\t\\t\\t'server_proxy_port': {},\\n\\t\\t\\t'server_proxy_user': {},\\n\\t\\t\\t'server_proxy_password': {'no_log': True},\\n\\t\\t\\t'release': {},\\n\\t\\t\\t'syspurpose': {\\n\\t\\t\\t\\t'type': 'dict',\\n\\t\\t\\t\\t'options': {\\n\\t\\t\\t\\t\\t'role': {},\\n\\t\\t\\t\\t\\t'usage': {},\\n\\t\\t\\t\\t\\t'service_level_agreement': {},\\n\\t\\t\\t\\t\\t'addons': {'type': 'list'},\\n\\t\\t\\t\\t\\t'sync': {'type': 'bool', 'default': False}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t},\\n\\t\\trequired_together=[['username', 'password'],\\n\\t\\t\\t\\t\\t\\t   ['server_proxy_hostname', 'server_proxy_port'],\\n\\t\\t\\t\\t\\t\\t   ['server_proxy_user', 'server_proxy_password']],\\n\\t\\tmutually_exclusive=[['activationkey', 'username'],\\n\\t\\t\\t\\t\\t\\t\\t['activationkey', 'consumer_id'],\\n\\t\\t\\t\\t\\t\\t\\t['activationkey', 'environment'],\\n\\t\\t\\t\\t\\t\\t\\t['activationkey', 'autosubscribe'],\\n\\t\\t\\t\\t\\t\\t\\t['force', 'consumer_id'],\\n\\t\\t\\t\\t\\t\\t\\t['pool', 'pool_ids']],\\n\\t\\trequired_if=[['state', 'present', ['username', 'activationkey'], True]],\\n\\t)\\n\\trhsm.module = module\\n\\tstate = module.params['state']\\n\\tusername = module.params['username']\\n\\tpassword = module.params['password']\\n\\tserver_hostname = module.params['server_hostname']\\n\\tserver_insecure = module.params['server_insecure']\\n\\trhsm_baseurl = module.params['rhsm_baseurl']\\n\\trhsm_repo_ca_cert = module.params['rhsm_repo_ca_cert']\\n\\tauto_attach = module.params['auto_attach']\\n\\tactivationkey = module.params['activationkey']\\n\\torg_id = module.params['org_id']\\n\\tif activationkey and not org_id:\\n\\t\\tmodule.fail_json(msg='org_id is required when using activationkey')\\n\\tenvironment = module.params['environment']\\n\\tpool = module.params['pool']\\n\\tpool_ids = {}\\n\\tfor value in module.params['pool_ids']:\\n\\t\\tif isinstance(value, dict):\\n\\t\\t\\tif len(value) != 1:\\n\\t\\t\\t\\tmodule.fail_json(msg='Unable to parse pool_ids option.')\\n\\t\\t\\tpool_id, quantity = list(value.items())[0]\\n\\t\\telse:\\n\\t\\t\\tpool_id, quantity = value, 1\\n\\t\\tpool_ids[pool_id] = str(quantity)\\n\\tconsumer_type = module.params[\"consumer_type\"]\\n\\tconsumer_name = module.params[\"consumer_name\"]\\n\\tconsumer_id = module.params[\"consumer_id\"]\\n\\tforce_register = module.params[\"force_register\"]\\n\\tserver_proxy_hostname = module.params['server_proxy_hostname']\\n\\tserver_proxy_port = module.params['server_proxy_port']\\n\\tserver_proxy_user = module.params['server_proxy_user']\\n\\tserver_proxy_password = module.params['server_proxy_password']\\n\\trelease = module.params['release']\\n\\tsyspurpose = module.params['syspurpose']\\n\\tglobal SUBMAN_CMD\\n\\tSUBMAN_CMD = module.get_bin_path('subscription-manager', True)\\n\\tsyspurpose_changed = False\\n\\tif syspurpose is not None:\\n\\t\\ttry:\\n\\t\\t\\tsyspurpose_changed = SysPurpose().update_syspurpose(syspurpose)\\n\\t\\texcept Exception as err:\\n\\t\\t\\tmodule.fail_json(msg=\"Failed to update syspurpose attributes: %s\" % to_native(err))\\n\\tif state == 'present':\\n\\t\\tif rhsm.is_registered and not force_register:\\n\\t\\t\\tif syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\trhsm.sync_syspurpose()\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to synchronize syspurpose attributes: %s\" % to_native(e))\\n\\t\\t\\tif pool != '^$' or pool_ids:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tif pool_ids:\\n\\t\\t\\t\\t\\t\\tresult = rhsm.update_subscriptions_by_pool_ids(pool_ids)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tresult = rhsm.update_subscriptions(pool)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to update subscriptions for '%s': %s\" % (server_hostname, to_native(e)))\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tmodule.exit_json(**result)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif syspurpose_changed is True:\\n\\t\\t\\t\\t\\tmodule.exit_json(changed=True, msg=\"Syspurpose attributes changed.\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tmodule.exit_json(changed=False, msg=\"System already registered.\")\\n\\t\\telse:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\trhsm.enable()\\n\\t\\t\\t\\trhsm.configure(**module.params)\\n\\t\\t\\t\\trhsm.register(username, password, auto_attach, activationkey, org_id,\\n\\t\\t\\t\\t\\t\\t\\t  consumer_type, consumer_name, consumer_id, force_register,\\n\\t\\t\\t\\t\\t\\t\\t  environment, rhsm_baseurl, server_insecure, server_hostname,\\n\\t\\t\\t\\t\\t\\t\\t  server_proxy_hostname, server_proxy_port, server_proxy_user, server_proxy_password, release)\\n\\t\\t\\t\\tif syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:\\n\\t\\t\\t\\t\\trhsm.sync_syspurpose()\\n\\t\\t\\t\\tif pool_ids:\\n\\t\\t\\t\\t\\tsubscribed_pool_ids = rhsm.subscribe_by_pool_ids(pool_ids)\\n\\t\\t\\t\\telif pool != '^$':\\n\\t\\t\\t\\t\\tsubscribed_pool_ids = rhsm.subscribe(pool)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tsubscribed_pool_ids = []\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to register with '%s': %s\" % (server_hostname, to_native(e)))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmodule.exit_json(changed=True,\\n\\t\\t\\t\\t\\t\\t\\t\\t msg=\"System successfully registered to '%s'.\" % server_hostname,\\n\\t\\t\\t\\t\\t\\t\\t\\t subscribed_pool_ids=subscribed_pool_ids)\\n\\tif state == 'absent':\\n\\t\\tif not rhsm.is_registered:\\n\\t\\t\\tmodule.exit_json(changed=False, msg=\"System already unregistered.\")\\n\\t\\telse:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\trhsm.unsubscribe()\\n\\t\\t\\t\\trhsm.unregister()\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to unregister: %s\" % to_native(e))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmodule.exit_json(changed=True, msg=\"System successfully unregistered from %s.\" % server_hostname)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "When no pool quantity is set, then do not set quantity to 1 (#66807)\\n\\n  default value 1. When more quantities are required, then\\n  candlepin server can automatically choose correct minimal\\n  value.",
    "fixed_code": "def main():\\n\\trhsm = Rhsm(None)\\n\\tmodule = AnsibleModule(\\n\\t\\targument_spec={\\n\\t\\t\\t'state': {'default': 'present', 'choices': ['present', 'absent']},\\n\\t\\t\\t'username': {},\\n\\t\\t\\t'password': {'no_log': True},\\n\\t\\t\\t'server_hostname': {},\\n\\t\\t\\t'server_insecure': {},\\n\\t\\t\\t'rhsm_baseurl': {},\\n\\t\\t\\t'rhsm_repo_ca_cert': {},\\n\\t\\t\\t'auto_attach': {'aliases': ['autosubscribe'], 'type': 'bool'},\\n\\t\\t\\t'activationkey': {'no_log': True},\\n\\t\\t\\t'org_id': {},\\n\\t\\t\\t'environment': {},\\n\\t\\t\\t'pool': {'default': '^$'},\\n\\t\\t\\t'pool_ids': {'default': [], 'type': 'list'},\\n\\t\\t\\t'consumer_type': {},\\n\\t\\t\\t'consumer_name': {},\\n\\t\\t\\t'consumer_id': {},\\n\\t\\t\\t'force_register': {'default': False, 'type': 'bool'},\\n\\t\\t\\t'server_proxy_hostname': {},\\n\\t\\t\\t'server_proxy_port': {},\\n\\t\\t\\t'server_proxy_user': {},\\n\\t\\t\\t'server_proxy_password': {'no_log': True},\\n\\t\\t\\t'release': {},\\n\\t\\t\\t'syspurpose': {\\n\\t\\t\\t\\t'type': 'dict',\\n\\t\\t\\t\\t'options': {\\n\\t\\t\\t\\t\\t'role': {},\\n\\t\\t\\t\\t\\t'usage': {},\\n\\t\\t\\t\\t\\t'service_level_agreement': {},\\n\\t\\t\\t\\t\\t'addons': {'type': 'list'},\\n\\t\\t\\t\\t\\t'sync': {'type': 'bool', 'default': False}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t},\\n\\t\\trequired_together=[['username', 'password'],\\n\\t\\t\\t\\t\\t\\t   ['server_proxy_hostname', 'server_proxy_port'],\\n\\t\\t\\t\\t\\t\\t   ['server_proxy_user', 'server_proxy_password']],\\n\\t\\tmutually_exclusive=[['activationkey', 'username'],\\n\\t\\t\\t\\t\\t\\t\\t['activationkey', 'consumer_id'],\\n\\t\\t\\t\\t\\t\\t\\t['activationkey', 'environment'],\\n\\t\\t\\t\\t\\t\\t\\t['activationkey', 'autosubscribe'],\\n\\t\\t\\t\\t\\t\\t\\t['force', 'consumer_id'],\\n\\t\\t\\t\\t\\t\\t\\t['pool', 'pool_ids']],\\n\\t\\trequired_if=[['state', 'present', ['username', 'activationkey'], True]],\\n\\t)\\n\\trhsm.module = module\\n\\tstate = module.params['state']\\n\\tusername = module.params['username']\\n\\tpassword = module.params['password']\\n\\tserver_hostname = module.params['server_hostname']\\n\\tserver_insecure = module.params['server_insecure']\\n\\trhsm_baseurl = module.params['rhsm_baseurl']\\n\\trhsm_repo_ca_cert = module.params['rhsm_repo_ca_cert']\\n\\tauto_attach = module.params['auto_attach']\\n\\tactivationkey = module.params['activationkey']\\n\\torg_id = module.params['org_id']\\n\\tif activationkey and not org_id:\\n\\t\\tmodule.fail_json(msg='org_id is required when using activationkey')\\n\\tenvironment = module.params['environment']\\n\\tpool = module.params['pool']\\n\\tpool_ids = {}\\n\\tfor value in module.params['pool_ids']:\\n\\t\\tif isinstance(value, dict):\\n\\t\\t\\tif len(value) != 1:\\n\\t\\t\\t\\tmodule.fail_json(msg='Unable to parse pool_ids option.')\\n\\t\\t\\tpool_id, quantity = list(value.items())[0]\\n\\t\\telse:\\n\\t\\t\\tpool_id, quantity = value, None\\n\\t\\tpool_ids[pool_id] = quantity\\n\\tconsumer_type = module.params[\"consumer_type\"]\\n\\tconsumer_name = module.params[\"consumer_name\"]\\n\\tconsumer_id = module.params[\"consumer_id\"]\\n\\tforce_register = module.params[\"force_register\"]\\n\\tserver_proxy_hostname = module.params['server_proxy_hostname']\\n\\tserver_proxy_port = module.params['server_proxy_port']\\n\\tserver_proxy_user = module.params['server_proxy_user']\\n\\tserver_proxy_password = module.params['server_proxy_password']\\n\\trelease = module.params['release']\\n\\tsyspurpose = module.params['syspurpose']\\n\\tglobal SUBMAN_CMD\\n\\tSUBMAN_CMD = module.get_bin_path('subscription-manager', True)\\n\\tsyspurpose_changed = False\\n\\tif syspurpose is not None:\\n\\t\\ttry:\\n\\t\\t\\tsyspurpose_changed = SysPurpose().update_syspurpose(syspurpose)\\n\\t\\texcept Exception as err:\\n\\t\\t\\tmodule.fail_json(msg=\"Failed to update syspurpose attributes: %s\" % to_native(err))\\n\\tif state == 'present':\\n\\t\\tif rhsm.is_registered and not force_register:\\n\\t\\t\\tif syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\trhsm.sync_syspurpose()\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to synchronize syspurpose attributes: %s\" % to_native(e))\\n\\t\\t\\tif pool != '^$' or pool_ids:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tif pool_ids:\\n\\t\\t\\t\\t\\t\\tresult = rhsm.update_subscriptions_by_pool_ids(pool_ids)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tresult = rhsm.update_subscriptions(pool)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to update subscriptions for '%s': %s\" % (server_hostname, to_native(e)))\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tmodule.exit_json(**result)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif syspurpose_changed is True:\\n\\t\\t\\t\\t\\tmodule.exit_json(changed=True, msg=\"Syspurpose attributes changed.\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tmodule.exit_json(changed=False, msg=\"System already registered.\")\\n\\t\\telse:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\trhsm.enable()\\n\\t\\t\\t\\trhsm.configure(**module.params)\\n\\t\\t\\t\\trhsm.register(username, password, auto_attach, activationkey, org_id,\\n\\t\\t\\t\\t\\t\\t\\t  consumer_type, consumer_name, consumer_id, force_register,\\n\\t\\t\\t\\t\\t\\t\\t  environment, rhsm_baseurl, server_insecure, server_hostname,\\n\\t\\t\\t\\t\\t\\t\\t  server_proxy_hostname, server_proxy_port, server_proxy_user, server_proxy_password, release)\\n\\t\\t\\t\\tif syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:\\n\\t\\t\\t\\t\\trhsm.sync_syspurpose()\\n\\t\\t\\t\\tif pool_ids:\\n\\t\\t\\t\\t\\tsubscribed_pool_ids = rhsm.subscribe_by_pool_ids(pool_ids)\\n\\t\\t\\t\\telif pool != '^$':\\n\\t\\t\\t\\t\\tsubscribed_pool_ids = rhsm.subscribe(pool)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tsubscribed_pool_ids = []\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to register with '%s': %s\" % (server_hostname, to_native(e)))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmodule.exit_json(changed=True,\\n\\t\\t\\t\\t\\t\\t\\t\\t msg=\"System successfully registered to '%s'.\" % server_hostname,\\n\\t\\t\\t\\t\\t\\t\\t\\t subscribed_pool_ids=subscribed_pool_ids)\\n\\tif state == 'absent':\\n\\t\\tif not rhsm.is_registered:\\n\\t\\t\\tmodule.exit_json(changed=False, msg=\"System already unregistered.\")\\n\\t\\telse:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\trhsm.unsubscribe()\\n\\t\\t\\t\\trhsm.unregister()\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tmodule.fail_json(msg=\"Failed to unregister: %s\" % to_native(e))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmodule.exit_json(changed=True, msg=\"System successfully unregistered from %s.\" % server_hostname)"
  },
  {
    "code": "def run_luks_create(self, device, keyfile, passphrase, keysize):\\n        luks_type = self._module.params['type']\\n        label = self._module.params['label']\\n        options = []\\n        if keysize is not None:\\n            options.append('--key-size=' + str(keysize))\\n        if label is not None:\\n            options.extend(['--label', label])\\n            luks_type = 'luks2'\\n        if luks_type is not None:\\n            options.extend(['--type', luks_type])\\n        args = [self._cryptsetup_bin, 'luksFormat']\\n        args.extend(options)\\n        args.extend(['-q', device])\\n        if keyfile:\\n            args.append(keyfile)\\n        result = self._run_command(args, data=passphrase)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while creating LUKS on %s: %s'\\n                             % (device, result[STDERR]))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run_luks_create(self, device, keyfile, passphrase, keysize):\\n        luks_type = self._module.params['type']\\n        label = self._module.params['label']\\n        options = []\\n        if keysize is not None:\\n            options.append('--key-size=' + str(keysize))\\n        if label is not None:\\n            options.extend(['--label', label])\\n            luks_type = 'luks2'\\n        if luks_type is not None:\\n            options.extend(['--type', luks_type])\\n        args = [self._cryptsetup_bin, 'luksFormat']\\n        args.extend(options)\\n        args.extend(['-q', device])\\n        if keyfile:\\n            args.append(keyfile)\\n        result = self._run_command(args, data=passphrase)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while creating LUKS on %s: %s'\\n                             % (device, result[STDERR]))"
  },
  {
    "code": "def __iter__(self):\\n\\t\\tkeys = list(self._data)\\n\\t\\tfor key in keys:\\n\\t\\t\\tyield self.decodekey(key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __iter__(self):\\n\\t\\tkeys = list(self._data)\\n\\t\\tfor key in keys:\\n\\t\\t\\tyield self.decodekey(key)"
  },
  {
    "code": "def track(self, release_gil=False, nframe=1):\\t\\tframes = get_frames(nframe, 2)\\t\\t_testcapi.tracemalloc_track(self.domain, self.ptr, self.size,\\t\\t\\t\\t\\t\\t\\t\\t\\trelease_gil)\\t\\treturn frames",
    "label": 1,
    "bug_type": "numeric",
    "bug_description": "bpo-12458: Fix line numbers for multiline expressions. (GH-8774)",
    "fixed_code": "def track(self, release_gil=False, nframe=1):\\n\\t\\tframes = get_frames(nframe, 1)\\n\\t\\t_testcapi.tracemalloc_track(self.domain, self.ptr, self.size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\trelease_gil)\\n\\t\\treturn frames"
  },
  {
    "code": "def __pow__(self, other, modulo=None, context=None):\\n        if modulo is not None:\\n            return self._power_modulo(other, modulo, context)\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        if context is None:\\n            context = getcontext()\\n        ans = self._check_nans(other, context)\\n        if ans:\\n            return ans\\n        if not other:\\n            if not self:\\n                return context._raise_error(InvalidOperation, '0 ** 0')\\n            else:\\n                return Dec_p1\\n        result_sign = 0\\n        if self._sign == 1:\\n            if other._isinteger():\\n                if not other._iseven():\\n                    result_sign = 1\\n            else:\\n                if self:\\n                    return context._raise_error(InvalidOperation,\\n                        'x ** y with x negative and y not an integer')\\n            self = self.copy_negate()\\n        if not self:\\n            if other._sign == 0:\\n                return _dec_from_triple(result_sign, '0', 0)\\n            else:\\n                return Infsign[result_sign]\\n        if self._isinfinity():\\n            if other._sign == 0:\\n                return Infsign[result_sign]\\n            else:\\n                return _dec_from_triple(result_sign, '0', 0)\\n        if self == Dec_p1:\\n            if other._isinteger():\\n                if other._sign == 1:\\n                    multiplier = 0\\n                elif other > context.prec:\\n                    multiplier = context.prec\\n                else:\\n                    multiplier = int(other)\\n                exp = self._exp * multiplier\\n                if exp < 1-context.prec:\\n                    exp = 1-context.prec\\n                    context._raise_error(Rounded)\\n            else:\\n                context._raise_error(Inexact)\\n                context._raise_error(Rounded)\\n                exp = 1-context.prec\\n            return _dec_from_triple(result_sign, '1'+'0'*-exp, exp)\\n        self_adj = self.adjusted()\\n        if other._isinfinity():\\n            if (other._sign == 0) == (self_adj < 0):\\n                return _dec_from_triple(result_sign, '0', 0)\\n            else:\\n                return Infsign[result_sign]\\n        ans = None\\n        bound = self._log10_exp_bound() + other.adjusted()\\n        if (self_adj >= 0) == (other._sign == 0):\\n            if bound >= len(str(context.Emax)):\\n                ans = _dec_from_triple(result_sign, '1', context.Emax+1)\\n        else:\\n            Etiny = context.Etiny()\\n            if bound >= len(str(-Etiny)):\\n                ans = _dec_from_triple(result_sign, '1', Etiny-1)\\n        if ans is None:\\n            ans = self._power_exact(other, context.prec + 1)\\n            if ans is not None and result_sign == 1:\\n                ans = _dec_from_triple(1, ans._int, ans._exp)\\n        if ans is None:\\n            p = context.prec\\n            x = _WorkRep(self)\\n            xc, xe = x.int, x.exp\\n            y = _WorkRep(other)\\n            yc, ye = y.int, y.exp\\n            if y.sign == 1:\\n                yc = -yc\\n            extra = 3\\n            while True:\\n                coeff, exp = _dpower(xc, xe, yc, ye, p+extra)\\n                if coeff % (5*10**(len(str(coeff))-p-1)):\\n                    break\\n                extra += 3\\n            ans = _dec_from_triple(result_sign, str(coeff), exp)\\n        if not other._isinteger():\\n            context._raise_error(Inexact)\\n            if len(ans._int) <= context.prec:\\n                expdiff = context.prec+1 - len(ans._int)\\n                ans = _dec_from_triple(ans._sign, ans._int+'0'*expdiff,\\n                                       ans._exp-expdiff)\\n            if ans.adjusted() < context.Emin:\\n                context._raise_error(Underflow)\\n        ans = ans._fix(context)\\n        return ans",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __pow__(self, other, modulo=None, context=None):\\n        if modulo is not None:\\n            return self._power_modulo(other, modulo, context)\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        if context is None:\\n            context = getcontext()\\n        ans = self._check_nans(other, context)\\n        if ans:\\n            return ans\\n        if not other:\\n            if not self:\\n                return context._raise_error(InvalidOperation, '0 ** 0')\\n            else:\\n                return Dec_p1\\n        result_sign = 0\\n        if self._sign == 1:\\n            if other._isinteger():\\n                if not other._iseven():\\n                    result_sign = 1\\n            else:\\n                if self:\\n                    return context._raise_error(InvalidOperation,\\n                        'x ** y with x negative and y not an integer')\\n            self = self.copy_negate()\\n        if not self:\\n            if other._sign == 0:\\n                return _dec_from_triple(result_sign, '0', 0)\\n            else:\\n                return Infsign[result_sign]\\n        if self._isinfinity():\\n            if other._sign == 0:\\n                return Infsign[result_sign]\\n            else:\\n                return _dec_from_triple(result_sign, '0', 0)\\n        if self == Dec_p1:\\n            if other._isinteger():\\n                if other._sign == 1:\\n                    multiplier = 0\\n                elif other > context.prec:\\n                    multiplier = context.prec\\n                else:\\n                    multiplier = int(other)\\n                exp = self._exp * multiplier\\n                if exp < 1-context.prec:\\n                    exp = 1-context.prec\\n                    context._raise_error(Rounded)\\n            else:\\n                context._raise_error(Inexact)\\n                context._raise_error(Rounded)\\n                exp = 1-context.prec\\n            return _dec_from_triple(result_sign, '1'+'0'*-exp, exp)\\n        self_adj = self.adjusted()\\n        if other._isinfinity():\\n            if (other._sign == 0) == (self_adj < 0):\\n                return _dec_from_triple(result_sign, '0', 0)\\n            else:\\n                return Infsign[result_sign]\\n        ans = None\\n        bound = self._log10_exp_bound() + other.adjusted()\\n        if (self_adj >= 0) == (other._sign == 0):\\n            if bound >= len(str(context.Emax)):\\n                ans = _dec_from_triple(result_sign, '1', context.Emax+1)\\n        else:\\n            Etiny = context.Etiny()\\n            if bound >= len(str(-Etiny)):\\n                ans = _dec_from_triple(result_sign, '1', Etiny-1)\\n        if ans is None:\\n            ans = self._power_exact(other, context.prec + 1)\\n            if ans is not None and result_sign == 1:\\n                ans = _dec_from_triple(1, ans._int, ans._exp)\\n        if ans is None:\\n            p = context.prec\\n            x = _WorkRep(self)\\n            xc, xe = x.int, x.exp\\n            y = _WorkRep(other)\\n            yc, ye = y.int, y.exp\\n            if y.sign == 1:\\n                yc = -yc\\n            extra = 3\\n            while True:\\n                coeff, exp = _dpower(xc, xe, yc, ye, p+extra)\\n                if coeff % (5*10**(len(str(coeff))-p-1)):\\n                    break\\n                extra += 3\\n            ans = _dec_from_triple(result_sign, str(coeff), exp)\\n        if not other._isinteger():\\n            context._raise_error(Inexact)\\n            if len(ans._int) <= context.prec:\\n                expdiff = context.prec+1 - len(ans._int)\\n                ans = _dec_from_triple(ans._sign, ans._int+'0'*expdiff,\\n                                       ans._exp-expdiff)\\n            if ans.adjusted() < context.Emin:\\n                context._raise_error(Underflow)\\n        ans = ans._fix(context)\\n        return ans"
  },
  {
    "code": "def timesince(d, now=None, reversed=False, time_strings=None, depth=2):\\n    if time_strings is None:\\n        time_strings = TIME_STRINGS\\n    if depth <= 0:\\n        raise ValueError('depth must be greater than 0.')\\n    if not isinstance(d, datetime.datetime):\\n        d = datetime.datetime(d.year, d.month, d.day)\\n    if now and not isinstance(now, datetime.datetime):\\n        now = datetime.datetime(now.year, now.month, now.day)\\n    now = now or datetime.datetime.now(utc if is_aware(d) else None)\\n    if reversed:\\n        d, now = now, d\\n    delta = now - d\\n    leapdays = calendar.leapdays(d.year, now.year)\\n    if leapdays != 0:\\n        if calendar.isleap(d.year):\\n            leapdays -= 1\\n        elif calendar.isleap(now.year):\\n            leapdays += 1\\n    delta -= datetime.timedelta(leapdays)\\n    since = delta.days * 24 * 60 * 60 + delta.seconds\\n    if since <= 0:\\n        return avoid_wrapping(time_strings['minute'] % {'num': 0})\\n    for i, (seconds, name) in enumerate(TIMESINCE_CHUNKS):\\n        count = since // seconds\\n        if count != 0:\\n            break\\n    else:\\n        return avoid_wrapping(time_strings['minute'] % {'num': 0})\\n    result = []\\n    current_depth = 0\\n    while i < len(TIMESINCE_CHUNKS) and current_depth < depth:\\n        seconds, name = TIMESINCE_CHUNKS[i]\\n        count = since // seconds\\n        if count == 0:\\n            break\\n        result.append(avoid_wrapping(time_strings[name] % {'num': count}))\\n        since -= seconds * count\\n        current_depth += 1\\n        i += 1\\n    return gettext(', ').join(result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def timesince(d, now=None, reversed=False, time_strings=None, depth=2):\\n    if time_strings is None:\\n        time_strings = TIME_STRINGS\\n    if depth <= 0:\\n        raise ValueError('depth must be greater than 0.')\\n    if not isinstance(d, datetime.datetime):\\n        d = datetime.datetime(d.year, d.month, d.day)\\n    if now and not isinstance(now, datetime.datetime):\\n        now = datetime.datetime(now.year, now.month, now.day)\\n    now = now or datetime.datetime.now(utc if is_aware(d) else None)\\n    if reversed:\\n        d, now = now, d\\n    delta = now - d\\n    leapdays = calendar.leapdays(d.year, now.year)\\n    if leapdays != 0:\\n        if calendar.isleap(d.year):\\n            leapdays -= 1\\n        elif calendar.isleap(now.year):\\n            leapdays += 1\\n    delta -= datetime.timedelta(leapdays)\\n    since = delta.days * 24 * 60 * 60 + delta.seconds\\n    if since <= 0:\\n        return avoid_wrapping(time_strings['minute'] % {'num': 0})\\n    for i, (seconds, name) in enumerate(TIMESINCE_CHUNKS):\\n        count = since // seconds\\n        if count != 0:\\n            break\\n    else:\\n        return avoid_wrapping(time_strings['minute'] % {'num': 0})\\n    result = []\\n    current_depth = 0\\n    while i < len(TIMESINCE_CHUNKS) and current_depth < depth:\\n        seconds, name = TIMESINCE_CHUNKS[i]\\n        count = since // seconds\\n        if count == 0:\\n            break\\n        result.append(avoid_wrapping(time_strings[name] % {'num': count}))\\n        since -= seconds * count\\n        current_depth += 1\\n        i += 1\\n    return gettext(', ').join(result)"
  },
  {
    "code": "def cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\\n\\tif axis is not None:\\n\\t\\taxisa, axisb, axisc=(axis,)*3\\n\\ta = asarray(a).swapaxes(axisa, 0)\\n\\tb = asarray(b).swapaxes(axisb, 0)\\n\\tmsg = \"incompatible dimensions for cross product\\n\"\\\\n\\t\\t  \"(dimension must be 2 or 3)\"\\n\\tif (a.shape[0] not in [2, 3]) or (b.shape[0] not in [2, 3]):\\n\\t\\traise ValueError(msg)\\n\\tif a.shape[0] == 2:\\n\\t\\tif (b.shape[0] == 2):\\n\\t\\t\\tcp = a[0]*b[1] - a[1]*b[0]\\n\\t\\t\\tif cp.ndim == 0:\\n\\t\\t\\t\\treturn cp\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn cp.swapaxes(0, axisc)\\n\\t\\telse:\\n\\t\\t\\tx = a[1]*b[2]\\n\\t\\t\\ty = -a[0]*b[2]\\n\\t\\t\\tz = a[0]*b[1] - a[1]*b[0]\\n\\telif a.shape[0] == 3:\\n\\t\\tif (b.shape[0] == 3):\\n\\t\\t\\tx = a[1]*b[2] - a[2]*b[1]\\n\\t\\t\\ty = a[2]*b[0] - a[0]*b[2]\\n\\t\\t\\tz = a[0]*b[1] - a[1]*b[0]\\n\\t\\telse:\\n\\t\\t\\tx = -a[2]*b[1]\\n\\t\\t\\ty = a[2]*b[0]\\n\\t\\t\\tz = a[0]*b[1] - a[1]*b[0]\\n\\tcp = array([x, y, z])\\n\\tif cp.ndim == 1:\\n\\t\\treturn cp\\n\\telse:\\n\\t\\treturn cp.swapaxes(0, axisc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix broadcasting in np.cross (solves #2624)\\n\\nChanged np.cross to move the axis it's operating on to the end, not the\\nbeginning of the shape tuple, and used rollaxis, not swapaxes, so that\\nnormal broadcasting rules apply.\\nThere were no tests in place, so I added some validation of results and\\nbroadcasted shapes, including the one reported in #2624",
    "fixed_code": "def cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\\n\\tif axis is not None:\\n\\t\\taxisa, axisb, axisc = (axis,) * 3\\n\\ta = asarray(a)\\n\\tb = asarray(b)\\n\\ta = rollaxis(a, axisa, a.ndim)\\n\\tb = rollaxis(b, axisb, b.ndim)\\n\\tmsg = (\"incompatible dimensions for cross product\\n\"\\n\\t\\t   \"(dimension must be 2 or 3)\")\\n\\tif a.shape[-1] not in [2, 3] or b.shape[-1] not in [2, 3]:\\n\\t\\traise ValueError(msg)\\n\\tshape = broadcast(a[..., 0], b[..., 0]).shape\\n\\tif a.shape[-1] == 3 or b.shape[-1] == 3:\\n\\t\\tshape += (3,)\\n\\tdtype = promote_types(a.dtype, b.dtype)\\n\\tcp = empty(shape, dtype)\\n\\tif a.shape[-1] == 2:\\n\\t\\tif b.shape[-1] == 2:\\n\\t\\t\\tmultiply(a[..., 0], b[..., 1], out=cp)\\n\\t\\t\\tcp -= a[..., 1]*b[..., 0]\\n\\t\\t\\tif cp.ndim == 0:\\n\\t\\t\\t\\treturn cp\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn rollaxis(cp, -1, axisc)\\n\\t\\telse:\\n\\t\\t\\tmultiply(a[..., 1], b[..., 2], out=cp[..., 0])\\n\\t\\t\\tmultiply(a[..., 0], b[..., 2], out=cp[..., 1])\\n\\t\\t\\tcp[..., 1] *= - 1\\n\\t\\t\\tmultiply(a[..., 0], b[..., 1], out=cp[..., 2])\\n\\t\\t\\tcp[..., 2] -= a[..., 1]*b[..., 0]\\n\\telif a.shape[-1] == 3:\\n\\t\\tif b.shape[-1] == 3:\\n\\t\\t\\tmultiply(a[..., 1], b[..., 2], out=cp[..., 0])\\n\\t\\t\\tcp[..., 0] -= a[..., 2]*b[..., 1]\\n\\t\\t\\tmultiply(a[..., 2], b[..., 0], out=cp[..., 1])\\n\\t\\t\\tcp[..., 1] -= a[..., 0]*b[..., 2]\\n\\t\\t\\tmultiply(a[..., 0], b[..., 1], out=cp[..., 2])\\n\\t\\t\\tcp[..., 2] -= a[..., 1]*b[..., 0]\\n\\t\\telse:\\n\\t\\t\\tmultiply(a[..., 2], b[..., 1], out=cp[..., 0])\\n\\t\\t\\tcp[..., 0] *= - 1\\n\\t\\t\\tmultiply(a[..., 2], b[..., 0], out=cp[..., 1])\\n\\t\\t\\tmultiply(a[..., 0], b[..., 1], out=cp[..., 2])\\n\\t\\t\\tcp[..., 2] -= a[..., 1]*b[..., 0]\\n\\tif cp.ndim == 1:\\n\\t\\treturn cp\\n\\telse:\\n\\t\\treturn rollaxis(cp, -1, axisc)"
  },
  {
    "code": "def _get_new_index(self):\\n        ax = self.ax\\n        ax_attrs = ax._get_attributes_dict()\\n        ax_attrs['freq'] = self.freq\\n        obj = self._selected_obj\\n        if len(ax) == 0:\\n            new_index = PeriodIndex(data=[], **ax_attrs)\\n            return obj.reindex(new_index)\\n        start = ax[0].asfreq(self.freq, how=self.convention)\\n        end = ax[-1].asfreq(self.freq, how='end')\\n        return period_range(start, end, **ax_attrs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: PeriodIndex count & resample-on-same-freq fix\\n\\ncloses #12774\\ncloses #12868\\ncloses #12770\\ncloses #12874",
    "fixed_code": "def _get_new_index(self):\\n        ax = self.ax\\n        if len(ax) == 0:\\n            values = []\\n        else:\\n            start = ax[0].asfreq(self.freq, how=self.convention)\\n            end = ax[-1].asfreq(self.freq, how='end')\\n            values = period_range(start, end, freq=self.freq).values\\n        return ax._shallow_copy(values, freq=self.freq)"
  },
  {
    "code": "class RedirectAgent:\\n\\t_redirectResponses = [\\n\\t\\thttp.MOVED_PERMANENTLY,\\n\\t\\thttp.FOUND,\\n\\t\\thttp.TEMPORARY_REDIRECT,\\n\\t\\thttp.PERMANENT_REDIRECT,\\n\\t]\\n\\t_seeOtherResponses = [http.SEE_OTHER]\\n\\tdef __init__(\\n\\t\\tself,\\n\\t\\tagent: IAgent,\\n\\t\\tredirectLimit: int = 20,\\n\\t\\tsensitiveHeaderNames: Iterable[bytes] = (),\\n\\t):\\n\\t\\tself._agent = agent\\n\\t\\tself._redirectLimit = redirectLimit\\n\\t\\tsensitive = set(_canonicalHeaderName(each) for each in sensitiveHeaderNames)\\n\\t\\tsensitive.update(_defaultSensitiveHeaders)\\n\\t\\tself._sensitiveHeaderNames = sensitive",
    "label": 1,
    "bug_type": "security",
    "bug_description": "reblackening",
    "fixed_code": "class RedirectAgent:\\n\\t_redirectResponses = [\\n\\t\\thttp.MOVED_PERMANENTLY,\\n\\t\\thttp.FOUND,\\n\\t\\thttp.TEMPORARY_REDIRECT,\\n\\t\\thttp.PERMANENT_REDIRECT,\\n\\t]\\n\\t_seeOtherResponses = [http.SEE_OTHER]\\n\\tdef __init__(\\n\\t\\tself,\\n\\t\\tagent: IAgent,\\n\\t\\tredirectLimit: int = 20,\\n\\t\\tsensitiveHeaderNames: Iterable[bytes] = (),\\n\\t):\\n\\t\\tself._agent = agent\\n\\t\\tself._redirectLimit = redirectLimit\\n\\t\\tsensitive = {_canonicalHeaderName(each) for each in sensitiveHeaderNames}\\n\\t\\tsensitive.update(_defaultSensitiveHeaders)\\n\\t\\tself._sensitiveHeaderNames = sensitive"
  },
  {
    "code": "def ip_address_validators(protocol, unpack_ipv4):\\n\\tif protocol != 'both' and unpack_ipv4:\\n\\t\\traise ValueError(\\n\\t\\t\\t\"You can only use `unpack_ipv4` if `protocol` is set to 'both'\")\\n\\ttry:\\n\\t\\treturn ip_address_validator_map[protocol.lower()]\\n\\texcept KeyError:\\n\\t\\traise ValueError(\"The protocol '%s' is unknown. Supported: %s\"\\n\\t\\t\\t\\t\\t\\t % (protocol, ip_address_validator_map.keys()))\\ncomma_separated_int_list_re = re.compile('^[\\d,]+$')\\nvalidate_comma_separated_integer_list = RegexValidator(comma_separated_int_list_re, _(u'Enter only digits separated by commas.'), 'invalid')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[1.4.x] Prevented newlines from being accepted in some validators.\\n\\nThis is a security fix; disclosure to follow shortly.\\n\\nThanks to Sjoerd Job Postmus for the report and draft patch.",
    "fixed_code": "def ip_address_validators(protocol, unpack_ipv4):\\n\\tif protocol != 'both' and unpack_ipv4:\\n\\t\\traise ValueError(\\n\\t\\t\\t\"You can only use `unpack_ipv4` if `protocol` is set to 'both'\")\\n\\ttry:\\n\\t\\treturn ip_address_validator_map[protocol.lower()]\\n\\texcept KeyError:\\n\\t\\traise ValueError(\"The protocol '%s' is unknown. Supported: %s\"\\n\\t\\t\\t\\t\\t\\t % (protocol, ip_address_validator_map.keys()))\\ncomma_separated_int_list_re = re.compile('^[\\d,]+\\Z')\\nvalidate_comma_separated_integer_list = RegexValidator(comma_separated_int_list_re, _(u'Enter only digits separated by commas.'), 'invalid')"
  },
  {
    "code": "def password_reset_confirm(request, uidb64=None, token=None,\\n\\t\\t\\t\\t\\t\\t   template_name='registration/password_reset_confirm.html',\\n\\t\\t\\t\\t\\t\\t   token_generator=default_token_generator,\\n\\t\\t\\t\\t\\t\\t   set_password_form=SetPasswordForm,\\n\\t\\t\\t\\t\\t\\t   post_reset_redirect=None,\\n\\t\\t\\t\\t\\t\\t   current_app=None, extra_context=None):\\n\\tUserModel = get_user_model()\\n\\tassert uidb64 is not None and token is not None  \\n\\tif post_reset_redirect is None:\\n\\t\\tpost_reset_redirect = reverse('password_reset_complete')\\n\\telse:\\n\\t\\tpost_reset_redirect = resolve_url(post_reset_redirect)\\n\\ttry:\\n\\t\\tuid = urlsafe_base64_decode(uidb64)\\n\\t\\tuser = UserModel._default_manager.get(pk=uid)\\n\\texcept (TypeError, ValueError, OverflowError, UserModel.DoesNotExist):\\n\\t\\tuser = None\\n\\tif user is not None and token_generator.check_token(user, token):\\n\\t\\tvalidlink = True\\n\\t\\ttitle = _('Enter new password')\\n\\t\\tif request.method == 'POST':\\n\\t\\t\\tform = set_password_form(user, request.POST)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\tform.save()\\n\\t\\t\\t\\treturn HttpResponseRedirect(post_reset_redirect)\\n\\t\\telse:\\n\\t\\t\\tform = set_password_form(user)\\n\\telse:\\n\\t\\tvalidlink = False\\n\\t\\tform = None\\n\\t\\ttitle = _('Password reset unsuccessful')\\n\\tcontext = {\\n\\t\\t'form': form,\\n\\t\\t'title': title,\\n\\t\\t'validlink': validlink,\\n\\t}\\n\\tif extra_context is not None:\\n\\t\\tcontext.update(extra_context)\\n\\tif current_app is not None:\\n\\t\\trequest.current_app = current_app\\n\\treturn TemplateResponse(request, template_name, context)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #24315 -- Fixed auth.views.password_reset_confirm() with a UUID user.",
    "fixed_code": "def password_reset_confirm(request, uidb64=None, token=None,\\n\\t\\t\\t\\t\\t\\t   template_name='registration/password_reset_confirm.html',\\n\\t\\t\\t\\t\\t\\t   token_generator=default_token_generator,\\n\\t\\t\\t\\t\\t\\t   set_password_form=SetPasswordForm,\\n\\t\\t\\t\\t\\t\\t   post_reset_redirect=None,\\n\\t\\t\\t\\t\\t\\t   current_app=None, extra_context=None):\\n\\tUserModel = get_user_model()\\n\\tassert uidb64 is not None and token is not None  \\n\\tif post_reset_redirect is None:\\n\\t\\tpost_reset_redirect = reverse('password_reset_complete')\\n\\telse:\\n\\t\\tpost_reset_redirect = resolve_url(post_reset_redirect)\\n\\ttry:\\n\\t\\tuid = force_text(urlsafe_base64_decode(uidb64))\\n\\t\\tuser = UserModel._default_manager.get(pk=uid)\\n\\texcept (TypeError, ValueError, OverflowError, UserModel.DoesNotExist):\\n\\t\\tuser = None\\n\\tif user is not None and token_generator.check_token(user, token):\\n\\t\\tvalidlink = True\\n\\t\\ttitle = _('Enter new password')\\n\\t\\tif request.method == 'POST':\\n\\t\\t\\tform = set_password_form(user, request.POST)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\tform.save()\\n\\t\\t\\t\\treturn HttpResponseRedirect(post_reset_redirect)\\n\\t\\telse:\\n\\t\\t\\tform = set_password_form(user)\\n\\telse:\\n\\t\\tvalidlink = False\\n\\t\\tform = None\\n\\t\\ttitle = _('Password reset unsuccessful')\\n\\tcontext = {\\n\\t\\t'form': form,\\n\\t\\t'title': title,\\n\\t\\t'validlink': validlink,\\n\\t}\\n\\tif extra_context is not None:\\n\\t\\tcontext.update(extra_context)\\n\\tif current_app is not None:\\n\\t\\trequest.current_app = current_app\\n\\treturn TemplateResponse(request, template_name, context)"
  },
  {
    "code": "def load_lookups_data(lang, tables):\\n\\tutil.logger.debug(f\"Loading lookups from spacy-lookups-data: {tables}\")\\n\\tlookups = load_lookups(lang=lang, tables=tables)\\n\\treturn lookups",
    "label": 1,
    "bug_type": "perf",
    "bug_description": "Have logging calls use string formatting types (#12215)",
    "fixed_code": "def load_lookups_data(lang, tables):\\n\\tutil.logger.debug(\"Loading lookups from spacy-lookups-data: %s\", tables)\\n\\tlookups = load_lookups(lang=lang, tables=tables)\\n\\treturn lookups"
  },
  {
    "code": "def get_device_facts(self):\\n        device_facts = {}\\n        device_facts['devices'] = {}\\n        lspci = self.module.get_bin_path('lspci')\\n        if lspci:\\n            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\\n        else:\\n            pcidata = None\\n        try:\\n            block_devs = os.listdir(\"/sys/block\")\\n        except OSError:\\n            return device_facts\\n        devs_wwn = {}\\n        try:\\n            devs_by_id = os.listdir(\"/dev/disk/by-id\")\\n        except OSError:\\n            pass\\n        else:\\n            for link_name in devs_by_id:\\n                if link_name.startswith(\"wwn-\"):\\n                    try:\\n                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\\n                    except OSError:\\n                        continue\\n                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\\n        links = self.get_all_device_links()\\n        device_facts['device_links'] = links\\n        for block in block_devs:\\n            virtual = 1\\n            sysfs_no_links = 0\\n            try:\\n                path = os.readlink(os.path.join(\"/sys/block/\", block))\\n            except OSError:\\n                e = sys.exc_info()[1]\\n                if e.errno == errno.EINVAL:\\n                    path = block\\n                    sysfs_no_links = 1\\n                else:\\n                    continue\\n            sysdir = os.path.join(\"/sys/block\", path)\\n            if sysfs_no_links == 1:\\n                for folder in os.listdir(sysdir):\\n                    if \"device\" in folder:\\n                        virtual = 0\\n                        break\\n            d = {}\\n            d['virtual'] = virtual\\n            d['links'] = {}\\n            for (link_type, link_values) in iteritems(links):\\n                d['links'][link_type] = link_values.get(block, [])\\n            diskname = os.path.basename(sysdir)\\n            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\\n                d[key] = get_file_content(sysdir + \"/device/\" + key)\\n            sg_inq = self.module.get_bin_path('sg_inq')\\n            serial_path = \"/sys/block/%s/device/serial\" % (block)\\n            if sg_inq:\\n                device = \"/dev/%s\" % (block)\\n                rc, drivedata, err = self.module.run_command([sg_inq, device])\\n                if rc == 0:\\n                    serial = re.search(r\"Unit serial number:\\s+(\\w+)\", drivedata)\\n                    if serial:\\n                        d['serial'] = serial.group(1)\\n            else:\\n                serial = get_file_content(serial_path)\\n                if serial:\\n                    d['serial'] = serial\\n            for key, test in [('removable', '/removable'),\\n                              ('support_discard', '/queue/discard_granularity'),\\n                              ]:\\n                d[key] = get_file_content(sysdir + test)\\n            if diskname in devs_wwn:\\n                d['wwn'] = devs_wwn[diskname]\\n            d['partitions'] = {}\\n            for folder in os.listdir(sysdir):\\n                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\\n                if m:\\n                    part = {}\\n                    partname = m.group(1)\\n                    part_sysdir = sysdir + \"/\" + partname\\n                    part['links'] = {}\\n                    for (link_type, link_values) in iteritems(links):\\n                        part['links'][link_type] = link_values.get(partname, [])\\n                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\\n                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\\n                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\\n                    if not part['sectorsize']:\\n                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\\n                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\\n                    part['uuid'] = get_partition_uuid(partname)\\n                    self.get_holders(part, part_sysdir)\\n                    d['partitions'][partname] = part\\n            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\\n            d['scheduler_mode'] = \"\"\\n            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\\n            if scheduler is not None:\\n                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\\n                if m:\\n                    d['scheduler_mode'] = m.group(2)\\n            d['sectors'] = get_file_content(sysdir + \"/size\")\\n            if not d['sectors']:\\n                d['sectors'] = 0\\n            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\\n            if not d['sectorsize']:\\n                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\\n            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\\n            d['host'] = \"\"\\n            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\\n            if m and pcidata:\\n                pciid = m.group(1)\\n                did = re.escape(pciid)\\n                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\\n                if m:\\n                    d['host'] = m.group(1)\\n            self.get_holders(d, sysdir)\\n            device_facts['devices'][diskname] = d\\n        return device_facts",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix parsing of device serial number for RHEL8 (#75876)",
    "fixed_code": "def get_device_facts(self):\\n        device_facts = {}\\n        device_facts['devices'] = {}\\n        lspci = self.module.get_bin_path('lspci')\\n        if lspci:\\n            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\\n        else:\\n            pcidata = None\\n        try:\\n            block_devs = os.listdir(\"/sys/block\")\\n        except OSError:\\n            return device_facts\\n        devs_wwn = {}\\n        try:\\n            devs_by_id = os.listdir(\"/dev/disk/by-id\")\\n        except OSError:\\n            pass\\n        else:\\n            for link_name in devs_by_id:\\n                if link_name.startswith(\"wwn-\"):\\n                    try:\\n                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\\n                    except OSError:\\n                        continue\\n                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\\n        links = self.get_all_device_links()\\n        device_facts['device_links'] = links\\n        for block in block_devs:\\n            virtual = 1\\n            sysfs_no_links = 0\\n            try:\\n                path = os.readlink(os.path.join(\"/sys/block/\", block))\\n            except OSError:\\n                e = sys.exc_info()[1]\\n                if e.errno == errno.EINVAL:\\n                    path = block\\n                    sysfs_no_links = 1\\n                else:\\n                    continue\\n            sysdir = os.path.join(\"/sys/block\", path)\\n            if sysfs_no_links == 1:\\n                for folder in os.listdir(sysdir):\\n                    if \"device\" in folder:\\n                        virtual = 0\\n                        break\\n            d = {}\\n            d['virtual'] = virtual\\n            d['links'] = {}\\n            for (link_type, link_values) in iteritems(links):\\n                d['links'][link_type] = link_values.get(block, [])\\n            diskname = os.path.basename(sysdir)\\n            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\\n                d[key] = get_file_content(sysdir + \"/device/\" + key)\\n            sg_inq = self.module.get_bin_path('sg_inq')\\n            serial_path = \"/sys/block/%s/device/serial\" % (block)\\n            if sg_inq:\\n                serial = self._get_sg_inq_serial(sg_inq, block)\\n                if serial:\\n                    d['serial'] = serial\\n            else:\\n                serial = get_file_content(serial_path)\\n                if serial:\\n                    d['serial'] = serial\\n            for key, test in [('removable', '/removable'),\\n                              ('support_discard', '/queue/discard_granularity'),\\n                              ]:\\n                d[key] = get_file_content(sysdir + test)\\n            if diskname in devs_wwn:\\n                d['wwn'] = devs_wwn[diskname]\\n            d['partitions'] = {}\\n            for folder in os.listdir(sysdir):\\n                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\\n                if m:\\n                    part = {}\\n                    partname = m.group(1)\\n                    part_sysdir = sysdir + \"/\" + partname\\n                    part['links'] = {}\\n                    for (link_type, link_values) in iteritems(links):\\n                        part['links'][link_type] = link_values.get(partname, [])\\n                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\\n                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\\n                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\\n                    if not part['sectorsize']:\\n                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\\n                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\\n                    part['uuid'] = get_partition_uuid(partname)\\n                    self.get_holders(part, part_sysdir)\\n                    d['partitions'][partname] = part\\n            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\\n            d['scheduler_mode'] = \"\"\\n            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\\n            if scheduler is not None:\\n                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\\n                if m:\\n                    d['scheduler_mode'] = m.group(2)\\n            d['sectors'] = get_file_content(sysdir + \"/size\")\\n            if not d['sectors']:\\n                d['sectors'] = 0\\n            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\\n            if not d['sectorsize']:\\n                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\\n            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\\n            d['host'] = \"\"\\n            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\\n            if m and pcidata:\\n                pciid = m.group(1)\\n                did = re.escape(pciid)\\n                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\\n                if m:\\n                    d['host'] = m.group(1)\\n            self.get_holders(d, sysdir)\\n            device_facts['devices'][diskname] = d\\n        return device_facts"
  },
  {
    "code": "def process_response(self, request, response, spider):\\n\\t\\tif (request.meta.get('dont_redirect', False) or\\n\\t\\t\\t\\tresponse.status in getattr(spider, 'handle_httpstatus_list', []) or\\n\\t\\t\\t\\tresponse.status in request.meta.get('handle_httpstatus_list', []) or\\n\\t\\t\\t\\trequest.meta.get('handle_httpstatus_all', False)):\\n\\t\\t\\treturn response\\n\\t\\tallowed_status = (301, 302, 303, 307)\\n\\t\\tif 'Location' not in response.headers or response.status not in allowed_status:\\n\\t\\t\\treturn response\\n\\t\\tlocation = to_native_str(response.headers['location'].decode('latin1'))\\n\\t\\tredirected_url = urljoin(request.url, location)\\n\\t\\tif response.status in (301, 307) or request.method == 'HEAD':\\n\\t\\t\\tredirected = request.replace(url=redirected_url)\\n\\t\\t\\treturn self._redirect(redirected, request, spider, response.status)\\n\\t\\tredirected = self._redirect_request_using_get(request, redirected_url)\\n\\t\\treturn self._redirect(redirected, request, spider, response.status)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not interpret non-ASCII bytes in \"Location\" and percent-encode them (#2322)\\n\\n\\nFixes GH-2321\\n\\nThe idea is to not guess the encoding of \"Location\" header value\\nand simply percent-encode non-ASCII bytes,\\nwhich should then be re-interpreted correctly by the remote website\\nin whatever encoding was used originally.\\n\\n\\nThis is similar to the changes to safe_url_string in",
    "fixed_code": "def process_response(self, request, response, spider):\\n\\t\\tif (request.meta.get('dont_redirect', False) or\\n\\t\\t\\t\\tresponse.status in getattr(spider, 'handle_httpstatus_list', []) or\\n\\t\\t\\t\\tresponse.status in request.meta.get('handle_httpstatus_list', []) or\\n\\t\\t\\t\\trequest.meta.get('handle_httpstatus_all', False)):\\n\\t\\t\\treturn response\\n\\t\\tallowed_status = (301, 302, 303, 307)\\n\\t\\tif 'Location' not in response.headers or response.status not in allowed_status:\\n\\t\\t\\treturn response\\n\\t\\tlocation = safe_url_string(response.headers['location'])\\n\\t\\tredirected_url = urljoin(request.url, location)\\n\\t\\tif response.status in (301, 307) or request.method == 'HEAD':\\n\\t\\t\\tredirected = request.replace(url=redirected_url)\\n\\t\\t\\treturn self._redirect(redirected, request, spider, response.status)\\n\\t\\tredirected = self._redirect_request_using_get(request, redirected_url)\\n\\t\\treturn self._redirect(redirected, request, spider, response.status)"
  },
  {
    "code": "def mark_success_url(self):\\n        iso = quote(self.execution_date.isoformat())\\n        base_url = conf.get('webserver', 'BASE_URL')\\n        return base_url + (\\n            \"/confirm\"\\n            f\"?task_id={self.task_id}\"\\n            f\"&dag_id={self.dag_id}\"\\n            f\"&execution_date={iso}\"\\n            \"&upstream=false\"\\n            \"&downstream=false\"\\n            \"&state=success\"\\n        )\\n    @provide_session",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use run_id for ti.mark_success_url (#23330)",
    "fixed_code": "def mark_success_url(self):\\n        base_url = conf.get('webserver', 'BASE_URL')\\n        return base_url + (\\n            \"/confirm\"\\n            f\"?task_id={self.task_id}\"\\n            f\"&dag_id={self.dag_id}\"\\n            f\"&dag_run_id={quote(self.run_id)}\"\\n            \"&upstream=false\"\\n            \"&downstream=false\"\\n            \"&state=success\"\\n        )\\n    @provide_session"
  },
  {
    "code": "def step(self, closure):\\n\\t\\tassert len(self.param_groups) == 1\\n\\t\\tgroup = self.param_groups[0]\\n\\t\\tlr = group['lr']\\n\\t\\tmax_iter = group['max_iter']\\n\\t\\tmax_eval = group['max_eval']\\n\\t\\ttolerance_grad = group['tolerance_grad']\\n\\t\\ttolerance_change = group['tolerance_change']\\n\\t\\tline_search_fn = group['line_search_fn']\\n\\t\\thistory_size = group['history_size']\\n\\t\\tstate = self.state[self._params[0]]\\n\\t\\tstate.setdefault('func_evals', 0)\\n\\t\\tstate.setdefault('n_iter', 0)\\n\\t\\torig_loss = closure()\\n\\t\\tloss = float(orig_loss)\\n\\t\\tcurrent_evals = 1\\n\\t\\tstate['func_evals'] += 1\\n\\t\\tflat_grad = self._gather_flat_grad()\\n\\t\\tabs_grad_sum = flat_grad.abs().sum()\\n\\t\\tif abs_grad_sum <= tolerance_grad:\\n\\t\\t\\treturn orig_loss\\n\\t\\td = state.get('d')\\n\\t\\tt = state.get('t')\\n\\t\\told_dirs = state.get('old_dirs')\\n\\t\\told_stps = state.get('old_stps')\\n\\t\\tH_diag = state.get('H_diag')\\n\\t\\tprev_flat_grad = state.get('prev_flat_grad')\\n\\t\\tprev_loss = state.get('prev_loss')\\n\\t\\tn_iter = 0\\n\\t\\twhile n_iter < max_iter:\\n\\t\\t\\tn_iter += 1\\n\\t\\t\\tstate['n_iter'] += 1\\n\\t\\t\\tif state['n_iter'] == 1:\\n\\t\\t\\t\\td = flat_grad.neg()\\n\\t\\t\\t\\told_dirs = []\\n\\t\\t\\t\\told_stps = []\\n\\t\\t\\t\\tH_diag = 1\\n\\t\\t\\telse:\\n\\t\\t\\t\\ty = flat_grad.sub(prev_flat_grad)\\n\\t\\t\\t\\ts = d.mul(t)\\n\\t\\t\\t\\tys = y.dot(s)  \\n\\t\\t\\t\\tif ys > 1e-10:\\n\\t\\t\\t\\t\\tif len(old_dirs) == history_size:\\n\\t\\t\\t\\t\\t\\told_dirs.pop(0)\\n\\t\\t\\t\\t\\t\\told_stps.pop(0)\\n\\t\\t\\t\\t\\told_dirs.append(y)\\n\\t\\t\\t\\t\\told_stps.append(s)\\n\\t\\t\\t\\t\\tH_diag = ys / y.dot(y)  \\n\\t\\t\\t\\tnum_old = len(old_dirs)\\n\\t\\t\\t\\tif 'ro' not in state:\\n\\t\\t\\t\\t\\tstate['ro'] = [None] * history_size\\n\\t\\t\\t\\t\\tstate['al'] = [None] * history_size\\n\\t\\t\\t\\tro = state['ro']\\n\\t\\t\\t\\tal = state['al']\\n\\t\\t\\t\\tfor i in range(num_old):\\n\\t\\t\\t\\t\\tro[i] = 1. / old_dirs[i].dot(old_stps[i])\\n\\t\\t\\t\\tq = flat_grad.neg()\\n\\t\\t\\t\\tfor i in range(num_old - 1, -1, -1):\\n\\t\\t\\t\\t\\tal[i] = old_stps[i].dot(q) * ro[i]\\n\\t\\t\\t\\t\\tq.add_(-al[i], old_dirs[i])\\n\\t\\t\\t\\td = r = torch.mul(q, H_diag)\\n\\t\\t\\t\\tfor i in range(num_old):\\n\\t\\t\\t\\t\\tbe_i = old_dirs[i].dot(r) * ro[i]\\n\\t\\t\\t\\t\\tr.add_(al[i] - be_i, old_stps[i])\\n\\t\\t\\tif prev_flat_grad is None:\\n\\t\\t\\t\\tprev_flat_grad = flat_grad.clone()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprev_flat_grad.copy_(flat_grad)\\n\\t\\t\\tprev_loss = loss\\n\\t\\t\\tif state['n_iter'] == 1:\\n\\t\\t\\t\\tt = min(1., 1. / abs_grad_sum) * lr\\n\\t\\t\\telse:\\n\\t\\t\\t\\tt = lr\\n\\t\\t\\tgtd = flat_grad.dot(d)  \\n\\t\\t\\tls_func_evals = 0\\n\\t\\t\\tif line_search_fn is not None:\\n\\t\\t\\t\\traise RuntimeError(\"line search function is not supported yet\")\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself._add_grad(t, d)\\n\\t\\t\\t\\tif n_iter != max_iter:\\n\\t\\t\\t\\t\\tloss = float(closure())\\n\\t\\t\\t\\t\\tflat_grad = self._gather_flat_grad()\\n\\t\\t\\t\\t\\tabs_grad_sum = flat_grad.abs().sum()\\n\\t\\t\\t\\t\\tls_func_evals = 1\\n\\t\\t\\tcurrent_evals += ls_func_evals\\n\\t\\t\\tstate['func_evals'] += ls_func_evals\\n\\t\\t\\tif n_iter == max_iter:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif current_evals >= max_eval:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif abs_grad_sum <= tolerance_grad:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif gtd > -tolerance_change:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif d.mul(t).abs_().sum() <= tolerance_change:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif abs(loss - prev_loss) < tolerance_change:\\n\\t\\t\\t\\tbreak\\n\\t\\tstate['d'] = d\\n\\t\\tstate['t'] = t\\n\\t\\tstate['old_dirs'] = old_dirs\\n\\t\\tstate['old_stps'] = old_stps\\n\\t\\tstate['H_diag'] = H_diag\\n\\t\\tstate['prev_flat_grad'] = prev_flat_grad\\n\\t\\tstate['prev_loss'] = prev_loss\\n\\t\\treturn orig_loss",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def step(self, closure):\\n\\t\\tassert len(self.param_groups) == 1\\n\\t\\tgroup = self.param_groups[0]\\n\\t\\tlr = group['lr']\\n\\t\\tmax_iter = group['max_iter']\\n\\t\\tmax_eval = group['max_eval']\\n\\t\\ttolerance_grad = group['tolerance_grad']\\n\\t\\ttolerance_change = group['tolerance_change']\\n\\t\\tline_search_fn = group['line_search_fn']\\n\\t\\thistory_size = group['history_size']\\n\\t\\tstate = self.state[self._params[0]]\\n\\t\\tstate.setdefault('func_evals', 0)\\n\\t\\tstate.setdefault('n_iter', 0)\\n\\t\\torig_loss = closure()\\n\\t\\tloss = float(orig_loss)\\n\\t\\tcurrent_evals = 1\\n\\t\\tstate['func_evals'] += 1\\n\\t\\tflat_grad = self._gather_flat_grad()\\n\\t\\tabs_grad_sum = flat_grad.abs().sum()\\n\\t\\tif abs_grad_sum <= tolerance_grad:\\n\\t\\t\\treturn orig_loss\\n\\t\\td = state.get('d')\\n\\t\\tt = state.get('t')\\n\\t\\told_dirs = state.get('old_dirs')\\n\\t\\told_stps = state.get('old_stps')\\n\\t\\tH_diag = state.get('H_diag')\\n\\t\\tprev_flat_grad = state.get('prev_flat_grad')\\n\\t\\tprev_loss = state.get('prev_loss')\\n\\t\\tn_iter = 0\\n\\t\\twhile n_iter < max_iter:\\n\\t\\t\\tn_iter += 1\\n\\t\\t\\tstate['n_iter'] += 1\\n\\t\\t\\tif state['n_iter'] == 1:\\n\\t\\t\\t\\td = flat_grad.neg()\\n\\t\\t\\t\\told_dirs = []\\n\\t\\t\\t\\told_stps = []\\n\\t\\t\\t\\tH_diag = 1\\n\\t\\t\\telse:\\n\\t\\t\\t\\ty = flat_grad.sub(prev_flat_grad)\\n\\t\\t\\t\\ts = d.mul(t)\\n\\t\\t\\t\\tys = y.dot(s)  \\n\\t\\t\\t\\tif ys > 1e-10:\\n\\t\\t\\t\\t\\tif len(old_dirs) == history_size:\\n\\t\\t\\t\\t\\t\\told_dirs.pop(0)\\n\\t\\t\\t\\t\\t\\told_stps.pop(0)\\n\\t\\t\\t\\t\\told_dirs.append(y)\\n\\t\\t\\t\\t\\told_stps.append(s)\\n\\t\\t\\t\\t\\tH_diag = ys / y.dot(y)  \\n\\t\\t\\t\\tnum_old = len(old_dirs)\\n\\t\\t\\t\\tif 'ro' not in state:\\n\\t\\t\\t\\t\\tstate['ro'] = [None] * history_size\\n\\t\\t\\t\\t\\tstate['al'] = [None] * history_size\\n\\t\\t\\t\\tro = state['ro']\\n\\t\\t\\t\\tal = state['al']\\n\\t\\t\\t\\tfor i in range(num_old):\\n\\t\\t\\t\\t\\tro[i] = 1. / old_dirs[i].dot(old_stps[i])\\n\\t\\t\\t\\tq = flat_grad.neg()\\n\\t\\t\\t\\tfor i in range(num_old - 1, -1, -1):\\n\\t\\t\\t\\t\\tal[i] = old_stps[i].dot(q) * ro[i]\\n\\t\\t\\t\\t\\tq.add_(-al[i], old_dirs[i])\\n\\t\\t\\t\\td = r = torch.mul(q, H_diag)\\n\\t\\t\\t\\tfor i in range(num_old):\\n\\t\\t\\t\\t\\tbe_i = old_dirs[i].dot(r) * ro[i]\\n\\t\\t\\t\\t\\tr.add_(al[i] - be_i, old_stps[i])\\n\\t\\t\\tif prev_flat_grad is None:\\n\\t\\t\\t\\tprev_flat_grad = flat_grad.clone()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprev_flat_grad.copy_(flat_grad)\\n\\t\\t\\tprev_loss = loss\\n\\t\\t\\tif state['n_iter'] == 1:\\n\\t\\t\\t\\tt = min(1., 1. / abs_grad_sum) * lr\\n\\t\\t\\telse:\\n\\t\\t\\t\\tt = lr\\n\\t\\t\\tgtd = flat_grad.dot(d)  \\n\\t\\t\\tls_func_evals = 0\\n\\t\\t\\tif line_search_fn is not None:\\n\\t\\t\\t\\traise RuntimeError(\"line search function is not supported yet\")\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself._add_grad(t, d)\\n\\t\\t\\t\\tif n_iter != max_iter:\\n\\t\\t\\t\\t\\tloss = float(closure())\\n\\t\\t\\t\\t\\tflat_grad = self._gather_flat_grad()\\n\\t\\t\\t\\t\\tabs_grad_sum = flat_grad.abs().sum()\\n\\t\\t\\t\\t\\tls_func_evals = 1\\n\\t\\t\\tcurrent_evals += ls_func_evals\\n\\t\\t\\tstate['func_evals'] += ls_func_evals\\n\\t\\t\\tif n_iter == max_iter:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif current_evals >= max_eval:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif abs_grad_sum <= tolerance_grad:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif gtd > -tolerance_change:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif d.mul(t).abs_().sum() <= tolerance_change:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tif abs(loss - prev_loss) < tolerance_change:\\n\\t\\t\\t\\tbreak\\n\\t\\tstate['d'] = d\\n\\t\\tstate['t'] = t\\n\\t\\tstate['old_dirs'] = old_dirs\\n\\t\\tstate['old_stps'] = old_stps\\n\\t\\tstate['H_diag'] = H_diag\\n\\t\\tstate['prev_flat_grad'] = prev_flat_grad\\n\\t\\tstate['prev_loss'] = prev_loss\\n\\t\\treturn orig_loss"
  },
  {
    "code": "def kneighbors(self, data, return_distance=True, **params):\\n        self._set_params(**params)\\n        return self.ball_tree.query(\\n            data, k=self.n_neighbors, return_distance=return_distance)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix kneighbors method in high dim",
    "fixed_code": "def kneighbors(self, X, return_distance=True, **params):\\n        self._set_params(**params)\\n        X = np.atleast_2d(X)\\n        if self.ball_tree is None:\\n            if self.algorithm == 'brute_inplace' and not return_distance:\\n                return knn_brute(self._fit_X, X, self.n_neighbors)\\n            else:\\n                dist = euclidean_distances(X, self._fit_X, squared=True)\\n                neigh_ind = dist.argsort(axis=1)[:, :self.n_neighbors]\\n            if not return_distance:\\n                return neigh_ind\\n            else:\\n                return dist.T[neigh_ind], neigh_ind\\n        else:\\n            return self.ball_tree.query(X, self.n_neighbors,\\n                                        return_distance=return_distance)"
  },
  {
    "code": "def _partial_tup_index(self, tup, side='left'):\\n        if len(tup) > self.lexsort_depth:\\n            raise Exception('MultiIndex lexsort depth %d, key was %d long' %\\n                            (self.lexsort_depth, len(tup)))\\n        n = len(tup)\\n        start, end = 0, len(self)\\n        zipped = izip(tup, self.levels, self.labels)\\n        for k, (lab, lev, labs) in enumerate(zipped):\\n            section = labs[start:end]\\n            if lab not in lev:\\n                loc = lev.searchsorted(lab, side=side)\\n                if side == 'right' and loc > 0:\\n                    loc -= 1\\n                return start + section.searchsorted(loc, side=side)\\n            idx = lev.get_loc(lab)\\n            if k < n - 1:\\n                end = start + section.searchsorted(idx, side='right')\\n                start = start + section.searchsorted(idx, side='left')\\n            else:\\n                return start + section.searchsorted(idx, side=side)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _partial_tup_index(self, tup, side='left'):\\n        if len(tup) > self.lexsort_depth:\\n            raise Exception('MultiIndex lexsort depth %d, key was %d long' %\\n                            (self.lexsort_depth, len(tup)))\\n        n = len(tup)\\n        start, end = 0, len(self)\\n        zipped = izip(tup, self.levels, self.labels)\\n        for k, (lab, lev, labs) in enumerate(zipped):\\n            section = labs[start:end]\\n            if lab not in lev:\\n                loc = lev.searchsorted(lab, side=side)\\n                if side == 'right' and loc > 0:\\n                    loc -= 1\\n                return start + section.searchsorted(loc, side=side)\\n            idx = lev.get_loc(lab)\\n            if k < n - 1:\\n                end = start + section.searchsorted(idx, side='right')\\n                start = start + section.searchsorted(idx, side='left')\\n            else:\\n                return start + section.searchsorted(idx, side=side)"
  },
  {
    "code": "def _mpl_repr(self):\\n        return self.asobject.values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _mpl_repr(self):\\n        return self.asobject.values"
  },
  {
    "code": "def _get_na_values(col, na_values, na_fvalues, keep_default_na):\\n    if isinstance(na_values, dict):\\n        if col in na_values:\\n            return na_values[col], na_fvalues[col]\\n        else:\\n            if keep_default_na:\\n                return STR_NA_VALUES, set()\\n            return set(), set()\\n    else:\\n        return na_values, na_fvalues",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_na_values(col, na_values, na_fvalues, keep_default_na):\\n    if isinstance(na_values, dict):\\n        if col in na_values:\\n            return na_values[col], na_fvalues[col]\\n        else:\\n            if keep_default_na:\\n                return STR_NA_VALUES, set()\\n            return set(), set()\\n    else:\\n        return na_values, na_fvalues"
  },
  {
    "code": "def extract_index(data):\\n    from pandas.core.index import _union_indexes\\n    index = None\\n    if len(data) == 0:\\n        index = Index([])\\n    elif len(data) > 0 and index is None:\\n        raw_lengths = []\\n        indexes = []\\n        have_raw_arrays = False\\n        have_series = False\\n        have_dicts = False\\n        for v in data.values():\\n            if isinstance(v, Series):\\n                have_series = True\\n                indexes.append(v.index)\\n            elif isinstance(v, dict):\\n                have_dicts = True\\n                indexes.append(v.keys())\\n            elif isinstance(v, (list, np.ndarray)):\\n                have_raw_arrays = True\\n                raw_lengths.append(len(v))\\n        if not indexes and not raw_lengths:\\n            raise ValueError('If use all scalar values, must pass index')\\n        if have_series or have_dicts:\\n            index = _union_indexes(indexes)\\n        if have_raw_arrays:\\n            lengths = list(set(raw_lengths))\\n            if len(lengths) > 1:\\n                raise ValueError('arrays must all be same length')\\n            if have_dicts:\\n                raise ValueError('Mixing dicts with non-Series may lead to '\\n                                 'ambiguous ordering.')\\n            if have_series:\\n                assert(lengths[0] == len(index))\\n            else:\\n                index = Index(np.arange(lengths[0]))\\n    return _ensure_index(index)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def extract_index(data):\\n    from pandas.core.index import _union_indexes\\n    index = None\\n    if len(data) == 0:\\n        index = Index([])\\n    elif len(data) > 0 and index is None:\\n        raw_lengths = []\\n        indexes = []\\n        have_raw_arrays = False\\n        have_series = False\\n        have_dicts = False\\n        for v in data.values():\\n            if isinstance(v, Series):\\n                have_series = True\\n                indexes.append(v.index)\\n            elif isinstance(v, dict):\\n                have_dicts = True\\n                indexes.append(v.keys())\\n            elif isinstance(v, (list, np.ndarray)):\\n                have_raw_arrays = True\\n                raw_lengths.append(len(v))\\n        if not indexes and not raw_lengths:\\n            raise ValueError('If use all scalar values, must pass index')\\n        if have_series or have_dicts:\\n            index = _union_indexes(indexes)\\n        if have_raw_arrays:\\n            lengths = list(set(raw_lengths))\\n            if len(lengths) > 1:\\n                raise ValueError('arrays must all be same length')\\n            if have_dicts:\\n                raise ValueError('Mixing dicts with non-Series may lead to '\\n                                 'ambiguous ordering.')\\n            if have_series:\\n                assert(lengths[0] == len(index))\\n            else:\\n                index = Index(np.arange(lengths[0]))\\n    return _ensure_index(index)"
  },
  {
    "code": "def append(self, other, ignore_index=False, verify_integrity=False, sort=False):\\n\\t\\tif isinstance(other, (Series, dict)):\\n\\t\\t\\tif isinstance(other, dict):\\n\\t\\t\\t\\tother = Series(other)\\n\\t\\t\\tif other.name is None and not ignore_index:\\n\\t\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\\t\"Can only append a Series if ignore_index=True\"\\n\\t\\t\\t\\t\\t\" or if the Series has a name\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tif other.name is None:\\n\\t\\t\\t\\tindex = None\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindex = Index([other.name], name=self.index.name)\\n\\t\\t\\tidx_diff = other.index.difference(self.columns)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcombined_columns = self.columns.append(idx_diff)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tcombined_columns = self.columns.astype(object).append(idx_diff)\\n\\t\\t\\tother = other.reindex(combined_columns, copy=False)\\n\\t\\t\\tother = DataFrame(\\n\\t\\t\\t\\tother.values.reshape((1, len(other))),\\n\\t\\t\\t\\tindex=index,\\n\\t\\t\\t\\tcolumns=combined_columns,\\n\\t\\t\\t)\\n\\t\\t\\tother = other._convert(datetime=True, timedelta=True)\\n\\t\\t\\tif not self.columns.equals(combined_columns):\\n\\t\\t\\t\\tself = self.reindex(columns=combined_columns)\\n\\t\\telif isinstance(other, list):\\n\\t\\t\\tif not other:\\n\\t\\t\\t\\tpass\\n\\t\\t\\telif not isinstance(other[0], DataFrame):\\n\\t\\t\\t\\tother = DataFrame(other)\\n\\t\\t\\t\\tif (self.columns.get_indexer(other.columns) >= 0).all():\\n\\t\\t\\t\\t\\tother = other.reindex(columns=self.columns)\\n\\t\\tfrom pandas.core.reshape.concat import concat\\n\\t\\tif isinstance(other, (list, tuple)):\\n\\t\\t\\tto_concat = [self] + other\\n\\t\\telse:\\n\\t\\t\\tto_concat = [self, other]\\n\\t\\treturn concat(\\n\\t\\t\\tto_concat,\\n\\t\\t\\tignore_index=ignore_index,\\n\\t\\t\\tverify_integrity=verify_integrity,\\n\\t\\t\\tsort=sort,\\n\\t\\t)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[bug] don't remove timezone-awareness when using the  method from Dat\u2026 (#30277)",
    "fixed_code": "def append(self, other, ignore_index=False, verify_integrity=False, sort=False):\\n\\t\\tif isinstance(other, (Series, dict)):\\n\\t\\t\\tif isinstance(other, dict):\\n\\t\\t\\t\\tother = Series(other)\\n\\t\\t\\tif other.name is None and not ignore_index:\\n\\t\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\\t\"Can only append a Series if ignore_index=True\"\\n\\t\\t\\t\\t\\t\" or if the Series has a name\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tindex = Index([other.name], name=self.index.name)\\n\\t\\t\\tidx_diff = other.index.difference(self.columns)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcombined_columns = self.columns.append(idx_diff)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tcombined_columns = self.columns.astype(object).append(idx_diff)\\n\\t\\t\\tother = (\\n\\t\\t\\t\\tother.reindex(combined_columns, copy=False)\\n\\t\\t\\t\\t.to_frame()\\n\\t\\t\\t\\t.T.infer_objects()\\n\\t\\t\\t\\t.rename_axis(index.names, copy=False)\\n\\t\\t\\t)\\n\\t\\t\\tif not self.columns.equals(combined_columns):\\n\\t\\t\\t\\tself = self.reindex(columns=combined_columns)\\n\\t\\telif isinstance(other, list):\\n\\t\\t\\tif not other:\\n\\t\\t\\t\\tpass\\n\\t\\t\\telif not isinstance(other[0], DataFrame):\\n\\t\\t\\t\\tother = DataFrame(other)\\n\\t\\t\\t\\tif (self.columns.get_indexer(other.columns) >= 0).all():\\n\\t\\t\\t\\t\\tother = other.reindex(columns=self.columns)\\n\\t\\tfrom pandas.core.reshape.concat import concat\\n\\t\\tif isinstance(other, (list, tuple)):\\n\\t\\t\\tto_concat = [self] + other\\n\\t\\telse:\\n\\t\\t\\tto_concat = [self, other]\\n\\t\\treturn concat(\\n\\t\\t\\tto_concat,\\n\\t\\t\\tignore_index=ignore_index,\\n\\t\\t\\tverify_integrity=verify_integrity,\\n\\t\\t\\tsort=sort,\\n\\t\\t)"
  },
  {
    "code": "def runshell(self):\\n\\t\\targs = [self.executable_name,\\n\\t\\t\\t\\tstr(self.connection.settings_dict['NAME'])]\\n\\t\\tsubprocess.run(args, check=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def runshell(self):\\n\\t\\targs = [self.executable_name,\\n\\t\\t\\t\\tstr(self.connection.settings_dict['NAME'])]\\n\\t\\tsubprocess.run(args, check=True)"
  },
  {
    "code": "def __enter__(self):\\n        return self\\n    @abc.abstractmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __enter__(self):\\n        return self\\n    @abc.abstractmethod"
  },
  {
    "code": "def process_lhs(self, compiler, connection):\\n        lhs, lhs_params = super().process_lhs(compiler, connection)\\n        if connection.vendor == 'sqlite':\\n            rhs, rhs_params = super().process_rhs(compiler, connection)\\n            if rhs == '%s' and rhs_params == [None]:\\n                lhs = \"JSON_TYPE(%s, '$')\" % lhs\\n        return lhs, lhs_params",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #32411 -- Fixed __icontains lookup for JSONField on MySQL.",
    "fixed_code": "def process_lhs(self, compiler, connection):\\n        lhs, lhs_params = super().process_lhs(compiler, connection)\\n        if connection.vendor == 'mysql':\\n            return 'LOWER(%s)' % lhs, lhs_params\\n        return lhs, lhs_params"
  },
  {
    "code": "def _reduce(self, name, axis=0, skipna=True, **kwargs):\\n        func = getattr(self, name, None)\\n        if func is None:\\n            msg = 'Categorical cannot perform the operation {op}'\\n            raise TypeError(msg.format(op=name))\\n        return func(**kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix passing of numeric_only argument for categorical reduce (#25304)",
    "fixed_code": "def _reduce(self, name, axis=0, **kwargs):\\n        func = getattr(self, name, None)\\n        if func is None:\\n            msg = 'Categorical cannot perform the operation {op}'\\n            raise TypeError(msg.format(op=name))\\n        return func(**kwargs)"
  },
  {
    "code": "def process_response(self, request, response):\\n\\t\\tif (self.sts_seconds and request.is_secure() and\\n\\t\\t\\t\\t'Strict-Transport-Security' not in response):\\n\\t\\t\\tsts_header = \"max-age=%s\" % self.sts_seconds\\n\\t\\t\\tif self.sts_include_subdomains:\\n\\t\\t\\t\\tsts_header = sts_header + \"; includeSubDomains\"\\n\\t\\t\\tif self.sts_preload:\\n\\t\\t\\t\\tsts_header = sts_header + \"; preload\"\\n\\t\\t\\tresponse['Strict-Transport-Security'] = sts_header\\n\\t\\tif self.content_type_nosniff:\\n\\t\\t\\tresponse.setdefault('X-Content-Type-Options', 'nosniff')\\n\\t\\tif self.xss_filter:\\n\\t\\t\\tresponse.setdefault('X-XSS-Protection', '1; mode=block')\\n\\t\\treturn response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_response(self, request, response):\\n\\t\\tif (self.sts_seconds and request.is_secure() and\\n\\t\\t\\t\\t'Strict-Transport-Security' not in response):\\n\\t\\t\\tsts_header = \"max-age=%s\" % self.sts_seconds\\n\\t\\t\\tif self.sts_include_subdomains:\\n\\t\\t\\t\\tsts_header = sts_header + \"; includeSubDomains\"\\n\\t\\t\\tif self.sts_preload:\\n\\t\\t\\t\\tsts_header = sts_header + \"; preload\"\\n\\t\\t\\tresponse['Strict-Transport-Security'] = sts_header\\n\\t\\tif self.content_type_nosniff:\\n\\t\\t\\tresponse.setdefault('X-Content-Type-Options', 'nosniff')\\n\\t\\tif self.xss_filter:\\n\\t\\t\\tresponse.setdefault('X-XSS-Protection', '1; mode=block')\\n\\t\\treturn response"
  },
  {
    "code": "def loop_fn(i):\\n\\tgathered_elems = nest.map_structure(lambda x: array_ops.gather(x, i), elems)\\n\\treturn fn(gathered_elems)\\n  batch_size = None\\n  first_elem_shape = nest.flatten(elems)[0].shape\\n  if first_elem_shape.rank is not None:\\n\\tbatch_size = first_elem_shape.as_list()[0]\\n  if batch_size is None:\\n\\tbatch_size = array_ops.shape()[0]\\n  return pfor(loop_fn, batch_size)",
    "label": 1,
    "bug_type": "corruption - memory",
    "bug_description": "Fix bug where `vectorized_map` failed on inputs with unknown static shape.",
    "fixed_code": "def loop_fn(i):\\n\\tgathered_elems = nest.map_structure(lambda x: array_ops.gather(x, i), elems)\\n\\treturn fn(gathered_elems)\\n  batch_size = None\\n  first_elem = ops.convert_to_tensor(nest.flatten(elems)[0])\\n  if first_elem.shape.rank is not None:\\n\\tbatch_size = first_elem.shape.as_list()[0]\\n  if batch_size is None:\\n\\tbatch_size = array_ops.shape(first_elem)[0]\\n  return pfor(loop_fn, batch_size)"
  },
  {
    "code": "def _possibly_castable(arr):\\n    return arr.dtype.name not in _POSSIBLY_CAST_DTYPES",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Bug in DataFrame construction with recarray and non-ns datetime dtype (GH6140)",
    "fixed_code": "def _possibly_castable(arr):\\n    kind = arr.dtype.kind\\n    if kind == 'M' or kind == 'm':\\n        return arr.dtype in _DATELIKE_DTYPES\\n    return arr.dtype.name not in _POSSIBLY_CAST_DTYPES"
  },
  {
    "code": "def _check_object_id_field(self):\\n        try:\\n            self.model._meta.get_field(self.fk_field)\\n        except FieldDoesNotExist:\\n            return [\\n                checks.Error(\\n                    'The field refers to \"%s\" field which is missing.' % self.fk_field,\\n                    hint=None,\\n                    obj=self,\\n                    id='contenttypes.E001',\\n                )\\n            ]\\n        else:\\n            return []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Cleanup of contrib.contenttypes check messages.",
    "fixed_code": "def _check_object_id_field(self):\\n        try:\\n            self.model._meta.get_field(self.fk_field)\\n        except FieldDoesNotExist:\\n            return [\\n                checks.Error(\\n                    \"The GenericForeignKey object ID references the non-existent field '%s'.\" % self.fk_field,\\n                    hint=None,\\n                    obj=self,\\n                    id='contenttypes.E002',\\n                )\\n            ]\\n        else:\\n            return []"
  },
  {
    "code": "def _fill_zeros(result, x, y, name, fill):\\n    if fill is None or is_float_dtype(result):\\n        return result\\n    if name.startswith(('r', '__r')):\\n        x,y = y,x\\n    if np.isscalar(y):\\n        y = np.array(y)\\n    if is_integer_dtype(y):\\n        if (y == 0).any():\\n            mask = ((y == 0) & ~np.isnan(result)).ravel()\\n            shape = result.shape\\n            result = result.astype('float64', copy=False).ravel()\\n            np.putmask(result, mask, fill)\\n            if np.isinf(fill):\\n                signs = np.sign(y if name.startswith(('r', '__r')) else x)\\n                negative_inf_mask = (signs.ravel() < 0) & mask\\n                np.putmask(result, negative_inf_mask, -fill)\\n            if \"floordiv\" in name:  \\n                nan_mask = ((y == 0) & (x == 0)).ravel()\\n                np.putmask(result, nan_mask, np.nan)\\n            result = result.reshape(shape)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: division of Decimal would crash on fill because Decimal does not support type or dtype. (GH9787) ENH: replace np.isscalar with better lib.isscalar ADD: Test decimal division",
    "fixed_code": "def _fill_zeros(result, x, y, name, fill):\\n    if fill is None or is_float_dtype(result):\\n        return result\\n    if name.startswith(('r', '__r')):\\n        x,y = y,x\\n    is_typed_variable = (hasattr(y, 'dtype') or hasattr(y,'type'))\\n    is_scalar = lib.isscalar(y)\\n    if not is_typed_variable and not is_scalar:\\n        return result\\n    if is_scalar:\\n        y = np.array(y)\\n    if is_integer_dtype(y):\\n        if (y == 0).any():\\n            mask = ((y == 0) & ~np.isnan(result)).ravel()\\n            shape = result.shape\\n            result = result.astype('float64', copy=False).ravel()\\n            np.putmask(result, mask, fill)\\n            if np.isinf(fill):\\n                signs = np.sign(y if name.startswith(('r', '__r')) else x)\\n                negative_inf_mask = (signs.ravel() < 0) & mask\\n                np.putmask(result, negative_inf_mask, -fill)\\n            if \"floordiv\" in name:  \\n                nan_mask = ((y == 0) & (x == 0)).ravel()\\n                np.putmask(result, nan_mask, np.nan)\\n            result = result.reshape(shape)\\n    return result"
  },
  {
    "code": "def __setstate__(self, state):\\n        columns, ref_columns, values = state\\n        self.columns = Index(columns)\\n        self.ref_columns = Index(ref_columns)\\n        self.values = values\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __setstate__(self, state):\\n        columns, ref_columns, values = state\\n        self.columns = Index(columns)\\n        self.ref_columns = Index(ref_columns)\\n        self.values = values\\n    @property"
  },
  {
    "code": "def construct_1d_ndarray_preserving_na(\\n    values: Sequence, dtype: Optional[DtypeObj] = None, copy: bool = False\\n) -> np.ndarray:\\n    if dtype is not None and dtype.kind == \"U\":\\n        subarr = lib.ensure_string_array(values, convert_na_value=False, copy=copy)\\n    else:\\n        if dtype is not None:\\n            _disallow_mismatched_datetimelike(values, dtype)\\n        if (\\n            dtype == object\\n            and isinstance(values, np.ndarray)\\n            and values.dtype.kind in [\"m\", \"M\"]\\n        ):\\n            subarr = construct_1d_object_array_from_listlike(list(values))\\n        else:\\n            subarr = np.array(values, dtype=dtype, copy=copy)\\n    return subarr",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def construct_1d_ndarray_preserving_na(\\n    values: Sequence, dtype: Optional[DtypeObj] = None, copy: bool = False\\n) -> np.ndarray:\\n    if dtype is not None and dtype.kind == \"U\":\\n        subarr = lib.ensure_string_array(values, convert_na_value=False, copy=copy)\\n    else:\\n        if dtype is not None:\\n            _disallow_mismatched_datetimelike(values, dtype)\\n        if (\\n            dtype == object\\n            and isinstance(values, np.ndarray)\\n            and values.dtype.kind in [\"m\", \"M\"]\\n        ):\\n            subarr = construct_1d_object_array_from_listlike(list(values))\\n        else:\\n            subarr = np.array(values, dtype=dtype, copy=copy)\\n    return subarr"
  },
  {
    "code": "def sqrt(self, context=None):\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n            if self._isinfinity() and self._sign == 0:\\n                return Decimal(self)\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', self._exp // 2)\\n            return ans._fix(context)\\n        if context is None:\\n            context = getcontext()\\n        if self._sign == 1:\\n            return context._raise_error(InvalidOperation, 'sqrt(-x), x > 0')\\n        prec = context.prec+1\\n        op = _WorkRep(self)\\n        e = op.exp >> 1\\n        if op.exp & 1:\\n            c = op.int * 10\\n            l = (len(self._int) >> 1) + 1\\n        else:\\n            c = op.int\\n            l = len(self._int)+1 >> 1\\n        shift = prec-l\\n        if shift >= 0:\\n            c *= 100**shift\\n            exact = True\\n        else:\\n            c, remainder = divmod(c, 100**-shift)\\n            exact = not remainder\\n        e -= shift\\n        n = 10**prec\\n        while True:\\n            q = c//n\\n            if n <= q:\\n                break\\n            else:\\n                n = n + q >> 1\\n        exact = exact and n*n == c\\n        if exact:\\n            if shift >= 0:\\n                n //= 10**shift\\n            else:\\n                n *= 10**-shift\\n            e += shift\\n        else:\\n            if n % 5 == 0:\\n                n += 1\\n        ans = _dec_from_triple(0, str(n), e)\\n        context = context._shallow_copy()\\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\\n        ans = ans._fix(context)\\n        context.rounding = rounding\\n        return ans",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 61892,61900 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r61892 | mark.dickinson | 2008-03-25 15:33:23 +0100 (Tue, 25 Mar 2008) | 3 lines\\n\\n  Issue #2478: Decimal(sqrt(0)) failed when the decimal context\\n  was not explicitly supplied.\\n........\\n  r61900 | georg.brandl | 2008-03-25 18:36:43 +0100 (Tue, 25 Mar 2008) | 2 lines\\n\\n  Add Benjamin.\\n........",
    "fixed_code": "def sqrt(self, context=None):\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n            if self._isinfinity() and self._sign == 0:\\n                return Decimal(self)\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', self._exp // 2)\\n            return ans._fix(context)\\n        if self._sign == 1:\\n            return context._raise_error(InvalidOperation, 'sqrt(-x), x > 0')\\n        prec = context.prec+1\\n        op = _WorkRep(self)\\n        e = op.exp >> 1\\n        if op.exp & 1:\\n            c = op.int * 10\\n            l = (len(self._int) >> 1) + 1\\n        else:\\n            c = op.int\\n            l = len(self._int)+1 >> 1\\n        shift = prec-l\\n        if shift >= 0:\\n            c *= 100**shift\\n            exact = True\\n        else:\\n            c, remainder = divmod(c, 100**-shift)\\n            exact = not remainder\\n        e -= shift\\n        n = 10**prec\\n        while True:\\n            q = c//n\\n            if n <= q:\\n                break\\n            else:\\n                n = n + q >> 1\\n        exact = exact and n*n == c\\n        if exact:\\n            if shift >= 0:\\n                n //= 10**shift\\n            else:\\n                n *= 10**-shift\\n            e += shift\\n        else:\\n            if n % 5 == 0:\\n                n += 1\\n        ans = _dec_from_triple(0, str(n), e)\\n        context = context._shallow_copy()\\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\\n        ans = ans._fix(context)\\n        context.rounding = rounding\\n        return ans"
  },
  {
    "code": "def load_template(self, template_name, template_dirs=None):\\n\\t\\twarnings.warn(\\n\\t\\t\\t'The load_template() method is deprecated. Use get_template() '\\n\\t\\t\\t'instead.', RemovedInDjango20Warning,\\n\\t\\t)\\n\\t\\tkey = self.cache_key(template_name, template_dirs)\\n\\t\\ttemplate_tuple = self.template_cache.get(key)\\n\\t\\tif template_tuple is TemplateDoesNotExist:\\n\\t\\t\\traise TemplateDoesNotExist\\n\\t\\telif template_tuple is None:\\n\\t\\t\\ttemplate, origin = self.find_template(template_name, template_dirs)\\n\\t\\t\\tif not hasattr(template, 'render'):\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ttemplate = Template(template, origin, template_name, self.engine)\\n\\t\\t\\t\\texcept TemplateDoesNotExist:\\n\\t\\t\\t\\t\\tself.template_cache[key] = (template, origin)\\n\\t\\t\\tself.template_cache[key] = (template, None)\\n\\t\\treturn self.template_cache[key]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26280 -- Fixed cached template loader crash when loading nonexistent template.",
    "fixed_code": "def load_template(self, template_name, template_dirs=None):\\n\\t\\twarnings.warn(\\n\\t\\t\\t'The load_template() method is deprecated. Use get_template() '\\n\\t\\t\\t'instead.', RemovedInDjango20Warning,\\n\\t\\t)\\n\\t\\tkey = self.cache_key(template_name, template_dirs)\\n\\t\\ttemplate_tuple = self.template_cache.get(key)\\n\\t\\tif template_tuple is TemplateDoesNotExist:\\n\\t\\t\\traise TemplateDoesNotExist(template_name)\\n\\t\\telif template_tuple is None:\\n\\t\\t\\ttemplate, origin = self.find_template(template_name, template_dirs)\\n\\t\\t\\tif not hasattr(template, 'render'):\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ttemplate = Template(template, origin, template_name, self.engine)\\n\\t\\t\\t\\texcept TemplateDoesNotExist:\\n\\t\\t\\t\\t\\tself.template_cache[key] = (template, origin)\\n\\t\\t\\tself.template_cache[key] = (template, None)\\n\\t\\treturn self.template_cache[key]"
  },
  {
    "code": "def __str__(self):\\n        if self.negated:\\n            return '(NOT (%s: %s))' % (self.connector, ', '.join(str(c) for c\\n                    in self.children))\\n        return '(%s: %s)' % (self.connector, ', '.join(str(c) for c in\\n                self.children))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25875 -- Prevented UnicodeDecodeError for Q object repr\\n\\nThanks Ben Kraft for the report, and Simon Charette for the review.",
    "fixed_code": "def __str__(self):\\n        template = '(NOT (%s: %s))' if self.negated else '(%s: %s)'\\n        return force_str(template % (self.connector, ', '.join(force_text(c) for c in self.children)))"
  },
  {
    "code": "def stop(self):\\n\\t\\tif self._thread is None:\\n\\t\\t\\treturn\\n\\t\\tself._stopped = True\\n\\t\\tself._thread.kill()\\n\\t\\tself._thread.wait()\\n\\t\\tself._thread = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "vshield task manager: abort tasks in stop() on termination\\n\\nThis patch kills the manager thread, and aborts active tasks\\nrather than sending an exception to the manager thread and\\nhave it do the abort on termination.\\n\\nUnit tests involving vshield task manager might take longer\\nas a side effect of this patch.\\n\\nRelated-bug: #1282452",
    "fixed_code": "def stop(self):\\n\\t\\tif self._thread is None:\\n\\t\\t\\treturn\\n\\t\\tself._stopped = True\\n\\t\\tself._thread.kill()\\n\\t\\tself._thread = None\\n\\t\\tself._monitor.stop()\\n\\t\\tif self._monitor_busy:\\n\\t\\t\\tself._monitor.wait()\\n\\t\\tself._abort()\\n\\t\\tLOG.info(_(\"TaskManager terminated\"))"
  },
  {
    "code": "def _get_upstream_dataset_events(*, dag_run: DagRun, session: Session) -> List[\"DagRun\"]:\\n    if not dag_run.run_type == DagRunType.DATASET_TRIGGERED:\\n        return []\\n    previous_dag_run = (\\n        session.query(DagRun)\\n        .filter(\\n            DagRun.dag_id == dag_run.dag_id,\\n            DagRun.execution_date < dag_run.execution_date,\\n            DagRun.run_type == DagRunType.DATASET_TRIGGERED,\\n        )\\n        .order_by(DagRun.execution_date.desc())\\n        .first()\\n    )\\n    dataset_event_filters = [\\n        DatasetDagRef.dag_id == dag_run.dag_id,\\n        DatasetEvent.timestamp <= dag_run.execution_date,\\n    ]\\n    if previous_dag_run:\\n        dataset_event_filters.append(DatasetEvent.timestamp > previous_dag_run.execution_date)\\n    dataset_events = (\\n        session.query(DatasetEvent)\\n        .join(DatasetDagRef, DatasetEvent.dataset_id == DatasetDagRef.dataset_id)\\n        .filter(*dataset_event_filters)\\n        .order_by(DatasetEvent.timestamp)\\n        .all()\\n    )\\n    return dataset_events",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_upstream_dataset_events(*, dag_run: DagRun, session: Session) -> List[\"DagRun\"]:\\n    if not dag_run.run_type == DagRunType.DATASET_TRIGGERED:\\n        return []\\n    previous_dag_run = (\\n        session.query(DagRun)\\n        .filter(\\n            DagRun.dag_id == dag_run.dag_id,\\n            DagRun.execution_date < dag_run.execution_date,\\n            DagRun.run_type == DagRunType.DATASET_TRIGGERED,\\n        )\\n        .order_by(DagRun.execution_date.desc())\\n        .first()\\n    )\\n    dataset_event_filters = [\\n        DatasetDagRef.dag_id == dag_run.dag_id,\\n        DatasetEvent.timestamp <= dag_run.execution_date,\\n    ]\\n    if previous_dag_run:\\n        dataset_event_filters.append(DatasetEvent.timestamp > previous_dag_run.execution_date)\\n    dataset_events = (\\n        session.query(DatasetEvent)\\n        .join(DatasetDagRef, DatasetEvent.dataset_id == DatasetDagRef.dataset_id)\\n        .filter(*dataset_event_filters)\\n        .order_by(DatasetEvent.timestamp)\\n        .all()\\n    )\\n    return dataset_events"
  },
  {
    "code": "def _clean_na_values(na_values, keep_default_na=True):\\n    if na_values is None:\\n        if keep_default_na:\\n            na_values = _NA_VALUES\\n        else:\\n            na_values = []\\n        na_fvalues = set()\\n    elif isinstance(na_values, dict):\\n        if keep_default_na:\\n            for k, v in compat.iteritems(na_values):\\n                v = set(list(v)) | _NA_VALUES\\n                na_values[k] = v\\n        na_fvalues = dict([ (k, _floatify_na_values(v)) for k, v in na_values.items() ])\\n    else:\\n        if not com.is_list_like(na_values):\\n            na_values = [na_values]\\n        na_values = _stringify_na_values(na_values)\\n        if keep_default_na:\\n            na_values = na_values | _NA_VALUES\\n        na_fvalues = _floatify_na_values(na_values)\\n    return na_values, na_fvalues",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _clean_na_values(na_values, keep_default_na=True):\\n    if na_values is None:\\n        if keep_default_na:\\n            na_values = _NA_VALUES\\n        else:\\n            na_values = []\\n        na_fvalues = set()\\n    elif isinstance(na_values, dict):\\n        if keep_default_na:\\n            for k, v in compat.iteritems(na_values):\\n                v = set(list(v)) | _NA_VALUES\\n                na_values[k] = v\\n        na_fvalues = dict([ (k, _floatify_na_values(v)) for k, v in na_values.items() ])\\n    else:\\n        if not com.is_list_like(na_values):\\n            na_values = [na_values]\\n        na_values = _stringify_na_values(na_values)\\n        if keep_default_na:\\n            na_values = na_values | _NA_VALUES\\n        na_fvalues = _floatify_na_values(na_values)\\n    return na_values, na_fvalues"
  },
  {
    "code": "def clear_dag_runs():\\n    session = settings.Session()\\n    drs = session.query(DagRun).filter(\\n        DagRun.dag_id.in_(DAG_IDS),\\n    ).all()\\n    for dr in drs:\\n        logging.info('Deleting DagRun :: %s', dr)\\n        session.delete(dr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def clear_dag_runs():\\n    session = settings.Session()\\n    drs = session.query(DagRun).filter(\\n        DagRun.dag_id.in_(DAG_IDS),\\n    ).all()\\n    for dr in drs:\\n        logging.info('Deleting DagRun :: %s', dr)\\n        session.delete(dr)"
  },
  {
    "code": "def string_at(ptr, size=-1):\\n    return _string_at(ptr, size)\\ntry:\\n    from _ctypes import _wstring_at_addr\\nexcept ImportError:\\n    pass\\nelse:\\n    _wstring_at = CFUNCTYPE(py_object, c_void_p, c_int)(_wstring_at_addr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def string_at(ptr, size=-1):\\n    return _string_at(ptr, size)\\ntry:\\n    from _ctypes import _wstring_at_addr\\nexcept ImportError:\\n    pass\\nelse:\\n    _wstring_at = CFUNCTYPE(py_object, c_void_p, c_int)(_wstring_at_addr)"
  },
  {
    "code": "def _convert_to_ndarrays(dct):\\n    result = {}\\n    for c, values in dct.iteritems():\\n        try:\\n            values = np.array(values, dtype=float)\\n        except Exception:\\n            values = np.array(values, dtype=object)\\n        result[c] = _maybe_convert_int(values)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_to_ndarrays(dct):\\n    result = {}\\n    for c, values in dct.iteritems():\\n        try:\\n            values = np.array(values, dtype=float)\\n        except Exception:\\n            values = np.array(values, dtype=object)\\n        result[c] = _maybe_convert_int(values)\\n    return result"
  },
  {
    "code": "def merge_provider_user_param(self, result, provider):\\n        if self.validate_params('user', provider):\\n            result['user'] = provider['user']\\n        elif self.validate_params('F5_USER', os.environ):\\n            result['user'] = os.environ.get('F5_USER')\\n        elif self.validate_params('ANSIBLE_NET_USERNAME', os.environ):\\n            result['user'] = os.environ.get('ANSIBLE_NET_USERNAME')\\n        else:\\n            result['user'] = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def merge_provider_user_param(self, result, provider):\\n        if self.validate_params('user', provider):\\n            result['user'] = provider['user']\\n        elif self.validate_params('F5_USER', os.environ):\\n            result['user'] = os.environ.get('F5_USER')\\n        elif self.validate_params('ANSIBLE_NET_USERNAME', os.environ):\\n            result['user'] = os.environ.get('ANSIBLE_NET_USERNAME')\\n        else:\\n            result['user'] = None"
  },
  {
    "code": "def clear(\\n        self,\\n        task_ids=None,\\n        start_date=None,\\n        end_date=None,\\n        only_failed=False,\\n        only_running=False,\\n        confirm_prompt=False,\\n        include_subdags=True,\\n        include_parentdag=True,\\n        dag_run_state: str = State.RUNNING,\\n        dry_run=False,\\n        session=None,\\n        get_tis=False,\\n        recursion_depth=0,\\n        max_recursion_depth=None,\\n        dag_bag=None,\\n        visited_external_tis=None,\\n    ):\\n        TI = TaskInstance\\n        tis = session.query(TI)\\n        dag_ids = []\\n        if include_subdags:\\n            conditions = []\\n            for dag in self.subdags + [self]:\\n                conditions.append((TI.dag_id == dag.dag_id) & TI.task_id.in_(dag.task_ids))\\n                dag_ids.append(dag.dag_id)\\n            tis = tis.filter(or_(*conditions))\\n        else:\\n            tis = session.query(TI).filter(TI.dag_id == self.dag_id)\\n            tis = tis.filter(TI.task_id.in_(self.task_ids))\\n        if include_parentdag and self.is_subdag and self.parent_dag is not None:\\n            p_dag = self.parent_dag.sub_dag(\\n                task_ids_or_regex=r\"^{}$\".format(self.dag_id.split('.')[1]),\\n                include_upstream=False,\\n                include_downstream=True,\\n            )\\n            tis = tis.union(\\n                p_dag.clear(\\n                    start_date=start_date,\\n                    end_date=end_date,\\n                    only_failed=only_failed,\\n                    only_running=only_running,\\n                    confirm_prompt=confirm_prompt,\\n                    include_subdags=include_subdags,\\n                    include_parentdag=False,\\n                    dag_run_state=dag_run_state,\\n                    get_tis=True,\\n                    session=session,\\n                    recursion_depth=recursion_depth,\\n                    max_recursion_depth=max_recursion_depth,\\n                    dag_bag=dag_bag,\\n                    visited_external_tis=visited_external_tis,\\n                )\\n            )\\n        if start_date:\\n            tis = tis.filter(TI.execution_date >= start_date)\\n        if end_date:\\n            tis = tis.filter(TI.execution_date <= end_date)\\n        if only_failed:\\n            tis = tis.filter(or_(TI.state == State.FAILED, TI.state == State.UPSTREAM_FAILED))\\n        if only_running:\\n            tis = tis.filter(TI.state == State.RUNNING)\\n        if task_ids:\\n            tis = tis.filter(TI.task_id.in_(task_ids))\\n        if include_subdags:\\n            from airflow.sensors.external_task import ExternalTaskMarker\\n            instances = tis.all()\\n            for ti in instances:\\n                if ti.operator == ExternalTaskMarker.__name__:\\n                    if visited_external_tis is None:\\n                        visited_external_tis = set()\\n                    ti_key = ti.key.primary\\n                    if ti_key not in visited_external_tis:\\n                        task: ExternalTaskMarker = cast(\\n                            ExternalTaskMarker, copy.copy(self.get_task(ti.task_id))\\n                        )\\n                        ti.task = task\\n                        if recursion_depth == 0:\\n                            max_recursion_depth = task.recursion_depth\\n                        if recursion_depth + 1 > max_recursion_depth:\\n                            raise AirflowException(\\n                                \"Maximum recursion depth {} reached for {} {}. \"\\n                                \"Attempted to clear too many tasks \"\\n                                \"or there may be a cyclic dependency.\".format(\\n                                    max_recursion_depth, ExternalTaskMarker.__name__, ti.task_id\\n                                )\\n                            )\\n                        ti.render_templates()\\n                        external_tis = session.query(TI).filter(\\n                            TI.dag_id == task.external_dag_id,\\n                            TI.task_id == task.external_task_id,\\n                            TI.execution_date == pendulum.parse(task.execution_date),\\n                        )\\n                        for tii in external_tis:\\n                            if not dag_bag:\\n                                dag_bag = DagBag(read_dags_from_db=True)\\n                            external_dag = dag_bag.get_dag(tii.dag_id)\\n                            if not external_dag:\\n                                raise AirflowException(f\"Could not find dag {tii.dag_id}\")\\n                            downstream = external_dag.sub_dag(\\n                                task_ids_or_regex=fr\"^{tii.task_id}$\",\\n                                include_upstream=False,\\n                                include_downstream=True,\\n                            )\\n                            tis = tis.union(\\n                                downstream.clear(\\n                                    start_date=tii.execution_date,\\n                                    end_date=tii.execution_date,\\n                                    only_failed=only_failed,\\n                                    only_running=only_running,\\n                                    confirm_prompt=confirm_prompt,\\n                                    include_subdags=include_subdags,\\n                                    include_parentdag=False,\\n                                    dag_run_state=dag_run_state,\\n                                    get_tis=True,\\n                                    session=session,\\n                                    recursion_depth=recursion_depth + 1,\\n                                    max_recursion_depth=max_recursion_depth,\\n                                    dag_bag=dag_bag,\\n                                    visited_external_tis=visited_external_tis,\\n                                )\\n                            )\\n                        visited_external_tis.add(ti_key)\\n        if get_tis:\\n            return tis\\n        tis = tis.all()\\n        if dry_run:\\n            session.expunge_all()\\n            return tis\\n        count = len(tis)\\n        do_it = True\\n        if count == 0:\\n            return 0\\n        if confirm_prompt:\\n            ti_list = \"\\n\".join([str(t) for t in tis])\\n            question = (\\n                \"You are about to delete these {count} tasks:\\n{ti_list}\\nAre you sure? (yes/no): \"\\n            ).format(count=count, ti_list=ti_list)\\n            do_it = utils.helpers.ask_yesno(question)\\n        if do_it:\\n            clear_task_instances(\\n                tis,\\n                session,\\n                dag=self,\\n                activate_dag_runs=False,  \\n            )\\n            self.set_dag_runs_state(\\n                session=session,\\n                start_date=start_date,\\n                end_date=end_date,\\n                state=dag_run_state,\\n                dag_ids=dag_ids,\\n            )\\n        else:\\n            count = 0\\n            print(\"Cancelled, nothing was cleared.\")\\n        session.commit()\\n        return count\\n    @classmethod\\n    def clear_dags(\\n        cls,\\n        dags,\\n        start_date=None,\\n        end_date=None,\\n        only_failed=False,\\n        only_running=False,\\n        confirm_prompt=False,\\n        include_subdags=True,\\n        include_parentdag=False,\\n        dag_run_state=State.RUNNING,\\n        dry_run=False,\\n    ):\\n        all_tis = []\\n        for dag in dags:\\n            tis = dag.clear(\\n                start_date=start_date,\\n                end_date=end_date,\\n                only_failed=only_failed,\\n                only_running=only_running,\\n                confirm_prompt=False,\\n                include_subdags=include_subdags,\\n                include_parentdag=include_parentdag,\\n                dag_run_state=dag_run_state,\\n                dry_run=True,\\n            )\\n            all_tis.extend(tis)\\n        if dry_run:\\n            return all_tis\\n        count = len(all_tis)\\n        do_it = True\\n        if count == 0:\\n            print(\"Nothing to clear.\")\\n            return 0\\n        if confirm_prompt:\\n            ti_list = \"\\n\".join([str(t) for t in all_tis])\\n            question = f\"You are about to delete these {count} tasks:\\n{ti_list}\\nAre you sure? (yes/no): \"\\n            do_it = utils.helpers.ask_yesno(question)\\n        if do_it:\\n            for dag in dags:\\n                dag.clear(\\n                    start_date=start_date,\\n                    end_date=end_date,\\n                    only_failed=only_failed,\\n                    only_running=only_running,\\n                    confirm_prompt=False,\\n                    include_subdags=include_subdags,\\n                    dag_run_state=dag_run_state,\\n                    dry_run=False,\\n                )\\n        else:\\n            count = 0\\n            print(\"Cancelled, nothing was cleared.\")\\n        return count",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def clear(\\n        self,\\n        task_ids=None,\\n        start_date=None,\\n        end_date=None,\\n        only_failed=False,\\n        only_running=False,\\n        confirm_prompt=False,\\n        include_subdags=True,\\n        include_parentdag=True,\\n        dag_run_state: str = State.RUNNING,\\n        dry_run=False,\\n        session=None,\\n        get_tis=False,\\n        recursion_depth=0,\\n        max_recursion_depth=None,\\n        dag_bag=None,\\n        visited_external_tis=None,\\n    ):\\n        TI = TaskInstance\\n        tis = session.query(TI)\\n        dag_ids = []\\n        if include_subdags:\\n            conditions = []\\n            for dag in self.subdags + [self]:\\n                conditions.append((TI.dag_id == dag.dag_id) & TI.task_id.in_(dag.task_ids))\\n                dag_ids.append(dag.dag_id)\\n            tis = tis.filter(or_(*conditions))\\n        else:\\n            tis = session.query(TI).filter(TI.dag_id == self.dag_id)\\n            tis = tis.filter(TI.task_id.in_(self.task_ids))\\n        if include_parentdag and self.is_subdag and self.parent_dag is not None:\\n            p_dag = self.parent_dag.sub_dag(\\n                task_ids_or_regex=r\"^{}$\".format(self.dag_id.split('.')[1]),\\n                include_upstream=False,\\n                include_downstream=True,\\n            )\\n            tis = tis.union(\\n                p_dag.clear(\\n                    start_date=start_date,\\n                    end_date=end_date,\\n                    only_failed=only_failed,\\n                    only_running=only_running,\\n                    confirm_prompt=confirm_prompt,\\n                    include_subdags=include_subdags,\\n                    include_parentdag=False,\\n                    dag_run_state=dag_run_state,\\n                    get_tis=True,\\n                    session=session,\\n                    recursion_depth=recursion_depth,\\n                    max_recursion_depth=max_recursion_depth,\\n                    dag_bag=dag_bag,\\n                    visited_external_tis=visited_external_tis,\\n                )\\n            )\\n        if start_date:\\n            tis = tis.filter(TI.execution_date >= start_date)\\n        if end_date:\\n            tis = tis.filter(TI.execution_date <= end_date)\\n        if only_failed:\\n            tis = tis.filter(or_(TI.state == State.FAILED, TI.state == State.UPSTREAM_FAILED))\\n        if only_running:\\n            tis = tis.filter(TI.state == State.RUNNING)\\n        if task_ids:\\n            tis = tis.filter(TI.task_id.in_(task_ids))\\n        if include_subdags:\\n            from airflow.sensors.external_task import ExternalTaskMarker\\n            instances = tis.all()\\n            for ti in instances:\\n                if ti.operator == ExternalTaskMarker.__name__:\\n                    if visited_external_tis is None:\\n                        visited_external_tis = set()\\n                    ti_key = ti.key.primary\\n                    if ti_key not in visited_external_tis:\\n                        task: ExternalTaskMarker = cast(\\n                            ExternalTaskMarker, copy.copy(self.get_task(ti.task_id))\\n                        )\\n                        ti.task = task\\n                        if recursion_depth == 0:\\n                            max_recursion_depth = task.recursion_depth\\n                        if recursion_depth + 1 > max_recursion_depth:\\n                            raise AirflowException(\\n                                \"Maximum recursion depth {} reached for {} {}. \"\\n                                \"Attempted to clear too many tasks \"\\n                                \"or there may be a cyclic dependency.\".format(\\n                                    max_recursion_depth, ExternalTaskMarker.__name__, ti.task_id\\n                                )\\n                            )\\n                        ti.render_templates()\\n                        external_tis = session.query(TI).filter(\\n                            TI.dag_id == task.external_dag_id,\\n                            TI.task_id == task.external_task_id,\\n                            TI.execution_date == pendulum.parse(task.execution_date),\\n                        )\\n                        for tii in external_tis:\\n                            if not dag_bag:\\n                                dag_bag = DagBag(read_dags_from_db=True)\\n                            external_dag = dag_bag.get_dag(tii.dag_id)\\n                            if not external_dag:\\n                                raise AirflowException(f\"Could not find dag {tii.dag_id}\")\\n                            downstream = external_dag.sub_dag(\\n                                task_ids_or_regex=fr\"^{tii.task_id}$\",\\n                                include_upstream=False,\\n                                include_downstream=True,\\n                            )\\n                            tis = tis.union(\\n                                downstream.clear(\\n                                    start_date=tii.execution_date,\\n                                    end_date=tii.execution_date,\\n                                    only_failed=only_failed,\\n                                    only_running=only_running,\\n                                    confirm_prompt=confirm_prompt,\\n                                    include_subdags=include_subdags,\\n                                    include_parentdag=False,\\n                                    dag_run_state=dag_run_state,\\n                                    get_tis=True,\\n                                    session=session,\\n                                    recursion_depth=recursion_depth + 1,\\n                                    max_recursion_depth=max_recursion_depth,\\n                                    dag_bag=dag_bag,\\n                                    visited_external_tis=visited_external_tis,\\n                                )\\n                            )\\n                        visited_external_tis.add(ti_key)\\n        if get_tis:\\n            return tis\\n        tis = tis.all()\\n        if dry_run:\\n            session.expunge_all()\\n            return tis\\n        count = len(tis)\\n        do_it = True\\n        if count == 0:\\n            return 0\\n        if confirm_prompt:\\n            ti_list = \"\\n\".join([str(t) for t in tis])\\n            question = (\\n                \"You are about to delete these {count} tasks:\\n{ti_list}\\nAre you sure? (yes/no): \"\\n            ).format(count=count, ti_list=ti_list)\\n            do_it = utils.helpers.ask_yesno(question)\\n        if do_it:\\n            clear_task_instances(\\n                tis,\\n                session,\\n                dag=self,\\n                activate_dag_runs=False,  \\n            )\\n            self.set_dag_runs_state(\\n                session=session,\\n                start_date=start_date,\\n                end_date=end_date,\\n                state=dag_run_state,\\n                dag_ids=dag_ids,\\n            )\\n        else:\\n            count = 0\\n            print(\"Cancelled, nothing was cleared.\")\\n        session.commit()\\n        return count\\n    @classmethod\\n    def clear_dags(\\n        cls,\\n        dags,\\n        start_date=None,\\n        end_date=None,\\n        only_failed=False,\\n        only_running=False,\\n        confirm_prompt=False,\\n        include_subdags=True,\\n        include_parentdag=False,\\n        dag_run_state=State.RUNNING,\\n        dry_run=False,\\n    ):\\n        all_tis = []\\n        for dag in dags:\\n            tis = dag.clear(\\n                start_date=start_date,\\n                end_date=end_date,\\n                only_failed=only_failed,\\n                only_running=only_running,\\n                confirm_prompt=False,\\n                include_subdags=include_subdags,\\n                include_parentdag=include_parentdag,\\n                dag_run_state=dag_run_state,\\n                dry_run=True,\\n            )\\n            all_tis.extend(tis)\\n        if dry_run:\\n            return all_tis\\n        count = len(all_tis)\\n        do_it = True\\n        if count == 0:\\n            print(\"Nothing to clear.\")\\n            return 0\\n        if confirm_prompt:\\n            ti_list = \"\\n\".join([str(t) for t in all_tis])\\n            question = f\"You are about to delete these {count} tasks:\\n{ti_list}\\nAre you sure? (yes/no): \"\\n            do_it = utils.helpers.ask_yesno(question)\\n        if do_it:\\n            for dag in dags:\\n                dag.clear(\\n                    start_date=start_date,\\n                    end_date=end_date,\\n                    only_failed=only_failed,\\n                    only_running=only_running,\\n                    confirm_prompt=False,\\n                    include_subdags=include_subdags,\\n                    dag_run_state=dag_run_state,\\n                    dry_run=False,\\n                )\\n        else:\\n            count = 0\\n            print(\"Cancelled, nothing was cleared.\")\\n        return count"
  },
  {
    "code": "def map_obj_to_commands(updates, module):\\n    commands = list()\\n    want, have = updates\\n    state = module.params['state']\\n    if state == 'absent' and 'text' in have.keys() and have['text']:\\n        commands.append('no banner %s' % module.params['banner'])\\n    elif state == 'present':\\n        if want['text'] and (want['text'] != have.get('text')):\\n            banner_cmd = 'banner %s' % module.params['banner']\\n            banner_cmd += ' @\\n'\\n            banner_cmd += want['text'].strip()\\n            banner_cmd += '\\n@'\\n            commands.append(banner_cmd)\\n    return commands",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "allowing banner without stripping spaces (#62573)",
    "fixed_code": "def map_obj_to_commands(updates, module):\\n    commands = list()\\n    want, have = updates\\n    state = module.params['state']\\n    if state == 'absent' and 'text' in have.keys() and have['text']:\\n        commands.append('no banner %s' % module.params['banner'])\\n    elif state == 'present':\\n        if want['text'] and (want['text'] != have.get('text')):\\n            banner_cmd = 'banner %s' % module.params['banner']\\n            banner_cmd += ' @\\n'\\n            banner_cmd += want['text'].strip('\\n')\\n            banner_cmd += '\\n@'\\n            commands.append(banner_cmd)\\n    return commands"
  },
  {
    "code": "def get_full_url(self):\\n        return self.__original",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix Issue11703 - urllib2.get_url does not handle fragment in url properly.",
    "fixed_code": "def get_full_url(self):\\n        if self.__fragment:\\n            return '%s\\n        else:\\n            return self.__original"
  },
  {
    "code": "def _make_concat_multiindex(indexes, keys, groupings):\\n    if not _all_indexes_same(indexes):\\n        label_list = []\\n        if len(groupings) == 1:\\n            zipped = [keys]\\n        else:\\n            zipped = zip(*keys)\\n        for hlevel in zipped:\\n            to_concat = []\\n            for k, index in zip(hlevel, indexes):\\n                to_concat.append(np.repeat(k, len(index)))\\n            label_list.append(np.concatenate(to_concat))\\n        label_list.append(np.concatenate(indexes))\\n        return MultiIndex.from_arrays(label_list)\\n    new_index = indexes[0]\\n    n = len(new_index)\\n    levels = [ping.group_index for ping in  groupings]\\n    levels.append(new_index)\\n    labels = []\\n    for hlevel, ping in zip(zip(*keys), groupings):\\n        get_id = ping.reverse_ids.__getitem__\\n        mapped = [get_id(x) for x in hlevel]\\n        labels.append(np.repeat(mapped, n))\\n    labels.append(np.tile(np.arange(n), len(indexes)))\\n    return MultiIndex(levels=levels, labels=labels)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _make_concat_multiindex(indexes, keys, groupings):\\n    if not _all_indexes_same(indexes):\\n        label_list = []\\n        if len(groupings) == 1:\\n            zipped = [keys]\\n        else:\\n            zipped = zip(*keys)\\n        for hlevel in zipped:\\n            to_concat = []\\n            for k, index in zip(hlevel, indexes):\\n                to_concat.append(np.repeat(k, len(index)))\\n            label_list.append(np.concatenate(to_concat))\\n        label_list.append(np.concatenate(indexes))\\n        return MultiIndex.from_arrays(label_list)\\n    new_index = indexes[0]\\n    n = len(new_index)\\n    levels = [ping.group_index for ping in  groupings]\\n    levels.append(new_index)\\n    labels = []\\n    for hlevel, ping in zip(zip(*keys), groupings):\\n        get_id = ping.reverse_ids.__getitem__\\n        mapped = [get_id(x) for x in hlevel]\\n        labels.append(np.repeat(mapped, n))\\n    labels.append(np.tile(np.arange(n), len(indexes)))\\n    return MultiIndex(levels=levels, labels=labels)"
  },
  {
    "code": "def __neg__(self):\\n        return self.__class__(-self.n, normalize=self.normalize, **self.kwds)\\n    @apply_wraps",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __neg__(self):\\n        return self.__class__(-self.n, normalize=self.normalize, **self.kwds)\\n    @apply_wraps"
  },
  {
    "code": "def open(file, flag='c', mode=0o666):\\n    try:\\n        um = _os.umask(0)\\n        _os.umask(um)\\n    except AttributeError:\\n        pass\\n    else:\\n        mode = mode & (~um)\\n    return _Database(file, mode, flag=flag)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def open(file, flag='c', mode=0o666):\\n    try:\\n        um = _os.umask(0)\\n        _os.umask(um)\\n    except AttributeError:\\n        pass\\n    else:\\n        mode = mode & (~um)\\n    return _Database(file, mode, flag=flag)"
  },
  {
    "code": "def _aggregate_simple(self, arg):\\n        values = self.obj.values\\n        result = {}\\n        for k, v in self.primary.indices.iteritems():\\n            result[k] = arg(values.take(v))\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: multi-key groupby now works with multiple functions. starting to add *args, **kwargs. address GH #133",
    "fixed_code": "def _aggregate_simple(self, func, *args, **kwargs):\\n        values = self.obj.values\\n        result = {}\\n        for k, v in self.primary.indices.iteritems():\\n            result[k] = func(values.take(v), *args, **kwargs)\\n        return result"
  },
  {
    "code": "def update(self, kwargs):\\n\\t\\tsentinel = object()  \\n\\t\\tbbox = kwargs.pop(\"bbox\", sentinel)\\n\\t\\tsuper().update(kwargs)\\n\\t\\tif bbox is not sentinel:\\n\\t\\t\\tself.set_bbox(bbox)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Lower Text's FontProperties priority when updating\\n\\nCloses: #16389",
    "fixed_code": "def update(self, kwargs):\\n\\t\\tsentinel = object()  \\n\\t\\tfontproperties = kwargs.pop(\"fontproperties\", sentinel)\\n\\t\\tif fontproperties is not sentinel:\\n\\t\\t\\tself.set_fontproperties(fontproperties)\\n\\t\\tbbox = kwargs.pop(\"bbox\", sentinel)\\n\\t\\tsuper().update(kwargs)\\n\\t\\tif bbox is not sentinel:\\n\\t\\t\\tself.set_bbox(bbox)"
  },
  {
    "code": "def codes_info(self) -> np.ndarray:\\n        ids, _, _ = self.group_info\\n        if self.indexer is not None:\\n            sorter = np.lexsort((ids, self.indexer))\\n            ids = ids[sorter]\\n        return ids\\n    @final",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TYP: GroupBy (#43806)",
    "fixed_code": "def codes_info(self) -> npt.NDArray[np.intp]:\\n        ids, _, _ = self.group_info\\n        if self.indexer is not None:\\n            sorter = np.lexsort((ids, self.indexer))\\n            ids = ids[sorter]\\n            ids = ensure_platform_int(ids)\\n        return ids\\n    @final"
  },
  {
    "code": "def prefetch_related_objects(model_instances, *related_lookups):\\n\\tif len(model_instances) == 0:\\n\\t\\treturn  \\n\\trelated_lookups = normalize_prefetch_lookups(related_lookups)\\n\\tdone_queries = {}\\t\\n\\tauto_lookups = set()  \\n\\tfollowed_descriptors = set()  \\n\\tall_lookups = deque(related_lookups)\\n\\twhile all_lookups:\\n\\t\\tlookup = all_lookups.popleft()\\n\\t\\tif lookup.prefetch_to in done_queries:\\n\\t\\t\\tif lookup.queryset:\\n\\t\\t\\t\\traise ValueError(\"'%s' lookup was already seen with a different queryset. \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\\n\\t\\t\\tcontinue\\n\\t\\tobj_list = model_instances\\n\\t\\tthrough_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\\n\\t\\tfor level, through_attr in enumerate(through_attrs):\\n\\t\\t\\tif len(obj_list) == 0:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tprefetch_to = lookup.get_current_prefetch_to(level)\\n\\t\\t\\tif prefetch_to in done_queries:\\n\\t\\t\\t\\tobj_list = done_queries[prefetch_to]\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tgood_objects = True\\n\\t\\t\\tfor obj in obj_list:\\n\\t\\t\\t\\tif not hasattr(obj, '_prefetched_objects_cache'):\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tobj._prefetched_objects_cache = {}\\n\\t\\t\\t\\t\\texcept (AttributeError, TypeError):\\n\\t\\t\\t\\t\\t\\tgood_objects = False\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tif not good_objects:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tfirst_obj = obj_list[0]\\n\\t\\t\\tprefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr)\\n\\t\\t\\tif not attr_found:\\n\\t\\t\\t\\traise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"parameter to prefetch_related()\" %\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\\n\\t\\t\\tif level == len(through_attrs) - 1 and prefetcher is None:\\n\\t\\t\\t\\traise ValueError(\"'%s' does not resolve to an item that supports \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"prefetching - this is an invalid parameter to \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"prefetch_related().\" % lookup.prefetch_through)\\n\\t\\t\\tif prefetcher is not None and not is_fetched:\\n\\t\\t\\t\\tobj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)\\n\\t\\t\\t\\tif not (lookup in auto_lookups and descriptor in followed_descriptors):\\n\\t\\t\\t\\t\\tdone_queries[prefetch_to] = obj_list\\n\\t\\t\\t\\t\\tnew_lookups = normalize_prefetch_lookups(additional_lookups, prefetch_to)\\n\\t\\t\\t\\t\\tauto_lookups.update(new_lookups)\\n\\t\\t\\t\\t\\tall_lookups.extendleft(new_lookups)\\n\\t\\t\\t\\tfollowed_descriptors.add(descriptor)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tnew_obj_list = []\\n\\t\\t\\t\\tfor obj in obj_list:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tnew_obj = getattr(obj, through_attr)\\n\\t\\t\\t\\t\\texcept exceptions.ObjectDoesNotExist:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\tif new_obj is None:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\tif isinstance(new_obj, list):\\n\\t\\t\\t\\t\\t\\tnew_obj_list.extend(new_obj)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tnew_obj_list.append(new_obj)\\n\\t\\t\\t\\tobj_list = new_obj_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def prefetch_related_objects(model_instances, *related_lookups):\\n\\tif len(model_instances) == 0:\\n\\t\\treturn  \\n\\trelated_lookups = normalize_prefetch_lookups(related_lookups)\\n\\tdone_queries = {}\\t\\n\\tauto_lookups = set()  \\n\\tfollowed_descriptors = set()  \\n\\tall_lookups = deque(related_lookups)\\n\\twhile all_lookups:\\n\\t\\tlookup = all_lookups.popleft()\\n\\t\\tif lookup.prefetch_to in done_queries:\\n\\t\\t\\tif lookup.queryset:\\n\\t\\t\\t\\traise ValueError(\"'%s' lookup was already seen with a different queryset. \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\\n\\t\\t\\tcontinue\\n\\t\\tobj_list = model_instances\\n\\t\\tthrough_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\\n\\t\\tfor level, through_attr in enumerate(through_attrs):\\n\\t\\t\\tif len(obj_list) == 0:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tprefetch_to = lookup.get_current_prefetch_to(level)\\n\\t\\t\\tif prefetch_to in done_queries:\\n\\t\\t\\t\\tobj_list = done_queries[prefetch_to]\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tgood_objects = True\\n\\t\\t\\tfor obj in obj_list:\\n\\t\\t\\t\\tif not hasattr(obj, '_prefetched_objects_cache'):\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tobj._prefetched_objects_cache = {}\\n\\t\\t\\t\\t\\texcept (AttributeError, TypeError):\\n\\t\\t\\t\\t\\t\\tgood_objects = False\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tif not good_objects:\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tfirst_obj = obj_list[0]\\n\\t\\t\\tprefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr)\\n\\t\\t\\tif not attr_found:\\n\\t\\t\\t\\traise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"parameter to prefetch_related()\" %\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\\n\\t\\t\\tif level == len(through_attrs) - 1 and prefetcher is None:\\n\\t\\t\\t\\traise ValueError(\"'%s' does not resolve to an item that supports \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"prefetching - this is an invalid parameter to \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"prefetch_related().\" % lookup.prefetch_through)\\n\\t\\t\\tif prefetcher is not None and not is_fetched:\\n\\t\\t\\t\\tobj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)\\n\\t\\t\\t\\tif not (lookup in auto_lookups and descriptor in followed_descriptors):\\n\\t\\t\\t\\t\\tdone_queries[prefetch_to] = obj_list\\n\\t\\t\\t\\t\\tnew_lookups = normalize_prefetch_lookups(additional_lookups, prefetch_to)\\n\\t\\t\\t\\t\\tauto_lookups.update(new_lookups)\\n\\t\\t\\t\\t\\tall_lookups.extendleft(new_lookups)\\n\\t\\t\\t\\tfollowed_descriptors.add(descriptor)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tnew_obj_list = []\\n\\t\\t\\t\\tfor obj in obj_list:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tnew_obj = getattr(obj, through_attr)\\n\\t\\t\\t\\t\\texcept exceptions.ObjectDoesNotExist:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\tif new_obj is None:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\tif isinstance(new_obj, list):\\n\\t\\t\\t\\t\\t\\tnew_obj_list.extend(new_obj)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tnew_obj_list.append(new_obj)\\n\\t\\t\\t\\tobj_list = new_obj_list"
  },
  {
    "code": "def do_extends(parser, token):\\n    bits = token.contents.split()\\n    if len(bits) != 2:\\n        raise TemplateSyntaxError, \"'%s' takes one argument\" % bits[0]\\n    parent_name, parent_name_expr = None, None\\n    if bits[1][0] in ('\"', \"'\") and bits[1][-1] == bits[1][0]:\\n        parent_name = bits[1][1:-1]\\n    else:\\n        parent_name_expr = parser.compile_filter(bits[1])\\n    nodelist = parser.parse()\\n    if nodelist.get_nodes_by_type(ExtendsNode):\\n        raise TemplateSyntaxError, \"'%s' cannot appear more than once in the same template\" % bits[0]\\n    return ExtendsNode(nodelist, parent_name, parent_name_expr)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #10094 -- Fixed the `include` and `extends` template tags to work with filenames with spaces, patch from mcroydon.",
    "fixed_code": "def do_extends(parser, token):\\n    bits = token.split_contents()\\n    if len(bits) != 2:\\n        raise TemplateSyntaxError, \"'%s' takes one argument\" % bits[0]\\n    parent_name, parent_name_expr = None, None\\n    if bits[1][0] in ('\"', \"'\") and bits[1][-1] == bits[1][0]:\\n        parent_name = bits[1][1:-1]\\n    else:\\n        parent_name_expr = parser.compile_filter(bits[1])\\n    nodelist = parser.parse()\\n    if nodelist.get_nodes_by_type(ExtendsNode):\\n        raise TemplateSyntaxError, \"'%s' cannot appear more than once in the same template\" % bits[0]\\n    return ExtendsNode(nodelist, parent_name, parent_name_expr)"
  },
  {
    "code": "def create_use_drop(self):\\n        idx = Index(list(range(1000 * 1000)))\\n        idx._engine",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PERF: make RangeIndex iterate over ._range (#35676)",
    "fixed_code": "def create_use_drop(self):\\n        idx = Index(list(range(1_000_000)))\\n        idx._engine"
  },
  {
    "code": "def _form_blocks(arrays: list[ArrayLike], consolidate: bool) -> list[Block]:\\n    tuples = list(enumerate(arrays))\\n    if not consolidate:\\n        nbs = _tuples_to_blocks_no_consolidate(tuples, dtype=None)\\n        return nbs\\n    grouper = itertools.groupby(tuples, _grouping_func)\\n    nbs = []\\n    for (_, dtype), tup_block in grouper:\\n        block_type = get_block_type(None, dtype)\\n        if isinstance(dtype, np.dtype):\\n            is_dtlike = dtype.kind in [\"m\", \"M\"]\\n            if issubclass(dtype.type, (str, bytes)):\\n                dtype = np.dtype(object)\\n            values, placement = _stack_arrays(list(tup_block), dtype)\\n            if is_dtlike:\\n                values = ensure_wrapped_if_datetimelike(values)\\n            blk = block_type(values, placement=BlockPlacement(placement), ndim=2)\\n            nbs.append(blk)\\n        elif is_1d_only_ea_dtype(dtype):\\n            dtype_blocks = [\\n                block_type(x[1], placement=BlockPlacement(x[0]), ndim=2)\\n                for x in tup_block\\n            ]\\n            nbs.extend(dtype_blocks)\\n        else:\\n            dtype_blocks = [\\n                block_type(\\n                    ensure_block_shape(x[1], 2), placement=BlockPlacement(x[0]), ndim=2\\n                )\\n                for x in tup_block\\n            ]\\n            nbs.extend(dtype_blocks)\\n    return nbs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _form_blocks(arrays: list[ArrayLike], consolidate: bool) -> list[Block]:\\n    tuples = list(enumerate(arrays))\\n    if not consolidate:\\n        nbs = _tuples_to_blocks_no_consolidate(tuples, dtype=None)\\n        return nbs\\n    grouper = itertools.groupby(tuples, _grouping_func)\\n    nbs = []\\n    for (_, dtype), tup_block in grouper:\\n        block_type = get_block_type(None, dtype)\\n        if isinstance(dtype, np.dtype):\\n            is_dtlike = dtype.kind in [\"m\", \"M\"]\\n            if issubclass(dtype.type, (str, bytes)):\\n                dtype = np.dtype(object)\\n            values, placement = _stack_arrays(list(tup_block), dtype)\\n            if is_dtlike:\\n                values = ensure_wrapped_if_datetimelike(values)\\n            blk = block_type(values, placement=BlockPlacement(placement), ndim=2)\\n            nbs.append(blk)\\n        elif is_1d_only_ea_dtype(dtype):\\n            dtype_blocks = [\\n                block_type(x[1], placement=BlockPlacement(x[0]), ndim=2)\\n                for x in tup_block\\n            ]\\n            nbs.extend(dtype_blocks)\\n        else:\\n            dtype_blocks = [\\n                block_type(\\n                    ensure_block_shape(x[1], 2), placement=BlockPlacement(x[0]), ndim=2\\n                )\\n                for x in tup_block\\n            ]\\n            nbs.extend(dtype_blocks)\\n    return nbs"
  },
  {
    "code": "def get_slice_bound(self, label, side: str_t, kind=None) -> int:\\n        assert kind in [\"loc\", \"getitem\", None]\\n        if side not in (\"left\", \"right\"):\\n            raise ValueError(\\n                \"Invalid value for side kwarg, must be either \"\\n                f\"'left' or 'right': {side}\"\\n            )\\n        original_label = label\\n        label = self._maybe_cast_slice_bound(label, side)\\n        try:\\n            slc = self.get_loc(label)\\n        except KeyError as err:\\n            try:\\n                return self._searchsorted_monotonic(label, side)\\n            except ValueError:\\n                raise err\\n        if isinstance(slc, np.ndarray):\\n            if is_bool_dtype(slc):\\n                slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\\n            else:\\n                slc = lib.maybe_indices_to_slice(\\n                    slc.astype(np.intp, copy=False), len(self)\\n                )\\n            if isinstance(slc, np.ndarray):\\n                raise KeyError(\\n                    f\"Cannot get {side} slice bound for non-unique \"\\n                    f\"label: {repr(original_label)}\"\\n                )\\n        if isinstance(slc, slice):\\n            if side == \"left\":\\n                return slc.start\\n            else:\\n                return slc.stop\\n        else:\\n            if side == \"right\":\\n                return slc + 1\\n            else:\\n                return slc",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_slice_bound(self, label, side: str_t, kind=None) -> int:\\n        assert kind in [\"loc\", \"getitem\", None]\\n        if side not in (\"left\", \"right\"):\\n            raise ValueError(\\n                \"Invalid value for side kwarg, must be either \"\\n                f\"'left' or 'right': {side}\"\\n            )\\n        original_label = label\\n        label = self._maybe_cast_slice_bound(label, side)\\n        try:\\n            slc = self.get_loc(label)\\n        except KeyError as err:\\n            try:\\n                return self._searchsorted_monotonic(label, side)\\n            except ValueError:\\n                raise err\\n        if isinstance(slc, np.ndarray):\\n            if is_bool_dtype(slc):\\n                slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\\n            else:\\n                slc = lib.maybe_indices_to_slice(\\n                    slc.astype(np.intp, copy=False), len(self)\\n                )\\n            if isinstance(slc, np.ndarray):\\n                raise KeyError(\\n                    f\"Cannot get {side} slice bound for non-unique \"\\n                    f\"label: {repr(original_label)}\"\\n                )\\n        if isinstance(slc, slice):\\n            if side == \"left\":\\n                return slc.start\\n            else:\\n                return slc.stop\\n        else:\\n            if side == \"right\":\\n                return slc + 1\\n            else:\\n                return slc"
  },
  {
    "code": "def _strptime(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\\n    global _TimeRE_cache, _regex_cache\\n    with _cache_lock:\\n        if _getlang() != _TimeRE_cache.locale_time.lang:\\n            _TimeRE_cache = TimeRE()\\n            _regex_cache.clear()\\n        if len(_regex_cache) > _CACHE_MAX_SIZE:\\n            _regex_cache.clear()\\n        locale_time = _TimeRE_cache.locale_time\\n        format_regex = _regex_cache.get(format)\\n        if not format_regex:\\n            try:\\n                format_regex = _TimeRE_cache.compile(format)\\n            except KeyError as err:\\n                bad_directive = err.args[0]\\n                if bad_directive == \"\\\\\":\\n                    bad_directive = \"%\"\\n                del err\\n                raise ValueError(\"'%s' is a bad directive in format '%s'\" %\\n                                    (bad_directive, format))\\n            except IndexError:\\n                raise ValueError(\"stray %% in format '%s'\" % format)\\n            _regex_cache[format] = format_regex\\n    found = format_regex.match(data_string)\\n    if not found:\\n        raise ValueError(\"time data %r does not match format %r\" %\\n                         (data_string, format))\\n    if len(data_string) != found.end():\\n        raise ValueError(\"unconverted data remains: %s\" %\\n                          data_string[found.end():])\\n    year = 1900\\n    month = day = 1\\n    hour = minute = second = fraction = 0\\n    tz = -1\\n    week_of_year = -1\\n    week_of_year_start = -1\\n    weekday = julian = -1\\n    found_dict = found.groupdict()\\n    for group_key in found_dict.keys():\\n        if group_key == 'y':\\n            year = int(found_dict['y'])\\n            if year <= 68:\\n                year += 2000\\n            else:\\n                year += 1900\\n        elif group_key == 'Y':\\n            year = int(found_dict['Y'])\\n        elif group_key == 'm':\\n            month = int(found_dict['m'])\\n        elif group_key == 'B':\\n            month = locale_time.f_month.index(found_dict['B'].lower())\\n        elif group_key == 'b':\\n            month = locale_time.a_month.index(found_dict['b'].lower())\\n        elif group_key == 'd':\\n            day = int(found_dict['d'])\\n        elif group_key == 'H':\\n            hour = int(found_dict['H'])\\n        elif group_key == 'I':\\n            hour = int(found_dict['I'])\\n            ampm = found_dict.get('p', '').lower()\\n            if ampm in ('', locale_time.am_pm[0]):\\n                if hour == 12:\\n                    hour = 0\\n            elif ampm == locale_time.am_pm[1]:\\n                if hour != 12:\\n                    hour += 12\\n        elif group_key == 'M':\\n            minute = int(found_dict['M'])\\n        elif group_key == 'S':\\n            second = int(found_dict['S'])\\n        elif group_key == 'f':\\n            s = found_dict['f']\\n            s += \"0\" * (6 - len(s))\\n            fraction = int(s)\\n        elif group_key == 'A':\\n            weekday = locale_time.f_weekday.index(found_dict['A'].lower())\\n        elif group_key == 'a':\\n            weekday = locale_time.a_weekday.index(found_dict['a'].lower())\\n        elif group_key == 'w':\\n            weekday = int(found_dict['w'])\\n            if weekday == 0:\\n                weekday = 6\\n            else:\\n                weekday -= 1\\n        elif group_key == 'j':\\n            julian = int(found_dict['j'])\\n        elif group_key in ('U', 'W'):\\n            week_of_year = int(found_dict[group_key])\\n            if group_key == 'U':\\n                week_of_year_start = 6\\n            else:\\n                week_of_year_start = 0\\n        elif group_key == 'Z':\\n            found_zone = found_dict['Z'].lower()\\n            for value, tz_values in enumerate(locale_time.timezone):\\n                if found_zone in tz_values:\\n                    if (time.tzname[0] == time.tzname[1] and\\n                       time.daylight and found_zone not in (\"utc\", \"gmt\")):\\n                        break\\n                    else:\\n                        tz = value\\n                        break\\n    if julian == -1 and week_of_year != -1 and weekday != -1:\\n        week_starts_Mon = True if week_of_year_start == 0 else False\\n        julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,\\n                                            week_starts_Mon)\\n    if julian == -1:\\n        julian = datetime_date(year, month, day).toordinal() - \\\\n                  datetime_date(year, 1, 1).toordinal() + 1\\n    else:  \\n        datetime_result = datetime_date.fromordinal((julian - 1) + datetime_date(year, 1, 1).toordinal())\\n        year = datetime_result.year\\n        month = datetime_result.month\\n        day = datetime_result.day\\n    if weekday == -1:\\n        weekday = datetime_date(year, month, day).weekday()\\n    return (time.struct_time((year, month, day,\\n                              hour, minute, second,\\n                              weekday, julian, tz)), fraction)\\ndef _strptime_time(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\\n    return _strptime(data_string, format)[0]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make sure time.strptime only accepts strings (and document the fact like strftime). Already didn't accept bytes but make the check earlier. This also lifts the limitation of requiring ASCII.\\n\\nCloses issue #5236. Thanks Tennessee Leeuwenburg.",
    "fixed_code": "def _strptime(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\\n    for index, arg in enumerate([data_string, format]):\\n        if not isinstance(arg, str):\\n            msg = \"strptime() argument {} must be str, not {}\"\\n            raise TypeError(msg.format(arg, index))\\n    global _TimeRE_cache, _regex_cache\\n    with _cache_lock:\\n        if _getlang() != _TimeRE_cache.locale_time.lang:\\n            _TimeRE_cache = TimeRE()\\n            _regex_cache.clear()\\n        if len(_regex_cache) > _CACHE_MAX_SIZE:\\n            _regex_cache.clear()\\n        locale_time = _TimeRE_cache.locale_time\\n        format_regex = _regex_cache.get(format)\\n        if not format_regex:\\n            try:\\n                format_regex = _TimeRE_cache.compile(format)\\n            except KeyError as err:\\n                bad_directive = err.args[0]\\n                if bad_directive == \"\\\\\":\\n                    bad_directive = \"%\"\\n                del err\\n                raise ValueError(\"'%s' is a bad directive in format '%s'\" %\\n                                    (bad_directive, format))\\n            except IndexError:\\n                raise ValueError(\"stray %% in format '%s'\" % format)\\n            _regex_cache[format] = format_regex\\n    found = format_regex.match(data_string)\\n    if not found:\\n        raise ValueError(\"time data %r does not match format %r\" %\\n                         (data_string, format))\\n    if len(data_string) != found.end():\\n        raise ValueError(\"unconverted data remains: %s\" %\\n                          data_string[found.end():])\\n    year = 1900\\n    month = day = 1\\n    hour = minute = second = fraction = 0\\n    tz = -1\\n    week_of_year = -1\\n    week_of_year_start = -1\\n    weekday = julian = -1\\n    found_dict = found.groupdict()\\n    for group_key in found_dict.keys():\\n        if group_key == 'y':\\n            year = int(found_dict['y'])\\n            if year <= 68:\\n                year += 2000\\n            else:\\n                year += 1900\\n        elif group_key == 'Y':\\n            year = int(found_dict['Y'])\\n        elif group_key == 'm':\\n            month = int(found_dict['m'])\\n        elif group_key == 'B':\\n            month = locale_time.f_month.index(found_dict['B'].lower())\\n        elif group_key == 'b':\\n            month = locale_time.a_month.index(found_dict['b'].lower())\\n        elif group_key == 'd':\\n            day = int(found_dict['d'])\\n        elif group_key == 'H':\\n            hour = int(found_dict['H'])\\n        elif group_key == 'I':\\n            hour = int(found_dict['I'])\\n            ampm = found_dict.get('p', '').lower()\\n            if ampm in ('', locale_time.am_pm[0]):\\n                if hour == 12:\\n                    hour = 0\\n            elif ampm == locale_time.am_pm[1]:\\n                if hour != 12:\\n                    hour += 12\\n        elif group_key == 'M':\\n            minute = int(found_dict['M'])\\n        elif group_key == 'S':\\n            second = int(found_dict['S'])\\n        elif group_key == 'f':\\n            s = found_dict['f']\\n            s += \"0\" * (6 - len(s))\\n            fraction = int(s)\\n        elif group_key == 'A':\\n            weekday = locale_time.f_weekday.index(found_dict['A'].lower())\\n        elif group_key == 'a':\\n            weekday = locale_time.a_weekday.index(found_dict['a'].lower())\\n        elif group_key == 'w':\\n            weekday = int(found_dict['w'])\\n            if weekday == 0:\\n                weekday = 6\\n            else:\\n                weekday -= 1\\n        elif group_key == 'j':\\n            julian = int(found_dict['j'])\\n        elif group_key in ('U', 'W'):\\n            week_of_year = int(found_dict[group_key])\\n            if group_key == 'U':\\n                week_of_year_start = 6\\n            else:\\n                week_of_year_start = 0\\n        elif group_key == 'Z':\\n            found_zone = found_dict['Z'].lower()\\n            for value, tz_values in enumerate(locale_time.timezone):\\n                if found_zone in tz_values:\\n                    if (time.tzname[0] == time.tzname[1] and\\n                       time.daylight and found_zone not in (\"utc\", \"gmt\")):\\n                        break\\n                    else:\\n                        tz = value\\n                        break\\n    if julian == -1 and week_of_year != -1 and weekday != -1:\\n        week_starts_Mon = True if week_of_year_start == 0 else False\\n        julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,\\n                                            week_starts_Mon)\\n    if julian == -1:\\n        julian = datetime_date(year, month, day).toordinal() - \\\\n                  datetime_date(year, 1, 1).toordinal() + 1\\n    else:  \\n        datetime_result = datetime_date.fromordinal((julian - 1) + datetime_date(year, 1, 1).toordinal())\\n        year = datetime_result.year\\n        month = datetime_result.month\\n        day = datetime_result.day\\n    if weekday == -1:\\n        weekday = datetime_date(year, month, day).weekday()\\n    return (time.struct_time((year, month, day,\\n                              hour, minute, second,\\n                              weekday, julian, tz)), fraction)\\ndef _strptime_time(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\\n    return _strptime(data_string, format)[0]"
  },
  {
    "code": "def ping_google(sitemap_url=None, ping_url=PING_URL):\\n\\tif sitemap_url is None:\\n\\t\\ttry:\\n\\t\\t\\tsitemap_url = urlresolvers.reverse('django.contrib.sitemaps.views.index')\\n\\t\\texcept urlresolvers.NoReverseMatch:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tsitemap_url = urlresolvers.reverse('django.contrib.sitemaps.views.sitemap')\\n\\t\\t\\texcept urlresolvers.NoReverseMatch:\\n\\t\\t\\t\\tpass\\n\\tif sitemap_url is None:\\n\\t\\traise SitemapNotFound(\"You didn't provide a sitemap_url, and the sitemap URL couldn't be auto-detected.\")\\n\\tcurrent_site = Site.objects.get_current()\\n\\turl = \"http://%s%s\" % (current_site.domain, sitemap_url)\\n\\tparams = urlencode({'sitemap': url})\\n\\turlopen(\"%s?%s\" % (ping_url, params))\\nclass Sitemap(object):\\n\\tlimit = 50000\\n\\tprotocol = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ping_google(sitemap_url=None, ping_url=PING_URL):\\n\\tif sitemap_url is None:\\n\\t\\ttry:\\n\\t\\t\\tsitemap_url = urlresolvers.reverse('django.contrib.sitemaps.views.index')\\n\\t\\texcept urlresolvers.NoReverseMatch:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tsitemap_url = urlresolvers.reverse('django.contrib.sitemaps.views.sitemap')\\n\\t\\t\\texcept urlresolvers.NoReverseMatch:\\n\\t\\t\\t\\tpass\\n\\tif sitemap_url is None:\\n\\t\\traise SitemapNotFound(\"You didn't provide a sitemap_url, and the sitemap URL couldn't be auto-detected.\")\\n\\tcurrent_site = Site.objects.get_current()\\n\\turl = \"http://%s%s\" % (current_site.domain, sitemap_url)\\n\\tparams = urlencode({'sitemap': url})\\n\\turlopen(\"%s?%s\" % (ping_url, params))\\nclass Sitemap(object):\\n\\tlimit = 50000\\n\\tprotocol = None"
  },
  {
    "code": "def transform(\\n    obj: FrameOrSeries, func: AggFuncType, axis: Axis, *args, **kwargs\\n) -> FrameOrSeriesUnion:\\n    is_series = obj.ndim == 1\\n    if obj._get_axis_number(axis) == 1:\\n        assert not is_series\\n        return transform(obj.T, func, 0, *args, **kwargs).T\\n    if is_list_like(func) and not is_dict_like(func):\\n        func = cast(List[AggFuncTypeBase], func)\\n        if is_series:\\n            func = {com.get_callable_name(v) or v: v for v in func}\\n        else:\\n            func = {col: func for col in obj}\\n    if is_dict_like(func):\\n        func = cast(Dict[Label, Union[AggFuncTypeBase, List[AggFuncTypeBase]]], func)\\n        return transform_dict_like(obj, func, *args, **kwargs)\\n    func = cast(AggFuncTypeBase, func)\\n    try:\\n        result = transform_str_or_callable(obj, func, *args, **kwargs)\\n    except Exception:\\n        raise ValueError(\"Transform function failed\")\\n    if isinstance(result, (ABCSeries, ABCDataFrame)) and result.empty:\\n        raise ValueError(\"Transform function failed\")\\n    if not isinstance(result, (ABCSeries, ABCDataFrame)) or not result.index.equals(\\n        obj.index\\n    ):\\n        raise ValueError(\"Function did not transform\")\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def transform(\\n    obj: FrameOrSeries, func: AggFuncType, axis: Axis, *args, **kwargs\\n) -> FrameOrSeriesUnion:\\n    is_series = obj.ndim == 1\\n    if obj._get_axis_number(axis) == 1:\\n        assert not is_series\\n        return transform(obj.T, func, 0, *args, **kwargs).T\\n    if is_list_like(func) and not is_dict_like(func):\\n        func = cast(List[AggFuncTypeBase], func)\\n        if is_series:\\n            func = {com.get_callable_name(v) or v: v for v in func}\\n        else:\\n            func = {col: func for col in obj}\\n    if is_dict_like(func):\\n        func = cast(Dict[Label, Union[AggFuncTypeBase, List[AggFuncTypeBase]]], func)\\n        return transform_dict_like(obj, func, *args, **kwargs)\\n    func = cast(AggFuncTypeBase, func)\\n    try:\\n        result = transform_str_or_callable(obj, func, *args, **kwargs)\\n    except Exception:\\n        raise ValueError(\"Transform function failed\")\\n    if isinstance(result, (ABCSeries, ABCDataFrame)) and result.empty:\\n        raise ValueError(\"Transform function failed\")\\n    if not isinstance(result, (ABCSeries, ABCDataFrame)) or not result.index.equals(\\n        obj.index\\n    ):\\n        raise ValueError(\"Function did not transform\")\\n    return result"
  },
  {
    "code": "def _get_col_names(colspec, columns):\\n    colset = set(columns)\\n    colnames = []\\n    for c in colspec:\\n        if c in colset:\\n            colnames.append(str(c))\\n        elif isinstance(c, int):\\n            colnames.append(str(columns[c]))\\n    return colnames",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: change default header names in read_* functions from X.1, X.2, ... to X0, X1, ... close #2000",
    "fixed_code": "def _get_col_names(colspec, columns):\\n    colset = set(columns)\\n    colnames = []\\n    for c in colspec:\\n        if c in colset:\\n            colnames.append(c)\\n        elif isinstance(c, int):\\n            colnames.append(columns[c])\\n    return colnames"
  },
  {
    "code": "def set_script(fmg, script_name, script_type, script_content, script_desc, script_target, adom):\\n    datagram = {\\n        'content': script_content,\\n        'desc': script_desc,\\n        'name': script_name,\\n        'target': script_target,\\n        'type': script_type,\\n    }\\n    url = '/dvmdb/adom/{adom}/script/'.format(adom=adom)\\n    response = fmg.set(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_script (#52786)",
    "fixed_code": "def set_script(fmgr, paramgram):\\n    datagram = {\\n        'content': paramgram[\"script_content\"],\\n        'desc': paramgram[\"script_description\"],\\n        'name': paramgram[\"script_name\"],\\n        'target': paramgram[\"script_target\"],\\n        'type': paramgram[\"script_type\"],\\n    }\\n    url = '/dvmdb/adom/{adom}/script/'.format(adom=paramgram[\"adom\"])\\n    response = fmgr.process_request(url, datagram, FMGRMethods.SET)\\n    return response"
  },
  {
    "code": "def purge():\\n    \"Clear the regular expression caches\"\\n    _compile_typed.cache_clear()\\n    _compile_repl.cache_clear()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue 13227: Option to make the lru_cache() type specific (suggested by Andrew Koenig).",
    "fixed_code": "def purge():\\n    \"Clear the regular expression caches\"\\n    _compile.cache_clear()\\n    _compile_repl.cache_clear()"
  },
  {
    "code": "def min(self):\\n        return capi.get_band_minimum(self._ptr, byref(c_int()))\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25734 -- Made GDALBand min and max properties use GDALComputeRasterStatistics.\\n\\nThanks Sergey Fedoseev and Tim Graham for the review.",
    "fixed_code": "def min(self):\\n        return self.statistics()[0]\\n    @property"
  },
  {
    "code": "def copy_sign(self, other):\\n        return _dec_from_triple(other._sign, self._int,\\n                                self._exp, self._is_special)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #7633: Context method in the decimal module (with the exception of the 'canonical' and 'is_canonical' methods) now consistently accept integer arguments wherever a Decimal instance is accepted.  Thanks Juan Jos\u00e9 Conti for the patch.",
    "fixed_code": "def copy_sign(self, other):\\n        other = _convert_other(other, raiseit=True)\\n        return _dec_from_triple(other._sign, self._int,\\n                                self._exp, self._is_special)"
  },
  {
    "code": "def get_loc(self, key, method=None, tolerance=None):\\n\\t\\tif isinstance(key, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tloc = self._get_string_slice(key)\\n\\t\\t\\t\\treturn loc\\n\\t\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\t\\tpass\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tasdt, reso = parse_time_string(key, self.freq)\\n\\t\\t\\texcept DateParseError:\\n\\t\\t\\t\\traise KeyError(f\"Cannot interpret '{key}' as period\")\\n\\t\\t\\tgrp = resolution.Resolution.get_freq_group(reso)\\n\\t\\t\\tfreqn = resolution.get_freq_group(self.freq)\\n\\t\\t\\tassert grp >= freqn\\n\\t\\t\\tif grp == freqn:\\n\\t\\t\\t\\tkey = Period(asdt, freq=self.freq)\\n\\t\\t\\t\\tloc = self.get_loc(key, method=method, tolerance=tolerance)\\n\\t\\t\\t\\treturn loc\\n\\t\\t\\telif method is None:\\n\\t\\t\\t\\traise KeyError(key)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tkey = asdt\\n\\t\\telif is_integer(key):\\n\\t\\t\\traise KeyError(key)\\n\\t\\ttry:\\n\\t\\t\\tkey = Period(key, freq=self.freq)\\n\\t\\texcept ValueError:\\n\\t\\t\\tif is_list_like(key):\\n\\t\\t\\t\\traise TypeError(f\"'{key}' is an invalid key\")\\n\\t\\t\\traise KeyError(key)\\n\\t\\tordinal = key.ordinal if key is not NaT else key.value\\n\\t\\ttry:\\n\\t\\t\\treturn self._engine.get_loc(ordinal)\\n\\t\\texcept KeyError:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif tolerance is not None:\\n\\t\\t\\t\\t\\ttolerance = self._convert_tolerance(tolerance, np.asarray(key))\\n\\t\\t\\t\\treturn self._int64index.get_loc(ordinal, method, tolerance)\\n\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\traise KeyError(key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_loc(self, key, method=None, tolerance=None):\\n\\t\\tif isinstance(key, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tloc = self._get_string_slice(key)\\n\\t\\t\\t\\treturn loc\\n\\t\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\t\\tpass\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tasdt, reso = parse_time_string(key, self.freq)\\n\\t\\t\\texcept DateParseError:\\n\\t\\t\\t\\traise KeyError(f\"Cannot interpret '{key}' as period\")\\n\\t\\t\\tgrp = resolution.Resolution.get_freq_group(reso)\\n\\t\\t\\tfreqn = resolution.get_freq_group(self.freq)\\n\\t\\t\\tassert grp >= freqn\\n\\t\\t\\tif grp == freqn:\\n\\t\\t\\t\\tkey = Period(asdt, freq=self.freq)\\n\\t\\t\\t\\tloc = self.get_loc(key, method=method, tolerance=tolerance)\\n\\t\\t\\t\\treturn loc\\n\\t\\t\\telif method is None:\\n\\t\\t\\t\\traise KeyError(key)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tkey = asdt\\n\\t\\telif is_integer(key):\\n\\t\\t\\traise KeyError(key)\\n\\t\\ttry:\\n\\t\\t\\tkey = Period(key, freq=self.freq)\\n\\t\\texcept ValueError:\\n\\t\\t\\tif is_list_like(key):\\n\\t\\t\\t\\traise TypeError(f\"'{key}' is an invalid key\")\\n\\t\\t\\traise KeyError(key)\\n\\t\\tordinal = key.ordinal if key is not NaT else key.value\\n\\t\\ttry:\\n\\t\\t\\treturn self._engine.get_loc(ordinal)\\n\\t\\texcept KeyError:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif tolerance is not None:\\n\\t\\t\\t\\t\\ttolerance = self._convert_tolerance(tolerance, np.asarray(key))\\n\\t\\t\\t\\treturn self._int64index.get_loc(ordinal, method, tolerance)\\n\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\traise KeyError(key)"
  },
  {
    "code": "def euclidean_distances_argmin(X, Y=None, axis=1,\\n                               chunk_x_size=None, chunk_y_size=None,\\n                               return_distances=False, squared=False):\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        chunk_x_size, chunk_y_size = chunk_y_size, chunk_x_size\\n    if chunk_x_size is None and chunk_y_size is None:\\n        chunk_num = 50\\n        if X.shape[0] >= Y.shape[0] / 2 and Y.shape[0] >= X.shape[0] / 2:\\n            chunk_x_size = max(int(X.shape[0] / 7), 1)\\n            chunk_y_size = max(int(Y.shape[0] / 7), 1)\\n        elif X.shape[0] > Y.shape[0]:\\n            chunk_x_size = max(int(X.shape[0] / chunk_num), 1)\\n            chunk_y_size = Y.shape[0]\\n        else:\\n            chunk_x_size = X.shape[0]\\n            chunk_y_size = max(int(Y.shape[0] / chunk_num), 1)\\n    if chunk_x_size is None or chunk_x_size <= 0:\\n        chunk_x_size = X.shape[0]\\n    if chunk_y_size is None or chunk_y_size <= 0:\\n        chunk_y_size = Y.shape[0]\\n    n_chunks_x = (X.shape[0] - 1) // chunk_x_size + 1\\n    n_chunks_y = (Y.shape[0] - 1) // chunk_y_size + 1\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(float('inf'))\\n    for chunk_x in range(n_chunks_x):\\n        x_sind = chunk_x * chunk_x_size\\n        x_eind = min((chunk_x + 1) * chunk_x_size, X.shape[0])\\n        X_chunk = X[x_sind:x_eind, :]\\n        for chunk_y in range(n_chunks_y):\\n            y_sind = chunk_y * chunk_y_size\\n            y_eind = (chunk_y + 1) * chunk_y_size\\n            Y_chunk = Y[y_sind:y_eind, :]\\n            tvar = np.dot(X_chunk, Y_chunk.T)\\n            tvar *= -2\\n            tvar += (X_chunk * X_chunk).sum(axis=1)[:, np.newaxis]\\n            tvar += (Y_chunk * Y_chunk).sum(axis=1)[np.newaxis, :]\\n            np.maximum(tvar, 0, tvar)\\n            min_indices = tvar.argmin(axis=1)\\n            min_values = tvar[range(x_eind - x_sind), min_indices]\\n            flags = values[x_sind:x_eind] > min_values\\n            indices[x_sind:x_eind] = np.where(\\n                flags, min_indices + y_sind, indices[x_sind:x_eind])\\n            values[x_sind:x_eind] = np.where(\\n                flags, min_values, values[x_sind:x_eind])\\n    if return_distances:\\n        if not squared:\\n            values = np.sqrt(values)\\n        return indices, values\\n    else:\\n        return indices",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def euclidean_distances_argmin(X, Y=None, axis=1,\\n                               chunk_x_size=None, chunk_y_size=None,\\n                               return_distances=False, squared=False):\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        chunk_x_size, chunk_y_size = chunk_y_size, chunk_x_size\\n    if chunk_x_size is None and chunk_y_size is None:\\n        chunk_num = 50\\n        if X.shape[0] >= Y.shape[0] / 2 and Y.shape[0] >= X.shape[0] / 2:\\n            chunk_x_size = max(int(X.shape[0] / 7), 1)\\n            chunk_y_size = max(int(Y.shape[0] / 7), 1)\\n        elif X.shape[0] > Y.shape[0]:\\n            chunk_x_size = max(int(X.shape[0] / chunk_num), 1)\\n            chunk_y_size = Y.shape[0]\\n        else:\\n            chunk_x_size = X.shape[0]\\n            chunk_y_size = max(int(Y.shape[0] / chunk_num), 1)\\n    if chunk_x_size is None or chunk_x_size <= 0:\\n        chunk_x_size = X.shape[0]\\n    if chunk_y_size is None or chunk_y_size <= 0:\\n        chunk_y_size = Y.shape[0]\\n    n_chunks_x = (X.shape[0] - 1) // chunk_x_size + 1\\n    n_chunks_y = (Y.shape[0] - 1) // chunk_y_size + 1\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(float('inf'))\\n    for chunk_x in range(n_chunks_x):\\n        x_sind = chunk_x * chunk_x_size\\n        x_eind = min((chunk_x + 1) * chunk_x_size, X.shape[0])\\n        X_chunk = X[x_sind:x_eind, :]\\n        for chunk_y in range(n_chunks_y):\\n            y_sind = chunk_y * chunk_y_size\\n            y_eind = (chunk_y + 1) * chunk_y_size\\n            Y_chunk = Y[y_sind:y_eind, :]\\n            tvar = np.dot(X_chunk, Y_chunk.T)\\n            tvar *= -2\\n            tvar += (X_chunk * X_chunk).sum(axis=1)[:, np.newaxis]\\n            tvar += (Y_chunk * Y_chunk).sum(axis=1)[np.newaxis, :]\\n            np.maximum(tvar, 0, tvar)\\n            min_indices = tvar.argmin(axis=1)\\n            min_values = tvar[range(x_eind - x_sind), min_indices]\\n            flags = values[x_sind:x_eind] > min_values\\n            indices[x_sind:x_eind] = np.where(\\n                flags, min_indices + y_sind, indices[x_sind:x_eind])\\n            values[x_sind:x_eind] = np.where(\\n                flags, min_values, values[x_sind:x_eind])\\n    if return_distances:\\n        if not squared:\\n            values = np.sqrt(values)\\n        return indices, values\\n    else:\\n        return indices"
  },
  {
    "code": "def get_payload_from_parameters(params):\\n    payload = {}\\n    for parameter in params:\\n        parameter_value = params[parameter]\\n        if parameter_value is not None and is_checkpoint_param(parameter):\\n            if isinstance(parameter_value, dict):\\n                payload[parameter.replace(\"_\", \"-\")] = get_payload_from_parameters(parameter_value)\\n            elif isinstance(parameter_value, list) and len(parameter_value) != 0 and isinstance(parameter_value[0], dict):\\n                payload_list = []\\n                for element_dict in parameter_value:\\n                    payload_list.append(get_payload_from_parameters(element_dict))\\n                payload[parameter.replace(\"_\", \"-\")] = payload_list\\n            else:\\n                payload[parameter.replace(\"_\", \"-\")] = parameter_value\\n    return payload",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_payload_from_parameters(params):\\n    payload = {}\\n    for parameter in params:\\n        parameter_value = params[parameter]\\n        if parameter_value is not None and is_checkpoint_param(parameter):\\n            if isinstance(parameter_value, dict):\\n                payload[parameter.replace(\"_\", \"-\")] = get_payload_from_parameters(parameter_value)\\n            elif isinstance(parameter_value, list) and len(parameter_value) != 0 and isinstance(parameter_value[0], dict):\\n                payload_list = []\\n                for element_dict in parameter_value:\\n                    payload_list.append(get_payload_from_parameters(element_dict))\\n                payload[parameter.replace(\"_\", \"-\")] = payload_list\\n            else:\\n                payload[parameter.replace(\"_\", \"-\")] = parameter_value\\n    return payload"
  },
  {
    "code": "def reindex_columns(self, new_columns):\\n        indexer, mask = self.columns.get_indexer(new_columns)\\n        new_values = self.values.take(indexer, axis=1)\\n        notmask = -mask\\n        if len(mask) > 0 and notmask.any():\\n            if issubclass(new_values.dtype.type, np.int_):\\n                new_values = new_values.astype(float)\\n            elif issubclass(new_values.dtype.type, np.bool_):\\n                new_values = new_values.astype(object)\\n            common.null_out_axis(new_values, notmask, 1)\\n        return Block(new_values, new_columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reindex_columns(self, new_columns):\\n        indexer, mask = self.columns.get_indexer(new_columns)\\n        new_values = self.values.take(indexer, axis=1)\\n        notmask = -mask\\n        if len(mask) > 0 and notmask.any():\\n            if issubclass(new_values.dtype.type, np.int_):\\n                new_values = new_values.astype(float)\\n            elif issubclass(new_values.dtype.type, np.bool_):\\n                new_values = new_values.astype(object)\\n            common.null_out_axis(new_values, notmask, 1)\\n        return Block(new_values, new_columns)"
  },
  {
    "code": "def min_mag(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special or other._is_special:\\n            sn = self._isnan()\\n            on = other._isnan()\\n            if sn or on:\\n                if on == 1 and sn != 2:\\n                    return self._fix_nan(context)\\n                if sn == 1 and on != 2:\\n                    return other._fix_nan(context)\\n                return self._check_nans(other, context)\\n        c = self.copy_abs().__cmp__(other.copy_abs())\\n        if c == 0:\\n            c = self.compare_total(other)\\n        if c == -1:\\n            ans = self\\n        else:\\n            ans = other\\n        return ans._fix(context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def min_mag(self, other, context=None):\\n        other = _convert_other(other, raiseit=True)\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special or other._is_special:\\n            sn = self._isnan()\\n            on = other._isnan()\\n            if sn or on:\\n                if on == 1 and sn != 2:\\n                    return self._fix_nan(context)\\n                if sn == 1 and on != 2:\\n                    return other._fix_nan(context)\\n                return self._check_nans(other, context)\\n        c = self.copy_abs().__cmp__(other.copy_abs())\\n        if c == 0:\\n            c = self.compare_total(other)\\n        if c == -1:\\n            ans = self\\n        else:\\n            ans = other\\n        return ans._fix(context)"
  },
  {
    "code": "def parse_duration(value):\\n    match = (\\n        standard_duration_re.match(value) or\\n        iso8601_duration_re.match(value) or\\n        postgres_interval_re.match(value)\\n    )\\n    if match:\\n        kw = match.groupdict()\\n        sign = -1 if kw.pop('sign', '+') == '-' else 1\\n        if kw.get('microseconds'):\\n            kw['microseconds'] = kw['microseconds'].ljust(6, '0')\\n        if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):\\n            kw['microseconds'] = '-' + kw['microseconds']\\n        kw = {k: float(v.replace(',', '.')) for k, v in kw.items() if v is not None}\\n        days = datetime.timedelta(kw.pop('days', .0) or .0)\\n        if match.re == iso8601_duration_re:\\n            days *= sign\\n        return days + sign * datetime.timedelta(**kw)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_duration(value):\\n    match = (\\n        standard_duration_re.match(value) or\\n        iso8601_duration_re.match(value) or\\n        postgres_interval_re.match(value)\\n    )\\n    if match:\\n        kw = match.groupdict()\\n        sign = -1 if kw.pop('sign', '+') == '-' else 1\\n        if kw.get('microseconds'):\\n            kw['microseconds'] = kw['microseconds'].ljust(6, '0')\\n        if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):\\n            kw['microseconds'] = '-' + kw['microseconds']\\n        kw = {k: float(v.replace(',', '.')) for k, v in kw.items() if v is not None}\\n        days = datetime.timedelta(kw.pop('days', .0) or .0)\\n        if match.re == iso8601_duration_re:\\n            days *= sign\\n        return days + sign * datetime.timedelta(**kw)"
  },
  {
    "code": "def _create_pipe():\\n            fds = os.pipe()\\n            _set_cloexec(fds[0], True)\\n            _set_cloexec(fds[1], True)\\n            return fds\\n__all__ = [\"Popen\", \"PIPE\", \"STDOUT\", \"call\", \"check_call\", \"getstatusoutput\",\\n           \"getoutput\", \"check_output\", \"CalledProcessError\"]\\nif mswindows:\\n    from _subprocess import CREATE_NEW_CONSOLE, CREATE_NEW_PROCESS_GROUP\\n    __all__.extend([\"CREATE_NEW_CONSOLE\", \"CREATE_NEW_PROCESS_GROUP\"])\\ntry:\\n    MAXFD = os.sysconf(\"SC_OPEN_MAX\")\\nexcept:\\n    MAXFD = 256\\n_active = []",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_pipe():\\n            fds = os.pipe()\\n            _set_cloexec(fds[0], True)\\n            _set_cloexec(fds[1], True)\\n            return fds\\n__all__ = [\"Popen\", \"PIPE\", \"STDOUT\", \"call\", \"check_call\", \"getstatusoutput\",\\n           \"getoutput\", \"check_output\", \"CalledProcessError\"]\\nif mswindows:\\n    from _subprocess import CREATE_NEW_CONSOLE, CREATE_NEW_PROCESS_GROUP\\n    __all__.extend([\"CREATE_NEW_CONSOLE\", \"CREATE_NEW_PROCESS_GROUP\"])\\ntry:\\n    MAXFD = os.sysconf(\"SC_OPEN_MAX\")\\nexcept:\\n    MAXFD = 256\\n_active = []"
  },
  {
    "code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(terms)))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(terms)))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)"
  },
  {
    "code": "def _clean_deprecated_modules_files():\\n    for (_, deprecated_path, _) in _DEPRECATED_MODULES:\\n        with suppress(FileNotFoundError):\\n            _get_deprecated_path(deprecated_path).unlink()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MNT Simplify module deprecation procedures with automatic tests (#15294)",
    "fixed_code": "def _clean_deprecated_modules_files():\\n    for _, deprecated_path, _, _ in _DEPRECATED_MODULES:\\n        with suppress(FileNotFoundError):\\n            _get_deprecated_path(deprecated_path).unlink()"
  },
  {
    "code": "def accuracy_score(y_true, y_pred, normalize=True):\\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\\n    if y_type == 'multilabel-indicator':\\n        score = (y_pred != y_true).sum(axis=1) == 0\\n    elif y_type == 'multilabel-sequences':\\n        score = np.array([len(set(true) ^ set(pred)) == 0\\n                          for pred, true in zip(y_pred, y_true)])\\n    else:\\n        y_true, y_pred = check_arrays(y_true, y_pred)\\n        y_true = np.squeeze(y_true)\\n        y_pred = np.squeeze(y_pred)\\n        score = y_true == y_pred\\n    if normalize:\\n        return np.mean(score)\\n    else:\\n        return np.sum(score)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH handle properly row vector",
    "fixed_code": "def accuracy_score(y_true, y_pred, normalize=True):\\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\\n    if y_type == 'multilabel-indicator':\\n        score = (y_pred != y_true).sum(axis=1) == 0\\n    elif y_type == 'multilabel-sequences':\\n        score = np.array([len(set(true) ^ set(pred)) == 0\\n                          for pred, true in zip(y_pred, y_true)])\\n    else:\\n        score = y_true == y_pred\\n    if normalize:\\n        return np.mean(score)\\n    else:\\n        return np.sum(score)"
  },
  {
    "code": "def server_bind(self):\\n        socketserver.TCPServer.server_bind(self)\\n        host, port = self.socket.getsockname()[:2]\\n        self.server_name = socket.getfqdn(host)\\n        self.server_port = port",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #26586: Simple enhancements to BaseHTTPRequestHandler by Xiang Zhang",
    "fixed_code": "def server_bind(self):\\n        socketserver.TCPServer.server_bind(self)\\n        host, port = self.server_address[:2]\\n        self.server_name = socket.getfqdn(host)\\n        self.server_port = port"
  },
  {
    "code": "def sum(self):\\n        try:\\n            return self._cython_agg_general('add')\\n        except Exception:\\n            return self.aggregate(lambda x: np.sum(x, axis=self.axis))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sum(self):\\n        try:\\n            return self._cython_agg_general('add')\\n        except Exception:\\n            return self.aggregate(lambda x: np.sum(x, axis=self.axis))"
  },
  {
    "code": "def _trigger_dag(\\n    dag_id: str,\\n    dag_bag: DagBag,\\n    run_id: Optional[str] = None,\\n    conf: Optional[Union[dict, str]] = None,\\n    execution_date: Optional[datetime] = None,\\n    replace_microseconds: bool = True,\\n) -> List[Optional[DagRun]]:\\n    dag = dag_bag.get_dag(dag_id)  \\n    if dag is None or dag_id not in dag_bag.dags:\\n        raise DagNotFound(f\"Dag id {dag_id} not found\")\\n    execution_date = execution_date if execution_date else timezone.utcnow()\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"The execution_date should be localized\")\\n    if replace_microseconds:\\n        execution_date = execution_date.replace(microsecond=0)\\n    if dag.default_args and 'start_date' in dag.default_args:\\n        min_dag_start_date = dag.default_args[\"start_date\"]\\n        if min_dag_start_date and execution_date < min_dag_start_date:\\n            raise ValueError(\\n                f\"The execution_date [{execution_date.isoformat()}] should be >= start_date \"\\n                f\"[{min_dag_start_date.isoformat()}] from DAG's default_args\"\\n            )\\n    logical_date = timezone.coerce_datetime(execution_date)\\n    data_interval = dag.timetable.infer_manual_data_interval(run_after=logical_date)\\n    run_id = run_id or dag.timetable.generate_run_id(\\n        run_type=DagRunType.MANUAL, logical_date=logical_date, data_interval=data_interval\\n    )\\n    dag_run = DagRun.find_duplicate(dag_id=dag_id, execution_date=execution_date, run_id=run_id)\\n    if dag_run:\\n        raise DagRunAlreadyExists(\\n            f\"A Dag Run already exists for dag id {dag_id} at {execution_date} with run id {run_id}\"\\n        )\\n    run_conf = None\\n    if conf:\\n        run_conf = conf if isinstance(conf, dict) else json.loads(conf)\\n    dag_runs = []\\n    dags_to_run = [dag] + dag.subdags\\n    for _dag in dags_to_run:\\n        dag_run = _dag.create_dagrun(\\n            run_id=run_id,\\n            execution_date=execution_date,\\n            state=DagRunState.QUEUED,\\n            conf=run_conf,\\n            external_trigger=True,\\n            dag_hash=dag_bag.dags_hash.get(dag_id),\\n            data_interval=data_interval,\\n        )\\n        dag_runs.append(dag_run)\\n    return dag_runs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _trigger_dag(\\n    dag_id: str,\\n    dag_bag: DagBag,\\n    run_id: Optional[str] = None,\\n    conf: Optional[Union[dict, str]] = None,\\n    execution_date: Optional[datetime] = None,\\n    replace_microseconds: bool = True,\\n) -> List[Optional[DagRun]]:\\n    dag = dag_bag.get_dag(dag_id)  \\n    if dag is None or dag_id not in dag_bag.dags:\\n        raise DagNotFound(f\"Dag id {dag_id} not found\")\\n    execution_date = execution_date if execution_date else timezone.utcnow()\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"The execution_date should be localized\")\\n    if replace_microseconds:\\n        execution_date = execution_date.replace(microsecond=0)\\n    if dag.default_args and 'start_date' in dag.default_args:\\n        min_dag_start_date = dag.default_args[\"start_date\"]\\n        if min_dag_start_date and execution_date < min_dag_start_date:\\n            raise ValueError(\\n                f\"The execution_date [{execution_date.isoformat()}] should be >= start_date \"\\n                f\"[{min_dag_start_date.isoformat()}] from DAG's default_args\"\\n            )\\n    logical_date = timezone.coerce_datetime(execution_date)\\n    data_interval = dag.timetable.infer_manual_data_interval(run_after=logical_date)\\n    run_id = run_id or dag.timetable.generate_run_id(\\n        run_type=DagRunType.MANUAL, logical_date=logical_date, data_interval=data_interval\\n    )\\n    dag_run = DagRun.find_duplicate(dag_id=dag_id, execution_date=execution_date, run_id=run_id)\\n    if dag_run:\\n        raise DagRunAlreadyExists(\\n            f\"A Dag Run already exists for dag id {dag_id} at {execution_date} with run id {run_id}\"\\n        )\\n    run_conf = None\\n    if conf:\\n        run_conf = conf if isinstance(conf, dict) else json.loads(conf)\\n    dag_runs = []\\n    dags_to_run = [dag] + dag.subdags\\n    for _dag in dags_to_run:\\n        dag_run = _dag.create_dagrun(\\n            run_id=run_id,\\n            execution_date=execution_date,\\n            state=DagRunState.QUEUED,\\n            conf=run_conf,\\n            external_trigger=True,\\n            dag_hash=dag_bag.dags_hash.get(dag_id),\\n            data_interval=data_interval,\\n        )\\n        dag_runs.append(dag_run)\\n    return dag_runs"
  },
  {
    "code": "def select_related_descend(field, restricted, requested, load_fields, reverse=False):\\n    if not field.remote_field:\\n        return False\\n    if field.remote_field.parent_link and not reverse:\\n        return False\\n    if restricted:\\n        if reverse and field.related_query_name() not in requested:\\n            return False\\n        if not reverse and field.name not in requested:\\n            return False\\n    if not restricted and field.null:\\n        return False\\n    if (\\n        restricted\\n        and load_fields\\n        and field.name in requested\\n        and field.attname not in load_fields\\n    ):\\n        raise FieldError(\\n            f\"Field {field.model._meta.object_name}.{field.name} cannot be both \"\\n            \"deferred and traversed using select_related at the same time.\"\\n        )\\n    return True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #21204 -- Tracked field deferrals by field instead of models.\\n\\nThis ensures field deferral works properly when a model is involved\\nmore than once in the same query with a distinct deferral mask.",
    "fixed_code": "def select_related_descend(field, restricted, requested, select_mask, reverse=False):\\n    if not field.remote_field:\\n        return False\\n    if field.remote_field.parent_link and not reverse:\\n        return False\\n    if restricted:\\n        if reverse and field.related_query_name() not in requested:\\n            return False\\n        if not reverse and field.name not in requested:\\n            return False\\n    if not restricted and field.null:\\n        return False\\n    if (\\n        restricted\\n        and select_mask\\n        and field.name in requested\\n        and field not in select_mask\\n    ):\\n        raise FieldError(\\n            f\"Field {field.model._meta.object_name}.{field.name} cannot be both \"\\n            \"deferred and traversed using select_related at the same time.\"\\n        )\\n    return True"
  },
  {
    "code": "def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\\n    if getattr(obj, '__no_type_check__', None):\\n        return {}\\n    if isinstance(obj, type):\\n        hints = {}\\n        for base in reversed(obj.__mro__):\\n            if globalns is None:\\n                base_globals = sys.modules[base.__module__].__dict__\\n            else:\\n                base_globals = globalns\\n            ann = base.__dict__.get('__annotations__', {})\\n            for name, value in ann.items():\\n                if value is None:\\n                    value = type(None)\\n                if isinstance(value, str):\\n                    value = ForwardRef(value, is_argument=False)\\n                value = _eval_type(value, base_globals, localns)\\n                hints[name] = value\\n        return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\\n    if globalns is None:\\n        if isinstance(obj, types.ModuleType):\\n            globalns = obj.__dict__\\n        else:\\n            nsobj = obj\\n            while hasattr(nsobj, '__wrapped__'):\\n                nsobj = nsobj.__wrapped__\\n            globalns = getattr(nsobj, '__globals__', {})\\n        if localns is None:\\n            localns = globalns\\n    elif localns is None:\\n        localns = globalns\\n    hints = getattr(obj, '__annotations__', None)\\n    if hints is None:\\n        if isinstance(obj, _allowed_types):\\n            return {}\\n        else:\\n            raise TypeError('{!r} is not a module, class, method, '\\n                            'or function.'.format(obj))\\n    defaults = _get_defaults(obj)\\n    hints = dict(hints)\\n    for name, value in hints.items():\\n        if value is None:\\n            value = type(None)\\n        if isinstance(value, str):\\n            value = ForwardRef(value)\\n        value = _eval_type(value, globalns, localns)\\n        if name in defaults and defaults[name] is None:\\n            value = Optional[value]\\n        hints[name] = value\\n    return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-42904: Fix get_type_hints for class local namespaces (GH-24201)",
    "fixed_code": "def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\\n    if getattr(obj, '__no_type_check__', None):\\n        return {}\\n    if isinstance(obj, type):\\n        hints = {}\\n        for base in reversed(obj.__mro__):\\n            if globalns is None:\\n                base_globals = sys.modules[base.__module__].__dict__\\n            else:\\n                base_globals = globalns\\n            ann = base.__dict__.get('__annotations__', {})\\n            base_locals = dict(vars(base)) if localns is None else localns\\n            for name, value in ann.items():\\n                if value is None:\\n                    value = type(None)\\n                if isinstance(value, str):\\n                    value = ForwardRef(value, is_argument=False)\\n                value = _eval_type(value, base_globals, base_locals)\\n                hints[name] = value\\n        return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\\n    if globalns is None:\\n        if isinstance(obj, types.ModuleType):\\n            globalns = obj.__dict__\\n        else:\\n            nsobj = obj\\n            while hasattr(nsobj, '__wrapped__'):\\n                nsobj = nsobj.__wrapped__\\n            globalns = getattr(nsobj, '__globals__', {})\\n        if localns is None:\\n            localns = globalns\\n    elif localns is None:\\n        localns = globalns\\n    hints = getattr(obj, '__annotations__', None)\\n    if hints is None:\\n        if isinstance(obj, _allowed_types):\\n            return {}\\n        else:\\n            raise TypeError('{!r} is not a module, class, method, '\\n                            'or function.'.format(obj))\\n    defaults = _get_defaults(obj)\\n    hints = dict(hints)\\n    for name, value in hints.items():\\n        if value is None:\\n            value = type(None)\\n        if isinstance(value, str):\\n            value = ForwardRef(value)\\n        value = _eval_type(value, globalns, localns)\\n        if name in defaults and defaults[name] is None:\\n            value = Optional[value]\\n        hints[name] = value\\n    return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}"
  },
  {
    "code": "def task_group(python_callable: Optional[Callable] = None, *tg_args, **tg_kwargs) -> Callable[[T], T]:",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix MyPy issues in Core airflow packages (#20425)\\n\\nPart of #19891",
    "fixed_code": "def task_group(\\n    group_id: Optional[str] = None,\\n    prefix_group_id: bool = True,\\n    parent_group: Optional[\"TaskGroup\"] = None,\\n    dag: Optional[\"DAG\"] = None,\\n    default_args: Optional[Dict[str, Any]] = None,\\n    tooltip: str = \"\",\\n    ui_color: str = \"CornflowerBlue\",\\n    ui_fgcolor: str = \"\\n    add_suffix_on_collision: bool = False,\\n) -> Callable[[F], F]:\\n    ..."
  },
  {
    "code": "def is_python36(node: Node) -> bool:\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                return True\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    return True\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            return True\\n    return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "autodetect Python 3.6 on the basis of underscores (#461)",
    "fixed_code": "def is_python36(node: Node) -> bool:\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                return True\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                return True\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    return True\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            return True\\n    return False"
  },
  {
    "code": "def size_internal(input, name=None, optimize=True, out_type=dtypes.int32):\\n  if context.executing_eagerly() and not isinstance(\\n\\t  input, (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)):\\n\\tinput = ops.convert_to_tensor(input)\\n\\tnp_out_type = out_type.as_numpy_dtype\\n\\tnum_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  \\n\\treturn ops.convert_to_tensor(num_elements, dtype=out_type)\\n  with ops.name_scope(name, \"Size\", [input]) as name:\\n\\tif isinstance(input, (sparse_tensor.SparseTensor,\\n\\t\\t\\t\\t\\t\\t  sparse_tensor.SparseTensorValue)):\\n\\t  return gen_math_ops.prod(\\n\\t\\t  gen_math_ops.cast(input.dense_shape, out_type), 0, name=name)\\n\\telse:\\n\\t  input_tensor = ops.convert_to_tensor(input)\\n\\t  input_shape = input_tensor.get_shape()\\n\\t  if optimize:\\n\\t\\tif input_shape.is_fully_defined():\\n\\t\\t  return constant(input_shape.num_elements(), out_type, name=name)\\n\\t\\tif input_shape.dims and any(dim == 0 for dim in input_shape.dims):\\n\\t\\t  return constant(0, out_type, name=name)\\n\\t  return gen_array_ops.size(input, name=name, out_type=out_type)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def size_internal(input, name=None, optimize=True, out_type=dtypes.int32):\\n  if context.executing_eagerly() and not isinstance(\\n\\t  input, (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)):\\n\\tinput = ops.convert_to_tensor(input)\\n\\tnp_out_type = out_type.as_numpy_dtype\\n\\tnum_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  \\n\\treturn ops.convert_to_tensor(num_elements, dtype=out_type)\\n  with ops.name_scope(name, \"Size\", [input]) as name:\\n\\tif isinstance(input, (sparse_tensor.SparseTensor,\\n\\t\\t\\t\\t\\t\\t  sparse_tensor.SparseTensorValue)):\\n\\t  return gen_math_ops.prod(\\n\\t\\t  gen_math_ops.cast(input.dense_shape, out_type), 0, name=name)\\n\\telse:\\n\\t  input_tensor = ops.convert_to_tensor(input)\\n\\t  input_shape = input_tensor.get_shape()\\n\\t  if optimize:\\n\\t\\tif input_shape.is_fully_defined():\\n\\t\\t  return constant(input_shape.num_elements(), out_type, name=name)\\n\\t\\tif input_shape.dims and any(dim == 0 for dim in input_shape.dims):\\n\\t\\t  return constant(0, out_type, name=name)\\n\\t  return gen_array_ops.size(input, name=name, out_type=out_type)"
  },
  {
    "code": "def _getclosed(self):\\n        return self._sock is None\\n    closed = property(_getclosed, doc=\"True if the file is closed\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _getclosed(self):\\n        return self._sock is None\\n    closed = property(_getclosed, doc=\"True if the file is closed\")"
  },
  {
    "code": "def infer_axes(self):\\n        self.index_axes, self.values_axes = [ a.infer(self.table) for a in self.indexables if a.is_indexable ], [ a.infer(self.table) for a in self.indexables if not a.is_indexable ]\\n        self.non_index_axes = getattr(self.attrs,'non_index_axes',None) or []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: ndim tables in HDFStore      changes axes_to_index keyword in table creation to axes      allow passing of numeric or named axes (e.g. 0 or 'minor_axis') in axes      create_axes now checks for current table scheme; raises if this indexing scheme is violated      added many p4d tests for appending/selection/partial selection/and axis permuation      added addition Term tests to include p4d      add __eq__ operators to IndexCol/DataCol/Table to comparisons      updated docs with Panel4D saving & issues relating to threading      supporting non-regular indexables:        e.g. can index a Panel4D on say [labels,major_axis,minor_axis], rather             than the default of [items,major_axis,minor_axis]      support column oriented DataFrames (e.g. queryable by the columns)",
    "fixed_code": "def infer_axes(self):\\n        table = self.table\\n        if table is None:\\n            return False\\n        self.index_axes, self.values_axes = [ a.infer(self.table) for a in self.indexables if a.is_indexable ], [ a.infer(self.table) for a in self.indexables if not a.is_indexable ]\\n        self.non_index_axes   = getattr(self.attrs,'non_index_axes',None) or []\\n        return True"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        domain=dict(required=True, type='str'),\\n        role_priority=dict(required=False, type='str'),\\n        system_priority=dict(required=False, type='str'),\\n        pkl_src=dict(required=False),\\n        pkl_dest=dict(required=False),\\n        pkl_vrf=dict(required=False),\\n        peer_gw=dict(required=False, type='bool'),\\n        auto_recovery=dict(required=False, type='bool'),\\n        auto_recovery_reload_delay=dict(required=False, type='str'),\\n        delay_restore=dict(required=False, type='str'),\\n        delay_restore_interface_vlan=dict(required=False, type='str'),\\n        delay_restore_orphan_port=dict(required=False, type='str'),\\n        state=dict(choices=['absent', 'present'], default='present'),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    mutually_exclusive = [('auto_recovery', 'auto_recovery_reload_delay')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    results = {'changed': False, 'warnings': warnings}\\n    domain = module.params['domain']\\n    role_priority = module.params['role_priority']\\n    system_priority = module.params['system_priority']\\n    pkl_src = module.params['pkl_src']\\n    pkl_dest = module.params['pkl_dest']\\n    pkl_vrf = module.params['pkl_vrf']\\n    peer_gw = module.params['peer_gw']\\n    auto_recovery = module.params['auto_recovery']\\n    auto_recovery_reload_delay = module.params['auto_recovery_reload_delay']\\n    delay_restore = module.params['delay_restore']\\n    delay_restore_interface_vlan = module.params['delay_restore_interface_vlan']\\n    delay_restore_orphan_port = module.params['delay_restore_orphan_port']\\n    state = module.params['state']\\n    args = dict(domain=domain, role_priority=role_priority,\\n                system_priority=system_priority, pkl_src=pkl_src,\\n                pkl_dest=pkl_dest, pkl_vrf=pkl_vrf, peer_gw=peer_gw,\\n                auto_recovery=auto_recovery,\\n                auto_recovery_reload_delay=auto_recovery_reload_delay,\\n                delay_restore=delay_restore,\\n                delay_restore_interface_vlan=delay_restore_interface_vlan,\\n                delay_restore_orphan_port=delay_restore_orphan_port,\\n                )\\n    if not pkl_dest:\\n        if pkl_src:\\n            module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                 ' when src IP is present')\\n        elif pkl_vrf:\\n            if pkl_vrf != 'management':\\n                module.fail_json(msg='dest and src IP for peer-keepalive are required'\\n                                     ' when vrf is present')\\n            else:\\n                module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                     ' when vrf is present')\\n    if pkl_vrf:\\n        if pkl_vrf.lower() not in get_vrf_list(module):\\n            module.fail_json(msg='The VRF you are trying to use for the peer '\\n                                 'keepalive link is not on device yet. Add it'\\n                                 ' first, please.')\\n    proposed = dict((k, v) for k, v in args.items() if v is not None)\\n    existing = get_vpc(module)\\n    commands = []\\n    if state == 'present':\\n        delta = {}\\n        for key, value in proposed.items():\\n            if str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n            if existing.get(key) != value:\\n                delta[key] = value\\n        if delta:\\n            command = get_commands_to_config_vpc(module, delta, domain, existing)\\n            commands.append(command)\\n    elif state == 'absent':\\n        if existing:\\n            if domain != existing['domain']:\\n                module.fail_json(msg=\"You are trying to remove a domain that \"\\n                                     \"does not exist on the device\")\\n            else:\\n                commands.append('terminal dont-ask')\\n                commands.append('no vpc domain {0}'.format(domain))\\n    cmds = flatten_list(commands)\\n    results['commands'] = cmds\\n    if cmds:\\n        results['changed'] = True\\n        if not module.check_mode:\\n            load_config(module, cmds)\\n            if 'configure' in cmds:\\n                cmds.pop(0)\\n    module.exit_json(**results)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_vpc: pkl_vrf fixes for #57069 (#57370)\\n\\n\\nFixes #57069\\n\\n- Symptom: When playbooks specify `pkl_vrf: default`, the result is that the cli does not set the `vrf` state.\\n\\n- Analysis:\\n - First issue: 'default' is a reserved word when used with the `peer-keepalive` `vrf` keyword. It refers to the default rib.\\n   - This is confusing in several ways because `peer-keepalive`'s *default* vrf is the `management` vrf.\\n\\n - Second issue: When changing only one optional value (`pkl_vrf`) while other optional values are idempotent (`pkl_src`), the result is that the idempotent values are ignored; unfortunately the device cli *replaces* the entire command, in which case the idempotent values are removed.\\n   - e.g. playbook specifies this:\\n     ```\\n     { pkl_dest: 10.1.1.1, pkl_src: 10.2.2.2, pkl_vrf: my_vrf }\\n     ```\\n\\n     ```\\n     peer-keepalive dest 10.1.1.1 src 10.2.2.2             # original\\n\\n     peer-keepalive dest 10.1.1.1 src 10.2.2.2 vrf my_vrf  # intended result\\n\\n     peer-keepalive dest 10.1.1.1 vrf my_vrf               # actual result\\n     ```\\n\\n - Third issue: the `pkl` getter was relying on positional data. This broke when the `udp` keyword nvgen'd where `vrf` used to appear (shifting all keywords to the right).\\n\\n- Tested on regression platforms: `N3K,N6k,N7K,N9K,N3K-F,N9K-F`",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        domain=dict(required=True, type='str'),\\n        role_priority=dict(required=False, type='str'),\\n        system_priority=dict(required=False, type='str'),\\n        pkl_src=dict(required=False),\\n        pkl_dest=dict(required=False),\\n        pkl_vrf=dict(required=False),\\n        peer_gw=dict(required=False, type='bool'),\\n        auto_recovery=dict(required=False, type='bool'),\\n        auto_recovery_reload_delay=dict(required=False, type='str'),\\n        delay_restore=dict(required=False, type='str'),\\n        delay_restore_interface_vlan=dict(required=False, type='str'),\\n        delay_restore_orphan_port=dict(required=False, type='str'),\\n        state=dict(choices=['absent', 'present'], default='present'),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    mutually_exclusive = [('auto_recovery', 'auto_recovery_reload_delay')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    results = {'changed': False, 'warnings': warnings}\\n    domain = module.params['domain']\\n    role_priority = module.params['role_priority']\\n    system_priority = module.params['system_priority']\\n    pkl_src = module.params['pkl_src']\\n    pkl_dest = module.params['pkl_dest']\\n    pkl_vrf = module.params['pkl_vrf']\\n    peer_gw = module.params['peer_gw']\\n    auto_recovery = module.params['auto_recovery']\\n    auto_recovery_reload_delay = module.params['auto_recovery_reload_delay']\\n    delay_restore = module.params['delay_restore']\\n    delay_restore_interface_vlan = module.params['delay_restore_interface_vlan']\\n    delay_restore_orphan_port = module.params['delay_restore_orphan_port']\\n    state = module.params['state']\\n    args = dict(domain=domain, role_priority=role_priority,\\n                system_priority=system_priority, pkl_src=pkl_src,\\n                pkl_dest=pkl_dest, pkl_vrf=pkl_vrf, peer_gw=peer_gw,\\n                auto_recovery=auto_recovery,\\n                auto_recovery_reload_delay=auto_recovery_reload_delay,\\n                delay_restore=delay_restore,\\n                delay_restore_interface_vlan=delay_restore_interface_vlan,\\n                delay_restore_orphan_port=delay_restore_orphan_port,\\n                )\\n    if not pkl_dest:\\n        if pkl_src:\\n            module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                 ' when src IP is present')\\n        elif pkl_vrf:\\n            if pkl_vrf != 'management':\\n                module.fail_json(msg='dest and src IP for peer-keepalive are required'\\n                                     ' when vrf is present')\\n            else:\\n                module.fail_json(msg='dest IP for peer-keepalive is required'\\n                                     ' when vrf is present')\\n    if pkl_vrf:\\n        if pkl_vrf.lower() not in get_vrf_list(module):\\n            module.fail_json(msg='The VRF you are trying to use for the peer '\\n                                 'keepalive link is not on device yet. Add it'\\n                                 ' first, please.')\\n    proposed = dict((k, v) for k, v in args.items() if v is not None)\\n    existing = get_vpc(module)\\n    commands = []\\n    if state == 'present':\\n        delta = {}\\n        for key, value in proposed.items():\\n            if str(value).lower() == 'default' and key != 'pkl_vrf':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n            if existing.get(key) != value:\\n                delta[key] = value\\n        if delta:\\n            pkl_dependencies(module, delta, existing)\\n            command = get_commands_to_config_vpc(module, delta, domain, existing)\\n            commands.append(command)\\n    elif state == 'absent':\\n        if existing:\\n            if domain != existing['domain']:\\n                module.fail_json(msg=\"You are trying to remove a domain that \"\\n                                     \"does not exist on the device\")\\n            else:\\n                commands.append('terminal dont-ask')\\n                commands.append('no vpc domain {0}'.format(domain))\\n    cmds = flatten_list(commands)\\n    results['commands'] = cmds\\n    if cmds:\\n        results['changed'] = True\\n        if not module.check_mode:\\n            load_config(module, cmds)\\n            if 'configure' in cmds:\\n                cmds.pop(0)\\n    module.exit_json(**results)"
  },
  {
    "code": "def wrapped_markdown(s, css_class='rich_doc'):\\n    if s is None:\\n        return None\\n    s = textwrap.dedent(s)\\n    return Markup(f'<div class=\"{css_class}\" >' + markdown.markdown(s, extensions=['tables']) + \"</div>\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def wrapped_markdown(s, css_class='rich_doc'):\\n    if s is None:\\n        return None\\n    s = textwrap.dedent(s)\\n    return Markup(f'<div class=\"{css_class}\" >' + markdown.markdown(s, extensions=['tables']) + \"</div>\")"
  },
  {
    "code": "def _nanvar(values, axis=None, skipna=True, ddof=1):\\n    mask = isnull(values)\\n    if is_any_int_dtype(values):\\n        values = values.astype('f8')\\n    if is_float_dtype(values):\\n        count, d = _get_counts_nanvar(mask, axis, ddof, values.dtype)\\n    else:\\n        count, d = _get_counts_nanvar(mask, axis, ddof)\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    X = _ensure_numeric(values.sum(axis))\\n    XX = _ensure_numeric((values ** 2).sum(axis))\\n    result = np.fabs((XX - X * X / count) / d)\\n    return result\\n@disallow('M8')\\n@bottleneck_switch(ddof=1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _nanvar(values, axis=None, skipna=True, ddof=1):\\n    mask = isnull(values)\\n    if is_any_int_dtype(values):\\n        values = values.astype('f8')\\n    if is_float_dtype(values):\\n        count, d = _get_counts_nanvar(mask, axis, ddof, values.dtype)\\n    else:\\n        count, d = _get_counts_nanvar(mask, axis, ddof)\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    X = _ensure_numeric(values.sum(axis))\\n    XX = _ensure_numeric((values ** 2).sum(axis))\\n    result = np.fabs((XX - X * X / count) / d)\\n    return result\\n@disallow('M8')\\n@bottleneck_switch(ddof=1)"
  },
  {
    "code": "def deprecated(self):\\n        pattern = re.compile('.. deprecated:: ')\\n        return (self.method_name.startswith('pandas.Panel') or\\n                bool(pattern.search(self.summary)) or\\n                bool(pattern.search(self.extended_summary)))\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Changes to validate_docstring script to be able to check all docstrings at once (#22408)",
    "fixed_code": "def deprecated(self):\\n        pattern = re.compile('.. deprecated:: ')\\n        return (self.name.startswith('pandas.Panel')\\n                or bool(pattern.search(self.summary))\\n                or bool(pattern.search(self.extended_summary)))\\n    @property"
  },
  {
    "code": "def _setitem_with_indexer(self, indexer, value):\\n        from pandas import Panel, DataFrame, Series\\n        if isinstance(indexer,tuple):\\n            nindexer = []\\n            for i, idx in enumerate(indexer):\\n                if isinstance(idx, dict):\\n                    key,_ = _convert_missing_indexer(idx)\\n                    labels = self.obj._get_axis(i) + Index([key])\\n                    self.obj._data = self.obj.reindex_axis(labels,i)._data\\n                    nindexer.append(labels.get_loc(key))\\n                else:\\n                    nindexer.append(idx)\\n            indexer = tuple(nindexer)\\n        else:\\n            indexer, missing = _convert_missing_indexer(indexer)\\n            if missing:\\n                if self.ndim == 1:\\n                    self.obj._data = self.obj.append(Series(value,index=[indexer]))._data\\n                    return\\n                elif self.ndim == 2:\\n                    labels = self.obj._get_axis(0) + Index([indexer])\\n                    self.obj._data = self.obj.reindex_axis(labels,0)._data\\n                    return getattr(self.obj,self.name).__setitem__(indexer,value)\\n                elif self.ndim >= 3:\\n                    return self.obj.__setitem__(indexer,value)\\n        if self.obj._is_mixed_type:\\n            if not isinstance(indexer, tuple):\\n                indexer = self._tuplify(indexer)\\n            if isinstance(value, ABCSeries):\\n                value = self._align_series(indexer, value)\\n            info_axis = self.obj._info_axis_number\\n            info_idx = indexer[info_axis]\\n            if com.is_integer(info_idx):\\n                info_idx = [info_idx]\\n            plane_indexer = indexer[:info_axis] + indexer[info_axis + 1:]\\n            item_labels = self.obj._get_axis(info_axis)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _setitem_with_indexer(self, indexer, value):\\n        from pandas import Panel, DataFrame, Series\\n        if isinstance(indexer,tuple):\\n            nindexer = []\\n            for i, idx in enumerate(indexer):\\n                if isinstance(idx, dict):\\n                    key,_ = _convert_missing_indexer(idx)\\n                    labels = self.obj._get_axis(i) + Index([key])\\n                    self.obj._data = self.obj.reindex_axis(labels,i)._data\\n                    nindexer.append(labels.get_loc(key))\\n                else:\\n                    nindexer.append(idx)\\n            indexer = tuple(nindexer)\\n        else:\\n            indexer, missing = _convert_missing_indexer(indexer)\\n            if missing:\\n                if self.ndim == 1:\\n                    self.obj._data = self.obj.append(Series(value,index=[indexer]))._data\\n                    return\\n                elif self.ndim == 2:\\n                    labels = self.obj._get_axis(0) + Index([indexer])\\n                    self.obj._data = self.obj.reindex_axis(labels,0)._data\\n                    return getattr(self.obj,self.name).__setitem__(indexer,value)\\n                elif self.ndim >= 3:\\n                    return self.obj.__setitem__(indexer,value)\\n        if self.obj._is_mixed_type:\\n            if not isinstance(indexer, tuple):\\n                indexer = self._tuplify(indexer)\\n            if isinstance(value, ABCSeries):\\n                value = self._align_series(indexer, value)\\n            info_axis = self.obj._info_axis_number\\n            info_idx = indexer[info_axis]\\n            if com.is_integer(info_idx):\\n                info_idx = [info_idx]\\n            plane_indexer = indexer[:info_axis] + indexer[info_axis + 1:]\\n            item_labels = self.obj._get_axis(info_axis)"
  },
  {
    "code": "def __init__(self, model, data, mapping, layer=0,\\n                 source_srs=None, encoding=None,\\n                 transaction_mode='commit_on_success',\\n                 transform=True, unique=None, using=DEFAULT_DB_ALIAS):\\n        if isinstance(data, basestring):\\n            self.ds = DataSource(data)\\n        else:\\n            self.ds = data\\n        self.layer = self.ds[layer]\\n        self.using = using\\n        self.spatial_backend = connections[using].ops\\n        self.mapping = mapping\\n        self.model = model\\n        self.check_layer()\\n        if self.spatial_backend.mysql:\\n            transform = False\\n        else:\\n            self.geo_col = self.geometry_column()\\n        if transform:\\n            self.source_srs = self.check_srs(source_srs)\\n            self.transform = self.coord_transform()\\n        else:\\n            self.transform = transform\\n        if encoding:\\n            from codecs import lookup\\n            lookup(encoding)\\n            self.encoding = encoding\\n        else:\\n            self.encoding = None\\n        if unique:\\n            self.check_unique(unique)\\n            transaction_mode = 'autocommit' \\n            self.unique = unique\\n        else:\\n            self.unique = None\\n        if transaction_mode in self.TRANSACTION_MODES:\\n            self.transaction_decorator = self.TRANSACTION_MODES[transaction_mode]\\n            self.transaction_mode = transaction_mode\\n        else:\\n            raise LayerMapError('Unrecognized transaction mode: %s' % transaction_mode)\\n        if using is None:\\n            pass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed `LayerMapping` to work with PostGIS geography fields; removed `LayerMapping.geometry_column` and replaced with `LayerMapping.geometry_field` because getting the `geometry_columns` entry was completely unnecessary, and only the geometry field instance is needed; cleaned up and fleshed out the `geogapp` tests.",
    "fixed_code": "def __init__(self, model, data, mapping, layer=0,\\n                 source_srs=None, encoding=None,\\n                 transaction_mode='commit_on_success',\\n                 transform=True, unique=None, using=DEFAULT_DB_ALIAS):\\n        if isinstance(data, basestring):\\n            self.ds = DataSource(data)\\n        else:\\n            self.ds = data\\n        self.layer = self.ds[layer]\\n        self.using = using\\n        self.spatial_backend = connections[using].ops\\n        self.mapping = mapping\\n        self.model = model\\n        self.check_layer()\\n        if self.spatial_backend.mysql:\\n            transform = False\\n        else:\\n            self.geo_field = self.geometry_field()\\n        if transform:\\n            self.source_srs = self.check_srs(source_srs)\\n            self.transform = self.coord_transform()\\n        else:\\n            self.transform = transform\\n        if encoding:\\n            from codecs import lookup\\n            lookup(encoding)\\n            self.encoding = encoding\\n        else:\\n            self.encoding = None\\n        if unique:\\n            self.check_unique(unique)\\n            transaction_mode = 'autocommit' \\n            self.unique = unique\\n        else:\\n            self.unique = None\\n        if transaction_mode in self.TRANSACTION_MODES:\\n            self.transaction_decorator = self.TRANSACTION_MODES[transaction_mode]\\n            self.transaction_mode = transaction_mode\\n        else:\\n            raise LayerMapError('Unrecognized transaction mode: %s' % transaction_mode)\\n        if using is None:\\n            pass"
  },
  {
    "code": "def name(self):\\n        if self._selection is None:\\n            return None \\n        else:\\n            return self._selection\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def name(self):\\n        if self._selection is None:\\n            return None \\n        else:\\n            return self._selection\\n    @property"
  },
  {
    "code": "def parse_makefile(fn, g=None):\\n    from distutils.text_file import TextFile\\n    fp = TextFile(fn, strip_comments=1, skip_blanks=1, join_lines=1)\\n    if g is None:\\n        g = {}\\n    done = {}\\n    notdone = {}\\n    while 1:\\n        line = fp.readline()\\n        if line is None:  \\n            break\\n        m = _variable_rx.match(line)\\n        if m:\\n            n, v = m.group(1, 2)\\n            v = v.strip()\\n            tmpv = v.replace('$$', '')\\n            if \"$\" in tmpv:\\n                notdone[n] = v\\n            else:\\n                try:\\n                    v = int(v)\\n                except ValueError:\\n                    done[n] = v.replace('$$', '$')\\n                else:\\n                    done[n] = v\\n    while notdone:\\n        for name in notdone.keys():\\n            value = notdone[name]\\n            m = _findvar1_rx.search(value) or _findvar2_rx.search(value)\\n            if m:\\n                n = m.group(1)\\n                found = True\\n                if n in done:\\n                    item = str(done[n])\\n                elif n in notdone:\\n                    found = False\\n                elif n in os.environ:\\n                    item = os.environ[n]\\n                else:\\n                    done[n] = item = \"\"\\n                if found:\\n                    after = value[m.end():]\\n                    value = value[:m.start()] + item + after\\n                    if \"$\" in after:\\n                        notdone[name] = value\\n                    else:\\n                        try: value = int(value)\\n                        except ValueError:\\n                            done[name] = value.strip()\\n                        else:\\n                            done[name] = value\\n                        del notdone[name]\\n            else:\\n                del notdone[name]\\n    fp.close()\\n    g.update(done)\\n    return g",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_makefile(fn, g=None):\\n    from distutils.text_file import TextFile\\n    fp = TextFile(fn, strip_comments=1, skip_blanks=1, join_lines=1)\\n    if g is None:\\n        g = {}\\n    done = {}\\n    notdone = {}\\n    while 1:\\n        line = fp.readline()\\n        if line is None:  \\n            break\\n        m = _variable_rx.match(line)\\n        if m:\\n            n, v = m.group(1, 2)\\n            v = v.strip()\\n            tmpv = v.replace('$$', '')\\n            if \"$\" in tmpv:\\n                notdone[n] = v\\n            else:\\n                try:\\n                    v = int(v)\\n                except ValueError:\\n                    done[n] = v.replace('$$', '$')\\n                else:\\n                    done[n] = v\\n    while notdone:\\n        for name in notdone.keys():\\n            value = notdone[name]\\n            m = _findvar1_rx.search(value) or _findvar2_rx.search(value)\\n            if m:\\n                n = m.group(1)\\n                found = True\\n                if n in done:\\n                    item = str(done[n])\\n                elif n in notdone:\\n                    found = False\\n                elif n in os.environ:\\n                    item = os.environ[n]\\n                else:\\n                    done[n] = item = \"\"\\n                if found:\\n                    after = value[m.end():]\\n                    value = value[:m.start()] + item + after\\n                    if \"$\" in after:\\n                        notdone[name] = value\\n                    else:\\n                        try: value = int(value)\\n                        except ValueError:\\n                            done[name] = value.strip()\\n                        else:\\n                            done[name] = value\\n                        del notdone[name]\\n            else:\\n                del notdone[name]\\n    fp.close()\\n    g.update(done)\\n    return g"
  },
  {
    "code": "def variables_list(args):\\n    with db.create_session() as session:\\n        vars = session.query(Variable)\\n    print(\"\\n\".join(var.key for var in vars))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def variables_list(args):\\n    with db.create_session() as session:\\n        variables = session.query(Variable)\\n    print(\"\\n\".join(var.key for var in variables))"
  },
  {
    "code": "def get_frees(self):\\n        if self.__frees is None:\\n            is_free = lambda x:((x >> SCOPE_OFF) & SCOPE_MASK) == FREE\\n            self.__frees = self.__idents_matching(is_free)\\n        return self.__frees",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_frees(self):\\n        if self.__frees is None:\\n            is_free = lambda x:((x >> SCOPE_OFF) & SCOPE_MASK) == FREE\\n            self.__frees = self.__idents_matching(is_free)\\n        return self.__frees"
  },
  {
    "code": "def map_params_to_obj(module):\\n    text = module.params['text']\\n    if text:\\n        text = str(text).strip()\\n    return {\\n        'banner': module.params['banner'],\\n        'text': text,\\n        'state': module.params['state']\\n    }",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "allowing banner without stripping spaces (#62573)",
    "fixed_code": "def map_params_to_obj(module):\\n    text = module.params['text']\\n    return {\\n        'banner': module.params['banner'],\\n        'text': text,\\n        'state': module.params['state']\\n    }"
  },
  {
    "code": "def _factorize_keys(lk, rk, sort=True):\\n\\tlk = extract_array(lk, extract_numpy=True)\\n\\trk = extract_array(rk, extract_numpy=True)\\n\\tif is_datetime64tz_dtype(lk.dtype) and is_datetime64tz_dtype(rk.dtype):\\n\\t\\tlk, _ = lk._values_for_factorize()\\n\\t\\trk, _ = rk._values_for_factorize()\\n\\telif (\\n\\t\\tis_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)\\n\\t):\\n\\t\\tif lk.categories.equals(rk.categories):\\n\\t\\t\\trk = rk.codes\\n\\t\\telse:\\n\\t\\t\\trk = _recode_for_categories(rk.codes, rk.categories, lk.categories)\\n\\t\\tlk = ensure_int64(lk.codes)\\n\\t\\trk = ensure_int64(rk)\\n\\telif is_extension_array_dtype(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):\\n\\t\\tlk, _ = lk._values_for_factorize()\\n\\t\\trk, _ = rk._values_for_factorize()\\n\\tif is_integer_dtype(lk) and is_integer_dtype(rk):\\n\\t\\tklass = libhashtable.Int64Factorizer\\n\\t\\tlk = ensure_int64(np.asarray(lk))\\n\\t\\trk = ensure_int64(np.asarray(rk))\\n\\telif needs_i8_conversion(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):\\n\\t\\tklass = libhashtable.Int64Factorizer\\n\\t\\tlk = ensure_int64(np.asarray(lk, dtype=np.int64))\\n\\t\\trk = ensure_int64(np.asarray(rk, dtype=np.int64))\\n\\telse:\\n\\t\\tklass = libhashtable.Factorizer\\n\\t\\tlk = ensure_object(lk)\\n\\t\\trk = ensure_object(rk)\\n\\trizer = klass(max(len(lk), len(rk)))\\n\\tllab = rizer.factorize(lk)\\n\\trlab = rizer.factorize(rk)\\n\\tcount = rizer.get_count()\\n\\tif sort:\\n\\t\\tuniques = rizer.uniques.to_array()\\n\\t\\tllab, rlab = _sort_labels(uniques, llab, rlab)\\n\\tlmask = llab == -1\\n\\tlany = lmask.any()\\n\\trmask = rlab == -1\\n\\trany = rmask.any()\\n\\tif lany or rany:\\n\\t\\tif lany:\\n\\t\\t\\tnp.putmask(llab, lmask, count)\\n\\t\\tif rany:\\n\\t\\t\\tnp.putmask(rlab, rmask, count)\\n\\t\\tcount += 1\\n\\treturn llab, rlab, count",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: 27453 right merge order (#31278)",
    "fixed_code": "def _factorize_keys(\\n\\tlk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\\n) -> Tuple[np.array, np.array, int]:\\n\\tlk = extract_array(lk, extract_numpy=True)\\n\\trk = extract_array(rk, extract_numpy=True)\\n\\tif is_datetime64tz_dtype(lk.dtype) and is_datetime64tz_dtype(rk.dtype):\\n\\t\\tlk, _ = lk._values_for_factorize()\\n\\t\\trk, _ = rk._values_for_factorize()\\n\\telif (\\n\\t\\tis_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\\n\\t):\\n\\t\\tassert is_categorical(lk) and is_categorical(rk)\\n\\t\\tlk = cast(Categorical, lk)\\n\\t\\trk = cast(Categorical, rk)\\n\\t\\tif lk.categories.equals(rk.categories):\\n\\t\\t\\trk = rk.codes\\n\\t\\telse:\\n\\t\\t\\trk = _recode_for_categories(rk.codes, rk.categories, lk.categories)\\n\\t\\tlk = ensure_int64(lk.codes)\\n\\t\\trk = ensure_int64(rk)\\n\\telif is_extension_array_dtype(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):\\n\\t\\tlk, _ = lk._values_for_factorize()\\n\\t\\trk, _ = rk._values_for_factorize()\\n\\tif is_integer_dtype(lk) and is_integer_dtype(rk):\\n\\t\\tklass = libhashtable.Int64Factorizer\\n\\t\\tlk = ensure_int64(np.asarray(lk))\\n\\t\\trk = ensure_int64(np.asarray(rk))\\n\\telif needs_i8_conversion(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):\\n\\t\\tklass = libhashtable.Int64Factorizer\\n\\t\\tlk = ensure_int64(np.asarray(lk, dtype=np.int64))\\n\\t\\trk = ensure_int64(np.asarray(rk, dtype=np.int64))\\n\\telse:\\n\\t\\tklass = libhashtable.Factorizer\\n\\t\\tlk = ensure_object(lk)\\n\\t\\trk = ensure_object(rk)\\n\\trizer = klass(max(len(lk), len(rk)))\\n\\tllab = rizer.factorize(lk)\\n\\trlab = rizer.factorize(rk)\\n\\tcount = rizer.get_count()\\n\\tif sort:\\n\\t\\tuniques = rizer.uniques.to_array()\\n\\t\\tllab, rlab = _sort_labels(uniques, llab, rlab)\\n\\tlmask = llab == -1\\n\\tlany = lmask.any()\\n\\trmask = rlab == -1\\n\\trany = rmask.any()\\n\\tif lany or rany:\\n\\t\\tif lany:\\n\\t\\t\\tnp.putmask(llab, lmask, count)\\n\\t\\tif rany:\\n\\t\\t\\tnp.putmask(rlab, rmask, count)\\n\\t\\tcount += 1\\n\\tif how == \"right\":\\n\\t\\treturn rlab, llab, count\\n\\treturn llab, rlab, count"
  },
  {
    "code": "def _find_mac(command, args, hw_identifiers, get_index):\\n    import os\\n    for dir in ['', '/sbin/', '/usr/sbin']:\\n        executable = os.path.join(dir, command)\\n        if not os.path.exists(executable):\\n            continue\\n        try:\\n            cmd = 'LC_ALL=C %s %s 2>/dev/null' % (executable, args)\\n            with os.popen(cmd) as pipe:\\n                for line in pipe:\\n                    words = line.lower().split()\\n                    for i in range(len(words)):\\n                        if words[i] in hw_identifiers:\\n                            return int(\\n                                words[get_index(i)].replace(':', ''), 16)\\n        except IOError:\\n            continue\\n    return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #11508: Fixed uuid.getnode() and uuid.uuid1() on environment with virtual interface.  Original patch by Kent Frazier.",
    "fixed_code": "def _find_mac(command, args, hw_identifiers, get_index):\\n    import os\\n    for dir in ['', '/sbin/', '/usr/sbin']:\\n        executable = os.path.join(dir, command)\\n        if not os.path.exists(executable):\\n            continue\\n        try:\\n            cmd = 'LC_ALL=C %s %s 2>/dev/null' % (executable, args)\\n            with os.popen(cmd) as pipe:\\n                for line in pipe:\\n                    words = line.lower().split()\\n                    for i in range(len(words)):\\n                        if words[i] in hw_identifiers:\\n                            try:\\n                                return int(\\n                                    words[get_index(i)].replace(':', ''), 16)\\n                            except (ValueError, IndexError):\\n                                pass\\n        except IOError:\\n            continue\\n    return None"
  },
  {
    "code": "def __init__(self, optionalRelease, mandatoryRelease, compiler_flag):\\n        self.optional = optionalRelease\\n        self.mandatory = mandatoryRelease\\n        self.compiler_flag = compiler_flag",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, optionalRelease, mandatoryRelease, compiler_flag):\\n        self.optional = optionalRelease\\n        self.mandatory = mandatoryRelease\\n        self.compiler_flag = compiler_flag"
  },
  {
    "code": "def _pprint_dict(seq, _nest_lvl=0,**kwds):\\n    fmt = u\"{%s}\"\\n    pairs = []\\n    pfmt = u\"%s: %s\"\\n    nitems = get_option(\"max_seq_items\") or len(seq)\\n    for k, v in seq.items()[:nitems]:\\n        pairs.append(pfmt % (pprint_thing(k,_nest_lvl+1,**kwds),\\n                             pprint_thing(v,_nest_lvl+1,**kwds)))\\n    if nitems < len(seq):\\n        return fmt % (\", \".join(pairs) + \", ...\")\\n    else:\\n        return fmt % \", \".join(pairs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _pprint_dict(seq, _nest_lvl=0,**kwds):\\n    fmt = u\"{%s}\"\\n    pairs = []\\n    pfmt = u\"%s: %s\"\\n    nitems = get_option(\"max_seq_items\") or len(seq)\\n    for k, v in seq.items()[:nitems]:\\n        pairs.append(pfmt % (pprint_thing(k,_nest_lvl+1,**kwds),\\n                             pprint_thing(v,_nest_lvl+1,**kwds)))\\n    if nitems < len(seq):\\n        return fmt % (\", \".join(pairs) + \", ...\")\\n    else:\\n        return fmt % \", \".join(pairs)"
  },
  {
    "code": "def __str__(self, eng=False, context=None):\\n        sign = ['', '-'][self._sign]\\n        if self._is_special:\\n            if self._exp == 'F':\\n                return sign + 'Infinity'\\n            elif self._exp == 'n':\\n                return sign + 'NaN' + self._int\\n            else: \\n                return sign + 'sNaN' + self._int\\n        leftdigits = self._exp + len(self._int)\\n        if self._exp <= 0 and leftdigits > -6:\\n            dotplace = leftdigits\\n        elif not eng:\\n            dotplace = 1\\n        elif self._int == '0':\\n            dotplace = (leftdigits + 1) % 3 - 1\\n        else:\\n            dotplace = (leftdigits - 1) % 3 + 1\\n        if dotplace <= 0:\\n            intpart = '0'\\n            fracpart = '.' + '0'*(-dotplace) + self._int\\n        elif dotplace >= len(self._int):\\n            intpart = self._int+'0'*(dotplace-len(self._int))\\n            fracpart = ''\\n        else:\\n            intpart = self._int[:dotplace]\\n            fracpart = '.' + self._int[dotplace:]\\n        if leftdigits == dotplace:\\n            exp = ''\\n        else:\\n            if context is None:\\n                context = getcontext()\\n            exp = ['e', 'E'][context.capitals] + \"%+d\" % (leftdigits-dotplace)\\n        return sign + intpart + fracpart + exp",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __str__(self, eng=False, context=None):\\n        sign = ['', '-'][self._sign]\\n        if self._is_special:\\n            if self._exp == 'F':\\n                return sign + 'Infinity'\\n            elif self._exp == 'n':\\n                return sign + 'NaN' + self._int\\n            else: \\n                return sign + 'sNaN' + self._int\\n        leftdigits = self._exp + len(self._int)\\n        if self._exp <= 0 and leftdigits > -6:\\n            dotplace = leftdigits\\n        elif not eng:\\n            dotplace = 1\\n        elif self._int == '0':\\n            dotplace = (leftdigits + 1) % 3 - 1\\n        else:\\n            dotplace = (leftdigits - 1) % 3 + 1\\n        if dotplace <= 0:\\n            intpart = '0'\\n            fracpart = '.' + '0'*(-dotplace) + self._int\\n        elif dotplace >= len(self._int):\\n            intpart = self._int+'0'*(dotplace-len(self._int))\\n            fracpart = ''\\n        else:\\n            intpart = self._int[:dotplace]\\n            fracpart = '.' + self._int[dotplace:]\\n        if leftdigits == dotplace:\\n            exp = ''\\n        else:\\n            if context is None:\\n                context = getcontext()\\n            exp = ['e', 'E'][context.capitals] + \"%+d\" % (leftdigits-dotplace)\\n        return sign + intpart + fracpart + exp"
  },
  {
    "code": "def __init__(self, n_components='auto', density='auto', eps=0.5,\\n                 dense_output=False, random_state=None):\\n        self.n_components = n_components\\n        self.density = density\\n        self.eps = eps\\n        self.dense_output = dense_output\\n        self.random_state = random_state",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add gaussian projeciton + refactor sparse random matrix to reuse code",
    "fixed_code": "def __init__(self, n_components='auto', density='auto', eps=0.5,\\n                 dense_output=False, random_state=None,\\n                 distribution=\"bernouilli\"):\\n        self.n_components = n_components\\n        self.density = density\\n        self.eps = eps\\n        self.dense_output = dense_output\\n        self.random_state = random_state\\n        self.distribution = distribution\\n        self.components_ = None\\n        self.n_components_ = None"
  },
  {
    "code": "def validate_one(func_name):\\n    doc = Docstring(func_name)\\n    errs = []\\n    wrns = []\\n    if doc.start_blank_lines != 1:\\n        errs.append(error('GL01'))\\n    if doc.end_blank_lines != 1:\\n        errs.append(error('GL02'))\\n    if doc.double_blank_lines:\\n        errs.append(error('GL03'))\\n    mentioned_errs = doc.mentioned_private_classes\\n    if mentioned_errs:\\n        errs.append(error('GL04',\\n                          mentioned_private_classes=', '.join(mentioned_errs)))\\n    for line in doc.raw_doc.splitlines():\\n        if re.match(\"^ *\\t\", line):\\n            errs.append(error('GL05', line_with_tabs=line.lstrip()))\\n    unseen_sections = list(ALLOWED_SECTIONS)\\n    for section in doc.section_titles:\\n        if section not in ALLOWED_SECTIONS:\\n            errs.append(error('GL06',\\n                              section=section,\\n                              allowed_sections=', '.join(ALLOWED_SECTIONS)))\\n        else:\\n            if section in unseen_sections:\\n                section_idx = unseen_sections.index(section)\\n                unseen_sections = unseen_sections[section_idx + 1:]\\n            else:\\n                section_idx = ALLOWED_SECTIONS.index(section)\\n                goes_before = ALLOWED_SECTIONS[section_idx + 1]\\n                errs.append(error('GL07',\\n                                  sorted_sections=' > '.join(ALLOWED_SECTIONS),\\n                                  wrong_section=section,\\n                                  goes_before=goes_before))\\n                break\\n    if not doc.summary:\\n        errs.append(error('SS01'))\\n    else:\\n        if not doc.summary[0].isupper():\\n            errs.append(error('SS02'))\\n        if doc.summary[-1] != '.':\\n            errs.append(error('SS03'))\\n        if doc.summary != doc.summary.lstrip():\\n            errs.append(error('SS04'))\\n        elif (doc.is_function_or_method\\n                and doc.summary.split(' ')[0][-1] == 's'):\\n            errs.append(error('SS05'))\\n        if doc.num_summary_lines > 1:\\n            errs.append(error('SS06'))\\n    if not doc.extended_summary:\\n        wrns.append(('ES01', 'No extended summary found'))\\n    errs += doc.parameter_mismatches\\n    for param in doc.doc_parameters:\\n        if not param.startswith(\"*\"):  \\n            if not doc.parameter_type(param):\\n                if ':' in param:\\n                    errs.append(error('PR10',\\n                                      param_name=param.split(':')[0]))\\n                else:\\n                    errs.append(error('PR04', param_name=param))\\n            else:\\n                if doc.parameter_type(param)[-1] == '.':\\n                    errs.append(error('PR05', param_name=param))\\n                common_type_errors = [('integer', 'int'),\\n                                      ('boolean', 'bool'),\\n                                      ('string', 'str')]\\n                for wrong_type, right_type in common_type_errors:\\n                    if wrong_type in doc.parameter_type(param):\\n                        errs.append(error('PR06',\\n                                          param_name=param,\\n                                          right_type=right_type,\\n                                          wrong_type=wrong_type))\\n        if not doc.parameter_desc(param):\\n            errs.append(error('PR07', param_name=param))\\n        else:\\n            if not doc.parameter_desc(param)[0].isupper():\\n                errs.append(error('PR08', param_name=param))\\n            if doc.parameter_desc(param)[-1] != '.':\\n                errs.append(error('PR09', param_name=param))\\n    if doc.is_function_or_method:\\n        if not doc.returns and 'return' in doc.method_source:\\n            errs.append(error('RT01'))\\n        if not doc.yields and 'yield' in doc.method_source:\\n            errs.append(error('YD01'))\\n    if not doc.see_also:\\n        wrns.append(error('SA01'))\\n    else:\\n        for rel_name, rel_desc in doc.see_also.items():\\n            if rel_desc:\\n                if not rel_desc.endswith('.'):\\n                    errs.append(error('SA02', reference_name=rel_name))\\n                if not rel_desc[0].isupper():\\n                    errs.append(error('SA03', reference_name=rel_name))\\n            else:\\n                errs.append(error('SA04', reference_name=rel_name))\\n            if rel_name.startswith('pandas.'):\\n                errs.append(error('SA05',\\n                                  reference_name=rel_name,\\n                                  right_reference=rel_name[len('pandas.'):]))\\n    examples_errs = ''\\n    if not doc.examples:\\n        wrns.append(error('EX01'))\\n    else:\\n        examples_errs = doc.examples_errors\\n        if examples_errs:\\n            errs.append(error('EX02', doctest_log=examples_errs))\\n        for err in doc.validate_pep8():\\n            errs.append(error('EX03',\\n                              error_code=err.error_code,\\n                              error_message=err.message,\\n                              times_happening=' ({} times)'.format(err.count)\\n                                              if err.count > 1 else ''))\\n        examples_source_code = ''.join(doc.examples_source_code)\\n        for wrong_import in ('numpy', 'pandas'):\\n            if 'import {}'.format(wrong_import) in examples_source_code:\\n                errs.append(error('EX04', imported_library=wrong_import))\\n    return {'type': doc.type,\\n            'docstring': doc.clean_doc,\\n            'deprecated': doc.deprecated,\\n            'file': doc.source_file_name,\\n            'file_line': doc.source_file_def_line,\\n            'github_link': doc.github_url,\\n            'errors': errs,\\n            'warnings': wrns,\\n            'examples_errors': examples_errs}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate_one(func_name):\\n    doc = Docstring(func_name)\\n    errs = []\\n    wrns = []\\n    if doc.start_blank_lines != 1:\\n        errs.append(error('GL01'))\\n    if doc.end_blank_lines != 1:\\n        errs.append(error('GL02'))\\n    if doc.double_blank_lines:\\n        errs.append(error('GL03'))\\n    mentioned_errs = doc.mentioned_private_classes\\n    if mentioned_errs:\\n        errs.append(error('GL04',\\n                          mentioned_private_classes=', '.join(mentioned_errs)))\\n    for line in doc.raw_doc.splitlines():\\n        if re.match(\"^ *\\t\", line):\\n            errs.append(error('GL05', line_with_tabs=line.lstrip()))\\n    unseen_sections = list(ALLOWED_SECTIONS)\\n    for section in doc.section_titles:\\n        if section not in ALLOWED_SECTIONS:\\n            errs.append(error('GL06',\\n                              section=section,\\n                              allowed_sections=', '.join(ALLOWED_SECTIONS)))\\n        else:\\n            if section in unseen_sections:\\n                section_idx = unseen_sections.index(section)\\n                unseen_sections = unseen_sections[section_idx + 1:]\\n            else:\\n                section_idx = ALLOWED_SECTIONS.index(section)\\n                goes_before = ALLOWED_SECTIONS[section_idx + 1]\\n                errs.append(error('GL07',\\n                                  sorted_sections=' > '.join(ALLOWED_SECTIONS),\\n                                  wrong_section=section,\\n                                  goes_before=goes_before))\\n                break\\n    if not doc.summary:\\n        errs.append(error('SS01'))\\n    else:\\n        if not doc.summary[0].isupper():\\n            errs.append(error('SS02'))\\n        if doc.summary[-1] != '.':\\n            errs.append(error('SS03'))\\n        if doc.summary != doc.summary.lstrip():\\n            errs.append(error('SS04'))\\n        elif (doc.is_function_or_method\\n                and doc.summary.split(' ')[0][-1] == 's'):\\n            errs.append(error('SS05'))\\n        if doc.num_summary_lines > 1:\\n            errs.append(error('SS06'))\\n    if not doc.extended_summary:\\n        wrns.append(('ES01', 'No extended summary found'))\\n    errs += doc.parameter_mismatches\\n    for param in doc.doc_parameters:\\n        if not param.startswith(\"*\"):  \\n            if not doc.parameter_type(param):\\n                if ':' in param:\\n                    errs.append(error('PR10',\\n                                      param_name=param.split(':')[0]))\\n                else:\\n                    errs.append(error('PR04', param_name=param))\\n            else:\\n                if doc.parameter_type(param)[-1] == '.':\\n                    errs.append(error('PR05', param_name=param))\\n                common_type_errors = [('integer', 'int'),\\n                                      ('boolean', 'bool'),\\n                                      ('string', 'str')]\\n                for wrong_type, right_type in common_type_errors:\\n                    if wrong_type in doc.parameter_type(param):\\n                        errs.append(error('PR06',\\n                                          param_name=param,\\n                                          right_type=right_type,\\n                                          wrong_type=wrong_type))\\n        if not doc.parameter_desc(param):\\n            errs.append(error('PR07', param_name=param))\\n        else:\\n            if not doc.parameter_desc(param)[0].isupper():\\n                errs.append(error('PR08', param_name=param))\\n            if doc.parameter_desc(param)[-1] != '.':\\n                errs.append(error('PR09', param_name=param))\\n    if doc.is_function_or_method:\\n        if not doc.returns and 'return' in doc.method_source:\\n            errs.append(error('RT01'))\\n        if not doc.yields and 'yield' in doc.method_source:\\n            errs.append(error('YD01'))\\n    if not doc.see_also:\\n        wrns.append(error('SA01'))\\n    else:\\n        for rel_name, rel_desc in doc.see_also.items():\\n            if rel_desc:\\n                if not rel_desc.endswith('.'):\\n                    errs.append(error('SA02', reference_name=rel_name))\\n                if not rel_desc[0].isupper():\\n                    errs.append(error('SA03', reference_name=rel_name))\\n            else:\\n                errs.append(error('SA04', reference_name=rel_name))\\n            if rel_name.startswith('pandas.'):\\n                errs.append(error('SA05',\\n                                  reference_name=rel_name,\\n                                  right_reference=rel_name[len('pandas.'):]))\\n    examples_errs = ''\\n    if not doc.examples:\\n        wrns.append(error('EX01'))\\n    else:\\n        examples_errs = doc.examples_errors\\n        if examples_errs:\\n            errs.append(error('EX02', doctest_log=examples_errs))\\n        for err in doc.validate_pep8():\\n            errs.append(error('EX03',\\n                              error_code=err.error_code,\\n                              error_message=err.message,\\n                              times_happening=' ({} times)'.format(err.count)\\n                                              if err.count > 1 else ''))\\n        examples_source_code = ''.join(doc.examples_source_code)\\n        for wrong_import in ('numpy', 'pandas'):\\n            if 'import {}'.format(wrong_import) in examples_source_code:\\n                errs.append(error('EX04', imported_library=wrong_import))\\n    return {'type': doc.type,\\n            'docstring': doc.clean_doc,\\n            'deprecated': doc.deprecated,\\n            'file': doc.source_file_name,\\n            'file_line': doc.source_file_def_line,\\n            'github_link': doc.github_url,\\n            'errors': errs,\\n            'warnings': wrns,\\n            'examples_errors': examples_errs}"
  },
  {
    "code": "def deb_version(self):\\n        v = self._parsed_version\\n        match = self._parsed_regex_match\\n        if v.is_prerelease:\\n            if match.group('pre'):\\n                tag_value = match.group('pre')\\n                tag_type = match.group('pre_l')\\n                tag_ver = match.group('pre_n')\\n            elif match.group('dev'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('dev')\\n                tag_ver = match.group('dev_n')\\n            else:\\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\\n        elif v.is_postrelease:\\n            raise Exception(\"post-release identifiers are not supported\")\\n        else:\\n            tag_type = None\\n            tag_value = ''\\n            tag_ver = 0\\n        if not tag_type:\\n            return '{base_version}'.format(base_version=self.base_version)\\n        return '{base_version}~{tag_value}'.format(base_version=self.base_version, tag_value=tag_value)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "allow dev and prerelease at the same time (#41464)",
    "fixed_code": "def deb_version(self):\\n        v = self._parsed_version\\n        match = self._parsed_regex_match\\n        if v.is_prerelease or match.group('dev'):\\n            if match.group('pre'):\\n                tag_value = match.group('pre')\\n                tag_type = match.group('pre_l')\\n                if match.group('dev'):\\n                    tag_value += ('~%s' % match.group('dev').strip('.'))\\n            elif match.group('dev'):\\n                tag_type = \"dev\"\\n                tag_value = match.group('dev').strip('.')\\n            else:\\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\\n        elif v.is_postrelease:\\n            raise Exception(\"post-release identifiers are not supported\")\\n        else:\\n            tag_type = None\\n            tag_value = ''\\n        if not tag_type:\\n            return '{base_version}'.format(base_version=self.base_version)\\n        return '{base_version}~{tag_value}'.format(base_version=self.base_version, tag_value=tag_value)\\n    @property"
  },
  {
    "code": "def _execute_insert(self, conn, keys, data_iter):\\n        data = [{k: v for k, v in zip(keys, row)} for row in data_iter]\\n        conn.execute(self.insert_statement(), data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _execute_insert(self, conn, keys, data_iter):\\n        data = [{k: v for k, v in zip(keys, row)} for row in data_iter]\\n        conn.execute(self.insert_statement(), data)"
  },
  {
    "code": "def __neg__(self):\\n        return self * -1\\n    __eq__ = comp_method(operator.eq, '__eq__')\\n    __ne__ = comp_method(operator.ne, '__ne__')\\n    __lt__ = comp_method(operator.lt, '__lt__')\\n    __gt__ = comp_method(operator.gt, '__gt__')\\n    __le__ = comp_method(operator.le, '__le__')\\n    __ge__ = comp_method(operator.ge, '__ge__')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix unary - on boolean DataFrame",
    "fixed_code": "def __neg__(self):\\n        arr = operator.neg(self.values)\\n        return self._wrap_array(arr, self.axes, copy=False)\\n    __eq__ = comp_method(operator.eq, '__eq__')\\n    __ne__ = comp_method(operator.ne, '__ne__')\\n    __lt__ = comp_method(operator.lt, '__lt__')\\n    __gt__ = comp_method(operator.gt, '__gt__')\\n    __le__ = comp_method(operator.le, '__le__')\\n    __ge__ = comp_method(operator.ge, '__ge__')"
  },
  {
    "code": "def rebuild_auth(self, prepared_request, response):\\n\\t\\theaders = prepared_request.headers\\n\\t\\turl = prepared_request.url\\n\\t\\tif 'Authorization' in headers:\\n\\t\\t\\toriginal_parsed = urlparse(response.request.url)\\n\\t\\t\\tredirect_parsed = urlparse(url)\\n\\t\\t\\tif (original_parsed.hostname != redirect_parsed.hostname):\\n\\t\\t\\t\\tdel headers['Authorization']\\n\\t\\tnew_auth = get_netrc_auth(url) if self.trust_env else None\\n\\t\\tif new_auth is not None:\\n\\t\\t\\tprepared_request.prepare_auth(new_auth)\\n\\t\\treturn",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Strip Authorization header whenever root URL changes\\n\\nPreviously the header was stripped only if the hostname changed, but in\\nan https -> http redirect that can leak the credentials on the wire\\n(#4716). Based on with RFC 7235 section 2.2, the header is now stripped\\nif the \"canonical root URL\" (scheme+authority) has changed, by checking\\nscheme, hostname and port.",
    "fixed_code": "def rebuild_auth(self, prepared_request, response):\\n\\t\\theaders = prepared_request.headers\\n\\t\\turl = prepared_request.url\\n\\t\\tif 'Authorization' in headers:\\n\\t\\t\\toriginal_parsed = urlparse(response.request.url)\\n\\t\\t\\tredirect_parsed = urlparse(url)\\n\\t\\t\\tif (original_parsed.hostname != redirect_parsed.hostname\\n\\t\\t\\t\\t\\tor original_parsed.port != redirect_parsed.port\\n\\t\\t\\t\\t\\tor original_parsed.scheme != redirect_parsed.scheme):\\n\\t\\t\\t\\tdel headers['Authorization']\\n\\t\\tnew_auth = get_netrc_auth(url) if self.trust_env else None\\n\\t\\tif new_auth is not None:\\n\\t\\t\\tprepared_request.prepare_auth(new_auth)\\n\\t\\treturn"
  },
  {
    "code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(terms)))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "make CountVectorizer able to output binary occurrence info",
    "fixed_code": "def fit_transform(self, raw_documents, y=None):\\n        if self.fixed_vocabulary is not None:\\n            return self.transform(raw_documents)\\n        self.vocabulary_ = {}\\n        term_counts_per_doc = []\\n        term_counts = Counter()\\n        document_counts = Counter()\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        analyze = self.build_analyzer()\\n        for doc in raw_documents:\\n            term_count_current = Counter(analyze(doc))\\n            term_counts.update(term_count_current)\\n            if max_df < 1.0:\\n                document_counts.update(term_count_current.iterkeys())\\n            term_counts_per_doc.append(term_count_current)\\n        n_doc = len(term_counts_per_doc)\\n        if max_df < 1.0:\\n            max_document_count = max_df * n_doc\\n            stop_words = set(t for t, dc in document_counts.iteritems()\\n                               if dc > max_document_count)\\n        else:\\n            stop_words = set()\\n        if max_features is None:\\n            terms = set(term_counts) - stop_words\\n        else:\\n            terms = set()\\n            for t, tc in term_counts.most_common():\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        self.max_df_stop_words_ = stop_words\\n        self.vocabulary_ = dict(((t, i) for i, t in enumerate(sorted(terms))))\\n        return self._term_count_dicts_to_matrix(term_counts_per_doc)"
  },
  {
    "code": "def _binary_clf_curve(y_true, y_score, pos_label=None):\\n    y_true, y_score = check_arrays(y_true, y_score)\\n    if (not (y_true.ndim == 1 or\\n            (y_true.ndim == 2 and y_true.shape[1] == 1)) or\\n            not (y_score.ndim == 1 or\\n                (y_score.ndim == 2 and y_score.shape[1] == 1))):\\n        raise ValueError(\"Bad input shape\")\\n    y_true = np.squeeze(y_true)\\n    y_score = np.squeeze(y_score)\\n    classes = np.unique(y_true)\\n    if (pos_label is None and\\n        not (np.all(classes == [0, 1]) or\\n             np.all(classes == [-1, 1]) or\\n             np.all(classes == [0]) or\\n             np.all(classes == [-1]) or\\n             np.all(classes == [1]))):\\n        raise ValueError(\"Data is not binary and pos_label is not specified\")\\n    elif pos_label is None:\\n        pos_label = 1.\\n    y_true = (y_true == pos_label)\\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\\n    y_score = y_score[desc_score_indices]\\n    y_true = y_true[desc_score_indices]\\n    distinct_value_indices = np.where(np.diff(y_score))[0]\\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\\n    tps = y_true.cumsum()[threshold_idxs]\\n    fps = 1 + threshold_idxs - tps\\n    return fps, tps, y_score[threshold_idxs]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _binary_clf_curve(y_true, y_score, pos_label=None):\\n    y_true, y_score = check_arrays(y_true, y_score)\\n    if (not (y_true.ndim == 1 or\\n            (y_true.ndim == 2 and y_true.shape[1] == 1)) or\\n            not (y_score.ndim == 1 or\\n                (y_score.ndim == 2 and y_score.shape[1] == 1))):\\n        raise ValueError(\"Bad input shape\")\\n    y_true = np.squeeze(y_true)\\n    y_score = np.squeeze(y_score)\\n    classes = np.unique(y_true)\\n    if (pos_label is None and\\n        not (np.all(classes == [0, 1]) or\\n             np.all(classes == [-1, 1]) or\\n             np.all(classes == [0]) or\\n             np.all(classes == [-1]) or\\n             np.all(classes == [1]))):\\n        raise ValueError(\"Data is not binary and pos_label is not specified\")\\n    elif pos_label is None:\\n        pos_label = 1.\\n    y_true = (y_true == pos_label)\\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\\n    y_score = y_score[desc_score_indices]\\n    y_true = y_true[desc_score_indices]\\n    distinct_value_indices = np.where(np.diff(y_score))[0]\\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\\n    tps = y_true.cumsum()[threshold_idxs]\\n    fps = 1 + threshold_idxs - tps\\n    return fps, tps, y_score[threshold_idxs]"
  },
  {
    "code": "def delete_dag(dag_id: str, keep_records_in_log: bool = True, session=None) -> int:\\n    log.info(\"Deleting DAG: %s\", dag_id)\\n    dag = session.query(DagModel).filter(DagModel.dag_id == dag_id).first()\\n    if dag is None:\\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\\n    if STORE_SERIALIZED_DAGS and SerializedDagModel.has_dag(dag_id=dag_id, session=session):\\n        SerializedDagModel.remove_dag(dag_id=dag_id, session=session)\\n    count = 0\\n    for model in models.base.Base._decl_class_registry.values():  \\n        if hasattr(model, \"dag_id\"):\\n            if keep_records_in_log and model.__name__ == 'Log':\\n                continue\\n            cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))\\n            count += session.query(model).filter(cond).delete(synchronize_session='fetch')\\n    if dag.is_subdag:\\n        parent_dag_id, task_id = dag_id.rsplit(\".\", 1)\\n        for model in models.DagRun, TaskFail, models.TaskInstance:\\n            count += session.query(model).filter(model.dag_id == parent_dag_id,\\n                                                 model.task_id == task_id).delete()\\n    session.query(models.ImportError).filter(\\n        models.ImportError.filename == dag.fileloc\\n    ).delete(synchronize_session='fetch')\\n    return count",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Call delete_dag on subdag without AttributeError (#8017)\\n\\nThe DagRun object does not have a task_id attribute, DagRuns are deleted in the\\nblock above so no need to do it here or remove ones belonging to the\\nparent.",
    "fixed_code": "def delete_dag(dag_id: str, keep_records_in_log: bool = True, session=None) -> int:\\n    log.info(\"Deleting DAG: %s\", dag_id)\\n    dag = session.query(DagModel).filter(DagModel.dag_id == dag_id).first()\\n    if dag is None:\\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\\n    if STORE_SERIALIZED_DAGS and SerializedDagModel.has_dag(dag_id=dag_id, session=session):\\n        SerializedDagModel.remove_dag(dag_id=dag_id, session=session)\\n    count = 0\\n    for model in models.base.Base._decl_class_registry.values():  \\n        if hasattr(model, \"dag_id\"):\\n            if keep_records_in_log and model.__name__ == 'Log':\\n                continue\\n            cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))\\n            count += session.query(model).filter(cond).delete(synchronize_session='fetch')\\n    if dag.is_subdag:\\n        parent_dag_id, task_id = dag_id.rsplit(\".\", 1)\\n        for model in TaskFail, models.TaskInstance:\\n            count += session.query(model).filter(model.dag_id == parent_dag_id,\\n                                                 model.task_id == task_id).delete()\\n    session.query(models.ImportError).filter(\\n        models.ImportError.filename == dag.fileloc\\n    ).delete(synchronize_session='fetch')\\n    return count"
  },
  {
    "code": "def write_pot_file(potfile, msgs):\\n    pot_lines = msgs.splitlines()\\n    if os.path.exists(potfile):\\n        lines = dropwhile(len, pot_lines)\\n    else:\\n        lines = []\\n        found, header_read = False, False\\n        for line in pot_lines:\\n            if not found and not header_read:\\n                if 'charset=CHARSET' in line:\\n                    found = True\\n                    line = line.replace('charset=CHARSET', 'charset=UTF-8')\\n            if not line and not found:\\n                header_read = True\\n            lines.append(line)\\n    msgs = '\\n'.join(lines)\\n' to work around\\n    with open(potfile, 'a', encoding='utf-8', newline='\\n') as fp:\\n        fp.write(msgs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def write_pot_file(potfile, msgs):\\n    pot_lines = msgs.splitlines()\\n    if os.path.exists(potfile):\\n        lines = dropwhile(len, pot_lines)\\n    else:\\n        lines = []\\n        found, header_read = False, False\\n        for line in pot_lines:\\n            if not found and not header_read:\\n                if 'charset=CHARSET' in line:\\n                    found = True\\n                    line = line.replace('charset=CHARSET', 'charset=UTF-8')\\n            if not line and not found:\\n                header_read = True\\n            lines.append(line)\\n    msgs = '\\n'.join(lines)\\n' to work around\\n    with open(potfile, 'a', encoding='utf-8', newline='\\n') as fp:\\n        fp.write(msgs)"
  },
  {
    "code": "def _initMatrix(self, data, index, columns, dtype):\\n        N, K = data.shape\\n        if len(index) != N:\\n            raise Exception('Index length mismatch: %d vs. %d' %\\n                            (len(index), N))\\n        if len(columns) != K:\\n            raise Exception('Index length mismatch: %d vs. %d' %\\n                            (len(columns), K))\\n        data = dict([(idx, data[:, i]) for i, idx in enumerate(columns)])\\n        return self._initDict(data, index, columns, dtype)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "test coverage",
    "fixed_code": "def _initMatrix(self, data, index, columns, dtype):\\n        if not isinstance(data, np.ndarray):\\n            arr = np.array(data)\\n            if issubclass(arr.dtype.type, basestring):\\n                arr = np.array(data, dtype=object, copy=True)\\n            data = arr\\n        if data.ndim == 1:\\n            data = data.reshape((len(data), 1))\\n        elif data.ndim != 2:\\n            raise Exception('Must pass 2-d input!')\\n        if columns is None:\\n            raise Exception('Must pass column names')\\n        if index is None:\\n            raise Exception('Must pass index')\\n        N, K = data.shape\\n        if len(index) != N:\\n            raise Exception('Index length mismatch: %d vs. %d' %\\n                            (len(index), N))\\n        if len(columns) != K:\\n            raise Exception('Index length mismatch: %d vs. %d' %\\n                            (len(columns), K))\\n        data = dict([(idx, data[:, i]) for i, idx in enumerate(columns)])\\n        return self._initDict(data, index, columns, dtype)\\n    @property"
  },
  {
    "code": "def str_len(a):\\n    return _vec_string(a, int_, '__len__')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def str_len(a):\\n    return _vec_string(a, int_, '__len__')"
  },
  {
    "code": "def _parse_sitemap(self, response):\\n\\t\\tif response.url.endswith('/robots.txt'):\\n\\t\\t\\tfor url in sitemap_urls_from_robots(response.text):\\n\\t\\t\\t\\tyield Request(url, callback=self._parse_sitemap)\\n\\t\\telse:\\n\\t\\t\\tbody = self._get_sitemap_body(response)\\n\\t\\t\\tif body is None:\\n\\t\\t\\t\\tlogger.warning(\"Ignoring invalid sitemap: %(response)s\",\\n\\t\\t\\t\\t\\t\\t\\t   {'response': response}, extra={'spider': self})\\n\\t\\t\\t\\treturn\\n\\t\\t\\ts = Sitemap(body)\\n\\t\\t\\tif s.type == 'sitemapindex':\\n\\t\\t\\t\\tfor loc in iterloc(s, self.sitemap_alternate_links):\\n\\t\\t\\t\\t\\tif any(x.search(loc) for x in self._follow):\\n\\t\\t\\t\\t\\t\\tyield Request(loc, callback=self._parse_sitemap)\\n\\t\\t\\telif s.type == 'urlset':\\n\\t\\t\\t\\tfor loc in iterloc(s):\\n\\t\\t\\t\\t\\tfor r, c in self._cbs:\\n\\t\\t\\t\\t\\t\\tif r.search(loc):\\n\\t\\t\\t\\t\\t\\t\\tyield Request(loc, callback=c)\\n\\t\\t\\t\\t\\t\\t\\tbreak",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_sitemap(self, response):\\n\\t\\tif response.url.endswith('/robots.txt'):\\n\\t\\t\\tfor url in sitemap_urls_from_robots(response.text):\\n\\t\\t\\t\\tyield Request(url, callback=self._parse_sitemap)\\n\\t\\telse:\\n\\t\\t\\tbody = self._get_sitemap_body(response)\\n\\t\\t\\tif body is None:\\n\\t\\t\\t\\tlogger.warning(\"Ignoring invalid sitemap: %(response)s\",\\n\\t\\t\\t\\t\\t\\t\\t   {'response': response}, extra={'spider': self})\\n\\t\\t\\t\\treturn\\n\\t\\t\\ts = Sitemap(body)\\n\\t\\t\\tif s.type == 'sitemapindex':\\n\\t\\t\\t\\tfor loc in iterloc(s, self.sitemap_alternate_links):\\n\\t\\t\\t\\t\\tif any(x.search(loc) for x in self._follow):\\n\\t\\t\\t\\t\\t\\tyield Request(loc, callback=self._parse_sitemap)\\n\\t\\t\\telif s.type == 'urlset':\\n\\t\\t\\t\\tfor loc in iterloc(s):\\n\\t\\t\\t\\t\\tfor r, c in self._cbs:\\n\\t\\t\\t\\t\\t\\tif r.search(loc):\\n\\t\\t\\t\\t\\t\\t\\tyield Request(loc, callback=c)\\n\\t\\t\\t\\t\\t\\t\\tbreak"
  },
  {
    "code": "def apply(self):\\n        self.autosupport_log()\\n        current = self.get_if_grp()\\n        cd_action = self.na_helper.get_cd_action(current, self.parameters)\\n        if current and self.parameters['state'] == 'present':\\n            current_ports = self.get_if_grp_ports()\\n            modify = self.na_helper.get_modified_attributes(current_ports, self.parameters)\\n        if self.na_helper.changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if cd_action == 'create':\\n                    self.create_if_grp()\\n                elif cd_action == 'delete':\\n                    self.delete_if_grp()\\n                elif modify:\\n                    self.modify_ports(current_ports['ports'])\\n        self.module.exit_json(changed=self.na_helper.changed)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Modify port would remove and re-add ports on a modify which can results in an error (#53341)",
    "fixed_code": "def apply(self):\\n        self.autosupport_log()\\n        current, modify = self.get_if_grp(), None\\n        cd_action = self.na_helper.get_cd_action(current, self.parameters)\\n        if cd_action is None and self.parameters['state'] == 'present':\\n            current_ports = self.get_if_grp_ports()\\n            modify = self.na_helper.get_modified_attributes(current_ports, self.parameters)\\n        if self.na_helper.changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if cd_action == 'create':\\n                    self.create_if_grp()\\n                elif cd_action == 'delete':\\n                    self.delete_if_grp()\\n                elif modify:\\n                    self.modify_ports(current_ports['ports'])\\n        self.module.exit_json(changed=self.na_helper.changed)"
  },
  {
    "code": "def custom_sql_for_model(model, style, connection):\\n    opts = model._meta\\n    app_dirs = []\\n    app_dir = apps.get_app_config(model._meta.app_label).path\\n    app_dirs.append(os.path.normpath(os.path.join(app_dir, 'sql')))\\n    old_app_dir = os.path.normpath(os.path.join(app_dir, 'models/sql'))\\n    if os.path.exists(old_app_dir):\\n        warnings.warn(\"Custom SQL location '<app_label>/models/sql' is \"\\n                      \"deprecated, use '<app_label>/sql' instead.\",\\n                      RemovedInDjango19Warning)\\n        app_dirs.append(old_app_dir)\\n    output = []\\n    if opts.managed:\\n        post_sql_fields = [f for f in opts.local_fields if hasattr(f, 'post_create_sql')]\\n        for f in post_sql_fields:\\n            output.extend(f.post_create_sql(style, model._meta.db_table))\\n    backend_name = connection.settings_dict['ENGINE'].split('.')[-1]\\n    sql_files = []\\n    for app_dir in app_dirs:\\n        sql_files.append(os.path.join(app_dir, \"%s.%s.sql\" % (opts.model_name, backend_name)))\\n        sql_files.append(os.path.join(app_dir, \"%s.sql\" % opts.model_name))\\n    for sql_file in sql_files:\\n        if os.path.exists(sql_file):\\n            with codecs.open(sql_file, 'r' if six.PY3 else 'U', encoding=settings.FILE_CHARSET) as fp:\\n                output.extend(_split_statements(fp.read()))\\n    return output",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #3214 -- Stopped parsing SQL with regex.\\n\\nAvoided introducing a new regex-based SQL splitter in the migrations\\nframework, before we're bound by backwards compatibility.\\n\\nAdapted this change to the legacy \"initial SQL data\" feature, even\\nthough it's already deprecated, in order to facilitate the transition\\nto migrations.\\n\\nsqlparse becomes mandatory for RunSQL on some databases (all but\\nPostgreSQL). There's no API to provide a single statement and tell\\nDjango not to attempt splitting. Since we have a more robust splitting\\nimplementation, that seems like a good tradeoff. It's easier to add a\\nnew keyword argument later if necessary than to remove one.\\n\\nMany people contributed to both tickets, thank you all, and especially\\nClaude for the review.\\n\\nRefs #22401.",
    "fixed_code": "def custom_sql_for_model(model, style, connection):\\n    opts = model._meta\\n    app_dirs = []\\n    app_dir = apps.get_app_config(model._meta.app_label).path\\n    app_dirs.append(os.path.normpath(os.path.join(app_dir, 'sql')))\\n    old_app_dir = os.path.normpath(os.path.join(app_dir, 'models/sql'))\\n    if os.path.exists(old_app_dir):\\n        warnings.warn(\"Custom SQL location '<app_label>/models/sql' is \"\\n                      \"deprecated, use '<app_label>/sql' instead.\",\\n                      RemovedInDjango19Warning)\\n        app_dirs.append(old_app_dir)\\n    output = []\\n    if opts.managed:\\n        post_sql_fields = [f for f in opts.local_fields if hasattr(f, 'post_create_sql')]\\n        for f in post_sql_fields:\\n            output.extend(f.post_create_sql(style, model._meta.db_table))\\n    backend_name = connection.settings_dict['ENGINE'].split('.')[-1]\\n    sql_files = []\\n    for app_dir in app_dirs:\\n        sql_files.append(os.path.join(app_dir, \"%s.%s.sql\" % (opts.model_name, backend_name)))\\n        sql_files.append(os.path.join(app_dir, \"%s.sql\" % opts.model_name))\\n    for sql_file in sql_files:\\n        if os.path.exists(sql_file):\\n            with codecs.open(sql_file, 'r' if six.PY3 else 'U', encoding=settings.FILE_CHARSET) as fp:\\n                output.extend(connection.ops.prepare_sql_script(fp.read(), _allow_fallback=True))\\n    return output"
  },
  {
    "code": "def __sub__(self, other):\\n        other_dtype = getattr(other, \"dtype\", None)\\n        if other is NaT:\\n            result = self._sub_nat()\\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n            result = self._add_timedeltalike_scalar(-other)\\n        elif isinstance(other, BaseOffset):\\n            result = self._add_offset(-other)\\n        elif isinstance(other, (datetime, np.datetime64)):\\n            result = self._sub_datetimelike_scalar(other)\\n        elif lib.is_integer(other):\\n            if not is_period_dtype(self.dtype):\\n                raise integer_op_not_supported(self)\\n            result = cast(\"PeriodArray\", self)._addsub_int_array_or_scalar(\\n                other * self.freq.n, operator.sub\\n            )\\n        elif isinstance(other, Period):\\n            result = self._sub_period(other)\\n        elif is_timedelta64_dtype(other_dtype):\\n            result = self._add_timedelta_arraylike(-other)\\n        elif is_object_dtype(other_dtype):\\n            result = self._addsub_object_array(other, operator.sub)\\n        elif is_datetime64_dtype(other_dtype) or is_datetime64tz_dtype(other_dtype):\\n            result = self._sub_datetime_arraylike(other)\\n        elif is_period_dtype(other_dtype):\\n            result = self._sub_period_array(other)\\n        elif is_integer_dtype(other_dtype):\\n            if not is_period_dtype(self.dtype):\\n                raise integer_op_not_supported(self)\\n            result = cast(\"PeriodArray\", self)._addsub_int_array_or_scalar(\\n                other * self.freq.n, operator.sub\\n            )\\n        else:\\n            return NotImplemented\\n        if isinstance(result, np.ndarray) and is_timedelta64_dtype(result.dtype):\\n            from pandas.core.arrays import TimedeltaArray\\n            return TimedeltaArray(result)\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: tighter typing in datetimelike arith methods (#48962)",
    "fixed_code": "def __sub__(self, other):\\n        other_dtype = getattr(other, \"dtype\", None)\\n        other = ensure_wrapped_if_datetimelike(other)\\n        if other is NaT:\\n            result = self._sub_nat()\\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n            result = self._add_timedeltalike_scalar(-other)\\n        elif isinstance(other, BaseOffset):\\n            result = self._add_offset(-other)\\n        elif isinstance(other, (datetime, np.datetime64)):\\n            result = self._sub_datetimelike_scalar(other)\\n        elif lib.is_integer(other):\\n            if not is_period_dtype(self.dtype):\\n                raise integer_op_not_supported(self)\\n            result = cast(\"PeriodArray\", self)._addsub_int_array_or_scalar(\\n                other * self.freq.n, operator.sub\\n            )\\n        elif isinstance(other, Period):\\n            result = self._sub_periodlike(other)\\n        elif is_timedelta64_dtype(other_dtype):\\n            result = self._add_timedelta_arraylike(-other)\\n        elif is_object_dtype(other_dtype):\\n            result = self._addsub_object_array(other, operator.sub)\\n        elif is_datetime64_dtype(other_dtype) or is_datetime64tz_dtype(other_dtype):\\n            result = self._sub_datetime_arraylike(other)\\n        elif is_period_dtype(other_dtype):\\n            result = self._sub_periodlike(other)\\n        elif is_integer_dtype(other_dtype):\\n            if not is_period_dtype(self.dtype):\\n                raise integer_op_not_supported(self)\\n            result = cast(\"PeriodArray\", self)._addsub_int_array_or_scalar(\\n                other * self.freq.n, operator.sub\\n            )\\n        else:\\n            return NotImplemented\\n        if isinstance(result, np.ndarray) and is_timedelta64_dtype(result.dtype):\\n            from pandas.core.arrays import TimedeltaArray\\n            return TimedeltaArray(result)\\n        return result"
  },
  {
    "code": "def __repr__(self):\\n        if self.method is None or not self.get_full_path():\\n            return force_str('<%s>' % self.__class__.__name__)\\n        return force_str(\\n            '<%s: %s %r>' % (self.__class__.__name__, self.method, force_str(self.get_full_path()))\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n        if self.method is None or not self.get_full_path():\\n            return force_str('<%s>' % self.__class__.__name__)\\n        return force_str(\\n            '<%s: %s %r>' % (self.__class__.__name__, self.method, force_str(self.get_full_path()))\\n        )"
  },
  {
    "code": "def to_datetime(arg, errors='ignore', dayfirst=False):\\n    from pandas.core.series import Series\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, datetime):\\n        return arg\\n    elif isinstance(arg, Series):\\n        values = lib.string_to_datetime(com._ensure_object(arg.values),\\n                                        raise_=errors == 'raise',\\n                                        dayfirst=dayfirst)\\n        return Series(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (np.ndarray, list)):\\n        if isinstance(arg, list):\\n            arg = np.array(arg, dtype='O')\\n        return lib.string_to_datetime(com._ensure_object(arg),\\n                                      raise_=errors == 'raise',\\n                                      dayfirst=dayfirst)\\n    try:\\n        return parser.parse(arg, dayfirst=dayfirst)\\n    except Exception:\\n        if errors == 'raise':\\n            raise\\n        return arg",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: treat empty string as NaT in to_datetime, close #1085",
    "fixed_code": "def to_datetime(arg, errors='ignore', dayfirst=False):\\n    from pandas.core.series import Series\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, datetime):\\n        return arg\\n    elif isinstance(arg, Series):\\n        values = lib.string_to_datetime(com._ensure_object(arg.values),\\n                                        raise_=errors == 'raise',\\n                                        dayfirst=dayfirst)\\n        return Series(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (np.ndarray, list)):\\n        if isinstance(arg, list):\\n            arg = np.array(arg, dtype='O')\\n        return lib.string_to_datetime(com._ensure_object(arg),\\n                                      raise_=errors == 'raise',\\n                                      dayfirst=dayfirst)\\n    try:\\n        if not arg:\\n            return arg\\n        return parser.parse(arg, dayfirst=dayfirst)\\n    except Exception:\\n        if errors == 'raise':\\n            raise\\n        return arg"
  },
  {
    "code": "def read(self, filenames):\\n        super(AirflowConfigParser, self).read(filenames)\\n        self._validate()\\n    def read_dict(self, *args, **kwargs):\\n        super(AirflowConfigParser, self).read_dict(*args, **kwargs)\\n        self._validate()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3742] Fix handling of \"fallback\" for AirflowConfigParsxer.getint/boolean (#4674)\\n\\nWe added (and used) fallback as an argument on `getboolean` but didn't\\nadd it to the method, or add tests covering those \"casting\" accessors,\\nso they broke.\\n\\nThis fixes those methods, and adds tests covering them",
    "fixed_code": "def read(self, filenames, **kwargs):\\n        super(AirflowConfigParser, self).read(filenames, **kwargs)\\n        self._validate()\\n    def read_dict(self, *args, **kwargs):\\n        super(AirflowConfigParser, self).read_dict(*args, **kwargs)\\n        self._validate()"
  },
  {
    "code": "def inaxes(self, xy):\\n\\t\\taxes_list = [a for a in self.figure.get_axes()\\n\\t\\t\\t\\t\\t if a.patch.contains_point(xy) and a.get_visible()]\\n\\t\\tif axes_list:\\n\\t\\t\\taxes = cbook._topmost_artist(axes_list)\\n\\t\\telse:\\n\\t\\t\\taxes = None\\n\\t\\treturn axes",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def inaxes(self, xy):\\n\\t\\taxes_list = [a for a in self.figure.get_axes()\\n\\t\\t\\t\\t\\t if a.patch.contains_point(xy) and a.get_visible()]\\n\\t\\tif axes_list:\\n\\t\\t\\taxes = cbook._topmost_artist(axes_list)\\n\\t\\telse:\\n\\t\\t\\taxes = None\\n\\t\\treturn axes"
  },
  {
    "code": "def parse_table_schema(json, precise_float):\\n    table = loads(json, precise_float=precise_float)\\n    col_order = [field[\"name\"] for field in table[\"schema\"][\"fields\"]]\\n    df = DataFrame(table[\"data\"], columns=col_order)[col_order]\\n    dtypes = {\\n        field[\"name\"]: convert_json_field_to_pandas_type(field)\\n        for field in table[\"schema\"][\"fields\"]\\n    }\\n    if any(str(x).startswith(\"datetime64[ns, \") for x in dtypes.values()):\\n        raise NotImplementedError('table=\"orient\" can not yet read timezone data')\\n    if \"timedelta64\" in dtypes.values():\\n        raise NotImplementedError(\\n            'table=\"orient\" can not yet read ISO-formatted Timedelta data'\\n        )\\n    df = df.astype(dtypes)\\n    if \"primaryKey\" in table[\"schema\"]:\\n        df = df.set_index(table[\"schema\"][\"primaryKey\"])\\n        if len(df.index.names) == 1:\\n            if df.index.name == \"index\":\\n                df.index.name = None\\n        else:\\n            df.index.names = [\\n                None if x.startswith(\"level_\") else x for x in df.index.names\\n            ]\\n    return df",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: implement timeszones support for read_json(orient='table') and astype() from 'object' (#35973)",
    "fixed_code": "def parse_table_schema(json, precise_float):\\n    table = loads(json, precise_float=precise_float)\\n    col_order = [field[\"name\"] for field in table[\"schema\"][\"fields\"]]\\n    df = DataFrame(table[\"data\"], columns=col_order)[col_order]\\n    dtypes = {\\n        field[\"name\"]: convert_json_field_to_pandas_type(field)\\n        for field in table[\"schema\"][\"fields\"]\\n    }\\n    if \"timedelta64\" in dtypes.values():\\n        raise NotImplementedError(\\n            'table=\"orient\" can not yet read ISO-formatted Timedelta data'\\n        )\\n    df = df.astype(dtypes)\\n    if \"primaryKey\" in table[\"schema\"]:\\n        df = df.set_index(table[\"schema\"][\"primaryKey\"])\\n        if len(df.index.names) == 1:\\n            if df.index.name == \"index\":\\n                df.index.name = None\\n        else:\\n            df.index.names = [\\n                None if x.startswith(\"level_\") else x for x in df.index.names\\n            ]\\n    return df"
  },
  {
    "code": "def sqrt(self, context=None):\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n            if self._isinfinity() and self._sign == 0:\\n                return Decimal(self)\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', self._exp // 2)\\n            return ans._fix(context)\\n        if self._sign == 1:\\n            return context._raise_error(InvalidOperation, 'sqrt(-x), x > 0')\\n        prec = context.prec+1\\n        op = _WorkRep(self)\\n        e = op.exp >> 1\\n        if op.exp & 1:\\n            c = op.int * 10\\n            l = (len(self._int) >> 1) + 1\\n        else:\\n            c = op.int\\n            l = len(self._int)+1 >> 1\\n        shift = prec-l\\n        if shift >= 0:\\n            c *= 100**shift\\n            exact = True\\n        else:\\n            c, remainder = divmod(c, 100**-shift)\\n            exact = not remainder\\n        e -= shift\\n        n = 10**prec\\n        while True:\\n            q = c//n\\n            if n <= q:\\n                break\\n            else:\\n                n = n + q >> 1\\n        exact = exact and n*n == c\\n        if exact:\\n            if shift >= 0:\\n                n //= 10**shift\\n            else:\\n                n *= 10**-shift\\n            e += shift\\n        else:\\n            if n % 5 == 0:\\n                n += 1\\n        ans = _dec_from_triple(0, str(n), e)\\n        context = context._shallow_copy()\\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\\n        ans = ans._fix(context)\\n        context.rounding = rounding\\n        return ans",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sqrt(self, context=None):\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n            if self._isinfinity() and self._sign == 0:\\n                return Decimal(self)\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', self._exp // 2)\\n            return ans._fix(context)\\n        if self._sign == 1:\\n            return context._raise_error(InvalidOperation, 'sqrt(-x), x > 0')\\n        prec = context.prec+1\\n        op = _WorkRep(self)\\n        e = op.exp >> 1\\n        if op.exp & 1:\\n            c = op.int * 10\\n            l = (len(self._int) >> 1) + 1\\n        else:\\n            c = op.int\\n            l = len(self._int)+1 >> 1\\n        shift = prec-l\\n        if shift >= 0:\\n            c *= 100**shift\\n            exact = True\\n        else:\\n            c, remainder = divmod(c, 100**-shift)\\n            exact = not remainder\\n        e -= shift\\n        n = 10**prec\\n        while True:\\n            q = c//n\\n            if n <= q:\\n                break\\n            else:\\n                n = n + q >> 1\\n        exact = exact and n*n == c\\n        if exact:\\n            if shift >= 0:\\n                n //= 10**shift\\n            else:\\n                n *= 10**-shift\\n            e += shift\\n        else:\\n            if n % 5 == 0:\\n                n += 1\\n        ans = _dec_from_triple(0, str(n), e)\\n        context = context._shallow_copy()\\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\\n        ans = ans._fix(context)\\n        context.rounding = rounding\\n        return ans"
  },
  {
    "code": "def connectionMade(self):\\n\\t\\tmethod = getattr(self.factory, 'method', b'GET')\\n\\t\\tself.sendCommand(method, self.factory.path)\\n\\t\\tif self.factory.scheme == b'http' and self.factory.port != 80:\\n\\t\\t\\thost = self.factory.host + b':' + intToBytes(self.factory.port)\\n\\t\\telif self.factory.scheme == b'https' and self.factory.port != 443:\\n\\t\\t\\thost = self.factory.host + b':' + intToBytes(self.factory.port)\\n\\t\\telse:\\n\\t\\t\\thost = self.factory.host\\n\\t\\tself.sendHeader(b'Host', self.factory.headers.get(b\"host\", host))\\n\\t\\tself.sendHeader(b'User-Agent', self.factory.agent)\\n\\t\\tdata = getattr(self.factory, 'postdata', None)\\n\\t\\tif data is not None:\\n\\t\\t\\tself.sendHeader(b\"Content-Length\", intToBytes(len(data)))\\n\\t\\tcookieData = []\\n\\t\\tfor (key, value) in self.factory.headers.items():\\n\\t\\t\\tif key.lower() not in self._specialHeaders:\\n\\t\\t\\t\\tself.sendHeader(key, value)\\n\\t\\t\\tif key.lower() == b'cookie':\\n\\t\\t\\t\\tcookieData.append(value)\\n\\t\\tfor cookie, cookval in self.factory.cookies.items():\\n\\t\\t\\tcookieData.append(cookie + b'=' + cookval)\\n\\t\\tif cookieData:\\n\\t\\t\\tself.sendHeader(b'Cookie', b'; '.join(cookieData))\\n\\t\\tself.endHeaders()\\n\\t\\tself.headers = {}\\n\\t\\tif data is not None:\\n\\t\\t\\tself.transport.write(data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Prevent CRLF injections described in CVE-2019-12387\\n\\nAuthor: markrwilliams\\n\\nReviewers: glyph\\n\\nFixes: ticket:9647\\n\\nTwisted's HTTP client APIs were vulnerable to maliciously constructed\\nHTTP methods, hosts, and/or paths, URI components such as paths and\\nquery parameters.  These vulnerabilities were beyond the header name\\nand value injection vulnerabilities addressed in:\\n\\n\\nThe following client APIs will raise a ValueError if given a method,\\nhost, or URI that includes newlines or other disallowed characters:\\n\\n- twisted.web.client.Agent.request\\n- twisted.web.client.ProxyAgent.request\\n- twisted.web.client.Request.__init__\\n- twisted.web.client.Request.writeTo\\n\\nProxyAgent is patched separately from Agent because unlike other\\nagents (e.g. CookieAgent) it is not implemented as an Agent wrapper.\\n\\nRequest.__init__ checks its method and URI so that errors occur closer\\nto their originating input.  Request.method and Request.uri are both\\npublic APIs, however, so Request.writeTo (via Request._writeHeaders)\\nalso checks the validity of both before writing anything to the wire.\\n\\nAdditionally, the following deprecated client APIs have also been\\npatched:\\n\\n- twisted.web.client.HTTPPageGetter.__init__\\n- twisted.web.client.HTTPPageDownloader.__init__\\n- twisted.web.client.HTTPClientFactory.__init__\\n- twisted.web.client.HTTPClientFactory.setURL\\n- twisted.web.client.HTTPDownloader.__init__\\n- twisted.web.client.HTTPDownloader.setURL\\n- twisted.web.client.getPage\\n- twisted.web.client.downloadPage\\n\\nThese have been patched prior to their removal so that they won't be\\nvulnerable in the last Twisted release that includes them.  They\\nrepresent a best effort, because testing every combination of these\\npublic APIs would require more code than deprecated APIs warrant.\\n\\nIn all cases URI components, including hostnames, are restricted to\\nthe characters allowed in path components.  This mirrors the CPython\\npatch (for bpo-30458) that addresses equivalent vulnerabilities:\\n\\n\\nHTTP methods, however, are checked against the set of characters\\ndescribed in RFC-7230.",
    "fixed_code": "def connectionMade(self):\\n\\t\\tmethod = _ensureValidMethod(getattr(self.factory, 'method', b'GET'))\\n\\t\\tself.sendCommand(method, _ensureValidURI(self.factory.path))\\n\\t\\tif self.factory.scheme == b'http' and self.factory.port != 80:\\n\\t\\t\\thost = self.factory.host + b':' + intToBytes(self.factory.port)\\n\\t\\telif self.factory.scheme == b'https' and self.factory.port != 443:\\n\\t\\t\\thost = self.factory.host + b':' + intToBytes(self.factory.port)\\n\\t\\telse:\\n\\t\\t\\thost = self.factory.host\\n\\t\\tself.sendHeader(b'Host', self.factory.headers.get(b\"host\", host))\\n\\t\\tself.sendHeader(b'User-Agent', self.factory.agent)\\n\\t\\tdata = getattr(self.factory, 'postdata', None)\\n\\t\\tif data is not None:\\n\\t\\t\\tself.sendHeader(b\"Content-Length\", intToBytes(len(data)))\\n\\t\\tcookieData = []\\n\\t\\tfor (key, value) in self.factory.headers.items():\\n\\t\\t\\tif key.lower() not in self._specialHeaders:\\n\\t\\t\\t\\tself.sendHeader(key, value)\\n\\t\\t\\tif key.lower() == b'cookie':\\n\\t\\t\\t\\tcookieData.append(value)\\n\\t\\tfor cookie, cookval in self.factory.cookies.items():\\n\\t\\t\\tcookieData.append(cookie + b'=' + cookval)\\n\\t\\tif cookieData:\\n\\t\\t\\tself.sendHeader(b'Cookie', b'; '.join(cookieData))\\n\\t\\tself.endHeaders()\\n\\t\\tself.headers = {}\\n\\t\\tif data is not None:\\n\\t\\t\\tself.transport.write(data)"
  },
  {
    "code": "def __mul__(self, b):\\n        if not isinstance(b, Kernel):\\n            return Product(self, ConstantKernel(b))\\n        return Product(self, b)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REFACTOR GP kernels use separate specification of bounds and params (no joint param_space)",
    "fixed_code": "def __mul__(self, b):\\n        if not isinstance(b, Kernel):\\n            return Product(self, ConstantKernel.from_literal(b))\\n        return Product(self, b)"
  },
  {
    "code": "def wstring_at(ptr, size=-1):\\n        return _wstring_at(ptr, size)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def wstring_at(ptr, size=-1):\\n        return _wstring_at(ptr, size)"
  },
  {
    "code": "def index(a, sub, start=0, end=None):\\n    return _vec_string(\\n        a, int_, 'index', [sub, start] + _clean_args(end))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def index(a, sub, start=0, end=None):\\n    return _vec_string(\\n        a, int_, 'index', [sub, start] + _clean_args(end))"
  },
  {
    "code": "def patch_table(self,  \\n                    dataset_id: str,\\n                    table_id: str,\\n                    project_id: Optional[str] = None,\\n                    description: Optional[str] = None,\\n                    expiration_time: Optional[int] = None,\\n                    external_data_configuration: Optional[Dict] = None,\\n                    friendly_name: Optional[str] = None,\\n                    labels: Optional[Dict] = None,\\n                    schema: Optional[List] = None,\\n                    time_partitioning: Optional[Dict] = None,\\n                    view: Optional[Dict] = None,\\n                    require_partition_filter: Optional[bool] = None,\\n                    encryption_configuration: Optional[Dict] = None) -> None:\\n        project_id = project_id if project_id is not None else self.project_id\\n        table_resource = {}  \\n        if description is not None:\\n            table_resource['description'] = description\\n        if expiration_time is not None:\\n            table_resource['expirationTime'] = expiration_time\\n        if external_data_configuration:\\n            table_resource['externalDataConfiguration'] = external_data_configuration\\n        if friendly_name is not None:\\n            table_resource['friendlyName'] = friendly_name\\n        if labels:\\n            table_resource['labels'] = labels\\n        if schema:\\n            table_resource['schema'] = {'fields': schema}\\n        if time_partitioning:\\n            table_resource['timePartitioning'] = time_partitioning\\n        if view:\\n            table_resource['view'] = view\\n        if require_partition_filter is not None:\\n            table_resource['requirePartitionFilter'] = require_partition_filter\\n        if encryption_configuration:\\n            table_resource[\"encryptionConfiguration\"] = encryption_configuration\\n        self.log.info('Patching Table %s:%s.%s',\\n                      project_id, dataset_id, table_id)\\n        try:\\n            self.service.tables().patch(\\n                projectId=project_id,\\n                datasetId=dataset_id,\\n                tableId=table_id,\\n                body=table_resource).execute(num_retries=self.num_retries)\\n            self.log.info('Table patched successfully: %s:%s.%s',\\n                          project_id, dataset_id, table_id)\\n        except HttpError as err:\\n            raise AirflowException(\\n                'BigQuery job failed. Error was: {}'.format(err.content)\\n            )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6442] BigQuery hook - standardize handling http exceptions (#7028)",
    "fixed_code": "def patch_table(self,  \\n                    dataset_id: str,\\n                    table_id: str,\\n                    project_id: Optional[str] = None,\\n                    description: Optional[str] = None,\\n                    expiration_time: Optional[int] = None,\\n                    external_data_configuration: Optional[Dict] = None,\\n                    friendly_name: Optional[str] = None,\\n                    labels: Optional[Dict] = None,\\n                    schema: Optional[List] = None,\\n                    time_partitioning: Optional[Dict] = None,\\n                    view: Optional[Dict] = None,\\n                    require_partition_filter: Optional[bool] = None,\\n                    encryption_configuration: Optional[Dict] = None) -> None:\\n        project_id = project_id if project_id is not None else self.project_id\\n        table_resource = {}  \\n        if description is not None:\\n            table_resource['description'] = description\\n        if expiration_time is not None:\\n            table_resource['expirationTime'] = expiration_time\\n        if external_data_configuration:\\n            table_resource['externalDataConfiguration'] = external_data_configuration\\n        if friendly_name is not None:\\n            table_resource['friendlyName'] = friendly_name\\n        if labels:\\n            table_resource['labels'] = labels\\n        if schema:\\n            table_resource['schema'] = {'fields': schema}\\n        if time_partitioning:\\n            table_resource['timePartitioning'] = time_partitioning\\n        if view:\\n            table_resource['view'] = view\\n        if require_partition_filter is not None:\\n            table_resource['requirePartitionFilter'] = require_partition_filter\\n        if encryption_configuration:\\n            table_resource[\"encryptionConfiguration\"] = encryption_configuration\\n        self.log.info('Patching Table %s:%s.%s',\\n                      project_id, dataset_id, table_id)\\n        self.service.tables().patch(\\n            projectId=project_id,\\n            datasetId=dataset_id,\\n            tableId=table_id,\\n            body=table_resource).execute(num_retries=self.num_retries)\\n        self.log.info('Table patched successfully: %s:%s.%s',\\n                      project_id, dataset_id, table_id)"
  },
  {
    "code": "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\\n\\tlabel_length = tf.to_int32(tf.squeeze(label_length))\\n\\tinput_length = tf.to_int32(tf.squeeze(input_length))\\n\\tsparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\\n\\ty_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\\n\\treturn tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   labels=sparse_labels,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   sequence_length=input_length), 1)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix ctc_batch_cost() error when batch_size = 1 (#9775)",
    "fixed_code": "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\\n\\tlabel_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\\n\\tinput_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\\n\\tsparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\\n\\ty_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\\n\\treturn tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   labels=sparse_labels,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t   sequence_length=input_length), 1)"
  },
  {
    "code": "def __repr__(self):\\n        return '<factory>'\\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\\nclass _MISSING_TYPE:\\n    pass\\nMISSING = _MISSING_TYPE()\\n_EMPTY_METADATA = types.MappingProxyType({})\\n_FIELD = object()                 \\n_FIELD_CLASSVAR = object()        \\n_FIELD_INITVAR = object()         \\n_FIELDS = '__dataclass_fields__'\\n_PARAMS = '__dataclass_params__'\\n_POST_INIT_NAME = '__post_init__'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n        return '<factory>'\\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\\nclass _MISSING_TYPE:\\n    pass\\nMISSING = _MISSING_TYPE()\\n_EMPTY_METADATA = types.MappingProxyType({})\\n_FIELD = object()                 \\n_FIELD_CLASSVAR = object()        \\n_FIELD_INITVAR = object()         \\n_FIELDS = '__dataclass_fields__'\\n_PARAMS = '__dataclass_params__'\\n_POST_INIT_NAME = '__post_init__'"
  },
  {
    "code": "def array(\\n    data: Sequence[object] | AnyArrayLike,\\n    dtype: Dtype | None = None,\\n    copy: bool = True,\\n) -> ExtensionArray:\\n    from pandas.core.arrays import (\\n        BooleanArray,\\n        DatetimeArray,\\n        ExtensionArray,\\n        FloatingArray,\\n        IntegerArray,\\n        IntervalArray,\\n        PandasArray,\\n        PeriodArray,\\n        TimedeltaArray,\\n    )\\n    from pandas.core.arrays.string_ import StringDtype\\n    if lib.is_scalar(data):\\n        msg = f\"Cannot pass scalar '{data}' to 'pandas.array'.\"\\n        raise ValueError(msg)\\n    if dtype is None and isinstance(data, (ABCSeries, ABCIndex, ExtensionArray)):\\n        dtype = data.dtype\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(dtype, str):\\n        dtype = registry.find(dtype) or dtype\\n    if is_extension_array_dtype(dtype):\\n        cls = cast(ExtensionDtype, dtype).construct_array_type()\\n        return cls._from_sequence(data, dtype=dtype, copy=copy)\\n    if dtype is None:\\n        inferred_dtype = lib.infer_dtype(data, skipna=True)\\n        if inferred_dtype == \"period\":\\n            period_data = cast(Union[Sequence[Optional[Period]], AnyArrayLike], data)\\n            return PeriodArray._from_sequence(period_data, copy=copy)\\n        elif inferred_dtype == \"interval\":\\n            return IntervalArray(data, copy=copy)\\n        elif inferred_dtype.startswith(\"datetime\"):\\n            try:\\n                return DatetimeArray._from_sequence(data, copy=copy)\\n            except ValueError:\\n                pass\\n        elif inferred_dtype.startswith(\"timedelta\"):\\n            return TimedeltaArray._from_sequence(data, copy=copy)\\n        elif inferred_dtype == \"string\":\\n            return StringDtype().construct_array_type()._from_sequence(data, copy=copy)\\n        elif inferred_dtype == \"integer\":\\n            return IntegerArray._from_sequence(data, copy=copy)\\n        elif (\\n            inferred_dtype in (\"floating\", \"mixed-integer-float\")\\n            and getattr(data, \"dtype\", None) != np.float16\\n        ):\\n            return FloatingArray._from_sequence(data, copy=copy)\\n        elif inferred_dtype == \"boolean\":\\n            return BooleanArray._from_sequence(data, copy=copy)\\n    if is_datetime64_ns_dtype(dtype):\\n        return DatetimeArray._from_sequence(data, dtype=dtype, copy=copy)\\n    elif is_timedelta64_ns_dtype(dtype):\\n        return TimedeltaArray._from_sequence(data, dtype=dtype, copy=copy)\\n    return PandasArray._from_sequence(data, dtype=dtype, copy=copy)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: preserve EA in pd.array (#48889)",
    "fixed_code": "def array(\\n    data: Sequence[object] | AnyArrayLike,\\n    dtype: Dtype | None = None,\\n    copy: bool = True,\\n) -> ExtensionArray:\\n    from pandas.core.arrays import (\\n        BooleanArray,\\n        DatetimeArray,\\n        ExtensionArray,\\n        FloatingArray,\\n        IntegerArray,\\n        IntervalArray,\\n        PandasArray,\\n        PeriodArray,\\n        TimedeltaArray,\\n    )\\n    from pandas.core.arrays.string_ import StringDtype\\n    if lib.is_scalar(data):\\n        msg = f\"Cannot pass scalar '{data}' to 'pandas.array'.\"\\n        raise ValueError(msg)\\n    if dtype is None and isinstance(data, (ABCSeries, ABCIndex, ExtensionArray)):\\n        dtype = data.dtype\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(dtype, str):\\n        dtype = registry.find(dtype) or dtype\\n    if isinstance(data, ExtensionArray) and (\\n        dtype is None or is_dtype_equal(dtype, data.dtype)\\n    ):\\n        if copy:\\n            return data.copy()\\n        return data\\n    if is_extension_array_dtype(dtype):\\n        cls = cast(ExtensionDtype, dtype).construct_array_type()\\n        return cls._from_sequence(data, dtype=dtype, copy=copy)\\n    if dtype is None:\\n        inferred_dtype = lib.infer_dtype(data, skipna=True)\\n        if inferred_dtype == \"period\":\\n            period_data = cast(Union[Sequence[Optional[Period]], AnyArrayLike], data)\\n            return PeriodArray._from_sequence(period_data, copy=copy)\\n        elif inferred_dtype == \"interval\":\\n            return IntervalArray(data, copy=copy)\\n        elif inferred_dtype.startswith(\"datetime\"):\\n            try:\\n                return DatetimeArray._from_sequence(data, copy=copy)\\n            except ValueError:\\n                pass\\n        elif inferred_dtype.startswith(\"timedelta\"):\\n            return TimedeltaArray._from_sequence(data, copy=copy)\\n        elif inferred_dtype == \"string\":\\n            return StringDtype().construct_array_type()._from_sequence(data, copy=copy)\\n        elif inferred_dtype == \"integer\":\\n            return IntegerArray._from_sequence(data, copy=copy)\\n        elif (\\n            inferred_dtype in (\"floating\", \"mixed-integer-float\")\\n            and getattr(data, \"dtype\", None) != np.float16\\n        ):\\n            return FloatingArray._from_sequence(data, copy=copy)\\n        elif inferred_dtype == \"boolean\":\\n            return BooleanArray._from_sequence(data, copy=copy)\\n    if is_datetime64_ns_dtype(dtype):\\n        return DatetimeArray._from_sequence(data, dtype=dtype, copy=copy)\\n    elif is_timedelta64_ns_dtype(dtype):\\n        return TimedeltaArray._from_sequence(data, dtype=dtype, copy=copy)\\n    return PandasArray._from_sequence(data, dtype=dtype, copy=copy)"
  },
  {
    "code": "def get_commands_to_config_vpc(module, vpc, domain, existing):\\n    vpc = dict(vpc)\\n    domain_only = vpc.get('domain')\\n    commands = []\\n    if 'pkl_dest' in vpc:\\n        pkl_command = 'peer-keepalive destination {pkl_dest}'.format(**vpc)\\n        if 'pkl_src' in vpc:\\n            pkl_command += ' source {pkl_src}'.format(**vpc)\\n        if 'pkl_vrf' in vpc and vpc['pkl_vrf'] != 'management':\\n            pkl_command += ' vrf {pkl_vrf}'.format(**vpc)\\n        commands.append(pkl_command)\\n    if 'auto_recovery' in vpc:\\n        if not vpc.get('auto_recovery'):\\n            vpc['auto_recovery'] = 'no'\\n        else:\\n            vpc['auto_recovery'] = ''\\n    if 'peer_gw' in vpc:\\n        if not vpc.get('peer_gw'):\\n            vpc['peer_gw'] = 'no'\\n        else:\\n            vpc['peer_gw'] = ''\\n    for param in vpc:\\n        command = CONFIG_ARGS.get(param)\\n        if command is not None:\\n            command = command.format(**vpc).strip()\\n            if 'peer-gateway' in command:\\n                commands.append('terminal dont-ask')\\n            commands.append(command)\\n    if commands or domain_only:\\n        commands.insert(0, 'vpc domain {0}'.format(domain))\\n    return commands",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_vpc: pkl_vrf fixes for #57069 (#57370)\\n\\n\\nFixes #57069\\n\\n- Symptom: When playbooks specify `pkl_vrf: default`, the result is that the cli does not set the `vrf` state.\\n\\n- Analysis:\\n - First issue: 'default' is a reserved word when used with the `peer-keepalive` `vrf` keyword. It refers to the default rib.\\n   - This is confusing in several ways because `peer-keepalive`'s *default* vrf is the `management` vrf.\\n\\n - Second issue: When changing only one optional value (`pkl_vrf`) while other optional values are idempotent (`pkl_src`), the result is that the idempotent values are ignored; unfortunately the device cli *replaces* the entire command, in which case the idempotent values are removed.\\n   - e.g. playbook specifies this:\\n     ```\\n     { pkl_dest: 10.1.1.1, pkl_src: 10.2.2.2, pkl_vrf: my_vrf }\\n     ```\\n\\n     ```\\n     peer-keepalive dest 10.1.1.1 src 10.2.2.2             # original\\n\\n     peer-keepalive dest 10.1.1.1 src 10.2.2.2 vrf my_vrf  # intended result\\n\\n     peer-keepalive dest 10.1.1.1 vrf my_vrf               # actual result\\n     ```\\n\\n - Third issue: the `pkl` getter was relying on positional data. This broke when the `udp` keyword nvgen'd where `vrf` used to appear (shifting all keywords to the right).\\n\\n- Tested on regression platforms: `N3K,N6k,N7K,N9K,N3K-F,N9K-F`",
    "fixed_code": "def get_commands_to_config_vpc(module, vpc, domain, existing):\\n    vpc = dict(vpc)\\n    domain_only = vpc.get('domain')\\n    commands = []\\n    if 'pkl_dest' in vpc:\\n        pkl_command = 'peer-keepalive destination {pkl_dest}'.format(**vpc)\\n        if 'pkl_src' in vpc:\\n            pkl_command += ' source {pkl_src}'.format(**vpc)\\n        if 'pkl_vrf' in vpc:\\n            pkl_command += ' vrf {pkl_vrf}'.format(**vpc)\\n        commands.append(pkl_command)\\n    if 'auto_recovery' in vpc:\\n        if not vpc.get('auto_recovery'):\\n            vpc['auto_recovery'] = 'no'\\n        else:\\n            vpc['auto_recovery'] = ''\\n    if 'peer_gw' in vpc:\\n        if not vpc.get('peer_gw'):\\n            vpc['peer_gw'] = 'no'\\n        else:\\n            vpc['peer_gw'] = ''\\n    for param in vpc:\\n        command = CONFIG_ARGS.get(param)\\n        if command is not None:\\n            command = command.format(**vpc).strip()\\n            if 'peer-gateway' in command:\\n                commands.append('terminal dont-ask')\\n            commands.append(command)\\n    if commands or domain_only:\\n        commands.insert(0, 'vpc domain {0}'.format(domain))\\n    return commands"
  },
  {
    "code": "def describe(self, percentile_width=None, percentiles=None):\\n        if self.ndim >= 3:\\n            msg = \"describe is not implemented on on Panel or PanelND objects.\"\\n            raise NotImplementedError(msg)\\n        if percentile_width is not None and percentiles is not None:\\n            msg = \"Cannot specify both 'percentile_width' and 'percentiles.'\"\\n            raise ValueError(msg)\\n        if percentiles is not None:\\n            percentiles = np.asarray(percentiles)\\n            if (percentiles > 1).any():\\n                percentiles = percentiles / 100.0\\n                msg = (\"percentiles should all be in the interval [0, 1]. \"\\n                       \"Try {0} instead.\")\\n                raise ValueError(msg.format(list(percentiles)))\\n        else:\\n            if percentile_width is not None:\\n                do_warn = True\\n            else:\\n                do_warn = False\\n            percentile_width = percentile_width or 50\\n            lb = .5 * (1. - percentile_width / 100.)\\n            ub = 1. - lb\\n            percentiles = np.array([lb, 0.5, ub])\\n            if do_warn:\\n                msg = (\"The `percentile_width` keyword is deprecated. \"\\n                       \"Use percentiles={0} instead\".format(list(percentiles)))\\n                warnings.warn(msg, FutureWarning)\\n        if (percentiles != 0.5).all():  \\n            lh = percentiles[percentiles < .5]\\n            uh = percentiles[percentiles > .5]\\n            percentiles = np.hstack([lh, 0.5, uh])\\n        data = self._get_numeric_data()\\n        if self.ndim > 1:\\n            if len(data._info_axis) == 0:\\n                is_object = True\\n            else:\\n                is_object = False\\n        else:\\n            is_object = not self._is_numeric_mixed_type",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "More flexible describe() via include/exclude type filtering\\n\\nThis enhance describe()'s output via new include/exclude list arguments,\\nletting the user specify the dtypes to be summarized as output.\\nThis provides an simple way to overcome the automatic type-filtering done\\nby default; it's also convenient with groupby().\\nAlso includes documentation and changelog entries.",
    "fixed_code": "def describe(self, percentile_width=None, percentiles=None, include=None, exclude=None ):\\n        if self.ndim >= 3:\\n            msg = \"describe is not implemented on on Panel or PanelND objects.\"\\n            raise NotImplementedError(msg)\\n        if percentile_width is not None and percentiles is not None:\\n            msg = \"Cannot specify both 'percentile_width' and 'percentiles.'\"\\n            raise ValueError(msg)\\n        if percentiles is not None:\\n            percentiles = np.asarray(percentiles)\\n            if (percentiles > 1).any():\\n                percentiles = percentiles / 100.0\\n                msg = (\"percentiles should all be in the interval [0, 1]. \"\\n                       \"Try {0} instead.\")\\n                raise ValueError(msg.format(list(percentiles)))\\n        else:\\n            if percentile_width is not None:\\n                do_warn = True\\n            else:\\n                do_warn = False\\n            percentile_width = percentile_width or 50\\n            lb = .5 * (1. - percentile_width / 100.)\\n            ub = 1. - lb\\n            percentiles = np.array([lb, 0.5, ub])\\n            if do_warn:\\n                msg = (\"The `percentile_width` keyword is deprecated. \"\\n                       \"Use percentiles={0} instead\".format(list(percentiles)))\\n                warnings.warn(msg, FutureWarning)\\n        if (percentiles != 0.5).all():  \\n            lh = percentiles[percentiles < .5]\\n            uh = percentiles[percentiles > .5]\\n            percentiles = np.hstack([lh, 0.5, uh])"
  },
  {
    "code": "def _save_FIELD_file(self, field, filename, raw_contents):\\n        directory = field.get_directory_name()\\n        try: \\n            os.makedirs(os.path.join(settings.MEDIA_ROOT, directory))\\n        except OSError: \\n            pass\\n        filename = field.get_filename(filename)\\n        while os.path.exists(os.path.join(settings.MEDIA_ROOT, filename)):\\n            try:\\n                dot_index = filename.rindex('.')\\n            except ValueError: \\n                filename += '_'\\n            else:\\n                filename = filename[:dot_index] + '_' + filename[dot_index:]\\n        setattr(self, field.attname, filename)\\n        full_filename = self._get_FIELD_filename(field)\\n        fp = open(full_filename, 'wb')\\n        fp.write(raw_contents)\\n        fp.close()\\n        if isinstance(field, ImageField) and (field.width_field or field.height_field):\\n            from django.utils.images import get_image_dimensions\\n            width, height = get_image_dimensions(full_filename)\\n            if field.width_field:\\n                setattr(self, field.width_field, width)\\n            if field.height_field:\\n                setattr(self, field.height_field, height)\\n        self.save()\\n    _save_FIELD_file.alters_data = True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Objects with FileFields no longer get save() called multiple times from the AutomaticManipulator! This fixes #639, #572, and likely others I don't know of.\\n\\nThis may be slightly backwards-incompatible: if you've been relying on the multiple-save behavior (why?), then you'll no longer see that happen.",
    "fixed_code": "def _save_FIELD_file(self, field, filename, raw_contents, save=True):\\n        directory = field.get_directory_name()\\n        try: \\n            os.makedirs(os.path.join(settings.MEDIA_ROOT, directory))\\n        except OSError: \\n            pass\\n        filename = field.get_filename(filename)\\n        while os.path.exists(os.path.join(settings.MEDIA_ROOT, filename)):\\n            try:\\n                dot_index = filename.rindex('.')\\n            except ValueError: \\n                filename += '_'\\n            else:\\n                filename = filename[:dot_index] + '_' + filename[dot_index:]\\n        setattr(self, field.attname, filename)\\n        full_filename = self._get_FIELD_filename(field)\\n        fp = open(full_filename, 'wb')\\n        fp.write(raw_contents)\\n        fp.close()\\n        if isinstance(field, ImageField) and (field.width_field or field.height_field):\\n            from django.utils.images import get_image_dimensions\\n            width, height = get_image_dimensions(full_filename)\\n            if field.width_field:\\n                setattr(self, field.width_field, width)\\n            if field.height_field:\\n                setattr(self, field.height_field, height)\\n        if save:\\n            self.save()\\n    _save_FIELD_file.alters_data = True"
  },
  {
    "code": "def _get_non_gfk_field(opts, name):\\n    field = opts.get_field(name)\\n    if field.is_relation and field.one_to_many and not field.related_model:\\n        raise FieldDoesNotExist()\\n    return field",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24289 -- Reversed usage of Field.many_to_one and one_to_many.\\n\\nThanks Carl Meyer and Tim Graham for the reviews and to all involved\\nin the discussion.",
    "fixed_code": "def _get_non_gfk_field(opts, name):\\n    field = opts.get_field(name)\\n    if field.is_relation and field.many_to_one and not field.related_model:\\n        raise FieldDoesNotExist()\\n    return field"
  },
  {
    "code": "def remainder_near(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        if self._is_special or other._is_special:\\n            ans = self._check_nans(other, context)\\n            if ans:\\n                return ans\\n        if self and not other:\\n            return context._raise_error(InvalidOperation, 'x % 0')\\n        if context is None:\\n            context = getcontext()\\n        context = context._shallow_copy()\\n        flags = context._ignore_flags(Rounded, Inexact)\\n        (side, r) = self.__divmod__(other, context=context)\\n        if r._isnan():\\n            context._regard_flags(*flags)\\n            return r\\n        context = context._shallow_copy()\\n        rounding = context._set_rounding_decision(NEVER_ROUND)\\n        if other._sign:\\n            comparison = other.__div__(Decimal(-2), context=context)\\n        else:\\n            comparison = other.__div__(Decimal(2), context=context)\\n        context._set_rounding_decision(rounding)\\n        context._regard_flags(*flags)\\n        s1, s2 = r._sign, comparison._sign\\n        r._sign, comparison._sign = 0, 0\\n        if r < comparison:\\n            r._sign, comparison._sign = s1, s2\\n            self.__divmod__(other, context=context)\\n            return r._fix(context)\\n        r._sign, comparison._sign = s1, s2\\n        rounding = context._set_rounding_decision(NEVER_ROUND)\\n        (side, r) = self.__divmod__(other, context=context)\\n        context._set_rounding_decision(rounding)\\n        if r._isnan():\\n            return r\\n        decrease = not side._iseven()\\n        rounding = context._set_rounding_decision(NEVER_ROUND)\\n        side = side.__abs__(context=context)\\n        context._set_rounding_decision(rounding)\\n        s1, s2 = r._sign, comparison._sign\\n        r._sign, comparison._sign = 0, 0\\n        if r > comparison or decrease and r == comparison:\\n            r._sign, comparison._sign = s1, s2\\n            context.prec += 1\\n            if len(side.__add__(Decimal(1), context=context)._int) >= context.prec:\\n                context.prec -= 1\\n                return context._raise_error(DivisionImpossible)[1]\\n            context.prec -= 1\\n            if self._sign == other._sign:\\n                r = r.__sub__(other, context=context)\\n            else:\\n                r = r.__add__(other, context=context)\\n        else:\\n            r._sign, comparison._sign = s1, s2\\n        return r._fix(context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def remainder_near(self, other, context=None):\\n        if context is None:\\n            context = getcontext()\\n        other = _convert_other(other, raiseit=True)\\n        ans = self._check_nans(other, context)\\n        if ans:\\n            return ans\\n        if self._isinfinity():\\n            return context._raise_error(InvalidOperation,\\n                                        'remainder_near(infinity, x)')\\n        if not other:\\n            if self:\\n                return context._raise_error(InvalidOperation,\\n                                            'remainder_near(x, 0)')\\n            else:\\n                return context._raise_error(DivisionUndefined,\\n                                            'remainder_near(0, 0)')\\n        if other._isinfinity():\\n            ans = Decimal(self)\\n            return ans._fix(context)\\n        ideal_exponent = min(self._exp, other._exp)\\n        if not self:\\n            ans = _dec_from_triple(self._sign, '0', ideal_exponent)\\n            return ans._fix(context)\\n        expdiff = self.adjusted() - other.adjusted()\\n        if expdiff >= context.prec + 1:\\n            return context._raise_error(DivisionImpossible)\\n        if expdiff <= -2:\\n            ans = self._rescale(ideal_exponent, context.rounding)\\n            return ans._fix(context)\\n        op1 = _WorkRep(self)\\n        op2 = _WorkRep(other)\\n        if op1.exp >= op2.exp:\\n            op1.int *= 10**(op1.exp - op2.exp)\\n        else:\\n            op2.int *= 10**(op2.exp - op1.exp)\\n        q, r = divmod(op1.int, op2.int)\\n        if 2*r + (q&1) > op2.int:\\n            r -= op2.int\\n            q += 1\\n        if q >= 10**context.prec:\\n            return context._raise_error(DivisionImpossible)\\n        sign = self._sign\\n        if r < 0:\\n            sign = 1-sign\\n            r = -r\\n        ans = _dec_from_triple(sign, str(r), ideal_exponent)\\n        return ans._fix(context)"
  },
  {
    "code": "def explain_query_prefix(self, format=None, **options):\\n        prefix = super().explain_query_prefix(format)\\n        extra = {}\\n        if format:\\n            extra[\"FORMAT\"] = format\\n        if options:\\n            extra.update(\\n                {\\n                    name.upper(): \"true\" if value else \"false\"\\n                    for name, value in options.items()\\n                }\\n            )\\n        if extra:\\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\\n        return prefix",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed CVE-2022-28347 -- Protected QuerySet.explain(**options) against SQL injection on PostgreSQL.",
    "fixed_code": "def explain_query_prefix(self, format=None, **options):\\n        extra = {}\\n        if options:\\n            options = {\\n                name.upper(): \"true\" if value else \"false\"\\n                for name, value in options.items()\\n            }\\n            for valid_option in self.explain_options:\\n                value = options.pop(valid_option, None)\\n                if value is not None:\\n                    extra[valid_option.upper()] = value\\n        prefix = super().explain_query_prefix(format, **options)\\n        if format:\\n            extra[\"FORMAT\"] = format\\n        if extra:\\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\\n        return prefix"
  },
  {
    "code": "def __eq__(self, other):\\n        return (isinstance(other, Model) and\\n                self._meta.concrete_model == other._meta.concrete_model and\\n                self._get_pk_val() == other._get_pk_val())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __eq__(self, other):\\n        return (isinstance(other, Model) and\\n                self._meta.concrete_model == other._meta.concrete_model and\\n                self._get_pk_val() == other._get_pk_val())"
  },
  {
    "code": "def _get_data_from(symbols, start, end, retry_count, pause, adjust_price,\\n                   ret_index, chunksize, source):\\n    src_fn = _source_functions[source]\\n    if isinstance(symbols, (compat.string_types, int)):\\n        hist_data = src_fn(symbols, start, end, retry_count, pause)\\n    elif isinstance(symbols, DataFrame):\\n        hist_data = _dl_mult_symbols(symbols.index, start, end, chunksize,\\n                                     retry_count, pause, src_fn)\\n    else:\\n        hist_data = _dl_mult_symbols(symbols, start, end, chunksize,\\n                                     retry_count, pause, src_fn)\\n    if source.lower() == 'yahoo':\\n        if ret_index:\\n            hist_data['Ret_Index'] = _calc_return_index(hist_data['Adj Close'])\\n        if adjust_price:\\n            hist_data = _adjust_prices(hist_data)\\n    return hist_data",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add interval kwarg to get_data_yahoo",
    "fixed_code": "def _get_data_from(symbols, start, end, interval, retry_count, pause, adjust_price,\\n                   ret_index, chunksize, source):\\n    src_fn = _source_functions[source]\\n    if isinstance(symbols, (compat.string_types, int)):\\n        hist_data = src_fn(symbols, start, end, interval, retry_count, pause)\\n    elif isinstance(symbols, DataFrame):\\n        hist_data = _dl_mult_symbols(symbols.index, start, end, interval, chunksize,\\n                                     retry_count, pause, src_fn)\\n    else:\\n        hist_data = _dl_mult_symbols(symbols, start, end, interval, chunksize,\\n                                     retry_count, pause, src_fn)\\n    if source.lower() == 'yahoo':\\n        if ret_index:\\n            hist_data['Ret_Index'] = _calc_return_index(hist_data['Adj Close'])\\n        if adjust_price:\\n            hist_data = _adjust_prices(hist_data)\\n    return hist_data"
  },
  {
    "code": "def get_stack_events(cfn, stack_name, events_limit, token_filter=None):\\n    ''\\n    ret = {'events': [], 'log': []}\\n    try:\\n        pg = cfn.get_paginator(\\n            'describe_stack_events'\\n        ).paginate(\\n            StackName=stack_name,\\n            PaginationConfig={'MaxItems': events_limit}\\n        )\\n        if token_filter is not None:\\n            events = list(pg.search(\\n                \"StackEvents[?ClientRequestToken == '{0}']\".format(token_filter)\\n            ))\\n        else:\\n            events = list(pg.search(\"StackEvents[*]\"))\\n    except (botocore.exceptions.ValidationError, botocore.exceptions.ClientError) as err:\\n        error_msg = boto_exception(err)\\n        if 'does not exist' in error_msg:\\n            ret['log'].append('Stack does not exist.')\\n            return ret\\n        ret['log'].append('Unknown error: ' + str(error_msg))\\n        return ret\\n    for e in events:\\n        eventline = 'StackEvent {ResourceType} {LogicalResourceId} {ResourceStatus}'.format(**e)\\n        ret['events'].append(eventline)\\n        if e['ResourceStatus'].endswith('FAILED'):\\n            failline = '{ResourceType} {LogicalResourceId} {ResourceStatus}: {ResourceStatusReason}'.format(**e)\\n            ret['log'].append(failline)\\n    return ret",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_stack_events(cfn, stack_name, events_limit, token_filter=None):\\n    ''\\n    ret = {'events': [], 'log': []}\\n    try:\\n        pg = cfn.get_paginator(\\n            'describe_stack_events'\\n        ).paginate(\\n            StackName=stack_name,\\n            PaginationConfig={'MaxItems': events_limit}\\n        )\\n        if token_filter is not None:\\n            events = list(pg.search(\\n                \"StackEvents[?ClientRequestToken == '{0}']\".format(token_filter)\\n            ))\\n        else:\\n            events = list(pg.search(\"StackEvents[*]\"))\\n    except (botocore.exceptions.ValidationError, botocore.exceptions.ClientError) as err:\\n        error_msg = boto_exception(err)\\n        if 'does not exist' in error_msg:\\n            ret['log'].append('Stack does not exist.')\\n            return ret\\n        ret['log'].append('Unknown error: ' + str(error_msg))\\n        return ret\\n    for e in events:\\n        eventline = 'StackEvent {ResourceType} {LogicalResourceId} {ResourceStatus}'.format(**e)\\n        ret['events'].append(eventline)\\n        if e['ResourceStatus'].endswith('FAILED'):\\n            failline = '{ResourceType} {LogicalResourceId} {ResourceStatus}: {ResourceStatusReason}'.format(**e)\\n            ret['log'].append(failline)\\n    return ret"
  },
  {
    "code": "def update_sub(x, decrement):\\n\\top = tf_state_ops.assign_sub(x, decrement)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_sub(x, decrement):\\n\\top = tf_state_ops.assign_sub(x, decrement)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)"
  },
  {
    "code": "def create_superuser(self, username, email, password, **extra_fields):\\n        return self._create_user(username, email, password, True, True,\\n                                 **extra_fields)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_superuser(self, username, email, password, **extra_fields):\\n        return self._create_user(username, email, password, True, True,\\n                                 **extra_fields)"
  },
  {
    "code": "def description(self, group):\\n        return self._getdescriptions(group, False)\\n    def descriptions(self, group_pattern):\\n        return self._getdescriptions(group_pattern, True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def description(self, group):\\n        return self._getdescriptions(group, False)\\n    def descriptions(self, group_pattern):\\n        return self._getdescriptions(group_pattern, True)"
  },
  {
    "code": "def _download_file(bucket, filename, local_dir):\\n\\t\\tkey = bucket.get_key(filename)\\n\\t\\tlocal_filename = os.path.join(local_dir, filename)\\n\\t\\tkey.get_contents_to_filename(local_filename)\\n\\t\\treturn local_filename",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Sanitize EC2 manifests and image tarballs\\n\\nPrevent potential directory traversal with malicious EC2 image tarballs,\\nby making sure the tarfile is safe before unpacking it. Fixes bug 894755\\n\\nPrevent potential directory traversal with malicious file names in\\nEC2 image manifests. Fixes bug 885167",
    "fixed_code": "def _download_file(bucket, filename, local_dir):\\n\\t\\tkey = bucket.get_key(filename)\\n\\t\\tlocal_filename = os.path.join(local_dir, os.path.basename(filename))\\n\\t\\tkey.get_contents_to_filename(local_filename)\\n\\t\\treturn local_filename"
  },
  {
    "code": "def _uninstall_helper(*, verbosity=0):\\n    try:\\n        import pip\\n    except ImportError:\\n        return\\n    if pip.__version__ != _PIP_VERSION:\\n        msg = (\"ensurepip will only uninstall a matching pip \"\\n               \"({!r} installed, {!r} bundled)\")\\n        raise RuntimeError(msg.format(pip.__version__, _PIP_VERSION))\\n    _require_ssl_for_pip()\\n    _disable_pip_configuration_settings()\\n    args = [\"uninstall\", \"-y\"]\\n    if verbosity:\\n        args += [\"-\" + \"v\" * verbosity]\\n    _run_pip(args + [p[0] for p in reversed(_PROJECTS)])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _uninstall_helper(*, verbosity=0):\\n    try:\\n        import pip\\n    except ImportError:\\n        return\\n    if pip.__version__ != _PIP_VERSION:\\n        msg = (\"ensurepip will only uninstall a matching pip \"\\n               \"({!r} installed, {!r} bundled)\")\\n        raise RuntimeError(msg.format(pip.__version__, _PIP_VERSION))\\n    _require_ssl_for_pip()\\n    _disable_pip_configuration_settings()\\n    args = [\"uninstall\", \"-y\"]\\n    if verbosity:\\n        args += [\"-\" + \"v\" * verbosity]\\n    _run_pip(args + [p[0] for p in reversed(_PROJECTS)])"
  },
  {
    "code": "def time_strip(self, dtype):\\n        self.s.str.strip(\"A\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_strip(self, dtype):\\n        self.s.str.strip(\"A\")"
  },
  {
    "code": "def _combine_const(self, other, func, errors='raise', try_cast=True):\\n        if lib.is_scalar(other) or np.ndim(other) == 0:\\n            return ops.dispatch_to_series(self, other, func)\\n        new_data = self._data.eval(func=func, other=other,\\n                                   errors=errors,\\n                                   try_cast=try_cast)\\n        return self._constructor(new_data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _combine_const(self, other, func, errors='raise', try_cast=True):\\n        if lib.is_scalar(other) or np.ndim(other) == 0:\\n            return ops.dispatch_to_series(self, other, func)\\n        new_data = self._data.eval(func=func, other=other,\\n                                   errors=errors,\\n                                   try_cast=try_cast)\\n        return self._constructor(new_data)"
  },
  {
    "code": "def pprint(object, stream=None, indent=1, width=80, depth=None, *,\\n           compact=False):\\n    printer = PrettyPrinter(\\n        stream=stream, indent=indent, width=width, depth=depth,\\n        compact=compact)\\n    printer.pprint(object)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pprint(object, stream=None, indent=1, width=80, depth=None, *,\\n           compact=False):\\n    printer = PrettyPrinter(\\n        stream=stream, indent=indent, width=width, depth=depth,\\n        compact=compact)\\n    printer.pprint(object)"
  },
  {
    "code": "def timesince(value, arg=None):\\n    from django.utils.timesince import timesince\\n    if not value:\\n        return u''\\n    if arg:\\n        return timesince(value, arg)\\n    return timesince(value)\\ntimesince.is_safe = False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def timesince(value, arg=None):\\n    from django.utils.timesince import timesince\\n    if not value:\\n        return u''\\n    if arg:\\n        return timesince(value, arg)\\n    return timesince(value)\\ntimesince.is_safe = False"
  },
  {
    "code": "def newgroups(self, date, *, file=None):\\n        if not isinstance(date, (datetime.date, datetime.date)):\\n            raise TypeError(\\n                \"the date parameter must be a date or datetime object, \"\\n                \"not '{:40}'\".format(date.__class__.__name__))\\n        date_str, time_str = _unparse_datetime(date, self.nntp_version < 2)\\n        cmd = 'NEWGROUPS {0} {1}'.format(date_str, time_str)\\n        resp, lines = self._longcmdstring(cmd, file)\\n        return resp, self._grouplist(lines)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def newgroups(self, date, *, file=None):\\n        if not isinstance(date, (datetime.date, datetime.date)):\\n            raise TypeError(\\n                \"the date parameter must be a date or datetime object, \"\\n                \"not '{:40}'\".format(date.__class__.__name__))\\n        date_str, time_str = _unparse_datetime(date, self.nntp_version < 2)\\n        cmd = 'NEWGROUPS {0} {1}'.format(date_str, time_str)\\n        resp, lines = self._longcmdstring(cmd, file)\\n        return resp, self._grouplist(lines)"
  },
  {
    "code": "def set_precision(self, precision: int) -> Styler:\\n        warnings.warn(\\n            \"this method is deprecated in favour of `Styler.format(precision=..)`\",\\n            FutureWarning,\\n            stacklevel=2,\\n        )\\n        self.precision = precision\\n        return self.format(precision=precision, na_rep=self.na_rep)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_precision(self, precision: int) -> Styler:\\n        warnings.warn(\\n            \"this method is deprecated in favour of `Styler.format(precision=..)`\",\\n            FutureWarning,\\n            stacklevel=2,\\n        )\\n        self.precision = precision\\n        return self.format(precision=precision, na_rep=self.na_rep)"
  },
  {
    "code": "def check_classifiers_one_label(name, Classifier):\\n    error_string_fit = \"Classifier can't train when only one class is present.\"\\n    error_string_predict = (\"Classifier can't predict when only one class is \"\\n                            \"present.\")\\n    rnd = np.random.RandomState(0)\\n    X_train = rnd.uniform(size=(10, 3))\\n    X_test = rnd.uniform(size=(10, 3))\\n    y = np.ones(10)\\n    with ignore_warnings(category=DeprecationWarning):\\n        classifier = Classifier()\\n        set_testing_parameters(classifier)\\n        try:\\n            classifier.fit(X_train, y)\\n        except ValueError as e:\\n            if 'class' not in repr(e):\\n                print(error_string_fit, Classifier, e)\\n                traceback.print_exc(file=sys.stdout)\\n                raise e\\n            else:\\n                return\\n        except Exception as exc:\\n            print(error_string_fit, Classifier, exc)\\n            traceback.print_exc(file=sys.stdout)\\n            raise exc\\n        try:\\n            assert_array_equal(classifier.predict(X_test), y)\\n        except Exception as exc:\\n            print(error_string_predict, Classifier, exc)\\n            raise exc",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_classifiers_one_label(name, Classifier):\\n    error_string_fit = \"Classifier can't train when only one class is present.\"\\n    error_string_predict = (\"Classifier can't predict when only one class is \"\\n                            \"present.\")\\n    rnd = np.random.RandomState(0)\\n    X_train = rnd.uniform(size=(10, 3))\\n    X_test = rnd.uniform(size=(10, 3))\\n    y = np.ones(10)\\n    with ignore_warnings(category=DeprecationWarning):\\n        classifier = Classifier()\\n        set_testing_parameters(classifier)\\n        try:\\n            classifier.fit(X_train, y)\\n        except ValueError as e:\\n            if 'class' not in repr(e):\\n                print(error_string_fit, Classifier, e)\\n                traceback.print_exc(file=sys.stdout)\\n                raise e\\n            else:\\n                return\\n        except Exception as exc:\\n            print(error_string_fit, Classifier, exc)\\n            traceback.print_exc(file=sys.stdout)\\n            raise exc\\n        try:\\n            assert_array_equal(classifier.predict(X_test), y)\\n        except Exception as exc:\\n            print(error_string_predict, Classifier, exc)\\n            raise exc"
  },
  {
    "code": "def value(self):\\n        return self._value\\n    @value.setter\\n    def value(self, new_value):\\n        self._value = new_value\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def value(self):\\n        return self._value\\n    @value.setter\\n    def value(self, new_value):\\n        self._value = new_value\\n    @property"
  },
  {
    "code": "def call_command(name, *args, **options):\\n    try:\\n        app_name = get_commands()[name]\\n    except KeyError:\\n        raise CommandError(\"Unknown command: %r\" % name)\\n    if isinstance(app_name, BaseCommand):\\n        command = app_name\\n    else:\\n        command = load_command_class(app_name, name)\\n    parser = command.create_parser('', name)\\n    opt_mapping = {\\n        sorted(s_opt.option_strings)[0].lstrip('-').replace('-', '_'): s_opt.dest\\n        for s_opt in parser._actions if s_opt.option_strings\\n    }\\n    arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}\\n    defaults = parser.parse_args(args=[force_text(a) for a in args])\\n    defaults = dict(defaults._get_kwargs(), **arg_options)\\n    args = defaults.pop('args', ())\\n    if 'skip_checks' not in options:\\n        defaults['skip_checks'] = True\\n    return command.execute(*args, **defaults)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def call_command(name, *args, **options):\\n    try:\\n        app_name = get_commands()[name]\\n    except KeyError:\\n        raise CommandError(\"Unknown command: %r\" % name)\\n    if isinstance(app_name, BaseCommand):\\n        command = app_name\\n    else:\\n        command = load_command_class(app_name, name)\\n    parser = command.create_parser('', name)\\n    opt_mapping = {\\n        sorted(s_opt.option_strings)[0].lstrip('-').replace('-', '_'): s_opt.dest\\n        for s_opt in parser._actions if s_opt.option_strings\\n    }\\n    arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}\\n    defaults = parser.parse_args(args=[force_text(a) for a in args])\\n    defaults = dict(defaults._get_kwargs(), **arg_options)\\n    args = defaults.pop('args', ())\\n    if 'skip_checks' not in options:\\n        defaults['skip_checks'] = True\\n    return command.execute(*args, **defaults)"
  },
  {
    "code": "def _should_reindex_frame_op(\\n    left: \"DataFrame\", right, op, axis, default_axis, fill_value, level\\n) -> bool:\\n    assert isinstance(left, ABCDataFrame)\\n    if op is operator.pow or op is rpow:\\n        return False\\n    if not isinstance(right, ABCDataFrame):\\n        return False\\n    if fill_value is None and level is None and axis is default_axis:\\n        cols = left.columns.intersection(right.columns)\\n        if len(cols) and not (cols.equals(left.columns) and cols.equals(right.columns)):\\n            return True\\n    return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _should_reindex_frame_op(\\n    left: \"DataFrame\", right, op, axis, default_axis, fill_value, level\\n) -> bool:\\n    assert isinstance(left, ABCDataFrame)\\n    if op is operator.pow or op is rpow:\\n        return False\\n    if not isinstance(right, ABCDataFrame):\\n        return False\\n    if fill_value is None and level is None and axis is default_axis:\\n        cols = left.columns.intersection(right.columns)\\n        if len(cols) and not (cols.equals(left.columns) and cols.equals(right.columns)):\\n            return True\\n    return False"
  },
  {
    "code": "def get_diagnostics(self, context, instance=None, instance_uuid=None):\\n\\t\\tif not instance:\\n\\t\\t\\tinstance = self.db.instance_get_by_uuid(context, instance_uuid)\\n\\t\\tcurrent_power_state = self._get_power_state(context, instance)\\n\\t\\tif current_power_state == power_state.RUNNING:\\n\\t\\t\\tLOG.audit(_(\"Retrieving diagnostics\"), context=context,\\n\\t\\t\\t\\t\\t  instance=instance)\\n\\t\\t\\treturn self.driver.get_diagnostics(instance)\\n\\t@exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())\\n\\t@checks_instance_lock\\n\\t@wrap_instance_fault",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_diagnostics(self, context, instance=None, instance_uuid=None):\\n\\t\\tif not instance:\\n\\t\\t\\tinstance = self.db.instance_get_by_uuid(context, instance_uuid)\\n\\t\\tcurrent_power_state = self._get_power_state(context, instance)\\n\\t\\tif current_power_state == power_state.RUNNING:\\n\\t\\t\\tLOG.audit(_(\"Retrieving diagnostics\"), context=context,\\n\\t\\t\\t\\t\\t  instance=instance)\\n\\t\\t\\treturn self.driver.get_diagnostics(instance)\\n\\t@exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())\\n\\t@checks_instance_lock\\n\\t@wrap_instance_fault"
  },
  {
    "code": "def apply_raw(self):\\n        result, reduction_success = libreduction.compute_reduction(\\n            self.values, self.f, axis=self.axis\\n        )\\n        if not reduction_success:\\n            result = np.apply_along_axis(self.f, self.axis, self.values)\\n        if result.ndim == 2:\\n            return self.obj._constructor(result, index=self.index, columns=self.columns)\\n        else:\\n            return self.obj._constructor_sliced(result, index=self.agg_axis)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def apply_raw(self):\\n        result, reduction_success = libreduction.compute_reduction(\\n            self.values, self.f, axis=self.axis\\n        )\\n        if not reduction_success:\\n            result = np.apply_along_axis(self.f, self.axis, self.values)\\n        if result.ndim == 2:\\n            return self.obj._constructor(result, index=self.index, columns=self.columns)\\n        else:\\n            return self.obj._constructor_sliced(result, index=self.agg_axis)"
  },
  {
    "code": "def do_extends(parser, token):\\n    bits = token.split_contents()\\n    if len(bits) != 2:\\n        raise TemplateSyntaxError, \"'%s' takes one argument\" % bits[0]\\n    parent_name, parent_name_expr = None, None\\n    if bits[1][0] in ('\"', \"'\") and bits[1][-1] == bits[1][0]:\\n        parent_name = bits[1][1:-1]\\n    else:\\n        parent_name_expr = parser.compile_filter(bits[1])\\n    nodelist = parser.parse()\\n    if nodelist.get_nodes_by_type(ExtendsNode):\\n        raise TemplateSyntaxError, \"'%s' cannot appear more than once in the same template\" % bits[0]\\n    return ExtendsNode(nodelist, parent_name, parent_name_expr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_extends(parser, token):\\n    bits = token.split_contents()\\n    if len(bits) != 2:\\n        raise TemplateSyntaxError, \"'%s' takes one argument\" % bits[0]\\n    parent_name, parent_name_expr = None, None\\n    if bits[1][0] in ('\"', \"'\") and bits[1][-1] == bits[1][0]:\\n        parent_name = bits[1][1:-1]\\n    else:\\n        parent_name_expr = parser.compile_filter(bits[1])\\n    nodelist = parser.parse()\\n    if nodelist.get_nodes_by_type(ExtendsNode):\\n        raise TemplateSyntaxError, \"'%s' cannot appear more than once in the same template\" % bits[0]\\n    return ExtendsNode(nodelist, parent_name, parent_name_expr)"
  },
  {
    "code": "def __init__(\\n            self,\\n            dag_folder=None,\\n            include_examples=conf.getboolean('core', 'LOAD_EXAMPLES'),\\n            safe_mode=conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE'),\\n            store_serialized_dags=False,\\n    ):\\n        dag_folder = dag_folder or settings.DAGS_FOLDER\\n        self.dag_folder = dag_folder\\n        self.dags = {}\\n        self.file_last_changed = {}\\n        self.import_errors = {}\\n        self.has_logged = False\\n        self.store_serialized_dags = store_serialized_dags\\n        self.collect_dags(\\n            dag_folder=dag_folder,\\n            include_examples=include_examples,\\n            safe_mode=safe_mode)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n            self,\\n            dag_folder=None,\\n            include_examples=conf.getboolean('core', 'LOAD_EXAMPLES'),\\n            safe_mode=conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE'),\\n            store_serialized_dags=False,\\n    ):\\n        dag_folder = dag_folder or settings.DAGS_FOLDER\\n        self.dag_folder = dag_folder\\n        self.dags = {}\\n        self.file_last_changed = {}\\n        self.import_errors = {}\\n        self.has_logged = False\\n        self.store_serialized_dags = store_serialized_dags\\n        self.collect_dags(\\n            dag_folder=dag_folder,\\n            include_examples=include_examples,\\n            safe_mode=safe_mode)"
  },
  {
    "code": "def _checkflag(flag):\\n    if flag == 'r':\\n        flags = db.DB_RDONLY\\n    elif flag == 'rw':\\n        flags = 0\\n    elif flag == 'w':\\n        flags =  db.DB_CREATE\\n    elif flag == 'c':\\n        flags =  db.DB_CREATE\\n    elif flag == 'n':\\n        flags = db.DB_CREATE | db.DB_TRUNCATE\\n    else:\\n        raise error, \"flags should be one of 'r', 'w', 'c' or 'n'\"\\n    return flags | db.DB_THREAD",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _checkflag(flag):\\n    if flag == 'r':\\n        flags = db.DB_RDONLY\\n    elif flag == 'rw':\\n        flags = 0\\n    elif flag == 'w':\\n        flags =  db.DB_CREATE\\n    elif flag == 'c':\\n        flags =  db.DB_CREATE\\n    elif flag == 'n':\\n        flags = db.DB_CREATE | db.DB_TRUNCATE\\n    else:\\n        raise error, \"flags should be one of 'r', 'w', 'c' or 'n'\"\\n    return flags | db.DB_THREAD"
  },
  {
    "code": "def check_for_bucket(self, bucket_name):\\n        try:\\n            self.get_conn().head_bucket(Bucket=bucket_name)\\n            return True\\n        except ClientError as e:\\n            self.log.info(e.response[\"Error\"][\"Message\"])\\n            return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5057] Provide bucket name to functions in S3 Hook when none is specified (#5674)\\n\\nNote: The order of arguments has changed for `check_for_prefix`.\\nThe `bucket_name` is now optional. It falls back to the `connection schema` attribute.\\n- refactor code\\n- complete docs",
    "fixed_code": "def check_for_bucket(self, bucket_name=None):\\n        try:\\n            self.get_conn().head_bucket(Bucket=bucket_name)\\n            return True\\n        except ClientError as e:\\n            self.log.info(e.response[\"Error\"][\"Message\"])\\n            return False\\n    @provide_bucket_name"
  },
  {
    "code": "def _logical_method(self, other, op):\\n        assert op.__name__ in {\"or_\", \"ror_\", \"and_\", \"rand_\", \"xor\", \"rxor\"}\\n        other_is_booleanarray = isinstance(other, BooleanArray)\\n        other_is_scalar = lib.is_scalar(other)\\n        mask = None\\n        if other_is_booleanarray:\\n            other, mask = other._data, other._mask\\n        elif is_list_like(other):\\n            other = np.asarray(other, dtype=\"bool\")\\n            if other.ndim > 1:\\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\\n            other, mask = coerce_to_array(other, copy=False)\\n        elif isinstance(other, np.bool_):\\n            other = other.item()\\n        if other_is_scalar and other is not libmissing.NA and not lib.is_bool(other):\\n            raise TypeError(\\n                \"'other' should be pandas.NA or a bool. \"\\n                f\"Got {type(other).__name__} instead.\"\\n            )\\n        if not other_is_scalar and len(self) != len(other):\\n            raise ValueError(\"Lengths must match to compare\")\\n        if op.__name__ in {\"or_\", \"ror_\"}:\\n            result, mask = ops.kleene_or(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"and_\", \"rand_\"}:\\n            result, mask = ops.kleene_and(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"xor\", \"rxor\"}:\\n            result, mask = ops.kleene_xor(self._data, other, self._mask, mask)\\n        return BooleanArray(result, mask)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: NumericArray * td64_array (#45622)",
    "fixed_code": "def _logical_method(self, other, op):\\n        assert op.__name__ in {\"or_\", \"ror_\", \"and_\", \"rand_\", \"xor\", \"rxor\"}\\n        other_is_scalar = lib.is_scalar(other)\\n        mask = None\\n        if isinstance(other, BooleanArray):\\n            other, mask = other._data, other._mask\\n        elif is_list_like(other):\\n            other = np.asarray(other, dtype=\"bool\")\\n            if other.ndim > 1:\\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\\n            other, mask = coerce_to_array(other, copy=False)\\n        elif isinstance(other, np.bool_):\\n            other = other.item()\\n        if other_is_scalar and other is not libmissing.NA and not lib.is_bool(other):\\n            raise TypeError(\\n                \"'other' should be pandas.NA or a bool. \"\\n                f\"Got {type(other).__name__} instead.\"\\n            )\\n        if not other_is_scalar and len(self) != len(other):\\n            raise ValueError(\"Lengths must match\")\\n        if op.__name__ in {\"or_\", \"ror_\"}:\\n            result, mask = ops.kleene_or(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"and_\", \"rand_\"}:\\n            result, mask = ops.kleene_and(self._data, other, self._mask, mask)\\n        elif op.__name__ in {\"xor\", \"rxor\"}:\\n            result, mask = ops.kleene_xor(self._data, other, self._mask, mask)\\n        return BooleanArray(result, mask)"
  },
  {
    "code": "def __sub__(self, other):\\n        if isinstance(other, datetime):\\n            raise TypeError('Cannot subtract datetime from offset.')\\n        elif type(other) == type(self):\\n            return self.__class__(self.n - other.n, **self.kwds)\\n        else:  \\n            return NotImplemented",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __sub__(self, other):\\n        if isinstance(other, datetime):\\n            raise TypeError('Cannot subtract datetime from offset.')\\n        elif type(other) == type(self):\\n            return self.__class__(self.n - other.n, **self.kwds)\\n        else:  \\n            return NotImplemented"
  },
  {
    "code": "def get_vpc(module):\\n    body = run_commands(module, ['show vpc | json'])[0]\\n    if body:\\n        domain = str(body['vpc-domain-id'])\\n    else:\\n        body = run_commands(module, ['show run vpc | inc domain'])[0]\\n        if body:\\n            domain = body.split()[2]\\n        else:\\n            domain = 'not configured'\\n    vpc = {}\\n    if domain != 'not configured':\\n        run = get_config(module, flags=['vpc all'])\\n        if run:\\n            vpc['domain'] = domain\\n            for key in PARAM_TO_DEFAULT_KEYMAP.keys():\\n                vpc[key] = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n            vpc['auto_recovery'] = get_auto_recovery_default(module)\\n            vpc_list = run.split('\\n')\\n            for each in vpc_list:\\n                if 'role priority' in each:\\n                    line = each.split()\\n                    vpc['role_priority'] = line[-1]\\n                if 'system-priority' in each:\\n                    line = each.split()\\n                    vpc['system_priority'] = line[-1]\\n                if re.search(r'delay restore \\d+', each):\\n                    line = each.split()\\n                    vpc['delay_restore'] = line[-1]\\n                if 'delay restore interface-vlan' in each:\\n                    line = each.split()\\n                    vpc['delay_restore_interface_vlan'] = line[-1]\\n                if 'delay restore orphan-port' in each:\\n                    line = each.split()\\n                    vpc['delay_restore_orphan_port'] = line[-1]\\n                if 'auto-recovery' in each:\\n                    vpc['auto_recovery'] = False if 'no ' in each else True\\n                    line = each.split()\\n                    vpc['auto_recovery_reload_delay'] = line[-1]\\n                if 'peer-gateway' in each:\\n                    vpc['peer_gw'] = False if 'no ' in each else True\\n                if 'peer-keepalive destination' in each:\\n                    m = re.search(r'destination (?P<pkl_dest>[\\d.]+)'\\n                                  r'(?:.* source (?P<pkl_src>[\\d.]+))*'\\n                                  r'(?:.* vrf (?P<pkl_vrf>\\S+))*',\\n                                  each)\\n                    if m:\\n                        for pkl in m.groupdict().keys():\\n                            if m.group(pkl):\\n                                vpc[pkl] = m.group(pkl)\\n    return vpc",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_vpc(module):\\n    body = run_commands(module, ['show vpc | json'])[0]\\n    if body:\\n        domain = str(body['vpc-domain-id'])\\n    else:\\n        body = run_commands(module, ['show run vpc | inc domain'])[0]\\n        if body:\\n            domain = body.split()[2]\\n        else:\\n            domain = 'not configured'\\n    vpc = {}\\n    if domain != 'not configured':\\n        run = get_config(module, flags=['vpc all'])\\n        if run:\\n            vpc['domain'] = domain\\n            for key in PARAM_TO_DEFAULT_KEYMAP.keys():\\n                vpc[key] = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n            vpc['auto_recovery'] = get_auto_recovery_default(module)\\n            vpc_list = run.split('\\n')\\n            for each in vpc_list:\\n                if 'role priority' in each:\\n                    line = each.split()\\n                    vpc['role_priority'] = line[-1]\\n                if 'system-priority' in each:\\n                    line = each.split()\\n                    vpc['system_priority'] = line[-1]\\n                if re.search(r'delay restore \\d+', each):\\n                    line = each.split()\\n                    vpc['delay_restore'] = line[-1]\\n                if 'delay restore interface-vlan' in each:\\n                    line = each.split()\\n                    vpc['delay_restore_interface_vlan'] = line[-1]\\n                if 'delay restore orphan-port' in each:\\n                    line = each.split()\\n                    vpc['delay_restore_orphan_port'] = line[-1]\\n                if 'auto-recovery' in each:\\n                    vpc['auto_recovery'] = False if 'no ' in each else True\\n                    line = each.split()\\n                    vpc['auto_recovery_reload_delay'] = line[-1]\\n                if 'peer-gateway' in each:\\n                    vpc['peer_gw'] = False if 'no ' in each else True\\n                if 'peer-keepalive destination' in each:\\n                    m = re.search(r'destination (?P<pkl_dest>[\\d.]+)'\\n                                  r'(?:.* source (?P<pkl_src>[\\d.]+))*'\\n                                  r'(?:.* vrf (?P<pkl_vrf>\\S+))*',\\n                                  each)\\n                    if m:\\n                        for pkl in m.groupdict().keys():\\n                            if m.group(pkl):\\n                                vpc[pkl] = m.group(pkl)\\n    return vpc"
  },
  {
    "code": "def max(self, axis=0):\\n        values = self.values.copy()\\n        np.putmask(values, -np.isfinite(values), -np.inf)\\n        return Series(values.max(axis), index=self._get_agg_axis(axis))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: skipna in DataFrame stat functions",
    "fixed_code": "def max(self, axis=0, skipna=True):\\n        values = self.values.copy()\\n        if skipna:\\n            np.putmask(values, -np.isfinite(values), -np.inf)\\n        return Series(values.max(axis), index=self._get_agg_axis(axis))"
  },
  {
    "code": "def compile(p, flags=0):\\n    if isstring(p):\\n        pattern = p\\n        p = sre_parse.parse(p, flags)\\n    else:\\n        pattern = None\\n    code = _code(p, flags)\\n    groupindex = p.pattern.groupdict\\n    indexgroup = [None] * p.pattern.groups\\n    for k, i in groupindex.items():\\n        indexgroup[i] = k\\n    return _sre.compile(\\n        pattern, flags | p.pattern.flags, code,\\n        p.pattern.groups-1,\\n        groupindex, indexgroup\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #28765: _sre.compile() now checks the type of groupindex and indexgroup\\n\\ngroupindex must a dictionary and indexgroup must be a tuple.\\n\\nPreviously, indexgroup was a list. Use a tuple to reduce the memory usage.",
    "fixed_code": "def compile(p, flags=0):\\n    if isstring(p):\\n        pattern = p\\n        p = sre_parse.parse(p, flags)\\n    else:\\n        pattern = None\\n    code = _code(p, flags)\\n    groupindex = p.pattern.groupdict\\n    indexgroup = [None] * p.pattern.groups\\n    for k, i in groupindex.items():\\n        indexgroup[i] = k\\n    return _sre.compile(\\n        pattern, flags | p.pattern.flags, code,\\n        p.pattern.groups-1,\\n        groupindex, tuple(indexgroup)\\n        )"
  },
  {
    "code": "def _is_null_datelike_scalar(other):\\n    if other is pd.NaT or other is None:\\n        return True\\n    elif np.isscalar(other):\\n        if hasattr(other,'dtype'):\\n            return other.view('i8') == tslib.iNaT\\n        elif is_integer(other) and other == tslib.iNaT:\\n            return True\\n        return isnull(other)\\n    return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _is_null_datelike_scalar(other):\\n    if other is pd.NaT or other is None:\\n        return True\\n    elif np.isscalar(other):\\n        if hasattr(other,'dtype'):\\n            return other.view('i8') == tslib.iNaT\\n        elif is_integer(other) and other == tslib.iNaT:\\n            return True\\n        return isnull(other)\\n    return False"
  },
  {
    "code": "def _create_provider_info_schema_validator():\\n    with resource_files(\"airflow\").joinpath(\"provider_info.schema.json\").open(\"rb\") as f:\\n        schema = json.load(f)\\n    cls = jsonschema.validators.validator_for(schema)\\n    validator = cls(schema)\\n    return validator",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_provider_info_schema_validator():\\n    with resource_files(\"airflow\").joinpath(\"provider_info.schema.json\").open(\"rb\") as f:\\n        schema = json.load(f)\\n    cls = jsonschema.validators.validator_for(schema)\\n    validator = cls(schema)\\n    return validator"
  },
  {
    "code": "def __init__(self, argv=None):\\n        self.argv = argv or sys.argv[:]\\n        self.prog_name = os.path.basename(self.argv[0])\\n        if self.prog_name == '__main__.py':\\n            self.prog_name = 'python -m django'\\n        self.settings_exception = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, argv=None):\\n        self.argv = argv or sys.argv[:]\\n        self.prog_name = os.path.basename(self.argv[0])\\n        if self.prog_name == '__main__.py':\\n            self.prog_name = 'python -m django'\\n        self.settings_exception = None"
  },
  {
    "code": "def __init__(\\n        self,\\n        data: BlockManager,\\n        copy: bool = False,\\n        attrs: Optional[Mapping[Optional[Hashable], Any]] = None,\\n    ):\\n        object.__setattr__(self, \"_is_copy\", None)\\n        object.__setattr__(self, \"_data\", data)\\n        object.__setattr__(self, \"_item_cache\", {})\\n        if attrs is None:\\n            attrs = {}\\n        else:\\n            attrs = dict(attrs)\\n        object.__setattr__(self, \"_attrs\", attrs)\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        data: BlockManager,\\n        copy: bool = False,\\n        attrs: Optional[Mapping[Optional[Hashable], Any]] = None,\\n    ):\\n        object.__setattr__(self, \"_is_copy\", None)\\n        object.__setattr__(self, \"_data\", data)\\n        object.__setattr__(self, \"_item_cache\", {})\\n        if attrs is None:\\n            attrs = {}\\n        else:\\n            attrs = dict(attrs)\\n        object.__setattr__(self, \"_attrs\", attrs)\\n    @classmethod"
  },
  {
    "code": "def load(self):\\n\\t\\ttry:\\n\\t\\t\\tsession_data = self._cache.get(self.cache_key)\\n\\t\\texcept Exception:\\n\\t\\t\\tsession_data = None\\n\\t\\tif session_data is not None:\\n\\t\\t\\treturn session_data\\n\\t\\tself.create()\\n\\t\\treturn {}",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #19324 -- Avoided creating a session record when loading the session.\\n\\nThe session record is now only created if/when the session is modified. This\\nprevents a potential DoS via creation of many empty session records.\\n\\nThis is a security fix; disclosure to follow shortly.",
    "fixed_code": "def load(self):\\n\\t\\ttry:\\n\\t\\t\\tsession_data = self._cache.get(self.cache_key)\\n\\t\\texcept Exception:\\n\\t\\t\\tsession_data = None\\n\\t\\tif session_data is not None:\\n\\t\\t\\treturn session_data\\n\\t\\tself._session_key = None\\n\\t\\treturn {}"
  },
  {
    "code": "def __init__(self, azure_data_lake_conn_id='azure_data_lake_default'):\\n        self.conn_id = azure_data_lake_conn_id\\n        self._conn = None\\n        self.account_name = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, azure_data_lake_conn_id='azure_data_lake_default'):\\n        self.conn_id = azure_data_lake_conn_id\\n        self._conn = None\\n        self.account_name = None"
  },
  {
    "code": "def _get_from_cache(self, opts):\\n        key = (opts.app_label, opts.model_name)\\n        return self.__class__._cache[self.db][key]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26286 -- Prevented content type managers from sharing their cache.\\n\\nThis should prevent managers methods from returning content type instances\\nregistered to foreign apps now that these managers are also attached to models\\ncreated during migration phases.\\n\\nThanks Tim for the review.\\n\\nRefs #23822.",
    "fixed_code": "def _get_from_cache(self, opts):\\n        key = (opts.app_label, opts.model_name)\\n        return self._cache[self.db][key]"
  },
  {
    "code": "def _find_and_load_unlocked(name, import_):\\n    path = None\\n    parent = name.rpartition('.')[0]\\n    if parent:\\n        if parent not in sys.modules:\\n            _call_with_frames_removed(import_, parent)\\n        if name in sys.modules:\\n            return sys.modules[name]\\n        parent_module = sys.modules[parent]\\n        try:\\n            path = parent_module.__path__\\n        except AttributeError:\\n            msg = (_ERR_MSG + '; {} is not a package').format(name, parent)\\n            raise ImportError(msg, name=name)\\n    loader = _find_module(name, path)\\n    if loader is None:\\n        exc = ImportError(_ERR_MSG.format(name), name=name)\\n        exc._not_found = True\\n        raise exc\\n    elif name not in sys.modules:\\n        loader.load_module(name)\\n        _verbose_message('import {!r} \\n    module = sys.modules[name]\\n    if parent:\\n        parent_module = sys.modules[parent]\\n        setattr(parent_module, name.rpartition('.')[2], module)\\n    if getattr(module, '__package__', None) is None:\\n        try:\\n            module.__package__ = module.__name__\\n            if not hasattr(module, '__path__'):\\n                module.__package__ = module.__package__.rpartition('.')[0]\\n        except AttributeError:\\n            pass\\n    if getattr(module, '__loader__', None) is None:\\n        try:\\n            module.__loader__ = loader\\n        except AttributeError:\\n            pass\\n    return module",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _find_and_load_unlocked(name, import_):\\n    path = None\\n    parent = name.rpartition('.')[0]\\n    if parent:\\n        if parent not in sys.modules:\\n            _call_with_frames_removed(import_, parent)\\n        if name in sys.modules:\\n            return sys.modules[name]\\n        parent_module = sys.modules[parent]\\n        try:\\n            path = parent_module.__path__\\n        except AttributeError:\\n            msg = (_ERR_MSG + '; {} is not a package').format(name, parent)\\n            raise ImportError(msg, name=name)\\n    loader = _find_module(name, path)\\n    if loader is None:\\n        exc = ImportError(_ERR_MSG.format(name), name=name)\\n        exc._not_found = True\\n        raise exc\\n    elif name not in sys.modules:\\n        loader.load_module(name)\\n        _verbose_message('import {!r} \\n    module = sys.modules[name]\\n    if parent:\\n        parent_module = sys.modules[parent]\\n        setattr(parent_module, name.rpartition('.')[2], module)\\n    if getattr(module, '__package__', None) is None:\\n        try:\\n            module.__package__ = module.__name__\\n            if not hasattr(module, '__path__'):\\n                module.__package__ = module.__package__.rpartition('.')[0]\\n        except AttributeError:\\n            pass\\n    if getattr(module, '__loader__', None) is None:\\n        try:\\n            module.__loader__ = loader\\n        except AttributeError:\\n            pass\\n    return module"
  },
  {
    "code": "def __init__(self,\\n                 datastore_conn_id='google_cloud_default',\\n                 delegate_to=None):\\n        super(DatastoreHook, self).__init__(datastore_conn_id, delegate_to)\\n        self.connection = self.get_conn()\\n        self.admin_connection = self.get_conn('v1beta1')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4014] Change DatastoreHook and add tests (#4842)\\n\\n- update default used version for connecting to the Admin API from v1beta1 to v1\\n- move the establishment of the connection to the function calls instead of the hook init\\n- change get_conn signature to be able to pass an is_admin arg to set an admin connection\\n- rename GoogleCloudBaseHook._authorize function to GoogleCloudBaseHook.authorize\\n- rename the `partialKeys` argument of function `allocate_ids` to `partial_keys`.\\n- add tests\\n- update docs\\n- refactor code\\n\\nMove version attribute from get_conn to __init__\\n\\n- revert renaming of authorize function\\n- improve docs\\n- refactor code",
    "fixed_code": "def __init__(self,\\n                 datastore_conn_id='google_cloud_default',\\n                 delegate_to=None,\\n                 api_version='v1'):\\n        super(DatastoreHook, self).__init__(datastore_conn_id, delegate_to)\\n        self.connection = None\\n        self.api_version = api_version"
  },
  {
    "code": "def inverse_transform(self, Xt):\\n        check_is_fitted(self)\\n        if 'onehot' in self.encode:\\n            Xt = self._encoder.inverse_transform(Xt)\\n        Xinv = check_array(Xt, copy=True, dtype=(np.float64, np.float32))\\n        n_features = self.n_bins_.shape[0]\\n        if Xinv.shape[1] != n_features:\\n            raise ValueError(\"Incorrect number of features. Expecting {}, \"\\n                             \"received {}.\".format(n_features, Xinv.shape[1]))\\n        for jj in range(n_features):\\n            bin_edges = self.bin_edges_[jj]\\n            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\\n            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]\\n        return Xinv",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def inverse_transform(self, Xt):\\n        check_is_fitted(self)\\n        if 'onehot' in self.encode:\\n            Xt = self._encoder.inverse_transform(Xt)\\n        Xinv = check_array(Xt, copy=True, dtype=(np.float64, np.float32))\\n        n_features = self.n_bins_.shape[0]\\n        if Xinv.shape[1] != n_features:\\n            raise ValueError(\"Incorrect number of features. Expecting {}, \"\\n                             \"received {}.\".format(n_features, Xinv.shape[1]))\\n        for jj in range(n_features):\\n            bin_edges = self.bin_edges_[jj]\\n            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\\n            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]\\n        return Xinv"
  },
  {
    "code": "def ewmvar(arg, com=None, span=None, halflife=None, min_periods=0, bias=False,\\n           freq=None, time_rule=None):\\n    com = _get_center_of_mass(com, span, halflife)\\n    arg = _conv_timerule(arg, freq, time_rule)\\n    moment2nd = ewma(arg * arg, com=com, min_periods=min_periods)\\n    moment1st = ewma(arg, com=com, min_periods=min_periods)\\n    result = moment2nd - moment1st ** 2\\n    if not bias:\\n        result *= (1.0 + 2.0 * com) / (2.0 * com)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ewmvar(arg, com=None, span=None, halflife=None, min_periods=0, bias=False,\\n           freq=None, time_rule=None):\\n    com = _get_center_of_mass(com, span, halflife)\\n    arg = _conv_timerule(arg, freq, time_rule)\\n    moment2nd = ewma(arg * arg, com=com, min_periods=min_periods)\\n    moment1st = ewma(arg, com=com, min_periods=min_periods)\\n    result = moment2nd - moment1st ** 2\\n    if not bias:\\n        result *= (1.0 + 2.0 * com) / (2.0 * com)\\n    return result"
  },
  {
    "code": "def read_pickle(\\n\\tfilepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\\n):\\n\\tfp_or_buf, _, compression, should_close = get_filepath_or_buffer(\\n\\t\\tfilepath_or_buffer, compression=compression\\n\\t)\\n\\tif not isinstance(fp_or_buf, str) and compression == \"infer\":\\n\\t\\tcompression = None\\n\\tf, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\\n\\texcs_to_catch = (AttributeError, ImportError, ModuleNotFoundError)\\n\\ttry:\\n\\t\\twith warnings.catch_warnings(record=True):\\n\\t\\t\\twarnings.simplefilter(\"ignore\", Warning)\\n\\t\\t\\treturn pickle.load(f)\\n\\texcept excs_to_catch:\\n\\t\\treturn pc.load(f, encoding=None)\\n\\texcept UnicodeDecodeError:\\n\\t\\treturn pc.load(f, encoding=\"latin-1\")\\n\\tfinally:\\n\\t\\tf.close()\\n\\t\\tfor _f in fh:\\n\\t\\t\\t_f.close()\\n\\t\\tif should_close:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tfp_or_buf.close()\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_pickle(\\n\\tfilepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\\n):\\n\\tfp_or_buf, _, compression, should_close = get_filepath_or_buffer(\\n\\t\\tfilepath_or_buffer, compression=compression\\n\\t)\\n\\tif not isinstance(fp_or_buf, str) and compression == \"infer\":\\n\\t\\tcompression = None\\n\\tf, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\\n\\texcs_to_catch = (AttributeError, ImportError, ModuleNotFoundError)\\n\\ttry:\\n\\t\\twith warnings.catch_warnings(record=True):\\n\\t\\t\\twarnings.simplefilter(\"ignore\", Warning)\\n\\t\\t\\treturn pickle.load(f)\\n\\texcept excs_to_catch:\\n\\t\\treturn pc.load(f, encoding=None)\\n\\texcept UnicodeDecodeError:\\n\\t\\treturn pc.load(f, encoding=\"latin-1\")\\n\\tfinally:\\n\\t\\tf.close()\\n\\t\\tfor _f in fh:\\n\\t\\t\\t_f.close()\\n\\t\\tif should_close:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tfp_or_buf.close()\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass"
  },
  {
    "code": "def format_file_in_place(\\n    src: Path,\\n    line_length: int,\\n    fast: bool,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> Changed:\\n    with tokenize.open(src) as src_buffer:\\n        src_contents = src_buffer.read()\\n    try:\\n        dst_contents = format_file_contents(\\n            src_contents, line_length=line_length, fast=fast\\n        )\\n    except NothingChanged:\\n        return Changed.NO\\n    if write_back == write_back.YES:\\n        with open(src, \"w\", encoding=src_buffer.encoding) as f:\\n            f.write(dst_contents)\\n    elif write_back == write_back.DIFF:\\n        src_name = f\"{src.name}  (original)\"\\n        dst_name = f\"{src.name}  (formatted)\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if lock:\\n            lock.acquire()\\n        try:\\n            sys.stdout.write(diff_contents)\\n        finally:\\n            if lock:\\n                lock.release()\\n    return Changed.YES",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert `format_file_in_place()` and `format_stdin_to_stdout()` to return bools\\n\\n`Changed.CACHED` is meaningless for those two functions.",
    "fixed_code": "def format_file_in_place(\\n    src: Path,\\n    line_length: int,\\n    fast: bool,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    with tokenize.open(src) as src_buffer:\\n        src_contents = src_buffer.read()\\n    try:\\n        dst_contents = format_file_contents(\\n            src_contents, line_length=line_length, fast=fast\\n        )\\n    except NothingChanged:\\n        return False\\n    if write_back == write_back.YES:\\n        with open(src, \"w\", encoding=src_buffer.encoding) as f:\\n            f.write(dst_contents)\\n    elif write_back == write_back.DIFF:\\n        src_name = f\"{src.name}  (original)\"\\n        dst_name = f\"{src.name}  (formatted)\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if lock:\\n            lock.acquire()\\n        try:\\n            sys.stdout.write(diff_contents)\\n        finally:\\n            if lock:\\n                lock.release()\\n    return True"
  },
  {
    "code": "def _period_str_to_code(freqstr):\\n    freqstr = _lite_rule_alias.get(freqstr, freqstr)\\n    if freqstr not in _dont_uppercase:\\n        lower = freqstr.lower()\\n        freqstr = _lite_rule_alias.get(lower, freqstr)\\n    if freqstr not in _dont_uppercase:\\n        freqstr = freqstr.upper()\\n    try:\\n        return _period_code_map[freqstr]\\n    except KeyError:\\n        raise ValueError(_INVALID_FREQ_ERROR.format(freqstr))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _period_str_to_code(freqstr):\\n    freqstr = _lite_rule_alias.get(freqstr, freqstr)\\n    if freqstr not in _dont_uppercase:\\n        lower = freqstr.lower()\\n        freqstr = _lite_rule_alias.get(lower, freqstr)\\n    if freqstr not in _dont_uppercase:\\n        freqstr = freqstr.upper()\\n    try:\\n        return _period_code_map[freqstr]\\n    except KeyError:\\n        raise ValueError(_INVALID_FREQ_ERROR.format(freqstr))"
  },
  {
    "code": "def _compare_constructor(self, other, func):\\n        new_data = {}\\n        for col in getattr(self,self._info_axis):\\n            new_data[col] = func(self[col], other[col])\\n        d = self._construct_axes_dict()\\n        d['copy'] = False\\n        return self._constructor(data=new_data, **d)\\n    __and__ = _arith_method(operator.and_, '__and__')\\n    __or__ = _arith_method(operator.or_, '__or__')\\n    __xor__ = _arith_method(operator.xor, '__xor__')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "add tests for align in comparison ops (we don't have align yet, so raise!)",
    "fixed_code": "def _compare_constructor(self, other, func):\\n        if not self._indexed_same(other):\\n            raise Exception('Can only compare identically-labeled '\\n                            'same type objects')\\n        new_data = {}\\n        for col in getattr(self,self._info_axis):\\n            new_data[col] = func(self[col], other[col])\\n        d = self._construct_axes_dict()\\n        d['copy'] = False\\n        return self._constructor(data=new_data, **d)\\n    __and__ = _arith_method(operator.and_, '__and__')\\n    __or__ = _arith_method(operator.or_, '__or__')\\n    __xor__ = _arith_method(operator.xor, '__xor__')"
  },
  {
    "code": "def add_legacy_name(apps, schema_editor):\\n\\talias = schema_editor.connection.alias\\n\\tContentType = apps.get_model(\"contenttypes\", \"ContentType\")\\n\\tfor ct in ContentType.objects.using(alias):\\n\\t\\ttry:\\n\\t\\t\\tct.name = apps.get_model(ct.app_label, ct.model)._meta.object_name\\n\\t\\texcept LookupError:\\n\\t\\t\\tct.name = ct.model\\n\\t\\tct.save()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def add_legacy_name(apps, schema_editor):\\n\\talias = schema_editor.connection.alias\\n\\tContentType = apps.get_model(\"contenttypes\", \"ContentType\")\\n\\tfor ct in ContentType.objects.using(alias):\\n\\t\\ttry:\\n\\t\\t\\tct.name = apps.get_model(ct.app_label, ct.model)._meta.object_name\\n\\t\\texcept LookupError:\\n\\t\\t\\tct.name = ct.model\\n\\t\\tct.save()"
  },
  {
    "code": "def _add_to_cache(self, using, ct):\\n        key = (ct.app_label, ct.model)\\n        self.__class__._cache.setdefault(using, {})[key] = ct\\n        self.__class__._cache.setdefault(using, {})[ct.id] = ct",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26286 -- Prevented content type managers from sharing their cache.\\n\\nThis should prevent managers methods from returning content type instances\\nregistered to foreign apps now that these managers are also attached to models\\ncreated during migration phases.\\n\\nThanks Tim for the review.\\n\\nRefs #23822.",
    "fixed_code": "def _add_to_cache(self, using, ct):\\n        key = (ct.app_label, ct.model)\\n        self._cache.setdefault(using, {})[key] = ct\\n        self._cache.setdefault(using, {})[ct.id] = ct"
  },
  {
    "code": "def setup(self, dtype, scalar, op):\\n        arr = np.random.randn(20000, 100)\\n        self.df = DataFrame(arr.astype(dtype))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setup(self, dtype, scalar, op):\\n        arr = np.random.randn(20000, 100)\\n        self.df = DataFrame(arr.astype(dtype))"
  },
  {
    "code": "def sort(self, axis=0, kind='quicksort', order=None):\\n        sortedSeries = self.order(na_last=True, kind=kind)\\n        true_base = self\\n        while true_base.base is not None:\\n            true_base = true_base.base\\n        if (true_base is not None and\\n            (true_base.ndim != 1 or true_base.shape != self.shape)):\\n            raise Exception('This Series is a view of some other array, to '\\n                            'sort in-place you must create a copy')\\n        self[:] = sortedSeries\\n        self.index = sortedSeries.index\\n    def sort_index(self, ascending=True):\\n        labels = self.index\\n        sort_index = labels.argsort()\\n        if not ascending:\\n            sort_index = sort_index[::-1]\\n        new_labels = labels.take(sort_index)\\n        new_values = self.values.take(sort_index)\\n        return Series(new_values, new_labels, name=self.name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sort(self, axis=0, kind='quicksort', order=None):\\n        sortedSeries = self.order(na_last=True, kind=kind)\\n        true_base = self\\n        while true_base.base is not None:\\n            true_base = true_base.base\\n        if (true_base is not None and\\n            (true_base.ndim != 1 or true_base.shape != self.shape)):\\n            raise Exception('This Series is a view of some other array, to '\\n                            'sort in-place you must create a copy')\\n        self[:] = sortedSeries\\n        self.index = sortedSeries.index\\n    def sort_index(self, ascending=True):\\n        labels = self.index\\n        sort_index = labels.argsort()\\n        if not ascending:\\n            sort_index = sort_index[::-1]\\n        new_labels = labels.take(sort_index)\\n        new_values = self.values.take(sort_index)\\n        return Series(new_values, new_labels, name=self.name)"
  },
  {
    "code": "def append(self, other, ignore_index=False, verify_integrity=False, sort=None):\\n\\t\\tif isinstance(other, (Series, dict)):\\n\\t\\t\\tif isinstance(other, dict):\\n\\t\\t\\t\\tother = Series(other)\\n\\t\\t\\tif other.name is None and not ignore_index:\\n\\t\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\\t\"Can only append a Series if ignore_index=True\"\\n\\t\\t\\t\\t\\t\" or if the Series has a name\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tif other.name is None:\\n\\t\\t\\t\\tindex = None\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindex = Index([other.name], name=self.index.name)\\n\\t\\t\\tidx_diff = other.index.difference(self.columns)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcombined_columns = self.columns.append(idx_diff)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tcombined_columns = self.columns.astype(object).append(idx_diff)\\n\\t\\t\\tother = other.reindex(combined_columns, copy=False)\\n\\t\\t\\tother = DataFrame(\\n\\t\\t\\t\\tother.values.reshape((1, len(other))),\\n\\t\\t\\t\\tindex=index,\\n\\t\\t\\t\\tcolumns=combined_columns,\\n\\t\\t\\t)\\n\\t\\t\\tother = other._convert(datetime=True, timedelta=True)\\n\\t\\t\\tif not self.columns.equals(combined_columns):\\n\\t\\t\\t\\tself = self.reindex(columns=combined_columns)\\n\\t\\telif isinstance(other, list):\\n\\t\\t\\tif not other:\\n\\t\\t\\t\\tpass\\n\\t\\t\\telif not isinstance(other[0], DataFrame):\\n\\t\\t\\t\\tother = DataFrame(other)\\n\\t\\t\\t\\tif (self.columns.get_indexer(other.columns) >= 0).all():\\n\\t\\t\\t\\t\\tother = other.reindex(columns=self.columns)\\n\\t\\tfrom pandas.core.reshape.concat import concat\\n\\t\\tif isinstance(other, (list, tuple)):\\n\\t\\t\\tto_concat = [self] + other\\n\\t\\telse:\\n\\t\\t\\tto_concat = [self, other]\\n\\t\\treturn concat(\\n\\t\\t\\tto_concat,\\n\\t\\t\\tignore_index=ignore_index,\\n\\t\\t\\tverify_integrity=verify_integrity,\\n\\t\\t\\tsort=sort,\\n\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def append(self, other, ignore_index=False, verify_integrity=False, sort=None):\\n\\t\\tif isinstance(other, (Series, dict)):\\n\\t\\t\\tif isinstance(other, dict):\\n\\t\\t\\t\\tother = Series(other)\\n\\t\\t\\tif other.name is None and not ignore_index:\\n\\t\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\\t\"Can only append a Series if ignore_index=True\"\\n\\t\\t\\t\\t\\t\" or if the Series has a name\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tif other.name is None:\\n\\t\\t\\t\\tindex = None\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindex = Index([other.name], name=self.index.name)\\n\\t\\t\\tidx_diff = other.index.difference(self.columns)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcombined_columns = self.columns.append(idx_diff)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tcombined_columns = self.columns.astype(object).append(idx_diff)\\n\\t\\t\\tother = other.reindex(combined_columns, copy=False)\\n\\t\\t\\tother = DataFrame(\\n\\t\\t\\t\\tother.values.reshape((1, len(other))),\\n\\t\\t\\t\\tindex=index,\\n\\t\\t\\t\\tcolumns=combined_columns,\\n\\t\\t\\t)\\n\\t\\t\\tother = other._convert(datetime=True, timedelta=True)\\n\\t\\t\\tif not self.columns.equals(combined_columns):\\n\\t\\t\\t\\tself = self.reindex(columns=combined_columns)\\n\\t\\telif isinstance(other, list):\\n\\t\\t\\tif not other:\\n\\t\\t\\t\\tpass\\n\\t\\t\\telif not isinstance(other[0], DataFrame):\\n\\t\\t\\t\\tother = DataFrame(other)\\n\\t\\t\\t\\tif (self.columns.get_indexer(other.columns) >= 0).all():\\n\\t\\t\\t\\t\\tother = other.reindex(columns=self.columns)\\n\\t\\tfrom pandas.core.reshape.concat import concat\\n\\t\\tif isinstance(other, (list, tuple)):\\n\\t\\t\\tto_concat = [self] + other\\n\\t\\telse:\\n\\t\\t\\tto_concat = [self, other]\\n\\t\\treturn concat(\\n\\t\\t\\tto_concat,\\n\\t\\t\\tignore_index=ignore_index,\\n\\t\\t\\tverify_integrity=verify_integrity,\\n\\t\\t\\tsort=sort,\\n\\t\\t)"
  },
  {
    "code": "def mgr_locs(self) -> BlockPlacement:\\n        return self._mgr_locs\\n    @mgr_locs.setter\\n    def mgr_locs(self, new_mgr_locs: BlockPlacement):\\n        assert isinstance(new_mgr_locs, BlockPlacement)\\n        self._mgr_locs = new_mgr_locs\\n    @final",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mgr_locs(self) -> BlockPlacement:\\n        return self._mgr_locs\\n    @mgr_locs.setter\\n    def mgr_locs(self, new_mgr_locs: BlockPlacement):\\n        assert isinstance(new_mgr_locs, BlockPlacement)\\n        self._mgr_locs = new_mgr_locs\\n    @final"
  },
  {
    "code": "def as_ul_with_errors(self):\\n        \"Returns this form rendered as HTML <li>s, with errors.\"\\n        output = []\\n        if self.errors().get(NON_FIELD_ERRORS):\\n            output.append('<li><ul>%s</ul></li>' % '\\n'.join(['<li>%s</li>' % e for e in self.errors()[NON_FIELD_ERRORS]]))\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = '<li>'\\n            if bf.errors:\\n                line += '<ul>%s</ul>' % '\\n'.join(['<li>%s</li>' % e for e in bf.errors])\\n            line += '%s: %s</li>' % (pretty_name(name), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #3065 -- newforms: Fixed rendering problem with RadioSelect as a member of a Form. Also fixed some Unicode issues and added unit tests. Thanks for reporting, Derek Hoy",
    "fixed_code": "def as_ul_with_errors(self):\\n        \"Returns this form rendered as HTML <li>s, with errors.\"\\n        output = []\\n        if self.errors().get(NON_FIELD_ERRORS):\\n            output.append(u'<li><ul>%s</ul></li>' % u'\\n'.join([u'<li>%s</li>' % e for e in self.errors()[NON_FIELD_ERRORS]]))\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            line = u'<li>'\\n            if bf.errors:\\n                line += u'<ul>%s</ul>' % u'\\n'.join([u'<li>%s</li>' % e for e in bf.errors])\\n            line += u'%s: %s</li>' % (pretty_name(name), bf)\\n            output.append(line)\\n        return u'\\n'.join(output)"
  },
  {
    "code": "def _atleast2d_or_sparse(X, dtype, order, copy, sparse_class, convmethod):\\n    if sparse.issparse(X):\\n        if dtype is None or X.dtype == dtype:\\n            X = getattr(X, convmethod)()\\n        else:\\n            X = sparse_class(X, dtype=dtype)\\n        _assert_all_finite(X.data)\\n    else:\\n        X = array2d(X, dtype=dtype, order=order, copy=copy)\\n        _assert_all_finite(X)\\n    return X",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH order *does* matter for sparse matrices",
    "fixed_code": "def _atleast2d_or_sparse(X, dtype, order, copy, sparse_class, convmethod):\\n    if sparse.issparse(X):\\n        if dtype is None or X.dtype == dtype:\\n            X = getattr(X, convmethod)()\\n        else:\\n            X = sparse_class(X, dtype=dtype)\\n        _assert_all_finite(X.data)\\n        X.data = np.array(X.data, copy=False, order=order)\\n    else:\\n        X = array2d(X, dtype=dtype, order=order, copy=copy)\\n        _assert_all_finite(X)\\n    return X"
  },
  {
    "code": "def radius_neighbors_graph(\\n        self, X=None, radius=None, mode=\"connectivity\", sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if radius is None:\\n            radius = self.radius\\n        if mode == \"connectivity\":\\n            A_ind = self.radius_neighbors(X, radius, return_distance=False)\\n            A_data = None\\n        elif mode == \"distance\":\\n            dist, A_ind = self.radius_neighbors(\\n                X, radius, return_distance=True, sort_results=sort_results\\n            )\\n            A_data = np.concatenate(list(dist))\\n        else:\\n            raise ValueError(\\n                'Unsupported mode, must be one of \"connectivity\", '\\n                'or \"distance\" but got %s instead' % mode\\n            )\\n        n_queries = A_ind.shape[0]\\n        n_samples_fit = self.n_samples_fit_\\n        n_neighbors = np.array([len(a) for a in A_ind])\\n        A_ind = np.concatenate(list(A_ind))\\n        if A_data is None:\\n            A_data = np.ones(len(A_ind))\\n        A_indptr = np.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))\\n        return csr_matrix((A_data, A_ind, A_indptr), shape=(n_queries, n_samples_fit))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def radius_neighbors_graph(\\n        self, X=None, radius=None, mode=\"connectivity\", sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if radius is None:\\n            radius = self.radius\\n        if mode == \"connectivity\":\\n            A_ind = self.radius_neighbors(X, radius, return_distance=False)\\n            A_data = None\\n        elif mode == \"distance\":\\n            dist, A_ind = self.radius_neighbors(\\n                X, radius, return_distance=True, sort_results=sort_results\\n            )\\n            A_data = np.concatenate(list(dist))\\n        else:\\n            raise ValueError(\\n                'Unsupported mode, must be one of \"connectivity\", '\\n                'or \"distance\" but got %s instead' % mode\\n            )\\n        n_queries = A_ind.shape[0]\\n        n_samples_fit = self.n_samples_fit_\\n        n_neighbors = np.array([len(a) for a in A_ind])\\n        A_ind = np.concatenate(list(A_ind))\\n        if A_data is None:\\n            A_data = np.ones(len(A_ind))\\n        A_indptr = np.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))\\n        return csr_matrix((A_data, A_ind, A_indptr), shape=(n_queries, n_samples_fit))"
  },
  {
    "code": "def _copy(master_fd, master_read=_read, stdin_read=_read):\\n    while 1:\\n        rfds, wfds, xfds = select(\\n                [master_fd, STDIN_FILENO], [], [])\\n        if master_fd in rfds:\\n            data = master_read(master_fd)\\n            os.write(STDOUT_FILENO, data)\\n        if STDIN_FILENO in rfds:\\n            data = stdin_read(STDIN_FILENO)\\n            _writen(master_fd, data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #2489: Fix bug in _copy loop that could consume 100% cpu on EOF.",
    "fixed_code": "def _copy(master_fd, master_read=_read, stdin_read=_read):\\n    fds = [master_fd, STDIN_FILENO]\\n    while True:\\n        rfds, wfds, xfds = select(fds, [], [])\\n        if master_fd in rfds:\\n            data = master_read(master_fd)\\n            if not data:  \\n                fds.remove(master_fd)\\n            else:\\n                os.write(STDOUT_FILENO, data)\\n        if STDIN_FILENO in rfds:\\n            data = stdin_read(STDIN_FILENO)\\n            if not data:\\n                fds.remove(STDIN_FILENO)\\n            else:\\n                _writen(master_fd, data)"
  },
  {
    "code": "def is_subperiod(source, target):\\n    if isinstance(source, offsets.DateOffset):\\n        source = source.rule_code\\n    if isinstance(target, offsets.DateOffset):\\n        target = target.rule_code\\n    target = target.upper()\\n    source = source.upper()\\n    if _is_annual(target):\\n        if _is_quarterly(source):\\n            return _quarter_months_conform(_get_rule_month(source),\\n                                           _get_rule_month(target))\\n        return source in ['D', 'B', 'M', 'H', 'T', 'S']\\n    elif _is_quarterly(target):\\n        return source in ['D', 'B', 'M', 'H', 'T', 'S']\\n    elif target == 'M':\\n        return source in ['D', 'B', 'H', 'T', 'S']\\n    elif _is_weekly(target):\\n        return source in [target, 'D', 'B', 'H', 'T', 'S']\\n    elif target == 'B':\\n        return source in ['B', 'H', 'T', 'S']\\n    elif target == 'D':\\n        return source in ['D', 'H', 'T', 'S']\\n    elif target == 'H':\\n        return source in ['H', 'T', 'S']\\n    elif target == 'T':\\n        return source in ['T', 'S']\\n    elif target == 'S':\\n        return source in ['S']",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_subperiod(source, target):\\n    if isinstance(source, offsets.DateOffset):\\n        source = source.rule_code\\n    if isinstance(target, offsets.DateOffset):\\n        target = target.rule_code\\n    target = target.upper()\\n    source = source.upper()\\n    if _is_annual(target):\\n        if _is_quarterly(source):\\n            return _quarter_months_conform(_get_rule_month(source),\\n                                           _get_rule_month(target))\\n        return source in ['D', 'B', 'M', 'H', 'T', 'S']\\n    elif _is_quarterly(target):\\n        return source in ['D', 'B', 'M', 'H', 'T', 'S']\\n    elif target == 'M':\\n        return source in ['D', 'B', 'H', 'T', 'S']\\n    elif _is_weekly(target):\\n        return source in [target, 'D', 'B', 'H', 'T', 'S']\\n    elif target == 'B':\\n        return source in ['B', 'H', 'T', 'S']\\n    elif target == 'D':\\n        return source in ['D', 'H', 'T', 'S']\\n    elif target == 'H':\\n        return source in ['H', 'T', 'S']\\n    elif target == 'T':\\n        return source in ['T', 'S']\\n    elif target == 'S':\\n        return source in ['S']"
  },
  {
    "code": "def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\\tvalid_sortby = ['cpu_time', 'cuda_time', 'cpu_time_total', 'cuda_time_total', 'count']\\tif sortby not in valid_sortby:\\t\\twarn = ('WARNING: invalid sorting option for autograd profiler results: {}'\\t\\t\\t\\t'Expected `cpu_time`, `cpu_time_total`, or `count`. '\\t\\t\\t\\t'Defaulting to `cpu_time`.')\\t\\tprint(warn.format(autograd_prof_sortby))\\t\\tsortby = 'cpu_time'\\tif mode is 'CUDA':\\t\\tcuda_warning = ('\\tBecause the autograd profiler uses the CUDA event API,'\\t\\t\\t\\t\\t\\t'\\tthe CUDA time column reports approximately max(cuda_time, cpu_time).'\\t\\t\\t\\t\\t\\t'\\tPlease ignore this output if your code does not use CUDA.')\\telse:\\t\\tcuda_warning = ''\\tsorted_events = sorted(prof.function_events,\\t\\t\\t\\t\\t\\t   key=lambda x: getattr(x, sortby), reverse=True)\\ttopk_events = sorted_events[:topk]\\tresult = {\\t\\t'mode': mode,\\t\\t'description': 'top {} events sorted by {}'.format(topk, sortby),\\t\\t'output': torch.autograd.profiler.build_table(topk_events),\\t\\t'cuda_warning': cuda_warning\\t}\\tprint(autograd_prof_summary.format(**result))",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "Fix the flake8 linter\\n\\n\\nReviewed By: bddppq\\n\\nDifferential Revision: D13877435\\n\\nPulled By: houseroad\\n\\nfbshipit-source-id: dbe575ba3f6dd30d27ac6aa5eec2eea025063540",
    "fixed_code": "def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\\n\\tvalid_sortby = ['cpu_time', 'cuda_time', 'cpu_time_total', 'cuda_time_total', 'count']\\n\\tif sortby not in valid_sortby:\\n\\t\\twarn = ('WARNING: invalid sorting option for autograd profiler results: {}\\n'\\n\\t\\t\\t\\t'Expected `cpu_time`, `cpu_time_total`, or `count`. '\\n\\t\\t\\t\\t'Defaulting to `cpu_time`.')\\n\\t\\tprint(warn.format(autograd_prof_sortby))\\n\\t\\tsortby = 'cpu_time'\\n\\tif mode == 'CUDA':\\n\\t\\tcuda_warning = ('\\n\\tBecause the autograd profiler uses the CUDA event API,\\n'\\n\\t\\t\\t\\t\\t\\t'\\tthe CUDA time column reports approximately max(cuda_time, cpu_time).\\n'\\n\\t\\t\\t\\t\\t\\t'\\tPlease ignore this output if your code does not use CUDA.\\n')\\n\\telse:\\n\\t\\tcuda_warning = ''\\n\\tsorted_events = sorted(prof.function_events,\\n\\t\\t\\t\\t\\t\\t   key=lambda x: getattr(x, sortby), reverse=True)\\n\\ttopk_events = sorted_events[:topk]\\n\\tresult = {\\n\\t\\t'mode': mode,\\n\\t\\t'description': 'top {} events sorted by {}'.format(topk, sortby),\\n\\t\\t'output': torch.autograd.profiler.build_table(topk_events),\\n\\t\\t'cuda_warning': cuda_warning\\n\\t}\\n\\tprint(autograd_prof_summary.format(**result))"
  },
  {
    "code": "def _check_sql_mode(self, **kwargs):\\n        if not (self.connection.sql_mode & {'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES'}):\\n            return [checks.Warning(\\n                \"MySQL Strict Mode is not set for database connection '%s'\" % self.connection.alias,\\n                hint=\"MySQL's Strict Mode fixes many data integrity problems in MySQL, \"\\n                     \"such as data truncation upon insertion, by escalating warnings into \"\\n                     \"errors. It is strongly recommended you activate it. See: \"\\n                     \"https://docs.djangoproject.com/en/%s/ref/databases/\\n                     % (get_docs_version(),),\\n                id='mysql.W002',\\n            )]\\n        return []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #29548 -- Mentioned MariaDB in database system checks.",
    "fixed_code": "def _check_sql_mode(self, **kwargs):\\n        if not (self.connection.sql_mode & {'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES'}):\\n            return [checks.Warning(\\n                \"%s Strict Mode is not set for database connection '%s'\"\\n                % (self.connection.display_name, self.connection.alias),\\n                hint=(\\n                    \"%s's Strict Mode fixes many data integrity problems in \"\\n                    \"%s, such as data truncation upon insertion, by \"\\n                    \"escalating warnings into errors. It is strongly \"\\n                    \"recommended you activate it. See: \"\\n                    \"https://docs.djangoproject.com/en/%s/ref/databases/\\n                    % (\\n                        self.connection.display_name,\\n                        self.connection.display_name,\\n                        get_docs_version(),\\n                    ),\\n                ),\\n                id='mysql.W002',\\n            )]\\n        return []"
  },
  {
    "code": "def remove(self, rule_type, rule_control, rule_path):\\n\\t\\tcurrent_line = self._head\\n\\t\\tchanged = 0\\n\\t\\twhile current_line is not None:\\n\\t\\t\\tif current_line.matches(rule_type, rule_control, rule_path):\\n\\t\\t\\t\\tif current_line.prev is not None:\\n\\t\\t\\t\\t\\tcurrent_line.prev.next = current_line.next\\n\\t\\t\\t\\t\\tcurrent_line.next.prev = current_line.prev\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself._head = current_line.next\\n\\t\\t\\t\\t\\tcurrent_line.next.prev = None\\n\\t\\t\\t\\tchanged += 1\\n\\t\\t\\tcurrent_line = current_line.next\\n\\t\\treturn changed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "pamd: Fix AttributeError when removing the first or last rule (#66398)",
    "fixed_code": "def remove(self, rule_type, rule_control, rule_path):\\n\\t\\tcurrent_line = self._head\\n\\t\\tchanged = 0\\n\\t\\twhile current_line is not None:\\n\\t\\t\\tif current_line.matches(rule_type, rule_control, rule_path):\\n\\t\\t\\t\\tif current_line.prev is not None:\\n\\t\\t\\t\\t\\tcurrent_line.prev.next = current_line.next\\n\\t\\t\\t\\t\\tif current_line.next is not None:\\n\\t\\t\\t\\t\\t\\tcurrent_line.next.prev = current_line.prev\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself._head = current_line.next\\n\\t\\t\\t\\t\\tcurrent_line.next.prev = None\\n\\t\\t\\t\\tchanged += 1\\n\\t\\t\\tcurrent_line = current_line.next\\n\\t\\treturn changed"
  },
  {
    "code": "def user_change_password(self, request, id, form_url=''):\\n        if not self.has_change_permission(request):\\n            raise PermissionDenied\\n        user = get_object_or_404(self.get_queryset(request), pk=id)\\n        if request.method == 'POST':\\n            form = self.change_password_form(user, request.POST)\\n            if form.is_valid():\\n                form.save()\\n                change_message = self.construct_change_message(request, form, None)\\n                self.log_change(request, user, change_message)\\n                msg = ugettext('Password changed successfully.')\\n                messages.success(request, msg)\\n                update_session_auth_hash(request, form.user)\\n                return HttpResponseRedirect(\\n                    reverse('%s:auth_user_change' % self.admin_site.name, args=(user.pk,))\\n                )\\n        else:\\n            form = self.change_password_form(user)\\n        fieldsets = [(None, {'fields': list(form.base_fields)})]\\n        adminForm = admin.helpers.AdminForm(form, fieldsets, {})\\n        context = {\\n            'title': _('Change password: %s') % escape(user.get_username()),\\n            'adminForm': adminForm,\\n            'form_url': form_url,\\n            'form': form,\\n            'is_popup': (IS_POPUP_VAR in request.POST or\\n                         IS_POPUP_VAR in request.GET),\\n            'add': True,\\n            'change': False,\\n            'has_delete_permission': False,\\n            'has_change_permission': True,\\n            'has_absolute_url': False,\\n            'opts': self.model._meta,\\n            'original': user,\\n            'save_as': False,\\n            'show_save': True,\\n        }\\n        context.update(admin.site.each_context(request))\\n        request.current_app = self.admin_site.name\\n        return TemplateResponse(request,\\n            self.change_user_password_template or\\n            'admin/auth/user/change_password.html',\\n            context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24334 -- Allowed admin password reset to work with non-digit custom user model primary key.\\n\\nThanks Loic for help and Simon for review.",
    "fixed_code": "def user_change_password(self, request, id, form_url=''):\\n        if not self.has_change_permission(request):\\n            raise PermissionDenied\\n        user = self.get_object(request, unquote(id))\\n        if user is None:\\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {\\n                'name': force_text(self.model._meta.verbose_name),\\n                'key': escape(id),\\n            })\\n        if request.method == 'POST':\\n            form = self.change_password_form(user, request.POST)\\n            if form.is_valid():\\n                form.save()\\n                change_message = self.construct_change_message(request, form, None)\\n                self.log_change(request, user, change_message)\\n                msg = ugettext('Password changed successfully.')\\n                messages.success(request, msg)\\n                update_session_auth_hash(request, form.user)\\n                return HttpResponseRedirect(\\n                    reverse(\\n                        '%s:auth_%s_change' % (\\n                            self.admin_site.name,\\n                            user._meta.model_name,\\n                        ),\\n                        args=(user.pk,),\\n                    )\\n                )\\n        else:\\n            form = self.change_password_form(user)\\n        fieldsets = [(None, {'fields': list(form.base_fields)})]\\n        adminForm = admin.helpers.AdminForm(form, fieldsets, {})\\n        context = {\\n            'title': _('Change password: %s') % escape(user.get_username()),\\n            'adminForm': adminForm,\\n            'form_url': form_url,\\n            'form': form,\\n            'is_popup': (IS_POPUP_VAR in request.POST or\\n                         IS_POPUP_VAR in request.GET),\\n            'add': True,\\n            'change': False,\\n            'has_delete_permission': False,\\n            'has_change_permission': True,\\n            'has_absolute_url': False,\\n            'opts': self.model._meta,\\n            'original': user,\\n            'save_as': False,\\n            'show_save': True,\\n        }\\n        context.update(admin.site.each_context(request))\\n        request.current_app = self.admin_site.name\\n        return TemplateResponse(request,\\n            self.change_user_password_template or\\n            'admin/auth/user/change_password.html',\\n            context)"
  },
  {
    "code": "def __repr__(self):\\n\\t\\tret = \"MockCall(\"\\n\\t\\tfor arg in self.args:\\n\\t\\t\\tret += repr(arg) + \", \"\\n\\t\\tif not self.kwargs:\\n\\t\\t\\tif self.args:\\n\\t\\t\\t\\tret = ret[:-2]\\n\\t\\telse:\\n\\t\\t\\tfor key, val in self.kwargs.items():\\n\\t\\t\\t\\tret += \"{}={}\".format(salt.utils.stringutils.to_str(key), repr(val))\\n\\t\\tret += \")\"\\n\\t\\treturn ret",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix merged forwarded code and test",
    "fixed_code": "def __repr__(self):\\n\\t\\tret = \"MockCall(\"\\n\\t\\tfor arg in self.args:\\n\\t\\t\\tret += repr(arg) + \", \"\\n\\t\\tif not self.kwargs:\\n\\t\\t\\tif self.args:\\n\\t\\t\\t\\tret = ret[:-2]\\n\\t\\telse:\\n\\t\\t\\tfor key, val in self.kwargs.items():\\n\\t\\t\\t\\tret += f\"{salt.utils.stringutils.to_str(key)}={repr(val)}\"\\n\\t\\tret += \")\"\\n\\t\\treturn ret"
  },
  {
    "code": "def _proc_function(self, fun, low, user, tag, jid, daemonize=True):\\n\\t\\t''\\n\\t\\tif daemonize and not salt.utils.platform.is_windows():\\n\\t\\t\\tsalt.log.setup.shutdown_multiprocessing_logging()\\n\\t\\t\\tsalt.utils.process.daemonize()\\n\\t\\t\\tsalt.log.setup.setup_multiprocessing_logging()\\n\\t\\tlow['__jid__'] = jid\\n\\t\\tlow['__user__'] = user\\n\\t\\tlow['__tag__'] = tag\\n\\t\\ttry:\\n\\t\\t\\treturn self.cmd_sync(low)\\n\\t\\texcept salt.exceptions.EauthAuthenticationError as exc:\\n\\t\\t\\tlog.error(exc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _proc_function(self, fun, low, user, tag, jid, daemonize=True):\\n\\t\\t''\\n\\t\\tif daemonize and not salt.utils.platform.is_windows():\\n\\t\\t\\tsalt.log.setup.shutdown_multiprocessing_logging()\\n\\t\\t\\tsalt.utils.process.daemonize()\\n\\t\\t\\tsalt.log.setup.setup_multiprocessing_logging()\\n\\t\\tlow['__jid__'] = jid\\n\\t\\tlow['__user__'] = user\\n\\t\\tlow['__tag__'] = tag\\n\\t\\ttry:\\n\\t\\t\\treturn self.cmd_sync(low)\\n\\t\\texcept salt.exceptions.EauthAuthenticationError as exc:\\n\\t\\t\\tlog.error(exc)"
  },
  {
    "code": "def _reduce(self, op, axis=0, skipna=True, numeric_only=None,\\n                filter_type=None, **kwds):\\n        f = lambda x: op(x, axis=axis, skipna=skipna, **kwds)\\n        labels = self._get_agg_axis(axis)\\n        if numeric_only is None:\\n            try:\\n                values = self.values\\n                result = f(values)\\n            except Exception:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                result = f(data.values)\\n                labels = data._get_agg_axis(axis)\\n        else:\\n            if numeric_only:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                values = data.values\\n                labels = data._get_agg_axis(axis)\\n            else:\\n                values = self.values\\n            result = f(values)\\n        if result.dtype == np.object_:\\n            try:\\n                if filter_type is None or filter_type == 'numeric':\\n                    result = result.astype(np.float64)\\n                elif filter_type == 'bool' and notnull(result).all():\\n                    result = result.astype(np.bool_)\\n            except (ValueError, TypeError):\\n                pass\\n        return Series(result, index=labels)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fixed issues where passing an axis of 'index' would fail (as its expecting an axis number)",
    "fixed_code": "def _reduce(self, op, axis=0, skipna=True, numeric_only=None,\\n                filter_type=None, **kwds):\\n        axis = self._get_axis_number(axis)\\n        f = lambda x: op(x, axis=axis, skipna=skipna, **kwds)\\n        labels = self._get_agg_axis(axis)\\n        if numeric_only is None:\\n            try:\\n                values = self.values\\n                result = f(values)\\n            except Exception:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                result = f(data.values)\\n                labels = data._get_agg_axis(axis)\\n        else:\\n            if numeric_only:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                values = data.values\\n                labels = data._get_agg_axis(axis)\\n            else:\\n                values = self.values\\n            result = f(values)\\n        if result.dtype == np.object_:\\n            try:\\n                if filter_type is None or filter_type == 'numeric':\\n                    result = result.astype(np.float64)\\n                elif filter_type == 'bool' and notnull(result).all():\\n                    result = result.astype(np.bool_)\\n            except (ValueError, TypeError):\\n                pass\\n        return Series(result, index=labels)"
  },
  {
    "code": "def splituser(host):\\n    global _userprog\\n    if _userprog is None:\\n        import re\\n        _userprog = re.compile('^(.*)@(.*)$')\\n    match = _userprog.match(host)\\n    if match: return map(unquote, match.group(1, 2))\\n    return None, host\\n_passwdprog = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix Issue2244 - urllib unquotes user and password info multiple times - Patch by Theodore Turocy",
    "fixed_code": "def splituser(host):\\n    global _userprog\\n    if _userprog is None:\\n        import re\\n        _userprog = re.compile('^(.*)@(.*)$')\\n    match = _userprog.match(host)\\n    if match: return match.group(1, 2)\\n    return None, host\\n_passwdprog = None"
  },
  {
    "code": "def transform_str_or_callable(\\n    obj: FrameOrSeries, func: AggFuncTypeBase, *args, **kwargs\\n) -> FrameOrSeriesUnion:\\n    if isinstance(func, str):\\n        return obj._try_aggregate_string_function(func, *args, **kwargs)\\n    if not args and not kwargs:\\n        f = obj._get_cython_func(func)\\n        if f:\\n            return getattr(obj, f)()\\n    try:\\n        return obj.apply(func, args=args, **kwargs)\\n    except Exception:\\n        return func(obj, *args, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def transform_str_or_callable(\\n    obj: FrameOrSeries, func: AggFuncTypeBase, *args, **kwargs\\n) -> FrameOrSeriesUnion:\\n    if isinstance(func, str):\\n        return obj._try_aggregate_string_function(func, *args, **kwargs)\\n    if not args and not kwargs:\\n        f = obj._get_cython_func(func)\\n        if f:\\n            return getattr(obj, f)()\\n    try:\\n        return obj.apply(func, args=args, **kwargs)\\n    except Exception:\\n        return func(obj, *args, **kwargs)"
  },
  {
    "code": "def get_action_args_with_defaults(action, args, defaults, templar, redirected_names=None):\\n    group_collection_map = {\\n        'acme': ['community.crypto'],\\n        'aws': ['amazon.aws', 'community.aws'],\\n        'azure': ['azure.azcollection'],\\n        'cpm': ['wti.remote'],\\n        'docker': ['community.general', 'community.docker'],\\n        'gcp': ['google.cloud'],\\n        'k8s': ['community.kubernetes', 'community.general', 'community.kubevirt', 'community.okd', 'kubernetes.core'],\\n        'os': ['openstack.cloud'],\\n        'ovirt': ['ovirt.ovirt', 'community.general'],\\n        'vmware': ['community.vmware'],\\n        'testgroup': ['testns.testcoll', 'testns.othercoll', 'testns.boguscoll']\\n    }\\n    if not redirected_names:\\n        redirected_names = [action]\\n    tmp_args = {}\\n    module_defaults = {}\\n    if isinstance(defaults, list):\\n        for default in defaults:\\n            module_defaults.update(default)\\n    if module_defaults:\\n        module_defaults = templar.template(module_defaults)\\n        for default in module_defaults:\\n            if not default.startswith('group/'):\\n                continue\\n            group_name = default.split('group/')[-1]\\n            for collection_name in group_collection_map.get(group_name, []):\\n                try:\\n                    action_group = _get_collection_metadata(collection_name).get('action_groups', {})\\n                except ValueError:\\n                    continue\\n                if any(name for name in redirected_names if name in action_group):\\n                    tmp_args.update((module_defaults.get('group/%s' % group_name) or {}).copy())\\n        for redirected_action in redirected_names:\\n            legacy = None\\n            if redirected_action.startswith('ansible.legacy.') and action == redirected_action:\\n                legacy = redirected_action.split('ansible.legacy.')[-1]\\n            if legacy and legacy in module_defaults:\\n                tmp_args.update(module_defaults[legacy].copy())\\n            if redirected_action in module_defaults:\\n                tmp_args.update(module_defaults[redirected_action].copy())\\n    tmp_args.update(args)\\n    return tmp_args",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_action_args_with_defaults(action, args, defaults, templar, redirected_names=None):\\n    group_collection_map = {\\n        'acme': ['community.crypto'],\\n        'aws': ['amazon.aws', 'community.aws'],\\n        'azure': ['azure.azcollection'],\\n        'cpm': ['wti.remote'],\\n        'docker': ['community.general', 'community.docker'],\\n        'gcp': ['google.cloud'],\\n        'k8s': ['community.kubernetes', 'community.general', 'community.kubevirt', 'community.okd', 'kubernetes.core'],\\n        'os': ['openstack.cloud'],\\n        'ovirt': ['ovirt.ovirt', 'community.general'],\\n        'vmware': ['community.vmware'],\\n        'testgroup': ['testns.testcoll', 'testns.othercoll', 'testns.boguscoll']\\n    }\\n    if not redirected_names:\\n        redirected_names = [action]\\n    tmp_args = {}\\n    module_defaults = {}\\n    if isinstance(defaults, list):\\n        for default in defaults:\\n            module_defaults.update(default)\\n    if module_defaults:\\n        module_defaults = templar.template(module_defaults)\\n        for default in module_defaults:\\n            if not default.startswith('group/'):\\n                continue\\n            group_name = default.split('group/')[-1]\\n            for collection_name in group_collection_map.get(group_name, []):\\n                try:\\n                    action_group = _get_collection_metadata(collection_name).get('action_groups', {})\\n                except ValueError:\\n                    continue\\n                if any(name for name in redirected_names if name in action_group):\\n                    tmp_args.update((module_defaults.get('group/%s' % group_name) or {}).copy())\\n        for redirected_action in redirected_names:\\n            legacy = None\\n            if redirected_action.startswith('ansible.legacy.') and action == redirected_action:\\n                legacy = redirected_action.split('ansible.legacy.')[-1]\\n            if legacy and legacy in module_defaults:\\n                tmp_args.update(module_defaults[legacy].copy())\\n            if redirected_action in module_defaults:\\n                tmp_args.update(module_defaults[redirected_action].copy())\\n    tmp_args.update(args)\\n    return tmp_args"
  },
  {
    "code": "def _get_combined_index(\\n    indexes: list[Index],\\n    intersect: bool = False,\\n    sort: bool = False,\\n    copy: bool = False,\\n) -> Index:\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = union_indexes(indexes, sort=False)\\n        index = ensure_index(index)\\n    if sort and not index.is_monotonic_increasing:\\n        index = safe_sort_index(index)\\n    if copy:\\n        index = index.copy()\\n    return index",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_combined_index(\\n    indexes: list[Index],\\n    intersect: bool = False,\\n    sort: bool = False,\\n    copy: bool = False,\\n) -> Index:\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = union_indexes(indexes, sort=False)\\n        index = ensure_index(index)\\n    if sort and not index.is_monotonic_increasing:\\n        index = safe_sort_index(index)\\n    if copy:\\n        index = index.copy()\\n    return index"
  },
  {
    "code": "def euclidean_distances_argmin(X, Y=None, axis=1,\\n                               chunk_x_size=None, chunk_y_size=None,\\n                              return_values=False):\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        chunk_x_size, chunk_y_size = chunk_y_size, chunk_x_size\\n    if chunk_x_size is None and chunk_y_size is None:\\n        chunk_num = 50\\n        if X.shape[0] >= Y.shape[0] / 2 and Y.shape[0] >= X.shape[0] / 2:\\n            chunk_x_size = max(int(X.shape[0] / 7), 1)\\n            chunk_y_size = max(int(Y.shape[0] / 7), 1)\\n        elif X.shape[0] > Y.shape[0]:\\n            chunk_x_size = max(int(X.shape[0] / chunk_num), 1)\\n            chunk_y_size = Y.shape[0]\\n        else:\\n            chunk_x_size = X.shape[0]\\n            chunk_y_size = max(int(Y.shape[0] / chunk_num), 1)\\n    if chunk_x_size is None or chunk_x_size <= 0:\\n        chunk_x_size = X.shape[0]\\n    if chunk_y_size is None or chunk_y_size <= 0:\\n        chunk_y_size = Y.shape[0]\\n    n_chunks_x = (X.shape[0] - 1) // chunk_x_size + 1\\n    n_chunks_y = (Y.shape[0] - 1) // chunk_y_size + 1\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(float('inf'))\\n    for chunk_x in range(n_chunks_x):\\n        x_sind = chunk_x * chunk_x_size\\n        x_eind = min((chunk_x + 1) * chunk_x_size, X.shape[0])\\n        X_chunk = X[x_sind:x_eind, :]\\n        for chunk_y in range(n_chunks_y):\\n            y_sind = chunk_y * chunk_y_size\\n            y_eind = (chunk_y + 1) * chunk_y_size\\n            Y_chunk = Y[y_sind:y_eind, :]\\n            tvar = np.dot(X_chunk, Y_chunk.T)\\n            tvar *= -2\\n            tvar += (X_chunk * X_chunk).sum(axis=1)[:, np.newaxis]\\n            tvar += (Y_chunk * Y_chunk).sum(axis=1)[np.newaxis, :]\\n            np.maximum(tvar, 0, tvar)\\n            min_indices = tvar.argmin(axis=1)\\n            min_values = tvar[range(x_eind - x_sind), min_indices]\\n            flags = values[x_sind:x_eind] > min_values\\n            indices[x_sind:x_eind] = np.where(\\n                flags, min_indices + y_sind, indices[x_sind:x_eind])\\n            values[x_sind:x_eind] = np.where(\\n                flags, min_values, values[x_sind:x_eind])\\n    if return_values:\\n        return indices, values\\n    else:\\n        return indices",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Wrote docstring for euclidean_distances_argmin",
    "fixed_code": "def euclidean_distances_argmin(X, Y=None, axis=1,\\n                               chunk_x_size=None, chunk_y_size=None,\\n                               return_distances=False, squared=False):\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        chunk_x_size, chunk_y_size = chunk_y_size, chunk_x_size\\n    if chunk_x_size is None and chunk_y_size is None:\\n        chunk_num = 50\\n        if X.shape[0] >= Y.shape[0] / 2 and Y.shape[0] >= X.shape[0] / 2:\\n            chunk_x_size = max(int(X.shape[0] / 7), 1)\\n            chunk_y_size = max(int(Y.shape[0] / 7), 1)\\n        elif X.shape[0] > Y.shape[0]:\\n            chunk_x_size = max(int(X.shape[0] / chunk_num), 1)\\n            chunk_y_size = Y.shape[0]\\n        else:\\n            chunk_x_size = X.shape[0]\\n            chunk_y_size = max(int(Y.shape[0] / chunk_num), 1)\\n    if chunk_x_size is None or chunk_x_size <= 0:\\n        chunk_x_size = X.shape[0]\\n    if chunk_y_size is None or chunk_y_size <= 0:\\n        chunk_y_size = Y.shape[0]\\n    n_chunks_x = (X.shape[0] - 1) // chunk_x_size + 1\\n    n_chunks_y = (Y.shape[0] - 1) // chunk_y_size + 1\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(float('inf'))\\n    for chunk_x in range(n_chunks_x):\\n        x_sind = chunk_x * chunk_x_size\\n        x_eind = min((chunk_x + 1) * chunk_x_size, X.shape[0])\\n        X_chunk = X[x_sind:x_eind, :]\\n        for chunk_y in range(n_chunks_y):\\n            y_sind = chunk_y * chunk_y_size\\n            y_eind = (chunk_y + 1) * chunk_y_size\\n            Y_chunk = Y[y_sind:y_eind, :]\\n            tvar = np.dot(X_chunk, Y_chunk.T)\\n            tvar *= -2\\n            tvar += (X_chunk * X_chunk).sum(axis=1)[:, np.newaxis]\\n            tvar += (Y_chunk * Y_chunk).sum(axis=1)[np.newaxis, :]\\n            np.maximum(tvar, 0, tvar)\\n            min_indices = tvar.argmin(axis=1)\\n            min_values = tvar[range(x_eind - x_sind), min_indices]\\n            flags = values[x_sind:x_eind] > min_values\\n            indices[x_sind:x_eind] = np.where(\\n                flags, min_indices + y_sind, indices[x_sind:x_eind])\\n            values[x_sind:x_eind] = np.where(\\n                flags, min_values, values[x_sind:x_eind])\\n    if return_distances:\\n        if not squared:\\n            values = np.sqrt(values)\\n        return indices, values\\n    else:\\n        return indices"
  },
  {
    "code": "def request(self, method, url, body=None, headers=None):\\n\\t\\tif headers is None:\\n\\t\\t\\theaders = {}\\n\\t\\tself.req = webob.Request.blank(url, method=method, headers=headers)\\n\\t\\tself.req.body = body",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def request(self, method, url, body=None, headers=None):\\n\\t\\tif headers is None:\\n\\t\\t\\theaders = {}\\n\\t\\tself.req = webob.Request.blank(url, method=method, headers=headers)\\n\\t\\tself.req.body = body"
  },
  {
    "code": "def _bar_plot(self, axes, subplots=False, use_index=True, grid=True,\\n                  rot=30, legend=True, ax=None, fontsize=None, **kwds):\\n        import pandas.tools.plotting as gfx\\n        N, K = self.shape\\n        xinds = np.arange(N) + 0.25\\n        colors = 'rgbyk'\\n        rects = []\\n        labels = []\\n        if not subplots and ax is None:\\n            ax = axes[0]\\n        for i, col in enumerate(self.columns):\\n            empty = self[col].count() == 0\\n            y = self[col].values if not empty else np.zeros(len(self))\\n            if subplots:\\n                ax = axes[i]\\n                ax.bar(xinds, y, 0.5,\\n                       bottom=np.zeros(N), linewidth=1, **kwds)\\n                ax.set_title(col)\\n            else:\\n                rects.append(ax.bar(xinds + i * 0.75 / K, y, 0.75 / K,\\n                                    bottom=np.zeros(N), label=col,\\n                                    color=colors[i % len(colors)], **kwds))\\n                labels.append(col)\\n        if fontsize is None:\\n            if N < 10:\\n                fontsize = 12\\n            else:\\n                fontsize = 10\\n        ax.set_xlim([xinds[0] - 1, xinds[-1] + 1])\\n        ax.set_xticks(xinds + 0.375)\\n        ax.set_xticklabels([gfx._stringify(key) for key in self.index],\\n                           rotation=rot,\\n                           fontsize=fontsize)\\n        if legend and not subplots:\\n            fig = ax.get_figure()\\n            fig.legend([r[0] for r in rects], labels, loc='upper center',\\n                       fancybox=True, ncol=6)\\n        import matplotlib.pyplot as plt\\n        plt.subplots_adjust(top=0.8)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _bar_plot(self, axes, subplots=False, use_index=True, grid=True,\\n                  rot=30, legend=True, ax=None, fontsize=None, **kwds):\\n        import pandas.tools.plotting as gfx\\n        N, K = self.shape\\n        xinds = np.arange(N) + 0.25\\n        colors = 'rgbyk'\\n        rects = []\\n        labels = []\\n        if not subplots and ax is None:\\n            ax = axes[0]\\n        for i, col in enumerate(self.columns):\\n            empty = self[col].count() == 0\\n            y = self[col].values if not empty else np.zeros(len(self))\\n            if subplots:\\n                ax = axes[i]\\n                ax.bar(xinds, y, 0.5,\\n                       bottom=np.zeros(N), linewidth=1, **kwds)\\n                ax.set_title(col)\\n            else:\\n                rects.append(ax.bar(xinds + i * 0.75 / K, y, 0.75 / K,\\n                                    bottom=np.zeros(N), label=col,\\n                                    color=colors[i % len(colors)], **kwds))\\n                labels.append(col)\\n        if fontsize is None:\\n            if N < 10:\\n                fontsize = 12\\n            else:\\n                fontsize = 10\\n        ax.set_xlim([xinds[0] - 1, xinds[-1] + 1])\\n        ax.set_xticks(xinds + 0.375)\\n        ax.set_xticklabels([gfx._stringify(key) for key in self.index],\\n                           rotation=rot,\\n                           fontsize=fontsize)\\n        if legend and not subplots:\\n            fig = ax.get_figure()\\n            fig.legend([r[0] for r in rects], labels, loc='upper center',\\n                       fancybox=True, ncol=6)\\n        import matplotlib.pyplot as plt\\n        plt.subplots_adjust(top=0.8)"
  },
  {
    "code": "def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\\n\\t\\t\\t  encoding='utf-8', errors='replace', max_num_fields=None):\\n\\tqs, _coerce_result = _coerce_args(qs)\\n\\tif max_num_fields is not None:\\n\\t\\tnum_fields = 1 + qs.count('&') + qs.count(';')\\n\\t\\tif max_num_fields < num_fields:\\n\\t\\t\\traise ValueError('Max number of fields exceeded')\\n\\tpairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\\n\\tr = []\\n\\tfor name_value in pairs:\\n\\t\\tif not name_value and not strict_parsing:\\n\\t\\t\\tcontinue\\n\\t\\tnv = name_value.split('=', 1)\\n\\t\\tif len(nv) != 2:\\n\\t\\t\\tif strict_parsing:\\n\\t\\t\\t\\traise ValueError(\"bad query field: %r\" % (name_value,))\\n\\t\\t\\tif keep_blank_values:\\n\\t\\t\\t\\tnv.append('')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontinue\\n\\t\\tif len(nv[1]) or keep_blank_values:\\n\\t\\t\\tname = nv[0].replace('+', ' ')\\n\\t\\t\\tname = unquote(name, encoding=encoding, errors=errors)\\n\\t\\t\\tname = _coerce_result(name)\\n\\t\\t\\tvalue = nv[1].replace('+', ' ')\\n\\t\\t\\tvalue = unquote(value, encoding=encoding, errors=errors)\\n\\t\\t\\tvalue = _coerce_result(value)\\n\\t\\t\\tr.append((name, value))\\n\\treturn r",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-42967: only use '&' as a query string separator (#24297)\\n\\nbpo-42967: [security] Address a web cache-poisoning issue reported in urllib.parse.parse_qsl().\\n\\nurllib.parse will only us \"&\" as query string separator by default instead of both \";\" and \"&\" as allowed in earlier versions. An optional argument seperator with default value \"&\" is added to specify the separator.",
    "fixed_code": "def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\\n\\t\\t\\t  encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\\n\\tqs, _coerce_result = _coerce_args(qs)\\n\\tif not separator or (not isinstance(separator, str)\\n\\t\\tand not isinstance(separator, bytes)):\\n\\t\\traise ValueError(\"Separator must be of type string or bytes.\")\\n\\tif max_num_fields is not None:\\n\\t\\tnum_fields = 1 + qs.count(separator)\\n\\t\\tif max_num_fields < num_fields:\\n\\t\\t\\traise ValueError('Max number of fields exceeded')\\n\\tpairs = [s1 for s1 in qs.split(separator)]\\n\\tr = []\\n\\tfor name_value in pairs:\\n\\t\\tif not name_value and not strict_parsing:\\n\\t\\t\\tcontinue\\n\\t\\tnv = name_value.split('=', 1)\\n\\t\\tif len(nv) != 2:\\n\\t\\t\\tif strict_parsing:\\n\\t\\t\\t\\traise ValueError(\"bad query field: %r\" % (name_value,))\\n\\t\\t\\tif keep_blank_values:\\n\\t\\t\\t\\tnv.append('')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontinue\\n\\t\\tif len(nv[1]) or keep_blank_values:\\n\\t\\t\\tname = nv[0].replace('+', ' ')\\n\\t\\t\\tname = unquote(name, encoding=encoding, errors=errors)\\n\\t\\t\\tname = _coerce_result(name)\\n\\t\\t\\tvalue = nv[1].replace('+', ' ')\\n\\t\\t\\tvalue = unquote(value, encoding=encoding, errors=errors)\\n\\t\\t\\tvalue = _coerce_result(value)\\n\\t\\t\\tr.append((name, value))\\n\\treturn r"
  },
  {
    "code": "def uquery(sql, con=None, cur=None, retry=True, params=None):\\n    cur = execute(sql, con, cur=cur, retry=retry, params=params)\\n    result = cur.rowcount\\n    try:\\n        con.commit()\\n    except Exception as e:\\n        excName = e.__class__.__name__\\n        if excName != 'OperationalError':\\n            raise\\n        traceback.print_exc()\\n        if retry:\\n            print('Looks like your connection failed, reconnecting...')\\n            return uquery(sql, con, retry=False)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH #4163 Use SQLAlchemy for DB abstraction\\n\\nTST Import sqlalchemy on Travis.\\n\\nDOC add docstrings to read sql\\n\\nENH read_sql connects via Connection, Engine, file path, or :memory: string\\n\\nCLN Separate legacy code into new file, and fallback so that all old tests pass.\\n\\nTST to use sqlachemy syntax in tests\\n\\nCLN sql into classes, legacy passes\\n\\nFIX few engine vs con calls\\n\\nCLN pep8 cleanup\\n\\nadd postgres support for pandas.io.sql.get_schema\\n\\nWIP: cleaup of sql io module - imported correct SQLALCHEMY type, delete redundant PandasSQLWithCon\\n\\nTODO: renamed _engine_read_table, need to think of a better name.\\nTODO: clean up get_conneciton function\\n\\nENH: cleanup of SQL io\\n\\nTODO: check that legacy mode works\\nTODO: run tests\\n\\ncorrectly enabled coerce_float option\\n\\nCleanup and bug-fixing mainly on legacy mode sql.\\nIMPORTANT - changed legacy to require connection rather than cursor. This is still not yet finalized.\\nTODO: tests and doc\\n\\nAdded Test coverage for basic functionality using in-memory SQLite database\\n\\nSimplified API by automatically distinguishing between engine and connection. Added warnings",
    "fixed_code": "def uquery(sql, con, cur=None, params=[], engine=None, flavor='sqlite'):\\n    pandas_sql = pandasSQL_builder(con=con, flavor=flavor)\\n    args = _convert_params(sql, params)\\n    return pandas_sql.uquery(*args)"
  },
  {
    "code": "def push_from_local(self, destination_filepath: str, local_filepath: str):\\n        with open(local_filepath, \"rb\") as f, self.open_file(destination_filepath, mode=\"w\") as g:\\n            copyfileobj(f, g)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def push_from_local(self, destination_filepath: str, local_filepath: str):\\n        with open(local_filepath, \"rb\") as f, self.open_file(destination_filepath, mode=\"w\") as g:\\n            copyfileobj(f, g)"
  },
  {
    "code": "def get_avatar_urls_uncached(self, user, size):\\n\\t\\tif user.email:\\n\\t\\t\\treturn {\\n\\t\\t\\t\\t'%dx' % resolution: mark_safe(get_gravatar_url_for_email(\\n\\t\\t\\t\\t\\temail=user.email, size=(size * resolution)))\\n\\t\\t\\t\\tfor resolution in (1, 2, 3)\\n\\t\\t\\t}\\n\\t\\telse:\\n\\t\\t\\treturn {}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_avatar_urls_uncached(self, user, size):\\n\\t\\tif user.email:\\n\\t\\t\\treturn {\\n\\t\\t\\t\\t'%dx' % resolution: mark_safe(get_gravatar_url_for_email(\\n\\t\\t\\t\\t\\temail=user.email, size=(size * resolution)))\\n\\t\\t\\t\\tfor resolution in (1, 2, 3)\\n\\t\\t\\t}\\n\\t\\telse:\\n\\t\\t\\treturn {}"
  },
  {
    "code": "def read_excel(\\n    io,\\n    sheet_name=0,\\n    header=0,\\n    names=None,\\n    index_col=None,\\n    usecols=None,\\n    squeeze=False,\\n    dtype=None,\\n    engine=None,\\n    converters=None,\\n    true_values=None,\\n    false_values=None,\\n    skiprows=None,\\n    nrows=None,\\n    na_values=None,\\n    keep_default_na=True,\\n    verbose=False,\\n    parse_dates=False,\\n    date_parser=None,\\n    thousands=None,\\n    comment=None,\\n    skipfooter=0,\\n    convert_float=True,\\n    mangle_dupe_cols=True,\\n    **kwds,\\n):\\n    for arg in (\"sheet\", \"sheetname\", \"parse_cols\"):\\n        if arg in kwds:\\n            raise TypeError(f\"read_excel() got an unexpected keyword argument `{arg}`\")\\n    if not isinstance(io, ExcelFile):\\n        io = ExcelFile(io, engine=engine)\\n    elif engine and engine != io.engine:\\n        raise ValueError(\\n            \"Engine should not be specified when passing \"\\n            \"an ExcelFile - ExcelFile already has the engine set\"\\n        )\\n    return io.parse(\\n        sheet_name=sheet_name,\\n        header=header,\\n        names=names,\\n        index_col=index_col,\\n        usecols=usecols,\\n        squeeze=squeeze,\\n        dtype=dtype,\\n        converters=converters,\\n        true_values=true_values,\\n        false_values=false_values,\\n        skiprows=skiprows,\\n        nrows=nrows,\\n        na_values=na_values,\\n        keep_default_na=keep_default_na,\\n        verbose=verbose,\\n        parse_dates=parse_dates,\\n        date_parser=date_parser,\\n        thousands=thousands,\\n        comment=comment,\\n        skipfooter=skipfooter,\\n        convert_float=convert_float,\\n        mangle_dupe_cols=mangle_dupe_cols,\\n        **kwds,\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_excel(\\n    io,\\n    sheet_name=0,\\n    header=0,\\n    names=None,\\n    index_col=None,\\n    usecols=None,\\n    squeeze=False,\\n    dtype=None,\\n    engine=None,\\n    converters=None,\\n    true_values=None,\\n    false_values=None,\\n    skiprows=None,\\n    nrows=None,\\n    na_values=None,\\n    keep_default_na=True,\\n    verbose=False,\\n    parse_dates=False,\\n    date_parser=None,\\n    thousands=None,\\n    comment=None,\\n    skipfooter=0,\\n    convert_float=True,\\n    mangle_dupe_cols=True,\\n    **kwds,\\n):\\n    for arg in (\"sheet\", \"sheetname\", \"parse_cols\"):\\n        if arg in kwds:\\n            raise TypeError(f\"read_excel() got an unexpected keyword argument `{arg}`\")\\n    if not isinstance(io, ExcelFile):\\n        io = ExcelFile(io, engine=engine)\\n    elif engine and engine != io.engine:\\n        raise ValueError(\\n            \"Engine should not be specified when passing \"\\n            \"an ExcelFile - ExcelFile already has the engine set\"\\n        )\\n    return io.parse(\\n        sheet_name=sheet_name,\\n        header=header,\\n        names=names,\\n        index_col=index_col,\\n        usecols=usecols,\\n        squeeze=squeeze,\\n        dtype=dtype,\\n        converters=converters,\\n        true_values=true_values,\\n        false_values=false_values,\\n        skiprows=skiprows,\\n        nrows=nrows,\\n        na_values=na_values,\\n        keep_default_na=keep_default_na,\\n        verbose=verbose,\\n        parse_dates=parse_dates,\\n        date_parser=date_parser,\\n        thousands=thousands,\\n        comment=comment,\\n        skipfooter=skipfooter,\\n        convert_float=convert_float,\\n        mangle_dupe_cols=mangle_dupe_cols,\\n        **kwds,\\n    )"
  },
  {
    "code": "def is_in_obj(gpr) -> bool:\\n\\t\\tif not hasattr(gpr, \"name\"):\\n\\t\\t\\treturn False\\n\\t\\ttry:\\n\\t\\t\\treturn gpr is obj[gpr.name]\\n\\t\\texcept (KeyError, IndexError):\\n\\t\\t\\treturn False\\n\\tfor i, (gpr, level) in enumerate(zip(keys, levels)):\\n\\t\\tif is_in_obj(gpr):  \\n\\t\\t\\tin_axis, name = True, gpr.name\\n\\t\\t\\texclusions.append(name)\\n\\t\\telif is_in_axis(gpr):  \\n\\t\\t\\tif gpr in obj:\\n\\t\\t\\t\\tif validate:\\n\\t\\t\\t\\t\\tobj._check_label_or_level_ambiguity(gpr, axis=axis)\\n\\t\\t\\t\\tin_axis, name, gpr = True, gpr, obj[gpr]\\n\\t\\t\\t\\texclusions.append(name)\\n\\t\\t\\telif obj._is_level_reference(gpr, axis=axis):\\n\\t\\t\\t\\tin_axis, name, level, gpr = False, None, gpr, None\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(gpr)\\n\\t\\telif isinstance(gpr, Grouper) and gpr.key is not None:\\n\\t\\t\\texclusions.append(gpr.key)\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\telse:\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\tif is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\tf\"Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) \"\\n\\t\\t\\t\\t\"must be same length\"\\n\\t\\t\\t)\\n\\t\\tping = (\\n\\t\\t\\tGrouping(\\n\\t\\t\\t\\tgroup_axis,\\n\\t\\t\\t\\tgpr,\\n\\t\\t\\t\\tobj=obj,\\n\\t\\t\\t\\tname=name,\\n\\t\\t\\t\\tlevel=level,\\n\\t\\t\\t\\tsort=sort,\\n\\t\\t\\t\\tobserved=observed,\\n\\t\\t\\t\\tin_axis=in_axis,\\n\\t\\t\\t\\tdropna=dropna,\\n\\t\\t\\t)\\n\\t\\t\\tif not isinstance(gpr, Grouping)\\n\\t\\t\\telse gpr\\n\\t\\t)\\n\\t\\tgroupings.append(ping)\\n\\tif len(groupings) == 0 and len(obj):\\n\\t\\traise ValueError(\"No group keys passed!\")\\n\\telif len(groupings) == 0:\\n\\t\\tgroupings.append(Grouping(Index([], dtype=\"int\"), np.array([], dtype=np.intp)))\\n\\tgrouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)\\n\\treturn grouper, exclusions, obj",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Bug in Series.groupby would raise ValueError when grouping by PeriodIndex level (#34049)",
    "fixed_code": "def is_in_obj(gpr) -> bool:\\n\\t\\tif not hasattr(gpr, \"name\"):\\n\\t\\t\\treturn False\\n\\t\\ttry:\\n\\t\\t\\treturn gpr is obj[gpr.name]\\n\\t\\texcept (KeyError, IndexError, ValueError):\\n\\t\\t\\treturn False\\n\\tfor i, (gpr, level) in enumerate(zip(keys, levels)):\\n\\t\\tif is_in_obj(gpr):  \\n\\t\\t\\tin_axis, name = True, gpr.name\\n\\t\\t\\texclusions.append(name)\\n\\t\\telif is_in_axis(gpr):  \\n\\t\\t\\tif gpr in obj:\\n\\t\\t\\t\\tif validate:\\n\\t\\t\\t\\t\\tobj._check_label_or_level_ambiguity(gpr, axis=axis)\\n\\t\\t\\t\\tin_axis, name, gpr = True, gpr, obj[gpr]\\n\\t\\t\\t\\texclusions.append(name)\\n\\t\\t\\telif obj._is_level_reference(gpr, axis=axis):\\n\\t\\t\\t\\tin_axis, name, level, gpr = False, None, gpr, None\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(gpr)\\n\\t\\telif isinstance(gpr, Grouper) and gpr.key is not None:\\n\\t\\t\\texclusions.append(gpr.key)\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\telse:\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\tif is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\tf\"Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) \"\\n\\t\\t\\t\\t\"must be same length\"\\n\\t\\t\\t)\\n\\t\\tping = (\\n\\t\\t\\tGrouping(\\n\\t\\t\\t\\tgroup_axis,\\n\\t\\t\\t\\tgpr,\\n\\t\\t\\t\\tobj=obj,\\n\\t\\t\\t\\tname=name,\\n\\t\\t\\t\\tlevel=level,\\n\\t\\t\\t\\tsort=sort,\\n\\t\\t\\t\\tobserved=observed,\\n\\t\\t\\t\\tin_axis=in_axis,\\n\\t\\t\\t\\tdropna=dropna,\\n\\t\\t\\t)\\n\\t\\t\\tif not isinstance(gpr, Grouping)\\n\\t\\t\\telse gpr\\n\\t\\t)\\n\\t\\tgroupings.append(ping)\\n\\tif len(groupings) == 0 and len(obj):\\n\\t\\traise ValueError(\"No group keys passed!\")\\n\\telif len(groupings) == 0:\\n\\t\\tgroupings.append(Grouping(Index([], dtype=\"int\"), np.array([], dtype=np.intp)))\\n\\tgrouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)\\n\\treturn grouper, exclusions, obj"
  },
  {
    "code": "def _homogenize_series(data, index, dtype=None):\\n    homogenized = {}\\n    for k, v in data.iteritems():\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index)\\n        else:\\n            if isinstance(v, dict):\\n                v = [v.get(i, nan) for i in index]\\n            else:\\n                assert(len(v) == len(index))\\n            try:\\n                v = Series(v, dtype=dtype, index=index)\\n            except Exception:\\n                v = Series(v, index=index)\\n        homogenized[k] = v\\n    return homogenized",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _homogenize_series(data, index, dtype=None):\\n    homogenized = {}\\n    for k, v in data.iteritems():\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index)\\n        else:\\n            if isinstance(v, dict):\\n                v = [v.get(i, nan) for i in index]\\n            else:\\n                assert(len(v) == len(index))\\n            try:\\n                v = Series(v, dtype=dtype, index=index)\\n            except Exception:\\n                v = Series(v, index=index)\\n        homogenized[k] = v\\n    return homogenized"
  },
  {
    "code": "def head(self, message_spec=None, *, file=None):\\n        if message_spec is not None:\\n            cmd = 'HEAD {0}'.format(message_spec)\\n        else:\\n            cmd = 'HEAD'\\n        return self._artcmd(cmd, file)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def head(self, message_spec=None, *, file=None):\\n        if message_spec is not None:\\n            cmd = 'HEAD {0}'.format(message_spec)\\n        else:\\n            cmd = 'HEAD'\\n        return self._artcmd(cmd, file)"
  },
  {
    "code": "def test_connection(self):\\n        status, message = False, ''\\n        try:\\n            if self.get_first(\"select 1\"):\\n                status = True\\n                message = 'Connection successfully tested'\\n        except Exception as e:\\n            status = False\\n            message = str(e)\\n        return status, message",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix oracle test connection (#21699)",
    "fixed_code": "def test_connection(self):\\n        status, message = False, ''\\n        try:\\n            if self.get_first(self._test_connection_sql):\\n                status = True\\n                message = 'Connection successfully tested'\\n        except Exception as e:\\n            status = False\\n            message = str(e)\\n        return status, message"
  },
  {
    "code": "def value_counts(values, sort=True, ascending=False, normalize=False, bins=None):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    values = Series(values).values\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.labels\\n    if com.is_integer_dtype(values.dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    elif issubclass(values.dtype.type, (np.datetime64,np.timedelta64)):\\n        dtype = values.dtype\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        keys = Series(keys, dtype=dtype)\\n    else:\\n        mask = com.isnull(values)\\n        values = com._ensure_object(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.levels)), fill_value=0)\\n        result.index = bins[:-1]\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def value_counts(values, sort=True, ascending=False, normalize=False, bins=None):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    values = Series(values).values\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.labels\\n    if com.is_integer_dtype(values.dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    elif issubclass(values.dtype.type, (np.datetime64,np.timedelta64)):\\n        dtype = values.dtype\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        keys = Series(keys, dtype=dtype)\\n    else:\\n        mask = com.isnull(values)\\n        values = com._ensure_object(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n    result = Series(counts, index=com._values_from_object(keys))\\n    if bins is not None:\\n        result = result.reindex(np.arange(len(cat.levels)), fill_value=0)\\n        result.index = bins[:-1]\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result"
  },
  {
    "code": "def _find_longest_prefix_match(bit_string_array, query, hash_size,\\n                               left_masks, right_masks):\\n    hi = hash_size\\n    lo = 0\\n    if _find_matching_indices(bit_string_array, query, left_masks[hi],\\n                              right_masks[hi]).shape[0] > 0:\\n        return hi\\n    while lo < hi:\\n        mid = (lo+hi)//2\\n        k = _find_matching_indices(bit_string_array, query,\\n                                   left_masks[mid],\\n                                   right_masks[mid]).shape[0]\\n        if k > 0:\\n            lo = mid + 1\\n            res = mid\\n        else:\\n            hi = mid\\n    return res",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _find_longest_prefix_match(bit_string_array, query, hash_size,\\n                               left_masks, right_masks):\\n    hi = hash_size\\n    lo = 0\\n    if _find_matching_indices(bit_string_array, query, left_masks[hi],\\n                              right_masks[hi]).shape[0] > 0:\\n        return hi\\n    while lo < hi:\\n        mid = (lo+hi)//2\\n        k = _find_matching_indices(bit_string_array, query,\\n                                   left_masks[mid],\\n                                   right_masks[mid]).shape[0]\\n        if k > 0:\\n            lo = mid + 1\\n            res = mid\\n        else:\\n            hi = mid\\n    return res"
  },
  {
    "code": "def patch_click() -> None:\\n    modules: List[Any] = []\\n    try:\\n        from click import core\\n    except ImportError:\\n        pass\\n    else:\\n        modules.append(core)\\n    try:\\n        from click import _unicodefun\\n    except ImportError:\\n        pass\\n    else:\\n        modules.append(_unicodefun)\\n    for module in modules:\\n        if hasattr(module, \"_verify_python3_env\"):\\n            module._verify_python3_env = lambda: None  \\n        if hasattr(module, \"_verify_python_env\"):\\n            module._verify_python_env = lambda: None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def patch_click() -> None:\\n    modules: List[Any] = []\\n    try:\\n        from click import core\\n    except ImportError:\\n        pass\\n    else:\\n        modules.append(core)\\n    try:\\n        from click import _unicodefun\\n    except ImportError:\\n        pass\\n    else:\\n        modules.append(_unicodefun)\\n    for module in modules:\\n        if hasattr(module, \"_verify_python3_env\"):\\n            module._verify_python3_env = lambda: None  \\n        if hasattr(module, \"_verify_python_env\"):\\n            module._verify_python_env = lambda: None"
  },
  {
    "code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    if _is_sqlalchemy_engine(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    else:\\n        if flavor == 'mysql':\\n            warnings.warn(_MYSQL_WARNING, FutureWarning)\\n        return SQLiteDatabase(con, flavor, is_cursor=is_cursor)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    if _is_sqlalchemy_engine(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    else:\\n        if flavor == 'mysql':\\n            warnings.warn(_MYSQL_WARNING, FutureWarning)\\n        return SQLiteDatabase(con, flavor, is_cursor=is_cursor)"
  },
  {
    "code": "def check(file):\\n    if os.path.isdir(file) and not os.path.islink(file):\\n        if verbose:\\n            print \"%r: listing directory\" % (file,)\\n        names = os.listdir(file)\\n        for name in names:\\n            fullname = os.path.join(file, name)\\n            if (os.path.isdir(fullname) and\\n                not os.path.islink(fullname) or\\n                os.path.normcase(name[-3:]) == \".py\"):\\n                check(fullname)\\n        return\\n    try:\\n        f = open(file)\\n    except IOError, msg:\\n        errprint(\"%r: I/O Error: %s\" % (file, msg))\\n        return\\n    if verbose > 1:\\n        print \"checking %r ...\" % file\\n    try:\\n        process_tokens(tokenize.generate_tokens(f.readline))\\n    except tokenize.TokenError, msg:\\n        errprint(\"%r: Token Error: %s\" % (file, msg))\\n        return\\n    except NannyNag, nag:\\n        badline = nag.get_lineno()\\n        line = nag.get_line()\\n        if verbose:\\n            print \"%r: *** Line %d: trouble in tab city! ***\" % (file, badline)\\n            print \"offending line: %r\" % (line,)\\n            print nag.get_msg()\\n        else:\\n            if ' ' in file: file = '\"' + file + '\"'\\n            if filename_only: print file\\n            else: print file, badline, repr(line)\\n        return\\n    if verbose:\\n        print \"%r: Clean bill of health.\" % (file,)\\nclass Whitespace:\\n    S, T = ' \\t'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make tabnanny recognize IndentationErrors raised by tokenize. Add a test to test_inspect to make sure indented source is recognized correctly. (fixes #1224621)",
    "fixed_code": "def check(file):\\n    if os.path.isdir(file) and not os.path.islink(file):\\n        if verbose:\\n            print \"%r: listing directory\" % (file,)\\n        names = os.listdir(file)\\n        for name in names:\\n            fullname = os.path.join(file, name)\\n            if (os.path.isdir(fullname) and\\n                not os.path.islink(fullname) or\\n                os.path.normcase(name[-3:]) == \".py\"):\\n                check(fullname)\\n        return\\n    try:\\n        f = open(file)\\n    except IOError, msg:\\n        errprint(\"%r: I/O Error: %s\" % (file, msg))\\n        return\\n    if verbose > 1:\\n        print \"checking %r ...\" % file\\n    try:\\n        process_tokens(tokenize.generate_tokens(f.readline))\\n    except tokenize.TokenError, msg:\\n        errprint(\"%r: Token Error: %s\" % (file, msg))\\n        return\\n    except IndentationError, msg:\\n        errprint(\"%r: Indentation Error: %s\" % (file, msg))\\n        return\\n    except NannyNag, nag:\\n        badline = nag.get_lineno()\\n        line = nag.get_line()\\n        if verbose:\\n            print \"%r: *** Line %d: trouble in tab city! ***\" % (file, badline)\\n            print \"offending line: %r\" % (line,)\\n            print nag.get_msg()\\n        else:\\n            if ' ' in file: file = '\"' + file + '\"'\\n            if filename_only: print file\\n            else: print file, badline, repr(line)\\n        return\\n    if verbose:\\n        print \"%r: Clean bill of health.\" % (file,)\\nclass Whitespace:\\n    S, T = ' \\t'"
  },
  {
    "code": "def filesizeformat(bytes):\\n\\ttry:\\n\\t\\tbytes = float(bytes)\\n\\texcept (TypeError, ValueError, UnicodeDecodeError):\\n\\t\\tvalue = ungettext(\"%(size)d byte\", \"%(size)d bytes\", 0) % {'size': 0}\\n\\t\\treturn avoid_wrapping(value)\\n\\tfilesize_number_format = lambda value: formats.number_format(round(value, 1), 1)\\n\\tKB = 1 << 10\\n\\tMB = 1 << 20\\n\\tGB = 1 << 30\\n\\tTB = 1 << 40\\n\\tPB = 1 << 50\\n\\tif bytes < KB:\\n\\t\\tvalue = ungettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\\n\\telif bytes < MB:\\n\\t\\tvalue = ugettext(\"%s KB\") % filesize_number_format(bytes / KB)\\n\\telif bytes < GB:\\n\\t\\tvalue = ugettext(\"%s MB\") % filesize_number_format(bytes / MB)\\n\\telif bytes < TB:\\n\\t\\tvalue = ugettext(\"%s GB\") % filesize_number_format(bytes / GB)\\n\\telif bytes < PB:\\n\\t\\tvalue = ugettext(\"%s TB\") % filesize_number_format(bytes / TB)\\n\\telse:\\n\\t\\tvalue = ugettext(\"%s PB\") % filesize_number_format(bytes / PB)\\n\\treturn avoid_wrapping(value)\\n@register.filter(is_safe=False)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def filesizeformat(bytes):\\n\\ttry:\\n\\t\\tbytes = float(bytes)\\n\\texcept (TypeError, ValueError, UnicodeDecodeError):\\n\\t\\tvalue = ungettext(\"%(size)d byte\", \"%(size)d bytes\", 0) % {'size': 0}\\n\\t\\treturn avoid_wrapping(value)\\n\\tfilesize_number_format = lambda value: formats.number_format(round(value, 1), 1)\\n\\tKB = 1 << 10\\n\\tMB = 1 << 20\\n\\tGB = 1 << 30\\n\\tTB = 1 << 40\\n\\tPB = 1 << 50\\n\\tif bytes < KB:\\n\\t\\tvalue = ungettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\\n\\telif bytes < MB:\\n\\t\\tvalue = ugettext(\"%s KB\") % filesize_number_format(bytes / KB)\\n\\telif bytes < GB:\\n\\t\\tvalue = ugettext(\"%s MB\") % filesize_number_format(bytes / MB)\\n\\telif bytes < TB:\\n\\t\\tvalue = ugettext(\"%s GB\") % filesize_number_format(bytes / GB)\\n\\telif bytes < PB:\\n\\t\\tvalue = ugettext(\"%s TB\") % filesize_number_format(bytes / TB)\\n\\telse:\\n\\t\\tvalue = ugettext(\"%s PB\") % filesize_number_format(bytes / PB)\\n\\treturn avoid_wrapping(value)\\n@register.filter(is_safe=False)"
  },
  {
    "code": "def __init__(self, azure_data_lake_conn_id: str = 'azure_data_lake_default'):\\n        super().__init__()\\n        self.conn_id = azure_data_lake_conn_id\\n        self._conn: Optional[core.AzureDLFileSystem] = None\\n        self.account_name: Optional[str] = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, azure_data_lake_conn_id: str = 'azure_data_lake_default'):\\n        super().__init__()\\n        self.conn_id = azure_data_lake_conn_id\\n        self._conn: Optional[core.AzureDLFileSystem] = None\\n        self.account_name: Optional[str] = None"
  },
  {
    "code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in map(os.fsdecode, seq):\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in map(os.fsdecode, seq):\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)"
  },
  {
    "code": "def discover_interpreter(action, interpreter_name, discovery_mode, task_vars):\\n    if interpreter_name != 'python':\\n        raise ValueError('Interpreter discovery not supported for {0}'.format(interpreter_name))\\n    host = task_vars.get('inventory_hostname', 'unknown')\\n    res = None\\n    platform_type = 'unknown'\\n    found_interpreters = [u'/usr/bin/python']  \\n    is_auto_legacy = discovery_mode.startswith('auto_legacy')\\n    is_silent = discovery_mode.endswith('_silent')\\n    try:\\n        platform_python_map = C.config.get_config_value('INTERPRETER_PYTHON_DISTRO_MAP', variables=task_vars)\\n        bootstrap_python_list = C.config.get_config_value('INTERPRETER_PYTHON_FALLBACK', variables=task_vars)\\n        display.vvv(msg=u\"Attempting {0} interpreter discovery\".format(interpreter_name), host=host)\\n        command_list = [\"command -v '%s'\" % py for py in bootstrap_python_list]\\n        shell_bootstrap = \"echo PLATFORM; uname; echo FOUND; {0}; echo ENDFOUND\".format('; '.join(command_list))\\n        res = action._low_level_execute_command(shell_bootstrap, sudoable=False)\\n        raw_stdout = res.get('stdout', u'')\\n        match = foundre.match(raw_stdout)\\n        if not match:\\n            display.debug(u'raw interpreter discovery output: {0}'.format(raw_stdout), host=host)\\n            raise ValueError('unexpected output from Python interpreter discovery')\\n        platform_type = match.groups()[0].lower().strip()\\n        found_interpreters = [interp.strip() for interp in match.groups()[1].splitlines() if interp.startswith('/')]\\n        display.debug(u\"found interpreters: {0}\".format(found_interpreters), host=host)\\n        if not found_interpreters:\\n            action._discovery_warnings.append(u'No python interpreters found for host {0} (tried {1})'.format(host, bootstrap_python_list))\\n            return u'/usr/bin/python'\\n        if platform_type != 'linux':\\n            raise NotImplementedError('unsupported platform for extended discovery: {0}'.format(to_native(platform_type)))\\n        platform_script = pkgutil.get_data('ansible.executor.discovery', 'python_target.py')\\n        if action._connection.has_pipelining:\\n            res = action._low_level_execute_command(found_interpreters[0], sudoable=False, in_data=platform_script)\\n        else:\\n            raise NotImplementedError('pipelining support required for extended interpreter discovery')\\n        platform_info = json.loads(res.get('stdout'))\\n        distro, version = _get_linux_distro(platform_info)\\n        if not distro or not version:\\n            raise NotImplementedError('unable to get Linux distribution/version info')\\n        version_map = platform_python_map.get(distro.lower().strip())\\n        if not version_map:\\n            raise NotImplementedError('unsupported Linux distribution: {0}'.format(distro))\\n        platform_interpreter = to_text(_version_fuzzy_match(version, version_map), errors='surrogate_or_strict')\\n        if is_auto_legacy:\\n            if platform_interpreter != u'/usr/bin/python' and u'/usr/bin/python' in found_interpreters:\\n                if not is_silent:\\n                    action._discovery_deprecation_warnings.append(dict(\\n                        msg=u\"Distribution {0} {1} on host {2} should use {3}, but is using \"\\n                            u\"/usr/bin/python for backward compatibility with prior Ansible releases. \"\\n                            u\"A future Ansible release will default to using the discovered platform \"\\n                            u\"python for this host. See {4} for more information\"\\n                            .format(distro, version, host, platform_interpreter,\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')),\\n                        version='2.12'))\\n                return u'/usr/bin/python'\\n        if platform_interpreter not in found_interpreters:\\n            if platform_interpreter not in bootstrap_python_list:\\n                if not is_silent:\\n                    action._discovery_warnings \\\\n                        .append(u\"Platform interpreter {0} on host {1} is missing from bootstrap list\"\\n                                .format(platform_interpreter, host))\\n            if not is_silent:\\n                action._discovery_warnings \\\\n                    .append(u\"Distribution {0} {1} on host {2} should use {3}, but is using {4}, since the \"\\n                            u\"discovered platform python interpreter was not present. See {5} \"\\n                            u\"for more information.\"\\n                            .format(distro, version, host, platform_interpreter, found_interpreters[0],\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n            return found_interpreters[0]\\n        return platform_interpreter\\n    except NotImplementedError as ex:\\n        display.vvv(msg=u'Python interpreter discovery fallback ({0})'.format(to_text(ex)), host=host)\\n    except Exception as ex:\\n        if not is_silent:\\n            display.warning(msg=u'Unhandled error in Python interpreter discovery for host {0}: {1}'.format(host, to_text(ex)))\\n            display.debug(msg=u'Interpreter discovery traceback:\\n{0}'.format(to_text(format_exc())), host=host)\\n            if res and res.get('stderr'):\\n                display.vvv(msg=u'Interpreter discovery remote stderr:\\n{0}'.format(to_text(res.get('stderr'))), host=host)\\n    if not is_silent:\\n        action._discovery_warnings \\\\n            .append(u\"Platform {0} on host {1} is using the discovered Python interpreter at {2}, but future installation of \"\\n                    u\"another Python interpreter could change this. See {3} \"\\n                    u\"for more information.\"\\n                    .format(platform_type, host, found_interpreters[0],\\n                            get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n    return found_interpreters[0]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make Python path warning say what it means to say (#69669)",
    "fixed_code": "def discover_interpreter(action, interpreter_name, discovery_mode, task_vars):\\n    if interpreter_name != 'python':\\n        raise ValueError('Interpreter discovery not supported for {0}'.format(interpreter_name))\\n    host = task_vars.get('inventory_hostname', 'unknown')\\n    res = None\\n    platform_type = 'unknown'\\n    found_interpreters = [u'/usr/bin/python']  \\n    is_auto_legacy = discovery_mode.startswith('auto_legacy')\\n    is_silent = discovery_mode.endswith('_silent')\\n    try:\\n        platform_python_map = C.config.get_config_value('INTERPRETER_PYTHON_DISTRO_MAP', variables=task_vars)\\n        bootstrap_python_list = C.config.get_config_value('INTERPRETER_PYTHON_FALLBACK', variables=task_vars)\\n        display.vvv(msg=u\"Attempting {0} interpreter discovery\".format(interpreter_name), host=host)\\n        command_list = [\"command -v '%s'\" % py for py in bootstrap_python_list]\\n        shell_bootstrap = \"echo PLATFORM; uname; echo FOUND; {0}; echo ENDFOUND\".format('; '.join(command_list))\\n        res = action._low_level_execute_command(shell_bootstrap, sudoable=False)\\n        raw_stdout = res.get('stdout', u'')\\n        match = foundre.match(raw_stdout)\\n        if not match:\\n            display.debug(u'raw interpreter discovery output: {0}'.format(raw_stdout), host=host)\\n            raise ValueError('unexpected output from Python interpreter discovery')\\n        platform_type = match.groups()[0].lower().strip()\\n        found_interpreters = [interp.strip() for interp in match.groups()[1].splitlines() if interp.startswith('/')]\\n        display.debug(u\"found interpreters: {0}\".format(found_interpreters), host=host)\\n        if not found_interpreters:\\n            action._discovery_warnings.append(u'No python interpreters found for host {0} (tried {1})'.format(host, bootstrap_python_list))\\n            return u'/usr/bin/python'\\n        if platform_type != 'linux':\\n            raise NotImplementedError('unsupported platform for extended discovery: {0}'.format(to_native(platform_type)))\\n        platform_script = pkgutil.get_data('ansible.executor.discovery', 'python_target.py')\\n        if action._connection.has_pipelining:\\n            res = action._low_level_execute_command(found_interpreters[0], sudoable=False, in_data=platform_script)\\n        else:\\n            raise NotImplementedError('pipelining support required for extended interpreter discovery')\\n        platform_info = json.loads(res.get('stdout'))\\n        distro, version = _get_linux_distro(platform_info)\\n        if not distro or not version:\\n            raise NotImplementedError('unable to get Linux distribution/version info')\\n        version_map = platform_python_map.get(distro.lower().strip())\\n        if not version_map:\\n            raise NotImplementedError('unsupported Linux distribution: {0}'.format(distro))\\n        platform_interpreter = to_text(_version_fuzzy_match(version, version_map), errors='surrogate_or_strict')\\n        if is_auto_legacy:\\n            if platform_interpreter != u'/usr/bin/python' and u'/usr/bin/python' in found_interpreters:\\n                if not is_silent:\\n                    action._discovery_deprecation_warnings.append(dict(\\n                        msg=u\"Distribution {0} {1} on host {2} should use {3}, but is using \"\\n                            u\"/usr/bin/python for backward compatibility with prior Ansible releases. \"\\n                            u\"A future Ansible release will default to using the discovered platform \"\\n                            u\"python for this host. See {4} for more information\"\\n                            .format(distro, version, host, platform_interpreter,\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')),\\n                        version='2.12'))\\n                return u'/usr/bin/python'\\n        if platform_interpreter not in found_interpreters:\\n            if platform_interpreter not in bootstrap_python_list:\\n                if not is_silent:\\n                    action._discovery_warnings \\\\n                        .append(u\"Platform interpreter {0} on host {1} is missing from bootstrap list\"\\n                                .format(platform_interpreter, host))\\n            if not is_silent:\\n                action._discovery_warnings \\\\n                    .append(u\"Distribution {0} {1} on host {2} should use {3}, but is using {4}, since the \"\\n                            u\"discovered platform python interpreter was not present. See {5} \"\\n                            u\"for more information.\"\\n                            .format(distro, version, host, platform_interpreter, found_interpreters[0],\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n            return found_interpreters[0]\\n        return platform_interpreter\\n    except NotImplementedError as ex:\\n        display.vvv(msg=u'Python interpreter discovery fallback ({0})'.format(to_text(ex)), host=host)\\n    except Exception as ex:\\n        if not is_silent:\\n            display.warning(msg=u'Unhandled error in Python interpreter discovery for host {0}: {1}'.format(host, to_text(ex)))\\n            display.debug(msg=u'Interpreter discovery traceback:\\n{0}'.format(to_text(format_exc())), host=host)\\n            if res and res.get('stderr'):\\n                display.vvv(msg=u'Interpreter discovery remote stderr:\\n{0}'.format(to_text(res.get('stderr'))), host=host)\\n    if not is_silent:\\n        action._discovery_warnings \\\\n            .append(u\"Platform {0} on host {1} is using the discovered Python interpreter at {2}, but future installation of \"\\n                    u\"another Python interpreter could change the meaning of that path. See {3} \"\\n                    u\"for more information.\"\\n                    .format(platform_type, host, found_interpreters[0],\\n                            get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n    return found_interpreters[0]"
  },
  {
    "code": "def get_near_stock_price(self, above_below=2, call=True, put=False,\\n                             month=None, year=None, expiry=None):\\n        expiry = self._try_parse_dates(year, month, expiry)\\n        data = self._get_data_in_date_range(expiry, call=call, put=put)\\n        return self.chop_data(data, above_below, self.underlying_price)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_near_stock_price(self, above_below=2, call=True, put=False,\\n                             month=None, year=None, expiry=None):\\n        expiry = self._try_parse_dates(year, month, expiry)\\n        data = self._get_data_in_date_range(expiry, call=call, put=put)\\n        return self.chop_data(data, above_below, self.underlying_price)"
  },
  {
    "code": "def update_confusion_matrix_variables(variables_to_update,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  y_true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  y_pred,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  thresholds,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  top_k=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  class_id=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  sample_weight=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  multi_label=False,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  label_weights=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  thresholds_distributed_evenly=False):\\n  if multi_label and label_weights is not None:\\n\\traise ValueError('`label_weights` for multilabel data should be handled '\\n\\t\\t\\t\\t\\t 'outside of `update_confusion_matrix_variables` when '\\n\\t\\t\\t\\t\\t '`multi_label` is True.')\\n  if variables_to_update is None:\\n\\treturn\\n  if not any(\\n\\t  key for key in variables_to_update if key in list(ConfusionMatrix)):\\n\\traise ValueError(\\n\\t\\t'Please provide at least one valid confusion matrix '\\n\\t\\t'variable to update. Valid variable key options are: '\\n\\t\\tf'\"{list(ConfusionMatrix)}\". Received: \"{variables_to_update.keys()}\"')\\n  variable_dtype = list(variables_to_update.values())[0].dtype\\n  y_true = tf.cast(y_true, dtype=variable_dtype)\\n  y_pred = tf.cast(y_pred, dtype=variable_dtype)\\n  if thresholds_distributed_evenly:\\n\\tthresholds_with_epsilon = thresholds[0] < 0.0 or thresholds[-1] > 1.0\\n  thresholds = tf.convert_to_tensor(\\n\\t  thresholds, dtype=variable_dtype)\\n  num_thresholds = thresholds.shape.as_list()[0]\\n  if multi_label:\\n\\tone_thresh = tf.equal(\\n\\t\\ttf.cast(1, dtype=tf.int32),\\n\\t\\ttf.rank(thresholds),\\n\\t\\tname='one_set_of_thresholds_cond')\\n  else:\\n\\t[y_pred,\\n\\t y_true], _ = ragged_assert_compatible_and_get_flat_values([y_pred, y_true],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   sample_weight)\\n\\tone_thresh = tf.cast(True, dtype=tf.bool)\\n  invalid_keys = [\\n\\t  key for key in variables_to_update if key not in list(ConfusionMatrix)\\n  ]\\n  if invalid_keys:\\n\\traise ValueError(\\n\\t\\tf'Invalid keys: \"{invalid_keys}\". '\\n\\t\\tf'Valid variable key options are: \"{list(ConfusionMatrix)}\"')\\n  with tf.control_dependencies([\\n\\t  tf.debugging.assert_greater_equal(\\n\\t\\t  y_pred,\\n\\t\\t  tf.cast(0.0, dtype=y_pred.dtype),\\n\\t\\t  message='predictions must be >= 0'),\\n\\t  tf.debugging.assert_less_equal(\\n\\t\\t  y_pred,\\n\\t\\t  tf.cast(1.0, dtype=y_pred.dtype),\\n\\t\\t  message='predictions must be <= 1')\\n  ]):\\n\\tif sample_weight is None:\\n\\t  y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\\n\\t\\t  y_pred, y_true)\\n\\telse:\\n\\t  sample_weight = tf.cast(sample_weight, dtype=variable_dtype)\\n\\t  y_pred, y_true, sample_weight = (\\n\\t\\t  losses_utils.squeeze_or_expand_dimensions(\\n\\t\\t\\t  y_pred, y_true, sample_weight=sample_weight))\\n  y_pred.shape.assert_is_compatible_with(y_true.shape)\\n  if top_k is not None:\\n\\ty_pred = _filter_top_k(y_pred, top_k)\\n  if class_id is not None:\\n\\ty_true = y_true[..., class_id]\\n\\ty_pred = y_pred[..., class_id]\\n  if thresholds_distributed_evenly:\\n\\treturn _update_confusion_matrix_variables_optimized(\\n\\t\\tvariables_to_update, y_true, y_pred, thresholds,\\n\\t\\tmulti_label=multi_label, sample_weights=sample_weight,\\n\\t\\tlabel_weights=label_weights,\\n\\t\\tthresholds_with_epsilon=thresholds_with_epsilon)\\n  pred_shape = tf.shape(y_pred)\\n  num_predictions = pred_shape[0]\\n  if y_pred.shape.ndims == 1:\\n\\tnum_labels = 1\\n  else:\\n\\tnum_labels = tf.math.reduce_prod(pred_shape[1:], axis=0)\\n  thresh_label_tile = tf.where(one_thresh, num_labels,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t tf.ones([], dtype=tf.int32))\\n  if multi_label:\\n\\tpredictions_extra_dim = tf.expand_dims(y_pred, 0)\\n\\tlabels_extra_dim = tf.expand_dims(\\n\\t\\ttf.cast(y_true, dtype=tf.bool), 0)\\n  else:\\n\\tpredictions_extra_dim = tf.reshape(y_pred, [1, -1])\\n\\tlabels_extra_dim = tf.reshape(\\n\\t\\ttf.cast(y_true, dtype=tf.bool), [1, -1])\\n  if multi_label:\\n\\tthresh_pretile_shape = [num_thresholds, 1, -1]\\n\\tthresh_tiles = [1, num_predictions, thresh_label_tile]\\n\\tdata_tiles = [num_thresholds, 1, 1]\\n  else:\\n\\tthresh_pretile_shape = [num_thresholds, -1]\\n\\tthresh_tiles = [1, num_predictions * num_labels]\\n\\tdata_tiles = [num_thresholds, 1]\\n  thresh_tiled = tf.tile(\\n\\t  tf.reshape(thresholds, thresh_pretile_shape),\\n\\t  tf.stack(thresh_tiles))\\n  preds_tiled = tf.tile(predictions_extra_dim, data_tiles)\\n  pred_is_pos = tf.greater(preds_tiled, thresh_tiled)\\n  label_is_pos = tf.tile(labels_extra_dim, data_tiles)\\n  if sample_weight is not None:\\n\\tsample_weight = tf.__internal__.ops.broadcast_weights(\\n\\t\\ttf.cast(sample_weight, dtype=variable_dtype), y_pred)\\n\\tweights_tiled = tf.tile(\\n\\t\\ttf.reshape(sample_weight, thresh_tiles), data_tiles)\\n  else:\\n\\tweights_tiled = None\\n  if label_weights is not None and not multi_label:\\n\\tlabel_weights = tf.expand_dims(label_weights, 0)\\n\\tlabel_weights = tf.__internal__.ops.broadcast_weights(label_weights,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ty_pred)\\n\\tlabel_weights_tiled = tf.tile(\\n\\t\\ttf.reshape(label_weights, thresh_tiles), data_tiles)\\n\\tif weights_tiled is None:\\n\\t  weights_tiled = label_weights_tiled\\n\\telse:\\n\\t  weights_tiled = tf.multiply(weights_tiled, label_weights_tiled)\\n  update_ops = []",
    "label": 1,
    "bug_type": "perf",
    "bug_description": "resolves #15715. Removing assert y_pred in range [0, 1] in metrics because they may be logits.",
    "fixed_code": "def update_confusion_matrix_variables(variables_to_update,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  y_true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  y_pred,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  thresholds,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  top_k=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  class_id=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  sample_weight=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  multi_label=False,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  label_weights=None,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  thresholds_distributed_evenly=False):\\n  if multi_label and label_weights is not None:\\n\\traise ValueError('`label_weights` for multilabel data should be handled '\\n\\t\\t\\t\\t\\t 'outside of `update_confusion_matrix_variables` when '\\n\\t\\t\\t\\t\\t '`multi_label` is True.')\\n  if variables_to_update is None:\\n\\treturn\\n  if not any(\\n\\t  key for key in variables_to_update if key in list(ConfusionMatrix)):\\n\\traise ValueError(\\n\\t\\t'Please provide at least one valid confusion matrix '\\n\\t\\t'variable to update. Valid variable key options are: '\\n\\t\\tf'\"{list(ConfusionMatrix)}\". Received: \"{variables_to_update.keys()}\"')\\n  variable_dtype = list(variables_to_update.values())[0].dtype\\n  y_true = tf.cast(y_true, dtype=variable_dtype)\\n  y_pred = tf.cast(y_pred, dtype=variable_dtype)\\n  if thresholds_distributed_evenly:\\n\\tthresholds_with_epsilon = thresholds[0] < 0.0 or thresholds[-1] > 1.0\\n  thresholds = tf.convert_to_tensor(\\n\\t  thresholds, dtype=variable_dtype)\\n  num_thresholds = thresholds.shape.as_list()[0]\\n  if multi_label:\\n\\tone_thresh = tf.equal(\\n\\t\\ttf.cast(1, dtype=tf.int32),\\n\\t\\ttf.rank(thresholds),\\n\\t\\tname='one_set_of_thresholds_cond')\\n  else:\\n\\t[y_pred,\\n\\t y_true], _ = ragged_assert_compatible_and_get_flat_values([y_pred, y_true],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   sample_weight)\\n\\tone_thresh = tf.cast(True, dtype=tf.bool)\\n  invalid_keys = [\\n\\t  key for key in variables_to_update if key not in list(ConfusionMatrix)\\n  ]\\n  if invalid_keys:\\n\\traise ValueError(\\n\\t\\tf'Invalid keys: \"{invalid_keys}\". '\\n\\t\\tf'Valid variable key options are: \"{list(ConfusionMatrix)}\"')\\n  if sample_weight is None:\\n\\ty_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\\n\\t\\ty_pred, y_true)\\n  else:\\n\\tsample_weight = tf.cast(sample_weight, dtype=variable_dtype)\\n\\ty_pred, y_true, sample_weight = (\\n\\t\\tlosses_utils.squeeze_or_expand_dimensions(\\n\\t\\t\\ty_pred, y_true, sample_weight=sample_weight))\\n  y_pred.shape.assert_is_compatible_with(y_true.shape)\\n  if top_k is not None:\\n\\ty_pred = _filter_top_k(y_pred, top_k)\\n  if class_id is not None:\\n\\ty_true = y_true[..., class_id]\\n\\ty_pred = y_pred[..., class_id]\\n  if thresholds_distributed_evenly:\\n\\treturn _update_confusion_matrix_variables_optimized(\\n\\t\\tvariables_to_update, y_true, y_pred, thresholds,\\n\\t\\tmulti_label=multi_label, sample_weights=sample_weight,\\n\\t\\tlabel_weights=label_weights,\\n\\t\\tthresholds_with_epsilon=thresholds_with_epsilon)\\n  pred_shape = tf.shape(y_pred)\\n  num_predictions = pred_shape[0]\\n  if y_pred.shape.ndims == 1:\\n\\tnum_labels = 1\\n  else:\\n\\tnum_labels = tf.math.reduce_prod(pred_shape[1:], axis=0)\\n  thresh_label_tile = tf.where(one_thresh, num_labels,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t tf.ones([], dtype=tf.int32))\\n  if multi_label:\\n\\tpredictions_extra_dim = tf.expand_dims(y_pred, 0)\\n\\tlabels_extra_dim = tf.expand_dims(\\n\\t\\ttf.cast(y_true, dtype=tf.bool), 0)\\n  else:\\n\\tpredictions_extra_dim = tf.reshape(y_pred, [1, -1])\\n\\tlabels_extra_dim = tf.reshape(\\n\\t\\ttf.cast(y_true, dtype=tf.bool), [1, -1])\\n  if multi_label:\\n\\tthresh_pretile_shape = [num_thresholds, 1, -1]\\n\\tthresh_tiles = [1, num_predictions, thresh_label_tile]\\n\\tdata_tiles = [num_thresholds, 1, 1]\\n  else:\\n\\tthresh_pretile_shape = [num_thresholds, -1]\\n\\tthresh_tiles = [1, num_predictions * num_labels]\\n\\tdata_tiles = [num_thresholds, 1]\\n  thresh_tiled = tf.tile(\\n\\t  tf.reshape(thresholds, thresh_pretile_shape),\\n\\t  tf.stack(thresh_tiles))\\n  preds_tiled = tf.tile(predictions_extra_dim, data_tiles)\\n  pred_is_pos = tf.greater(preds_tiled, thresh_tiled)\\n  label_is_pos = tf.tile(labels_extra_dim, data_tiles)\\n  if sample_weight is not None:\\n\\tsample_weight = tf.__internal__.ops.broadcast_weights(\\n\\t\\ttf.cast(sample_weight, dtype=variable_dtype), y_pred)\\n\\tweights_tiled = tf.tile(\\n\\t\\ttf.reshape(sample_weight, thresh_tiles), data_tiles)\\n  else:\\n\\tweights_tiled = None\\n  if label_weights is not None and not multi_label:\\n\\tlabel_weights = tf.expand_dims(label_weights, 0)\\n\\tlabel_weights = tf.__internal__.ops.broadcast_weights(label_weights,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ty_pred)\\n\\tlabel_weights_tiled = tf.tile(\\n\\t\\ttf.reshape(label_weights, thresh_tiles), data_tiles)\\n\\tif weights_tiled is None:\\n\\t  weights_tiled = label_weights_tiled\\n\\telse:\\n\\t  weights_tiled = tf.multiply(weights_tiled, label_weights_tiled)\\n  update_ops = []"
  },
  {
    "code": "def __exit__(self, exc_type, exc_val, exc_tb):\\n        if exc_type is futures.CancelledError and self._cancelled:\\n            self._cancel_handler = None\\n            self._task = None\\n            raise futures.TimeoutError\\n        if self._timeout is not None:\\n            self._cancel_handler.cancel()\\n            self._cancel_handler = None\\n        self._task = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __exit__(self, exc_type, exc_val, exc_tb):\\n        if exc_type is futures.CancelledError and self._cancelled:\\n            self._cancel_handler = None\\n            self._task = None\\n            raise futures.TimeoutError\\n        if self._timeout is not None:\\n            self._cancel_handler.cancel()\\n            self._cancel_handler = None\\n        self._task = None"
  },
  {
    "code": "def _get_proxy(self, url, orig_type):\\n\\t\\tproxy_type, user, password, hostport = _parse_proxy(url)\\n\\t\\tproxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\\n\\t\\tif user:\\n\\t\\t\\tuser_pass = '%s:%s' % (unquote(user), unquote(password))\\n\\t\\t\\tcreds = base64.b64encode(user_pass).strip()\\n\\t\\telse:\\n\\t\\t\\tcreds = None\\n\\t\\treturn creds, proxy_url",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "py3 fix HttpProxy and Retry Middlewares",
    "fixed_code": "def _get_proxy(self, url, orig_type):\\n\\t\\tproxy_type, user, password, hostport = _parse_proxy(url)\\n\\t\\tproxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\\n\\t\\tif user:\\n\\t\\t\\tuser_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\\n\\t\\t\\tcreds = base64.b64encode(user_pass).strip()\\n\\t\\telse:\\n\\t\\t\\tcreds = None\\n\\t\\treturn creds, proxy_url"
  },
  {
    "code": "def execute_dml(self, project_id, instance_id, database_id, queries):\\n        instance = self.get_client(project_id).instance(instance_id)\\n        Database(database_id, instance).run_in_transaction(\\n            lambda transaction: self._execute_sql_in_transaction(transaction, queries))\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute_dml(self, project_id, instance_id, database_id, queries):\\n        instance = self.get_client(project_id).instance(instance_id)\\n        Database(database_id, instance).run_in_transaction(\\n            lambda transaction: self._execute_sql_in_transaction(transaction, queries))\\n    @staticmethod"
  },
  {
    "code": "def _array2string(a, options, separator=' ', prefix=\"\"):\\n    data = asarray(a)\\n    if a.size > options['threshold']:\\n        summary_insert = \"...\"\\n        data = _leading_trailing(data, options['edgeitems'])\\n    else:\\n        summary_insert = \"\"\\n    format_function = _get_format_function(data, **options)\\n    next_line_prefix = \" \"\\n    next_line_prefix += \" \"*len(prefix)\\n    lst = _formatArray(a, format_function, a.ndim, options['linewidth'],\\n                       next_line_prefix, separator, options['edgeitems'],\\n                       summary_insert, options['legacy'])[:-1]\\n    return lst",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _array2string(a, options, separator=' ', prefix=\"\"):\\n    data = asarray(a)\\n    if a.size > options['threshold']:\\n        summary_insert = \"...\"\\n        data = _leading_trailing(data, options['edgeitems'])\\n    else:\\n        summary_insert = \"\"\\n    format_function = _get_format_function(data, **options)\\n    next_line_prefix = \" \"\\n    next_line_prefix += \" \"*len(prefix)\\n    lst = _formatArray(a, format_function, a.ndim, options['linewidth'],\\n                       next_line_prefix, separator, options['edgeitems'],\\n                       summary_insert, options['legacy'])[:-1]\\n    return lst"
  },
  {
    "code": "def guess_scheme(url):\\n    if _is_path(url):\\n        return any_to_uri(url)\\n    return add_http_if_no_scheme(url)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def guess_scheme(url):\\n    if _is_path(url):\\n        return any_to_uri(url)\\n    return add_http_if_no_scheme(url)"
  },
  {
    "code": "def test_smoothing():\\n\\ttimer = DiscreteTimer()\\n\\twith closing(StringIO()) as our_file:\\n\\t\\twith tqdm(_range(3), file=our_file, smoothing=None, leave=True) as t:\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\tfor _ in t:\\n\\t\\t\\t\\tpass\\n\\t\\tour_file.seek(0)\\n\\t\\tassert '| 3/3 ' in our_file.read()\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=None, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\twith tqdm(_range(3), file=our_file, smoothing=None, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0) as t2:\\n\\t\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\t\\tt.update()\\n\\t\\t\\tn_old = len(tqdm._instances)\\n\\t\\t\\tt.close()\\n\\t\\t\\tassert len(tqdm._instances) == n_old - 1\\n\\t\\t\\ta = progressbar_rate(get_bar(our_file, 3))\\n\\t\\ta2 = progressbar_rate(get_bar(our_file2, 3))\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=1, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\twith tqdm(_range(3), file=our_file, smoothing=1, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0) as t2:\\n\\t\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\t\\tt.update()\\n\\t\\t\\tt.close()\\n\\t\\t\\tb = progressbar_rate(get_bar(our_file, 3))\\n\\t\\tb2 = progressbar_rate(get_bar(our_file2, 3))\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=0.5, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\tt2 = tqdm(_range(3), file=our_file, smoothing=0.5, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\tt.update()\\n\\t\\t\\tt2.close()\\n\\t\\t\\tt.close()\\n\\t\\t\\tc = progressbar_rate(get_bar(our_file, 3))\\n\\t\\tc2 = progressbar_rate(get_bar(our_file2, 3))\\n\\tassert a == c < b\\n\\tassert a2 < c2 < b2",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_smoothing():\\n\\ttimer = DiscreteTimer()\\n\\twith closing(StringIO()) as our_file:\\n\\t\\twith tqdm(_range(3), file=our_file, smoothing=None, leave=True) as t:\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\tfor _ in t:\\n\\t\\t\\t\\tpass\\n\\t\\tour_file.seek(0)\\n\\t\\tassert '| 3/3 ' in our_file.read()\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=None, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\twith tqdm(_range(3), file=our_file, smoothing=None, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0) as t2:\\n\\t\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\t\\tt.update()\\n\\t\\t\\tn_old = len(tqdm._instances)\\n\\t\\t\\tt.close()\\n\\t\\t\\tassert len(tqdm._instances) == n_old - 1\\n\\t\\t\\ta = progressbar_rate(get_bar(our_file, 3))\\n\\t\\ta2 = progressbar_rate(get_bar(our_file2, 3))\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=1, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\twith tqdm(_range(3), file=our_file, smoothing=1, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0) as t2:\\n\\t\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\t\\tt.update()\\n\\t\\t\\tt.close()\\n\\t\\t\\tb = progressbar_rate(get_bar(our_file, 3))\\n\\t\\tb2 = progressbar_rate(get_bar(our_file2, 3))\\n\\twith closing(StringIO()) as our_file2:\\n\\t\\twith closing(StringIO()) as our_file:\\n\\t\\t\\tt = tqdm(_range(3), file=our_file2, smoothing=0.5, leave=True,\\n\\t\\t\\t\\t\\t miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t, timer)\\n\\t\\t\\tt2 = tqdm(_range(3), file=our_file, smoothing=0.5, leave=True,\\n\\t\\t\\t\\t\\t  miniters=1, mininterval=0)\\n\\t\\t\\tcpu_timify(t2, timer)\\n\\t\\t\\tfor i in t2:\\n\\t\\t\\t\\tif i == 0:\\n\\t\\t\\t\\t\\ttimer.sleep(0.01)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\ttimer.sleep(0.001)\\n\\t\\t\\t\\tt.update()\\n\\t\\t\\tt2.close()\\n\\t\\t\\tt.close()\\n\\t\\t\\tc = progressbar_rate(get_bar(our_file, 3))\\n\\t\\tc2 = progressbar_rate(get_bar(our_file2, 3))\\n\\tassert a == c < b\\n\\tassert a2 < c2 < b2"
  },
  {
    "code": "def find_entry(self, entryid):\\n        if entryid == -1:  \\n            names = self.conn.listNetworks() + self.conn.listDefinedNetworks()\\n            return [self.conn.networkLookupByName(n) for n in names]\\n        try:\\n            return self.conn.networkLookupByName(entryid)\\n        except libvirt.libvirtError as e:\\n            if e.get_error_code() == libvirt.VIR_ERR_NO_NETWORK:\\n                raise EntryNotFound(\"network %s not found\" % entryid)\\n            raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def find_entry(self, entryid):\\n        if entryid == -1:  \\n            names = self.conn.listNetworks() + self.conn.listDefinedNetworks()\\n            return [self.conn.networkLookupByName(n) for n in names]\\n        try:\\n            return self.conn.networkLookupByName(entryid)\\n        except libvirt.libvirtError as e:\\n            if e.get_error_code() == libvirt.VIR_ERR_NO_NETWORK:\\n                raise EntryNotFound(\"network %s not found\" % entryid)\\n            raise"
  },
  {
    "code": "def get_operation(self, name):\\n        resp = self.connection.projects().operations().get(name=name).execute()\\n        return resp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4014] Change DatastoreHook and add tests (#4842)\\n\\n- update default used version for connecting to the Admin API from v1beta1 to v1\\n- move the establishment of the connection to the function calls instead of the hook init\\n- change get_conn signature to be able to pass an is_admin arg to set an admin connection\\n- rename GoogleCloudBaseHook._authorize function to GoogleCloudBaseHook.authorize\\n- rename the `partialKeys` argument of function `allocate_ids` to `partial_keys`.\\n- add tests\\n- update docs\\n- refactor code\\n\\nMove version attribute from get_conn to __init__\\n\\n- revert renaming of authorize function\\n- improve docs\\n- refactor code",
    "fixed_code": "def get_operation(self, name):\\n        conn = self.get_conn()\\n        resp = conn.projects().operations().get(name=name).execute()\\n        return resp"
  },
  {
    "code": "def predict_proba(self, X):\\n        Xt = X\\n        for name, transform in self.steps[:-1]:\\n            Xt = transform.transform(Xt)\\n        return self.steps[-1][-1].predict_proba(Xt)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def predict_proba(self, X):\\n        Xt = X\\n        for name, transform in self.steps[:-1]:\\n            Xt = transform.transform(Xt)\\n        return self.steps[-1][-1].predict_proba(Xt)"
  },
  {
    "code": "def _validate_dt64_dtype(dtype):\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n        if is_dtype_equal(dtype, np.dtype(\"M8\")):\\n            msg = (\\n                \"Passing in 'datetime64' dtype with no precision is not allowed. \"\\n                \"Please pass in 'datetime64[ns]' instead.\"\\n            )\\n            raise ValueError(msg)\\n        if (isinstance(dtype, np.dtype) and dtype != DT64NS_DTYPE) or not isinstance(\\n            dtype, (np.dtype, DatetimeTZDtype)\\n        ):\\n            raise ValueError(\\n                f\"Unexpected value for 'dtype': '{dtype}'. \"\\n                \"Must be 'datetime64[ns]' or DatetimeTZDtype'.\"\\n            )\\n        if getattr(dtype, \"tz\", None):\\n            dtype = cast(DatetimeTZDtype, dtype)\\n            dtype = DatetimeTZDtype(tz=timezones.tz_standardize(dtype.tz))\\n    return dtype",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: retain non-nano dtype in DatetimeArray constructor (#49058)",
    "fixed_code": "def _validate_dt64_dtype(dtype):\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n        if is_dtype_equal(dtype, np.dtype(\"M8\")):\\n            msg = (\\n                \"Passing in 'datetime64' dtype with no precision is not allowed. \"\\n                \"Please pass in 'datetime64[ns]' instead.\"\\n            )\\n            raise ValueError(msg)\\n        if (\\n            isinstance(dtype, np.dtype)\\n            and (dtype.kind != \"M\" or not is_supported_unit(get_unit_from_dtype(dtype)))\\n        ) or not isinstance(dtype, (np.dtype, DatetimeTZDtype)):\\n            raise ValueError(\\n                f\"Unexpected value for 'dtype': '{dtype}'. \"\\n                \"Must be 'datetime64[s]', 'datetime64[ms]', 'datetime64[us]', \"\\n                \"'datetime64[ns]' or DatetimeTZDtype'.\"\\n            )\\n        if getattr(dtype, \"tz\", None):\\n            dtype = cast(DatetimeTZDtype, dtype)\\n            dtype = DatetimeTZDtype(tz=timezones.tz_standardize(dtype.tz))\\n    return dtype"
  },
  {
    "code": "def _fit(self, X):\\n        X = check_array(X, dtype=[np.float64], ensure_2d=True,\\n                        copy=self.copy)\\n        if self.n_components is None:\\n            n_components = X.shape[1]\\n        else:\\n            n_components = self.n_components\\n        svd_solver = self.svd_solver\\n        if svd_solver == 'auto':\\n            if max(X.shape) <= 500:\\n                svd_solver = 'full'\\n            elif n_components >= 1 and n_components < .8 * min(X.shape):\\n                svd_solver = 'randomized'\\n            else:\\n                svd_solver = 'full'\\n        if svd_solver == 'full':\\n            return self._fit_full(X, n_components)\\n        elif svd_solver in ['arpack', 'randomized']:\\n            return self._fit_truncated(X, n_components, svd_solver)\\n    def _fit_full(self, X, n_components):\\n        n_samples, n_features = X.shape\\n        if n_components == 'mle':\\n            if n_samples < n_features:\\n                raise ValueError(\"n_components='mle' is only supported \"\\n                                 \"if n_samples >= n_features\")\\n        elif not 0 <= n_components <= n_features:\\n            raise ValueError(\"n_components=%r must be between 0 and \"\\n                             \"n_features=%r with svd_solver='full'\"\\n                             % (n_components, n_features))\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        U, S, V = linalg.svd(X, full_matrices=False)\\n        U, V = svd_flip(U, V)\\n        components_ = V\\n        explained_variance_ = (S ** 2) / n_samples\\n        total_var = explained_variance_.sum()\\n        explained_variance_ratio_ = explained_variance_ / total_var\\n        if n_components == 'mle':\\n            n_components = \\\\n                _infer_dimension_(explained_variance_, n_samples, n_features)\\n        elif 0 < n_components < 1.0:\\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\\n            n_components = np.searchsorted(ratio_cumsum, n_components) + 1\\n        if n_components < min(n_features, n_samples):\\n            self.noise_variance_ = explained_variance_[n_components:].mean()\\n        else:\\n            self.noise_variance_ = 0.\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = components_[:n_components]\\n        self.n_components_ = n_components\\n        self.explained_variance_ = explained_variance_[:n_components]\\n        self.explained_variance_ratio_ = \\\\n            explained_variance_ratio_[:n_components]\\n        return U, S, V\\n    def _fit_truncated(self, X, n_components, svd_solver):\\n        n_samples, n_features = X.shape\\n        if isinstance(n_components, six.string_types):\\n            raise ValueError(\"n_components=%r cannot be a string \"\\n                             \"with svd_solver='%s'\"\\n                             % (n_components, svd_solver))\\n        elif not 1 <= n_components <= n_features:\\n            raise ValueError(\"n_components=%r must be between 1 and \"\\n                             \"n_features=%r with svd_solver='%s'\"\\n                             % (n_components, n_features, svd_solver))\\n        elif svd_solver == 'arpack' and n_components == n_features:\\n            raise ValueError(\"n_components=%r must be stricly less than \"\\n                             \"n_features=%r with svd_solver='%s'\"\\n                             % (n_components, n_features, svd_solver))\\n        random_state = check_random_state(self.random_state)\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        if svd_solver == 'arpack':\\n            v0 = random_state.uniform(-1, 1, size=min(X.shape))\\n            U, S, V = svds(X, k=n_components, tol=self.tol, v0=v0)\\n            S = S[::-1]\\n            U, V = svd_flip(U[:, ::-1], V[::-1])\\n        elif svd_solver == 'randomized':\\n            U, S, V = randomized_svd(X, n_components=n_components,\\n                                     n_iter=self.iterated_power,\\n                                     flip_sign=True,\\n                                     random_state=random_state)\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = V\\n        self.n_components_ = n_components\\n        self.explained_variance_ = (S ** 2) / n_samples\\n        total_var = np.var(X, axis=0)\\n        self.explained_variance_ratio_ = \\\\n            self.explained_variance_ / total_var.sum()\\n        if self.n_components_ < n_features:\\n            self.noise_variance_ = (total_var.sum() -\\n                                    self.explained_variance_.sum())\\n        else:\\n            self.noise_variance_ = 0.\\n        return U, S, V",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "warning for PCA with sparse input (#7649)",
    "fixed_code": "def _fit(self, X):\\n        if issparse(X):\\n            raise TypeError('PCA does not support sparse input. See '\\n                            'TruncatedSVD for a possible alternative.')\\n        X = check_array(X, dtype=[np.float64], ensure_2d=True,\\n                        copy=self.copy)\\n        if self.n_components is None:\\n            n_components = X.shape[1]\\n        else:\\n            n_components = self.n_components\\n        svd_solver = self.svd_solver\\n        if svd_solver == 'auto':\\n            if max(X.shape) <= 500:\\n                svd_solver = 'full'\\n            elif n_components >= 1 and n_components < .8 * min(X.shape):\\n                svd_solver = 'randomized'\\n            else:\\n                svd_solver = 'full'\\n        if svd_solver == 'full':\\n            return self._fit_full(X, n_components)\\n        elif svd_solver in ['arpack', 'randomized']:\\n            return self._fit_truncated(X, n_components, svd_solver)\\n    def _fit_full(self, X, n_components):\\n        n_samples, n_features = X.shape\\n        if n_components == 'mle':\\n            if n_samples < n_features:\\n                raise ValueError(\"n_components='mle' is only supported \"\\n                                 \"if n_samples >= n_features\")\\n        elif not 0 <= n_components <= n_features:\\n            raise ValueError(\"n_components=%r must be between 0 and \"\\n                             \"n_features=%r with svd_solver='full'\"\\n                             % (n_components, n_features))\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        U, S, V = linalg.svd(X, full_matrices=False)\\n        U, V = svd_flip(U, V)\\n        components_ = V\\n        explained_variance_ = (S ** 2) / n_samples\\n        total_var = explained_variance_.sum()\\n        explained_variance_ratio_ = explained_variance_ / total_var\\n        if n_components == 'mle':\\n            n_components = \\\\n                _infer_dimension_(explained_variance_, n_samples, n_features)\\n        elif 0 < n_components < 1.0:\\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\\n            n_components = np.searchsorted(ratio_cumsum, n_components) + 1\\n        if n_components < min(n_features, n_samples):\\n            self.noise_variance_ = explained_variance_[n_components:].mean()\\n        else:\\n            self.noise_variance_ = 0.\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = components_[:n_components]\\n        self.n_components_ = n_components\\n        self.explained_variance_ = explained_variance_[:n_components]\\n        self.explained_variance_ratio_ = \\\\n            explained_variance_ratio_[:n_components]\\n        return U, S, V\\n    def _fit_truncated(self, X, n_components, svd_solver):\\n        n_samples, n_features = X.shape\\n        if isinstance(n_components, six.string_types):\\n            raise ValueError(\"n_components=%r cannot be a string \"\\n                             \"with svd_solver='%s'\"\\n                             % (n_components, svd_solver))\\n        elif not 1 <= n_components <= n_features:\\n            raise ValueError(\"n_components=%r must be between 1 and \"\\n                             \"n_features=%r with svd_solver='%s'\"\\n                             % (n_components, n_features, svd_solver))\\n        elif svd_solver == 'arpack' and n_components == n_features:\\n            raise ValueError(\"n_components=%r must be stricly less than \"\\n                             \"n_features=%r with svd_solver='%s'\"\\n                             % (n_components, n_features, svd_solver))\\n        random_state = check_random_state(self.random_state)\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        if svd_solver == 'arpack':\\n            v0 = random_state.uniform(-1, 1, size=min(X.shape))\\n            U, S, V = svds(X, k=n_components, tol=self.tol, v0=v0)\\n            S = S[::-1]\\n            U, V = svd_flip(U[:, ::-1], V[::-1])\\n        elif svd_solver == 'randomized':\\n            U, S, V = randomized_svd(X, n_components=n_components,\\n                                     n_iter=self.iterated_power,\\n                                     flip_sign=True,\\n                                     random_state=random_state)\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = V\\n        self.n_components_ = n_components\\n        self.explained_variance_ = (S ** 2) / n_samples\\n        total_var = np.var(X, axis=0)\\n        self.explained_variance_ratio_ = \\\\n            self.explained_variance_ / total_var.sum()\\n        if self.n_components_ < n_features:\\n            self.noise_variance_ = (total_var.sum() -\\n                                    self.explained_variance_.sum())\\n        else:\\n            self.noise_variance_ = 0.\\n        return U, S, V"
  },
  {
    "code": "def format_file_in_place(\\n    src: Path,\\n    line_length: int,\\n    fast: bool,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    is_pyi = src.suffix == \".pyi\"\\n    with tokenize.open(src) as src_buffer:\\n        src_contents = src_buffer.read()\\n    try:\\n        dst_contents = format_file_contents(\\n            src_contents, line_length=line_length, fast=fast, is_pyi=is_pyi\\n        )\\n    except NothingChanged:\\n        return False\\n    if write_back == write_back.YES:\\n        with open(src, \"w\", encoding=src_buffer.encoding) as f:\\n            f.write(dst_contents)\\n    elif write_back == write_back.DIFF:\\n        src_name = f\"{src}  (original)\"\\n        dst_name = f\"{src}  (formatted)\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if lock:\\n            lock.acquire()\\n        try:\\n            sys.stdout.write(diff_contents)\\n        finally:\\n            if lock:\\n                lock.release()\\n    return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_file_in_place(\\n    src: Path,\\n    line_length: int,\\n    fast: bool,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    is_pyi = src.suffix == \".pyi\"\\n    with tokenize.open(src) as src_buffer:\\n        src_contents = src_buffer.read()\\n    try:\\n        dst_contents = format_file_contents(\\n            src_contents, line_length=line_length, fast=fast, is_pyi=is_pyi\\n        )\\n    except NothingChanged:\\n        return False\\n    if write_back == write_back.YES:\\n        with open(src, \"w\", encoding=src_buffer.encoding) as f:\\n            f.write(dst_contents)\\n    elif write_back == write_back.DIFF:\\n        src_name = f\"{src}  (original)\"\\n        dst_name = f\"{src}  (formatted)\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if lock:\\n            lock.acquire()\\n        try:\\n            sys.stdout.write(diff_contents)\\n        finally:\\n            if lock:\\n                lock.release()\\n    return True"
  },
  {
    "code": "def coerce_to_array(\\n    values, mask=None, copy: bool = False\\n) -> tuple[np.ndarray, np.ndarray]:\\n    if isinstance(values, BooleanArray):\\n        if mask is not None:\\n            raise ValueError(\"cannot pass mask for BooleanArray input\")\\n        values, mask = values._data, values._mask\\n        if copy:\\n            values = values.copy()\\n            mask = mask.copy()\\n        return values, mask\\n    mask_values = None\\n    if isinstance(values, np.ndarray) and values.dtype == np.bool_:\\n        if copy:\\n            values = values.copy()\\n    elif isinstance(values, np.ndarray) and is_numeric_dtype(values.dtype):\\n        mask_values = isna(values)\\n        values_bool = np.zeros(len(values), dtype=bool)\\n        values_bool[~mask_values] = values[~mask_values].astype(bool)\\n        if not np.all(\\n            values_bool[~mask_values].astype(values.dtype) == values[~mask_values]\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n        values = values_bool\\n    else:\\n        values_object = np.asarray(values, dtype=object)\\n        inferred_dtype = lib.infer_dtype(values_object, skipna=True)\\n        integer_like = (\"floating\", \"integer\", \"mixed-integer-float\")\\n        if inferred_dtype not in (\"boolean\", \"empty\") + integer_like:\\n            raise TypeError(\"Need to pass bool-like values\")\\n        mask_values = isna(values_object)\\n        values = np.zeros(len(values), dtype=bool)\\n        values[~mask_values] = values_object[~mask_values].astype(bool)\\n        if (inferred_dtype in integer_like) and not (\\n            np.all(\\n                values[~mask_values].astype(float)\\n                == values_object[~mask_values].astype(float)\\n            )\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n    if mask is None and mask_values is None:\\n        mask = np.zeros(len(values), dtype=bool)\\n    elif mask is None:\\n        mask = mask_values\\n    else:\\n        if isinstance(mask, np.ndarray) and mask.dtype == np.bool_:\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n            else:\\n                if copy:\\n                    mask = mask.copy()\\n        else:\\n            mask = np.array(mask, dtype=bool)\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n    if values.shape != mask.shape:\\n        raise ValueError(\"values.shape and mask.shape must match\")\\n    return values, mask",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DataFrame[dt64].where downcasting (#45837)",
    "fixed_code": "def coerce_to_array(\\n    values, mask=None, copy: bool = False\\n) -> tuple[np.ndarray, np.ndarray]:\\n    if isinstance(values, BooleanArray):\\n        if mask is not None:\\n            raise ValueError(\"cannot pass mask for BooleanArray input\")\\n        values, mask = values._data, values._mask\\n        if copy:\\n            values = values.copy()\\n            mask = mask.copy()\\n        return values, mask\\n    mask_values = None\\n    if isinstance(values, np.ndarray) and values.dtype == np.bool_:\\n        if copy:\\n            values = values.copy()\\n    elif isinstance(values, np.ndarray) and is_numeric_dtype(values.dtype):\\n        mask_values = isna(values)\\n        values_bool = np.zeros(len(values), dtype=bool)\\n        values_bool[~mask_values] = values[~mask_values].astype(bool)\\n        if not np.all(\\n            values_bool[~mask_values].astype(values.dtype) == values[~mask_values]\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n        values = values_bool\\n    else:\\n        values_object = np.asarray(values, dtype=object)\\n        inferred_dtype = lib.infer_dtype(values_object, skipna=True)\\n        integer_like = (\"floating\", \"integer\", \"mixed-integer-float\")\\n        if inferred_dtype not in (\"boolean\", \"empty\") + integer_like:\\n            raise TypeError(\"Need to pass bool-like values\")\\n        mask_values = isna(values_object)\\n        values = np.zeros(len(values), dtype=bool)\\n        values[~mask_values] = values_object[~mask_values].astype(bool)\\n        if (inferred_dtype in integer_like) and not (\\n            np.all(\\n                values[~mask_values].astype(float)\\n                == values_object[~mask_values].astype(float)\\n            )\\n        ):\\n            raise TypeError(\"Need to pass bool-like values\")\\n    if mask is None and mask_values is None:\\n        mask = np.zeros(values.shape, dtype=bool)\\n    elif mask is None:\\n        mask = mask_values\\n    else:\\n        if isinstance(mask, np.ndarray) and mask.dtype == np.bool_:\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n            else:\\n                if copy:\\n                    mask = mask.copy()\\n        else:\\n            mask = np.array(mask, dtype=bool)\\n            if mask_values is not None:\\n                mask = mask | mask_values\\n    if values.shape != mask.shape:\\n        raise ValueError(\"values.shape and mask.shape must match\")\\n    return values, mask"
  },
  {
    "code": "def drop_duplicates(self, take_last=False):\\n        result = super(Index, self).drop_duplicates(take_last=take_last)\\n        return self._constructor(result)\\n    @Appender(_shared_docs['duplicated'] % _index_doc_kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: drop_duplicates drops name(s).",
    "fixed_code": "def drop_duplicates(self, take_last=False):\\n        return super(Index, self).drop_duplicates(take_last=take_last)\\n    @Appender(_shared_docs['duplicated'] % _index_doc_kwargs)"
  },
  {
    "code": "def deconstruct(self):\\n\\t\\tpath = '%s.%s' % (self.__class__.__module__, self.__class__.__name__)\\n\\t\\tif path.startswith('django.db.models.query_utils'):\\n\\t\\t\\tpath = path.replace('django.db.models.query_utils', 'django.db.models')\\n\\t\\targs, kwargs = (), {}\\n\\t\\tif len(self.children) == 1 and not isinstance(self.children[0], Q):\\n\\t\\t\\tchild = self.children[0]\\n\\t\\t\\tkwargs = {child[0]: child[1]}\\n\\t\\telse:\\n\\t\\t\\targs = tuple(self.children)\\n\\t\\t\\tif self.connector != self.default:\\n\\t\\t\\t\\tkwargs = {'_connector': self.connector}\\n\\t\\tif self.negated:\\n\\t\\t\\tkwargs['_negated'] = True\\n\\t\\treturn path, args, kwargs",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #32548 -- Fixed crash when combining Q() objects with boolean expressions.",
    "fixed_code": "def deconstruct(self):\\n\\t\\tpath = '%s.%s' % (self.__class__.__module__, self.__class__.__name__)\\n\\t\\tif path.startswith('django.db.models.query_utils'):\\n\\t\\t\\tpath = path.replace('django.db.models.query_utils', 'django.db.models')\\n\\t\\targs = tuple(self.children)\\n\\t\\tkwargs = {}\\n\\t\\tif self.connector != self.default:\\n\\t\\t\\tkwargs['_connector'] = self.connector\\n\\t\\tif self.negated:\\n\\t\\t\\tkwargs['_negated'] = True\\n\\t\\treturn path, args, kwargs"
  },
  {
    "code": "def _prepare_tzname_delta(self, tzname):\\n        tzname, sign, offset = split_tzname_delta(tzname)\\n        if offset:\\n            sign = '-' if sign == '+' else '+'\\n            return f'{tzname}{sign}{offset}'\\n        return tzname",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _prepare_tzname_delta(self, tzname):\\n        tzname, sign, offset = split_tzname_delta(tzname)\\n        if offset:\\n            sign = '-' if sign == '+' else '+'\\n            return f'{tzname}{sign}{offset}'\\n        return tzname"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\"),\\n        host=dict(required=True, type=\"str\"),\\n        password=dict(fallback=(env_fallback, [\"ANSIBLE_NET_PASSWORD\"]), no_log=True),\\n        username=dict(fallback=(env_fallback, [\"ANSIBLE_NET_USERNAME\"]), no_log=True),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\"], type=\"str\", default=\"add\"),\\n        allow_routing=dict(required=False, type=\"str\", choices=['enable', 'disable']),\\n        associated_interface=dict(required=False, type=\"str\"),\\n        cache_ttl=dict(required=False, type=\"str\"),\\n        color=dict(required=False, type=\"str\"),\\n        comment=dict(required=False, type=\"str\"),\\n        country=dict(required=False, type=\"str\"),\\n        fqdn=dict(required=False, type=\"str\"),\\n        name=dict(required=False, type=\"str\"),\\n        start_ip=dict(required=False, type=\"str\"),\\n        end_ip=dict(required=False, type=\"str\"),\\n        ipv4=dict(required=False, type=\"str\", choices=['ipmask', 'iprange', 'fqdn', 'wildcard',\\n                                                       'geography', 'wildcard-fqdn', 'group']),\\n        visibility=dict(required=False, type=\"str\", choices=['enable', 'disable']),\\n        wildcard=dict(required=False, type=\"str\"),\\n        wildcard_fqdn=dict(required=False, type=\"str\"),\\n        ipv6=dict(required=False, type=\"str\", choices=['ip', 'iprange', 'group']),\\n        group_members=dict(required=False, type=\"str\"),\\n        group_name=dict(required=False, type=\"str\"),\\n        ipv4addr=dict(required=False, type=\"str\"),\\n        ipv6addr=dict(required=False, type=\"str\"),\\n        multicast=dict(required=False, type=\"str\", choices=['multicastrange', 'broadcastmask', 'ip6']),\\n        obj_id=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec, supports_check_mode=False, )\\n    host = module.params[\"host\"]\\n    password = module.params[\"password\"]\\n    username = module.params[\"username\"]\\n    if host is None or username is None:\\n        module.fail_json(msg=\"Host and username are required\")\\n    fmg = AnsibleFortiManager(module, module.params[\"host\"], module.params[\"username\"], module.params[\"password\"])\\n    try:\\n        response = fmg.login()\\n        if response[1]['status']['code'] != 0:\\n            module.fail_json(msg=\"Connection to FortiManager Failed\")\\n    except Exception:\\n        module.fail_json(msg=\"Connection to FortiManager Failed\")\\n    else:\\n        paramgram = {\\n            \"adom\": module.params[\"adom\"],\\n            \"allow-routing\": module.params[\"allow_routing\"],\\n            \"associated-interface\": module.params[\"associated_interface\"],\\n            \"cache-ttl\": module.params[\"cache_ttl\"],\\n            \"color\": module.params[\"color\"],\\n            \"comment\": module.params[\"comment\"],\\n            \"country\": module.params[\"country\"],\\n            \"end-ip\": module.params[\"end_ip\"],\\n            \"fqdn\": module.params[\"fqdn\"],\\n            \"name\": module.params[\"name\"],\\n            \"start-ip\": module.params[\"start_ip\"],\\n            \"visibility\": module.params[\"visibility\"],\\n            \"wildcard\": module.params[\"wildcard\"],\\n            \"wildcard-fqdn\": module.params[\"wildcard_fqdn\"],\\n            \"ipv6\": module.params[\"ipv6\"],\\n            \"ipv4\": module.params[\"ipv4\"],\\n            \"group_members\": module.params[\"group_members\"],\\n            \"group_name\": module.params[\"group_name\"],\\n            \"ipv4addr\": module.params[\"ipv4addr\"],\\n            \"ipv6addr\": module.params[\"ipv6addr\"],\\n            \"multicast\": module.params[\"multicast\"],\\n            \"mode\": module.params[\"mode\"],\\n            \"obj-id\": module.params[\"obj_id\"],\\n        }\\n        if paramgram[\"adom\"] is None:\\n            paramgram[\"adom\"] = \"root\"\\n        if paramgram[\"mode\"] is None:\\n            paramgram[\"mode\"] = \"add\"\\n        if paramgram[\"color\"] is None:\\n            paramgram[\"color\"] = 22\\n        if paramgram[\"comment\"] is None:\\n            paramgram[\"comment\"] = \"Created by Ansible\"\\n        if paramgram[\"allow-routing\"] is None:\\n            paramgram[\"allow-routing\"] = \"disable\"\\n        if paramgram[\"visibility\"] is None:\\n            paramgram[\"visibility\"] = \"enable\"\\n        if paramgram[\"ipv4\"] is not None and paramgram[\"ipv6\"] is None and paramgram[\"multicast\"] is None:\\n            results = fmgr_fwobj_ipv4(fmg, paramgram)\\n            fmgr_logout(fmg, module, results=results, good_codes=[0, -2, -3])\\n        if paramgram[\"ipv4\"] is None and paramgram[\"ipv6\"] is not None and paramgram[\"multicast\"] is None:\\n            results = fmgr_fwobj_ipv6(fmg, paramgram)\\n            if results[0] not in [0, -2, -3]:\\n                module.fail_json(msg=\"Failed to process IPv6 Object\", **results[1])\\n        if paramgram[\"ipv4\"] is None and paramgram[\"ipv6\"] is None and paramgram[\"multicast\"] is not None:\\n            results = fmgr_fwobj_multicast(fmg, paramgram)\\n            if results[0] not in [0, -2, -3]:\\n                module.fail_json(msg=\"Failed to process Multicast Object\", **results[1])\\n    fmg.logout()\\n    if results is not None:\\n        return module.exit_json(**results[1])\\n    else:\\n        return module.exit_json(msg=\"Couldn't find a proper ipv4 or ipv6 or multicast parameter \"\\n                                    \"to run in the logic tree. Exiting...\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_fwobj_address (#52773)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\"], type=\"str\", default=\"add\"),\\n        allow_routing=dict(required=False, type=\"str\", choices=['enable', 'disable'], default=\"disable\"),\\n        associated_interface=dict(required=False, type=\"str\"),\\n        cache_ttl=dict(required=False, type=\"str\"),\\n        color=dict(required=False, type=\"str\", default=22),\\n        comment=dict(required=False, type=\"str\"),\\n        country=dict(required=False, type=\"str\"),\\n        fqdn=dict(required=False, type=\"str\"),\\n        name=dict(required=False, type=\"str\"),\\n        start_ip=dict(required=False, type=\"str\"),\\n        end_ip=dict(required=False, type=\"str\"),\\n        ipv4=dict(required=False, type=\"str\", choices=['ipmask', 'iprange', 'fqdn', 'wildcard',\\n                                                       'geography', 'wildcard-fqdn', 'group']),\\n        visibility=dict(required=False, type=\"str\", choices=['enable', 'disable'], default=\"enable\"),\\n        wildcard=dict(required=False, type=\"str\"),\\n        wildcard_fqdn=dict(required=False, type=\"str\"),\\n        ipv6=dict(required=False, type=\"str\", choices=['ip', 'iprange', 'group']),\\n        group_members=dict(required=False, type=\"str\"),\\n        group_name=dict(required=False, type=\"str\"),\\n        ipv4addr=dict(required=False, type=\"str\"),\\n        ipv6addr=dict(required=False, type=\"str\"),\\n        multicast=dict(required=False, type=\"str\", choices=['multicastrange', 'broadcastmask', 'ip6']),\\n        obj_id=dict(required=False, type=\"str\"),\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False,\\n                           mutually_exclusive=[\\n                               ['ipv4', 'ipv6'],\\n                               ['ipv4', 'multicast'],\\n                               ['ipv6', 'multicast']\\n                           ])\\n    paramgram = {\\n        \"adom\": module.params[\"adom\"],\\n        \"allow-routing\": module.params[\"allow_routing\"],\\n        \"associated-interface\": module.params[\"associated_interface\"],\\n        \"cache-ttl\": module.params[\"cache_ttl\"],\\n        \"color\": module.params[\"color\"],\\n        \"comment\": module.params[\"comment\"],\\n        \"country\": module.params[\"country\"],\\n        \"end-ip\": module.params[\"end_ip\"],\\n        \"fqdn\": module.params[\"fqdn\"],\\n        \"name\": module.params[\"name\"],\\n        \"start-ip\": module.params[\"start_ip\"],\\n        \"visibility\": module.params[\"visibility\"],\\n        \"wildcard\": module.params[\"wildcard\"],\\n        \"wildcard-fqdn\": module.params[\"wildcard_fqdn\"],\\n        \"ipv6\": module.params[\"ipv6\"],\\n        \"ipv4\": module.params[\"ipv4\"],\\n        \"group_members\": module.params[\"group_members\"],\\n        \"group_name\": module.params[\"group_name\"],\\n        \"ipv4addr\": module.params[\"ipv4addr\"],\\n        \"ipv6addr\": module.params[\"ipv6addr\"],\\n        \"multicast\": module.params[\"multicast\"],\\n        \"mode\": module.params[\"mode\"],\\n        \"obj-id\": module.params[\"obj_id\"],\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr._tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"ipv4\"]:\\n            results = fmgr_fwobj_ipv4(fmgr, paramgram)\\n        elif paramgram[\"ipv6\"]:\\n            results = fmgr_fwobj_ipv6(fmgr, paramgram)\\n        elif paramgram[\"multicast\"]:\\n            results = fmgr_fwobj_multicast(fmgr, paramgram)\\n        fmgr.govern_response(module=module, results=results,\\n                             ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    if results is not None:\\n        return module.exit_json(**results[1])\\n    else:\\n        return module.exit_json(msg=\"Couldn't find a proper ipv4 or ipv6 or multicast parameter \"\\n                                    \"to run in the logic tree. Exiting...\")"
  },
  {
    "code": "def _load_collections(self, attr, ds):\\n        ds = self.get_validated_value('collections', self._collections, ds, None)\\n        _ensure_default_collection(collection_list=ds)\\n        if not ds:  \\n            return None\\n        env = Environment()\\n        for collection_name in ds:\\n            if is_template(collection_name, env):\\n                display.warning('\"collections\" is not templatable, but we found: %s, '\\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\\n        return ds",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _load_collections(self, attr, ds):\\n        ds = self.get_validated_value('collections', self._collections, ds, None)\\n        _ensure_default_collection(collection_list=ds)\\n        if not ds:  \\n            return None\\n        env = Environment()\\n        for collection_name in ds:\\n            if is_template(collection_name, env):\\n                display.warning('\"collections\" is not templatable, but we found: %s, '\\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\\n        return ds"
  },
  {
    "code": "def setdiff1d(ar1, ar2, assume_unique=False):\\n    aux = in1d(ar1, ar2, assume_unique=assume_unique)\\n    if aux.size == 0:\\n        return aux\\n    else:\\n        return np.asarray(ar1)[aux == 0]\\n@deprecate_with_doc('')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix to setdiff1d (and masked version) + tests (#1133, by N.C.)",
    "fixed_code": "def setdiff1d(ar1, ar2, assume_unique=False):\\n    if not assume_unique:\\n        ar1 = unique(ar1)\\n        ar2 = unique(ar2)\\n    aux = in1d(ar1, ar2, assume_unique=True)\\n    if aux.size == 0:\\n        return aux\\n    else:\\n        return np.asarray(ar1)[aux == 0]\\n@deprecate_with_doc('')"
  },
  {
    "code": "def __init__(self, executor=None, heartrate=None, *args, **kwargs):\\n        self.hostname = get_hostname()\\n        if executor:\\n            self.executor = executor\\n            self.executor_class = executor.__class__.__name__\\n        else:\\n            self.executor_class = conf.get('core', 'EXECUTOR')\\n        self.start_date = timezone.utcnow()\\n        self.latest_heartbeat = timezone.utcnow()\\n        if heartrate is not None:\\n            self.heartrate = heartrate\\n        self.unixname = getuser()\\n        self.max_tis_per_query = conf.getint('scheduler', 'max_tis_per_query')\\n        super().__init__(*args, **kwargs)\\n    @cached_property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, executor=None, heartrate=None, *args, **kwargs):\\n        self.hostname = get_hostname()\\n        if executor:\\n            self.executor = executor\\n            self.executor_class = executor.__class__.__name__\\n        else:\\n            self.executor_class = conf.get('core', 'EXECUTOR')\\n        self.start_date = timezone.utcnow()\\n        self.latest_heartbeat = timezone.utcnow()\\n        if heartrate is not None:\\n            self.heartrate = heartrate\\n        self.unixname = getuser()\\n        self.max_tis_per_query = conf.getint('scheduler', 'max_tis_per_query')\\n        super().__init__(*args, **kwargs)\\n    @cached_property"
  },
  {
    "code": "def match_tag_start(self):\\n\\t\\tmatch = self.match(\\n\\t\\t\\tr,\\n\\t\\t\\tre.I | re.S | re.X,\\n\\t\\t)\\n\\t\\tif not match:\\n\\t\\t\\treturn False\\n\\t\\tkeyword, attr, isend = match.groups()\\n\\t\\tself.keyword = keyword\\n\\t\\tattributes = {}\\n\\t\\tif attr:\\n\\t\\t\\tfor att in re.findall(\\n\\t\\t\\t\\tr\"\\s*(\\w+)\\s*=\\s*(?:'([^']*)'|\\\"([^\\\"]*)\\\")\", attr\\n\\t\\t\\t):\\n\\t\\t\\t\\tkey, val1, val2 = att\\n\\t\\t\\t\\ttext = val1 or val2\\n\\t\\t\\t\\ttext = text.replace(\"\\r\\n\", \"\\n\")\\n\\t\\t\\t\\tattributes[key] = text\\n\\t\\tself.append_node(parsetree.Tag, keyword, attributes)\\n\\t\\tif isend:\\n\\t\\t\\tself.tag.pop()\\n\\t\\telif keyword == \"text\":\\n\\t\\t\\tmatch = self.match(r\"(.*?)(?=\\</%text>)\", re.S)\\n\\t\\t\\tif not match:\\n\\t\\t\\t\\traise exceptions.SyntaxException(\\n\\t\\t\\t\\t\\t\"Unclosed tag: <%%%s>\" % self.tag[-1].keyword,\\n\\t\\t\\t\\t\\t**self.exception_kwargs,\\n\\t\\t\\t\\t)\\n\\t\\t\\tself.append_node(parsetree.Text, match.group(1))\\n\\t\\t\\treturn self.match_tag_end()\\n\\t\\treturn True",
    "label": 1,
    "bug_type": "security",
    "bug_description": "fix tag regexp to match quoted groups correctly\\n\\nFixed issue in lexer where the regexp used to match tags would not\\ncorrectly interpret quoted sections individually. While this parsing issue\\nstill produced the same expected tag structure later on, the mis-handling\\nof quoted sections was also subject to a regexp crash if a tag had a large\\nnumber of quotes within its quoted sections.\\n\\nFixes: #366",
    "fixed_code": "def match_tag_start(self):\\n\\t\\treg = r\\n\\t\\tmatch = self.match(\\n\\t\\t\\treg,\\n\\t\\t\\tre.I | re.S | re.X,\\n\\t\\t)\\n\\t\\tif not match:\\n\\t\\t\\treturn False\\n\\t\\tkeyword, attr, isend = match.groups()\\n\\t\\tself.keyword = keyword\\n\\t\\tattributes = {}\\n\\t\\tif attr:\\n\\t\\t\\tfor att in re.findall(\\n\\t\\t\\t\\tr\"\\s*(\\w+)\\s*=\\s*(?:'([^']*)'|\\\"([^\\\"]*)\\\")\", attr\\n\\t\\t\\t):\\n\\t\\t\\t\\tkey, val1, val2 = att\\n\\t\\t\\t\\ttext = val1 or val2\\n\\t\\t\\t\\ttext = text.replace(\"\\r\\n\", \"\\n\")\\n\\t\\t\\t\\tattributes[key] = text\\n\\t\\tself.append_node(parsetree.Tag, keyword, attributes)\\n\\t\\tif isend:\\n\\t\\t\\tself.tag.pop()\\n\\t\\telif keyword == \"text\":\\n\\t\\t\\tmatch = self.match(r\"(.*?)(?=\\</%text>)\", re.S)\\n\\t\\t\\tif not match:\\n\\t\\t\\t\\traise exceptions.SyntaxException(\\n\\t\\t\\t\\t\\t\"Unclosed tag: <%%%s>\" % self.tag[-1].keyword,\\n\\t\\t\\t\\t\\t**self.exception_kwargs,\\n\\t\\t\\t\\t)\\n\\t\\t\\tself.append_node(parsetree.Text, match.group(1))\\n\\t\\t\\treturn self.match_tag_end()\\n\\t\\treturn True"
  },
  {
    "code": "def operator_extra_link_dict(self):\\n        return {link.name: link for link in self.operator_extra_links}\\n    @cached_property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5636] Allow adding or overriding existing Operator Links (#6302)",
    "fixed_code": "def operator_extra_link_dict(self):\\n        from airflow.plugins_manager import operator_extra_links\\n        op_extra_links_from_plugin = {}\\n        for ope in operator_extra_links:\\n            if ope.operators and self.__class__ in ope.operators:\\n                op_extra_links_from_plugin.update({ope.name: ope})\\n        operator_extra_links_all = {\\n            link.name: link for link in self.operator_extra_links\\n        }\\n        operator_extra_links_all.update(op_extra_links_from_plugin)\\n        return operator_extra_links_all\\n    @cached_property"
  },
  {
    "code": "def get_serialized_fields(cls):\\n        if not cls.__serialized_fields:\\n            cls.__serialized_fields = frozenset(\\n                vars(BaseOperator(task_id='test')).keys()\\n                - {\\n                    'inlets',\\n                    'outlets',\\n                    '_upstream_task_ids',\\n                    'default_args',\\n                    'dag',\\n                    '_dag',\\n                    '_BaseOperator__instantiated',\\n                }\\n                | {\\n                    '_task_type',\\n                    'subdag',\\n                    'ui_color',\\n                    'ui_fgcolor',\\n                    'template_fields',\\n                    'template_fields_renderers',\\n                }\\n            )\\n        return cls.__serialized_fields",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix Dag Serialization crash caused by preset DagContext (#12530)",
    "fixed_code": "def get_serialized_fields(cls):\\n        if not cls.__serialized_fields:\\n            from airflow.models.dag import DagContext\\n            DagContext.push_context_managed_dag(None)\\n            cls.__serialized_fields = frozenset(\\n                vars(BaseOperator(task_id='test')).keys()\\n                - {\\n                    'inlets',\\n                    'outlets',\\n                    '_upstream_task_ids',\\n                    'default_args',\\n                    'dag',\\n                    '_dag',\\n                    '_BaseOperator__instantiated',\\n                }\\n                | {\\n                    '_task_type',\\n                    'subdag',\\n                    'ui_color',\\n                    'ui_fgcolor',\\n                    'template_fields',\\n                    'template_fields_renderers',\\n                }\\n            )\\n            DagContext.pop_context_managed_dag()\\n        return cls.__serialized_fields"
  },
  {
    "code": "def prepare_test_settings(self, alias):\\n        try:\\n            conn = self.databases[alias]\\n        except KeyError:\\n            raise ConnectionDoesNotExist(\"The connection %s doesn't exist\" % alias)\\n        test_dict_set = 'TEST' in conn\\n        test_settings = conn.setdefault('TEST', {})\\n        old_test_settings = {}\\n        for key, value in six.iteritems(conn):\\n            if key.startswith('TEST_'):\\n                new_key = key[5:]\\n                new_key = self.TEST_SETTING_RENAMES.get(new_key, new_key)\\n                old_test_settings[new_key] = value\\n        if old_test_settings:\\n            if test_dict_set:\\n                if test_settings != old_test_settings:\\n                    raise ImproperlyConfigured(\\n                        \"Connection '%s' has mismatched TEST and TEST_* \"\\n                        \"database settings.\" % alias)\\n            else:\\n                test_settings = old_test_settings\\n                for key, _ in six.iteritems(old_test_settings):\\n                    warnings.warn(\"In Django 1.9 the %s connection setting will be moved \"\\n                                  \"to a %s entry in the TEST setting\" %\\n                                  (self.TEST_SETTING_RENAMES_REVERSE.get(key, key), key),\\n                                  RemovedInDjango19Warning, stacklevel=2)\\n        for key in list(conn.keys()):\\n            if key.startswith('TEST_'):\\n                del conn[key]\\n        for key, new_key in six.iteritems(self.TEST_SETTING_RENAMES):\\n            if key in test_settings:\\n                warnings.warn(\"Test setting %s was renamed to %s; specified value (%s) ignored\" %\\n                              (key, new_key, test_settings[key]), stacklevel=2)\\n        for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']:\\n            test_settings.setdefault(key, None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def prepare_test_settings(self, alias):\\n        try:\\n            conn = self.databases[alias]\\n        except KeyError:\\n            raise ConnectionDoesNotExist(\"The connection %s doesn't exist\" % alias)\\n        test_dict_set = 'TEST' in conn\\n        test_settings = conn.setdefault('TEST', {})\\n        old_test_settings = {}\\n        for key, value in six.iteritems(conn):\\n            if key.startswith('TEST_'):\\n                new_key = key[5:]\\n                new_key = self.TEST_SETTING_RENAMES.get(new_key, new_key)\\n                old_test_settings[new_key] = value\\n        if old_test_settings:\\n            if test_dict_set:\\n                if test_settings != old_test_settings:\\n                    raise ImproperlyConfigured(\\n                        \"Connection '%s' has mismatched TEST and TEST_* \"\\n                        \"database settings.\" % alias)\\n            else:\\n                test_settings = old_test_settings\\n                for key, _ in six.iteritems(old_test_settings):\\n                    warnings.warn(\"In Django 1.9 the %s connection setting will be moved \"\\n                                  \"to a %s entry in the TEST setting\" %\\n                                  (self.TEST_SETTING_RENAMES_REVERSE.get(key, key), key),\\n                                  RemovedInDjango19Warning, stacklevel=2)\\n        for key in list(conn.keys()):\\n            if key.startswith('TEST_'):\\n                del conn[key]\\n        for key, new_key in six.iteritems(self.TEST_SETTING_RENAMES):\\n            if key in test_settings:\\n                warnings.warn(\"Test setting %s was renamed to %s; specified value (%s) ignored\" %\\n                              (key, new_key, test_settings[key]), stacklevel=2)\\n        for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']:\\n            test_settings.setdefault(key, None)"
  },
  {
    "code": "def urlopen(url, data=None, proxies=None):\\n    from warnings import warnpy3k\\n    warnings.warnpy3k(\"urllib.urlopen() has been removed in Python 3.0 in \"\\n                        \"favor of urllib2.urlopen()\", stacklevel=2)\\n    global _urlopener\\n    if proxies is not None:\\n        opener = FancyURLopener(proxies=proxies)\\n    elif not _urlopener:\\n        opener = FancyURLopener()\\n        _urlopener = opener\\n    else:\\n        opener = _urlopener\\n    if data is None:\\n        return opener.open(url)\\n    else:\\n        return opener.open(url, data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def urlopen(url, data=None, proxies=None):\\n    from warnings import warnpy3k\\n    warnings.warnpy3k(\"urllib.urlopen() has been removed in Python 3.0 in \"\\n                        \"favor of urllib2.urlopen()\", stacklevel=2)\\n    global _urlopener\\n    if proxies is not None:\\n        opener = FancyURLopener(proxies=proxies)\\n    elif not _urlopener:\\n        opener = FancyURLopener()\\n        _urlopener = opener\\n    else:\\n        opener = _urlopener\\n    if data is None:\\n        return opener.open(url)\\n    else:\\n        return opener.open(url, data)"
  },
  {
    "code": "def hash_params(params):\\n    if isinstance(params, Container) and not isinstance(params, (text_type, binary_type)):\\n        if isinstance(params, Mapping):\\n            try:\\n                new_params = frozenset(params.items())\\n            except TypeError:\\n                new_params = set()\\n                for k, v in params.items():\\n                    new_params.add((k, hash_params(v)))\\n                new_params = frozenset(new_params)\\n        elif isinstance(params, (Set, Sequence)):\\n            try:\\n                new_params = frozenset(params)\\n            except TypeError:\\n                new_params = set()\\n                for v in params:\\n                    new_params.add(hash_params(v))\\n                new_params = frozenset(new_params)\\n        else:\\n            new_params = frozenset(params)\\n        return new_params\\n    return frozenset((params,))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def hash_params(params):\\n    if isinstance(params, Container) and not isinstance(params, (text_type, binary_type)):\\n        if isinstance(params, Mapping):\\n            try:\\n                new_params = frozenset(params.items())\\n            except TypeError:\\n                new_params = set()\\n                for k, v in params.items():\\n                    new_params.add((k, hash_params(v)))\\n                new_params = frozenset(new_params)\\n        elif isinstance(params, (Set, Sequence)):\\n            try:\\n                new_params = frozenset(params)\\n            except TypeError:\\n                new_params = set()\\n                for v in params:\\n                    new_params.add(hash_params(v))\\n                new_params = frozenset(new_params)\\n        else:\\n            new_params = frozenset(params)\\n        return new_params\\n    return frozenset((params,))"
  },
  {
    "code": "def _get_level_lengths(\\n    index: Index,\\n    sparsify: bool,\\n    max_index: int,\\n    hidden_elements: Sequence[int] | None = None,\\n):\\n    if isinstance(index, MultiIndex):\\n        levels = index.format(sparsify=lib.no_default, adjoin=False)\\n    else:\\n        levels = index.format()\\n    if hidden_elements is None:\\n        hidden_elements = []\\n    lengths = {}\\n    if not isinstance(index, MultiIndex):\\n        for i, value in enumerate(levels):\\n            if i not in hidden_elements:\\n                lengths[(0, i)] = 1\\n        return lengths\\n    for i, lvl in enumerate(levels):\\n        visible_row_count = 0  \\n        for j, row in enumerate(lvl):\\n            if visible_row_count > max_index:\\n                break\\n            if not sparsify:\\n                if j not in hidden_elements:\\n                    lengths[(i, j)] = 1\\n                    visible_row_count += 1\\n            elif (row is not lib.no_default) and (j not in hidden_elements):\\n                last_label = j\\n                lengths[(i, last_label)] = 1\\n                visible_row_count += 1\\n            elif row is not lib.no_default:\\n                last_label = j\\n                lengths[(i, last_label)] = 0\\n            elif j not in hidden_elements:\\n                visible_row_count += 1\\n                if visible_row_count > max_index:\\n                    break  \\n                if lengths[(i, last_label)] == 0:\\n                    last_label = j\\n                    lengths[(i, last_label)] = 1\\n                else:\\n                    lengths[(i, last_label)] += 1\\n    non_zero_lengths = {\\n        element: length for element, length in lengths.items() if length >= 1\\n    }\\n    return non_zero_lengths",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_level_lengths(\\n    index: Index,\\n    sparsify: bool,\\n    max_index: int,\\n    hidden_elements: Sequence[int] | None = None,\\n):\\n    if isinstance(index, MultiIndex):\\n        levels = index.format(sparsify=lib.no_default, adjoin=False)\\n    else:\\n        levels = index.format()\\n    if hidden_elements is None:\\n        hidden_elements = []\\n    lengths = {}\\n    if not isinstance(index, MultiIndex):\\n        for i, value in enumerate(levels):\\n            if i not in hidden_elements:\\n                lengths[(0, i)] = 1\\n        return lengths\\n    for i, lvl in enumerate(levels):\\n        visible_row_count = 0  \\n        for j, row in enumerate(lvl):\\n            if visible_row_count > max_index:\\n                break\\n            if not sparsify:\\n                if j not in hidden_elements:\\n                    lengths[(i, j)] = 1\\n                    visible_row_count += 1\\n            elif (row is not lib.no_default) and (j not in hidden_elements):\\n                last_label = j\\n                lengths[(i, last_label)] = 1\\n                visible_row_count += 1\\n            elif row is not lib.no_default:\\n                last_label = j\\n                lengths[(i, last_label)] = 0\\n            elif j not in hidden_elements:\\n                visible_row_count += 1\\n                if visible_row_count > max_index:\\n                    break  \\n                if lengths[(i, last_label)] == 0:\\n                    last_label = j\\n                    lengths[(i, last_label)] = 1\\n                else:\\n                    lengths[(i, last_label)] += 1\\n    non_zero_lengths = {\\n        element: length for element, length in lengths.items() if length >= 1\\n    }\\n    return non_zero_lengths"
  },
  {
    "code": "def getcontext(_local=local):\\n    try:\\n        return _local.__decimal_context__\\n    except AttributeError:\\n        context = Context()\\n        _local.__decimal_context__ = context\\n        return context",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getcontext(_local=local):\\n    try:\\n        return _local.__decimal_context__\\n    except AttributeError:\\n        context = Context()\\n        _local.__decimal_context__ = context\\n        return context"
  },
  {
    "code": "def label_for_field(name, model, model_admin=None, return_attr=False):\\n    attr = None\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n        try:\\n            label = field.verbose_name\\n        except AttributeError:\\n            label = field.related_model._meta.verbose_name\\n    except FieldDoesNotExist:\\n        if name == \"__unicode__\":\\n            label = force_text(model._meta.verbose_name)\\n            attr = six.text_type\\n        elif name == \"__str__\":\\n            label = force_str(model._meta.verbose_name)\\n            attr = bytes\\n        else:\\n            if callable(name):\\n                attr = name\\n            elif model_admin is not None and hasattr(model_admin, name):\\n                attr = getattr(model_admin, name)\\n            elif hasattr(model, name):\\n                attr = getattr(model, name)\\n            else:\\n                message = \"Unable to lookup '%s' on %s\" % (name, model._meta.object_name)\\n                if model_admin:\\n                    message += \" or %s\" % (model_admin.__class__.__name__,)\\n                raise AttributeError(message)\\n            if hasattr(attr, \"short_description\"):\\n                label = attr.short_description\\n            elif (isinstance(attr, property) and\\n                  hasattr(attr, \"fget\") and\\n                  hasattr(attr.fget, \"short_description\")):\\n                label = attr.fget.short_description\\n            elif callable(attr):\\n                if attr.__name__ == \"<lambda>\":\\n                    label = \"--\"\\n                else:\\n                    label = pretty_name(attr.__name__)\\n            else:\\n                label = pretty_name(name)\\n    except FieldIsAForeignKeyColumnName:\\n        label = pretty_name(name)\\n        attr = name\\n    if return_attr:\\n        return (label, attr)\\n    else:\\n        return label",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def label_for_field(name, model, model_admin=None, return_attr=False):\\n    attr = None\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n        try:\\n            label = field.verbose_name\\n        except AttributeError:\\n            label = field.related_model._meta.verbose_name\\n    except FieldDoesNotExist:\\n        if name == \"__unicode__\":\\n            label = force_text(model._meta.verbose_name)\\n            attr = six.text_type\\n        elif name == \"__str__\":\\n            label = force_str(model._meta.verbose_name)\\n            attr = bytes\\n        else:\\n            if callable(name):\\n                attr = name\\n            elif model_admin is not None and hasattr(model_admin, name):\\n                attr = getattr(model_admin, name)\\n            elif hasattr(model, name):\\n                attr = getattr(model, name)\\n            else:\\n                message = \"Unable to lookup '%s' on %s\" % (name, model._meta.object_name)\\n                if model_admin:\\n                    message += \" or %s\" % (model_admin.__class__.__name__,)\\n                raise AttributeError(message)\\n            if hasattr(attr, \"short_description\"):\\n                label = attr.short_description\\n            elif (isinstance(attr, property) and\\n                  hasattr(attr, \"fget\") and\\n                  hasattr(attr.fget, \"short_description\")):\\n                label = attr.fget.short_description\\n            elif callable(attr):\\n                if attr.__name__ == \"<lambda>\":\\n                    label = \"--\"\\n                else:\\n                    label = pretty_name(attr.__name__)\\n            else:\\n                label = pretty_name(name)\\n    except FieldIsAForeignKeyColumnName:\\n        label = pretty_name(name)\\n        attr = name\\n    if return_attr:\\n        return (label, attr)\\n    else:\\n        return label"
  },
  {
    "code": "def __init__(self, n_neighbors=5, n_components=2, eigen_solver='auto',\\n            tol=0, max_iter=None, path_method='auto',\\n            neighbors_algorithm='auto', out_dim=None):\\n        if not out_dim is None:\\n            warnings.warn(\"Parameter ``out_dim`` was renamed to \"\\n                \"``n_components`` and is now deprecated.\", DeprecationWarning)\\n            n_components = n_components\\n        self.n_neighbors = n_neighbors\\n        self.n_components = n_components\\n        self.eigen_solver = eigen_solver\\n        self.tol = tol\\n        self.max_iter = max_iter\\n        self.path_method = path_method\\n        self.neighbors_algorithm = neighbors_algorithm\\n        self.nbrs_ = NearestNeighbors(n_neighbors=n_neighbors,\\n                                      algorithm=neighbors_algorithm)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, n_neighbors=5, n_components=2, eigen_solver='auto',\\n            tol=0, max_iter=None, path_method='auto',\\n            neighbors_algorithm='auto', out_dim=None):\\n        if not out_dim is None:\\n            warnings.warn(\"Parameter ``out_dim`` was renamed to \"\\n                \"``n_components`` and is now deprecated.\", DeprecationWarning)\\n            n_components = n_components\\n        self.n_neighbors = n_neighbors\\n        self.n_components = n_components\\n        self.eigen_solver = eigen_solver\\n        self.tol = tol\\n        self.max_iter = max_iter\\n        self.path_method = path_method\\n        self.neighbors_algorithm = neighbors_algorithm\\n        self.nbrs_ = NearestNeighbors(n_neighbors=n_neighbors,\\n                                      algorithm=neighbors_algorithm)"
  },
  {
    "code": "def curselection(self):\\n        return self._getints(self.tk.call(self._w, 'curselection')) or ()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def curselection(self):\\n        return self._getints(self.tk.call(self._w, 'curselection')) or ()"
  },
  {
    "code": "def read_excel(\\n    io,\\n    sheet_name: str | int = ...,\\n    *,\\n    header: int | Sequence[int] | None = ...,\\n    names: list[str] | None = ...,\\n    index_col: int | Sequence[int] | None = ...,\\n    usecols: int\\n    | str\\n    | Sequence[int]\\n    | Sequence[str]\\n    | Callable[[str], bool]\\n    | None = ...,\\n    squeeze: bool | None = ...,\\n    dtype: DtypeArg | None = ...,\\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\\n    converters: dict[str, Callable] | dict[int, Callable] | None = ...,\\n    true_values: Iterable[Hashable] | None = ...,\\n    false_values: Iterable[Hashable] | None = ...,\\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\\n    nrows: int | None = ...,\\n    na_values=...,\\n    keep_default_na: bool = ...,\\n    na_filter: bool = ...,\\n    verbose: bool = ...,\\n    parse_dates: list | dict | bool = ...,\\n    date_parser: Callable | None = ...,\\n    thousands: str | None = ...,\\n    decimal: str = ...,\\n    comment: str | None = ...,\\n    skipfooter: int = ...,\\n    storage_options: StorageOptions = ...,\\n) -> DataFrame:\\n    ...",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_excel(\\n    io,\\n    sheet_name: str | int = ...,\\n    *,\\n    header: int | Sequence[int] | None = ...,\\n    names: list[str] | None = ...,\\n    index_col: int | Sequence[int] | None = ...,\\n    usecols: int\\n    | str\\n    | Sequence[int]\\n    | Sequence[str]\\n    | Callable[[str], bool]\\n    | None = ...,\\n    squeeze: bool | None = ...,\\n    dtype: DtypeArg | None = ...,\\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\\n    converters: dict[str, Callable] | dict[int, Callable] | None = ...,\\n    true_values: Iterable[Hashable] | None = ...,\\n    false_values: Iterable[Hashable] | None = ...,\\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\\n    nrows: int | None = ...,\\n    na_values=...,\\n    keep_default_na: bool = ...,\\n    na_filter: bool = ...,\\n    verbose: bool = ...,\\n    parse_dates: list | dict | bool = ...,\\n    date_parser: Callable | None = ...,\\n    thousands: str | None = ...,\\n    decimal: str = ...,\\n    comment: str | None = ...,\\n    skipfooter: int = ...,\\n    storage_options: StorageOptions = ...,\\n) -> DataFrame:\\n    ..."
  },
  {
    "code": "def extend_mode(self, mode: str) -> str:\\n        mode = mode.replace(\"b\", \"\")\\n        if mode != \"w\":\\n            return mode\\n        if self.name is not None:\\n            suffix = Path(self.name).suffix\\n            if suffix in (\".gz\", \".xz\", \".bz2\"):\\n                mode = f\"{mode}:{suffix[1:]}\"\\n        return mode",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def extend_mode(self, mode: str) -> str:\\n        mode = mode.replace(\"b\", \"\")\\n        if mode != \"w\":\\n            return mode\\n        if self.name is not None:\\n            suffix = Path(self.name).suffix\\n            if suffix in (\".gz\", \".xz\", \".bz2\"):\\n                mode = f\"{mode}:{suffix[1:]}\"\\n        return mode"
  },
  {
    "code": "def safe_sort(\\n    values,\\n    codes=None,\\n    na_sentinel: int | None = -1,\\n    assume_unique: bool = False,\\n    verify: bool = True,\\n) -> np.ndarray | MultiIndex | tuple[np.ndarray | MultiIndex, np.ndarray]:\\n    if not is_list_like(values):\\n        raise TypeError(\\n            \"Only list-like objects are allowed to be passed to safe_sort as values\"\\n        )\\n    if not isinstance(values, (np.ndarray, ABCExtensionArray, ABCMultiIndex)):\\n        dtype, _ = infer_dtype_from_array(values)\\n        values = np.asarray(values, dtype=dtype)  \\n    sorter = None\\n    ordered: np.ndarray | MultiIndex\\n    if (\\n        not is_extension_array_dtype(values)\\n        and lib.infer_dtype(values, skipna=False) == \"mixed-integer\"\\n    ):\\n        ordered = _sort_mixed(values)\\n    else:\\n        try:\\n            sorter = values.argsort()\\n            ordered = values.take(sorter)\\n        except TypeError:\\n            if values.size and isinstance(values[0], tuple):\\n                ordered = _sort_tuples(values)\\n            else:\\n                ordered = _sort_mixed(values)\\n    if codes is None:\\n        return ordered\\n    if not is_list_like(codes):\\n        raise TypeError(\\n            \"Only list-like objects or None are allowed to \"\\n            \"be passed to safe_sort as codes\"\\n        )\\n    codes = ensure_platform_int(np.asarray(codes))\\n    if not assume_unique and not len(unique(values)) == len(values):\\n        raise ValueError(\"values should be unique if codes is not None\")\\n    if sorter is None:\\n        hash_klass, values = _get_hashtable_algo(values)\\n        t = hash_klass(len(values))\\n        t.map_locations(values)\\n        sorter = ensure_platform_int(t.lookup(ordered))\\n    if na_sentinel == -1:\\n        order2 = sorter.argsort()\\n        new_codes = take_nd(order2, codes, fill_value=-1)\\n        if verify:\\n            mask = (codes < -len(values)) | (codes >= len(values))\\n        else:\\n            mask = None\\n    else:\\n        reverse_indexer = np.empty(len(sorter), dtype=np.int_)\\n        reverse_indexer.put(sorter, np.arange(len(sorter)))\\n        new_codes = reverse_indexer.take(codes, mode=\"wrap\")\\n        if na_sentinel is not None:\\n            mask = codes == na_sentinel\\n            if verify:\\n                mask = mask | (codes < -len(values)) | (codes >= len(values))\\n    if na_sentinel is not None and mask is not None:\\n        np.putmask(new_codes, mask, na_sentinel)\\n    return ordered, ensure_platform_int(new_codes)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PERF: pd.concat EA-backed indexes and sort=True (#49178)",
    "fixed_code": "def safe_sort(\\n    values,\\n    codes=None,\\n    na_sentinel: int | None = -1,\\n    assume_unique: bool = False,\\n    verify: bool = True,\\n) -> AnyArrayLike | tuple[AnyArrayLike, np.ndarray]:\\n    if not is_list_like(values):\\n        raise TypeError(\\n            \"Only list-like objects are allowed to be passed to safe_sort as values\"\\n        )\\n    if not is_array_like(values):\\n        dtype, _ = infer_dtype_from_array(values)\\n        values = np.asarray(values, dtype=dtype)  \\n    sorter = None\\n    ordered: AnyArrayLike\\n    if (\\n        not is_extension_array_dtype(values)\\n        and lib.infer_dtype(values, skipna=False) == \"mixed-integer\"\\n    ):\\n        ordered = _sort_mixed(values)\\n    else:\\n        try:\\n            sorter = values.argsort()\\n            ordered = values.take(sorter)\\n        except TypeError:\\n            if values.size and isinstance(values[0], tuple):\\n                ordered = _sort_tuples(values)\\n            else:\\n                ordered = _sort_mixed(values)\\n    if codes is None:\\n        return ordered\\n    if not is_list_like(codes):\\n        raise TypeError(\\n            \"Only list-like objects or None are allowed to \"\\n            \"be passed to safe_sort as codes\"\\n        )\\n    codes = ensure_platform_int(np.asarray(codes))\\n    if not assume_unique and not len(unique(values)) == len(values):\\n        raise ValueError(\"values should be unique if codes is not None\")\\n    if sorter is None:\\n        hash_klass, values = _get_hashtable_algo(values)\\n        t = hash_klass(len(values))\\n        t.map_locations(values)\\n        sorter = ensure_platform_int(t.lookup(ordered))\\n    if na_sentinel == -1:\\n        order2 = sorter.argsort()\\n        new_codes = take_nd(order2, codes, fill_value=-1)\\n        if verify:\\n            mask = (codes < -len(values)) | (codes >= len(values))\\n        else:\\n            mask = None\\n    else:\\n        reverse_indexer = np.empty(len(sorter), dtype=np.int_)\\n        reverse_indexer.put(sorter, np.arange(len(sorter)))\\n        new_codes = reverse_indexer.take(codes, mode=\"wrap\")\\n        if na_sentinel is not None:\\n            mask = codes == na_sentinel\\n            if verify:\\n                mask = mask | (codes < -len(values)) | (codes >= len(values))\\n    if na_sentinel is not None and mask is not None:\\n        np.putmask(new_codes, mask, na_sentinel)\\n    return ordered, ensure_platform_int(new_codes)"
  },
  {
    "code": "def http_error_302(self, req, fp, code, msg, headers):\\n        if \"location\" in headers:\\n            newurl = headers[\"location\"]\\n        elif \"uri\" in headers:\\n            newurl = headers[\"uri\"]\\n        else:\\n            return\\n        urlparts = urlparse(newurl)\\n        if not urlparts.path:\\n            urlparts = list(urlparts)\\n            urlparts[2] = \"/\"\\n        newurl = urlunparse(urlparts)\\n        newurl = urljoin(req.full_url, newurl)\\n        new = self.redirect_request(req, fp, code, msg, headers, newurl)\\n        if new is None:\\n            return\\n        if hasattr(req, 'redirect_dict'):\\n            visited = new.redirect_dict = req.redirect_dict\\n            if (visited.get(newurl, 0) >= self.max_repeats or\\n                len(visited) >= self.max_redirections):\\n                raise HTTPError(req.full_url, code,\\n                                self.inf_msg + msg, headers, fp)\\n        else:\\n            visited = new.redirect_dict = req.redirect_dict = {}\\n        visited[newurl] = visited.get(newurl, 0) + 1\\n        fp.read()\\n        fp.close()\\n        return self.parent.open(new, timeout=req.timeout)\\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\\n    inf_msg = \"The HTTP server returned a redirect error that would \" \\\\n              \"lead to an infinite loop.\\n\" \\\\n              \"The last 30x error message was:\\n\"",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def http_error_302(self, req, fp, code, msg, headers):\\n        if \"location\" in headers:\\n            newurl = headers[\"location\"]\\n        elif \"uri\" in headers:\\n            newurl = headers[\"uri\"]\\n        else:\\n            return\\n        urlparts = urlparse(newurl)\\n        if not urlparts.path:\\n            urlparts = list(urlparts)\\n            urlparts[2] = \"/\"\\n        newurl = urlunparse(urlparts)\\n        newurl = urljoin(req.full_url, newurl)\\n        new = self.redirect_request(req, fp, code, msg, headers, newurl)\\n        if new is None:\\n            return\\n        if hasattr(req, 'redirect_dict'):\\n            visited = new.redirect_dict = req.redirect_dict\\n            if (visited.get(newurl, 0) >= self.max_repeats or\\n                len(visited) >= self.max_redirections):\\n                raise HTTPError(req.full_url, code,\\n                                self.inf_msg + msg, headers, fp)\\n        else:\\n            visited = new.redirect_dict = req.redirect_dict = {}\\n        visited[newurl] = visited.get(newurl, 0) + 1\\n        fp.read()\\n        fp.close()\\n        return self.parent.open(new, timeout=req.timeout)\\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\\n    inf_msg = \"The HTTP server returned a redirect error that would \" \\\\n              \"lead to an infinite loop.\\n\" \\\\n              \"The last 30x error message was:\\n\""
  },
  {
    "code": "def get_grouper(\\n\\tobj: FrameOrSeries,\\n\\tkey=None,\\n\\taxis: int = 0,\\n\\tlevel=None,\\n\\tsort: bool = True,\\n\\tobserved: bool = False,\\n\\tmutated: bool = False,\\n\\tvalidate: bool = True,\\n\\tdropna: bool = True,\\n) -> Tuple[\"ops.BaseGrouper\", Set[Label], FrameOrSeries]:\\n\\tgroup_axis = obj._get_axis(axis)\\n\\tif level is not None:\\n\\t\\tif isinstance(group_axis, MultiIndex):\\n\\t\\t\\tif is_list_like(level) and len(level) == 1:\\n\\t\\t\\t\\tlevel = level[0]\\n\\t\\t\\tif key is None and is_scalar(level):\\n\\t\\t\\t\\tkey = group_axis.get_level_values(level)\\n\\t\\t\\t\\tlevel = None\\n\\t\\telse:\\n\\t\\t\\tif is_list_like(level):\\n\\t\\t\\t\\tnlevels = len(level)\\n\\t\\t\\t\\tif nlevels == 1:\\n\\t\\t\\t\\t\\tlevel = level[0]\\n\\t\\t\\t\\telif nlevels == 0:\\n\\t\\t\\t\\t\\traise ValueError(\"No group keys passed!\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise ValueError(\"multiple levels only valid with MultiIndex\")\\n\\t\\t\\tif isinstance(level, str):\\n\\t\\t\\t\\tif obj._get_axis(axis).name != level:\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\tf\"level name {level} is not the name \"\\n\\t\\t\\t\\t\\t\\tf\"of the {obj._get_axis_name(axis)}\"\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\telif level > 0 or level < -1:\\n\\t\\t\\t\\traise ValueError(\"level > 0 or level < -1 only valid with MultiIndex\")\\n\\t\\t\\tlevel = None\\n\\t\\t\\tkey = group_axis\\n\\tif isinstance(key, Grouper):\\n\\t\\tbinner, grouper, obj = key._get_grouper(obj, validate=False)\\n\\t\\tif key.key is None:\\n\\t\\t\\treturn grouper, set(), obj\\n\\t\\telse:\\n\\t\\t\\treturn grouper, {key.key}, obj\\n\\telif isinstance(key, ops.BaseGrouper):\\n\\t\\treturn key, set(), obj\\n\\tif not isinstance(key, list):\\n\\t\\tkeys = [key]\\n\\t\\tmatch_axis_length = False\\n\\telse:\\n\\t\\tkeys = key\\n\\t\\tmatch_axis_length = len(keys) == len(group_axis)\\n\\tany_callable = any(callable(g) or isinstance(g, dict) for g in keys)\\n\\tany_groupers = any(isinstance(g, Grouper) for g in keys)\\n\\tany_arraylike = any(\\n\\t\\tisinstance(g, (list, tuple, Series, Index, np.ndarray)) for g in keys\\n\\t)\\n\\tif (\\n\\t\\tnot any_callable\\n\\t\\tand not any_arraylike\\n\\t\\tand not any_groupers\\n\\t\\tand match_axis_length\\n\\t\\tand level is None\\n\\t):\\n\\t\\tif isinstance(obj, DataFrame):\\n\\t\\t\\tall_in_columns_index = all(\\n\\t\\t\\t\\tg in obj.columns or g in obj.index.names for g in keys\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tassert isinstance(obj, Series)\\n\\t\\t\\tall_in_columns_index = all(g in obj.index.names for g in keys)\\n\\t\\tif not all_in_columns_index:\\n\\t\\t\\tkeys = [com.asarray_tuplesafe(keys)]\\n\\tif isinstance(level, (tuple, list)):\\n\\t\\tif key is None:\\n\\t\\t\\tkeys = [None] * len(level)\\n\\t\\tlevels = level\\n\\telse:\\n\\t\\tlevels = [level] * len(keys)\\n\\tgroupings: List[Grouping] = []\\n\\texclusions: Set[Label] = set()\\n\\tdef is_in_axis(key) -> bool:\\n\\t\\tif not _is_label_like(key):\\n\\t\\t\\titems = obj.axes[-1]\\n\\t\\t\\ttry:\\n\\t\\t\\t\\titems.get_loc(key)\\n\\t\\t\\texcept (KeyError, TypeError, InvalidIndexError):\\n\\t\\t\\t\\treturn False\\n\\t\\treturn True\\n\\tdef is_in_obj(gpr) -> bool:\\n\\t\\tif not hasattr(gpr, \"name\"):\\n\\t\\t\\treturn False\\n\\t\\ttry:\\n\\t\\t\\treturn gpr is obj[gpr.name]\\n\\t\\texcept (KeyError, IndexError):\\n\\t\\t\\treturn False\\n\\tfor i, (gpr, level) in enumerate(zip(keys, levels)):\\n\\t\\tif is_in_obj(gpr):  \\n\\t\\t\\tin_axis, name = True, gpr.name\\n\\t\\t\\texclusions.add(name)\\n\\t\\telif is_in_axis(gpr):  \\n\\t\\t\\tif gpr in obj:\\n\\t\\t\\t\\tif validate:\\n\\t\\t\\t\\t\\tobj._check_label_or_level_ambiguity(gpr, axis=axis)\\n\\t\\t\\t\\tin_axis, name, gpr = True, gpr, obj[gpr]\\n\\t\\t\\t\\texclusions.add(name)\\n\\t\\t\\telif obj._is_level_reference(gpr, axis=axis):\\n\\t\\t\\t\\tin_axis, name, level, gpr = False, None, gpr, None\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(gpr)\\n\\t\\telif isinstance(gpr, Grouper) and gpr.key is not None:\\n\\t\\t\\texclusions.add(gpr.key)\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\telse:\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\tif is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\tf\"Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) \"\\n\\t\\t\\t\\t\"must be same length\"\\n\\t\\t\\t)\\n\\t\\tping = (\\n\\t\\t\\tGrouping(\\n\\t\\t\\t\\tgroup_axis,\\n\\t\\t\\t\\tgpr,\\n\\t\\t\\t\\tobj=obj,\\n\\t\\t\\t\\tname=name,\\n\\t\\t\\t\\tlevel=level,\\n\\t\\t\\t\\tsort=sort,\\n\\t\\t\\t\\tobserved=observed,\\n\\t\\t\\t\\tin_axis=in_axis,\\n\\t\\t\\t\\tdropna=dropna,\\n\\t\\t\\t)\\n\\t\\t\\tif not isinstance(gpr, Grouping)\\n\\t\\t\\telse gpr\\n\\t\\t)\\n\\t\\tgroupings.append(ping)\\n\\tif len(groupings) == 0 and len(obj):\\n\\t\\traise ValueError(\"No group keys passed!\")\\n\\telif len(groupings) == 0:\\n\\t\\tgroupings.append(Grouping(Index([], dtype=\"int\"), np.array([], dtype=np.intp)))\\n\\tgrouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)\\n\\treturn grouper, exclusions, obj",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix consolidation with MultiIndex, fixes GH #366",
    "fixed_code": "def get_grouper(\\n\\tobj: FrameOrSeries,\\n\\tkey=None,\\n\\taxis: int = 0,\\n\\tlevel=None,\\n\\tsort: bool = True,\\n\\tobserved: bool = False,\\n\\tmutated: bool = False,\\n\\tvalidate: bool = True,\\n\\tdropna: bool = True,\\n) -> Tuple[\"ops.BaseGrouper\", Set[Label], FrameOrSeries]:\\n\\tgroup_axis = obj._get_axis(axis)\\n\\tif level is not None:\\n\\t\\tif isinstance(group_axis, MultiIndex):\\n\\t\\t\\tif is_list_like(level) and len(level) == 1:\\n\\t\\t\\t\\tlevel = level[0]\\n\\t\\t\\tif key is None and is_scalar(level):\\n\\t\\t\\t\\tkey = group_axis.get_level_values(level)\\n\\t\\t\\t\\tlevel = None\\n\\t\\telse:\\n\\t\\t\\tif is_list_like(level):\\n\\t\\t\\t\\tnlevels = len(level)\\n\\t\\t\\t\\tif nlevels == 1:\\n\\t\\t\\t\\t\\tlevel = level[0]\\n\\t\\t\\t\\telif nlevels == 0:\\n\\t\\t\\t\\t\\traise ValueError(\"No group keys passed!\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\traise ValueError(\"multiple levels only valid with MultiIndex\")\\n\\t\\t\\tif isinstance(level, str):\\n\\t\\t\\t\\tif obj._get_axis(axis).name != level:\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\tf\"level name {level} is not the name \"\\n\\t\\t\\t\\t\\t\\tf\"of the {obj._get_axis_name(axis)}\"\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\telif level > 0 or level < -1:\\n\\t\\t\\t\\traise ValueError(\"level > 0 or level < -1 only valid with MultiIndex\")\\n\\t\\t\\tlevel = None\\n\\t\\t\\tkey = group_axis\\n\\tif isinstance(key, Grouper):\\n\\t\\tbinner, grouper, obj = key._get_grouper(obj, validate=False)\\n\\t\\tif key.key is None:\\n\\t\\t\\treturn grouper, set(), obj\\n\\t\\telse:\\n\\t\\t\\treturn grouper, {key.key}, obj\\n\\telif isinstance(key, ops.BaseGrouper):\\n\\t\\treturn key, set(), obj\\n\\tif not isinstance(key, list):\\n\\t\\tkeys = [key]\\n\\t\\tmatch_axis_length = False\\n\\telse:\\n\\t\\tkeys = key\\n\\t\\tmatch_axis_length = len(keys) == len(group_axis)\\n\\tany_callable = any(callable(g) or isinstance(g, dict) for g in keys)\\n\\tany_groupers = any(isinstance(g, Grouper) for g in keys)\\n\\tany_arraylike = any(\\n\\t\\tisinstance(g, (list, tuple, Series, Index, np.ndarray)) for g in keys\\n\\t)\\n\\tif (\\n\\t\\tnot any_callable\\n\\t\\tand not any_arraylike\\n\\t\\tand not any_groupers\\n\\t\\tand match_axis_length\\n\\t\\tand level is None\\n\\t):\\n\\t\\tif isinstance(obj, DataFrame):\\n\\t\\t\\tall_in_columns_index = all(\\n\\t\\t\\t\\tg in obj.columns or g in obj.index.names for g in keys\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tassert isinstance(obj, Series)\\n\\t\\t\\tall_in_columns_index = all(g in obj.index.names for g in keys)\\n\\t\\tif not all_in_columns_index:\\n\\t\\t\\tkeys = [com.asarray_tuplesafe(keys)]\\n\\tif isinstance(level, (tuple, list)):\\n\\t\\tif key is None:\\n\\t\\t\\tkeys = [None] * len(level)\\n\\t\\tlevels = level\\n\\telse:\\n\\t\\tlevels = [level] * len(keys)\\n\\tgroupings: List[Grouping] = []\\n\\texclusions: Set[Label] = set()\\n\\tdef is_in_axis(key) -> bool:\\n\\t\\tif not _is_label_like(key):\\n\\t\\t\\titems = obj.axes[-1]\\n\\t\\t\\ttry:\\n\\t\\t\\t\\titems.get_loc(key)\\n\\t\\t\\texcept (KeyError, TypeError, InvalidIndexError):\\n\\t\\t\\t\\treturn False\\n\\t\\treturn True\\n\\tdef is_in_obj(gpr) -> bool:\\n\\t\\tif not hasattr(gpr, \"name\"):\\n\\t\\t\\treturn False\\n\\t\\ttry:\\n\\t\\t\\treturn gpr is obj[gpr.name]\\n\\t\\texcept (KeyError, IndexError):\\n\\t\\t\\treturn False\\n\\tfor i, (gpr, level) in enumerate(zip(keys, levels)):\\n\\t\\tif is_in_obj(gpr):  \\n\\t\\t\\tin_axis, name = True, gpr.name\\n\\t\\t\\texclusions.add(name)\\n\\t\\telif is_in_axis(gpr):  \\n\\t\\t\\tif gpr in obj:\\n\\t\\t\\t\\tif validate:\\n\\t\\t\\t\\t\\tobj._check_label_or_level_ambiguity(gpr, axis=axis)\\n\\t\\t\\t\\tin_axis, name, gpr = True, gpr, obj[gpr]\\n\\t\\t\\t\\texclusions.add(name)\\n\\t\\t\\telif obj._is_level_reference(gpr, axis=axis):\\n\\t\\t\\t\\tin_axis, name, level, gpr = False, None, gpr, None\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(gpr)\\n\\t\\telif isinstance(gpr, Grouper) and gpr.key is not None:\\n\\t\\t\\texclusions.add(gpr.key)\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\telse:\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\tif is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\tf\"Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) \"\\n\\t\\t\\t\\t\"must be same length\"\\n\\t\\t\\t)\\n\\t\\tping = (\\n\\t\\t\\tGrouping(\\n\\t\\t\\t\\tgroup_axis,\\n\\t\\t\\t\\tgpr,\\n\\t\\t\\t\\tobj=obj,\\n\\t\\t\\t\\tname=name,\\n\\t\\t\\t\\tlevel=level,\\n\\t\\t\\t\\tsort=sort,\\n\\t\\t\\t\\tobserved=observed,\\n\\t\\t\\t\\tin_axis=in_axis,\\n\\t\\t\\t\\tdropna=dropna,\\n\\t\\t\\t)\\n\\t\\t\\tif not isinstance(gpr, Grouping)\\n\\t\\t\\telse gpr\\n\\t\\t)\\n\\t\\tgroupings.append(ping)\\n\\tif len(groupings) == 0 and len(obj):\\n\\t\\traise ValueError(\"No group keys passed!\")\\n\\telif len(groupings) == 0:\\n\\t\\tgroupings.append(Grouping(Index([], dtype=\"int\"), np.array([], dtype=np.intp)))\\n\\tgrouper = ops.BaseGrouper(\\n\\t\\tgroup_axis, groupings, sort=sort, mutated=mutated, dropna=dropna\\n\\t)\\n\\treturn grouper, exclusions, obj"
  },
  {
    "code": "def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\\n\\t\\t\\t  encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\\n\\tqs, _coerce_result = _coerce_args(qs)\\n\\tif not separator or (not isinstance(separator, (str, bytes))):\\n\\t\\traise ValueError(\"Separator must be of type string or bytes.\")\\n\\tif max_num_fields is not None:\\n\\t\\tnum_fields = 1 + qs.count(separator)\\n\\t\\tif max_num_fields < num_fields:\\n\\t\\t\\traise ValueError('Max number of fields exceeded')\\n\\tpairs = [s1 for s1 in qs.split(separator)]\\n\\tr = []\\n\\tfor name_value in pairs:\\n\\t\\tif not name_value and not strict_parsing:\\n\\t\\t\\tcontinue\\n\\t\\tnv = name_value.split('=', 1)\\n\\t\\tif len(nv) != 2:\\n\\t\\t\\tif strict_parsing:\\n\\t\\t\\t\\traise ValueError(\"bad query field: %r\" % (name_value,))\\n\\t\\t\\tif keep_blank_values:\\n\\t\\t\\t\\tnv.append('')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontinue\\n\\t\\tif len(nv[1]) or keep_blank_values:\\n\\t\\t\\tname = nv[0].replace('+', ' ')\\n\\t\\t\\tname = unquote(name, encoding=encoding, errors=errors)\\n\\t\\t\\tname = _coerce_result(name)\\n\\t\\t\\tvalue = nv[1].replace('+', ' ')\\n\\t\\t\\tvalue = unquote(value, encoding=encoding, errors=errors)\\n\\t\\t\\tvalue = _coerce_result(value)\\n\\t\\t\\tr.append((name, value))\\n\\treturn r",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\\n\\t\\t\\t  encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\\n\\tqs, _coerce_result = _coerce_args(qs)\\n\\tif not separator or (not isinstance(separator, (str, bytes))):\\n\\t\\traise ValueError(\"Separator must be of type string or bytes.\")\\n\\tif max_num_fields is not None:\\n\\t\\tnum_fields = 1 + qs.count(separator)\\n\\t\\tif max_num_fields < num_fields:\\n\\t\\t\\traise ValueError('Max number of fields exceeded')\\n\\tpairs = [s1 for s1 in qs.split(separator)]\\n\\tr = []\\n\\tfor name_value in pairs:\\n\\t\\tif not name_value and not strict_parsing:\\n\\t\\t\\tcontinue\\n\\t\\tnv = name_value.split('=', 1)\\n\\t\\tif len(nv) != 2:\\n\\t\\t\\tif strict_parsing:\\n\\t\\t\\t\\traise ValueError(\"bad query field: %r\" % (name_value,))\\n\\t\\t\\tif keep_blank_values:\\n\\t\\t\\t\\tnv.append('')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontinue\\n\\t\\tif len(nv[1]) or keep_blank_values:\\n\\t\\t\\tname = nv[0].replace('+', ' ')\\n\\t\\t\\tname = unquote(name, encoding=encoding, errors=errors)\\n\\t\\t\\tname = _coerce_result(name)\\n\\t\\t\\tvalue = nv[1].replace('+', ' ')\\n\\t\\t\\tvalue = unquote(value, encoding=encoding, errors=errors)\\n\\t\\t\\tvalue = _coerce_result(value)\\n\\t\\t\\tr.append((name, value))\\n\\treturn r"
  },
  {
    "code": "def run_task(self, **kwargs) -> dict:\\n        \"\"\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html\\n        ...",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add reattach flag to ECSOperator (#10643)\\n\\n..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and attach to it.",
    "fixed_code": "def run_task(self, **kwargs) -> Dict:\\n        \"\"\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html\\n        ..."
  },
  {
    "code": "def get(self, key, default=None, version=None):\\n        return self.get_many([key], version).get(key, default)\\n    def get_many(self, keys, version=None):\\n        if not keys:\\n            return {}\\n        key_map = {}\\n        for key in keys:\\n            self.validate_key(key)\\n            key_map[self.make_key(key, version)] = key\\n        db = router.db_for_read(self.cache_model_class)\\n        connection = connections[db]\\n        quote_name = connection.ops.quote_name\\n        table = quote_name(self._table)\\n        with connection.cursor() as cursor:\\n            cursor.execute(\\n                'SELECT %s, %s, %s FROM %s WHERE %s IN (%s)' % (\\n                    quote_name('cache_key'),\\n                    quote_name('value'),\\n                    quote_name('expires'),\\n                    table,\\n                    quote_name('cache_key'),\\n                    ', '.join(['%s'] * len(key_map)),\\n                ),\\n                list(key_map),\\n            )\\n            rows = cursor.fetchall()\\n        result = {}\\n        expired_keys = []\\n        expression = models.Expression(output_field=models.DateTimeField())\\n        converters = (connection.ops.get_db_converters(expression) + expression.get_db_converters(connection))\\n        for key, value, expires in rows:\\n            for converter in converters:\\n                if func_supports_parameter(converter, 'context'):  \\n                    expires = converter(expires, expression, connection, {})\\n                else:\\n                    expires = converter(expires, expression, connection)\\n            if expires < timezone.now():\\n                expired_keys.append(key)\\n            else:\\n                value = connection.ops.process_clob(value)\\n                value = pickle.loads(base64.b64decode(value.encode()))\\n                result[key_map.get(key)] = value\\n        self._base_delete_many(expired_keys)\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get(self, key, default=None, version=None):\\n        return self.get_many([key], version).get(key, default)\\n    def get_many(self, keys, version=None):\\n        if not keys:\\n            return {}\\n        key_map = {}\\n        for key in keys:\\n            self.validate_key(key)\\n            key_map[self.make_key(key, version)] = key\\n        db = router.db_for_read(self.cache_model_class)\\n        connection = connections[db]\\n        quote_name = connection.ops.quote_name\\n        table = quote_name(self._table)\\n        with connection.cursor() as cursor:\\n            cursor.execute(\\n                'SELECT %s, %s, %s FROM %s WHERE %s IN (%s)' % (\\n                    quote_name('cache_key'),\\n                    quote_name('value'),\\n                    quote_name('expires'),\\n                    table,\\n                    quote_name('cache_key'),\\n                    ', '.join(['%s'] * len(key_map)),\\n                ),\\n                list(key_map),\\n            )\\n            rows = cursor.fetchall()\\n        result = {}\\n        expired_keys = []\\n        expression = models.Expression(output_field=models.DateTimeField())\\n        converters = (connection.ops.get_db_converters(expression) + expression.get_db_converters(connection))\\n        for key, value, expires in rows:\\n            for converter in converters:\\n                if func_supports_parameter(converter, 'context'):  \\n                    expires = converter(expires, expression, connection, {})\\n                else:\\n                    expires = converter(expires, expression, connection)\\n            if expires < timezone.now():\\n                expired_keys.append(key)\\n            else:\\n                value = connection.ops.process_clob(value)\\n                value = pickle.loads(base64.b64decode(value.encode()))\\n                result[key_map.get(key)] = value\\n        self._base_delete_many(expired_keys)\\n        return result"
  },
  {
    "code": "def _get_non_gfk_field(opts, name):\\n    field = opts.get_field(name)\\n    if field.is_relation and field.many_to_one and not field.related_model:\\n        raise FieldDoesNotExist()\\n    return field",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_non_gfk_field(opts, name):\\n    field = opts.get_field(name)\\n    if field.is_relation and field.many_to_one and not field.related_model:\\n        raise FieldDoesNotExist()\\n    return field"
  },
  {
    "code": "def name(self):\\n        if self._column is None:\\n            return 'result'\\n        else:\\n            return self._column\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: don't attach nonsense 'result' name to groupby results without obvious name, close #995",
    "fixed_code": "def name(self):\\n        if self._column is None:\\n            return None \\n        else:\\n            return self._column\\n    @property"
  },
  {
    "code": "def _engine(self):\\n\\t\\tperiod = weakref.ref(self)\\n\\t\\treturn self._engine_type(period, len(self))\\n\\t@doc(Index.__contains__)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: create new MI from MultiIndex._get_level_values (#33134)",
    "fixed_code": "def _engine(self):\\n\\t\\tperiod = weakref.ref(self._values)\\n\\t\\treturn self._engine_type(period, len(self))\\n\\t@doc(Index.__contains__)"
  },
  {
    "code": "def _make_id(target):\\n    if hasattr(target, 'im_func'):\\n        return (id(target.im_self), id(target.im_func))\\n    return id(target)\\nclass Signal(object):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Replaced im_func and im_self by __func__ and __self__.\\n\\nThe new names are Python 3 compatible.",
    "fixed_code": "def _make_id(target):\\n    if hasattr(target, '__func__'):\\n        return (id(target.__self__), id(target.__func__))\\n    return id(target)\\nclass Signal(object):"
  },
  {
    "code": "def get_handle(\\n    path_or_buf: FilePathOrBuffer,\\n    mode: str,\\n    encoding: str | None = None,\\n    compression: CompressionOptions = None,\\n    memory_map: bool = False,\\n    is_text: bool = True,\\n    errors: str | None = None,\\n    storage_options: StorageOptions = None,\\n) -> IOHandles:\\n    encoding = encoding or \"utf-8\"\\n    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\\n        mode += \"b\"\\n    if isinstance(errors, str):\\n        errors = errors.lower()\\n    if errors not in (\\n        None,\\n        \"strict\",\\n        \"ignore\",\\n        \"replace\",\\n        \"xmlcharrefreplace\",\\n        \"backslashreplace\",\\n        \"namereplace\",\\n        \"surrogateescape\",\\n        \"surrogatepass\",\\n    ):\\n        raise ValueError(\\n            f\"Invalid value for `encoding_errors` ({errors}). Please see \"\\n            + \"https://docs.python.org/3/library/codecs.html\\n            + \"for valid values.\"\\n        )\\n    ioargs = _get_filepath_or_buffer(\\n        path_or_buf,\\n        encoding=encoding,\\n        compression=compression,\\n        mode=mode,\\n        storage_options=storage_options,\\n    )\\n    handle = ioargs.filepath_or_buffer\\n    handles: list[Buffer]\\n    handle, memory_map, handles = _maybe_memory_map(\\n        handle,\\n        memory_map,\\n        ioargs.encoding,\\n        ioargs.mode,\\n        errors,\\n        ioargs.compression[\"method\"] not in _compression_to_extension,\\n    )\\n    is_path = isinstance(handle, str)\\n    compression_args = dict(ioargs.compression)\\n    compression = compression_args.pop(\"method\")\\n    if \"r\" not in mode and is_path:\\n        check_parent_directory(str(handle))\\n    if compression:\\n        ioargs.mode = ioargs.mode.replace(\"t\", \"\")\\n        if compression == \"gzip\":\\n            if is_path:\\n                assert isinstance(handle, str)\\n                handle = gzip.GzipFile(\\n                    filename=handle,\\n                    mode=ioargs.mode,\\n                    **compression_args,\\n                )\\n            else:\\n                handle = gzip.GzipFile(\\n                    fileobj=handle,  \\n                    mode=ioargs.mode,\\n                    **compression_args,\\n                )\\n        elif compression == \"bz2\":\\n            handle = bz2.BZ2File(\\n                handle,  \\n                mode=ioargs.mode,\\n                **compression_args,\\n            )\\n        elif compression == \"zip\":\\n            handle = _BytesZipFile(handle, ioargs.mode, **compression_args)\\n            if handle.mode == \"r\":\\n                handles.append(handle)\\n                zip_names = handle.namelist()\\n                if len(zip_names) == 1:\\n                    handle = handle.open(zip_names.pop())\\n                elif len(zip_names) == 0:\\n                    raise ValueError(f\"Zero files found in ZIP file {path_or_buf}\")\\n                else:\\n                    raise ValueError(\\n                        \"Multiple files found in ZIP file. \"\\n                        f\"Only one file per ZIP: {zip_names}\"\\n                    )\\n        elif compression == \"xz\":\\n            handle = get_lzma_file(lzma)(handle, ioargs.mode)\\n        else:\\n            msg = f\"Unrecognized compression type: {compression}\"\\n            raise ValueError(msg)\\n        assert not isinstance(handle, str)\\n        handles.append(handle)\\n    elif isinstance(handle, str):\\n        if ioargs.encoding and \"b\" not in ioargs.mode:\\n            handle = open(\\n                handle,\\n                ioargs.mode,\\n                encoding=ioargs.encoding,\\n                errors=errors,\\n                newline=\"\",\\n            )\\n        else:\\n            handle = open(handle, ioargs.mode)\\n        handles.append(handle)\\n    is_wrapped = False\\n    if not is_text and ioargs.mode == \"rb\" and isinstance(handle, TextIOBase):\\n        handle = BytesIOWrapper(\\n            handle,\\n            encoding=ioargs.encoding,\\n        )\\n        handles.append(handle)\\n        is_wrapped = True\\n    elif is_text and (compression or _is_binary_mode(handle, ioargs.mode)):\\n        handle = TextIOWrapper(\\n            handle,  \\n            encoding=ioargs.encoding,\\n            errors=errors,\\n            newline=\"\",\\n        )\\n        handles.append(handle)\\n        is_wrapped = not (\\n            isinstance(ioargs.filepath_or_buffer, str) or ioargs.should_close\\n        )\\n    if \"r\" in ioargs.mode and not hasattr(handle, \"read\"):\\n        raise TypeError(\\n            \"Expected file path name or file-like object, \"\\n            f\"got {type(ioargs.filepath_or_buffer)} type\"\\n        )\\n    handles.reverse()  \\n    if ioargs.should_close:\\n        assert not isinstance(ioargs.filepath_or_buffer, str)\\n        handles.append(ioargs.filepath_or_buffer)\\n    assert not isinstance(handle, str)\\n    return IOHandles(\\n        handle=handle,\\n        created_handles=handles,\\n        is_wrapped=is_wrapped,\\n        is_mmap=memory_map,\\n        compression=ioargs.compression,\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: let `codecs` validate the possible values of encoding and encoding errors (#43616)",
    "fixed_code": "def get_handle(\\n    path_or_buf: FilePathOrBuffer,\\n    mode: str,\\n    encoding: str | None = None,\\n    compression: CompressionOptions = None,\\n    memory_map: bool = False,\\n    is_text: bool = True,\\n    errors: str | None = None,\\n    storage_options: StorageOptions = None,\\n) -> IOHandles:\\n    encoding = encoding or \"utf-8\"\\n    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\\n        mode += \"b\"\\n    if isinstance(encoding, str):\\n        codecs.lookup(encoding)\\n    if isinstance(errors, str):\\n        codecs.lookup_error(errors)\\n    ioargs = _get_filepath_or_buffer(\\n        path_or_buf,\\n        encoding=encoding,\\n        compression=compression,\\n        mode=mode,\\n        storage_options=storage_options,\\n    )\\n    handle = ioargs.filepath_or_buffer\\n    handles: list[Buffer]\\n    handle, memory_map, handles = _maybe_memory_map(\\n        handle,\\n        memory_map,\\n        ioargs.encoding,\\n        ioargs.mode,\\n        errors,\\n        ioargs.compression[\"method\"] not in _compression_to_extension,\\n    )\\n    is_path = isinstance(handle, str)\\n    compression_args = dict(ioargs.compression)\\n    compression = compression_args.pop(\"method\")\\n    if \"r\" not in mode and is_path:\\n        check_parent_directory(str(handle))\\n    if compression:\\n        ioargs.mode = ioargs.mode.replace(\"t\", \"\")\\n        if compression == \"gzip\":\\n            if is_path:\\n                assert isinstance(handle, str)\\n                handle = gzip.GzipFile(\\n                    filename=handle,\\n                    mode=ioargs.mode,\\n                    **compression_args,\\n                )\\n            else:\\n                handle = gzip.GzipFile(\\n                    fileobj=handle,  \\n                    mode=ioargs.mode,\\n                    **compression_args,\\n                )\\n        elif compression == \"bz2\":\\n            handle = bz2.BZ2File(\\n                handle,  \\n                mode=ioargs.mode,\\n                **compression_args,\\n            )\\n        elif compression == \"zip\":\\n            handle = _BytesZipFile(handle, ioargs.mode, **compression_args)\\n            if handle.mode == \"r\":\\n                handles.append(handle)\\n                zip_names = handle.namelist()\\n                if len(zip_names) == 1:\\n                    handle = handle.open(zip_names.pop())\\n                elif len(zip_names) == 0:\\n                    raise ValueError(f\"Zero files found in ZIP file {path_or_buf}\")\\n                else:\\n                    raise ValueError(\\n                        \"Multiple files found in ZIP file. \"\\n                        f\"Only one file per ZIP: {zip_names}\"\\n                    )\\n        elif compression == \"xz\":\\n            handle = get_lzma_file(lzma)(handle, ioargs.mode)\\n        else:\\n            msg = f\"Unrecognized compression type: {compression}\"\\n            raise ValueError(msg)\\n        assert not isinstance(handle, str)\\n        handles.append(handle)\\n    elif isinstance(handle, str):\\n        if ioargs.encoding and \"b\" not in ioargs.mode:\\n            handle = open(\\n                handle,\\n                ioargs.mode,\\n                encoding=ioargs.encoding,\\n                errors=errors,\\n                newline=\"\",\\n            )\\n        else:\\n            handle = open(handle, ioargs.mode)\\n        handles.append(handle)\\n    is_wrapped = False\\n    if not is_text and ioargs.mode == \"rb\" and isinstance(handle, TextIOBase):\\n        handle = BytesIOWrapper(\\n            handle,\\n            encoding=ioargs.encoding,\\n        )\\n        handles.append(handle)\\n        is_wrapped = True\\n    elif is_text and (compression or _is_binary_mode(handle, ioargs.mode)):\\n        handle = TextIOWrapper(\\n            handle,  \\n            encoding=ioargs.encoding,\\n            errors=errors,\\n            newline=\"\",\\n        )\\n        handles.append(handle)\\n        is_wrapped = not (\\n            isinstance(ioargs.filepath_or_buffer, str) or ioargs.should_close\\n        )\\n    if \"r\" in ioargs.mode and not hasattr(handle, \"read\"):\\n        raise TypeError(\\n            \"Expected file path name or file-like object, \"\\n            f\"got {type(ioargs.filepath_or_buffer)} type\"\\n        )\\n    handles.reverse()  \\n    if ioargs.should_close:\\n        assert not isinstance(ioargs.filepath_or_buffer, str)\\n        handles.append(ioargs.filepath_or_buffer)\\n    assert not isinstance(handle, str)\\n    return IOHandles(\\n        handle=handle,\\n        created_handles=handles,\\n        is_wrapped=is_wrapped,\\n        is_mmap=memory_map,\\n        compression=ioargs.compression,\\n    )"
  },
  {
    "code": "def state_present(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if key == 'vrf':\\n            continue\\n        if value is True:\\n            commands.append(key)\\n        elif value is False:\\n            if key == 'passive-interface default':\\n                if existing_commands.get(key):\\n                    commands.append('no {0}'.format(key))\\n            else:\\n                commands.append('no {0}'.format(key))\\n        elif value == 'default' or value == '':\\n            if key == 'log-adjacency-changes':\\n                commands.append('no {0}'.format(key))\\n            elif existing_commands.get(key):\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n        else:\\n            if key == 'timers throttle lsa':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_lsa_start', proposed),\\n                    get_timer_prd('timer_throttle_lsa_hold', proposed),\\n                    get_timer_prd('timer_throttle_lsa_max', proposed))\\n            elif key == 'timers throttle spf':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_spf_start', proposed),\\n                    get_timer_prd('timer_throttle_spf_hold', proposed),\\n                    get_timer_prd('timer_throttle_spf_max', proposed))\\n            elif key == 'log-adjacency-changes':\\n                if value == 'log':\\n                    command = key\\n                elif value == 'detail':\\n                    command = '{0} {1}'.format(key, value)\\n            elif key == 'auto-cost reference-bandwidth':\\n                if len(value) < 5:\\n                    command = '{0} {1} Mbps'.format(key, value)\\n                else:\\n                    value = str(int(value) / 1000)\\n                    command = '{0} {1} Gbps'.format(key, value)\\n            elif key == 'bfd':\\n                command = 'no bfd' if value == 'disable' else 'bfd'\\n            else:\\n                command = '{0} {1}'.format(key, value.lower())\\n            if command not in commands:\\n                commands.append(command)\\n    if commands:\\n        parents = ['router ospf {0}'.format(module.params['ospf'])]\\n        if module.params['vrf'] != 'default':\\n            parents.append('vrf {0}'.format(module.params['vrf']))\\n        candidate.add(commands, parents=parents)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def state_present(module, existing, proposed, candidate):\\n    commands = list()\\n    proposed_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, proposed)\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in proposed_commands.items():\\n        if key == 'vrf':\\n            continue\\n        if value is True:\\n            commands.append(key)\\n        elif value is False:\\n            if key == 'passive-interface default':\\n                if existing_commands.get(key):\\n                    commands.append('no {0}'.format(key))\\n            else:\\n                commands.append('no {0}'.format(key))\\n        elif value == 'default' or value == '':\\n            if key == 'log-adjacency-changes':\\n                commands.append('no {0}'.format(key))\\n            elif existing_commands.get(key):\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n        else:\\n            if key == 'timers throttle lsa':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_lsa_start', proposed),\\n                    get_timer_prd('timer_throttle_lsa_hold', proposed),\\n                    get_timer_prd('timer_throttle_lsa_max', proposed))\\n            elif key == 'timers throttle spf':\\n                command = '{0} {1} {2} {3}'.format(\\n                    key,\\n                    get_timer_prd('timer_throttle_spf_start', proposed),\\n                    get_timer_prd('timer_throttle_spf_hold', proposed),\\n                    get_timer_prd('timer_throttle_spf_max', proposed))\\n            elif key == 'log-adjacency-changes':\\n                if value == 'log':\\n                    command = key\\n                elif value == 'detail':\\n                    command = '{0} {1}'.format(key, value)\\n            elif key == 'auto-cost reference-bandwidth':\\n                if len(value) < 5:\\n                    command = '{0} {1} Mbps'.format(key, value)\\n                else:\\n                    value = str(int(value) / 1000)\\n                    command = '{0} {1} Gbps'.format(key, value)\\n            elif key == 'bfd':\\n                command = 'no bfd' if value == 'disable' else 'bfd'\\n            else:\\n                command = '{0} {1}'.format(key, value.lower())\\n            if command not in commands:\\n                commands.append(command)\\n    if commands:\\n        parents = ['router ospf {0}'.format(module.params['ospf'])]\\n        if module.params['vrf'] != 'default':\\n            parents.append('vrf {0}'.format(module.params['vrf']))\\n        candidate.add(commands, parents=parents)"
  },
  {
    "code": "def get_value(arg, config, module):\\n    command = PARAM_TO_COMMAND_KEYMAP[arg]\\n    has_command = re.search(r'\\s+{0}\\s*$'.format(command), config, re.M)\\n    has_command_val = re.search(r'(?:{0}\\s)(?P<value>.*)$'.format(command), config, re.M)\\n    if command == 'ip router ospf':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'ospf':\\n                value = value_list[0]\\n            elif arg == 'area':\\n                value = value_list[2]\\n                value = normalize_area(value, module)\\n    elif command == 'ip ospf message-digest-key':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'message_digest_key_id':\\n                value = value_list[0]\\n            elif arg == 'message_digest_algorithm_type':\\n                value = value_list[1]\\n            elif arg == 'message_digest_encryption_type':\\n                value = value_list[2]\\n                if value == '3':\\n                    value = '3des'\\n                elif value == '7':\\n                    value = 'cisco_type_7'\\n            elif arg == 'message_digest_password':\\n                value = value_list[3]\\n    elif arg == 'passive_interface':\\n        has_no_command = re.search(r'\\s+no\\s+{0}\\s*$'.format(command), config, re.M)\\n        if has_no_command:\\n            value = False\\n        elif has_command:\\n            value = True\\n        else:\\n            value = None\\n    elif arg == 'bfd':\\n        m = re.search(r'\\s*ip ospf bfd(?P<disable> disable)?', config)\\n        if m:\\n            value = 'disable' if m.group('disable') else 'enable'\\n        else:\\n            value = 'default'\\n    elif arg in BOOL_PARAMS:\\n        value = bool(has_command)\\n    else:\\n        value = ''\\n        if has_command_val:\\n            value = has_command_val.group('value')\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_value(arg, config, module):\\n    command = PARAM_TO_COMMAND_KEYMAP[arg]\\n    has_command = re.search(r'\\s+{0}\\s*$'.format(command), config, re.M)\\n    has_command_val = re.search(r'(?:{0}\\s)(?P<value>.*)$'.format(command), config, re.M)\\n    if command == 'ip router ospf':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'ospf':\\n                value = value_list[0]\\n            elif arg == 'area':\\n                value = value_list[2]\\n                value = normalize_area(value, module)\\n    elif command == 'ip ospf message-digest-key':\\n        value = ''\\n        if has_command_val:\\n            value_list = has_command_val.group('value').split()\\n            if arg == 'message_digest_key_id':\\n                value = value_list[0]\\n            elif arg == 'message_digest_algorithm_type':\\n                value = value_list[1]\\n            elif arg == 'message_digest_encryption_type':\\n                value = value_list[2]\\n                if value == '3':\\n                    value = '3des'\\n                elif value == '7':\\n                    value = 'cisco_type_7'\\n            elif arg == 'message_digest_password':\\n                value = value_list[3]\\n    elif arg == 'passive_interface':\\n        has_no_command = re.search(r'\\s+no\\s+{0}\\s*$'.format(command), config, re.M)\\n        if has_no_command:\\n            value = False\\n        elif has_command:\\n            value = True\\n        else:\\n            value = None\\n    elif arg == 'bfd':\\n        m = re.search(r'\\s*ip ospf bfd(?P<disable> disable)?', config)\\n        if m:\\n            value = 'disable' if m.group('disable') else 'enable'\\n        else:\\n            value = 'default'\\n    elif arg in BOOL_PARAMS:\\n        value = bool(has_command)\\n    else:\\n        value = ''\\n        if has_command_val:\\n            value = has_command_val.group('value')\\n    return value"
  },
  {
    "code": "def set_printoptions(precision=None, column_space=None, max_rows=None,\\n                     max_columns=None, colheader_justify=None,\\n                     max_colwidth=None, notebook_repr_html=None,\\n                     date_dayfirst=None, date_yearfirst=None):\\n    if precision is not None:\\n        print_config.precision = precision\\n    if column_space is not None:\\n        print_config.column_space = column_space\\n    if max_rows is not None:\\n        print_config.max_rows = max_rows\\n    if max_colwidth is not None:\\n        print_config.max_colwidth = max_colwidth\\n    if max_columns is not None:\\n        print_config.max_columns = max_columns\\n    if colheader_justify is not None:\\n        print_config.colheader_justify = colheader_justify\\n    if notebook_repr_html is not None:\\n        print_config.notebook_repr_html = notebook_repr_html\\n    if date_dayfirst is not None:\\n        print_config.date_dayfirst = date_dayfirst\\n    if date_yearfirst is not None:\\n        print_config.date_yearfirst = date_yearfirst",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add multi_sparse option to set_printoptions to disable sparsification in MultiIndex.format, close #1538",
    "fixed_code": "def set_printoptions(precision=None, column_space=None, max_rows=None,\\n                     max_columns=None, colheader_justify=None,\\n                     max_colwidth=None, notebook_repr_html=None,\\n                     date_dayfirst=None, date_yearfirst=None,\\n                     multi_sparse=None):\\n    if precision is not None:\\n        print_config.precision = precision\\n    if column_space is not None:\\n        print_config.column_space = column_space\\n    if max_rows is not None:\\n        print_config.max_rows = max_rows\\n    if max_colwidth is not None:\\n        print_config.max_colwidth = max_colwidth\\n    if max_columns is not None:\\n        print_config.max_columns = max_columns\\n    if colheader_justify is not None:\\n        print_config.colheader_justify = colheader_justify\\n    if notebook_repr_html is not None:\\n        print_config.notebook_repr_html = notebook_repr_html\\n    if date_dayfirst is not None:\\n        print_config.date_dayfirst = date_dayfirst\\n    if date_yearfirst is not None:\\n        print_config.date_yearfirst = date_yearfirst\\n    if multi_sparse is not None:\\n        print_config.multi_sparse = multi_sparse"
  },
  {
    "code": "def as_blocks(self):\\n        self._consolidate_inplace()\\n        bd = {}\\n        for b in self._data.blocks:\\n            bd.setdefault(str(b.dtype), []).append(b)\\n        result = {}\\n        for dtype, blocks in bd.items():\\n            combined = self._data.combine(blocks, copy=True)\\n            result[dtype] = self._constructor(combined).__finalize__(self)\\n        return result\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_blocks(self):\\n        self._consolidate_inplace()\\n        bd = {}\\n        for b in self._data.blocks:\\n            bd.setdefault(str(b.dtype), []).append(b)\\n        result = {}\\n        for dtype, blocks in bd.items():\\n            combined = self._data.combine(blocks, copy=True)\\n            result[dtype] = self._constructor(combined).__finalize__(self)\\n        return result\\n    @property"
  },
  {
    "code": "def glob_to_re(pattern):\\n    pattern_re = fnmatch.translate(pattern)\\n    pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', r'\\1[^/]', pattern_re)\\n    return pattern_re",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def glob_to_re(pattern):\\n    pattern_re = fnmatch.translate(pattern)\\n    pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', r'\\1[^/]', pattern_re)\\n    return pattern_re"
  },
  {
    "code": "def do_cache(parser, token):\\n    nodelist = parser.parse(('endcache',))\\n    parser.delete_first_token()\\n    tokens = token.split_contents()\\n    if len(tokens) < 3:\\n        raise TemplateSyntaxError(\"'%r' tag requires at least 2 arguments.\" % tokens[0])\\n    return CacheNode(nodelist,\\n        parser.compile_filter(tokens[1]),\\n        parser.compile_filter(tokens[2]),\\n        [parser.compile_filter(token) for token in tokens[3:]])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #20130 -- Regression in {% cache %} template tag.",
    "fixed_code": "def do_cache(parser, token):\\n    nodelist = parser.parse(('endcache',))\\n    parser.delete_first_token()\\n    tokens = token.split_contents()\\n    if len(tokens) < 3:\\n        raise TemplateSyntaxError(\"'%r' tag requires at least 2 arguments.\" % tokens[0])\\n    return CacheNode(nodelist,\\n        parser.compile_filter(tokens[1]),\\n        tokens[2], \\n        [parser.compile_filter(token) for token in tokens[3:]])"
  },
  {
    "code": "def __getstate__(self):\\n        meta = dict((k, getattr(self, k, None)) for k in self._metadata)\\n        return dict(_data=self._data, _typ=self._typ,\\n                    _metadata=self._metadata, **meta)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getstate__(self):\\n        meta = dict((k, getattr(self, k, None)) for k in self._metadata)\\n        return dict(_data=self._data, _typ=self._typ,\\n                    _metadata=self._metadata, **meta)"
  },
  {
    "code": "def run_table_delete(self, deletion_dataset_table: str,\\n                         ignore_if_missing: bool = False) -> None:\\n        deletion_project, deletion_dataset, deletion_table = \\\\n            _split_tablename(table_input=deletion_dataset_table,\\n                             default_project_id=self.project_id)\\n        try:\\n            self.service.tables() \\\\n                .delete(projectId=deletion_project,\\n                        datasetId=deletion_dataset,\\n                        tableId=deletion_table) \\\\n                .execute(num_retries=self.num_retries)\\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\\n                          deletion_dataset, deletion_table)\\n        except HttpError:\\n            if not ignore_if_missing:\\n                raise Exception('Table deletion failed. Table does not exist.')\\n            else:\\n                self.log.info('Table does not exist. Skipping.')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6442] BigQuery hook - standardize handling http exceptions (#7028)",
    "fixed_code": "def run_table_delete(self, deletion_dataset_table: str,\\n                         ignore_if_missing: bool = False) -> None:\\n        deletion_project, deletion_dataset, deletion_table = \\\\n            _split_tablename(table_input=deletion_dataset_table,\\n                             default_project_id=self.project_id)\\n        try:\\n            self.service.tables() \\\\n                .delete(projectId=deletion_project,\\n                        datasetId=deletion_dataset,\\n                        tableId=deletion_table) \\\\n                .execute(num_retries=self.num_retries)\\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\\n                          deletion_dataset, deletion_table)\\n        except HttpError as e:\\n            if e.resp.status == 404 and ignore_if_missing:\\n                self.log.info('Table does not exist. Skipping.')\\n            else:\\n                raise e\\n    @CloudBaseHook.catch_http_exception"
  },
  {
    "code": "def main(opcode_py, outfile='Include/opcode.h', internaloutfile='Include/internal/pycore_opcode.h'):\\n    opcode = {}\\n    if hasattr(tokenize, 'open'):\\n        fp = tokenize.open(opcode_py)   \\n    else:\\n        fp = open(opcode_py)            \\n    with fp:\\n        code = fp.read()\\n    exec(code, opcode)\\n    opmap = opcode['opmap']\\n    opname = opcode['opname']\\n    hasconst = opcode['hasconst']\\n    hasjrel = opcode['hasjrel']\\n    hasjabs = opcode['hasjabs']\\n    used = [ False ] * 256\\n    next_op = 1\\n    for name, op in opmap.items():\\n        used[op] = True\\n    specialized_opmap = {}\\n    opname_including_specialized = opname.copy()\\n    for name in opcode['_specialized_instructions']:\\n        while used[next_op]:\\n            next_op += 1\\n        specialized_opmap[name] = next_op\\n        opname_including_specialized[next_op] = name\\n        used[next_op] = True\\n    specialized_opmap['DO_TRACING'] = 255\\n    opname_including_specialized[255] = 'DO_TRACING'\\n    used[255] = True\\n    with (open(outfile, 'w') as fobj, open(internaloutfile, 'w') as iobj):\\n        fobj.write(header)\\n        iobj.write(internal_header)\\n        for name in opname:\\n            if name in opmap:\\n                fobj.write(DEFINE.format(name, opmap[name]))\\n            if name == 'POP_EXCEPT': \\n                fobj.write(DEFINE.format(\"HAVE_ARGUMENT\", opcode[\"HAVE_ARGUMENT\"]))\\n        for name, op in specialized_opmap.items():\\n            fobj.write(DEFINE.format(name, op))\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Caches[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Deopt[256];\\n\")\\n        iobj.write(\"\\n\")\\n        write_int_array_from_ops(\"_PyOpcode_RelativeJump\", opcode['hasjrel'], iobj)\\n        write_int_array_from_ops(\"_PyOpcode_Jump\", opcode['hasjrel'] + opcode['hasjabs'], iobj)\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Caches[256] = {\\n\")\\n        for i, entries in enumerate(opcode[\"_inline_cache_entries\"]):\\n            if entries:\\n                iobj.write(f\"    [{opname[i]}] = {entries},\\n\")\\n        iobj.write(\"};\\n\")\\n        deoptcodes = {}\\n        for basic in opmap:\\n            deoptcodes[basic] = basic\\n        for basic, family in opcode[\"_specializations\"].items():\\n            for specialized in family:\\n                deoptcodes[specialized] = basic\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Deopt[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        fobj.write(\"\\n\")\\n        fobj.write(\"\\n        for op in hasconst:\\n            fobj.write(f\"\\n    || ((op) == {op}) \\\\\")\\n        fobj.write(\"\\n    )\\n\")\\n        fobj.write(\"\\n\")\\n        for i, (op, _) in enumerate(opcode[\"_nb_ops\"]):\\n            fobj.write(DEFINE.format(op, i))\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"static const char *const _PyOpcode_OpName[256] = {\\n\")\\n        for op, name in enumerate(opname_including_specialized):\\n            if name[0] != \"<\":\\n                op = name\\n            iobj.write(f'')\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        for i, flag in enumerate(used):\\n            if not flag:\\n                iobj.write(f\"    case {i}: \\\\\\n\")\\n        iobj.write(\"        ;\\n\")\\n        fobj.write(footer)\\n        iobj.write(internal_footer)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-92031: Deoptimize Static Code at Finalization (GH-92039)",
    "fixed_code": "def main(opcode_py, outfile='Include/opcode.h', internaloutfile='Include/internal/pycore_opcode.h'):\\n    opcode = {}\\n    if hasattr(tokenize, 'open'):\\n        fp = tokenize.open(opcode_py)   \\n    else:\\n        fp = open(opcode_py)            \\n    with fp:\\n        code = fp.read()\\n    exec(code, opcode)\\n    opmap = opcode['opmap']\\n    opname = opcode['opname']\\n    hasconst = opcode['hasconst']\\n    hasjrel = opcode['hasjrel']\\n    hasjabs = opcode['hasjabs']\\n    used = [ False ] * 256\\n    next_op = 1\\n    for name, op in opmap.items():\\n        used[op] = True\\n    specialized_opmap = {}\\n    opname_including_specialized = opname.copy()\\n    for name in opcode['_specialized_instructions']:\\n        while used[next_op]:\\n            next_op += 1\\n        specialized_opmap[name] = next_op\\n        opname_including_specialized[next_op] = name\\n        used[next_op] = True\\n    specialized_opmap['DO_TRACING'] = 255\\n    opname_including_specialized[255] = 'DO_TRACING'\\n    used[255] = True\\n    with (open(outfile, 'w') as fobj, open(internaloutfile, 'w') as iobj):\\n        fobj.write(header)\\n        iobj.write(internal_header)\\n        for name in opname:\\n            if name in opmap:\\n                fobj.write(DEFINE.format(name, opmap[name]))\\n            if name == 'POP_EXCEPT': \\n                fobj.write(DEFINE.format(\"HAVE_ARGUMENT\", opcode[\"HAVE_ARGUMENT\"]))\\n        for name, op in specialized_opmap.items():\\n            fobj.write(DEFINE.format(name, op))\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Caches[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Deopt[256];\\n\")\\n        iobj.write(\"\\nextern const uint8_t _PyOpcode_Original[256];\\n\")\\n        iobj.write(\"\\n\")\\n        write_int_array_from_ops(\"_PyOpcode_RelativeJump\", opcode['hasjrel'], iobj)\\n        write_int_array_from_ops(\"_PyOpcode_Jump\", opcode['hasjrel'] + opcode['hasjabs'], iobj)\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Caches[256] = {\\n\")\\n        for i, entries in enumerate(opcode[\"_inline_cache_entries\"]):\\n            if entries:\\n                iobj.write(f\"    [{opname[i]}] = {entries},\\n\")\\n        iobj.write(\"};\\n\")\\n        deoptcodes = {}\\n        for basic in opmap:\\n            deoptcodes[basic] = basic\\n        for basic, family in opcode[\"_specializations\"].items():\\n            for specialized in family:\\n                deoptcodes[specialized] = basic\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Deopt[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\nconst uint8_t _PyOpcode_Original[256] = {\\n\")\\n        for opt, deopt in sorted(deoptcodes.items()):\\n            if opt.startswith(\"EXTENDED_ARG\"):\\n                deopt = \"EXTENDED_ARG_QUICK\"\\n            iobj.write(f\"    [{opt}] = {deopt},\\n\")\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        fobj.write(\"\\n\")\\n        fobj.write(\"\\n        for op in hasconst:\\n            fobj.write(f\"\\n    || ((op) == {op}) \\\\\")\\n        fobj.write(\"\\n    )\\n\")\\n        fobj.write(\"\\n\")\\n        for i, (op, _) in enumerate(opcode[\"_nb_ops\"]):\\n            fobj.write(DEFINE.format(op, i))\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"static const char *const _PyOpcode_OpName[256] = {\\n\")\\n        for op, name in enumerate(opname_including_specialized):\\n            if name[0] != \"<\":\\n                op = name\\n            iobj.write(f'')\\n        iobj.write(\"};\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        iobj.write(\"\\n\")\\n        for i, flag in enumerate(used):\\n            if not flag:\\n                iobj.write(f\"    case {i}: \\\\\\n\")\\n        iobj.write(\"        ;\\n\")\\n        fobj.write(footer)\\n        iobj.write(internal_footer)"
  },
  {
    "code": "def _for(self, dist):\\n        vars(self).update(dist=dist)\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _for(self, dist):\\n        vars(self).update(dist=dist)\\n        return self"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        asn=dict(required=True, type='str'),\\n        vrf=dict(required=False, type='str', default='default'),\\n        bestpath_always_compare_med=dict(required=False, type='bool'),\\n        bestpath_aspath_multipath_relax=dict(required=False, type='bool'),\\n        bestpath_compare_neighborid=dict(required=False, type='bool'),\\n        bestpath_compare_routerid=dict(required=False, type='bool'),\\n        bestpath_cost_community_ignore=dict(required=False, type='bool'),\\n        bestpath_med_confed=dict(required=False, type='bool'),\\n        bestpath_med_missing_as_worst=dict(required=False, type='bool'),\\n        bestpath_med_non_deterministic=dict(required=False, type='bool'),\\n        cluster_id=dict(required=False, type='str'),\\n        confederation_id=dict(required=False, type='str'),\\n        confederation_peers=dict(required=False, type='str'),\\n        disable_policy_batching=dict(required=False, type='bool'),\\n        disable_policy_batching_ipv4_prefix_list=dict(required=False, type='str'),\\n        disable_policy_batching_ipv6_prefix_list=dict(required=False, type='str'),\\n        enforce_first_as=dict(required=False, type='bool'),\\n        event_history_cli=dict(required=False, choices=['true', 'false', 'default', 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        event_history_detail=dict(required=False, choices=['true', 'false', 'default', 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        event_history_events=dict(required=False, choices=['true', 'false', 'default' 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        event_history_periodic=dict(required=False, choices=['true', 'false', 'default', 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        fast_external_fallover=dict(required=False, type='bool'),\\n        flush_routes=dict(required=False, type='bool'),\\n        graceful_restart=dict(required=False, type='bool'),\\n        graceful_restart_helper=dict(required=False, type='bool'),\\n        graceful_restart_timers_restart=dict(required=False, type='str'),\\n        graceful_restart_timers_stalepath_time=dict(required=False, type='str'),\\n        isolate=dict(required=False, type='bool'),\\n        local_as=dict(required=False, type='str'),\\n        log_neighbor_changes=dict(required=False, type='bool'),\\n        maxas_limit=dict(required=False, type='str'),\\n        neighbor_down_fib_accelerate=dict(required=False, type='bool'),\\n        reconnect_interval=dict(required=False, type='str'),\\n        router_id=dict(required=False, type='str'),\\n        shutdown=dict(required=False, type='bool'),\\n        suppress_fib_pending=dict(required=False, type='bool'),\\n        timer_bestpath_limit=dict(required=False, type='str'),\\n        timer_bgp_hold=dict(required=False, type='str'),\\n        timer_bgp_keepalive=dict(required=False, type='str'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           required_together=[['timer_bgp_hold', 'timer_bgp_keepalive']],\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    state = module.params['state']\\n    if module.params['vrf'] != 'default':\\n        for param in GLOBAL_PARAMS:\\n            if module.params[param]:\\n                module.fail_json(msg='Global params can be modified only under \"default\" VRF.',\\n                                 vrf=module.params['vrf'],\\n                                 global_param=param)\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args, warnings)\\n    if existing.get('asn') and state == 'present':\\n        if existing.get('asn') != module.params['asn']:\\n            module.fail_json(msg='Another BGP ASN already exists.',\\n                             proposed_asn=module.params['asn'],\\n                             existing_asn=existing.get('asn'))\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key not in ['asn', 'vrf']:\\n            if str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key, 'default')\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif existing.get('asn') == module.params['asn']:\\n        state_absent(module, existing, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        warnings.extend(load_config(module, candidate))\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    else:\\n        result['commands'] = []\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        asn=dict(required=True, type='str'),\\n        vrf=dict(required=False, type='str', default='default'),\\n        bestpath_always_compare_med=dict(required=False, type='bool'),\\n        bestpath_aspath_multipath_relax=dict(required=False, type='bool'),\\n        bestpath_compare_neighborid=dict(required=False, type='bool'),\\n        bestpath_compare_routerid=dict(required=False, type='bool'),\\n        bestpath_cost_community_ignore=dict(required=False, type='bool'),\\n        bestpath_med_confed=dict(required=False, type='bool'),\\n        bestpath_med_missing_as_worst=dict(required=False, type='bool'),\\n        bestpath_med_non_deterministic=dict(required=False, type='bool'),\\n        cluster_id=dict(required=False, type='str'),\\n        confederation_id=dict(required=False, type='str'),\\n        confederation_peers=dict(required=False, type='str'),\\n        disable_policy_batching=dict(required=False, type='bool'),\\n        disable_policy_batching_ipv4_prefix_list=dict(required=False, type='str'),\\n        disable_policy_batching_ipv6_prefix_list=dict(required=False, type='str'),\\n        enforce_first_as=dict(required=False, type='bool'),\\n        event_history_cli=dict(required=False, choices=['true', 'false', 'default', 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        event_history_detail=dict(required=False, choices=['true', 'false', 'default', 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        event_history_events=dict(required=False, choices=['true', 'false', 'default' 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        event_history_periodic=dict(required=False, choices=['true', 'false', 'default', 'size_small', 'size_medium', 'size_large', 'size_disable']),\\n        fast_external_fallover=dict(required=False, type='bool'),\\n        flush_routes=dict(required=False, type='bool'),\\n        graceful_restart=dict(required=False, type='bool'),\\n        graceful_restart_helper=dict(required=False, type='bool'),\\n        graceful_restart_timers_restart=dict(required=False, type='str'),\\n        graceful_restart_timers_stalepath_time=dict(required=False, type='str'),\\n        isolate=dict(required=False, type='bool'),\\n        local_as=dict(required=False, type='str'),\\n        log_neighbor_changes=dict(required=False, type='bool'),\\n        maxas_limit=dict(required=False, type='str'),\\n        neighbor_down_fib_accelerate=dict(required=False, type='bool'),\\n        reconnect_interval=dict(required=False, type='str'),\\n        router_id=dict(required=False, type='str'),\\n        shutdown=dict(required=False, type='bool'),\\n        suppress_fib_pending=dict(required=False, type='bool'),\\n        timer_bestpath_limit=dict(required=False, type='str'),\\n        timer_bgp_hold=dict(required=False, type='str'),\\n        timer_bgp_keepalive=dict(required=False, type='str'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           required_together=[['timer_bgp_hold', 'timer_bgp_keepalive']],\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    state = module.params['state']\\n    if module.params['vrf'] != 'default':\\n        for param in GLOBAL_PARAMS:\\n            if module.params[param]:\\n                module.fail_json(msg='Global params can be modified only under \"default\" VRF.',\\n                                 vrf=module.params['vrf'],\\n                                 global_param=param)\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args, warnings)\\n    if existing.get('asn') and state == 'present':\\n        if existing.get('asn') != module.params['asn']:\\n            module.fail_json(msg='Another BGP ASN already exists.',\\n                             proposed_asn=module.params['asn'],\\n                             existing_asn=existing.get('asn'))\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key not in ['asn', 'vrf']:\\n            if str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key, 'default')\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    elif existing.get('asn') == module.params['asn']:\\n        state_absent(module, existing, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        warnings.extend(load_config(module, candidate))\\n        result['changed'] = True\\n        result['commands'] = candidate\\n    else:\\n        result['commands'] = []\\n    module.exit_json(**result)"
  },
  {
    "code": "def __add__(self, other):\\n        if isinstance(other, Index):\\n            warnings.warn(\"using '+' to provide set union with Indexes is deprecated, \"\\n                          \"use '|' or .union()\",FutureWarning)\\n            return self.union(other)\\n        return Index(np.array(self) + other)\\n    __iadd__ = __add__",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: provide deprecation warnings when using setlike operations on Indexes and lists (for +/-) (GH10038)",
    "fixed_code": "def __add__(self, other):\\n        if com.is_list_like(other):\\n            warnings.warn(\"using '+' to provide set union with Indexes is deprecated, \"\\n                          \"use '|' or .union()\",FutureWarning)\\n        if isinstance(other, Index):\\n            return self.union(other)\\n        return Index(np.array(self) + other)\\n    __iadd__ = __add__\\n    __radd__ = __add__"
  },
  {
    "code": "def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\\n\\ttry:\\n\\t\\tnetloc = parts.netloc.encode('idna')\\n\\texcept UnicodeError:\\n\\t\\tnetloc = parts.netloc\\n\\treturn (\\n\\t\\tto_native_str(parts.scheme),\\n\\t\\tto_native_str(netloc),\\n\\t\\tquote(to_bytes(parts.path, path_encoding), _safe_chars),\\n\\t\\tquote(to_bytes(parts.params, path_encoding), _safe_chars),\\n\\t\\tquote(to_bytes(parts.query, encoding), _safe_chars),\\n\\t\\tquote(to_bytes(parts.fragment, encoding), _safe_chars)\\n\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\\n\\ttry:\\n\\t\\tnetloc = parts.netloc.encode('idna')\\n\\texcept UnicodeError:\\n\\t\\tnetloc = parts.netloc\\n\\treturn (\\n\\t\\tto_native_str(parts.scheme),\\n\\t\\tto_native_str(netloc),\\n\\t\\tquote(to_bytes(parts.path, path_encoding), _safe_chars),\\n\\t\\tquote(to_bytes(parts.params, path_encoding), _safe_chars),\\n\\t\\tquote(to_bytes(parts.query, encoding), _safe_chars),\\n\\t\\tquote(to_bytes(parts.fragment, encoding), _safe_chars)\\n\\t)"
  },
  {
    "code": "def reset_printoptions():\\n    global GlobalPrintConfig\\n    GlobalPrintConfig.reset()\\nclass EngFormatter(object):\\n    ENG_PREFIXES = {\\n        -24: \"y\",\\n        -21: \"z\",\\n        -18: \"a\",\\n        -15: \"f\",\\n        -12: \"p\",\\n         -9: \"n\",\\n         -6: \"u\",\\n         -3: \"m\",\\n          0: \"\",\\n          3: \"k\",\\n          6: \"M\",\\n          9: \"G\",\\n         12: \"T\",\\n         15: \"P\",\\n         18: \"E\",\\n         21: \"Z\",\\n         24: \"Y\"\\n      }",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reset_printoptions():\\n    global GlobalPrintConfig\\n    GlobalPrintConfig.reset()\\nclass EngFormatter(object):\\n    ENG_PREFIXES = {\\n        -24: \"y\",\\n        -21: \"z\",\\n        -18: \"a\",\\n        -15: \"f\",\\n        -12: \"p\",\\n         -9: \"n\",\\n         -6: \"u\",\\n         -3: \"m\",\\n          0: \"\",\\n          3: \"k\",\\n          6: \"M\",\\n          9: \"G\",\\n         12: \"T\",\\n         15: \"P\",\\n         18: \"E\",\\n         21: \"Z\",\\n         24: \"Y\"\\n      }"
  },
  {
    "code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen,\\n                   match_args, kw_only, slots):\\n    fields = {}\\n    if cls.__module__ in sys.modules:\\n        globals = sys.modules[cls.__module__].__dict__\\n    else:\\n        globals = {}\\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n                                           unsafe_hash, frozen))\\n    any_frozen_base = False\\n    has_dataclass_bases = False\\n    for b in cls.__mro__[-1:0:-1]:\\n        base_fields = getattr(b, _FIELDS, None)\\n        if base_fields is not None:\\n            has_dataclass_bases = True\\n            for f in base_fields.values():\\n                fields[f.name] = f\\n            if getattr(b, _PARAMS).frozen:\\n                any_frozen_base = True\\n    cls_annotations = cls.__dict__.get('__annotations__', {})\\n    cls_fields = []\\n    dataclasses = sys.modules[__name__]\\n    for name, type in cls_annotations.items():\\n        if (_is_kw_only(type, dataclasses)\\n            or (isinstance(type, str)\\n                and _is_type(type, cls, dataclasses, dataclasses.KW_ONLY,\\n                             _is_kw_only))):\\n            kw_only = True\\n        else:\\n            cls_fields.append(_get_field(cls, name, type, kw_only))\\n    for f in cls_fields:\\n        fields[f.name] = f\\n        if isinstance(getattr(cls, f.name, None), Field):\\n            if f.default is MISSING:\\n                delattr(cls, f.name)\\n            else:\\n                setattr(cls, f.name, f.default)\\n    for name, value in cls.__dict__.items():\\n        if isinstance(value, Field) and not name in cls_annotations:\\n            raise TypeError(f'{name!r} is a field but has no type annotation')\\n    if has_dataclass_bases:\\n        if any_frozen_base and not frozen:\\n            raise TypeError('cannot inherit non-frozen dataclass from a '\\n                            'frozen one')\\n        if not any_frozen_base and frozen:\\n            raise TypeError('cannot inherit frozen dataclass from a '\\n                            'non-frozen one')\\n    setattr(cls, _FIELDS, fields)\\n    class_hash = cls.__dict__.get('__hash__', MISSING)\\n    has_explicit_hash = not (class_hash is MISSING or\\n                             (class_hash is None and '__eq__' in cls.__dict__))\\n    if order and not eq:\\n        raise ValueError('eq must be true if order is true')\\n    all_init_fields = [f for f in fields.values()\\n                       if f._field_type in (_FIELD, _FIELD_INITVAR)]\\n    (std_init_fields,\\n     kw_only_init_fields) = _fields_in_init_order(all_init_fields)\\n    if init:\\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\\n        _set_new_attribute(cls, '__init__',\\n                           _init_fn(all_init_fields,\\n                                    std_init_fields,\\n                                    kw_only_init_fields,\\n                                    frozen,\\n                                    has_post_init,\\n                                    '__dataclass_self__' if 'self' in fields\\n                                            else 'self',\\n                                    globals,\\n                          ))\\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\\n    if repr:\\n        flds = [f for f in field_list if f.repr]\\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n    if eq:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        _set_new_attribute(cls, '__eq__',\\n                           _cmp_fn('__eq__', '==',\\n                                   self_tuple, other_tuple,\\n                                   globals=globals))\\n    if order:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        for name, op in [('__lt__', '<'),\\n                         ('__le__', '<='),\\n                         ('__gt__', '>'),\\n                         ('__ge__', '>='),\\n                         ]:\\n            if _set_new_attribute(cls, name,\\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\\n                                          globals=globals)):\\n                raise TypeError(f'Cannot overwrite attribute {name} '\\n                                f'in class {cls.__name__}. Consider using '\\n                                'functools.total_ordering')\\n    if frozen:\\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\\n            if _set_new_attribute(cls, fn.__name__, fn):\\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n                                f'in class {cls.__name__}')\\n    hash_action = _hash_action[bool(unsafe_hash),\\n                               bool(eq),\\n                               bool(frozen),\\n                               has_explicit_hash]\\n    if hash_action:\\n        cls.__hash__ = hash_action(cls, field_list, globals)\\n    if not getattr(cls, '__doc__'):\\n        cls.__doc__ = (cls.__name__ +\\n                       str(inspect.signature(cls)).replace(' -> None', ''))\\n    if match_args:\\n        _set_new_attribute(cls, '__match_args__',\\n                           tuple(f.name for f in std_init_fields))\\n    if slots:\\n        cls = _add_slots(cls, frozen)\\n    abc.update_abstractmethods(cls)\\n    return cls",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-44015: dataclasses should allow KW_ONLY to be specified only once per class (GH-25841)\\n\\nbpo-44015: Raise a TypeError if KW_ONLY is specified more than once.",
    "fixed_code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen,\\n                   match_args, kw_only, slots):\\n    fields = {}\\n    if cls.__module__ in sys.modules:\\n        globals = sys.modules[cls.__module__].__dict__\\n    else:\\n        globals = {}\\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n                                           unsafe_hash, frozen))\\n    any_frozen_base = False\\n    has_dataclass_bases = False\\n    for b in cls.__mro__[-1:0:-1]:\\n        base_fields = getattr(b, _FIELDS, None)\\n        if base_fields is not None:\\n            has_dataclass_bases = True\\n            for f in base_fields.values():\\n                fields[f.name] = f\\n            if getattr(b, _PARAMS).frozen:\\n                any_frozen_base = True\\n    cls_annotations = cls.__dict__.get('__annotations__', {})\\n    cls_fields = []\\n    KW_ONLY_seen = False\\n    dataclasses = sys.modules[__name__]\\n    for name, type in cls_annotations.items():\\n        if (_is_kw_only(type, dataclasses)\\n            or (isinstance(type, str)\\n                and _is_type(type, cls, dataclasses, dataclasses.KW_ONLY,\\n                             _is_kw_only))):\\n            if KW_ONLY_seen:\\n                raise TypeError(f'{name!r} is KW_ONLY, but KW_ONLY '\\n                                'has already been specified')\\n            KW_ONLY_seen = True\\n            kw_only = True\\n        else:\\n            cls_fields.append(_get_field(cls, name, type, kw_only))\\n    for f in cls_fields:\\n        fields[f.name] = f\\n        if isinstance(getattr(cls, f.name, None), Field):\\n            if f.default is MISSING:\\n                delattr(cls, f.name)\\n            else:\\n                setattr(cls, f.name, f.default)\\n    for name, value in cls.__dict__.items():\\n        if isinstance(value, Field) and not name in cls_annotations:\\n            raise TypeError(f'{name!r} is a field but has no type annotation')\\n    if has_dataclass_bases:\\n        if any_frozen_base and not frozen:\\n            raise TypeError('cannot inherit non-frozen dataclass from a '\\n                            'frozen one')\\n        if not any_frozen_base and frozen:\\n            raise TypeError('cannot inherit frozen dataclass from a '\\n                            'non-frozen one')\\n    setattr(cls, _FIELDS, fields)\\n    class_hash = cls.__dict__.get('__hash__', MISSING)\\n    has_explicit_hash = not (class_hash is MISSING or\\n                             (class_hash is None and '__eq__' in cls.__dict__))\\n    if order and not eq:\\n        raise ValueError('eq must be true if order is true')\\n    all_init_fields = [f for f in fields.values()\\n                       if f._field_type in (_FIELD, _FIELD_INITVAR)]\\n    (std_init_fields,\\n     kw_only_init_fields) = _fields_in_init_order(all_init_fields)\\n    if init:\\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\\n        _set_new_attribute(cls, '__init__',\\n                           _init_fn(all_init_fields,\\n                                    std_init_fields,\\n                                    kw_only_init_fields,\\n                                    frozen,\\n                                    has_post_init,\\n                                    '__dataclass_self__' if 'self' in fields\\n                                            else 'self',\\n                                    globals,\\n                          ))\\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\\n    if repr:\\n        flds = [f for f in field_list if f.repr]\\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n    if eq:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        _set_new_attribute(cls, '__eq__',\\n                           _cmp_fn('__eq__', '==',\\n                                   self_tuple, other_tuple,\\n                                   globals=globals))\\n    if order:\\n        flds = [f for f in field_list if f.compare]\\n        self_tuple = _tuple_str('self', flds)\\n        other_tuple = _tuple_str('other', flds)\\n        for name, op in [('__lt__', '<'),\\n                         ('__le__', '<='),\\n                         ('__gt__', '>'),\\n                         ('__ge__', '>='),\\n                         ]:\\n            if _set_new_attribute(cls, name,\\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\\n                                          globals=globals)):\\n                raise TypeError(f'Cannot overwrite attribute {name} '\\n                                f'in class {cls.__name__}. Consider using '\\n                                'functools.total_ordering')\\n    if frozen:\\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\\n            if _set_new_attribute(cls, fn.__name__, fn):\\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n                                f'in class {cls.__name__}')\\n    hash_action = _hash_action[bool(unsafe_hash),\\n                               bool(eq),\\n                               bool(frozen),\\n                               has_explicit_hash]\\n    if hash_action:\\n        cls.__hash__ = hash_action(cls, field_list, globals)\\n    if not getattr(cls, '__doc__'):\\n        cls.__doc__ = (cls.__name__ +\\n                       str(inspect.signature(cls)).replace(' -> None', ''))\\n    if match_args:\\n        _set_new_attribute(cls, '__match_args__',\\n                           tuple(f.name for f in std_init_fields))\\n    if slots:\\n        cls = _add_slots(cls, frozen)\\n    abc.update_abstractmethods(cls)\\n    return cls"
  },
  {
    "code": "def _validate_params(self, set_max_iter=True, for_partial_fit=False):\\n        if not isinstance(self.shuffle, bool):\\n            raise ValueError(\"shuffle must be either True or False\")\\n        if self.max_iter is not None and self.max_iter <= 0:\\n            raise ValueError(\"max_iter must be > zero. Got %f\" % self.max_iter)\\n        if not (0.0 <= self.l1_ratio <= 1.0):\\n            raise ValueError(\"l1_ratio must be in [0, 1]\")\\n        if self.alpha < 0.0:\\n            raise ValueError(\"alpha must be >= 0\")\\n        if self.learning_rate in (\"constant\", \"invscaling\"):\\n            if self.eta0 <= 0.0:\\n                raise ValueError(\"eta0 must be > 0\")\\n        if self.learning_rate == \"optimal\" and self.alpha == 0:\\n            raise ValueError(\"alpha must be > 0 since \"\\n                             \"learning_rate is 'optimal'. alpha is used \"\\n                             \"to compute the optimal learning rate.\")\\n        self._get_penalty_type(self.penalty)\\n        self._get_learning_rate_type(self.learning_rate)\\n        if self.loss not in self.loss_functions:\\n            raise ValueError(\"The loss %s is not supported. \" % self.loss)\\n        if not set_max_iter:\\n            return\\n        self._tol = self.tol\\n        if self.n_iter is not None:\\n            warnings.warn(\"n_iter parameter is deprecated in 0.19 and will be\"\\n                          \" removed in 0.21. Use max_iter and tol instead.\",\\n                          DeprecationWarning)\\n            max_iter = self.n_iter\\n            self._tol = None\\n        elif self.tol is None and self.max_iter is None:\\n            if not for_partial_fit:\\n                warnings.warn(\\n                    \"max_iter and tol parameters have been \"\\n                    \"added in %s in 0.19. If both are left unset, \"\\n                    \"they default to max_iter=5 and tol=None. \"\\n                    \"If tol is not None, max_iter defaults to max_iter=1000. \"\\n                    \"From 0.21, default max_iter will be 1000, and\"\\n                    \" default tol will be 1e-3.\" % type(self), FutureWarning)\\n            max_iter = 5\\n        else:\\n            max_iter = self.max_iter if self.max_iter is not None else 1000\\n        self._max_iter = max_iter",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _validate_params(self, set_max_iter=True, for_partial_fit=False):\\n        if not isinstance(self.shuffle, bool):\\n            raise ValueError(\"shuffle must be either True or False\")\\n        if self.max_iter is not None and self.max_iter <= 0:\\n            raise ValueError(\"max_iter must be > zero. Got %f\" % self.max_iter)\\n        if not (0.0 <= self.l1_ratio <= 1.0):\\n            raise ValueError(\"l1_ratio must be in [0, 1]\")\\n        if self.alpha < 0.0:\\n            raise ValueError(\"alpha must be >= 0\")\\n        if self.learning_rate in (\"constant\", \"invscaling\"):\\n            if self.eta0 <= 0.0:\\n                raise ValueError(\"eta0 must be > 0\")\\n        if self.learning_rate == \"optimal\" and self.alpha == 0:\\n            raise ValueError(\"alpha must be > 0 since \"\\n                             \"learning_rate is 'optimal'. alpha is used \"\\n                             \"to compute the optimal learning rate.\")\\n        self._get_penalty_type(self.penalty)\\n        self._get_learning_rate_type(self.learning_rate)\\n        if self.loss not in self.loss_functions:\\n            raise ValueError(\"The loss %s is not supported. \" % self.loss)\\n        if not set_max_iter:\\n            return\\n        self._tol = self.tol\\n        if self.n_iter is not None:\\n            warnings.warn(\"n_iter parameter is deprecated in 0.19 and will be\"\\n                          \" removed in 0.21. Use max_iter and tol instead.\",\\n                          DeprecationWarning)\\n            max_iter = self.n_iter\\n            self._tol = None\\n        elif self.tol is None and self.max_iter is None:\\n            if not for_partial_fit:\\n                warnings.warn(\\n                    \"max_iter and tol parameters have been \"\\n                    \"added in %s in 0.19. If both are left unset, \"\\n                    \"they default to max_iter=5 and tol=None. \"\\n                    \"If tol is not None, max_iter defaults to max_iter=1000. \"\\n                    \"From 0.21, default max_iter will be 1000, and\"\\n                    \" default tol will be 1e-3.\" % type(self), FutureWarning)\\n            max_iter = 5\\n        else:\\n            max_iter = self.max_iter if self.max_iter is not None else 1000\\n        self._max_iter = max_iter"
  },
  {
    "code": "def _get_object(data, position, as_class, tz_aware, uuid_subtype):\\n\\tobj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\\n\\tencoded = data[position + 4:position + obj_size - 1]\\n\\tobject = _elements_to_dict(encoded, as_class, tz_aware, uuid_subtype)\\n\\tposition += obj_size\\n\\tif \"$ref\" in object:\\n\\t\\treturn (DBRef(object.pop(\"$ref\"), object.pop(\"$id\"),\\n\\t\\t\\t\\t\\t  object.pop(\"$db\", None), object), position)\\n\\treturn object, position",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix null pointer when decoding invalid DBRef PYTHON-532",
    "fixed_code": "def _get_object(data, position, as_class, tz_aware, uuid_subtype):\\n\\tobj_size = struct.unpack(\"<i\", data[position:position + 4])[0]\\n\\tencoded = data[position + 4:position + obj_size - 1]\\n\\tobject = _elements_to_dict(encoded, as_class, tz_aware, uuid_subtype)\\n\\tposition += obj_size\\n\\tif \"$ref\" in object:\\n\\t\\treturn (DBRef(object.pop(\"$ref\"), object.pop(\"$id\", None),\\n\\t\\t\\t\\t\\t  object.pop(\"$db\", None), object), position)\\n\\treturn object, position"
  },
  {
    "code": "def _setitem_frame(self, key, value):\\n\\t\\tif key.values.size and not com.is_bool_dtype(key.values):\\n\\t\\t\\traise TypeError('Must pass DataFrame with boolean values only')\\n\\t\\tself._check_inplace_setting(value)\\n\\t\\tself._check_setitem_copy()\\n\\t\\tself.where(-key, value, inplace=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _setitem_frame(self, key, value):\\n\\t\\tif key.values.size and not com.is_bool_dtype(key.values):\\n\\t\\t\\traise TypeError('Must pass DataFrame with boolean values only')\\n\\t\\tself._check_inplace_setting(value)\\n\\t\\tself._check_setitem_copy()\\n\\t\\tself.where(-key, value, inplace=True)"
  },
  {
    "code": "def validate_baseindexer_support(func_name: Optional[str]) -> None:\\n    BASEINDEXER_WHITELIST = {\\n        \"count\",\\n        \"min\",\\n        \"max\",\\n        \"mean\",\\n        \"sum\",\\n        \"median\",\\n        \"std\",\\n        \"var\",\\n        \"kurt\",\\n        \"quantile\",\\n    }\\n    if isinstance(func_name, str) and func_name not in BASEINDEXER_WHITELIST:\\n        raise NotImplementedError(\\n            f\"{func_name} is not supported with using a BaseIndexer \"\\n            f\"subclasses. You can use .apply() with {func_name}.\"\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: support skew function for custom BaseIndexer rolling windows (#33745)",
    "fixed_code": "def validate_baseindexer_support(func_name: Optional[str]) -> None:\\n    BASEINDEXER_WHITELIST = {\\n        \"count\",\\n        \"min\",\\n        \"max\",\\n        \"mean\",\\n        \"sum\",\\n        \"median\",\\n        \"std\",\\n        \"var\",\\n        \"skew\",\\n        \"kurt\",\\n        \"quantile\",\\n    }\\n    if isinstance(func_name, str) and func_name not in BASEINDEXER_WHITELIST:\\n        raise NotImplementedError(\\n            f\"{func_name} is not supported with using a BaseIndexer \"\\n            f\"subclasses. You can use .apply() with {func_name}.\"\\n        )"
  },
  {
    "code": "def factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None):\\n    values = _ensure_arraylike(values)\\n    original = values\\n    if is_extension_array_dtype(values):\\n        values = getattr(values, '_values', values)\\n        labels, uniques = values.factorize(na_sentinel=na_sentinel)\\n        dtype = original.dtype\\n    else:\\n        values, dtype, _ = _ensure_data(values)\\n        if (is_datetime64_any_dtype(original) or\\n                is_timedelta64_dtype(original) or\\n                is_period_dtype(original)):\\n            na_value = na_value_for_dtype(original.dtype)\\n        else:\\n            na_value = None\\n        labels, uniques = _factorize_array(values,\\n                                           na_sentinel=na_sentinel,\\n                                           size_hint=size_hint,\\n                                           na_value=na_value)\\n    if sort and len(uniques) > 0:\\n        from pandas.core.sorting import safe_sort\\n        try:\\n            order = uniques.argsort()\\n            order2 = order.argsort()\\n            labels = take_1d(order2, labels, fill_value=na_sentinel)\\n            uniques = uniques.take(order)\\n        except TypeError:\\n            uniques, labels = safe_sort(uniques, labels,\\n                                        na_sentinel=na_sentinel,\\n                                        assume_unique=True)\\n    uniques = _reconstruct_data(uniques, dtype, original)\\n    if isinstance(original, ABCIndexClass):\\n        uniques = original._shallow_copy(uniques, name=None)\\n    elif isinstance(original, ABCSeries):\\n        from pandas import Index\\n        uniques = Index(uniques)\\n    return labels, uniques",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix usage of na_sentinel with sort=True in factorize() (#25592)",
    "fixed_code": "def factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None):\\n    values = _ensure_arraylike(values)\\n    original = values\\n    if is_extension_array_dtype(values):\\n        values = getattr(values, '_values', values)\\n        labels, uniques = values.factorize(na_sentinel=na_sentinel)\\n        dtype = original.dtype\\n    else:\\n        values, dtype, _ = _ensure_data(values)\\n        if (is_datetime64_any_dtype(original) or\\n                is_timedelta64_dtype(original) or\\n                is_period_dtype(original)):\\n            na_value = na_value_for_dtype(original.dtype)\\n        else:\\n            na_value = None\\n        labels, uniques = _factorize_array(values,\\n                                           na_sentinel=na_sentinel,\\n                                           size_hint=size_hint,\\n                                           na_value=na_value)\\n    if sort and len(uniques) > 0:\\n        from pandas.core.sorting import safe_sort\\n        if na_sentinel == -1:\\n            try:\\n                order = uniques.argsort()\\n                order2 = order.argsort()\\n                labels = take_1d(order2, labels, fill_value=na_sentinel)\\n                uniques = uniques.take(order)\\n            except TypeError:\\n                uniques, labels = safe_sort(uniques, labels,\\n                                            na_sentinel=na_sentinel,\\n                                            assume_unique=True)\\n        else:\\n            uniques, labels = safe_sort(uniques, labels,\\n                                        na_sentinel=na_sentinel,\\n                                        assume_unique=True)\\n    uniques = _reconstruct_data(uniques, dtype, original)\\n    if isinstance(original, ABCIndexClass):\\n        uniques = original._shallow_copy(uniques, name=None)\\n    elif isinstance(original, ABCSeries):\\n        from pandas import Index\\n        uniques = Index(uniques)\\n    return labels, uniques"
  },
  {
    "code": "def time_is_quarter_end(self, tz):\\n        self.ts.is_quarter_end",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fastpaths for Timestamp properties (#18539)",
    "fixed_code": "def time_is_quarter_end(self, tz, freq):\\n        self.ts.is_quarter_end"
  },
  {
    "code": "def _dump_registry(cls, file=None):\\n        print(\"Class: %s.%s\" % (cls.__module__, cls.__name__), file=file)\\n        print(\"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter, file=file)\\n        for name in sorted(cls.__dict__.keys()):\\n            if name.startswith(\"_abc_\"):\\n                value = getattr(cls, name)\\n                print(\"%s: %r\" % (name, value), file=file)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #22032: __qualname__ instead of __name__ is now always used to format fully qualified class names of Python implemented classes.",
    "fixed_code": "def _dump_registry(cls, file=None):\\n        print(\"Class: %s.%s\" % (cls.__module__, cls.__qualname__), file=file)\\n        print(\"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter, file=file)\\n        for name in sorted(cls.__dict__.keys()):\\n            if name.startswith(\"_abc_\"):\\n                value = getattr(cls, name)\\n                print(\"%s: %r\" % (name, value), file=file)"
  },
  {
    "code": "def _check_actions_uniqueness(self, obj):\\n        errors = []\\n        names = collections.Counter(name for _, name, _ in obj._get_base_actions())\\n        for name, count in names.items():\\n            if count > 1:\\n                errors.append(checks.Error(\\n                    '__name__ attributes of actions defined in %s must be '\\n                    'unique. Name %r is not unique.' % (\\n                        obj.__class__.__name__,\\n                        name,\\n                    ),\\n                    obj=obj.__class__,\\n                    id='admin.E130',\\n                ))\\n        return errors",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_actions_uniqueness(self, obj):\\n        errors = []\\n        names = collections.Counter(name for _, name, _ in obj._get_base_actions())\\n        for name, count in names.items():\\n            if count > 1:\\n                errors.append(checks.Error(\\n                    '__name__ attributes of actions defined in %s must be '\\n                    'unique. Name %r is not unique.' % (\\n                        obj.__class__.__name__,\\n                        name,\\n                    ),\\n                    obj=obj.__class__,\\n                    id='admin.E130',\\n                ))\\n        return errors"
  },
  {
    "code": "def dependencies(self):\\n        if self._metadata:\\n            return self._metadata.dependencies\\n        elif len(self.versions) > 1:\\n            return None\\n        self._get_metadata()\\n        return self._metadata.dependencies",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not error if collection specified null dependencies (#67575)",
    "fixed_code": "def dependencies(self):\\n        if not self._metadata:\\n            if len(self.versions) > 1:\\n                return {}\\n            self._get_metadata()\\n        dependencies = self._metadata.dependencies\\n        if dependencies is None:\\n            return {}\\n        return dependencies"
  },
  {
    "code": "def do_related_class(self, other, cls):\\n        self.set_attributes_from_rel()\\n        related = RelatedObject(other, cls, self)\\n        if not cls._meta.abstract:\\n            self.contribute_to_related_class(other, related)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #9023 -- Corrected a problem where cached attribute values would cause a delete to cascade to a related object even when the relationship had been set to None. Thanks to TheShark for the report and test case, and to juriejan and Jacob for their work on the patch.",
    "fixed_code": "def do_related_class(self, other, cls):\\n        self.set_attributes_from_rel()\\n        self.related = RelatedObject(other, cls, self)\\n        if not cls._meta.abstract:\\n            self.contribute_to_related_class(other, self.related)"
  },
  {
    "code": "def _parse_data(schema, rows):\\n    dtype_map = {'FLOAT': np.dtype(float),\\n                 'TIMESTAMP': 'M8[ns]'}\\n    fields = schema['fields']\\n    col_types = [field['type'] for field in fields]\\n    col_names = [str(field['name']) for field in fields]\\n    col_dtypes = [dtype_map.get(field['type'], object) for field in fields]\\n    page_array = np.zeros((len(rows),), dtype=lzip(col_names, col_dtypes))\\n    for row_num, raw_row in enumerate(rows):\\n        entries = raw_row.get('f', [])\\n        for col_num, field_type in enumerate(col_types):\\n            field_value = _parse_entry(entries[col_num].get('v', ''),\\n                                       field_type)\\n            page_array[row_num][col_num] = field_value\\n    return DataFrame(page_array, columns=col_names)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_data(schema, rows):\\n    dtype_map = {'FLOAT': np.dtype(float),\\n                 'TIMESTAMP': 'M8[ns]'}\\n    fields = schema['fields']\\n    col_types = [field['type'] for field in fields]\\n    col_names = [str(field['name']) for field in fields]\\n    col_dtypes = [dtype_map.get(field['type'], object) for field in fields]\\n    page_array = np.zeros((len(rows),), dtype=lzip(col_names, col_dtypes))\\n    for row_num, raw_row in enumerate(rows):\\n        entries = raw_row.get('f', [])\\n        for col_num, field_type in enumerate(col_types):\\n            field_value = _parse_entry(entries[col_num].get('v', ''),\\n                                       field_type)\\n            page_array[row_num][col_num] = field_value\\n    return DataFrame(page_array, columns=col_names)"
  },
  {
    "code": "def _stack_multi_columns(frame, level=-1, dropna=True):\\n    this = frame.copy()\\n    if level != frame.columns.nlevels - 1:\\n        roll_columns = this.columns\\n        for i in range(level, frame.columns.nlevels - 1):\\n            roll_columns = roll_columns.swaplevel(i, i + 1)\\n        this.columns = roll_columns\\n    if not this.columns.is_lexsorted():\\n        this = this.sortlevel(0, axis=1)\\n    if len(frame.columns.levels) > 2:\\n        tuples = list(zip(*[\\n            lev.take(lab) for lev, lab in\\n            zip(this.columns.levels[:-1], this.columns.labels[:-1])\\n        ]))\\n        unique_groups = [key for key, _ in itertools.groupby(tuples)]\\n        new_names = this.columns.names[:-1]\\n        new_columns = MultiIndex.from_tuples(unique_groups, names=new_names)\\n    else:\\n        new_columns = unique_groups = this.columns.levels[0]\\n    new_data = {}\\n    level_vals = this.columns.levels[-1]\\n    levsize = len(level_vals)\\n    drop_cols = []\\n    for key in unique_groups:\\n        loc = this.columns.get_loc(key)\\n        slice_len = loc.stop - loc.start\\n        if slice_len == 0:\\n            drop_cols.append(key)\\n            continue\\n        elif slice_len != levsize:\\n            chunk = this.ix[:, this.columns[loc]]\\n            chunk.columns = level_vals.take(chunk.columns.labels[-1])\\n            value_slice = chunk.reindex(columns=level_vals).values\\n        else:\\n            if frame._is_mixed_type:\\n                value_slice = this.ix[:, this.columns[loc]].values\\n            else:\\n                value_slice = this.values[:, loc]\\n        new_data[key] = value_slice.ravel()\\n    if len(drop_cols) > 0:\\n        new_columns = new_columns - drop_cols\\n    N = len(this)\\n    if isinstance(this.index, MultiIndex):\\n        new_levels = list(this.index.levels)\\n        new_names = list(this.index.names)\\n        new_labels = [lab.repeat(levsize) for lab in this.index.labels]\\n    else:\\n        new_levels = [this.index]\\n        new_labels = [np.arange(N).repeat(levsize)]\\n        new_names = [this.index.name]  \\n    new_levels.append(frame.columns.levels[level])\\n    new_labels.append(np.tile(np.arange(levsize), N))\\n    new_names.append(frame.columns.names[level])\\n    new_index = MultiIndex(levels=new_levels, labels=new_labels,\\n                           names=new_names, verify_integrity=False)\\n    result = DataFrame(new_data, index=new_index, columns=new_columns)\\n    if dropna:\\n        result = result.dropna(axis=0, how='all')\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _stack_multi_columns(frame, level=-1, dropna=True):\\n    this = frame.copy()\\n    if level != frame.columns.nlevels - 1:\\n        roll_columns = this.columns\\n        for i in range(level, frame.columns.nlevels - 1):\\n            roll_columns = roll_columns.swaplevel(i, i + 1)\\n        this.columns = roll_columns\\n    if not this.columns.is_lexsorted():\\n        this = this.sortlevel(0, axis=1)\\n    if len(frame.columns.levels) > 2:\\n        tuples = list(zip(*[\\n            lev.take(lab) for lev, lab in\\n            zip(this.columns.levels[:-1], this.columns.labels[:-1])\\n        ]))\\n        unique_groups = [key for key, _ in itertools.groupby(tuples)]\\n        new_names = this.columns.names[:-1]\\n        new_columns = MultiIndex.from_tuples(unique_groups, names=new_names)\\n    else:\\n        new_columns = unique_groups = this.columns.levels[0]\\n    new_data = {}\\n    level_vals = this.columns.levels[-1]\\n    levsize = len(level_vals)\\n    drop_cols = []\\n    for key in unique_groups:\\n        loc = this.columns.get_loc(key)\\n        slice_len = loc.stop - loc.start\\n        if slice_len == 0:\\n            drop_cols.append(key)\\n            continue\\n        elif slice_len != levsize:\\n            chunk = this.ix[:, this.columns[loc]]\\n            chunk.columns = level_vals.take(chunk.columns.labels[-1])\\n            value_slice = chunk.reindex(columns=level_vals).values\\n        else:\\n            if frame._is_mixed_type:\\n                value_slice = this.ix[:, this.columns[loc]].values\\n            else:\\n                value_slice = this.values[:, loc]\\n        new_data[key] = value_slice.ravel()\\n    if len(drop_cols) > 0:\\n        new_columns = new_columns - drop_cols\\n    N = len(this)\\n    if isinstance(this.index, MultiIndex):\\n        new_levels = list(this.index.levels)\\n        new_names = list(this.index.names)\\n        new_labels = [lab.repeat(levsize) for lab in this.index.labels]\\n    else:\\n        new_levels = [this.index]\\n        new_labels = [np.arange(N).repeat(levsize)]\\n        new_names = [this.index.name]  \\n    new_levels.append(frame.columns.levels[level])\\n    new_labels.append(np.tile(np.arange(levsize), N))\\n    new_names.append(frame.columns.names[level])\\n    new_index = MultiIndex(levels=new_levels, labels=new_labels,\\n                           names=new_names, verify_integrity=False)\\n    result = DataFrame(new_data, index=new_index, columns=new_columns)\\n    if dropna:\\n        result = result.dropna(axis=0, how='all')\\n    return result"
  },
  {
    "code": "def put(self, item, block=1):\\n\\t\\tif block:\\n\\t\\t\\tself.fsema.acquire()\\n\\t\\telif not self.fsema.acquire(0):\\n\\t\\t\\traise Full\\n\\t\\tself.mutex.acquire()\\n\\t\\trelease_fsema = True\\n\\t\\ttry:\\n\\t\\t\\twas_empty = self._empty()\\n\\t\\t\\tself._put(item)\\n\\t\\t\\tif was_empty:\\n\\t\\t\\t\\tself.esema.release()\\n\\t\\t\\trelease_fsema = not self._full()\\n\\t\\tfinally:\\n\\t\\t\\tif release_fsema:\\n\\t\\t\\t\\tself.fsema.release()\\n\\t\\t\\tself.mutex.release()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def put(self, item, block=1):\\n\\t\\tif block:\\n\\t\\t\\tself.fsema.acquire()\\n\\t\\telif not self.fsema.acquire(0):\\n\\t\\t\\traise Full\\n\\t\\tself.mutex.acquire()\\n\\t\\trelease_fsema = True\\n\\t\\ttry:\\n\\t\\t\\twas_empty = self._empty()\\n\\t\\t\\tself._put(item)\\n\\t\\t\\tif was_empty:\\n\\t\\t\\t\\tself.esema.release()\\n\\t\\t\\trelease_fsema = not self._full()\\n\\t\\tfinally:\\n\\t\\t\\tif release_fsema:\\n\\t\\t\\t\\tself.fsema.release()\\n\\t\\t\\tself.mutex.release()"
  },
  {
    "code": "def subclass_exception(name, parents, module):\\n    return type(name, parents, {'__module__': module})",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #17776 - DoesNotExist is not picklable\\n\\nThanks to ambv for the report",
    "fixed_code": "def subclass_exception(name, parents, module, attached_to=None):\\n    class_dict = {'__module__': module}\\n    if attached_to is not None:"
  },
  {
    "code": "def _nanpercentile_1d(\\n    values: np.ndarray,\\n    mask: npt.NDArray[np.bool_],\\n    qs: npt.NDArray[np.float64],\\n    na_value: Scalar,\\n    interpolation,\\n) -> Scalar | np.ndarray:\\n    values = values[~mask]\\n    if len(values) == 0:\\n        return np.array([na_value] * len(qs), dtype=values.dtype)\\n    return np.percentile(values, qs, **{np_percentile_argname: interpolation})",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: PandasArray._quantile when empty (#46110)",
    "fixed_code": "def _nanpercentile_1d(\\n    values: np.ndarray,\\n    mask: npt.NDArray[np.bool_],\\n    qs: npt.NDArray[np.float64],\\n    na_value: Scalar,\\n    interpolation,\\n) -> Scalar | np.ndarray:\\n    values = values[~mask]\\n    if len(values) == 0:\\n        return np.full(len(qs), na_value)\\n    return np.percentile(values, qs, **{np_percentile_argname: interpolation})"
  },
  {
    "code": "def get_host_regex(self, spider):\\n\\t\\tallowed_domains = getattr(spider, 'allowed_domains', None)\\n\\t\\tif not allowed_domains:\\n\\t\\t\\treturn re.compile('')  \\n\\t\\turl_pattern = re.compile(\"^https?://.*$\")\\n\\t\\tdomains = []\\n\\t\\tfor domain in allowed_domains:\\n\\t\\t\\tif domain is None:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telif url_pattern.match(domain):\\n\\t\\t\\t\\tmessage = (\"allowed_domains accepts only domains, not URLs. \"\\n\\t\\t\\t\\t\\t\\t   \"Ignoring URL entry %s in allowed_domains.\" % domain)\\n\\t\\t\\t\\twarnings.warn(message, URLWarning)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdomains.append(re.escape(domain))\\n\\t\\tregex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\\n\\t\\treturn re.compile(regex)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_host_regex(self, spider):\\n\\t\\tallowed_domains = getattr(spider, 'allowed_domains', None)\\n\\t\\tif not allowed_domains:\\n\\t\\t\\treturn re.compile('')  \\n\\t\\turl_pattern = re.compile(\"^https?://.*$\")\\n\\t\\tdomains = []\\n\\t\\tfor domain in allowed_domains:\\n\\t\\t\\tif domain is None:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telif url_pattern.match(domain):\\n\\t\\t\\t\\tmessage = (\"allowed_domains accepts only domains, not URLs. \"\\n\\t\\t\\t\\t\\t\\t   \"Ignoring URL entry %s in allowed_domains.\" % domain)\\n\\t\\t\\t\\twarnings.warn(message, URLWarning)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdomains.append(re.escape(domain))\\n\\t\\tregex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\\n\\t\\treturn re.compile(regex)"
  },
  {
    "code": "def __isub__(self, other):  \\n        result = self - other\\n        self[:] = result[:]\\n        if not is_period_dtype(self):\\n            self._freq = result._freq\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __isub__(self, other):  \\n        result = self - other\\n        self[:] = result[:]\\n        if not is_period_dtype(self):\\n            self._freq = result._freq\\n        return self"
  },
  {
    "code": "def track(self, release_gil=False, nframe=1):\\n\\t\\tframes = get_frames(nframe, 1)\\n\\t\\t_testcapi.tracemalloc_track(self.domain, self.ptr, self.size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\trelease_gil)\\n\\t\\treturn frames",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def track(self, release_gil=False, nframe=1):\\n\\t\\tframes = get_frames(nframe, 1)\\n\\t\\t_testcapi.tracemalloc_track(self.domain, self.ptr, self.size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\trelease_gil)\\n\\t\\treturn frames"
  },
  {
    "code": "def __setattr__(self, name, value):\\n        if name in self._internal_names_set:\\n            object.__setattr__(self, name, value)\\n        elif name in self._metadata:\\n            return object.__setattr__(self, name, value)\\n        else:\\n            try:\\n                existing = getattr(self, name)\\n                if isinstance(existing, Index):\\n                    object.__setattr__(self, name, value)\\n                elif name in self._info_axis:\\n                    self[name] = value\\n                else:\\n                    object.__setattr__(self, name, value)\\n            except (AttributeError, TypeError):\\n                object.__setattr__(self, name, value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: update NDFrame __setattr__ to match behavior of __getattr__ (GH8994)",
    "fixed_code": "def __setattr__(self, name, value):\\n        try:\\n            object.__getattribute__(self, name)\\n            return object.__setattr__(self, name, value)\\n        except AttributeError:\\n            pass\\n        if name in self._internal_names_set:\\n            object.__setattr__(self, name, value)\\n        elif name in self._metadata:\\n            object.__setattr__(self, name, value)\\n        else:\\n            try:\\n                existing = getattr(self, name)\\n                if isinstance(existing, Index):\\n                    object.__setattr__(self, name, value)\\n                elif name in self._info_axis:\\n                    self[name] = value\\n                else:\\n                    object.__setattr__(self, name, value)\\n            except (AttributeError, TypeError):\\n                object.__setattr__(self, name, value)"
  },
  {
    "code": "def mask_missing(arr, values_to_mask):\\n    if not isinstance(values_to_mask, (list, np.ndarray)):\\n        values_to_mask = [values_to_mask]\\n    try:\\n        values_to_mask = np.array(values_to_mask, dtype=arr.dtype)\\n    except Exception:\\n        values_to_mask = np.array(values_to_mask, dtype=object)\\n    na_mask = isnull(values_to_mask)\\n    nonna = values_to_mask[-na_mask]\\n    mask = None\\n    for x in nonna:\\n        if mask is None:\\n            mask = arr == x\\n            if not isinstance(mask, np.ndarray):\\n                m = mask\\n                mask = np.empty(arr.shape, dtype=np.bool)\\n                mask.fill(m)\\n        else:\\n            mask = mask | (arr == x)\\n    if na_mask.any():\\n        if mask is None:\\n            mask = isnull(arr)\\n        else:\\n            mask = mask | isnull(arr)\\n    return mask",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix bool no-op replace calls",
    "fixed_code": "def mask_missing(arr, values_to_mask):\\n    if not isinstance(values_to_mask, (list, np.ndarray)):\\n        values_to_mask = [values_to_mask]\\n    try:\\n        values = np.array(values_to_mask)\\n        cant_cast = not np.can_cast(values.dtype, arr.dtype, casting='safe')\\n        if cant_cast and arr.dtype == np.bool_:\\n            values_to_mask = values\\n        else:\\n            values_to_mask = np.array(values_to_mask, dtype=arr.dtype)\\n    except Exception:\\n        values_to_mask = np.array(values_to_mask, dtype=object)\\n    na_mask = isnull(values_to_mask)\\n    nonna = values_to_mask[-na_mask]\\n    mask = None\\n    for x in nonna:\\n        if mask is None:\\n            mask = arr == x\\n            if not isinstance(mask, np.ndarray):\\n                m = mask\\n                mask = np.empty(arr.shape, dtype=np.bool)\\n                mask.fill(m)\\n        else:\\n            mask = mask | (arr == x)\\n    if na_mask.any():\\n        if mask is None:\\n            mask = isnull(arr)\\n        else:\\n            mask = mask | isnull(arr)\\n    return mask"
  },
  {
    "code": "def save_picklebuffer(self, obj):\\n        if self.proto < 5:\\n            raise PicklingError(\"PickleBuffer can only pickled with \"\\n                                \"protocol >= 5\")\\n        with obj.raw() as m:\\n            if not m.contiguous:\\n                raise PicklingError(\"PickleBuffer can not be pickled when \"\\n                                    \"pointing to a non-contiguous buffer\")\\n            in_band = True\\n            if self._buffer_callback is not None:\\n                in_band = bool(self._buffer_callback(obj))\\n            if in_band:\\n                if m.readonly:\\n                    self.save_bytes(m.tobytes())\\n                else:\\n                    self.save_bytearray(m.tobytes())\\n            else:\\n                self.write(NEXT_BUFFER)\\n                if m.readonly:\\n                    self.write(READONLY_BUFFER)\\n    dispatch[PickleBuffer] = save_picklebuffer",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-37210: Fix pure Python pickle when _pickle is unavailable (GH-14016)\\n\\nAllow pure Python implementation of pickle to work\\neven when the C _pickle module is unavailable.\\n\\nFix test_pickle when _pickle is missing: declare PyPicklerHookTests\\noutside \"if has_c_implementation:\" block.",
    "fixed_code": "def save_picklebuffer(self, obj):\\n            if self.proto < 5:\\n                raise PicklingError(\"PickleBuffer can only pickled with \"\\n                                    \"protocol >= 5\")\\n            with obj.raw() as m:\\n                if not m.contiguous:\\n                    raise PicklingError(\"PickleBuffer can not be pickled when \"\\n                                        \"pointing to a non-contiguous buffer\")\\n                in_band = True\\n                if self._buffer_callback is not None:\\n                    in_band = bool(self._buffer_callback(obj))\\n                if in_band:\\n                    if m.readonly:\\n                        self.save_bytes(m.tobytes())\\n                    else:\\n                        self.save_bytearray(m.tobytes())\\n                else:\\n                    self.write(NEXT_BUFFER)\\n                    if m.readonly:\\n                        self.write(READONLY_BUFFER)\\n        dispatch[PickleBuffer] = save_picklebuffer"
  },
  {
    "code": "def check_estimators_dtypes(name, Estimator):\\n    rnd = np.random.RandomState(0)\\n    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)\\n    X_train_64 = X_train_32.astype(np.float64)\\n    X_train_int_64 = X_train_32.astype(np.int64)\\n    X_train_int_32 = X_train_32.astype(np.int32)\\n    y = X_train_int_64[:, 0]\\n    y = multioutput_estimator_convert_y_2d(name, y)\\n    if name in DEPRECATED_TRANSFORM:\\n        methods = [\"predict\", \"decision_function\", \"predict_proba\"]\\n    else:\\n        methods = [\\n            \"predict\", \"transform\", \"decision_function\", \"predict_proba\"]\\n    for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:\\n        estimator = Estimator()\\n        set_testing_parameters(estimator)\\n        set_random_state(estimator, 1)\\n        estimator.fit(X_train, y)\\n        for method in methods:\\n            if hasattr(estimator, method):\\n                getattr(estimator, method)(X_train)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_estimators_dtypes(name, Estimator):\\n    rnd = np.random.RandomState(0)\\n    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)\\n    X_train_64 = X_train_32.astype(np.float64)\\n    X_train_int_64 = X_train_32.astype(np.int64)\\n    X_train_int_32 = X_train_32.astype(np.int32)\\n    y = X_train_int_64[:, 0]\\n    y = multioutput_estimator_convert_y_2d(name, y)\\n    if name in DEPRECATED_TRANSFORM:\\n        methods = [\"predict\", \"decision_function\", \"predict_proba\"]\\n    else:\\n        methods = [\\n            \"predict\", \"transform\", \"decision_function\", \"predict_proba\"]\\n    for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:\\n        estimator = Estimator()\\n        set_testing_parameters(estimator)\\n        set_random_state(estimator, 1)\\n        estimator.fit(X_train, y)\\n        for method in methods:\\n            if hasattr(estimator, method):\\n                getattr(estimator, method)(X_train)"
  },
  {
    "code": "def _validate_estimator(self, default=None):\\n        if self.n_estimators <= 0:\\n            raise ValueError(\"n_estimators must be greater than zero, \"\\n                             \"got {0}.\".format(self.n_estimators))\\n        if self.base_estimator is not None:\\n            self.base_estimator_ = self.base_estimator\\n        else:\\n            self.base_estimator_ = default\\n        if self.base_estimator_ is None:\\n            raise ValueError(\"base_estimator cannot be None\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _validate_estimator(self, default=None):\\n        if self.n_estimators <= 0:\\n            raise ValueError(\"n_estimators must be greater than zero, \"\\n                             \"got {0}.\".format(self.n_estimators))\\n        if self.base_estimator is not None:\\n            self.base_estimator_ = self.base_estimator\\n        else:\\n            self.base_estimator_ = default\\n        if self.base_estimator_ is None:\\n            raise ValueError(\"base_estimator cannot be None\")"
  },
  {
    "code": "def set_index(self, keys, drop=True, inplace=False,\\n                  verify_integrity=False):\\n        if not isinstance(keys, (list, tuple)):\\n            keys = [keys]\\n        if inplace:\\n            frame = self\\n        else:\\n            frame = self.copy()\\n        arrays = []\\n        for col in keys:\\n            if isinstance(col, (list, Series, np.ndarray)):\\n                level = col\\n            else:\\n                level = frame[col]\\n                if drop:\\n                    del frame[col]\\n            arrays.append(level)\\n        index = MultiIndex.from_arrays(arrays, names=keys)\\n        if verify_integrity and not index.is_unique:\\n            duplicates = index.get_duplicates()\\n            raise Exception('Index has duplicate keys: %s' % duplicates)\\n        index._cleanup()\\n        frame.index = index\\n        return frame",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_index(self, keys, drop=True, inplace=False,\\n                  verify_integrity=False):\\n        if not isinstance(keys, (list, tuple)):\\n            keys = [keys]\\n        if inplace:\\n            frame = self\\n        else:\\n            frame = self.copy()\\n        arrays = []\\n        for col in keys:\\n            if isinstance(col, (list, Series, np.ndarray)):\\n                level = col\\n            else:\\n                level = frame[col]\\n                if drop:\\n                    del frame[col]\\n            arrays.append(level)\\n        index = MultiIndex.from_arrays(arrays, names=keys)\\n        if verify_integrity and not index.is_unique:\\n            duplicates = index.get_duplicates()\\n            raise Exception('Index has duplicate keys: %s' % duplicates)\\n        index._cleanup()\\n        frame.index = index\\n        return frame"
  },
  {
    "code": "def str_replace(arr, pat, repl, n=-1, case=None, flags=0):\\n    if not (is_string_like(repl) or callable(repl)):\\n        raise TypeError(\"repl must be a string or callable\")\\n    is_compiled_re = is_re(pat)\\n    if is_compiled_re:\\n        if (case is not None) or (flags != 0):\\n            raise ValueError(\"case and flags cannot be set\"\\n                             \" when pat is a compiled regex\")\\n    else:\\n        if case is None:\\n            case = True\\n        if case is False:\\n            flags |= re.IGNORECASE\\n    use_re = is_compiled_re or len(pat) > 1 or flags or callable(repl)\\n    if use_re:\\n        n = n if n >= 0 else 0\\n        regex = re.compile(pat, flags=flags)\\n        f = lambda x: regex.sub(repl=repl, string=x, count=n)\\n    else:\\n        f = lambda x: x.replace(pat, repl, n)\\n    return _na_map(f, arr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def str_replace(arr, pat, repl, n=-1, case=None, flags=0):\\n    if not (is_string_like(repl) or callable(repl)):\\n        raise TypeError(\"repl must be a string or callable\")\\n    is_compiled_re = is_re(pat)\\n    if is_compiled_re:\\n        if (case is not None) or (flags != 0):\\n            raise ValueError(\"case and flags cannot be set\"\\n                             \" when pat is a compiled regex\")\\n    else:\\n        if case is None:\\n            case = True\\n        if case is False:\\n            flags |= re.IGNORECASE\\n    use_re = is_compiled_re or len(pat) > 1 or flags or callable(repl)\\n    if use_re:\\n        n = n if n >= 0 else 0\\n        regex = re.compile(pat, flags=flags)\\n        f = lambda x: regex.sub(repl=repl, string=x, count=n)\\n    else:\\n        f = lambda x: x.replace(pat, repl, n)\\n    return _na_map(f, arr)"
  },
  {
    "code": "def _EndRecData64(fpin, offset, endrec):\\n    try:\\n        fpin.seek(offset - sizeEndCentDir64Locator, 2)\\n    except IOError:\\n        return endrec\\n    data = fpin.read(sizeEndCentDir64Locator)\\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\\n    if sig != stringEndArchive64Locator:\\n        return endrec\\n    if diskno != 0 or disks != 1:\\n        raise BadZipfile(\"zipfiles that span multiple disks are not supported\")\\n    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)\\n    data = fpin.read(sizeEndCentDir64)\\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\\n            dircount, dircount2, dirsize, diroffset = \\\\n            struct.unpack(structEndArchive64, data)\\n    if sig != stringEndArchive64:\\n        return endrec\\n    endrec[_ECD_SIGNATURE] = sig\\n    endrec[_ECD_DISK_NUMBER] = disk_num\\n    endrec[_ECD_DISK_START] = disk_dir\\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\\n    endrec[_ECD_SIZE] = dirsize\\n    endrec[_ECD_OFFSET] = diroffset\\n    return endrec",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#7351: add more consistent exception name alias.",
    "fixed_code": "def _EndRecData64(fpin, offset, endrec):\\n    try:\\n        fpin.seek(offset - sizeEndCentDir64Locator, 2)\\n    except IOError:\\n        return endrec\\n    data = fpin.read(sizeEndCentDir64Locator)\\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\\n    if sig != stringEndArchive64Locator:\\n        return endrec\\n    if diskno != 0 or disks != 1:\\n        raise BadZipZile(\"zipfiles that span multiple disks are not supported\")\\n    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)\\n    data = fpin.read(sizeEndCentDir64)\\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\\n            dircount, dircount2, dirsize, diroffset = \\\\n            struct.unpack(structEndArchive64, data)\\n    if sig != stringEndArchive64:\\n        return endrec\\n    endrec[_ECD_SIGNATURE] = sig\\n    endrec[_ECD_DISK_NUMBER] = disk_num\\n    endrec[_ECD_DISK_START] = disk_dir\\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\\n    endrec[_ECD_SIZE] = dirsize\\n    endrec[_ECD_OFFSET] = diroffset\\n    return endrec"
  },
  {
    "code": "def duplicated(self, subset=None, take_last=False):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def duplicated(self, subset=None, take_last=False):"
  },
  {
    "code": "def read_axes(self, where):\\n        if not self.infer_axes(): return False\\n        self.selection = Selection(self, where)\\n        self.selection.select()\\n        for a in self.axes:\\n            a.convert(self.selection)\\n        return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_axes(self, where):\\n        if not self.infer_axes(): return False\\n        self.selection = Selection(self, where)\\n        self.selection.select()\\n        for a in self.axes:\\n            a.convert(self.selection)\\n        return True"
  },
  {
    "code": "def _convert_index(index):\\n    index_name = getattr(index,'name',None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(index.values, 'integer', atom, freq=getattr(index,'freq',None),\\n                        index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                               v.microsecond / 1E6) for v in values],\\n                             dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                             dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(converted, 'string', _tables().StringCol(itemsize), itemsize=itemsize,\\n                        index_name=index_name)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: provide py3k string decoding and compat",
    "fixed_code": "def _convert_index(index, encoding=None):\\n    index_name = getattr(index,'name',None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(index.values, 'integer', atom, freq=getattr(index,'freq',None),\\n                        index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                               v.microsecond / 1E6) for v in values],\\n                             dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                             dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(converted, 'string', _tables().StringCol(itemsize), itemsize=itemsize,\\n                        index_name=index_name)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)"
  },
  {
    "code": "def parse_uri(self, uri):\\n\\t\\tif not uri.startswith('sheepdog://'):\\n\\t\\t\\traise exception.BadStoreUri(uri, \"URI must start with %s://\" %\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'sheepdog')\\n\\t\\tself.image = uri[11:]",
    "label": 1,
    "bug_type": "security",
    "bug_description": "To prevent remote code injection on Sheepdog store",
    "fixed_code": "def parse_uri(self, uri):\\n\\t\\tvalid_schema = 'sheepdog://'\\n\\t\\tif not uri.startswith(valid_schema):\\n\\t\\t\\traise exception.BadStoreUri(_(\"URI must start with %s://\") %\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvalid_schema)\\n\\t\\tself.image = uri[len(valid_schema):]\\n\\t\\tif not utils.is_uuid_like(self.image):\\n\\t\\t\\traise exception.BadStoreUri(_(\"URI must contains well-formated \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \"image id\"))"
  },
  {
    "code": "def text_to_word_sequence(text,\\n\\t\\t\\t\\t\\t\\t  filters='!\"\\n',\\n\\t\\t\\t\\t\\t\\t  lower=True, split=\" \"):\\n\\tif lower:\\n\\t\\ttext = text.lower()\\n\\tif sys.version_info < (3,):\\n\\t\\tif isinstance(text, unicode):\\n\\t\\t\\ttranslate_map = dict((ord(c), unicode(split)) for c in filters)\\n\\t\\t\\ttext = text.translate(translate_map)\\n\\t\\telif len(split) == 1:\\n\\t\\t\\ttranslate_map = maketrans(filters, split * len(filters))\\n\\t\\t\\ttext = text.translate(translate_map)\\n\\t\\telse:\\n\\t\\t\\tfor c in filters:\\n\\t\\t\\t\\ttext = text.replace(c, split)\\n\\telse:\\n\\t\\ttranslate_dict = dict((c, split) for c in filters)\\n\\t\\ttranslate_map = maketrans(translate_dict)\\n\\t\\ttext = text.translate(translate_map)\\n\\tseq = text.split(split)\\n\\treturn [i for i in seq if i]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def text_to_word_sequence(text,\\n\\t\\t\\t\\t\\t\\t  filters='!\"\\n',\\n\\t\\t\\t\\t\\t\\t  lower=True, split=\" \"):\\n\\tif lower:\\n\\t\\ttext = text.lower()\\n\\tif sys.version_info < (3,):\\n\\t\\tif isinstance(text, unicode):\\n\\t\\t\\ttranslate_map = dict((ord(c), unicode(split)) for c in filters)\\n\\t\\t\\ttext = text.translate(translate_map)\\n\\t\\telif len(split) == 1:\\n\\t\\t\\ttranslate_map = maketrans(filters, split * len(filters))\\n\\t\\t\\ttext = text.translate(translate_map)\\n\\t\\telse:\\n\\t\\t\\tfor c in filters:\\n\\t\\t\\t\\ttext = text.replace(c, split)\\n\\telse:\\n\\t\\ttranslate_dict = dict((c, split) for c in filters)\\n\\t\\ttranslate_map = maketrans(translate_dict)\\n\\t\\ttext = text.translate(translate_map)\\n\\tseq = text.split(split)\\n\\treturn [i for i in seq if i]"
  },
  {
    "code": "def is_bool_indexer(key: Any) -> bool:\\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\\n        is_array_like(key) and is_extension_array_dtype(key.dtype)\\n    ):\\n        if key.dtype == np.object_:\\n            key = np.asarray(key)\\n            if not lib.is_bool_array(key):\\n                na_msg = \"Cannot mask with non-boolean array containing NA / NaN values\"\\n                if isna(key).any():\\n                    raise ValueError(na_msg)\\n                return False\\n            return True\\n        elif is_bool_dtype(key.dtype):\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  \\n            return False\\n    return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: loc.__getitem__[[na_value]] with CategoricalIndex containing NAs (#37722)",
    "fixed_code": "def is_bool_indexer(key: Any) -> bool:\\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\\n        is_array_like(key) and is_extension_array_dtype(key.dtype)\\n    ):\\n        if key.dtype == np.object_:\\n            key = np.asarray(key)\\n            if not lib.is_bool_array(key):\\n                na_msg = \"Cannot mask with non-boolean array containing NA / NaN values\"\\n                if lib.infer_dtype(key) == \"boolean\" and isna(key).any():\\n                    raise ValueError(na_msg)\\n                return False\\n            return True\\n        elif is_bool_dtype(key.dtype):\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  \\n            return False\\n    return False"
  },
  {
    "code": "def get_expiry_age(self, expiry=None):\\n        if expiry is None:\\n            expiry = self.get('_session_expiry')\\n        if not expiry:   \\n            return settings.SESSION_COOKIE_AGE\\n        if not isinstance(expiry, datetime):\\n            return expiry\\n        delta = expiry - timezone.now()\\n        return delta.days * 86400 + delta.seconds",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added optional kwargs to get_expiry_age/date.\\n\\nThis change allows for cleaner tests: we can test the exact output.\\n\\nRefs #18194: this change makes it possible to compute session expiry\\ndates at times other than when the session is saved.\\n\\nFixed #18458: the existence of the `modification` kwarg implies that you\\nmust pass it to get_expiry_age/date if you call these functions outside\\nof a short request - response cycle (the intended use case).",
    "fixed_code": "def get_expiry_age(self, **kwargs):\\n        try:\\n            modification = kwargs['modification']\\n        except KeyError:\\n            modification = timezone.now()\\n        try:\\n            expiry = kwargs['expiry']\\n        except KeyError:\\n            expiry = self.get('_session_expiry')\\n        if not expiry:   \\n            return settings.SESSION_COOKIE_AGE\\n        if not isinstance(expiry, datetime):\\n            return expiry\\n        delta = expiry - modification\\n        return delta.days * 86400 + delta.seconds"
  },
  {
    "code": "def main():\\n    argument_spec = cloudscale_argument_spec()\\n    argument_spec.update(dict(\\n        state=dict(default='running', choices=ALLOWED_STATES),\\n        name=dict(),\\n        uuid=dict(),\\n        flavor=dict(),\\n        image=dict(),\\n        volume_size_gb=dict(type='int', default=10),\\n        bulk_volume_size_gb=dict(type='int'),\\n        ssh_keys=dict(type='list'),\\n        use_public_network=dict(type='bool', default=True),\\n        use_private_network=dict(type='bool', default=False),\\n        use_ipv6=dict(type='bool', default=True),\\n        anti_affinity_with=dict(),\\n        user_data=dict(),\\n    ))\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_one_of=(('name', 'uuid'),),\\n        mutually_exclusive=(('name', 'uuid'),),\\n        supports_check_mode=True,\\n    )\\n    target_state = module.params['state']\\n    server = AnsibleCloudscaleServer(module)\\n    if server.info['state'] not in ALLOWED_STATES:\\n        server.wait_for_state(ALLOWED_STATES)\\n    current_state = server.info['state']\\n    if module.check_mode:\\n        module.exit_json(changed=not target_state == current_state,\\n                         **server.info)\\n    changed = False\\n    if current_state == 'absent' and target_state == 'running':\\n        server.create_server()\\n        changed = True\\n    elif current_state == 'absent' and target_state == 'stopped':\\n        server.create_server()\\n        server.stop_server()\\n        changed = True\\n    elif current_state == 'stopped' and target_state == 'running':\\n        server.start_server()\\n        changed = True\\n    elif current_state in ('running', 'stopped') and target_state == 'absent':\\n        server.delete_server()\\n        changed = True\\n    elif current_state == 'running' and target_state == 'stopped':\\n        server.stop_server()\\n        changed = True\\n    module.exit_json(changed=changed, **server.info)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = cloudscale_argument_spec()\\n    argument_spec.update(dict(\\n        state=dict(default='running', choices=ALLOWED_STATES),\\n        name=dict(),\\n        uuid=dict(),\\n        flavor=dict(),\\n        image=dict(),\\n        volume_size_gb=dict(type='int', default=10),\\n        bulk_volume_size_gb=dict(type='int'),\\n        ssh_keys=dict(type='list'),\\n        use_public_network=dict(type='bool', default=True),\\n        use_private_network=dict(type='bool', default=False),\\n        use_ipv6=dict(type='bool', default=True),\\n        anti_affinity_with=dict(),\\n        user_data=dict(),\\n    ))\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_one_of=(('name', 'uuid'),),\\n        mutually_exclusive=(('name', 'uuid'),),\\n        supports_check_mode=True,\\n    )\\n    target_state = module.params['state']\\n    server = AnsibleCloudscaleServer(module)\\n    if server.info['state'] not in ALLOWED_STATES:\\n        server.wait_for_state(ALLOWED_STATES)\\n    current_state = server.info['state']\\n    if module.check_mode:\\n        module.exit_json(changed=not target_state == current_state,\\n                         **server.info)\\n    changed = False\\n    if current_state == 'absent' and target_state == 'running':\\n        server.create_server()\\n        changed = True\\n    elif current_state == 'absent' and target_state == 'stopped':\\n        server.create_server()\\n        server.stop_server()\\n        changed = True\\n    elif current_state == 'stopped' and target_state == 'running':\\n        server.start_server()\\n        changed = True\\n    elif current_state in ('running', 'stopped') and target_state == 'absent':\\n        server.delete_server()\\n        changed = True\\n    elif current_state == 'running' and target_state == 'stopped':\\n        server.stop_server()\\n        changed = True\\n    module.exit_json(changed=changed, **server.info)"
  },
  {
    "code": "def _convert_index(index, encoding=None):\\n    index_name = getattr(index, 'name', None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None), tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(\\n            index.values, 'integer', atom, freq=getattr(index, 'freq', None),\\n                        index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise TypeError('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None), tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                               v.microsecond / 1E6) for v in values],\\n                             dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.array([v.toordinal() for v in values],\\n                             dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(\\n            converted, 'string', _tables().StringCol(itemsize), itemsize=itemsize,\\n                        index_name=index_name)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ER/API: unicode indices not supported on table formats in py2 (GH5386)",
    "fixed_code": "def _convert_index(index, encoding=None, format_type=None):\\n    index_name = getattr(index, 'name', None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None), tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(\\n            index.values, 'integer', atom, freq=getattr(index, 'freq', None),\\n                        index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise TypeError('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index, 'freq', None), tz=getattr(index, 'tz', None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                               v.microsecond / 1E6) for v in values],\\n                             dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.array([v.toordinal() for v in values],\\n                             dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(\\n            converted, 'string', _tables().StringCol(itemsize), itemsize=itemsize,\\n                        index_name=index_name)\\n    elif inferred_type == 'unicode':\\n        if format_type == 'fixed':\\n            atom = _tables().ObjectAtom()\\n            return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                            index_name=index_name)\\n        raise TypeError(\\n            \"[unicode] is not supported as a in index type for [{0}] formats\".format(format_type))\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)"
  },
  {
    "code": "def formfield_for_dbfield(self, db_field, **kwargs):\\n        if isinstance(db_field, models.GeometryField) and db_field.dim < 3:\\n            kwargs.pop('request', None)\\n            kwargs['widget'] = self.get_map_widget(db_field)\\n            return db_field.formfield(**kwargs)\\n        else:\\n            return super(GeoModelAdmin, self).formfield_for_dbfield(db_field, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25985 -- Updated signature of ModelAdmin.formfield_for_* to make request a positional argument.",
    "fixed_code": "def formfield_for_dbfield(self, db_field, request, **kwargs):\\n        if isinstance(db_field, models.GeometryField) and db_field.dim < 3:\\n            kwargs['widget'] = self.get_map_widget(db_field)\\n            return db_field.formfield(**kwargs)\\n        else:\\n            return super(GeoModelAdmin, self).formfield_for_dbfield(db_field, request, **kwargs)"
  },
  {
    "code": "def __init__(self, samba_conn_id: str = default_conn_name) -> None:\\n        super().__init__()\\n        self.conn = self.get_connection(samba_conn_id)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Switch to 'smbprotocol' library (#17273)",
    "fixed_code": "def __init__(self, samba_conn_id: str = default_conn_name, share: Optional[str] = None) -> None:\\n        super().__init__()\\n        conn = self.get_connection(samba_conn_id)\\n        if not conn.login:\\n            self.log.info(\"Login not provided\")\\n        if not conn.password:\\n            self.log.info(\"Password not provided\")\\n        self._host = conn.host\\n        self._share = share or conn.schema\\n        self._connection_cache = connection_cache = {}\\n        self._conn_kwargs = {\\n            \"username\": conn.login,\\n            \"password\": conn.password,\\n            \"port\": conn.port or 445,\\n            \"connection_cache\": connection_cache,\\n        }"
  },
  {
    "code": "def fetch_celery_task_state(celery_task: Tuple[TaskInstanceKeyType, AsyncResult]) \\\\n        -> Union[TaskInstanceStateType, ExceptionWithTraceback]:\\n    try:\\n        with timeout(seconds=OPERATION_TIMEOUT):\\n            return celery_task[0], celery_task[1].state\\n    except Exception as e:  \\n        exception_traceback = \"Celery Task ID: {}\\n{}\".format(celery_task[0],\\n                                                              traceback.format_exc())\\n        return ExceptionWithTraceback(e, exception_traceback)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fetch_celery_task_state(celery_task: Tuple[TaskInstanceKeyType, AsyncResult]) \\\\n        -> Union[TaskInstanceStateType, ExceptionWithTraceback]:\\n    try:\\n        with timeout(seconds=OPERATION_TIMEOUT):\\n            return celery_task[0], celery_task[1].state\\n    except Exception as e:  \\n        exception_traceback = \"Celery Task ID: {}\\n{}\".format(celery_task[0],\\n                                                              traceback.format_exc())\\n        return ExceptionWithTraceback(e, exception_traceback)"
  },
  {
    "code": "def get_distribution_version():\\n    ''\\n    version = None\\n    needs_best_version = frozenset((\\n        u'centos',\\n        u'debian',\\n    ))\\n    version = distro.version()\\n    distro_id = distro.id()\\n    if version is not None:\\n        if distro_id in needs_best_version:\\n            version_best = distro.version(best=True)\\n            if distro_id == u'centos':\\n                version = u'.'.join(version_best.split(u'.')[:2])\\n            if distro_id == u'debian':\\n                version = version_best\\n    else:\\n        version = u''\\n    return version",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_distribution_version():\\n    ''\\n    version = None\\n    needs_best_version = frozenset((\\n        u'centos',\\n        u'debian',\\n    ))\\n    version = distro.version()\\n    distro_id = distro.id()\\n    if version is not None:\\n        if distro_id in needs_best_version:\\n            version_best = distro.version(best=True)\\n            if distro_id == u'centos':\\n                version = u'.'.join(version_best.split(u'.')[:2])\\n            if distro_id == u'debian':\\n                version = version_best\\n    else:\\n        version = u''\\n    return version"
  },
  {
    "code": "def upgrade(migrate_engine):\\n\\tmeta = MetaData()\\n\\tmeta.bind = migrate_engine\\n\\tservices = Table('services', meta, autoload=True)\\n\\taggregates = Table('aggregates', meta, autoload=True)\\n\\taggregate_metadata = Table('aggregate_metadata', meta, autoload=True)\\n\\trecord_list = list(services.select().execute())\\n\\tfor rec in record_list:\\n\\t\\tif rec['binary'] != 'nova-compute':\\n\\t\\t\\tcontinue\\n\\t\\tresult = aggregate_metadata.select().where(\\n\\t\\t\\taggregate_metadata.c.key == 'availability_zone').where(\\n\\t\\t\\taggregate_metadata.c.value == rec['availability_zone']).execute()\\n\\t\\tresult = [r for r in result]\\n\\t\\tif len(result) > 0:\\n\\t\\t\\tagg_id = result[0].aggregate_id\\n\\t\\telse:\\n\\t\\t\\tagg = aggregates.insert()\\n\\t\\t\\tresult = agg.execute({'name': rec['availability_zone']})\\n\\t\\t\\tagg_id = result.inserted_primary_key[0]\\n\\t\\t\\trow = aggregate_metadata.insert()\\n\\t\\t\\trow.execute({'created_at': rec['created_at'],\\n\\t\\t\\t\\t\\t\\t'updated_at': rec['updated_at'],\\n\\t\\t\\t\\t\\t\\t'deleted_at': rec['deleted_at'],\\n\\t\\t\\t\\t\\t\\t'deleted': rec['deleted'],\\n\\t\\t\\t\\t\\t\\t'key': 'availability_zone',\\n\\t\\t\\t\\t\\t\\t'value': rec['availability_zone'],\\n\\t\\t\\t\\t\\t\\t'aggregate_id': agg_id,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\tagg_hosts = Table('aggregate_hosts', meta, autoload=True)\\n\\t\\trow = agg_hosts.insert()\\n\\t\\trow.execute({'host': rec['host'], 'aggregate_id': agg_id})\\n\\tservices.drop_column('availability_zone')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def upgrade(migrate_engine):\\n\\tmeta = MetaData()\\n\\tmeta.bind = migrate_engine\\n\\tservices = Table('services', meta, autoload=True)\\n\\taggregates = Table('aggregates', meta, autoload=True)\\n\\taggregate_metadata = Table('aggregate_metadata', meta, autoload=True)\\n\\trecord_list = list(services.select().execute())\\n\\tfor rec in record_list:\\n\\t\\tif rec['binary'] != 'nova-compute':\\n\\t\\t\\tcontinue\\n\\t\\tresult = aggregate_metadata.select().where(\\n\\t\\t\\taggregate_metadata.c.key == 'availability_zone').where(\\n\\t\\t\\taggregate_metadata.c.value == rec['availability_zone']).execute()\\n\\t\\tresult = [r for r in result]\\n\\t\\tif len(result) > 0:\\n\\t\\t\\tagg_id = result[0].aggregate_id\\n\\t\\telse:\\n\\t\\t\\tagg = aggregates.insert()\\n\\t\\t\\tresult = agg.execute({'name': rec['availability_zone']})\\n\\t\\t\\tagg_id = result.inserted_primary_key[0]\\n\\t\\t\\trow = aggregate_metadata.insert()\\n\\t\\t\\trow.execute({'created_at': rec['created_at'],\\n\\t\\t\\t\\t\\t\\t'updated_at': rec['updated_at'],\\n\\t\\t\\t\\t\\t\\t'deleted_at': rec['deleted_at'],\\n\\t\\t\\t\\t\\t\\t'deleted': rec['deleted'],\\n\\t\\t\\t\\t\\t\\t'key': 'availability_zone',\\n\\t\\t\\t\\t\\t\\t'value': rec['availability_zone'],\\n\\t\\t\\t\\t\\t\\t'aggregate_id': agg_id,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\tagg_hosts = Table('aggregate_hosts', meta, autoload=True)\\n\\t\\trow = agg_hosts.insert()\\n\\t\\trow.execute({'host': rec['host'], 'aggregate_id': agg_id})\\n\\tservices.drop_column('availability_zone')"
  },
  {
    "code": "def get_app_list(self, request, app_label=None):\\n        app_dict = self._build_app_dict(request, app_label)\\n        app_list = sorted(app_dict.values(), key=lambda x: x[\"name\"].lower())\\n        for app in app_list:\\n            app[\"models\"].sort(key=lambda x: x[\"name\"])\\n        return app_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_app_list(self, request, app_label=None):\\n        app_dict = self._build_app_dict(request, app_label)\\n        app_list = sorted(app_dict.values(), key=lambda x: x[\"name\"].lower())\\n        for app in app_list:\\n            app[\"models\"].sort(key=lambda x: x[\"name\"])\\n        return app_list"
  },
  {
    "code": "def __getitem__(self, indx):\\n        obj = ndarray.__getitem__(self, indx)\\n        if isinstance(obj, ndarray):\\n            if obj.dtype.fields:\\n                return obj.view(dtype=(self.dtype.type, obj.dtype.descr))\\n            else:\\n                return obj.view(type=ndarray)\\n        else:\\n            return obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, indx):\\n        obj = ndarray.__getitem__(self, indx)\\n        if isinstance(obj, ndarray):\\n            if obj.dtype.fields:\\n                return obj.view(dtype=(self.dtype.type, obj.dtype.descr))\\n            else:\\n                return obj.view(type=ndarray)\\n        else:\\n            return obj"
  },
  {
    "code": "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\\n\\tif (dtype(x) == 'float64' and\\n\\t\\t\\tStrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\\n\\t\\tx = tf.cast(x, 'float32')\\n\\ttf_data_format = 'NHWC'\\n\\tif data_format == 'channels_first':\\n\\t\\tif not _has_nchw_support() or force_transpose:\\n\\t\\t\\tx = tf.transpose(x, (0, 2, 3, 1))  \\n\\t\\telse:\\n\\t\\t\\ttf_data_format = 'NCHW'\\n\\treturn x, tf_data_format",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\\n\\tif (dtype(x) == 'float64' and\\n\\t\\t\\tStrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\\n\\t\\tx = tf.cast(x, 'float32')\\n\\ttf_data_format = 'NHWC'\\n\\tif data_format == 'channels_first':\\n\\t\\tif not _has_nchw_support() or force_transpose:\\n\\t\\t\\tx = tf.transpose(x, (0, 2, 3, 1))  \\n\\t\\telse:\\n\\t\\t\\ttf_data_format = 'NCHW'\\n\\treturn x, tf_data_format"
  },
  {
    "code": "def cascaded_union(self):\\n        \"Returns a cascaded union of this MultiPolygon.\"\\n        if GEOS_PREPARE:\\n            return GEOSGeometry(capi.geos_cascaded_union(self.ptr), self.srid)\\n        else:\\n            raise GEOSException('The cascaded union operation requires GEOS 3.1+.')\\nGeometryCollection._allowed = (Point, LineString, LinearRing, Polygon, MultiPoint, MultiLineString, MultiPolygon)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Dropped support for GEOS < 3.1",
    "fixed_code": "def cascaded_union(self):\\n        \"Returns a cascaded union of this MultiPolygon.\"\\n        return GEOSGeometry(capi.geos_cascaded_union(self.ptr), self.srid)\\nGeometryCollection._allowed = (Point, LineString, LinearRing, Polygon, MultiPoint, MultiLineString, MultiPolygon)"
  },
  {
    "code": "def close(self):\\n        self._sock = _closedsocket()\\n        self.send = self.recv = self.sendto = self.recvfrom = self._sock._dummy\\n    close.__doc__ = _realsocket.close.__doc__",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Release all forwarded functions in .close. Fixes #1513223.",
    "fixed_code": "def close(self):\\n        self._sock = _closedsocket()\\n        dummy = self._sock._dummy\\n        for method in _delegate_methods:\\n            setattr(self, method, dummy)\\n    close.__doc__ = _realsocket.close.__doc__"
  },
  {
    "code": "def addOptionAfterLine(option, value, iface, lines, last_line_dict, iface_options, address_family):\\n    if option == 'method':\\n        changed = False\\n        for ln in lines:\\n            if ln.get('line_type', '') == 'iface' and ln.get('iface', '') == iface and value != ln.get('params', {}).get('method', ''):\\n                changed = True\\n                ln['line'] = re.sub(ln.get('params', {}).get('method', '') + '$', value, ln.get('line'))\\n                ln['params']['method'] = value\\n        return changed, lines\\n    last_line = last_line_dict['line']\\n    prefix_start = last_line.find(last_line.split()[0])\\n    suffix_start = last_line.rfind(last_line.split()[-1]) + len(last_line.split()[-1])\\n    prefix = last_line[:prefix_start]\\n    if len(iface_options) < 1:\\n        prefix += \"    \"\\n    line = prefix + \"%s %s\" % (option, value) + last_line[suffix_start:]\\n    option_dict = optionDict(line, iface, option, value, address_family)\\n    index = len(lines) - lines[::-1].index(last_line_dict)\\n    lines.insert(index, option_dict)\\n    return True, lines",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def addOptionAfterLine(option, value, iface, lines, last_line_dict, iface_options, address_family):\\n    if option == 'method':\\n        changed = False\\n        for ln in lines:\\n            if ln.get('line_type', '') == 'iface' and ln.get('iface', '') == iface and value != ln.get('params', {}).get('method', ''):\\n                changed = True\\n                ln['line'] = re.sub(ln.get('params', {}).get('method', '') + '$', value, ln.get('line'))\\n                ln['params']['method'] = value\\n        return changed, lines\\n    last_line = last_line_dict['line']\\n    prefix_start = last_line.find(last_line.split()[0])\\n    suffix_start = last_line.rfind(last_line.split()[-1]) + len(last_line.split()[-1])\\n    prefix = last_line[:prefix_start]\\n    if len(iface_options) < 1:\\n        prefix += \"    \"\\n    line = prefix + \"%s %s\" % (option, value) + last_line[suffix_start:]\\n    option_dict = optionDict(line, iface, option, value, address_family)\\n    index = len(lines) - lines[::-1].index(last_line_dict)\\n    lines.insert(index, option_dict)\\n    return True, lines"
  },
  {
    "code": "def check_estimators_partial_fit_n_features(name, Alg):\\n    if not hasattr(Alg, 'partial_fit'):\\n        return\\n    X, y = make_blobs(n_samples=50, random_state=1)\\n    X -= X.min()\\n    with warnings.catch_warnings(record=True):\\n        alg = Alg()\\n    if not hasattr(alg, 'partial_fit'):\\n        return\\n    set_testing_parameters(alg)\\n    try:\\n        if isinstance(alg, ClassifierMixin):\\n            classes = np.unique(y)\\n            alg.partial_fit(X, y, classes=classes)\\n        else:\\n            alg.partial_fit(X, y)\\n    except NotImplementedError:\\n        return\\n    assert_raises(ValueError, alg.partial_fit, X[:, :-1], y)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_estimators_partial_fit_n_features(name, Alg):\\n    if not hasattr(Alg, 'partial_fit'):\\n        return\\n    X, y = make_blobs(n_samples=50, random_state=1)\\n    X -= X.min()\\n    with ignore_warnings(category=DeprecationWarning):\\n        alg = Alg()\\n    if not hasattr(alg, 'partial_fit'):\\n        return\\n    set_testing_parameters(alg)\\n    try:\\n        if isinstance(alg, ClassifierMixin):\\n            classes = np.unique(y)\\n            alg.partial_fit(X, y, classes=classes)\\n        else:\\n            alg.partial_fit(X, y)\\n    except NotImplementedError:\\n        return\\n    assert_raises(ValueError, alg.partial_fit, X[:, :-1], y)"
  },
  {
    "code": "def copy(self, deep=False):\\n        values = self.asi8.copy()\\n        return type(self)(values, dtype=self.dtype, freq=self.freq)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: do freq validation in DTA.__init__ (#24686)",
    "fixed_code": "def copy(self, deep=False):\\n        values = self.asi8.copy()\\n        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)"
  },
  {
    "code": "def remove(self, key, where):\\n        group = getattr(self.handle.root, key, None)\\n        if group is not None:\\n            return self._delete_group(group, where)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "remove unit tests, enable to remove full node. docs",
    "fixed_code": "def remove(self, key, where=None):\\n        if where is None:\\n            self.handle.removeNode(self.handle.root, key, recursive=True)\\n        else:\\n            group = getattr(self.handle.root, key)\\n            self._delete_from_table(group, where)"
  },
  {
    "code": "def _write_wide_table(self, group, panel, append=False, comp=None, **kwargs):\\n        self._write_table(group, items=panel.items, index=panel.major_axis,\\n                          columns=panel.minor_axis, values=panel.values,\\n                          append=append, compression=comp, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactor of PyTables support to allow multiple table types.\\n\\n  This commit allows for support of multiple table types in a pytables hdf file,\\n  supporting the existing infrastructure in a backwards compatible manner (LegacyTable)\\n  while extending to a slightly modified format to support AppendableTables and future support of WORMTables\\n\\n  AppendableTables are implementations of the current table format with two enhancements:\\n     1) mixed dtype support\\n     2) writing routines in cython for enhanced performance\\n\\n  WORMTables (not implemented - but pretty straightforward)\\n    these tables can support a fixed 'table' (meaning not-appendable), that is searchable via queries\\n    this would have greatly enhanced write performance compared with AppendableTables, and a similar read performance profile\\n\\n  In addition, the tables allow for arbitrary axes to be indexed (e.g. you could save a panel that allows indexing on major_axis,minor_axis AND items),\\n    so all dimensions are queryable (currently only major/minor axes allow this query)\\n\\n  all tests pass (with 1 exception)\\n    a frame table round-trip - we fail on a comparison of a sorted index of the frame vs the index of the table (which is as written), not sure why this should be the case?",
    "fixed_code": "def _write_wide_table(self, group, panel, append=False, comp=None, **kwargs):\\n        t = create_table(self, group, typ = 'appendable_panel')\\n        t.write(axes_to_index=[1,2], obj=panel,\\n                append=append, compression=comp, **kwargs)"
  },
  {
    "code": "def get_offset(name):\\n    if name not in _dont_uppercase:\\n        name = name.upper()\\n        name = _lite_rule_alias.get(name, name)\\n        name = _lite_rule_alias.get(name.lower(), name)\\n    else:\\n        name = _lite_rule_alias.get(name, name)\\n    if name not in _offset_map:\\n        try:\\n            split = name.split('-')\\n            klass = prefix_mapping[split[0]]\\n            offset = klass._from_name(*split[1:])\\n        except (ValueError, TypeError, KeyError):\\n            raise ValueError(_INVALID_FREQ_ERROR.format(name))\\n        _offset_map[name] = offset\\n    return _offset_map[name].copy()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_offset(name):\\n    if name not in _dont_uppercase:\\n        name = name.upper()\\n        name = _lite_rule_alias.get(name, name)\\n        name = _lite_rule_alias.get(name.lower(), name)\\n    else:\\n        name = _lite_rule_alias.get(name, name)\\n    if name not in _offset_map:\\n        try:\\n            split = name.split('-')\\n            klass = prefix_mapping[split[0]]\\n            offset = klass._from_name(*split[1:])\\n        except (ValueError, TypeError, KeyError):\\n            raise ValueError(_INVALID_FREQ_ERROR.format(name))\\n        _offset_map[name] = offset\\n    return _offset_map[name].copy()"
  },
  {
    "code": "def hist_series(self, by=None, ax=None, grid=True, xlabelsize=None,\\n                xrot=None, ylabelsize=None, yrot=None, figsize=None, bins=10, **kwds):\\n    import matplotlib.pyplot as plt\\n    if by is None:\\n        if kwds.get('layout', None) is not None:\\n            raise ValueError(\"The 'layout' keyword is not supported when \"\\n                             \"'by' is None\")\\n        fig = kwds.pop('figure', plt.gcf() if plt.get_fignums() else\\n                       plt.figure(figsize=figsize))\\n        if (figsize is not None and tuple(figsize) !=\\n            tuple(fig.get_size_inches())):\\n            fig.set_size_inches(*figsize, forward=True)\\n        if ax is None:\\n            ax = fig.gca()\\n        elif ax.get_figure() != fig:\\n            raise AssertionError('passed axis not bound to passed figure')\\n        values = self.dropna().values\\n        ax.hist(values, bins=bins, **kwds)\\n        ax.grid(grid)\\n        axes = np.array([ax])\\n        _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\\n                 ylabelsize=ylabelsize, yrot=yrot)\\n    else:\\n        if 'figure' in kwds:\\n            raise ValueError(\"Cannot pass 'figure' when using the \"\\n                             \"'by' argument, since a new 'Figure' instance \"\\n                             \"will be created\")\\n        axes = grouped_hist(self, by=by, ax=ax, grid=grid, figsize=figsize, bins=bins,\\n                            xlabelsize=xlabelsize, xrot=xrot, ylabelsize=ylabelsize, yrot=yrot,\\n                            **kwds)\\n    if axes.ndim == 1 and len(axes) == 1:\\n        return axes[0]\\n    return axes",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def hist_series(self, by=None, ax=None, grid=True, xlabelsize=None,\\n                xrot=None, ylabelsize=None, yrot=None, figsize=None, bins=10, **kwds):\\n    import matplotlib.pyplot as plt\\n    if by is None:\\n        if kwds.get('layout', None) is not None:\\n            raise ValueError(\"The 'layout' keyword is not supported when \"\\n                             \"'by' is None\")\\n        fig = kwds.pop('figure', plt.gcf() if plt.get_fignums() else\\n                       plt.figure(figsize=figsize))\\n        if (figsize is not None and tuple(figsize) !=\\n            tuple(fig.get_size_inches())):\\n            fig.set_size_inches(*figsize, forward=True)\\n        if ax is None:\\n            ax = fig.gca()\\n        elif ax.get_figure() != fig:\\n            raise AssertionError('passed axis not bound to passed figure')\\n        values = self.dropna().values\\n        ax.hist(values, bins=bins, **kwds)\\n        ax.grid(grid)\\n        axes = np.array([ax])\\n        _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\\n                 ylabelsize=ylabelsize, yrot=yrot)\\n    else:\\n        if 'figure' in kwds:\\n            raise ValueError(\"Cannot pass 'figure' when using the \"\\n                             \"'by' argument, since a new 'Figure' instance \"\\n                             \"will be created\")\\n        axes = grouped_hist(self, by=by, ax=ax, grid=grid, figsize=figsize, bins=bins,\\n                            xlabelsize=xlabelsize, xrot=xrot, ylabelsize=ylabelsize, yrot=yrot,\\n                            **kwds)\\n    if axes.ndim == 1 and len(axes) == 1:\\n        return axes[0]\\n    return axes"
  },
  {
    "code": "def __init__(self, *args, returncode=1, **kwargs):\\n        self.returncode = returncode\\n        super().__init__(*args, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, *args, returncode=1, **kwargs):\\n        self.returncode = returncode\\n        super().__init__(*args, **kwargs)"
  },
  {
    "code": "def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\\n\\t\\t\\t   label='', **kwargs):\\n\\t\\tself._process_unit_info(xdata=x, ydata=[ymin, ymax], kwargs=kwargs)\\n\\t\\tx = self.convert_xunits(x)\\n\\t\\tymin = self.convert_yunits(ymin)\\n\\t\\tymax = self.convert_yunits(ymax)\\n\\t\\tif not np.iterable(x):\\n\\t\\t\\tx = [x]\\n\\t\\tif not np.iterable(ymin):\\n\\t\\t\\tymin = [ymin]\\n\\t\\tif not np.iterable(ymax):\\n\\t\\t\\tymax = [ymax]\\n\\t\\tx, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\\n\\t\\tx = np.ravel(x)\\n\\t\\tymin = np.ravel(ymin)\\n\\t\\tymax = np.ravel(ymax)\\n\\t\\tmasked_verts = np.ma.empty((len(x), 2, 2))\\n\\t\\tmasked_verts[:, 0, 0] = x\\n\\t\\tmasked_verts[:, 0, 1] = ymin\\n\\t\\tmasked_verts[:, 1, 0] = x\\n\\t\\tmasked_verts[:, 1, 1] = ymax\\n\\t\\tlines = mcoll.LineCollection(masked_verts, colors=colors,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t linestyles=linestyles, label=label)\\n\\t\\tself.add_collection(lines, autolim=False)\\n\\t\\tlines.update(kwargs)\\n\\t\\tif len(x) > 0:\\n\\t\\t\\tminx = x.min()\\n\\t\\t\\tmaxx = x.max()\\n\\t\\t\\tminy = min(ymin.min(), ymax.min())\\n\\t\\t\\tmaxy = max(ymin.max(), ymax.max())\\n\\t\\t\\tcorners = (minx, miny), (maxx, maxy)\\n\\t\\t\\tself.update_datalim(corners)\\n\\t\\t\\tself._request_autoscale_view()\\n\\t\\treturn lines\\n\\t@_preprocess_data(replace_names=[\"positions\", \"lineoffsets\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"linelengths\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"colors\", \"linestyles\"])\\n\\t@docstring.dedent_interpd",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\\n\\t\\t\\t   label='', **kwargs):\\n\\t\\tself._process_unit_info(xdata=x, ydata=[ymin, ymax], kwargs=kwargs)\\n\\t\\tx = self.convert_xunits(x)\\n\\t\\tymin = self.convert_yunits(ymin)\\n\\t\\tymax = self.convert_yunits(ymax)\\n\\t\\tif not np.iterable(x):\\n\\t\\t\\tx = [x]\\n\\t\\tif not np.iterable(ymin):\\n\\t\\t\\tymin = [ymin]\\n\\t\\tif not np.iterable(ymax):\\n\\t\\t\\tymax = [ymax]\\n\\t\\tx, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\\n\\t\\tx = np.ravel(x)\\n\\t\\tymin = np.ravel(ymin)\\n\\t\\tymax = np.ravel(ymax)\\n\\t\\tmasked_verts = np.ma.empty((len(x), 2, 2))\\n\\t\\tmasked_verts[:, 0, 0] = x\\n\\t\\tmasked_verts[:, 0, 1] = ymin\\n\\t\\tmasked_verts[:, 1, 0] = x\\n\\t\\tmasked_verts[:, 1, 1] = ymax\\n\\t\\tlines = mcoll.LineCollection(masked_verts, colors=colors,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t linestyles=linestyles, label=label)\\n\\t\\tself.add_collection(lines, autolim=False)\\n\\t\\tlines.update(kwargs)\\n\\t\\tif len(x) > 0:\\n\\t\\t\\tminx = x.min()\\n\\t\\t\\tmaxx = x.max()\\n\\t\\t\\tminy = min(ymin.min(), ymax.min())\\n\\t\\t\\tmaxy = max(ymin.max(), ymax.max())\\n\\t\\t\\tcorners = (minx, miny), (maxx, maxy)\\n\\t\\t\\tself.update_datalim(corners)\\n\\t\\t\\tself._request_autoscale_view()\\n\\t\\treturn lines\\n\\t@_preprocess_data(replace_names=[\"positions\", \"lineoffsets\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"linelengths\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"colors\", \"linestyles\"])\\n\\t@docstring.dedent_interpd"
  },
  {
    "code": "def messages(self):\\n        if hasattr(self, 'error_dict'):\\n            return reduce(operator.add, dict(self).values())\\n        return list(self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Reduced reduce() usage; refs #23796.\\n\\ndjango.core.exceptions.ValidationError.messages() and\\ndjango.db.backends.schema.BaseDatabaseSchemaEditor._alter_field():\\nReplaced reduce(operator.add, ...) w/uncoupled, explicit sum()",
    "fixed_code": "def messages(self):\\n        if hasattr(self, 'error_dict'):\\n            return sum(dict(self).values(), [])\\n        return list(self)"
  },
  {
    "code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.name = filename\\n            self.min_readsize = 100\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        if self.mode == WRITE:\\n            self._write_gzip_header()\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #4272: Add an optional argument to the GzipFile constructor to override the timestamp in the gzip stream.",
    "fixed_code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None, mtime=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.name = filename\\n            self.min_readsize = 100\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        self.mtime = mtime\\n        if self.mode == WRITE:\\n            self._write_gzip_header()\\n    @property"
  },
  {
    "code": "def diff(a, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):\\n    if n == 0:\\n        return a\\n    if n < 0:\\n        raise ValueError(\\n            \"order must be non-negative but got \" + repr(n))\\n    a = asanyarray(a)\\n    nd = a.ndim\\n    axis = normalize_axis_index(axis, nd)\\n    combined = []\\n    if prepend is not np._NoValue:\\n        prepend = np.asanyarray(prepend)\\n        if prepend.ndim == 0:\\n            shape = list(a.shape)\\n            shape[axis] = 1\\n            prepend = np.broadcast_to(prepend, tuple(shape))\\n        combined.append(prepend)\\n    combined.append(a)\\n    if append is not np._NoValue:\\n        append = np.asanyarray(append)\\n        if append.ndim == 0:\\n            shape = list(a.shape)\\n            shape[axis] = 1\\n            append = np.broadcast_to(append, tuple(shape))\\n        combined.append(append)\\n    if len(combined) > 1:\\n        a = np.concatenate(combined, axis)\\n    slice1 = [slice(None)] * nd\\n    slice2 = [slice(None)] * nd\\n    slice1[axis] = slice(1, None)\\n    slice2[axis] = slice(None, -1)\\n    slice1 = tuple(slice1)\\n    slice2 = tuple(slice2)\\n    op = not_equal if a.dtype == np.bool_ else subtract\\n    for _ in range(n):\\n        a = op(a[slice1], a[slice2])\\n    return a",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add clearer error message for diff(0-d)",
    "fixed_code": "def diff(a, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):\\n    if n == 0:\\n        return a\\n    if n < 0:\\n        raise ValueError(\\n            \"order must be non-negative but got \" + repr(n))\\n    a = asanyarray(a)\\n    nd = a.ndim\\n    if nd == 0:\\n        raise ValueError(\"diff requires input that is at least one dimensional\")\\n    axis = normalize_axis_index(axis, nd)\\n    combined = []\\n    if prepend is not np._NoValue:\\n        prepend = np.asanyarray(prepend)\\n        if prepend.ndim == 0:\\n            shape = list(a.shape)\\n            shape[axis] = 1\\n            prepend = np.broadcast_to(prepend, tuple(shape))\\n        combined.append(prepend)\\n    combined.append(a)\\n    if append is not np._NoValue:\\n        append = np.asanyarray(append)\\n        if append.ndim == 0:\\n            shape = list(a.shape)\\n            shape[axis] = 1\\n            append = np.broadcast_to(append, tuple(shape))\\n        combined.append(append)\\n    if len(combined) > 1:\\n        a = np.concatenate(combined, axis)\\n    slice1 = [slice(None)] * nd\\n    slice2 = [slice(None)] * nd\\n    slice1[axis] = slice(1, None)\\n    slice2[axis] = slice(None, -1)\\n    slice1 = tuple(slice1)\\n    slice2 = tuple(slice2)\\n    op = not_equal if a.dtype == np.bool_ else subtract\\n    for _ in range(n):\\n        a = op(a[slice1], a[slice2])\\n    return a"
  },
  {
    "code": "def _trigger_dag(\\n    dag_id: str,\\n    dag_bag: DagBag,\\n    run_id: Optional[str] = None,\\n    conf: Optional[Union[dict, str]] = None,\\n    execution_date: Optional[datetime] = None,\\n    replace_microseconds: bool = True,\\n) -> List[Optional[DagRun]]:\\n    dag = dag_bag.get_dag(dag_id)  \\n    if dag is None or dag_id not in dag_bag.dags:\\n        raise DagNotFound(f\"Dag id {dag_id} not found\")\\n    execution_date = execution_date if execution_date else timezone.utcnow()\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"The execution_date should be localized\")\\n    if replace_microseconds:\\n        execution_date = execution_date.replace(microsecond=0)\\n    if dag.default_args and 'start_date' in dag.default_args:\\n        min_dag_start_date = dag.default_args[\"start_date\"]\\n        if min_dag_start_date and execution_date < min_dag_start_date:\\n            raise ValueError(\\n                f\"The execution_date [{execution_date.isoformat()}] should be >= start_date \"\\n                f\"[{min_dag_start_date.isoformat()}] from DAG's default_args\"\\n            )\\n    run_id = run_id or DagRun.generate_run_id(DagRunType.MANUAL, execution_date)\\n    dag_run = DagRun.find_duplicate(dag_id=dag_id, execution_date=execution_date, run_id=run_id)\\n    if dag_run:\\n        raise DagRunAlreadyExists(\\n            f\"A Dag Run already exists for dag id {dag_id} at {execution_date} with run id {run_id}\"\\n        )\\n    run_conf = None\\n    if conf:\\n        run_conf = conf if isinstance(conf, dict) else json.loads(conf)\\n    dag_runs = []\\n    dags_to_run = [dag] + dag.subdags\\n    for _dag in dags_to_run:\\n        dag_run = _dag.create_dagrun(\\n            run_id=run_id,\\n            execution_date=execution_date,\\n            state=DagRunState.QUEUED,\\n            conf=run_conf,\\n            external_trigger=True,\\n            dag_hash=dag_bag.dags_hash.get(dag_id),\\n            data_interval=_dag.timetable.infer_manual_data_interval(\\n                run_after=pendulum.instance(execution_date)\\n            ),\\n        )\\n        dag_runs.append(dag_run)\\n    return dag_runs",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Let timetables control generated run_ids. (#25795)",
    "fixed_code": "def _trigger_dag(\\n    dag_id: str,\\n    dag_bag: DagBag,\\n    run_id: Optional[str] = None,\\n    conf: Optional[Union[dict, str]] = None,\\n    execution_date: Optional[datetime] = None,\\n    replace_microseconds: bool = True,\\n) -> List[Optional[DagRun]]:\\n    dag = dag_bag.get_dag(dag_id)  \\n    if dag is None or dag_id not in dag_bag.dags:\\n        raise DagNotFound(f\"Dag id {dag_id} not found\")\\n    execution_date = execution_date if execution_date else timezone.utcnow()\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"The execution_date should be localized\")\\n    if replace_microseconds:\\n        execution_date = execution_date.replace(microsecond=0)\\n    if dag.default_args and 'start_date' in dag.default_args:\\n        min_dag_start_date = dag.default_args[\"start_date\"]\\n        if min_dag_start_date and execution_date < min_dag_start_date:\\n            raise ValueError(\\n                f\"The execution_date [{execution_date.isoformat()}] should be >= start_date \"\\n                f\"[{min_dag_start_date.isoformat()}] from DAG's default_args\"\\n            )\\n    logical_date = timezone.coerce_datetime(execution_date)\\n    data_interval = dag.timetable.infer_manual_data_interval(run_after=logical_date)\\n    run_id = run_id or dag.timetable.generate_run_id(\\n        run_type=DagRunType.MANUAL, logical_date=logical_date, data_interval=data_interval\\n    )\\n    dag_run = DagRun.find_duplicate(dag_id=dag_id, execution_date=execution_date, run_id=run_id)\\n    if dag_run:\\n        raise DagRunAlreadyExists(\\n            f\"A Dag Run already exists for dag id {dag_id} at {execution_date} with run id {run_id}\"\\n        )\\n    run_conf = None\\n    if conf:\\n        run_conf = conf if isinstance(conf, dict) else json.loads(conf)\\n    dag_runs = []\\n    dags_to_run = [dag] + dag.subdags\\n    for _dag in dags_to_run:\\n        dag_run = _dag.create_dagrun(\\n            run_id=run_id,\\n            execution_date=execution_date,\\n            state=DagRunState.QUEUED,\\n            conf=run_conf,\\n            external_trigger=True,\\n            dag_hash=dag_bag.dags_hash.get(dag_id),\\n            data_interval=data_interval,\\n        )\\n        dag_runs.append(dag_run)\\n    return dag_runs"
  },
  {
    "code": "def _infer_types(self, values, na_values, try_num_bool: bool = True):\\n        na_count = 0\\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\\n            na_values = np.array([val for val in na_values if not isinstance(val, str)])\\n            mask = algorithms.isin(values, na_values)\\n            na_count = mask.astype(\"uint8\", copy=False).sum()\\n            if na_count > 0:\\n                if is_integer_dtype(values):\\n                    values = values.astype(np.float64)\\n                np.putmask(values, mask, np.nan)\\n            return values, na_count\\n        if try_num_bool and is_object_dtype(values.dtype):\\n            try:\\n                result, _ = lib.maybe_convert_numeric(values, na_values, False)\\n            except (ValueError, TypeError):\\n                result = values\\n                na_count = parsers.sanitize_objects(result, na_values)\\n            else:\\n                na_count = isna(result).sum()\\n        else:\\n            result = values\\n            if values.dtype == np.object_:\\n                na_count = parsers.sanitize_objects(values, na_values)\\n        if result.dtype == np.object_ and try_num_bool:\\n            result, _ = libops.maybe_convert_bool(\\n                np.asarray(values),\\n                true_values=self.true_values,\\n                false_values=self.false_values,\\n            )\\n        return result, na_count",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Add option to use nullable dtypes in read_csv (#48776)",
    "fixed_code": "def _infer_types(\\n        self, values, na_values, no_dtype_specified, try_num_bool: bool = True\\n    ):\\n        na_count = 0\\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\\n            na_values = np.array([val for val in na_values if not isinstance(val, str)])\\n            mask = algorithms.isin(values, na_values)\\n            na_count = mask.astype(\"uint8\", copy=False).sum()\\n            if na_count > 0:\\n                if is_integer_dtype(values):\\n                    values = values.astype(np.float64)\\n                np.putmask(values, mask, np.nan)\\n            return values, na_count\\n        use_nullable_dtypes: Literal[True] | Literal[False] = (\\n            self.use_nullable_dtypes and no_dtype_specified\\n        )\\n        result: ArrayLike\\n        if try_num_bool and is_object_dtype(values.dtype):\\n            try:\\n                result, result_mask = lib.maybe_convert_numeric(\\n                    values,\\n                    na_values,\\n                    False,\\n                    convert_to_masked_nullable=use_nullable_dtypes,\\n                )\\n            except (ValueError, TypeError):\\n                na_count = parsers.sanitize_objects(values, na_values)\\n                result = values\\n            else:\\n                if use_nullable_dtypes:\\n                    if result_mask is None:\\n                        result_mask = np.zeros(result.shape, dtype=np.bool_)\\n                    if result_mask.all():\\n                        result = IntegerArray(\\n                            np.ones(result_mask.shape, dtype=np.int64), result_mask\\n                        )\\n                    elif is_integer_dtype(result):\\n                        result = IntegerArray(result, result_mask)\\n                    elif is_bool_dtype(result):\\n                        result = BooleanArray(result, result_mask)\\n                    elif is_float_dtype(result):\\n                        result = FloatingArray(result, result_mask)\\n                    na_count = result_mask.sum()\\n                else:\\n                    na_count = isna(result).sum()\\n        else:\\n            result = values\\n            if values.dtype == np.object_:\\n                na_count = parsers.sanitize_objects(values, na_values)\\n        if result.dtype == np.object_ and try_num_bool:\\n            result, bool_mask = libops.maybe_convert_bool(\\n                np.asarray(values),\\n                true_values=self.true_values,\\n                false_values=self.false_values,\\n                convert_to_masked_nullable=use_nullable_dtypes,\\n            )\\n            if result.dtype == np.bool_ and use_nullable_dtypes:\\n                if bool_mask is None:\\n                    bool_mask = np.zeros(result.shape, dtype=np.bool_)\\n                result = BooleanArray(result, bool_mask)\\n            elif result.dtype == np.object_ and use_nullable_dtypes:\\n                result = StringDtype().construct_array_type()._from_sequence(values)\\n        return result, na_count"
  },
  {
    "code": "def get_variable(self, key: str) -> Optional[str]:\\n        return self._get_secret(self.variables_prefix, key)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "azure key vault optional lookup (#12174)",
    "fixed_code": "def get_variable(self, key: str) -> Optional[str]:\\n        if self.variables_prefix is None:\\n            return None\\n        return self._get_secret(self.variables_prefix, key)"
  },
  {
    "code": "def __init__(self, callback, args, loop):\\n        assert not isinstance(callback, Handle), 'A Handle is not a callback'\\n        self._loop = loop\\n        self._callback = callback\\n        self._args = args\\n        self._cancelled = False\\n        if self._loop.get_debug():\\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\\n        else:\\n            self._source_traceback = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio, Tulip issue 206: In debug mode, keep the callback in the representation of Handle and TimerHandle after cancel().",
    "fixed_code": "def __init__(self, callback, args, loop):\\n        assert not isinstance(callback, Handle), 'A Handle is not a callback'\\n        self._loop = loop\\n        self._callback = callback\\n        self._args = args\\n        self._cancelled = False\\n        self._repr = None\\n        if self._loop.get_debug():\\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\\n        else:\\n            self._source_traceback = None"
  },
  {
    "code": "def inverse_transform(self, X, Y=None):\\n        check_is_fitted(self)\\n        X = check_array(X, dtype=FLOAT_DTYPES)\\n        X_reconstructed = np.matmul(X, self.x_loadings_.T)\\n        X_reconstructed *= self._x_std\\n        X_reconstructed += self._x_mean\\n        if Y is not None:\\n            Y = check_array(Y, dtype=FLOAT_DTYPES)\\n            Y_reconstructed = np.matmul(Y, self.y_loadings_.T)\\n            Y_reconstructed *= self._y_std\\n            Y_reconstructed += self._y_mean\\n            return X_reconstructed, Y_reconstructed\\n        return X_reconstructed",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def inverse_transform(self, X, Y=None):\\n        check_is_fitted(self)\\n        X = check_array(X, dtype=FLOAT_DTYPES)\\n        X_reconstructed = np.matmul(X, self.x_loadings_.T)\\n        X_reconstructed *= self._x_std\\n        X_reconstructed += self._x_mean\\n        if Y is not None:\\n            Y = check_array(Y, dtype=FLOAT_DTYPES)\\n            Y_reconstructed = np.matmul(Y, self.y_loadings_.T)\\n            Y_reconstructed *= self._y_std\\n            Y_reconstructed += self._y_mean\\n            return X_reconstructed, Y_reconstructed\\n        return X_reconstructed"
  },
  {
    "code": "def copy(self, deep=False):\\n        values = self.asi8.copy()\\n        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy(self, deep=False):\\n        values = self.asi8.copy()\\n        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)"
  },
  {
    "code": "def _mkstemp_inner(dir, pre, suf, flags):\\n    names = _get_candidate_names()\\n    for seq in range(TMP_MAX):\\n        name = next(names)\\n        file = _os.path.join(dir, pre + name + suf)\\n        try:\\n            fd = _os.open(file, flags, 0o600)\\n            _set_cloexec(fd)\\n            return (fd, _os.path.abspath(file))\\n        except FileExistsError:\\n            continue    \\n    raise FileExistsError(_errno.EEXIST,\\n                          \"No usable temporary file name found\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #18849: Fixed a Windows-specific tempfile bug where collision with an existing directory caused mkstemp and related APIs to fail instead of retrying. Report and fix by Vlad Shcherbina.",
    "fixed_code": "def _mkstemp_inner(dir, pre, suf, flags):\\n    names = _get_candidate_names()\\n    for seq in range(TMP_MAX):\\n        name = next(names)\\n        file = _os.path.join(dir, pre + name + suf)\\n        try:\\n            fd = _os.open(file, flags, 0o600)\\n            _set_cloexec(fd)\\n            return (fd, _os.path.abspath(file))\\n        except FileExistsError:\\n            continue    \\n        except PermissionError:\\n            if _os.name == 'nt':\\n                continue\\n            else:\\n                raise\\n    raise FileExistsError(_errno.EEXIST,\\n                          \"No usable temporary file name found\")"
  },
  {
    "code": "def __init__(self, stream, errors='strict'):\\n        self.stream = stream\\n        self.errors = errors",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Patch #1436130: codecs.lookup() now returns a CodecInfo object (a subclass of tuple) that provides incremental decoders and encoders (a way to use stateful codecs without the stream API). Functions codecs.getincrementaldecoder() and codecs.getincrementalencoder() have been added.",
    "fixed_code": "def __init__(self, errors='strict'):\\n        self.errors = errors\\n        self.buffer = \"\""
  },
  {
    "code": "def fit(self, X, y, check_input=True):\\n\\t\\tif self.alpha == 0:\\n\\t\\t\\twarnings.warn(\"With alpha=0, this algorithm does not converge \"\\n\\t\\t\\t\\t\\t\\t  \"well. You are advised to use the LinearRegression \"\\n\\t\\t\\t\\t\\t\\t  \"estimator\", stacklevel=2)\\n\\t\\tif isinstance(self.precompute, six.string_types):\\n\\t\\t\\traise ValueError('precompute should be one of True, False or'\\n\\t\\t\\t\\t\\t\\t\\t ' array-like. Got %r' % self.precompute)\\n\\t\\tX_copied = False\\n\\t\\tif check_input:\\n\\t\\t\\tX_copied = self.copy_X and self.fit_intercept\\n\\t\\t\\tX, y = check_X_y(X, y, accept_sparse='csc',\\n\\t\\t\\t\\t\\t\\t\\t order='F', dtype=[np.float64, np.float32],\\n\\t\\t\\t\\t\\t\\t\\t copy=X_copied, multi_output=True, y_numeric=True)\\n\\t\\t\\ty = check_array(y, order='F', copy=False, dtype=X.dtype.type,\\n\\t\\t\\t\\t\\t\\t\\tensure_2d=False)\\n\\t\\tshould_copy = self.copy_X and not X_copied\\n\\t\\tX, y, X_offset, y_offset, X_scale, precompute, Xy = \\\\n\\t\\t\\t_pre_fit(X, y, None, self.precompute, self.normalize,\\n\\t\\t\\t\\t\\t self.fit_intercept, copy=should_copy)\\n\\t\\tif y.ndim == 1:\\n\\t\\t\\ty = y[:, np.newaxis]\\n\\t\\tif Xy is not None and Xy.ndim == 1:\\n\\t\\t\\tXy = Xy[:, np.newaxis]\\n\\t\\tn_samples, n_features = X.shape\\n\\t\\tn_targets = y.shape[1]\\n\\t\\tif self.selection not in ['cyclic', 'random']:\\n\\t\\t\\traise ValueError(\"selection should be either random or cyclic.\")\\n\\t\\tif not self.warm_start or not hasattr(self, \"coef_\"):\\n\\t\\t\\tcoef_ = np.zeros((n_targets, n_features), dtype=X.dtype,\\n\\t\\t\\t\\t\\t\\t\\t order='F')\\n\\t\\telse:\\n\\t\\t\\tcoef_ = self.coef_\\n\\t\\t\\tif coef_.ndim == 1:\\n\\t\\t\\t\\tcoef_ = coef_[np.newaxis, :]\\n\\t\\tdual_gaps_ = np.zeros(n_targets, dtype=X.dtype)\\n\\t\\tself.n_iter_ = []\\n\\t\\tfor k in xrange(n_targets):\\n\\t\\t\\tif Xy is not None:\\n\\t\\t\\t\\tthis_Xy = Xy[:, k]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tthis_Xy = None\\n\\t\\t\\t_, this_coef, this_dual_gap, this_iter = \\\\n\\t\\t\\t\\tself.path(X, y[:, k],\\n\\t\\t\\t\\t\\t\\t  l1_ratio=self.l1_ratio, eps=None,\\n\\t\\t\\t\\t\\t\\t  n_alphas=None, alphas=[self.alpha],\\n\\t\\t\\t\\t\\t\\t  precompute=precompute, Xy=this_Xy,\\n\\t\\t\\t\\t\\t\\t  fit_intercept=False, normalize=False, copy_X=True,\\n\\t\\t\\t\\t\\t\\t  verbose=False, tol=self.tol, positive=self.positive,\\n\\t\\t\\t\\t\\t\\t  X_offset=X_offset, X_scale=X_scale, return_n_iter=True,\\n\\t\\t\\t\\t\\t\\t  coef_init=coef_[k], max_iter=self.max_iter,\\n\\t\\t\\t\\t\\t\\t  random_state=self.random_state,\\n\\t\\t\\t\\t\\t\\t  selection=self.selection,\\n\\t\\t\\t\\t\\t\\t  check_input=False)\\n\\t\\t\\tcoef_[k] = this_coef[:, 0]\\n\\t\\t\\tdual_gaps_[k] = this_dual_gap[0]\\n\\t\\t\\tself.n_iter_.append(this_iter[0])\\n\\t\\tif n_targets == 1:\\n\\t\\t\\tself.n_iter_ = self.n_iter_[0]\\n\\t\\t\\tself.coef_ = coef_[0]\\n\\t\\t\\tself.dual_gap_ = dual_gaps_[0]\\n\\t\\telse:\\n\\t\\t\\tself.coef_ = coef_\\n\\t\\t\\tself.dual_gap_ = dual_gaps_\\n\\t\\tself._set_intercept(X_offset, y_offset, X_scale)\\n\\t\\tself.coef_ = np.asarray(self.coef_, dtype=X.dtype)\\n\\t\\treturn self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, y, check_input=True):\\n\\t\\tif self.alpha == 0:\\n\\t\\t\\twarnings.warn(\"With alpha=0, this algorithm does not converge \"\\n\\t\\t\\t\\t\\t\\t  \"well. You are advised to use the LinearRegression \"\\n\\t\\t\\t\\t\\t\\t  \"estimator\", stacklevel=2)\\n\\t\\tif isinstance(self.precompute, six.string_types):\\n\\t\\t\\traise ValueError('precompute should be one of True, False or'\\n\\t\\t\\t\\t\\t\\t\\t ' array-like. Got %r' % self.precompute)\\n\\t\\tX_copied = False\\n\\t\\tif check_input:\\n\\t\\t\\tX_copied = self.copy_X and self.fit_intercept\\n\\t\\t\\tX, y = check_X_y(X, y, accept_sparse='csc',\\n\\t\\t\\t\\t\\t\\t\\t order='F', dtype=[np.float64, np.float32],\\n\\t\\t\\t\\t\\t\\t\\t copy=X_copied, multi_output=True, y_numeric=True)\\n\\t\\t\\ty = check_array(y, order='F', copy=False, dtype=X.dtype.type,\\n\\t\\t\\t\\t\\t\\t\\tensure_2d=False)\\n\\t\\tshould_copy = self.copy_X and not X_copied\\n\\t\\tX, y, X_offset, y_offset, X_scale, precompute, Xy = \\\\n\\t\\t\\t_pre_fit(X, y, None, self.precompute, self.normalize,\\n\\t\\t\\t\\t\\t self.fit_intercept, copy=should_copy)\\n\\t\\tif y.ndim == 1:\\n\\t\\t\\ty = y[:, np.newaxis]\\n\\t\\tif Xy is not None and Xy.ndim == 1:\\n\\t\\t\\tXy = Xy[:, np.newaxis]\\n\\t\\tn_samples, n_features = X.shape\\n\\t\\tn_targets = y.shape[1]\\n\\t\\tif self.selection not in ['cyclic', 'random']:\\n\\t\\t\\traise ValueError(\"selection should be either random or cyclic.\")\\n\\t\\tif not self.warm_start or not hasattr(self, \"coef_\"):\\n\\t\\t\\tcoef_ = np.zeros((n_targets, n_features), dtype=X.dtype,\\n\\t\\t\\t\\t\\t\\t\\t order='F')\\n\\t\\telse:\\n\\t\\t\\tcoef_ = self.coef_\\n\\t\\t\\tif coef_.ndim == 1:\\n\\t\\t\\t\\tcoef_ = coef_[np.newaxis, :]\\n\\t\\tdual_gaps_ = np.zeros(n_targets, dtype=X.dtype)\\n\\t\\tself.n_iter_ = []\\n\\t\\tfor k in xrange(n_targets):\\n\\t\\t\\tif Xy is not None:\\n\\t\\t\\t\\tthis_Xy = Xy[:, k]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tthis_Xy = None\\n\\t\\t\\t_, this_coef, this_dual_gap, this_iter = \\\\n\\t\\t\\t\\tself.path(X, y[:, k],\\n\\t\\t\\t\\t\\t\\t  l1_ratio=self.l1_ratio, eps=None,\\n\\t\\t\\t\\t\\t\\t  n_alphas=None, alphas=[self.alpha],\\n\\t\\t\\t\\t\\t\\t  precompute=precompute, Xy=this_Xy,\\n\\t\\t\\t\\t\\t\\t  fit_intercept=False, normalize=False, copy_X=True,\\n\\t\\t\\t\\t\\t\\t  verbose=False, tol=self.tol, positive=self.positive,\\n\\t\\t\\t\\t\\t\\t  X_offset=X_offset, X_scale=X_scale, return_n_iter=True,\\n\\t\\t\\t\\t\\t\\t  coef_init=coef_[k], max_iter=self.max_iter,\\n\\t\\t\\t\\t\\t\\t  random_state=self.random_state,\\n\\t\\t\\t\\t\\t\\t  selection=self.selection,\\n\\t\\t\\t\\t\\t\\t  check_input=False)\\n\\t\\t\\tcoef_[k] = this_coef[:, 0]\\n\\t\\t\\tdual_gaps_[k] = this_dual_gap[0]\\n\\t\\t\\tself.n_iter_.append(this_iter[0])\\n\\t\\tif n_targets == 1:\\n\\t\\t\\tself.n_iter_ = self.n_iter_[0]\\n\\t\\t\\tself.coef_ = coef_[0]\\n\\t\\t\\tself.dual_gap_ = dual_gaps_[0]\\n\\t\\telse:\\n\\t\\t\\tself.coef_ = coef_\\n\\t\\t\\tself.dual_gap_ = dual_gaps_\\n\\t\\tself._set_intercept(X_offset, y_offset, X_scale)\\n\\t\\tself.coef_ = np.asarray(self.coef_, dtype=X.dtype)\\n\\t\\treturn self"
  },
  {
    "code": "def fit(self, X, y=None):\\n        X = self._validate_data(X, dtype='numeric')\\n        valid_encode = ('onehot', 'onehot-dense', 'ordinal')\\n        if self.encode not in valid_encode:\\n            raise ValueError(\"Valid options for 'encode' are {}. \"\\n                             \"Got encode={!r} instead.\"\\n                             .format(valid_encode, self.encode))\\n        valid_strategy = ('uniform', 'quantile', 'kmeans')\\n        if self.strategy not in valid_strategy:\\n            raise ValueError(\"Valid options for 'strategy' are {}. \"\\n                             \"Got strategy={!r} instead.\"\\n                             .format(valid_strategy, self.strategy))\\n        n_features = X.shape[1]\\n        n_bins = self._validate_n_bins(n_features)\\n        bin_edges = np.zeros(n_features, dtype=object)\\n        for jj in range(n_features):\\n            column = X[:, jj]\\n            col_min, col_max = column.min(), column.max()\\n            if col_min == col_max:\\n                warnings.warn(\"Feature %d is constant and will be \"\\n                              \"replaced with 0.\" % jj)\\n                n_bins[jj] = 1\\n                bin_edges[jj] = np.array([-np.inf, np.inf])\\n                continue\\n            if self.strategy == 'uniform':\\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n            elif self.strategy == 'quantile':\\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\\n                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\\n            elif self.strategy == 'kmeans':\\n                from ..cluster import KMeans  \\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\\n                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\\n                centers.sort()\\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\\n            if self.strategy in ('quantile', 'kmeans'):\\n                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8\\n                bin_edges[jj] = bin_edges[jj][mask]\\n                if len(bin_edges[jj]) - 1 != n_bins[jj]:\\n                    warnings.warn('Bins whose width are too small (i.e., <= '\\n                                  '1e-8) in feature %d are removed. Consider '\\n                                  'decreasing the number of bins.' % jj)\\n                    n_bins[jj] = len(bin_edges[jj]) - 1\\n        self.bin_edges_ = bin_edges\\n        self.n_bins_ = n_bins\\n        if 'onehot' in self.encode:\\n            self._encoder = OneHotEncoder(\\n                categories=[np.arange(i) for i in self.n_bins_],\\n                sparse=self.encode == 'onehot')\\n            self._encoder.fit(np.zeros((1, len(self.n_bins_)), dtype=int))\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add dtype parameter to KBinsDiscretizer to manage the output data type (#16335)",
    "fixed_code": "def fit(self, X, y=None):\\n        X = self._validate_data(X, dtype='numeric')\\n        supported_dtype = (np.float64, np.float32)\\n        if self.dtype in supported_dtype:\\n            output_dtype = self.dtype\\n        elif self.dtype is None:\\n            output_dtype = X.dtype\\n        else:\\n            raise ValueError(\\n                f\"Valid options for 'dtype' are \"\\n                f\"{supported_dtype + (None,)}. Got dtype={self.dtype} \"\\n                f\" instead.\"\\n            )\\n        valid_encode = ('onehot', 'onehot-dense', 'ordinal')\\n        if self.encode not in valid_encode:\\n            raise ValueError(\"Valid options for 'encode' are {}. \"\\n                             \"Got encode={!r} instead.\"\\n                             .format(valid_encode, self.encode))\\n        valid_strategy = ('uniform', 'quantile', 'kmeans')\\n        if self.strategy not in valid_strategy:\\n            raise ValueError(\"Valid options for 'strategy' are {}. \"\\n                             \"Got strategy={!r} instead.\"\\n                             .format(valid_strategy, self.strategy))\\n        n_features = X.shape[1]\\n        n_bins = self._validate_n_bins(n_features)\\n        bin_edges = np.zeros(n_features, dtype=object)\\n        for jj in range(n_features):\\n            column = X[:, jj]\\n            col_min, col_max = column.min(), column.max()\\n            if col_min == col_max:\\n                warnings.warn(\"Feature %d is constant and will be \"\\n                              \"replaced with 0.\" % jj)\\n                n_bins[jj] = 1\\n                bin_edges[jj] = np.array([-np.inf, np.inf])\\n                continue\\n            if self.strategy == 'uniform':\\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n            elif self.strategy == 'quantile':\\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\\n                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\\n            elif self.strategy == 'kmeans':\\n                from ..cluster import KMeans  \\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\\n                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\\n                centers.sort()\\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\\n            if self.strategy in ('quantile', 'kmeans'):\\n                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8\\n                bin_edges[jj] = bin_edges[jj][mask]\\n                if len(bin_edges[jj]) - 1 != n_bins[jj]:\\n                    warnings.warn('Bins whose width are too small (i.e., <= '\\n                                  '1e-8) in feature %d are removed. Consider '\\n                                  'decreasing the number of bins.' % jj)\\n                    n_bins[jj] = len(bin_edges[jj]) - 1\\n        self.bin_edges_ = bin_edges\\n        self.n_bins_ = n_bins\\n        if 'onehot' in self.encode:\\n            self._encoder = OneHotEncoder(\\n                categories=[np.arange(i) for i in self.n_bins_],\\n                sparse=self.encode == 'onehot',\\n                dtype=output_dtype)\\n            self._encoder.fit(np.zeros((1, len(self.n_bins_))))\\n        return self"
  },
  {
    "code": "def packkey(ae, key, value):\\n    if hasattr(key, 'which'):\\n        keystr = key.which\\n    elif hasattr(key, 'want'):\\n        keystr = key.want\\n    else:\\n        keystr = key\\n    ae.AEPutParamDesc(keystr, pack(value))\\ndef pack(x, forcetype = None):\\n    if forcetype:\\n        if isinstance(x, str):\\n            return AE.AECreateDesc(forcetype, x)\\n        else:\\n            return pack(x).AECoerceDesc(forcetype)\\n    if x == None:\\n        return AE.AECreateDesc('null', '')\\n    if isinstance(x, AEDescType):\\n        return x\\n    if isinstance(x, FSSType):\\n        return AE.AECreateDesc('fss ', x.data)\\n    if isinstance(x, FSRefType):\\n        return AE.AECreateDesc('fsrf', x.data)\\n    if isinstance(x, AliasType):\\n        return AE.AECreateDesc('alis', x.data)\\n    if isinstance(x, int):\\n        return AE.AECreateDesc('long', struct.pack('l', x))\\n    if isinstance(x, float):\\n        return AE.AECreateDesc('doub', struct.pack('d', x))\\n    if isinstance(x, str):\\n        return AE.AECreateDesc('TEXT', x)\\n    if isinstance(x, unicode):\\n        data = x.encode('utf16')\\n        if data[:2] == '\\xfe\\xff':\\n            data = data[2:]\\n        return AE.AECreateDesc('utxt', data)\\n    if isinstance(x, list):\\n        lst = AE.AECreateList('', 0)\\n        for item in x:\\n            lst.AEPutDesc(0, pack(item))\\n        return lst\\n    if isinstance(x, dict):\\n        record = AE.AECreateList('', 1)\\n        for key, value in x.items():\\n            packkey(record, key, value)\\n        return record\\n    if isinstance(x, type) and issubclass(x, ObjectSpecifier):\\n        return AE.AECreateDesc('type', x.want)\\n    if hasattr(x, '__aepack__'):\\n        return x.__aepack__()\\n    if hasattr(x, 'which'):\\n        return AE.AECreateDesc('TEXT', x.which)\\n    if hasattr(x, 'want'):\\n        return AE.AECreateDesc('TEXT', x.want)\\n    return AE.AECreateDesc('TEXT', repr(x))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "SF patch# 1761465 by Jeffrey Yasskin. Fix test_aepack and test_applesingle.",
    "fixed_code": "def packkey(ae, key, value):\\n    if hasattr(key, 'which'):\\n        keystr = key.which\\n    elif hasattr(key, 'want'):\\n        keystr = key.want\\n    else:\\n        keystr = key\\n    ae.AEPutParamDesc(keystr, pack(value))\\ndef pack(x, forcetype = None):\\n    if forcetype:\\n        if isinstance(x, bytes):\\n            return AE.AECreateDesc(forcetype, x)\\n        else:\\n            return pack(x).AECoerceDesc(forcetype)\\n    if x == None:\\n        return AE.AECreateDesc(b'null', '')\\n    if isinstance(x, AEDescType):\\n        return x\\n    if isinstance(x, FSSType):\\n        return AE.AECreateDesc(b'fss ', x.data)\\n    if isinstance(x, FSRefType):\\n        return AE.AECreateDesc(b'fsrf', x.data)\\n    if isinstance(x, AliasType):\\n        return AE.AECreateDesc(b'alis', x.data)\\n    if isinstance(x, int):\\n        return AE.AECreateDesc(b'long', struct.pack('l', x))\\n    if isinstance(x, float):\\n        return AE.AECreateDesc(b'doub', struct.pack('d', x))\\n    if isinstance(x, (bytes, str8)):\\n        return AE.AECreateDesc(b'TEXT', x)\\n    if isinstance(x, str):\\n        data = x.encode('utf16')\\n        if data[:2] == '\\xfe\\xff':\\n            data = data[2:]\\n        return AE.AECreateDesc(b'utxt', data)\\n    if isinstance(x, list):\\n        lst = AE.AECreateList('', 0)\\n        for item in x:\\n            lst.AEPutDesc(0, pack(item))\\n        return lst\\n    if isinstance(x, dict):\\n        record = AE.AECreateList('', 1)\\n        for key, value in x.items():\\n            packkey(record, key, value)\\n        return record\\n    if isinstance(x, type) and issubclass(x, ObjectSpecifier):\\n        return AE.AECreateDesc(b'type', x.want)\\n    if hasattr(x, '__aepack__'):\\n        return x.__aepack__()\\n    if hasattr(x, 'which'):\\n        return AE.AECreateDesc(b'TEXT', x.which)\\n    if hasattr(x, 'want'):\\n        return AE.AECreateDesc(b'TEXT', x.want)\\n    return AE.AECreateDesc(b'TEXT', repr(x))"
  },
  {
    "code": "def value_counts(values, sort=True, ascending=False):\\n    from pandas.core.series import Series\\n    values = np.asarray(values)\\n    if com.is_integer_dtype(values.dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    elif issubclass(values.dtype.type, (np.datetime64,np.timedelta64)):\\n        dtype = values.dtype\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        keys = Series(keys,dtype=dtype)\\n    else:\\n        mask = com.isnull(values)\\n        values = com._ensure_object(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n    result = Series(counts, index=keys)\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def value_counts(values, sort=True, ascending=False):\\n    from pandas.core.series import Series\\n    values = np.asarray(values)\\n    if com.is_integer_dtype(values.dtype):\\n        values = com._ensure_int64(values)\\n        keys, counts = htable.value_count_int64(values)\\n    elif issubclass(values.dtype.type, (np.datetime64,np.timedelta64)):\\n        dtype = values.dtype\\n        values = values.view(np.int64)\\n        keys, counts = htable.value_count_int64(values)\\n        keys = Series(keys,dtype=dtype)\\n    else:\\n        mask = com.isnull(values)\\n        values = com._ensure_object(values)\\n        keys, counts = htable.value_count_object(values, mask)\\n    result = Series(counts, index=keys)\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    return result"
  },
  {
    "code": "def __getitem__(self, i):\\n        if isinstance(i, SliceType):\\n            return self.data[i.start : i.stop]\\n        else:\\n            return self.data[i]\\nclass _localized_month(_indexer):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, i):\\n        if isinstance(i, SliceType):\\n            return self.data[i.start : i.stop]\\n        else:\\n            return self.data[i]\\nclass _localized_month(_indexer):"
  },
  {
    "code": "def generate_comments(leaf: Leaf) -> Iterator[Leaf]:\\n    p = leaf.prefix\\n    if not p:\\n        return\\n    if '\\n        return\\n    consumed = 0\\n    nlines = 0\\n    for index, line in enumerate(p.split('\\n')):\\n        consumed += len(line) + 1  \\n'\\n        line = line.lstrip()\\n        if not line:\\n            nlines += 1\\n        if not line.startswith('\\n            continue\\n        if index == 0 and leaf.type != token.ENDMARKER:\\n            comment_type = token.COMMENT  \\n        else:\\n            comment_type = STANDALONE_COMMENT\\n        comment = make_comment(line)\\n        yield Leaf(comment_type, comment, prefix='\\n' * nlines)\\n        if comment in {'\\n            raise FormatOn(consumed)\\n        if comment in {'\\n            raise FormatOff(consumed)\\n        nlines = 0",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_comments(leaf: Leaf) -> Iterator[Leaf]:\\n    p = leaf.prefix\\n    if not p:\\n        return\\n    if '\\n        return\\n    consumed = 0\\n    nlines = 0\\n    for index, line in enumerate(p.split('\\n')):\\n        consumed += len(line) + 1  \\n'\\n        line = line.lstrip()\\n        if not line:\\n            nlines += 1\\n        if not line.startswith('\\n            continue\\n        if index == 0 and leaf.type != token.ENDMARKER:\\n            comment_type = token.COMMENT  \\n        else:\\n            comment_type = STANDALONE_COMMENT\\n        comment = make_comment(line)\\n        yield Leaf(comment_type, comment, prefix='\\n' * nlines)\\n        if comment in {'\\n            raise FormatOn(consumed)\\n        if comment in {'\\n            raise FormatOff(consumed)\\n        nlines = 0"
  },
  {
    "code": "def stop(self):\\n\\t\\t\\tself.docserver.quit = True\\n\\t\\t\\tself.join()\\n\\t\\t\\tself.docserver = None\\n\\t\\t\\tself.serving = False\\n\\t\\t\\tself.url = None\\n\\tthread = ServerThread(urlhandler, port)\\n\\tthread.start()\\n\\twhile not thread.error and not thread.serving:\\n\\t\\ttime.sleep(.01)\\n\\treturn thread",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-31128: Allow pydoc to bind to arbitrary hostnames (#3011)\\n\\nNew -n flag allow overriding localhost with custom value,\\nfor example to run from containers.",
    "fixed_code": "def stop(self):\\n\\t\\t\\tself.docserver.quit = True\\n\\t\\t\\tself.join()\\n\\t\\t\\tself.docserver = None\\n\\t\\t\\tself.serving = False\\n\\t\\t\\tself.url = None\\n\\tthread = ServerThread(urlhandler, hostname, port)\\n\\tthread.start()\\n\\twhile not thread.error and not thread.serving:\\n\\t\\ttime.sleep(.01)\\n\\treturn thread"
  },
  {
    "code": "def __iter__(self):\\n        groups = self.primary.indices.keys()\\n        try:\\n            groupNames = sorted(groups)\\n        except Exception: \\n            pass\\n        for name in groups:\\n            yield name, self[name]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: tests pass except for multi-column. renamed self.klass to DataFrame in test_frame.py",
    "fixed_code": "def __iter__(self):\\n        return iter(self.indices)\\n    _groups = None\\n    @property"
  },
  {
    "code": "def construct_rule(params):\\n    rule = []\\n    append_param(rule, params['protocol'], '-p', False)\\n    append_param(rule, params['source'], '-s', False)\\n    append_param(rule, params['destination'], '-d', False)\\n    append_param(rule, params['match'], '-m', True)\\n    append_tcp_flags(rule, params['tcp_flags'], '--tcp-flags')\\n    append_param(rule, params['jump'], '-j', False)\\n    if params.get('jump') and params['jump'].lower() == 'tee':\\n        append_param(rule, params['gateway'], '--gateway', False)\\n    append_param(rule, params['log_prefix'], '--log-prefix', False)\\n    append_param(rule, params['log_level'], '--log-level', False)\\n    append_param(rule, params['to_destination'], '--to-destination', False)\\n    append_param(rule, params['to_source'], '--to-source', False)\\n    append_param(rule, params['goto'], '-g', False)\\n    append_param(rule, params['in_interface'], '-i', False)\\n    append_param(rule, params['out_interface'], '-o', False)\\n    append_param(rule, params['fragment'], '-f', False)\\n    append_param(rule, params['set_counters'], '-c', False)\\n    append_param(rule, params['source_port'], '--source-port', False)\\n    append_param(rule, params['destination_port'], '--destination-port', False)\\n    append_param(rule, params['to_ports'], '--to-ports', False)\\n    append_param(rule, params['set_dscp_mark'], '--set-dscp', False)\\n    append_param(\\n        rule,\\n        params['set_dscp_mark_class'],\\n        '--set-dscp-class',\\n        False)\\n    append_match_flag(rule, params['syn'], '--syn', True)\\n    append_match(rule, params['comment'], 'comment')\\n    append_param(rule, params['comment'], '--comment', False)\\n    if 'conntrack' in params['match']:\\n        append_csv(rule, params['ctstate'], '--ctstate')\\n    elif 'state' in params['match']:\\n        append_csv(rule, params['ctstate'], '--state')\\n    elif params['ctstate']:\\n        append_match(rule, params['ctstate'], 'conntrack')\\n        append_csv(rule, params['ctstate'], '--ctstate')\\n    append_match(rule, params['limit'] or params['limit_burst'], 'limit')\\n    append_param(rule, params['limit'], '--limit', False)\\n    append_param(rule, params['limit_burst'], '--limit-burst', False)\\n    append_match(rule, params['uid_owner'], 'owner')\\n    append_match_flag(rule, params['uid_owner'], '--uid-owner', True)\\n    append_param(rule, params['uid_owner'], '--uid-owner', False)\\n    if params['jump'] is None:\\n        append_jump(rule, params['reject_with'], 'REJECT')\\n    append_param(rule, params['reject_with'], '--reject-with', False)\\n    append_param(\\n        rule,\\n        params['icmp_type'],\\n        ICMP_TYPE_OPTIONS[params['ip_version']],\\n        False)\\n    return rule",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def construct_rule(params):\\n    rule = []\\n    append_param(rule, params['protocol'], '-p', False)\\n    append_param(rule, params['source'], '-s', False)\\n    append_param(rule, params['destination'], '-d', False)\\n    append_param(rule, params['match'], '-m', True)\\n    append_tcp_flags(rule, params['tcp_flags'], '--tcp-flags')\\n    append_param(rule, params['jump'], '-j', False)\\n    if params.get('jump') and params['jump'].lower() == 'tee':\\n        append_param(rule, params['gateway'], '--gateway', False)\\n    append_param(rule, params['log_prefix'], '--log-prefix', False)\\n    append_param(rule, params['log_level'], '--log-level', False)\\n    append_param(rule, params['to_destination'], '--to-destination', False)\\n    append_param(rule, params['to_source'], '--to-source', False)\\n    append_param(rule, params['goto'], '-g', False)\\n    append_param(rule, params['in_interface'], '-i', False)\\n    append_param(rule, params['out_interface'], '-o', False)\\n    append_param(rule, params['fragment'], '-f', False)\\n    append_param(rule, params['set_counters'], '-c', False)\\n    append_param(rule, params['source_port'], '--source-port', False)\\n    append_param(rule, params['destination_port'], '--destination-port', False)\\n    append_param(rule, params['to_ports'], '--to-ports', False)\\n    append_param(rule, params['set_dscp_mark'], '--set-dscp', False)\\n    append_param(\\n        rule,\\n        params['set_dscp_mark_class'],\\n        '--set-dscp-class',\\n        False)\\n    append_match_flag(rule, params['syn'], '--syn', True)\\n    append_match(rule, params['comment'], 'comment')\\n    append_param(rule, params['comment'], '--comment', False)\\n    if 'conntrack' in params['match']:\\n        append_csv(rule, params['ctstate'], '--ctstate')\\n    elif 'state' in params['match']:\\n        append_csv(rule, params['ctstate'], '--state')\\n    elif params['ctstate']:\\n        append_match(rule, params['ctstate'], 'conntrack')\\n        append_csv(rule, params['ctstate'], '--ctstate')\\n    append_match(rule, params['limit'] or params['limit_burst'], 'limit')\\n    append_param(rule, params['limit'], '--limit', False)\\n    append_param(rule, params['limit_burst'], '--limit-burst', False)\\n    append_match(rule, params['uid_owner'], 'owner')\\n    append_match_flag(rule, params['uid_owner'], '--uid-owner', True)\\n    append_param(rule, params['uid_owner'], '--uid-owner', False)\\n    if params['jump'] is None:\\n        append_jump(rule, params['reject_with'], 'REJECT')\\n    append_param(rule, params['reject_with'], '--reject-with', False)\\n    append_param(\\n        rule,\\n        params['icmp_type'],\\n        ICMP_TYPE_OPTIONS[params['ip_version']],\\n        False)\\n    return rule"
  },
  {
    "code": "def load_current_config(self):\\n        self._current_config = dict()\\n        self._os_version = self._get_os_version()\\n        lag_types = set([lag_obj['type'] for lag_obj in self._required_config])\\n        for lag_type in lag_types:\\n            if_type = self.IF_TYPE_MAP[lag_type]\\n            lag_summary = self._get_port_channels(if_type)\\n            if lag_summary:\\n                self._parse_port_channels_summary(lag_type, lag_summary)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_current_config(self):\\n        self._current_config = dict()\\n        self._os_version = self._get_os_version()\\n        lag_types = set([lag_obj['type'] for lag_obj in self._required_config])\\n        for lag_type in lag_types:\\n            if_type = self.IF_TYPE_MAP[lag_type]\\n            lag_summary = self._get_port_channels(if_type)\\n            if lag_summary:\\n                self._parse_port_channels_summary(lag_type, lag_summary)"
  },
  {
    "code": "def __init__(self, stream_or_string, **options):\\n        super(Deserializer, self).__init__(stream_or_string, **options)\\n        self.event_stream = pulldom.parse(self.stream)\\n        self.db = options.pop('using', DEFAULT_DB_ALIAS)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Restrict the XML deserializer to prevent network and entity-expansion DoS attacks.\\n\\nThis is a security fix. Disclosure and advisory coming shortly.",
    "fixed_code": "def __init__(self, stream_or_string, **options):\\n        super(Deserializer, self).__init__(stream_or_string, **options)\\n        self.event_stream = pulldom.parse(self.stream, self._make_parser())\\n        self.db = options.pop('using', DEFAULT_DB_ALIAS)"
  },
  {
    "code": "def supports_json_field(self):\\n        try:\\n            with self.connection.cursor() as cursor, transaction.atomic():\\n                cursor.execute('SELECT JSON(\\'{\"a\": \"b\"}\\')')\\n        except OperationalError:\\n            return False\\n        return True\\n    can_introspect_json_field = property(operator.attrgetter('supports_json_field'))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #32224 -- Avoided suppressing connection errors in supports_json_field on SQLite.`\\n\\nRegression in 6789ded0a6ab797f0dcdfa6ad5d1cfa46e23abcd.\\n\\nThanks Juan Garcia Alvite for the report.",
    "fixed_code": "def supports_json_field(self):\\n        with self.connection.cursor() as cursor:\\n            try:\\n                with transaction.atomic(self.connection.alias):\\n                    cursor.execute('SELECT JSON(\\'{\"a\": \"b\"}\\')')\\n            except OperationalError:\\n                return False\\n        return True\\n    can_introspect_json_field = property(operator.attrgetter('supports_json_field'))"
  },
  {
    "code": "def call(self, api_method: str, **kwargs) -> None:\\n        self.client.api_call(api_method, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def call(self, api_method: str, **kwargs) -> None:\\n        self.client.api_call(api_method, **kwargs)"
  },
  {
    "code": "def book(self):\\n        pass\\n    @book.setter\\n    @abc.abstractmethod\\n    def book(self, other) -> None:\\n        pass",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def book(self):\\n        pass\\n    @book.setter\\n    @abc.abstractmethod\\n    def book(self, other) -> None:\\n        pass"
  },
  {
    "code": "def itertuples(self, index=True, name=\"Pandas\"):\\n        arrays = []\\n        fields = []\\n        if index:\\n            arrays.append(self.index)\\n            fields.append(\"Index\")\\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\\n        if len(self.columns) + index < 256:\\n            try:\\n                itertuple = collections.namedtuple(\\n                    name, fields+list(self.columns), rename=True)\\n                return (itertuple(*row) for row in zip(*arrays))\\n            except:\\n                pass\\n        return zip(*arrays)\\n    if compat.PY3:  \\n        items = iteritems",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def itertuples(self, index=True, name=\"Pandas\"):\\n        arrays = []\\n        fields = []\\n        if index:\\n            arrays.append(self.index)\\n            fields.append(\"Index\")\\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\\n        if len(self.columns) + index < 256:\\n            try:\\n                itertuple = collections.namedtuple(\\n                    name, fields+list(self.columns), rename=True)\\n                return (itertuple(*row) for row in zip(*arrays))\\n            except:\\n                pass\\n        return zip(*arrays)\\n    if compat.PY3:  \\n        items = iteritems"
  },
  {
    "code": "def _Compare(self, t):\\n        self.write(\"(\")\\n        self.dispatch(t.left)\\n        for o, e in zip(t.ops, t.comparators):\\n            self.write(\" \" + self.cmpops[o.__class__.__name__] + \" \")\\n            self.dispatch(e)\\n            self.write(\")\")\\n    boolops = {_ast.And: 'and', _ast.Or: 'or'}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unparse.py:  fix mispaced parentheses in chained comparisons",
    "fixed_code": "def _Compare(self, t):\\n        self.write(\"(\")\\n        self.dispatch(t.left)\\n        for o, e in zip(t.ops, t.comparators):\\n            self.write(\" \" + self.cmpops[o.__class__.__name__] + \" \")\\n            self.dispatch(e)\\n        self.write(\")\")\\n    boolops = {_ast.And: 'and', _ast.Or: 'or'}"
  },
  {
    "code": "def poly(seq_of_zeros):\\n\\tseq_of_zeros = atleast_1d(seq_of_zeros)\\n\\tsh = seq_of_zeros.shape\\n\\tif len(sh) == 2 and sh[0] == sh[1] and sh[0] != 0:\\n\\t\\tseq_of_zeros = eigvals(seq_of_zeros)\\n\\telif len(sh) == 1:\\n\\t\\tpass\\n\\telse:\\n\\t\\traise ValueError(\"input must be 1d or non-empty square 2d array.\")\\n\\tif len(seq_of_zeros) == 0:\\n\\t\\treturn 1.0\\n\\ta = [1]\\n\\tfor k in range(len(seq_of_zeros)):\\n\\t\\ta = NX.convolve(a, [1, -seq_of_zeros[k]], mode='full')\\n\\tif issubclass(a.dtype.type, NX.complexfloating):\\n\\t\\troots = NX.asarray(seq_of_zeros, complex)\\n\\t\\tpos_roots = sort_complex(NX.compress(roots.imag > 0, roots))\\n\\t\\tneg_roots = NX.conjugate(sort_complex(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tNX.compress(roots.imag < 0, roots)))\\n\\t\\tif (len(pos_roots) == len(neg_roots) and\\n\\t\\t\\t\\tNX.alltrue(neg_roots == pos_roots)):\\n\\t\\t\\ta = a.real.copy()\\n\\treturn a",
    "label": 1,
    "bug_type": "security",
    "bug_description": "ENH: Cast non-object arrays to float in np.poly\\n\\nCloses #5096. Casts integer arrays to np.double, to prevent\\ninteger overflow. Object arrays are left unchanged, to allow\\nuse of arbitrary precision objects.",
    "fixed_code": "def poly(seq_of_zeros):\\n\\tseq_of_zeros = atleast_1d(seq_of_zeros)\\n\\tsh = seq_of_zeros.shape\\n\\tif len(sh) == 2 and sh[0] == sh[1] and sh[0] != 0:\\n\\t\\tseq_of_zeros = eigvals(seq_of_zeros)\\n\\telif len(sh) == 1:\\n\\t\\tdt = seq_of_zeros.dtype\\n\\t\\tif dt != object:\\n\\t\\t\\tseq_of_zeros = seq_of_zeros.astype(mintypecode(dt.char))\\n\\telse:\\n\\t\\traise ValueError(\"input must be 1d or non-empty square 2d array.\")\\n\\tif len(seq_of_zeros) == 0:\\n\\t\\treturn 1.0\\n\\tdt = seq_of_zeros.dtype\\n\\ta = ones((1,), dtype=dt)\\n\\tfor k in range(len(seq_of_zeros)):\\n\\t\\ta = NX.convolve(a, array([1, -seq_of_zeros[k]], dtype=dt),\\n\\t\\t\\t\\t\\t\\tmode='full')\\n\\tif issubclass(a.dtype.type, NX.complexfloating):\\n\\t\\troots = NX.asarray(seq_of_zeros, complex)\\n\\t\\tpos_roots = sort_complex(NX.compress(roots.imag > 0, roots))\\n\\t\\tneg_roots = NX.conjugate(sort_complex(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tNX.compress(roots.imag < 0, roots)))\\n\\t\\tif (len(pos_roots) == len(neg_roots) and\\n\\t\\t\\t\\tNX.alltrue(neg_roots == pos_roots)):\\n\\t\\t\\ta = a.real.copy()\\n\\treturn a"
  },
  {
    "code": "def load(file, mmap_mode=None, allow_pickle=True, fix_imports=True,\\n\\t\\t encoding='ASCII'):\\n\\tif encoding not in ('ASCII', 'latin1', 'bytes'):\\n\\t\\traise ValueError(\"encoding must be 'ASCII', 'latin1', or 'bytes'\")\\n\\tif sys.version_info[0] >= 3:\\n\\t\\tpickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\\n\\telse:\\n\\t\\tpickle_kwargs = {}\\n\\tif hasattr(file, 'read'):\\n\\t\\tfid = file\\n\\t\\town_fid = False\\n\\telse:\\n\\t\\tfid = open(os_fspath(file), \"rb\")\\n\\t\\town_fid = True\\n\\ttry:\\n\\t\\t_ZIP_PREFIX = b'PK\\x03\\x04'\\n\\t\\t_ZIP_SUFFIX = b'PK\\x05\\x06' \\n\\t\\tN = len(format.MAGIC_PREFIX)\\n\\t\\tmagic = fid.read(N)\\n\\t\\tfid.seek(-min(N, len(magic)), 1)  \\n\\t\\tif magic.startswith(_ZIP_PREFIX) or magic.startswith(_ZIP_SUFFIX):\\n\\t\\t\\tret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\\n\\t\\t\\t\\t\\t\\t  pickle_kwargs=pickle_kwargs)\\n\\t\\t\\town_fid = False\\n\\t\\t\\treturn ret\\n\\t\\telif magic == format.MAGIC_PREFIX:\\n\\t\\t\\tif mmap_mode:\\n\\t\\t\\t\\treturn format.open_memmap(file, mode=mmap_mode)\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn format.read_array(fid, allow_pickle=allow_pickle,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t pickle_kwargs=pickle_kwargs)\\n\\t\\telse:\\n\\t\\t\\tif not allow_pickle:\\n\\t\\t\\t\\traise ValueError(\"Cannot load file containing pickled data \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"when allow_pickle=False\")\\n\\t\\t\\ttry:\\n\\t\\t\\t\\treturn pickle.load(fid, **pickle_kwargs)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\traise IOError(\\n\\t\\t\\t\\t\\t\"Failed to interpret file %s as a pickle\" % repr(file))\\n\\tfinally:\\n\\t\\tif own_fid:\\n\\t\\t\\tfid.close()",
    "label": 1,
    "bug_type": "security",
    "bug_description": "BUG: load fails when using pickle without allow_pickle=True\\n\\na partial mitigation of #12759.",
    "fixed_code": "def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\\n\\t\\t encoding='ASCII'):\\n\\tif encoding not in ('ASCII', 'latin1', 'bytes'):\\n\\t\\traise ValueError(\"encoding must be 'ASCII', 'latin1', or 'bytes'\")\\n\\tif sys.version_info[0] >= 3:\\n\\t\\tpickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\\n\\telse:\\n\\t\\tpickle_kwargs = {}\\n\\tif hasattr(file, 'read'):\\n\\t\\tfid = file\\n\\t\\town_fid = False\\n\\telse:\\n\\t\\tfid = open(os_fspath(file), \"rb\")\\n\\t\\town_fid = True\\n\\ttry:\\n\\t\\t_ZIP_PREFIX = b'PK\\x03\\x04'\\n\\t\\t_ZIP_SUFFIX = b'PK\\x05\\x06' \\n\\t\\tN = len(format.MAGIC_PREFIX)\\n\\t\\tmagic = fid.read(N)\\n\\t\\tfid.seek(-min(N, len(magic)), 1)  \\n\\t\\tif magic.startswith(_ZIP_PREFIX) or magic.startswith(_ZIP_SUFFIX):\\n\\t\\t\\tret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\\n\\t\\t\\t\\t\\t\\t  pickle_kwargs=pickle_kwargs)\\n\\t\\t\\town_fid = False\\n\\t\\t\\treturn ret\\n\\t\\telif magic == format.MAGIC_PREFIX:\\n\\t\\t\\tif mmap_mode:\\n\\t\\t\\t\\treturn format.open_memmap(file, mode=mmap_mode)\\n\\t\\t\\telse:\\n\\t\\t\\t\\treturn format.read_array(fid, allow_pickle=allow_pickle,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t pickle_kwargs=pickle_kwargs)\\n\\t\\telse:\\n\\t\\t\\tif not allow_pickle:\\n\\t\\t\\t\\traise ValueError(\"Cannot load file containing pickled data \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"when allow_pickle=False\")\\n\\t\\t\\ttry:\\n\\t\\t\\t\\treturn pickle.load(fid, **pickle_kwargs)\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\traise IOError(\\n\\t\\t\\t\\t\\t\"Failed to interpret file %s as a pickle\" % repr(file))\\n\\tfinally:\\n\\t\\tif own_fid:\\n\\t\\t\\tfid.close()"
  },
  {
    "code": "def __iter__(self):\\n        return self.data.keys()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Nailed test_weakref.py.  Pfew, messy!",
    "fixed_code": "def __iter__(self):\\n        return iter(self.data.keys())"
  },
  {
    "code": "def filepath_to_uri(path):\\n    if path is None:\\n        return path\\n    return quote(smart_bytes(path.replace(\"\\\\\", \"/\")), safe=b\"/~!*()'\")\\ntry:\\n    DEFAULT_LOCALE_ENCODING = locale.getdefaultlocale()[1] or 'ascii'\\n    codecs.lookup(DEFAULT_LOCALE_ENCODING)\\nexcept:\\n    DEFAULT_LOCALE_ENCODING = 'ascii'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def filepath_to_uri(path):\\n    if path is None:\\n        return path\\n    return quote(smart_bytes(path.replace(\"\\\\\", \"/\")), safe=b\"/~!*()'\")\\ntry:\\n    DEFAULT_LOCALE_ENCODING = locale.getdefaultlocale()[1] or 'ascii'\\n    codecs.lookup(DEFAULT_LOCALE_ENCODING)\\nexcept:\\n    DEFAULT_LOCALE_ENCODING = 'ascii'"
  },
  {
    "code": "def walk(top, func, arg):\\n    try:\\n        names = os.listdir(top)\\n    except os.error:\\n        return\\n    func(arg, top, names)\\n    for name in names:\\n        name = join(top, name)\\n        if isdir(name) and not islink(name):\\n            walk(name, func, arg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add Py3k warnings to os.path.walk",
    "fixed_code": "def walk(top, func, arg):\\n    warnings.warnpy3k(\"In 3.x, os.path.walk is removed in favor of os.walk.\")\\n    try:\\n        names = os.listdir(top)\\n    except os.error:\\n        return\\n    func(arg, top, names)\\n    for name in names:\\n        name = join(top, name)\\n        if isdir(name) and not islink(name):\\n            walk(name, func, arg)"
  },
  {
    "code": "def map_config_to_obj(module):\\n    output = run_commands(module, ['show banner %s' % module.params['banner']])\\n    obj = {'banner': module.params['banner'], 'state': 'absent'}\\n    if output:\\n        if module.params['transport'] == 'cli':\\n            obj['text'] = output[0]\\n        else:\\n            if module.params['banner'] == 'login':\\n                banner_response_key = 'loginBanner'\\n            else:\\n                banner_response_key = 'motd'\\n            if isinstance(output[0], dict) and banner_response_key in output[0].keys():\\n                obj['text'] = output[0][banner_response_key].strip('\\n')\\n        obj['state'] = 'present'\\n    return obj",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes #30281 eos_banner integration test failure (#30283)",
    "fixed_code": "def map_config_to_obj(module):\\n    output = run_commands(module, ['show banner %s' % module.params['banner']])\\n    obj = {'banner': module.params['banner'], 'state': 'absent'}\\n    if output:\\n        if module.params['transport'] == 'cli':\\n            obj['text'] = output[0]\\n        else:\\n            if module.params['banner'] == 'login':\\n                banner_response_key = 'loginBanner'\\n            else:\\n                banner_response_key = 'motd'\\n            if isinstance(output[0], dict) and banner_response_key in output[0].keys():\\n                obj['text'] = output[0]\\n        obj['state'] = 'present'\\n    return obj"
  },
  {
    "code": "def _interval_str_to_code(freqstr):\\n    freqstr = _rule_aliases.get(freqstr, freqstr)\\n    freqstr = _rule_aliases.get(freqstr.lower(), freqstr)\\n    try:\\n        freqstr = freqstr.upper()\\n        return _interval_code_map[freqstr]\\n    except:\\n        alias = _skts_alias_dict[freqstr]\\n        try:\\n            return _interval_code_map[alias]\\n        except:\\n            raise \"Could not interpret frequency %s\" % freqstr\\n_gfc = _get_freq_code",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _interval_str_to_code(freqstr):\\n    freqstr = _rule_aliases.get(freqstr, freqstr)\\n    freqstr = _rule_aliases.get(freqstr.lower(), freqstr)\\n    try:\\n        freqstr = freqstr.upper()\\n        return _interval_code_map[freqstr]\\n    except:\\n        alias = _skts_alias_dict[freqstr]\\n        try:\\n            return _interval_code_map[alias]\\n        except:\\n            raise \"Could not interpret frequency %s\" % freqstr\\n_gfc = _get_freq_code"
  },
  {
    "code": "def atomize(self, declarations):\\n        for prop, value in declarations:\\n            attr = \"expand_\" + prop.replace(\"-\", \"_\")\\n            try:\\n                expand = getattr(self, attr)\\n            except AttributeError:\\n                yield prop, value\\n            else:\\n                for prop, value in expand(prop, value):\\n                    yield prop, value\\n    expand_border_color = _side_expander(\"border-{:s}-color\")\\n    expand_border_style = _side_expander(\"border-{:s}-style\")\\n    expand_border_width = _side_expander(\"border-{:s}-width\")\\n    expand_margin = _side_expander(\"margin-{:s}\")\\n    expand_padding = _side_expander(\"padding-{:s}\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/ENH: Translate CSS border properties for `Styler.to_excel` (#45312)",
    "fixed_code": "def atomize(self, declarations) -> Generator[tuple[str, str], None, None]:\\n        for prop, value in declarations:\\n            attr = \"expand_\" + prop.replace(\"-\", \"_\")\\n            try:\\n                expand = getattr(self, attr)\\n            except AttributeError:\\n                yield prop, value\\n            else:\\n                for prop, value in expand(prop, value):\\n                    yield prop, value\\n    expand_border = _border_expander()\\n    expand_border_top = _border_expander(\"top\")\\n    expand_border_right = _border_expander(\"right\")\\n    expand_border_bottom = _border_expander(\"bottom\")\\n    expand_border_left = _border_expander(\"left\")\\n    expand_border_color = _side_expander(\"border-{:s}-color\")\\n    expand_border_style = _side_expander(\"border-{:s}-style\")\\n    expand_border_width = _side_expander(\"border-{:s}-width\")\\n    expand_margin = _side_expander(\"margin-{:s}\")\\n    expand_padding = _side_expander(\"padding-{:s}\")"
  },
  {
    "code": "def data_received(self, data):\\n        self._stream_reader.feed_data(data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34638: Store a weak reference to stream reader to break strong references loop (GH-9201)\\n\\nStore a weak reference to stream readerfor breaking strong references\\n\\nIt breaks the strong reference loop between reader and protocol and allows to detect and close the socket if the stream is deleted (garbage collected)",
    "fixed_code": "def data_received(self, data):\\n        reader = self._stream_reader\\n        if reader is not None:\\n            reader.feed_data(data)"
  },
  {
    "code": "def as_ul(self):\\n        \"Returns this form rendered as HTML <li>s -- excluding the <ul></ul>.\"\\n        top_errors = self.non_field_errors()\\n        output = []\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            if bf.is_hidden:\\n                new_errors = bf.errors \\n                if new_errors:\\n                    top_errors.extend(['(Hidden field %s) %s' % (name, e) for e in new_errors])\\n                output.append(unicode(bf))\\n            else:\\n                output.append(u'<li>%s%s %s</li>' % (bf.errors, bf.label_tag(escape(bf.verbose_name+':')), bf))\\n        if top_errors:\\n            output.insert(0, u'<li>%s</li>' % top_errors)\\n        return u'\\n'.join(output)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "newforms: Form.as_ul() no longer puts hidden fields between <li>s. Similar to [4175], which was the same thing for Form.as_table(). Refs #3101",
    "fixed_code": "def as_ul(self):\\n        \"Returns this form rendered as HTML <li>s -- excluding the <ul></ul>.\"\\n        top_errors = self.non_field_errors()\\n        output, hidden_fields = [], []\\n        for name, field in self.fields.items():\\n            bf = BoundField(self, field, name)\\n            if bf.is_hidden:\\n                new_errors = bf.errors \\n                if new_errors:\\n                    top_errors.extend(['(Hidden field %s) %s' % (name, e) for e in new_errors])\\n                hidden_fields.append(unicode(bf))\\n            else:\\n                output.append(u'<li>%s%s %s</li>' % (bf.errors, bf.label_tag(escape(bf.verbose_name+':')), bf))\\n        if top_errors:\\n            output.insert(0, u'<li>%s</li>' % top_errors)\\n        if hidden_fields: \\n            str_hidden = u''.join(hidden_fields)\\n            if output:\\n                last_li = output[-1]\\n                output[-1] = last_li[:-5] + str_hidden + '</li>'\\n            else: \\n                output.append(str_hidden)\\n        return u'\\n'.join(output)"
  },
  {
    "code": "def rfind(a, sub, start=0, end=None):\\n    return _vec_string(\\n        a, integer, 'rfind', [sub, start] + _clean_args(end))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEP: Do not allow \"abstract\" dtype conversion/creation\\n\\nThese dtypes do not really make sense as instances. We can (somewhat)\\nreasonably define np.dtype(np.int64) as the default (machine endianess)\\nint64. (Arguably, it is unclear that `np.array(arr_of_>f8, dtype=\"f\")`\\nshould return arr_of_<f8, but that would be very noisy!)\\n\\nHowever, `np.integer` as equivalent to long, is not well defined.\\nSimilarly, `dtype=Decimal` may be neat to spell `dtype=object` when you\\nintend to put Decimal objects into the array. But it is misleading,\\nsince there is no special meaning to it at this time.\\n\\nThe biggest issue with it, is that `arr.astype(np.floating)` looks\\nlike it will let float32 or float128 pass, but it will force a\\nfloat64 output! Arguably downcasting is a bug in this case.\\n\\nA related issue is `np.dtype(\"S\")` and especially \"S0\". The dtype \"S\"\\ndoes make sense for most or all places where `dtype=...` can be\\npassed. However, it is conceptionally different from other dtypes, since\\nit will not end up being attached to the array (unlike \"S2\" which\\nwould be). The dtype \"S\" really means the type number/DType class\\nof String, and not a specific dtype instance.",
    "fixed_code": "def rfind(a, sub, start=0, end=None):\\n    return _vec_string(\\n        a, int_, 'rfind', [sub, start] + _clean_args(end))"
  },
  {
    "code": "def select(self, css):\\n        return CSSSelectorList(super(CSSSelectorMixin, self).select(self.translator.css_to_xpath(css)))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "extend css selectors with \":text\" and :attribute(<name>) #176",
    "fixed_code": "def select(self, css):\\n        xpath = self._css2xpath(css)\\n        return super(CSSSelectorMixin, self).select(xpath)"
  },
  {
    "code": "def func_accepts_kwargs(func):\\n    return any(\\n        p for p in _get_callable_parameters(func)\\n        if p.kind == p.VAR_KEYWORD\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def func_accepts_kwargs(func):\\n    return any(\\n        p for p in _get_callable_parameters(func)\\n        if p.kind == p.VAR_KEYWORD\\n    )"
  },
  {
    "code": "def get_instance_value_for_fields(instance, fields):\\n        return tuple([getattr(instance, field.attname) for field in fields])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #20820 -- Model inheritance + m2m fixture loading regression\\n\\nTests by Tim Graham, report from jeroen.pulles@redslider.net.",
    "fixed_code": "def get_instance_value_for_fields(instance, fields):\\n        ret = []\\n        for field in fields:\\n            if field.primary_key:\\n                ret.append(instance.pk)\\n            else:\\n                ret.append(getattr(instance, field.attname))\\n        return tuple(ret)"
  },
  {
    "code": "def _merge_blocks(blocks):\\n    ref_cols = blocks[0].ref_columns\\n    new_values = np.hstack([b.values for b in blocks])\\n    new_locs = np.concatenate([b.ref_locs for b in blocks])\\n    new_block = make_block(new_values, new_locs, ref_cols)\\n    _ = new_block.columns\\n    return new_block",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def _merge_blocks(blocks):\\n    ref_cols = blocks[0].ref_columns\\n    new_values = np.hstack([b.values for b in blocks])\\n    new_cols = np.concatenate([b.columns for b in blocks])\\n    new_block = make_block(new_values, new_cols, ref_cols)\\n    return new_block"
  },
  {
    "code": "def read_table(\\n    filepath_or_buffer: FilePathOrBuffer,\\n    sep=lib.no_default,\\n    delimiter=None,\\n    header=\"infer\",\\n    names=None,\\n    index_col=None,\\n    usecols=None,\\n    squeeze=False,\\n    prefix=None,\\n    mangle_dupe_cols=True,\\n    dtype=None,\\n    engine=None,\\n    converters=None,\\n    true_values=None,\\n    false_values=None,\\n    skipinitialspace=False,\\n    skiprows=None,\\n    skipfooter=0,\\n    nrows=None,\\n    na_values=None,\\n    keep_default_na=True,\\n    na_filter=True,\\n    verbose=False,\\n    skip_blank_lines=True,\\n    parse_dates=False,\\n    infer_datetime_format=False,\\n    keep_date_col=False,\\n    date_parser=None,\\n    dayfirst=False,\\n    cache_dates=True,\\n    iterator=False,\\n    chunksize=None,\\n    compression=\"infer\",\\n    thousands=None,\\n    decimal: str = \".\",\\n    lineterminator=None,\\n    quotechar='\"',\\n    quoting=csv.QUOTE_MINIMAL,\\n    doublequote=True,\\n    escapechar=None,\\n    comment=None,\\n    encoding=None,\\n    dialect=None,\\n    error_bad_lines=True,\\n    warn_bad_lines=True,\\n    delim_whitespace=False,\\n    low_memory=_c_parser_defaults[\"low_memory\"],\\n    memory_map=False,\\n    float_precision=None,\\n):\\n    kwds = locals()\\n    del kwds[\"filepath_or_buffer\"]\\n    del kwds[\"sep\"]\\n    kwds_defaults = _check_defaults_read(\\n        dialect, delimiter, delim_whitespace, engine, sep, defaults={\"delimiter\": \"\\t\"}\\n    )\\n    kwds.update(kwds_defaults)\\n    return _read(filepath_or_buffer, kwds)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_table(\\n    filepath_or_buffer: FilePathOrBuffer,\\n    sep=lib.no_default,\\n    delimiter=None,\\n    header=\"infer\",\\n    names=None,\\n    index_col=None,\\n    usecols=None,\\n    squeeze=False,\\n    prefix=None,\\n    mangle_dupe_cols=True,\\n    dtype=None,\\n    engine=None,\\n    converters=None,\\n    true_values=None,\\n    false_values=None,\\n    skipinitialspace=False,\\n    skiprows=None,\\n    skipfooter=0,\\n    nrows=None,\\n    na_values=None,\\n    keep_default_na=True,\\n    na_filter=True,\\n    verbose=False,\\n    skip_blank_lines=True,\\n    parse_dates=False,\\n    infer_datetime_format=False,\\n    keep_date_col=False,\\n    date_parser=None,\\n    dayfirst=False,\\n    cache_dates=True,\\n    iterator=False,\\n    chunksize=None,\\n    compression=\"infer\",\\n    thousands=None,\\n    decimal: str = \".\",\\n    lineterminator=None,\\n    quotechar='\"',\\n    quoting=csv.QUOTE_MINIMAL,\\n    doublequote=True,\\n    escapechar=None,\\n    comment=None,\\n    encoding=None,\\n    dialect=None,\\n    error_bad_lines=True,\\n    warn_bad_lines=True,\\n    delim_whitespace=False,\\n    low_memory=_c_parser_defaults[\"low_memory\"],\\n    memory_map=False,\\n    float_precision=None,\\n):\\n    kwds = locals()\\n    del kwds[\"filepath_or_buffer\"]\\n    del kwds[\"sep\"]\\n    kwds_defaults = _check_defaults_read(\\n        dialect, delimiter, delim_whitespace, engine, sep, defaults={\"delimiter\": \"\\t\"}\\n    )\\n    kwds.update(kwds_defaults)\\n    return _read(filepath_or_buffer, kwds)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        lines=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'none']),\\n        comment=dict(default=DEFAULT_COMMENT),\\n        config=dict(),\\n        backup=dict(type='bool', default=False),\\n        save=dict(type='bool', default=False),\\n    )\\n    argument_spec.update(vyos_argument_spec)\\n    mutually_exclusive = [('lines', 'src')]\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        mutually_exclusive=mutually_exclusive,\\n        supports_check_mode=True\\n    )\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    if module.params['backup']:\\n        result['__backup__'] = get_config(module=module)\\n    if any((module.params['src'], module.params['lines'])):\\n        run(module, result)\\n    if module.params['save']:\\n        diff = run_commands(module, commands=['configure', 'compare saved'])[1]\\n        if diff != '[edit]':\\n            run_commands(module, commands=['save'])\\n            result['changed'] = True\\n        run_commands(module, commands=['exit'])\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        lines=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'none']),\\n        comment=dict(default=DEFAULT_COMMENT),\\n        config=dict(),\\n        backup=dict(type='bool', default=False),\\n        save=dict(type='bool', default=False),\\n    )\\n    argument_spec.update(vyos_argument_spec)\\n    mutually_exclusive = [('lines', 'src')]\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        mutually_exclusive=mutually_exclusive,\\n        supports_check_mode=True\\n    )\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, warnings=warnings)\\n    if module.params['backup']:\\n        result['__backup__'] = get_config(module=module)\\n    if any((module.params['src'], module.params['lines'])):\\n        run(module, result)\\n    if module.params['save']:\\n        diff = run_commands(module, commands=['configure', 'compare saved'])[1]\\n        if diff != '[edit]':\\n            run_commands(module, commands=['save'])\\n            result['changed'] = True\\n        run_commands(module, commands=['exit'])\\n    module.exit_json(**result)"
  },
  {
    "code": "def mask_missing(arr, values_to_mask):\\n    if not isinstance(values_to_mask, (list, np.ndarray)):\\n        values_to_mask = [values_to_mask]\\n    try:\\n        values = np.array(values_to_mask)\\n        cant_cast = not np.can_cast(values.dtype, arr.dtype, casting='safe')\\n        if cant_cast and arr.dtype == np.bool_:\\n            values_to_mask = values\\n        else:\\n            values_to_mask = np.array(values_to_mask, dtype=arr.dtype)\\n    except Exception:\\n        values_to_mask = np.array(values_to_mask, dtype=object)\\n    na_mask = isnull(values_to_mask)\\n    nonna = values_to_mask[-na_mask]\\n    mask = None\\n    for x in nonna:\\n        if mask is None:\\n            mask = arr == x\\n            if not isinstance(mask, np.ndarray):\\n                m = mask\\n                mask = np.empty(arr.shape, dtype=np.bool)\\n                mask.fill(m)\\n        else:\\n            mask = mask | (arr == x)\\n    if na_mask.any():\\n        if mask is None:\\n            mask = isnull(arr)\\n        else:\\n            mask = mask | isnull(arr)\\n    return mask",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mask_missing(arr, values_to_mask):\\n    if not isinstance(values_to_mask, (list, np.ndarray)):\\n        values_to_mask = [values_to_mask]\\n    try:\\n        values = np.array(values_to_mask)\\n        cant_cast = not np.can_cast(values.dtype, arr.dtype, casting='safe')\\n        if cant_cast and arr.dtype == np.bool_:\\n            values_to_mask = values\\n        else:\\n            values_to_mask = np.array(values_to_mask, dtype=arr.dtype)\\n    except Exception:\\n        values_to_mask = np.array(values_to_mask, dtype=object)\\n    na_mask = isnull(values_to_mask)\\n    nonna = values_to_mask[-na_mask]\\n    mask = None\\n    for x in nonna:\\n        if mask is None:\\n            mask = arr == x\\n            if not isinstance(mask, np.ndarray):\\n                m = mask\\n                mask = np.empty(arr.shape, dtype=np.bool)\\n                mask.fill(m)\\n        else:\\n            mask = mask | (arr == x)\\n    if na_mask.any():\\n        if mask is None:\\n            mask = isnull(arr)\\n        else:\\n            mask = mask | isnull(arr)\\n    return mask"
  },
  {
    "code": "def set_caption(self, caption: str) -> Styler:\\n        self.caption = caption\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add long and short captions to `Styler.to_latex` (#41659)",
    "fixed_code": "def set_caption(self, caption: str | tuple) -> Styler:\\n        self.caption = caption\\n        return self"
  },
  {
    "code": "def validate_external_location(uri):\\n\\tscheme = urlparse.urlparse(uri).scheme\\n\\treturn (scheme in store_api.get_known_schemes() and\\n\\t\\t\\tscheme not in RESTRICTED_URI_SCHEMAS)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def validate_external_location(uri):\\n\\tscheme = urlparse.urlparse(uri).scheme\\n\\treturn (scheme in store_api.get_known_schemes() and\\n\\t\\t\\tscheme not in RESTRICTED_URI_SCHEMAS)"
  },
  {
    "code": "def lexsort_indexer(\\n    keys, orders=None, na_position: str = \"last\", key: Optional[Callable] = None\\n):\\n    from pandas.core.arrays import Categorical\\n    labels = []\\n    shape = []\\n    if isinstance(orders, bool):\\n        orders = [orders] * len(keys)\\n    elif orders is None:\\n        orders = [True] * len(keys)\\n    keys = [ensure_key_mapped(k, key) for k in keys]\\n    for k, order in zip(keys, orders):\\n        if is_categorical_dtype(k):\\n            cat = k\\n        else:\\n            cat = Categorical(k, ordered=True)\\n        if na_position not in [\"last\", \"first\"]:\\n            raise ValueError(f\"invalid na_position: {na_position}\")\\n        n = len(cat.categories)\\n        codes = cat.codes.copy()\\n        mask = cat.codes == -1\\n        if order:  \\n            if na_position == \"last\":\\n                codes = np.where(mask, n, codes)\\n            elif na_position == \"first\":\\n                codes += 1\\n        else:  \\n            if na_position == \"last\":\\n                codes = np.where(mask, n, n - codes - 1)\\n            elif na_position == \"first\":\\n                codes = np.where(mask, 0, n - codes)\\n        if mask.any():\\n            n += 1\\n        shape.append(n)\\n        labels.append(codes)\\n    return indexer_from_factorized(labels, shape)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Always cast to Categorical in lexsort_indexer (#36385)",
    "fixed_code": "def lexsort_indexer(\\n    keys, orders=None, na_position: str = \"last\", key: Optional[Callable] = None\\n):\\n    from pandas.core.arrays import Categorical\\n    labels = []\\n    shape = []\\n    if isinstance(orders, bool):\\n        orders = [orders] * len(keys)\\n    elif orders is None:\\n        orders = [True] * len(keys)\\n    keys = [ensure_key_mapped(k, key) for k in keys]\\n    for k, order in zip(keys, orders):\\n        cat = Categorical(k, ordered=True)\\n        if na_position not in [\"last\", \"first\"]:\\n            raise ValueError(f\"invalid na_position: {na_position}\")\\n        n = len(cat.categories)\\n        codes = cat.codes.copy()\\n        mask = cat.codes == -1\\n        if order:  \\n            if na_position == \"last\":\\n                codes = np.where(mask, n, codes)\\n            elif na_position == \"first\":\\n                codes += 1\\n        else:  \\n            if na_position == \"last\":\\n                codes = np.where(mask, n, n - codes - 1)\\n            elif na_position == \"first\":\\n                codes = np.where(mask, 0, n - codes)\\n        if mask.any():\\n            n += 1\\n        shape.append(n)\\n        labels.append(codes)\\n    return indexer_from_factorized(labels, shape)"
  },
  {
    "code": "def to_gbq(self, destination_table, project_id=None, chunksize=10000,\\n               verbose=True, reauth=False):\\n        from pandas.io import gbq\\n        return gbq.to_gbq(self, destination_table, project_id=project_id,\\n                          chunksize=chunksize, verbose=verbose,\\n                          reauth=reauth)\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_gbq(self, destination_table, project_id=None, chunksize=10000,\\n               verbose=True, reauth=False):\\n        from pandas.io import gbq\\n        return gbq.to_gbq(self, destination_table, project_id=project_id,\\n                          chunksize=chunksize, verbose=verbose,\\n                          reauth=reauth)\\n    @classmethod"
  },
  {
    "code": "def post_dag_run(dag_id, session):\\n    if not session.query(DagModel).filter(DagModel.dag_id == dag_id).first():\\n        raise NotFound(title=\"DAG not found\", detail=f\"DAG with dag_id: '{dag_id}' not found\")\\n    try:\\n        post_body = dagrun_schema.load(request.json, session=session)\\n    except ValidationError as err:\\n        raise BadRequest(detail=str(err))\\n    dagrun_instance = (\\n        session.query(DagRun).filter(DagRun.dag_id == dag_id, DagRun.run_id == post_body[\"run_id\"]).first()\\n    )\\n    if not dagrun_instance:\\n        dag_run = DagRun(dag_id=dag_id, run_type=DagRunType.MANUAL, **post_body)\\n        session.add(dag_run)\\n        session.commit()\\n        return dagrun_schema.dump(dag_run)\\n    raise AlreadyExists(\\n        detail=f\"DAGRun with DAG ID: '{dag_id}' and DAGRun ID: '{post_body['run_id']}' already exists\"\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: Raise `AlreadyExists` exception when the execution_date is same (#15174)\\n\\nThis issue occurs when the execution_date is same as previous dag run, raise a AlreadyExists exception with 409 status code instead of 500 error\\nFIXES: #15150",
    "fixed_code": "def post_dag_run(dag_id, session):\\n    if not session.query(DagModel).filter(DagModel.dag_id == dag_id).first():\\n        raise NotFound(title=\"DAG not found\", detail=f\"DAG with dag_id: '{dag_id}' not found\")\\n    try:\\n        post_body = dagrun_schema.load(request.json, session=session)\\n    except ValidationError as err:\\n        raise BadRequest(detail=str(err))\\n    dagrun_instance = (\\n        session.query(DagRun)\\n        .filter(\\n            DagRun.dag_id == dag_id,\\n            or_(DagRun.run_id == post_body[\"run_id\"], DagRun.execution_date == post_body[\"execution_date\"]),\\n        )\\n        .first()\\n    )\\n    if not dagrun_instance:\\n        dag_run = DagRun(dag_id=dag_id, run_type=DagRunType.MANUAL, **post_body)\\n        session.add(dag_run)\\n        session.commit()\\n        return dagrun_schema.dump(dag_run)\\n    if dagrun_instance.execution_date == post_body[\"execution_date\"]:\\n        raise AlreadyExists(\\n            detail=f\"DAGRun with DAG ID: '{dag_id}' and \"\\n            f\"DAGRun ExecutionDate: '{post_body['execution_date']}' already exists\"\\n        )\\n    raise AlreadyExists(\\n        detail=f\"DAGRun with DAG ID: '{dag_id}' and DAGRun ID: '{post_body['run_id']}' already exists\"\\n    )"
  },
  {
    "code": "def get_supported_language_variant(lang_code, supported=None):\\n    if supported is None:\\n        from django.conf import settings\\n        supported = dict(settings.LANGUAGES)\\n    if lang_code:\\n        variants = (lang_code, lang_code.lower(), lang_code.split('-')[0],\\n                    lang_code.lower().split('-')[0])\\n        for code in variants:\\n            if code in supported and check_for_language(code):\\n                return code\\n    raise LookupError(lang_code)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_supported_language_variant(lang_code, supported=None):\\n    if supported is None:\\n        from django.conf import settings\\n        supported = dict(settings.LANGUAGES)\\n    if lang_code:\\n        variants = (lang_code, lang_code.lower(), lang_code.split('-')[0],\\n                    lang_code.lower().split('-')[0])\\n        for code in variants:\\n            if code in supported and check_for_language(code):\\n                return code\\n    raise LookupError(lang_code)"
  },
  {
    "code": "def create_session(self) -> boto3.session.Session:\\n        session_kwargs = {}\\n        if \"session_kwargs\" in self.extra_config:\\n            self.log.info(\\n                \"Retrieving session_kwargs from Connection.extra_config['session_kwargs']: %s\",\\n                self.extra_config[\"session_kwargs\"],\\n            )\\n            session_kwargs = self.extra_config[\"session_kwargs\"]\\n        self.basic_session = self._create_basic_session(session_kwargs=session_kwargs)\\n        self.role_arn = self._read_role_arn_from_extra_config()\\n        if self.role_arn is None:\\n            return self.basic_session\\n        return self._create_session_with_assume_role(session_kwargs=session_kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_session(self) -> boto3.session.Session:\\n        session_kwargs = {}\\n        if \"session_kwargs\" in self.extra_config:\\n            self.log.info(\\n                \"Retrieving session_kwargs from Connection.extra_config['session_kwargs']: %s\",\\n                self.extra_config[\"session_kwargs\"],\\n            )\\n            session_kwargs = self.extra_config[\"session_kwargs\"]\\n        self.basic_session = self._create_basic_session(session_kwargs=session_kwargs)\\n        self.role_arn = self._read_role_arn_from_extra_config()\\n        if self.role_arn is None:\\n            return self.basic_session\\n        return self._create_session_with_assume_role(session_kwargs=session_kwargs)"
  },
  {
    "code": "def display_for_field(value, field, empty_value_display):\\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\\n    if getattr(field, \"flatchoices\", None):\\n        try:\\n            return dict(field.flatchoices).get(value, empty_value_display)\\n        except TypeError:\\n            flatchoices = make_hashable(field.flatchoices)\\n            value = make_hashable(value)\\n            return dict(flatchoices).get(value, empty_value_display)\\n    elif isinstance(field, models.BooleanField):\\n        return _boolean_icon(value)\\n    elif value is None:\\n        return empty_value_display\\n    elif isinstance(field, models.DateTimeField):\\n        return formats.localize(timezone.template_localtime(value))\\n    elif isinstance(field, (models.DateField, models.TimeField)):\\n        return formats.localize(value)\\n    elif isinstance(field, models.DecimalField):\\n        return formats.number_format(value, field.decimal_places)\\n    elif isinstance(field, (models.IntegerField, models.FloatField)):\\n        return formats.number_format(value)\\n    elif isinstance(field, models.FileField) and value:\\n        return format_html('<a href=\"{}\">{}</a>', value.url, value)\\n    elif isinstance(field, models.JSONField) and value:\\n        try:\\n            return json.dumps(value, ensure_ascii=False, cls=field.encoder)\\n        except TypeError:\\n            return display_for_value(value, empty_value_display)\\n    else:\\n        return display_for_value(value, empty_value_display)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def display_for_field(value, field, empty_value_display):\\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\\n    if getattr(field, \"flatchoices\", None):\\n        try:\\n            return dict(field.flatchoices).get(value, empty_value_display)\\n        except TypeError:\\n            flatchoices = make_hashable(field.flatchoices)\\n            value = make_hashable(value)\\n            return dict(flatchoices).get(value, empty_value_display)\\n    elif isinstance(field, models.BooleanField):\\n        return _boolean_icon(value)\\n    elif value is None:\\n        return empty_value_display\\n    elif isinstance(field, models.DateTimeField):\\n        return formats.localize(timezone.template_localtime(value))\\n    elif isinstance(field, (models.DateField, models.TimeField)):\\n        return formats.localize(value)\\n    elif isinstance(field, models.DecimalField):\\n        return formats.number_format(value, field.decimal_places)\\n    elif isinstance(field, (models.IntegerField, models.FloatField)):\\n        return formats.number_format(value)\\n    elif isinstance(field, models.FileField) and value:\\n        return format_html('<a href=\"{}\">{}</a>', value.url, value)\\n    elif isinstance(field, models.JSONField) and value:\\n        try:\\n            return json.dumps(value, ensure_ascii=False, cls=field.encoder)\\n        except TypeError:\\n            return display_for_value(value, empty_value_display)\\n    else:\\n        return display_for_value(value, empty_value_display)"
  },
  {
    "code": "def __init__(self, files=None, inplace=False, backup=\"\", bufsize=0,\\n                 mode=\"r\", openhook=None):\\n        if isinstance(files, str):\\n            files = (files,)\\n        elif isinstance(files, os.PathLike):\\n            files = (os.fspath(files), )\\n        else:\\n            if files is None:\\n                files = sys.argv[1:]\\n            if not files:\\n                files = ('-',)\\n            else:\\n                files = tuple(files)\\n        self._files = files\\n        self._inplace = inplace\\n        self._backup = backup\\n        if bufsize:\\n            import warnings\\n            warnings.warn('bufsize is deprecated and ignored',\\n                          DeprecationWarning, stacklevel=2)\\n        self._savestdout = None\\n        self._output = None\\n        self._filename = None\\n        self._startlineno = 0\\n        self._filelineno = 0\\n        self._file = None\\n        self._isstdin = False\\n        self._backupfilename = None\\n        if mode not in ('r', 'rU', 'U', 'rb'):\\n            raise ValueError(\"FileInput opening mode must be one of \"\\n                             \"'r', 'rU', 'U' and 'rb'\")\\n        if 'U' in mode:\\n            import warnings\\n            warnings.warn(\"'U' mode is deprecated\",\\n                          DeprecationWarning, 2)\\n        self._mode = mode\\n        self._write_mode = mode.replace('r', 'w') if 'U' not in mode else 'w'\\n        if openhook:\\n            if inplace:\\n                raise ValueError(\"FileInput cannot use an opening hook in inplace mode\")\\n            if not callable(openhook):\\n                raise ValueError(\"FileInput openhook must be callable\")\\n        self._openhook = openhook",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-36952: Remove the bufsize parameter in fileinput.input(). (GH-13400)\\n\\nThis parameter is marked as deprecated since 3.6 and for removal in 3.8.\\nIt already had no effects.",
    "fixed_code": "def __init__(self, files=None, inplace=False, backup=\"\", *,\\n                 mode=\"r\", openhook=None):\\n        if isinstance(files, str):\\n            files = (files,)\\n        elif isinstance(files, os.PathLike):\\n            files = (os.fspath(files), )\\n        else:\\n            if files is None:\\n                files = sys.argv[1:]\\n            if not files:\\n                files = ('-',)\\n            else:\\n                files = tuple(files)\\n        self._files = files\\n        self._inplace = inplace\\n        self._backup = backup\\n        self._savestdout = None\\n        self._output = None\\n        self._filename = None\\n        self._startlineno = 0\\n        self._filelineno = 0\\n        self._file = None\\n        self._isstdin = False\\n        self._backupfilename = None\\n        if mode not in ('r', 'rU', 'U', 'rb'):\\n            raise ValueError(\"FileInput opening mode must be one of \"\\n                             \"'r', 'rU', 'U' and 'rb'\")\\n        if 'U' in mode:\\n            import warnings\\n            warnings.warn(\"'U' mode is deprecated\",\\n                          DeprecationWarning, 2)\\n        self._mode = mode\\n        self._write_mode = mode.replace('r', 'w') if 'U' not in mode else 'w'\\n        if openhook:\\n            if inplace:\\n                raise ValueError(\"FileInput cannot use an opening hook in inplace mode\")\\n            if not callable(openhook):\\n                raise ValueError(\"FileInput openhook must be callable\")\\n        self._openhook = openhook"
  },
  {
    "code": "def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\\n        self.stopframe = stopframe\\n        self.returnframe = returnframe\\n        self.quitting = 0\\n        self.stoplineno = stoplineno",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\\n        self.stopframe = stopframe\\n        self.returnframe = returnframe\\n        self.quitting = 0\\n        self.stoplineno = stoplineno"
  },
  {
    "code": "def _convert_listlike_datetimes(\\n    arg,\\n    box,\\n    format,\\n    name=None,\\n    tz=None,\\n    unit=None,\\n    errors=None,\\n    infer_datetime_format=None,\\n    dayfirst=None,\\n    yearfirst=None,\\n    exact=None,\\n):\\n    from pandas import DatetimeIndex\\n    from pandas.core.arrays import DatetimeArray\\n    from pandas.core.arrays.datetimes import (\\n        maybe_convert_dtype,\\n        objects_to_datetime64ns,\\n    )\\n    if isinstance(arg, (list, tuple)):\\n        arg = np.array(arg, dtype=\"O\")\\n    if is_datetime64tz_dtype(arg):\\n        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            return DatetimeIndex(arg, tz=tz, name=name)\\n        if tz == \"utc\":\\n            arg = arg.tz_convert(None).tz_localize(tz)\\n        return arg\\n    elif is_datetime64_ns_dtype(arg):\\n        if box and not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            try:\\n                return DatetimeIndex(arg, tz=tz, name=name)\\n            except ValueError:\\n                pass\\n        elif tz:\\n            return arg.tz_localize(tz)\\n        return arg\\n    elif unit is not None:\\n        if format is not None:\\n            raise ValueError(\"cannot specify both format and unit\")\\n        arg = getattr(arg, \"values\", arg)\\n        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\\n        if box:\\n            if errors == \"ignore\":\\n                from pandas import Index\\n                result = Index(result, name=name)\\n            else:\\n                result = DatetimeIndex(result, name=name)\\n            try:\\n                result = result.tz_localize(\"UTC\").tz_convert(tz_parsed)\\n            except AttributeError:\\n                return result\\n            if tz is not None:\\n                if result.tz is None:\\n                    result = result.tz_localize(tz)\\n                else:\\n                    result = result.tz_convert(tz)\\n        return result\\n    elif getattr(arg, \"ndim\", 1) > 1:\\n        raise TypeError(\\n            \"arg must be a string, datetime, list, tuple, 1-d array, or Series\"\\n        )\\n    orig_arg = arg\\n    arg, _ = maybe_convert_dtype(arg, copy=False)\\n    arg = ensure_object(arg)\\n    require_iso8601 = False\\n    if infer_datetime_format and format is None:\\n        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)\\n    if format is not None:\\n        format_is_iso8601 = _format_is_iso(format)\\n        if format_is_iso8601:\\n            require_iso8601 = not infer_datetime_format\\n            format = None\\n    tz_parsed = None\\n    result = None\\n    if format is not None:\\n        try:\\n            if format == \"%Y%m%d\":\\n                try:\\n                    orig_arg = ensure_object(orig_arg)\\n                    result = _attempt_YYYYMMDD(orig_arg, errors=errors)\\n                except (ValueError, TypeError, tslibs.OutOfBoundsDatetime):\\n                    raise ValueError(\"cannot convert the input to '%Y%m%d' date format\")\\n            if result is None:\\n                try:\\n                    result, timezones = array_strptime(\\n                        arg, format, exact=exact, errors=errors\\n                    )\\n                    if \"%Z\" in format or \"%z\" in format:\\n                        return _return_parsed_timezone_results(\\n                            result, timezones, box, tz, name\\n                        )\\n                except tslibs.OutOfBoundsDatetime:\\n                    if errors == \"raise\":\\n                        raise\\n                    elif errors == \"coerce\":\\n                        result = np.empty(arg.shape, dtype=\"M8[ns]\")\\n                        iresult = result.view(\"i8\")\\n                        iresult.fill(tslibs.iNaT)\\n                    else:\\n                        result = arg\\n                except ValueError:\\n                    if not infer_datetime_format:\\n                        if errors == \"raise\":\\n                            raise\\n                        elif errors == \"coerce\":\\n                            result = np.empty(arg.shape, dtype=\"M8[ns]\")\\n                            iresult = result.view(\"i8\")\\n                            iresult.fill(tslibs.iNaT)\\n                        else:\\n                            result = arg\\n        except ValueError as e:\\n            try:\\n                values, tz = conversion.datetime_to_datetime64(arg)\\n                return DatetimeIndex._simple_new(values, name=name, tz=tz)\\n            except (ValueError, TypeError):\\n                raise e\\n    if result is None:\\n        assert format is None or infer_datetime_format\\n        utc = tz == \"utc\"\\n        result, tz_parsed = objects_to_datetime64ns(\\n            arg,\\n            dayfirst=dayfirst,\\n            yearfirst=yearfirst,\\n            utc=utc,\\n            errors=errors,\\n            require_iso8601=require_iso8601,\\n            allow_object=True,\\n        )\\n    if tz_parsed is not None:\\n        if box:\\n            return DatetimeIndex._simple_new(result, name=name, tz=tz_parsed)\\n        else:\\n            result = [Timestamp(ts, tz=tz_parsed).to_pydatetime() for ts in result]\\n            return np.array(result, dtype=object)\\n    if box:\\n        utc = tz == \"utc\"\\n        return _box_as_indexlike(result, utc=utc, name=name)\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: box arg in to_datetime (#30111)",
    "fixed_code": "def _convert_listlike_datetimes(\\n    arg,\\n    format,\\n    name=None,\\n    tz=None,\\n    unit=None,\\n    errors=None,\\n    infer_datetime_format=None,\\n    dayfirst=None,\\n    yearfirst=None,\\n    exact=None,\\n):\\n    from pandas import DatetimeIndex\\n    from pandas.core.arrays import DatetimeArray\\n    from pandas.core.arrays.datetimes import (\\n        maybe_convert_dtype,\\n        objects_to_datetime64ns,\\n    )\\n    if isinstance(arg, (list, tuple)):\\n        arg = np.array(arg, dtype=\"O\")\\n    if is_datetime64tz_dtype(arg):\\n        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            return DatetimeIndex(arg, tz=tz, name=name)\\n        if tz == \"utc\":\\n            arg = arg.tz_convert(None).tz_localize(tz)\\n        return arg\\n    elif is_datetime64_ns_dtype(arg):\\n        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            try:\\n                return DatetimeIndex(arg, tz=tz, name=name)\\n            except ValueError:\\n                pass\\n        elif tz:\\n            return arg.tz_localize(tz)\\n        return arg\\n    elif unit is not None:\\n        if format is not None:\\n            raise ValueError(\"cannot specify both format and unit\")\\n        arg = getattr(arg, \"values\", arg)\\n        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\\n        if errors == \"ignore\":\\n            from pandas import Index\\n            result = Index(result, name=name)\\n        else:\\n            result = DatetimeIndex(result, name=name)\\n        try:\\n            result = result.tz_localize(\"UTC\").tz_convert(tz_parsed)\\n        except AttributeError:\\n            return result\\n        if tz is not None:\\n            if result.tz is None:\\n                result = result.tz_localize(tz)\\n            else:\\n                result = result.tz_convert(tz)\\n        return result\\n    elif getattr(arg, \"ndim\", 1) > 1:\\n        raise TypeError(\\n            \"arg must be a string, datetime, list, tuple, 1-d array, or Series\"\\n        )\\n    orig_arg = arg\\n    arg, _ = maybe_convert_dtype(arg, copy=False)\\n    arg = ensure_object(arg)\\n    require_iso8601 = False\\n    if infer_datetime_format and format is None:\\n        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)\\n    if format is not None:\\n        format_is_iso8601 = _format_is_iso(format)\\n        if format_is_iso8601:\\n            require_iso8601 = not infer_datetime_format\\n            format = None\\n    tz_parsed = None\\n    result = None\\n    if format is not None:\\n        try:\\n            if format == \"%Y%m%d\":\\n                try:\\n                    orig_arg = ensure_object(orig_arg)\\n                    result = _attempt_YYYYMMDD(orig_arg, errors=errors)\\n                except (ValueError, TypeError, tslibs.OutOfBoundsDatetime):\\n                    raise ValueError(\"cannot convert the input to '%Y%m%d' date format\")\\n            if result is None:\\n                try:\\n                    result, timezones = array_strptime(\\n                        arg, format, exact=exact, errors=errors\\n                    )\\n                    if \"%Z\" in format or \"%z\" in format:\\n                        return _return_parsed_timezone_results(\\n                            result, timezones, tz, name\\n                        )\\n                except tslibs.OutOfBoundsDatetime:\\n                    if errors == \"raise\":\\n                        raise\\n                    elif errors == \"coerce\":\\n                        result = np.empty(arg.shape, dtype=\"M8[ns]\")\\n                        iresult = result.view(\"i8\")\\n                        iresult.fill(tslibs.iNaT)\\n                    else:\\n                        result = arg\\n                except ValueError:\\n                    if not infer_datetime_format:\\n                        if errors == \"raise\":\\n                            raise\\n                        elif errors == \"coerce\":\\n                            result = np.empty(arg.shape, dtype=\"M8[ns]\")\\n                            iresult = result.view(\"i8\")\\n                            iresult.fill(tslibs.iNaT)\\n                        else:\\n                            result = arg\\n        except ValueError as e:\\n            try:\\n                values, tz = conversion.datetime_to_datetime64(arg)\\n                return DatetimeIndex._simple_new(values, name=name, tz=tz)\\n            except (ValueError, TypeError):\\n                raise e\\n    if result is None:\\n        assert format is None or infer_datetime_format\\n        utc = tz == \"utc\"\\n        result, tz_parsed = objects_to_datetime64ns(\\n            arg,\\n            dayfirst=dayfirst,\\n            yearfirst=yearfirst,\\n            utc=utc,\\n            errors=errors,\\n            require_iso8601=require_iso8601,\\n            allow_object=True,\\n        )\\n    if tz_parsed is not None:\\n        return DatetimeIndex._simple_new(result, name=name, tz=tz_parsed)\\n    utc = tz == \"utc\"\\n    return _box_as_indexlike(result, utc=utc, name=name)"
  },
  {
    "code": "def write_file(self, new_text, filename, old_text, encoding):\\n        orig_filename = filename\\n        if self._output_dir:\\n            if filename.startswith(self._input_base_dir):\\n                filename = os.path.join(self._output_dir,\\n                                        filename[len(self._input_base_dir):])\\n            else:\\n                raise ValueError('filename %s does not start with the '\\n                                 'input_base_dir %s' % (\\n                                         filename, self._input_base_dir))\\n        if self._append_suffix:\\n            filename += self._append_suffix\\n        if orig_filename != filename:\\n          output_dir = os.path.dirname(filename)\\n          if not os.path.isdir(output_dir):\\n              os.makedirs(output_dir)\\n          self.log_message('Writing converted %s to %s.', orig_filename,\\n                           filename)\\n        if not self.nobackups:\\n            backup = filename + \".bak\"\\n            if os.path.lexists(backup):\\n                try:\\n                    os.remove(backup)\\n                except os.error, err:\\n                    self.log_message(\"Can't remove backup %s\", backup)\\n            try:\\n                os.rename(filename, backup)\\n            except os.error, err:\\n                self.log_message(\"Can't rename %s to %s\", filename, backup)\\n        write = super(StdoutRefactoringTool, self).write_file\\n        write(new_text, filename, old_text, encoding)\\n        if not self.nobackups:\\n            shutil.copymode(backup, filename)\\n        if orig_filename != filename:\\n            shutil.copymode(orig_filename, filename)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def write_file(self, new_text, filename, old_text, encoding):\\n        orig_filename = filename\\n        if self._output_dir:\\n            if filename.startswith(self._input_base_dir):\\n                filename = os.path.join(self._output_dir,\\n                                        filename[len(self._input_base_dir):])\\n            else:\\n                raise ValueError('filename %s does not start with the '\\n                                 'input_base_dir %s' % (\\n                                         filename, self._input_base_dir))\\n        if self._append_suffix:\\n            filename += self._append_suffix\\n        if orig_filename != filename:\\n          output_dir = os.path.dirname(filename)\\n          if not os.path.isdir(output_dir):\\n              os.makedirs(output_dir)\\n          self.log_message('Writing converted %s to %s.', orig_filename,\\n                           filename)\\n        if not self.nobackups:\\n            backup = filename + \".bak\"\\n            if os.path.lexists(backup):\\n                try:\\n                    os.remove(backup)\\n                except os.error, err:\\n                    self.log_message(\"Can't remove backup %s\", backup)\\n            try:\\n                os.rename(filename, backup)\\n            except os.error, err:\\n                self.log_message(\"Can't rename %s to %s\", filename, backup)\\n        write = super(StdoutRefactoringTool, self).write_file\\n        write(new_text, filename, old_text, encoding)\\n        if not self.nobackups:\\n            shutil.copymode(backup, filename)\\n        if orig_filename != filename:\\n            shutil.copymode(orig_filename, filename)"
  },
  {
    "code": "def _args_from_interpreter_flags():\\n    flag_opt_map = {\\n        'debug': 'd',\\n        'dont_write_bytecode': 'B',\\n        'no_user_site': 's',\\n        'no_site': 'S',\\n        'ignore_environment': 'E',\\n        'verbose': 'v',\\n        'bytes_warning': 'b',\\n        'quiet': 'q',\\n    }\\n    args = _optim_args_from_interpreter_flags()\\n    for flag, opt in flag_opt_map.items():\\n        v = getattr(sys.flags, flag)\\n        if v > 0:\\n            args.append('-' + opt * v)\\n    warnopts = sys.warnoptions[:]\\n    bytes_warning = sys.flags.bytes_warning\\n    xoptions = getattr(sys, '_xoptions', {})\\n    dev_mode = ('dev' in xoptions)\\n    if bytes_warning > 1:\\n        warnopts.remove(\"error::BytesWarning\")\\n    elif bytes_warning:\\n        warnopts.remove(\"default::BytesWarning\")\\n    if dev_mode:\\n        warnopts.remove('default')\\n    for opt in warnopts:\\n        args.append('-W' + opt)\\n    if dev_mode:\\n        args.extend(('-X', 'dev'))\\n    for opt in ('faulthandler', 'tracemalloc', 'importtime',\\n                'showalloccount', 'showrefcount'):\\n        if opt in xoptions:\\n            value = xoptions[opt]\\n            if value is True:\\n                arg = opt\\n            else:\\n                arg = '%s=%s' % (opt, value)\\n            args.extend(('-X', arg))\\n    return args",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _args_from_interpreter_flags():\\n    flag_opt_map = {\\n        'debug': 'd',\\n        'dont_write_bytecode': 'B',\\n        'no_user_site': 's',\\n        'no_site': 'S',\\n        'ignore_environment': 'E',\\n        'verbose': 'v',\\n        'bytes_warning': 'b',\\n        'quiet': 'q',\\n    }\\n    args = _optim_args_from_interpreter_flags()\\n    for flag, opt in flag_opt_map.items():\\n        v = getattr(sys.flags, flag)\\n        if v > 0:\\n            args.append('-' + opt * v)\\n    warnopts = sys.warnoptions[:]\\n    bytes_warning = sys.flags.bytes_warning\\n    xoptions = getattr(sys, '_xoptions', {})\\n    dev_mode = ('dev' in xoptions)\\n    if bytes_warning > 1:\\n        warnopts.remove(\"error::BytesWarning\")\\n    elif bytes_warning:\\n        warnopts.remove(\"default::BytesWarning\")\\n    if dev_mode:\\n        warnopts.remove('default')\\n    for opt in warnopts:\\n        args.append('-W' + opt)\\n    if dev_mode:\\n        args.extend(('-X', 'dev'))\\n    for opt in ('faulthandler', 'tracemalloc', 'importtime',\\n                'showalloccount', 'showrefcount'):\\n        if opt in xoptions:\\n            value = xoptions[opt]\\n            if value is True:\\n                arg = opt\\n            else:\\n                arg = '%s=%s' % (opt, value)\\n            args.extend(('-X', arg))\\n    return args"
  },
  {
    "code": "def as_float_array(X, overwrite_X=False):\\n    if X.dtype in [np.float32, np.float64]:\\n        if overwrite_X:\\n            return X\\n        else:\\n            return X.copy()\\n    if X.dtype == np.int32:\\n        X = X.astype(np.float32)\\n    else:\\n        X = X.astype(np.float64)\\n    return X",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def as_float_array(X, overwrite_X=False):\\n    if X.dtype in [np.float32, np.float64]:\\n        if overwrite_X:\\n            return X\\n        else:\\n            return X.copy()\\n    if X.dtype == np.int32:\\n        X = X.astype(np.float32)\\n    else:\\n        X = X.astype(np.float64)\\n    return X"
  },
  {
    "code": "def _read_wide(self, group, where=None):\\n        items = _read_index(group, 'items')\\n        major_axis = _read_index(group, 'major_axis')\\n        minor_axis = _read_index(group, 'minor_axis')\\n        values = _read_array(group, 'values')\\n        return WidePanel(values, items, major_axis, minor_axis)\\n    def _read_wide_table(self, group, where=None):\\n        return self._read_panel_table(group, where)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read_wide(self, group, where=None):\\n        items = _read_index(group, 'items')\\n        major_axis = _read_index(group, 'major_axis')\\n        minor_axis = _read_index(group, 'minor_axis')\\n        values = _read_array(group, 'values')\\n        return WidePanel(values, items, major_axis, minor_axis)\\n    def _read_wide_table(self, group, where=None):\\n        return self._read_panel_table(group, where)"
  },
  {
    "code": "def get_existing(module, args):\\n    existing = {}\\n    netcfg = CustomNetworkConfig(indent=2, contents=get_config(module))\\n    parents = ['router ospf {0}'.format(module.params['ospf'])]\\n    if module.params['vrf'] != 'default':\\n        parents.append('vrf {0}'.format(module.params['vrf']))\\n    config = netcfg.get_section(parents)\\n    for arg in args:\\n        if arg not in ['ospf', 'vrf']:\\n            existing[arg] = PARAM_TO_DEFAULT_KEYMAP.get(arg)\\n    if config:\\n        if module.params['vrf'] == 'default':\\n            splitted_config = config.splitlines()\\n            vrf_index = False\\n            for index in range(0, len(splitted_config) - 1):\\n                if 'vrf' in splitted_config[index].strip():\\n                    vrf_index = index\\n                    break\\n            if vrf_index:\\n                config = '\\n'.join(splitted_config[0:vrf_index])\\n        splitted_config = config.splitlines()\\n        for line in splitted_config:\\n            if 'passive' in line:\\n                existing['passive_interface'] = True\\n            elif 'router-id' in line:\\n                existing['router_id'] = re.search(r'router-id (\\S+)', line).group(1)\\n            elif 'metric' in line:\\n                existing['default_metric'] = re.search(r'default-metric (\\S+)', line).group(1)\\n            elif 'adjacency' in line:\\n                log = re.search(r'log-adjacency-changes(?: (\\S+))?', line).group(1)\\n                if log:\\n                    existing['log_adjacency'] = log\\n                else:\\n                    existing['log_adjacency'] = 'log'\\n            elif 'auto' in line:\\n                cost = re.search(r'auto-cost reference-bandwidth (\\d+) (\\S+)', line).group(1)\\n                if 'Gbps' in line:\\n                    cost = int(cost) * 1000\\n                existing['auto_cost'] = str(cost)\\n            elif 'bfd' in line:\\n                existing['bfd'] = 'enable'\\n            elif 'timers throttle lsa' in line:\\n                tmp = re.search(r'timers throttle lsa (\\S+) (\\S+) (\\S+)', line)\\n                existing['timer_throttle_lsa_start'] = tmp.group(1)\\n                existing['timer_throttle_lsa_hold'] = tmp.group(2)\\n                existing['timer_throttle_lsa_max'] = tmp.group(3)\\n            elif 'timers throttle spf' in line:\\n                tmp = re.search(r'timers throttle spf (\\S+) (\\S+) (\\S+)', line)\\n                existing['timer_throttle_spf_start'] = tmp.group(1)\\n                existing['timer_throttle_spf_hold'] = tmp.group(2)\\n                existing['timer_throttle_spf_max'] = tmp.group(3)\\n        existing['vrf'] = module.params['vrf']\\n        existing['ospf'] = module.params['ospf']\\n    return existing",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_existing(module, args):\\n    existing = {}\\n    netcfg = CustomNetworkConfig(indent=2, contents=get_config(module))\\n    parents = ['router ospf {0}'.format(module.params['ospf'])]\\n    if module.params['vrf'] != 'default':\\n        parents.append('vrf {0}'.format(module.params['vrf']))\\n    config = netcfg.get_section(parents)\\n    for arg in args:\\n        if arg not in ['ospf', 'vrf']:\\n            existing[arg] = PARAM_TO_DEFAULT_KEYMAP.get(arg)\\n    if config:\\n        if module.params['vrf'] == 'default':\\n            splitted_config = config.splitlines()\\n            vrf_index = False\\n            for index in range(0, len(splitted_config) - 1):\\n                if 'vrf' in splitted_config[index].strip():\\n                    vrf_index = index\\n                    break\\n            if vrf_index:\\n                config = '\\n'.join(splitted_config[0:vrf_index])\\n        splitted_config = config.splitlines()\\n        for line in splitted_config:\\n            if 'passive' in line:\\n                existing['passive_interface'] = True\\n            elif 'router-id' in line:\\n                existing['router_id'] = re.search(r'router-id (\\S+)', line).group(1)\\n            elif 'metric' in line:\\n                existing['default_metric'] = re.search(r'default-metric (\\S+)', line).group(1)\\n            elif 'adjacency' in line:\\n                log = re.search(r'log-adjacency-changes(?: (\\S+))?', line).group(1)\\n                if log:\\n                    existing['log_adjacency'] = log\\n                else:\\n                    existing['log_adjacency'] = 'log'\\n            elif 'auto' in line:\\n                cost = re.search(r'auto-cost reference-bandwidth (\\d+) (\\S+)', line).group(1)\\n                if 'Gbps' in line:\\n                    cost = int(cost) * 1000\\n                existing['auto_cost'] = str(cost)\\n            elif 'bfd' in line:\\n                existing['bfd'] = 'enable'\\n            elif 'timers throttle lsa' in line:\\n                tmp = re.search(r'timers throttle lsa (\\S+) (\\S+) (\\S+)', line)\\n                existing['timer_throttle_lsa_start'] = tmp.group(1)\\n                existing['timer_throttle_lsa_hold'] = tmp.group(2)\\n                existing['timer_throttle_lsa_max'] = tmp.group(3)\\n            elif 'timers throttle spf' in line:\\n                tmp = re.search(r'timers throttle spf (\\S+) (\\S+) (\\S+)', line)\\n                existing['timer_throttle_spf_start'] = tmp.group(1)\\n                existing['timer_throttle_spf_hold'] = tmp.group(2)\\n                existing['timer_throttle_spf_max'] = tmp.group(3)\\n        existing['vrf'] = module.params['vrf']\\n        existing['ospf'] = module.params['ospf']\\n    return existing"
  },
  {
    "code": "def _asof_function(on_type):\\n    return getattr(_join, 'asof_join_%s' % on_type, None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Added 'direction' parameter to merge_asof() (#14887)\\n\\ncloses #14887\\n\\nAuthor: Christopher C. Aycock <christopher.aycock@twosigma.com>\\n\\nCloses #15129 from chrisaycock/GH14887 and squashes the following commits:\\n\\nda38483 [Christopher C. Aycock] Description of direction parameters is now a bullet-point list\\n879c9f0 [Christopher C. Aycock] Tweak prose to include statement on version and parameters\\nce5caaa [Christopher C. Aycock] Split Cython functions according to direction\\n50431ad [Christopher C. Aycock] ENH: Added 'direction' parameter to merge_asof() (#14887)",
    "fixed_code": "def _asof_function(direction, on_type):\\n    return getattr(_join, 'asof_join_%s_%s' % (direction, on_type), None)"
  },
  {
    "code": "def __init__(self, estimator, param_grid, loss_func=None,\\n                        fit_params={}, n_jobs=1):\\n        assert hasattr(estimator, 'fit') and hasattr(estimator, 'predict'), (\\n            \"estimator should a be an estimator implementing 'fit' and \"\\n            \"'predict' methods, %s (type %s) was passed\" % (clf, type(clf))\\n            )\\n        if loss_func is None:\\n            assert hasattr(estimator, 'score'), ValueError(\\n                    \"If no loss_func is specified, the estimator passed \"\\n                    \"should have a 'score' method. The estimator %s \"\\n                    \"does not.\" % estimator\\n                    )\\n        self.estimator = estimator\\n        self.param_grid = param_grid\\n        self.loss_func = loss_func\\n        self.n_jobs = n_jobs\\n        self.fit_params = fit_params",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, estimator, param_grid, loss_func=None,\\n                        fit_params={}, n_jobs=1):\\n        assert hasattr(estimator, 'fit') and hasattr(estimator, 'predict'), (\\n            \"estimator should a be an estimator implementing 'fit' and \"\\n            \"'predict' methods, %s (type %s) was passed\" % (clf, type(clf))\\n            )\\n        if loss_func is None:\\n            assert hasattr(estimator, 'score'), ValueError(\\n                    \"If no loss_func is specified, the estimator passed \"\\n                    \"should have a 'score' method. The estimator %s \"\\n                    \"does not.\" % estimator\\n                    )\\n        self.estimator = estimator\\n        self.param_grid = param_grid\\n        self.loss_func = loss_func\\n        self.n_jobs = n_jobs\\n        self.fit_params = fit_params"
  },
  {
    "code": "def pad_1d(values, limit=None):\\n    if is_float_dtype(values):\\n        _method = _algos.pad_inplace_float64\\n    elif is_datetime64_dtype(values):\\n        _method = _pad_1d_datetime\\n    elif values.dtype == np.object_:\\n        _method = _algos.pad_inplace_object\\n    else: \\n        raise ValueError('Invalid dtype for padding')\\n    _method(values, isnull(values).view(np.uint8), limit=limit)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Series.replace #929",
    "fixed_code": "def pad_1d(values, limit=None, mask=None):\\n    if is_float_dtype(values):\\n        _method = _algos.pad_inplace_float64\\n    elif is_datetime64_dtype(values):\\n        _method = _pad_1d_datetime\\n    elif values.dtype == np.object_:\\n        _method = _algos.pad_inplace_object\\n    else: \\n        raise ValueError('Invalid dtype for padding')\\n    if mask is None:\\n        mask = isnull(values)\\n    mask = mask.view(np.uint8)\\n    _method(values, mask, limit=limit)"
  },
  {
    "code": "def is_excluded_dtype(dtype) -> bool:\\n\\t\\tis_excluded_checks = (is_period_dtype, is_interval_dtype)\\n\\t\\treturn any(is_excluded(dtype) for is_excluded in is_excluded_checks)\\n\\treturn _is_dtype(arr_or_dtype, condition)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes is_string_dtype (#34294)",
    "fixed_code": "def is_excluded_dtype(dtype) -> bool:\\n\\t\\tis_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)\\n\\t\\treturn any(is_excluded(dtype) for is_excluded in is_excluded_checks)\\n\\treturn _is_dtype(arr_or_dtype, condition)"
  },
  {
    "code": "def _formatparam(param, value=None, quote=True):\\n    if value is not None and len(value) > 0:\\n        if isinstance(value, tuple):\\n            param += '*'\\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\\n            return '%s=%s' % (param, value)\\n        else:\\n            try:\\n                value.encode('ascii')\\n            except UnicodeEncodeError:\\n                param += '*'\\n                value = utils.encode_rfc2231(value, 'utf-8', '')\\n                return '%s=%s' % (param, value)\\n        if quote or tspecials.search(value):\\n            return '%s=\"%s\"' % (param, utils.quote(value))\\n        else:\\n            return '%s=%s' % (param, value)\\n    else:\\n        return param",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _formatparam(param, value=None, quote=True):\\n    if value is not None and len(value) > 0:\\n        if isinstance(value, tuple):\\n            param += '*'\\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\\n            return '%s=%s' % (param, value)\\n        else:\\n            try:\\n                value.encode('ascii')\\n            except UnicodeEncodeError:\\n                param += '*'\\n                value = utils.encode_rfc2231(value, 'utf-8', '')\\n                return '%s=%s' % (param, value)\\n        if quote or tspecials.search(value):\\n            return '%s=\"%s\"' % (param, utils.quote(value))\\n        else:\\n            return '%s=%s' % (param, value)\\n    else:\\n        return param"
  },
  {
    "code": "def _get_client(self, namenode: str, port: int, login: str, schema: str, extra_dejson: dict) -> Any:\\n        connection_str = f'http://{namenode}'\\n        session = requests.Session()\\n        if extra_dejson.get('use_ssl', 'False') == 'True' or extra_dejson.get('use_ssl', False):\\n            connection_str = f'https://{namenode}'\\n            session.verify = extra_dejson.get('verify', False)\\n        if port is not None:\\n            connection_str += f':{port}'\\n        if schema is not None:\\n            connection_str += f'/{schema}'\\n        if _kerberos_security_mode:\\n            return KerberosClient(connection_str, session=session)\\n        proxy_user = self.proxy_user or login\\n        return InsecureClient(connection_str, user=proxy_user, session=session)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_client(self, namenode: str, port: int, login: str, schema: str, extra_dejson: dict) -> Any:\\n        connection_str = f'http://{namenode}'\\n        session = requests.Session()\\n        if extra_dejson.get('use_ssl', 'False') == 'True' or extra_dejson.get('use_ssl', False):\\n            connection_str = f'https://{namenode}'\\n            session.verify = extra_dejson.get('verify', False)\\n        if port is not None:\\n            connection_str += f':{port}'\\n        if schema is not None:\\n            connection_str += f'/{schema}'\\n        if _kerberos_security_mode:\\n            return KerberosClient(connection_str, session=session)\\n        proxy_user = self.proxy_user or login\\n        return InsecureClient(connection_str, user=proxy_user, session=session)"
  },
  {
    "code": "def map_obj_to_commands(updates, module):\\n    commands = list()\\n    want, have = updates\\n    state = module.params['state']\\n    if state == 'absent' and have.get('text'):\\n        if isinstance(have['text'], str):\\n            commands.append('no banner %s' % module.params['banner'])\\n        elif have['text'].get('loginBanner') or have['text'].get('motd'):\\n            commands.append({'cmd': 'no banner %s' % module.params['banner']})\\n    elif state == 'present':\\n        if isinstance(have['text'], str):\\n            if want['text'] != have['text']:\\n                commands.append('banner %s' % module.params['banner'])\\n                commands.extend(want['text'].strip().split('\\n'))\\n                commands.append('EOF')\\n        else:\\n            have_text = have['text'].get('loginBanner') or have['text'].get('motd')\\n            if have_text:\\n                have_text = have_text.strip()\\n            if to_text(want['text']) != have_text or not have_text:\\n                commands.append({'cmd': 'banner %s' % module.params['banner'],\\n                                 'input': want['text'].strip('\\n')})",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes #30281 eos_banner integration test failure (#30283)",
    "fixed_code": "def map_obj_to_commands(updates, module):\\n    commands = list()\\n    want, have = updates\\n    state = module.params['state']\\n    if state == 'absent' and have.get('text'):\\n        if isinstance(have['text'], str):\\n            commands.append('no banner %s' % module.params['banner'])\\n        elif have['text'].get('loginBanner') or have['text'].get('motd'):\\n            commands.append({'cmd': 'no banner %s' % module.params['banner']})\\n    elif state == 'present':\\n        if isinstance(have['text'], string_types):\\n            if want['text'] != have['text']:\\n                commands.append('banner %s' % module.params['banner'])\\n                commands.extend(want['text'].strip().split('\\n'))\\n                commands.append('EOF')\\n        else:\\n            have_text = have['text'].get('loginBanner') or have['text'].get('motd')\\n            if have_text:\\n                have_text = have_text.strip()\\n            if to_text(want['text']) != have_text or not have_text:\\n                commands.append({'cmd': 'banner %s' % module.params['banner'],\\n                                 'input': want['text'].strip('\\n')})\\n    return commands"
  },
  {
    "code": "def __invert__(self):\\n        if self.optional:\\n            if self.optional==1:\\n                return Pattern(self.label + '...', self.pattern[:-1] + '*', optional=2,flags=self._flags)\\n            if self.optional==2:\\n                return Pattern('%s %s' % (self.label[1:-4].strip(), self.label), self.pattern[:-1] + '+',\\n                               optional=3, flags=self._flags)\\n            return self\\n        label = '[ %s ]' % (self.label)\\n        pattern = '(%s)?' % (self.pattern)\\n        return Pattern(label, pattern, optional=1, flags=self._flags)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __invert__(self):\\n        if self.optional:\\n            if self.optional==1:\\n                return Pattern(self.label + '...', self.pattern[:-1] + '*', optional=2,flags=self._flags)\\n            if self.optional==2:\\n                return Pattern('%s %s' % (self.label[1:-4].strip(), self.label), self.pattern[:-1] + '+',\\n                               optional=3, flags=self._flags)\\n            return self\\n        label = '[ %s ]' % (self.label)\\n        pattern = '(%s)?' % (self.pattern)\\n        return Pattern(label, pattern, optional=1, flags=self._flags)"
  },
  {
    "code": "def _iter_command_classes(module_name):\\n\\tfor module in walk_modules(module_name):\\n\\t\\tfor obj in vars(module).values():\\n\\t\\t\\tif inspect.isclass(obj) and \\\\n\\t\\t\\t\\t\\tissubclass(obj, ScrapyCommand) and \\\\n\\t\\t\\t\\t\\tobj.__module__ == module.__name__:\\n\\t\\t\\t\\tyield obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _iter_command_classes(module_name):\\n\\tfor module in walk_modules(module_name):\\n\\t\\tfor obj in vars(module).values():\\n\\t\\t\\tif inspect.isclass(obj) and \\\\n\\t\\t\\t\\t\\tissubclass(obj, ScrapyCommand) and \\\\n\\t\\t\\t\\t\\tobj.__module__ == module.__name__:\\n\\t\\t\\t\\tyield obj"
  },
  {
    "code": "def template_loaders(self):\\n        return self.get_template_loaders(self.loaders)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def template_loaders(self):\\n        return self.get_template_loaders(self.loaders)"
  },
  {
    "code": "def to_html(\\n        self,\\n        buf: FilePath | WriteBuffer[str] | None = None,\\n        *,\\n        table_uuid: str | None = None,\\n        table_attributes: str | None = None,\\n        sparse_index: bool | None = None,\\n        sparse_columns: bool | None = None,\\n        bold_headers: bool = False,\\n        caption: str | None = None,\\n        max_rows: int | None = None,\\n        max_columns: int | None = None,\\n        encoding: str | None = None,\\n        doctype_html: bool = False,\\n        exclude_styles: bool = False,\\n        **kwargs,\\n    ):\\n        obj = self._copy(deepcopy=True)  \\n        if table_uuid:\\n            obj.set_uuid(table_uuid)\\n        if table_attributes:\\n            obj.set_table_attributes(table_attributes)\\n        if sparse_index is None:\\n            sparse_index = get_option(\"styler.sparse.index\")\\n        if sparse_columns is None:\\n            sparse_columns = get_option(\"styler.sparse.columns\")\\n        if bold_headers:\\n            obj.set_table_styles(\\n                [{\"selector\": \"th\", \"props\": \"font-weight: bold;\"}], overwrite=False\\n            )\\n        if caption is not None:\\n            obj.set_caption(caption)\\n        html = obj._render_html(\\n            sparse_index=sparse_index,\\n            sparse_columns=sparse_columns,\\n            max_rows=max_rows,\\n            max_cols=max_columns,\\n            exclude_styles=exclude_styles,\\n            encoding=encoding or get_option(\"styler.render.encoding\"),\\n            doctype_html=doctype_html,\\n            **kwargs,\\n        )\\n        return save_to_buffer(\\n            html, buf=buf, encoding=(encoding if buf is not None else None)\\n        )\\n    @Substitution(buf=buf, encoding=encoding)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_html(\\n        self,\\n        buf: FilePath | WriteBuffer[str] | None = None,\\n        *,\\n        table_uuid: str | None = None,\\n        table_attributes: str | None = None,\\n        sparse_index: bool | None = None,\\n        sparse_columns: bool | None = None,\\n        bold_headers: bool = False,\\n        caption: str | None = None,\\n        max_rows: int | None = None,\\n        max_columns: int | None = None,\\n        encoding: str | None = None,\\n        doctype_html: bool = False,\\n        exclude_styles: bool = False,\\n        **kwargs,\\n    ):\\n        obj = self._copy(deepcopy=True)  \\n        if table_uuid:\\n            obj.set_uuid(table_uuid)\\n        if table_attributes:\\n            obj.set_table_attributes(table_attributes)\\n        if sparse_index is None:\\n            sparse_index = get_option(\"styler.sparse.index\")\\n        if sparse_columns is None:\\n            sparse_columns = get_option(\"styler.sparse.columns\")\\n        if bold_headers:\\n            obj.set_table_styles(\\n                [{\"selector\": \"th\", \"props\": \"font-weight: bold;\"}], overwrite=False\\n            )\\n        if caption is not None:\\n            obj.set_caption(caption)\\n        html = obj._render_html(\\n            sparse_index=sparse_index,\\n            sparse_columns=sparse_columns,\\n            max_rows=max_rows,\\n            max_cols=max_columns,\\n            exclude_styles=exclude_styles,\\n            encoding=encoding or get_option(\"styler.render.encoding\"),\\n            doctype_html=doctype_html,\\n            **kwargs,\\n        )\\n        return save_to_buffer(\\n            html, buf=buf, encoding=(encoding if buf is not None else None)\\n        )\\n    @Substitution(buf=buf, encoding=encoding)"
  },
  {
    "code": "def run(self, terms, variables, **kwargs):\\n        ret = []\\n        for term in terms:\\n            relpath, params = _parse_parameters(term, kwargs)\\n            path = self._loader.path_dwim(relpath)\\n            b_path = to_bytes(path, errors='surrogate_or_strict')\\n            chars = _gen_candidate_chars(params['chars'])\\n            changed = None\\n            first_process, lockfile = _get_lock(b_path)\\n            content = _read_password_file(b_path)\\n            if content is None or b_path == to_bytes('/dev/null'):\\n                plaintext_password = random_password(params['length'], chars, params['seed'])\\n                salt = None\\n                changed = True\\n            else:\\n                plaintext_password, salt = _parse_content(content)\\n            encrypt = params['encrypt']\\n            if encrypt and not salt:\\n                changed = True\\n                try:\\n                    salt = random_salt(BaseHash.algorithms[encrypt].salt_size)\\n                except KeyError:\\n                    salt = random_salt()\\n            ident = params['ident']\\n            if encrypt and not ident:\\n                changed = True\\n                try:\\n                    ident = BaseHash.algorithms[encrypt].implicit_ident\\n                except KeyError:\\n                    ident = None\\n            if changed and b_path != to_bytes('/dev/null'):\\n                content = _format_content(plaintext_password, salt, encrypt=encrypt, ident=ident)\\n                _write_password_file(b_path, content)\\n            if first_process:\\n                _release_lock(lockfile)\\n            if encrypt:\\n                password = do_encrypt(plaintext_password, encrypt, salt=salt, ident=ident)\\n                ret.append(password)\\n            else:\\n                ret.append(plaintext_password)\\n        return ret",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix password lookup's  use of f=v settings (#76551)\\n\\nupdate tests",
    "fixed_code": "def run(self, terms, variables, **kwargs):\\n        ret = []\\n        self.set_options(var_options=variables, direct=kwargs)\\n        for term in terms:\\n            relpath, params = self._parse_parameters(term)\\n            path = self._loader.path_dwim(relpath)\\n            b_path = to_bytes(path, errors='surrogate_or_strict')\\n            chars = _gen_candidate_chars(params['chars'])\\n            changed = None\\n            first_process, lockfile = _get_lock(b_path)\\n            content = _read_password_file(b_path)\\n            if content is None or b_path == to_bytes('/dev/null'):\\n                plaintext_password = random_password(params['length'], chars, params['seed'])\\n                salt = None\\n                changed = True\\n            else:\\n                plaintext_password, salt = _parse_content(content)\\n            encrypt = params['encrypt']\\n            if encrypt and not salt:\\n                changed = True\\n                try:\\n                    salt = random_salt(BaseHash.algorithms[encrypt].salt_size)\\n                except KeyError:\\n                    salt = random_salt()\\n            ident = params['ident']\\n            if encrypt and not ident:\\n                changed = True\\n                try:\\n                    ident = BaseHash.algorithms[encrypt].implicit_ident\\n                except KeyError:\\n                    ident = None\\n            if changed and b_path != to_bytes('/dev/null'):\\n                content = _format_content(plaintext_password, salt, encrypt=encrypt, ident=ident)\\n                _write_password_file(b_path, content)\\n            if first_process:\\n                _release_lock(lockfile)\\n            if encrypt:\\n                password = do_encrypt(plaintext_password, encrypt, salt=salt, ident=ident)\\n                ret.append(password)\\n            else:\\n                ret.append(plaintext_password)\\n        return ret"
  },
  {
    "code": "def _logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\\n                              max_iter=100, tol=1e-4, verbose=0,\\n                              solver='lbfgs', coef=None,\\n                              class_weight=None, dual=False, penalty='l2',\\n                              intercept_scaling=1., multi_class='warn',\\n                              random_state=None, check_input=True,\\n                              max_squared_sum=None, sample_weight=None,\\n                              l1_ratio=None):\\n    if isinstance(Cs, numbers.Integral):\\n        Cs = np.logspace(-4, 4, Cs)\\n    solver = _check_solver(solver, penalty, dual)\\n    if check_input:\\n        X = check_array(X, accept_sparse='csr', dtype=np.float64,\\n                        accept_large_sparse=solver != 'liblinear')\\n        y = check_array(y, ensure_2d=False, dtype=None)\\n        check_consistent_length(X, y)\\n    _, n_features = X.shape\\n    classes = np.unique(y)\\n    random_state = check_random_state(random_state)\\n    multi_class = _check_multi_class(multi_class, solver, len(classes))\\n    if pos_class is None and multi_class != 'multinomial':\\n        if (classes.size > 2):\\n            raise ValueError('To fit OvR, use the pos_class argument')\\n        pos_class = classes[1]\\n    if sample_weight is not None:\\n        sample_weight = np.array(sample_weight, dtype=X.dtype, order='C')\\n        check_consistent_length(y, sample_weight)\\n    else:\\n        sample_weight = np.ones(X.shape[0], dtype=X.dtype)\\n    le = LabelEncoder()\\n    if isinstance(class_weight, dict) or multi_class == 'multinomial':\\n        class_weight_ = compute_class_weight(class_weight, classes, y)\\n        sample_weight *= class_weight_[le.fit_transform(y)]\\n    if multi_class == 'ovr':\\n        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)\\n        mask_classes = np.array([-1, 1])\\n        mask = (y == pos_class)\\n        y_bin = np.ones(y.shape, dtype=X.dtype)\\n        y_bin[~mask] = -1.\\n        if class_weight == \"balanced\":\\n            class_weight_ = compute_class_weight(class_weight, mask_classes,\\n                                                 y_bin)\\n            sample_weight *= class_weight_[le.fit_transform(y_bin)]\\n    else:\\n        if solver not in ['sag', 'saga']:\\n            lbin = LabelBinarizer()\\n            Y_multi = lbin.fit_transform(y)\\n            if Y_multi.shape[1] == 1:\\n                Y_multi = np.hstack([1 - Y_multi, Y_multi])\\n        else:\\n            le = LabelEncoder()\\n            Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)\\n        w0 = np.zeros((classes.size, n_features + int(fit_intercept)),\\n                      order='F', dtype=X.dtype)\\n    if coef is not None:\\n        if multi_class == 'ovr':\\n            if coef.size not in (n_features, w0.size):\\n                raise ValueError(\\n                    'Initialization coef is of shape %d, expected shape '\\n                    '%d or %d' % (coef.size, n_features, w0.size))\\n            w0[:coef.size] = coef\\n        else:\\n            n_classes = classes.size\\n            if n_classes == 2:\\n                n_classes = 1\\n            if (coef.shape[0] != n_classes or\\n                    coef.shape[1] not in (n_features, n_features + 1)):\\n                raise ValueError(\\n                    'Initialization coef is of shape (%d, %d), expected '\\n                    'shape (%d, %d) or (%d, %d)' % (\\n                        coef.shape[0], coef.shape[1], classes.size,\\n                        n_features, classes.size, n_features + 1))\\n            if n_classes == 1:\\n                w0[0, :coef.shape[1]] = -coef\\n                w0[1, :coef.shape[1]] = coef\\n            else:\\n                w0[:, :coef.shape[1]] = coef\\n    if multi_class == 'multinomial':\\n        if solver in ['lbfgs', 'newton-cg']:\\n            w0 = w0.ravel()\\n        target = Y_multi\\n        if solver == 'lbfgs':\\n            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]\\n        elif solver == 'newton-cg':\\n            func = lambda x, *args: _multinomial_loss(x, *args)[0]\\n            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]\\n            hess = _multinomial_grad_hess\\n        warm_start_sag = {'coef': w0.T}\\n    else:\\n        target = y_bin\\n        if solver == 'lbfgs':\\n            func = _logistic_loss_and_grad\\n        elif solver == 'newton-cg':\\n            func = _logistic_loss\\n            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]\\n            hess = _logistic_grad_hess\\n        warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}\\n    coefs = list()\\n    n_iter = np.zeros(len(Cs), dtype=np.int32)\\n    for i, C in enumerate(Cs):\\n        if solver == 'lbfgs':\\n            iprint = [-1, 50, 1, 100, 101][\\n                np.searchsorted(np.array([0, 1, 2, 3]), verbose)]\\n            w0, loss, info = optimize.fmin_l_bfgs_b(\\n                func, w0, fprime=None,\\n                args=(X, target, 1. / C, sample_weight),\\n                iprint=iprint, pgtol=tol, maxiter=max_iter)\\n            if info[\"warnflag\"] == 1:\\n                warnings.warn(\"lbfgs failed to converge. Increase the number \"\\n                              \"of iterations.\", ConvergenceWarning)\\n            n_iter_i = min(info['nit'], max_iter)\\n        elif solver == 'newton-cg':\\n            args = (X, target, 1. / C, sample_weight)\\n            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,\\n                                     maxiter=max_iter, tol=tol)\\n        elif solver == 'liblinear':\\n            coef_, intercept_, n_iter_i, = _fit_liblinear(\\n                X, target, C, fit_intercept, intercept_scaling, None,\\n                penalty, dual, verbose, max_iter, tol, random_state,\\n                sample_weight=sample_weight)\\n            if fit_intercept:\\n                w0 = np.concatenate([coef_.ravel(), intercept_])\\n            else:\\n                w0 = coef_.ravel()\\n        elif solver in ['sag', 'saga']:\\n            if multi_class == 'multinomial':\\n                target = target.astype(X.dtype, copy=False)\\n                loss = 'multinomial'\\n            else:\\n                loss = 'log'\\n            if penalty == 'l1':\\n                alpha = 0.\\n                beta = 1. / C\\n            elif penalty == 'l2':\\n                alpha = 1. / C\\n                beta = 0.\\n            else:  \\n                alpha = (1. / C) * (1 - l1_ratio)\\n                beta = (1. / C) * l1_ratio\\n            w0, n_iter_i, warm_start_sag = sag_solver(\\n                X, target, sample_weight, loss, alpha,\\n                beta, max_iter, tol,\\n                verbose, random_state, False, max_squared_sum, warm_start_sag,\\n                is_saga=(solver == 'saga'))\\n        else:\\n            raise ValueError(\"solver must be one of {'liblinear', 'lbfgs', \"\\n                             \"'newton-cg', 'sag'}, got '%s' instead\" % solver)\\n        if multi_class == 'multinomial':\\n            n_classes = max(2, classes.size)\\n            multi_w0 = np.reshape(w0, (n_classes, -1))\\n            if n_classes == 2:\\n                multi_w0 = multi_w0[1][np.newaxis, :]\\n            coefs.append(multi_w0.copy())\\n        else:\\n            coefs.append(w0.copy())\\n        n_iter[i] = n_iter_i\\n    return np.array(coefs), np.array(Cs), n_iter",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEP change the default of multi_class to 'auto' in logistic regression (#13807)",
    "fixed_code": "def _logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\\n                              max_iter=100, tol=1e-4, verbose=0,\\n                              solver='lbfgs', coef=None,\\n                              class_weight=None, dual=False, penalty='l2',\\n                              intercept_scaling=1., multi_class='auto',\\n                              random_state=None, check_input=True,\\n                              max_squared_sum=None, sample_weight=None,\\n                              l1_ratio=None):\\n    if isinstance(Cs, numbers.Integral):\\n        Cs = np.logspace(-4, 4, Cs)\\n    solver = _check_solver(solver, penalty, dual)\\n    if check_input:\\n        X = check_array(X, accept_sparse='csr', dtype=np.float64,\\n                        accept_large_sparse=solver != 'liblinear')\\n        y = check_array(y, ensure_2d=False, dtype=None)\\n        check_consistent_length(X, y)\\n    _, n_features = X.shape\\n    classes = np.unique(y)\\n    random_state = check_random_state(random_state)\\n    multi_class = _check_multi_class(multi_class, solver, len(classes))\\n    if pos_class is None and multi_class != 'multinomial':\\n        if (classes.size > 2):\\n            raise ValueError('To fit OvR, use the pos_class argument')\\n        pos_class = classes[1]\\n    if sample_weight is not None:\\n        sample_weight = np.array(sample_weight, dtype=X.dtype, order='C')\\n        check_consistent_length(y, sample_weight)\\n    else:\\n        sample_weight = np.ones(X.shape[0], dtype=X.dtype)\\n    le = LabelEncoder()\\n    if isinstance(class_weight, dict) or multi_class == 'multinomial':\\n        class_weight_ = compute_class_weight(class_weight, classes, y)\\n        sample_weight *= class_weight_[le.fit_transform(y)]\\n    if multi_class == 'ovr':\\n        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)\\n        mask_classes = np.array([-1, 1])\\n        mask = (y == pos_class)\\n        y_bin = np.ones(y.shape, dtype=X.dtype)\\n        y_bin[~mask] = -1.\\n        if class_weight == \"balanced\":\\n            class_weight_ = compute_class_weight(class_weight, mask_classes,\\n                                                 y_bin)\\n            sample_weight *= class_weight_[le.fit_transform(y_bin)]\\n    else:\\n        if solver not in ['sag', 'saga']:\\n            lbin = LabelBinarizer()\\n            Y_multi = lbin.fit_transform(y)\\n            if Y_multi.shape[1] == 1:\\n                Y_multi = np.hstack([1 - Y_multi, Y_multi])\\n        else:\\n            le = LabelEncoder()\\n            Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)\\n        w0 = np.zeros((classes.size, n_features + int(fit_intercept)),\\n                      order='F', dtype=X.dtype)\\n    if coef is not None:\\n        if multi_class == 'ovr':\\n            if coef.size not in (n_features, w0.size):\\n                raise ValueError(\\n                    'Initialization coef is of shape %d, expected shape '\\n                    '%d or %d' % (coef.size, n_features, w0.size))\\n            w0[:coef.size] = coef\\n        else:\\n            n_classes = classes.size\\n            if n_classes == 2:\\n                n_classes = 1\\n            if (coef.shape[0] != n_classes or\\n                    coef.shape[1] not in (n_features, n_features + 1)):\\n                raise ValueError(\\n                    'Initialization coef is of shape (%d, %d), expected '\\n                    'shape (%d, %d) or (%d, %d)' % (\\n                        coef.shape[0], coef.shape[1], classes.size,\\n                        n_features, classes.size, n_features + 1))\\n            if n_classes == 1:\\n                w0[0, :coef.shape[1]] = -coef\\n                w0[1, :coef.shape[1]] = coef\\n            else:\\n                w0[:, :coef.shape[1]] = coef\\n    if multi_class == 'multinomial':\\n        if solver in ['lbfgs', 'newton-cg']:\\n            w0 = w0.ravel()\\n        target = Y_multi\\n        if solver == 'lbfgs':\\n            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]\\n        elif solver == 'newton-cg':\\n            func = lambda x, *args: _multinomial_loss(x, *args)[0]\\n            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]\\n            hess = _multinomial_grad_hess\\n        warm_start_sag = {'coef': w0.T}\\n    else:\\n        target = y_bin\\n        if solver == 'lbfgs':\\n            func = _logistic_loss_and_grad\\n        elif solver == 'newton-cg':\\n            func = _logistic_loss\\n            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]\\n            hess = _logistic_grad_hess\\n        warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}\\n    coefs = list()\\n    n_iter = np.zeros(len(Cs), dtype=np.int32)\\n    for i, C in enumerate(Cs):\\n        if solver == 'lbfgs':\\n            iprint = [-1, 50, 1, 100, 101][\\n                np.searchsorted(np.array([0, 1, 2, 3]), verbose)]\\n            w0, loss, info = optimize.fmin_l_bfgs_b(\\n                func, w0, fprime=None,\\n                args=(X, target, 1. / C, sample_weight),\\n                iprint=iprint, pgtol=tol, maxiter=max_iter)\\n            if info[\"warnflag\"] == 1:\\n                warnings.warn(\"lbfgs failed to converge. Increase the number \"\\n                              \"of iterations.\", ConvergenceWarning)\\n            n_iter_i = min(info['nit'], max_iter)\\n        elif solver == 'newton-cg':\\n            args = (X, target, 1. / C, sample_weight)\\n            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,\\n                                     maxiter=max_iter, tol=tol)\\n        elif solver == 'liblinear':\\n            coef_, intercept_, n_iter_i, = _fit_liblinear(\\n                X, target, C, fit_intercept, intercept_scaling, None,\\n                penalty, dual, verbose, max_iter, tol, random_state,\\n                sample_weight=sample_weight)\\n            if fit_intercept:\\n                w0 = np.concatenate([coef_.ravel(), intercept_])\\n            else:\\n                w0 = coef_.ravel()\\n        elif solver in ['sag', 'saga']:\\n            if multi_class == 'multinomial':\\n                target = target.astype(X.dtype, copy=False)\\n                loss = 'multinomial'\\n            else:\\n                loss = 'log'\\n            if penalty == 'l1':\\n                alpha = 0.\\n                beta = 1. / C\\n            elif penalty == 'l2':\\n                alpha = 1. / C\\n                beta = 0.\\n            else:  \\n                alpha = (1. / C) * (1 - l1_ratio)\\n                beta = (1. / C) * l1_ratio\\n            w0, n_iter_i, warm_start_sag = sag_solver(\\n                X, target, sample_weight, loss, alpha,\\n                beta, max_iter, tol,\\n                verbose, random_state, False, max_squared_sum, warm_start_sag,\\n                is_saga=(solver == 'saga'))\\n        else:\\n            raise ValueError(\"solver must be one of {'liblinear', 'lbfgs', \"\\n                             \"'newton-cg', 'sag'}, got '%s' instead\" % solver)\\n        if multi_class == 'multinomial':\\n            n_classes = max(2, classes.size)\\n            multi_w0 = np.reshape(w0, (n_classes, -1))\\n            if n_classes == 2:\\n                multi_w0 = multi_w0[1][np.newaxis, :]\\n            coefs.append(multi_w0.copy())\\n        else:\\n            coefs.append(w0.copy())\\n        n_iter[i] = n_iter_i\\n    return np.array(coefs), np.array(Cs), n_iter"
  },
  {
    "code": "def _create_indexer(cls, name, indexer):\\n        if getattr(cls, name, None) is None:\\n            iname = '_%s' % name\\n            setattr(cls, iname, None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_indexer(cls, name, indexer):\\n        if getattr(cls, name, None) is None:\\n            iname = '_%s' % name\\n            setattr(cls, iname, None)"
  },
  {
    "code": "def safe_substitute(self, *args, **kws):\\n        if len(args) > 1:\\n            raise TypeError('Too many positional arguments')\\n        if not args:\\n            mapping = kws\\n        elif kws:\\n            mapping = _multimap(kws, args[0])\\n        else:\\n            mapping = args[0]\\n        delimiter = self.delimiter[-1]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def safe_substitute(self, *args, **kws):\\n        if len(args) > 1:\\n            raise TypeError('Too many positional arguments')\\n        if not args:\\n            mapping = kws\\n        elif kws:\\n            mapping = _multimap(kws, args[0])\\n        else:\\n            mapping = args[0]\\n        delimiter = self.delimiter[-1]"
  },
  {
    "code": "def _try_parse_dates(year, month, expiry):\\n        if (month is not None and year is None) or (month is None and year is not None) and expiry is None:\\n            msg = \"You must specify either (`year` and `month`) or `expiry` \" \\\\n                  \"or none of these options for the current month.\"\\n            raise ValueError(msg)\\n        if (year is not None or month is not None) and expiry is None:\\n            warnings.warn(\"month, year arguments are deprecated, use expiry\"\\n                          \" instead\", FutureWarning)\\n        if expiry is not None:\\n            year = expiry.year\\n            month = expiry.month\\n        elif year is None and month is None:\\n            year = CUR_YEAR\\n            month = CUR_MONTH\\n            expiry = dt.date(year, month, 1)\\n        else:\\n            expiry = dt.date(year, month, 1)\\n        return year, month, expiry",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix pandas.io.data.Options for change in format of Yahoo Option page\\n\\nENH: Automatically choose next expiry if expiry date isn't valid\\n\\nCOMPAT: Remove dictionary comprehension to pass 2.6 test\\n\\nBUG: Add check that tables were downloaded.\\n\\nENH: Replace third_saturday function with Pandas offset\\n\\nBUG: Check to make sure enough option tables are downloaded.\\n\\nTST: Add sample options page during market open.\\n\\nDOC: Add bug fix report to v0.15.1.txt for data.Options\\n\\nBUG: Ensure that the value used for chopping is within the range of strikes.\\n\\nTST: Add test for requesting out of range chop\\n\\nBUG: Fix missing underlying price and quote time in first process data\\n\\nBUG: Fix AM/PM on quote time\\n\\nENH: Refactor to expose available expiry dates\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nBUG: Undo accidental deletion of privat emethods\\n\\nENH: Add ability to use list as expiry parameter.\\n\\nAlso has year & month return all data for the specified year and month.\\n\\nTST: Remove tests for warnings that are no longer in use.\\n\\nDOC: Update docstrings of Options class.",
    "fixed_code": "def _try_parse_dates(self, year, month, expiry):\\n        if (month is not None and year is None) or (month is None and year is not None) and expiry is None:\\n            msg = \"You must specify either (`year` and `month`) or `expiry` \" \\\\n                  \"or none of these options for the next expiry.\"\\n            raise ValueError(msg)\\n        if expiry is not None:\\n            if hasattr(expiry, '__iter__'):\\n                expiry = [self._validate_expiry(exp) for exp in expiry]\\n            else:\\n                expiry = [self._validate_expiry(expiry)]\\n            if len(expiry) == 0:\\n                raise ValueError('No expiries available for given input.')\\n        elif year is None and month is None:\\n            year = CUR_YEAR\\n            month = CUR_MONTH\\n            expiry = dt.date(year, month, 1)\\n            expiry = [self._validate_expiry(expiry)]\\n        else:\\n            expiry = [expiry for expiry in self.expiry_dates if expiry.year == year and expiry.month == month]\\n            if len(expiry) == 0:\\n                raise ValueError('No expiries available in %s-%s' % (year, month))\\n        return expiry"
  },
  {
    "code": "def check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train,\\n                                            X_test, y_test, weights):\\n    with ignore_warnings(category=DeprecationWarning):\\n        classifier = Classifier()\\n    if hasattr(classifier, \"n_iter\"):\\n        classifier.set_params(n_iter=100)\\n    set_random_state(classifier)\\n    classifier.fit(X_train, y_train)\\n    y_pred = classifier.predict(X_test)\\n    classifier.set_params(class_weight='balanced')\\n    classifier.fit(X_train, y_train)\\n    y_pred_balanced = classifier.predict(X_test)\\n    assert_greater(f1_score(y_test, y_pred_balanced, average='weighted'),\\n                   f1_score(y_test, y_pred, average='weighted'))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train,\\n                                            X_test, y_test, weights):\\n    with ignore_warnings(category=DeprecationWarning):\\n        classifier = Classifier()\\n    if hasattr(classifier, \"n_iter\"):\\n        classifier.set_params(n_iter=100)\\n    set_random_state(classifier)\\n    classifier.fit(X_train, y_train)\\n    y_pred = classifier.predict(X_test)\\n    classifier.set_params(class_weight='balanced')\\n    classifier.fit(X_train, y_train)\\n    y_pred_balanced = classifier.predict(X_test)\\n    assert_greater(f1_score(y_test, y_pred_balanced, average='weighted'),\\n                   f1_score(y_test, y_pred, average='weighted'))"
  },
  {
    "code": "def apply(self):\\n        ''\\n        changed = False\\n        adapter_detail = self.get_adapter()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def apply(self):\\n        ''\\n        changed = False\\n        adapter_detail = self.get_adapter()"
  },
  {
    "code": "def invalid_shape_exception(csize, xsize):\\n\\t\\t\\treturn ValueError(\\n\\t\\t\\t\\tf\"'c' argument has {csize} elements, which is inconsistent \"\\n\\t\\t\\t\\tf\"with 'x' and 'y' with size {xsize}.\")\\n\\t\\tc_is_mapped = False  \\n\\t\\tvalid_shape = True  \\n\\t\\tif not c_was_none and kwcolor is None and not c_is_string_or_strings:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tc = np.asanyarray(c, dtype=float)\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass  \\n\\t\\t\\telse:\\n\\t\\t\\t\\tif c.size == xsize:\\n\\t\\t\\t\\t\\tc = c.ravel()\\n\\t\\t\\t\\t\\tc_is_mapped = True\\n\\t\\t\\t\\telse:  \\n\\t\\t\\t\\t\\tif c.shape in ((3,), (4,)):\\n\\t\\t\\t\\t\\t\\t_log.warning(\\n\\t\\t\\t\\t\\t\\t\\t\"'c' argument looks like a single numeric RGB or \"\\n\\t\\t\\t\\t\\t\\t\\t\"RGBA sequence, which should be avoided as value-\"\\n\\t\\t\\t\\t\\t\\t\\t\"mapping will have precedence in case its length \"\\n\\t\\t\\t\\t\\t\\t\\t\"matches with 'x' & 'y'.  Please use a 2-D array \"\\n\\t\\t\\t\\t\\t\\t\\t\"with a single row if you really want to specify \"\\n\\t\\t\\t\\t\\t\\t\\t\"the same RGB or RGBA value for all points.\")\\n\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\tif not c_is_mapped:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tcolors = mcolors.to_rgba_array(c)\\n\\t\\t\\texcept (TypeError, ValueError) as err:\\n\\t\\t\\t\\tif \"RGBA values should be within 0-1 range\" in str(err):\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not valid_shape:\\n\\t\\t\\t\\t\\t\\traise invalid_shape_exception(c.size, xsize) from err\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\tf\"'c' argument must be a color, a sequence of colors, \"\\n\\t\\t\\t\\t\\t\\tf\"or a sequence of numbers, not {c}\") from err\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif len(colors) not in (0, 1, xsize):\\n\\t\\t\\t\\t\\traise invalid_shape_exception(len(colors), xsize)\\n\\t\\telse:\\n\\t\\t\\tcolors = None  \\n\\t\\treturn c, colors, edgecolors\\n\\t@_preprocess_data(replace_names=[\"x\", \"y\", \"s\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"edgecolors\", \"c\", \"facecolor\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"facecolors\", \"color\"],\\n\\t\\t\\t\\t\\t  label_namer=\"y\")\\n\\t@cbook._delete_parameter(\"3.2\", \"verts\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: fix support for single color 2D array as c to mean single color\\n\\nCloses #17423",
    "fixed_code": "def invalid_shape_exception(csize, xsize):\\n\\t\\t\\treturn ValueError(\\n\\t\\t\\t\\tf\"'c' argument has {csize} elements, which is inconsistent \"\\n\\t\\t\\t\\tf\"with 'x' and 'y' with size {xsize}.\")\\n\\t\\tc_is_mapped = False  \\n\\t\\tvalid_shape = True  \\n\\t\\tif not c_was_none and kwcolor is None and not c_is_string_or_strings:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tc = np.asanyarray(c, dtype=float)\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass  \\n\\t\\t\\telse:\\n\\t\\t\\t\\tif c.shape == (1, 4) or c.shape == (1, 3):\\n\\t\\t\\t\\t\\tc_is_mapped = False\\n\\t\\t\\t\\t\\tif c.size != xsize:\\n\\t\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\t\\t\\telif c.size == xsize:\\n\\t\\t\\t\\t\\tc = c.ravel()\\n\\t\\t\\t\\t\\tc_is_mapped = True\\n\\t\\t\\t\\telse:  \\n\\t\\t\\t\\t\\tif c.shape in ((3,), (4,)):\\n\\t\\t\\t\\t\\t\\t_log.warning(\\n\\t\\t\\t\\t\\t\\t\\t\"'c' argument looks like a single numeric RGB or \"\\n\\t\\t\\t\\t\\t\\t\\t\"RGBA sequence, which should be avoided as value-\"\\n\\t\\t\\t\\t\\t\\t\\t\"mapping will have precedence in case its length \"\\n\\t\\t\\t\\t\\t\\t\\t\"matches with 'x' & 'y'.  Please use a 2-D array \"\\n\\t\\t\\t\\t\\t\\t\\t\"with a single row if you really want to specify \"\\n\\t\\t\\t\\t\\t\\t\\t\"the same RGB or RGBA value for all points.\")\\n\\t\\t\\t\\t\\tvalid_shape = False\\n\\t\\tif not c_is_mapped:\\n\\t\\t\\ttry:  \\n\\t\\t\\t\\tcolors = mcolors.to_rgba_array(c)\\n\\t\\t\\texcept (TypeError, ValueError) as err:\\n\\t\\t\\t\\tif \"RGBA values should be within 0-1 range\" in str(err):\\n\\t\\t\\t\\t\\traise\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif not valid_shape:\\n\\t\\t\\t\\t\\t\\traise invalid_shape_exception(c.size, xsize) from err\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\tf\"'c' argument must be a color, a sequence of colors, \"\\n\\t\\t\\t\\t\\t\\tf\"or a sequence of numbers, not {c}\") from err\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif len(colors) not in (0, 1, xsize):\\n\\t\\t\\t\\t\\traise invalid_shape_exception(len(colors), xsize)\\n\\t\\telse:\\n\\t\\t\\tcolors = None  \\n\\t\\treturn c, colors, edgecolors\\n\\t@_preprocess_data(replace_names=[\"x\", \"y\", \"s\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"edgecolors\", \"c\", \"facecolor\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"facecolors\", \"color\"],\\n\\t\\t\\t\\t\\t  label_namer=\"y\")\\n\\t@cbook._delete_parameter(\"3.2\", \"verts\")"
  },
  {
    "code": "def iteritems(self):\\n        if not self.db:\\n            return\\n        self._kill_iteration = False\\n        self._in_iter += 1\\n        try:\\n            try:\\n                cur = self._make_iter_cursor()\\n                kv = _DeadlockWrap(cur.first)\\n                key = kv[0]\\n                yield kv\\n                next = cur.next\\n                while 1:\\n                    try:\\n                        kv = _DeadlockWrap(next)\\n                        key = kv[0]\\n                        yield kv\\n                    except _bsddb.DBCursorClosedError:\\n                        if self._kill_iteration:\\n                            raise RuntimeError('Database changed size '\\n                                               'during iteration.')\\n                        cur = self._make_iter_cursor()\\n                        _DeadlockWrap(cur.set, key,0,0,0)\\n                        next = cur.next\\n            except _bsddb.DBNotFoundError:\\n                pass\\n            except _bsddb.DBCursorClosedError:\\n                pass\\n        finally:\\n            self._in_iter -= 1",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bsddb module updated to version 4.7.2devel9.\\n\\nThis patch publishes the work done until now\\nfor Python 3.0 compatibility. Still a lot\\nto be done.\\n\\nWhen possible, we use 3.0 features in Python 2.6,\\neasing development and testing, and exposing internal\\nchanges to a wider audience, for better test coverage.\\n\\nSome mode details:",
    "fixed_code": "def iteritems(self):\\n        if not self.db:\\n            return\\n        self._kill_iteration = False\\n        self._in_iter += 1\\n        try:\\n            try:\\n                cur = self._make_iter_cursor()\\n                kv = _DeadlockWrap(cur.first)\\n                key = kv[0]\\n                yield kv\\n                next = cur.next\\n                while 1:\\n                    try:\\n                        kv = _DeadlockWrap(next)\\n                        key = kv[0]\\n                        yield kv\\n                    except _bsddb.DBCursorClosedError:\\n                        if self._kill_iteration:\\n                            raise RuntimeError('Database changed size '\\n                                               'during iteration.')\\n                        cur = self._make_iter_cursor()\\n                        _DeadlockWrap(cur.set, key,0,0,0)\\n                        next = cur.next\\n            except _bsddb.DBNotFoundError:\\n                pass\\n            except _bsddb.DBCursorClosedError:\\n                pass\\n        except :\\n            self._in_iter -= 1\\n            raise\\n        self._in_iter -= 1"
  },
  {
    "code": "def time_extract(self, dtype):\\n        with warnings.catch_warnings(record=True):\\n            self.s.str.extract(\"(\\\\w*)A(\\\\w*)\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_extract(self, dtype):\\n        with warnings.catch_warnings(record=True):\\n            self.s.str.extract(\"(\\\\w*)A(\\\\w*)\")"
  },
  {
    "code": "def clear_dag_task_instances():\\n    session = settings.Session()\\n    TI = TaskInstance\\n    tis = (\\n        session\\n        .query(TI)\\n        .filter(TI.dag_id.in_(DAG_IDS))\\n        .all()\\n    )\\n    for ti in tis:\\n        logging.info('Deleting TaskInstance :: %s', ti)\\n        session.delete(ti)\\n    session.commit()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def clear_dag_task_instances():\\n    session = settings.Session()\\n    TI = TaskInstance\\n    tis = (\\n        session\\n        .query(TI)\\n        .filter(TI.dag_id.in_(DAG_IDS))\\n        .all()\\n    )\\n    for ti in tis:\\n        logging.info('Deleting TaskInstance :: %s', ti)\\n        session.delete(ti)\\n    session.commit()"
  },
  {
    "code": "def state_absent(module, existing, proposed, candidate):\\n    commands = []\\n    parents = ['interface {0}'.format(module.params['interface'].capitalize())]\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in existing_commands.items():\\n        if value:\\n            if key.startswith('ip ospf message-digest-key'):\\n                if 'options' not in key:\\n                    if existing['message_digest_encryption_type'] == '3des':\\n                        encryption_type = '3'\\n                    elif existing['message_digest_encryption_type'] == 'cisco_type_7':\\n                        encryption_type = '7'\\n                    command = 'no {0} {1} {2} {3} {4}'.format(\\n                        key,\\n                        existing['message_digest_key_id'],\\n                        existing['message_digest_algorithm_type'],\\n                        encryption_type,\\n                        existing['message_digest_password'])\\n                    commands.append(command)\\n            elif key in ['ip ospf authentication message-digest',\\n                         'ip ospf passive-interface', 'ip ospf network']:\\n                if value:\\n                    commands.append('no {0}'.format(key))\\n            elif key == 'ip router ospf':\\n                command = 'no {0} {1} area {2}'.format(key, proposed['ospf'], proposed['area'])\\n                if command not in commands:\\n                    commands.append(command)\\n            else:\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n    candidate.add(commands, parents=parents)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_interfaces_ospf: fix passive-interface states & check_mode (#54260)\\n\\n\\nThis fix addresses issues #41704 and #45343.\\n\\nThe crux of the problem is that `passive-interface` should have been treated as a tri-state value instead of a boolean.\\n\\nThe `no` form of the command disables the passive state on an interface (allows it to form adjacencies and send routing updates).  It's essentially an override for `passive-interface default` which enables passive state on all OSPF interfaces.\\*\\nThis `no` config will be present in `running-config`.\\n\\n   \\**See `router ospf` configuration.*\\n\\nSince both enable and disable states are explicit configs, the proper way to remove either of these is with the `default` syntax.\\n\\nPassive-interface config syntax:\\n```\\n  ip ospf passive-interface              # enable  (nvgens)\\n  no ip ospf passive-interface           # disable (nvgens)\\n  default ip ospf passive-interface      # default (removes config, does not nvgen)\\n```\\n\\nCode changes:\\n\\n\\n\\n\\nSanity verified on: N9K,N7K,N3K,N6K",
    "fixed_code": "def state_absent(module, existing, proposed, candidate):\\n    commands = []\\n    parents = ['interface {0}'.format(module.params['interface'].capitalize())]\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in existing_commands.items():\\n        if 'ip ospf passive-interface' in key:\\n            commands.append('default ip ospf passive-interface')\\n            continue\\n        if value:\\n            if key.startswith('ip ospf message-digest-key'):\\n                if 'options' not in key:\\n                    if existing['message_digest_encryption_type'] == '3des':\\n                        encryption_type = '3'\\n                    elif existing['message_digest_encryption_type'] == 'cisco_type_7':\\n                        encryption_type = '7'\\n                    command = 'no {0} {1} {2} {3} {4}'.format(\\n                        key,\\n                        existing['message_digest_key_id'],\\n                        existing['message_digest_algorithm_type'],\\n                        encryption_type,\\n                        existing['message_digest_password'])\\n                    commands.append(command)\\n            elif key in ['ip ospf authentication message-digest', 'ip ospf network']:\\n                if value:\\n                    commands.append('no {0}'.format(key))\\n            elif key == 'ip router ospf':\\n                command = 'no {0} {1} area {2}'.format(key, proposed['ospf'], proposed['area'])\\n                if command not in commands:\\n                    commands.append(command)\\n            else:\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n    candidate.add(commands, parents=parents)"
  },
  {
    "code": "def pop(self, item):\\n        result = self[item]\\n        del self[item]\\n        return result\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: refactoring and micro-optimizations to support #437, uncovered panel bug",
    "fixed_code": "def pop(self, item):\\n        return NDFrame.pop(self, item)\\n    @property"
  },
  {
    "code": "def _isinteger(self):\\n        if self._is_special:\\n            return False\\n        if self._exp >= 0:\\n            return True\\n        rest = self._int[self._exp:]\\n        return rest == '0'*len(rest)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _isinteger(self):\\n        if self._is_special:\\n            return False\\n        if self._exp >= 0:\\n            return True\\n        rest = self._int[self._exp:]\\n        return rest == '0'*len(rest)"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            state=dict(type='str', default='present',\\n                       choices=['absent', 'active', 'deleted', 'present', 'restarted', 'started', 'stopped']),\\n            api_key=dict(type='str', no_log=True),\\n            name=dict(type='str'),\\n            alert_bwin_enabled=dict(type='bool'),\\n            alert_bwin_threshold=dict(type='int'),\\n            alert_bwout_enabled=dict(type='bool'),\\n            alert_bwout_threshold=dict(type='int'),\\n            alert_bwquota_enabled=dict(type='bool'),\\n            alert_bwquota_threshold=dict(type='int'),\\n            alert_cpu_enabled=dict(type='bool'),\\n            alert_cpu_threshold=dict(type='int'),\\n            alert_diskio_enabled=dict(type='bool'),\\n            alert_diskio_threshold=dict(type='int'),\\n            backupsenabled=dict(type='int'),\\n            backupweeklyday=dict(type='int'),\\n            backupwindow=dict(type='int'),\\n            displaygroup=dict(type='str', default=''),\\n            plan=dict(type='int'),\\n            additional_disks=dict(type='list'),\\n            distribution=dict(type='int'),\\n            datacenter=dict(type='int'),\\n            kernel_id=dict(type='int'),\\n            linode_id=dict(type='int', aliases=['lid']),\\n            payment_term=dict(type='int', default=1, choices=[1, 12, 24]),\\n            password=dict(type='str', no_log=True),\\n            private_ip=dict(type='bool'),\\n            ssh_pub_key=dict(type='str'),\\n            swap=dict(type='int', default=512),\\n            wait=dict(type='bool', default=True),\\n            wait_timeout=dict(default=300),\\n            watchdog=dict(type='bool', default=True),\\n        ),\\n    )\\n    if not HAS_LINODE:\\n        module.fail_json(msg='linode-python required for this module')\\n    state = module.params.get('state')\\n    api_key = module.params.get('api_key')\\n    name = module.params.get('name')\\n    alert_bwin_enabled = module.params.get('alert_bwin_enabled')\\n    alert_bwin_threshold = module.params.get('alert_bwin_threshold')\\n    alert_bwout_enabled = module.params.get('alert_bwout_enabled')\\n    alert_bwout_threshold = module.params.get('alert_bwout_threshold')\\n    alert_bwquota_enabled = module.params.get('alert_bwquota_enabled')\\n    alert_bwquota_threshold = module.params.get('alert_bwquota_threshold')\\n    alert_cpu_enabled = module.params.get('alert_cpu_enabled')\\n    alert_cpu_threshold = module.params.get('alert_cpu_threshold')\\n    alert_diskio_enabled = module.params.get('alert_diskio_enabled')\\n    alert_diskio_threshold = module.params.get('alert_diskio_threshold')\\n    backupsenabled = module.params.get('backupsenabled')\\n    backupweeklyday = module.params.get('backupweeklyday')\\n    backupwindow = module.params.get('backupwindow')\\n    displaygroup = module.params.get('displaygroup')\\n    plan = module.params.get('plan')\\n    additional_disks = module.params.get('additional_disks')\\n    distribution = module.params.get('distribution')\\n    datacenter = module.params.get('datacenter')\\n    kernel_id = module.params.get('kernel_id')\\n    linode_id = module.params.get('linode_id')\\n    payment_term = module.params.get('payment_term')\\n    password = module.params.get('password')\\n    private_ip = module.params.get('private_ip')\\n    ssh_pub_key = module.params.get('ssh_pub_key')\\n    swap = module.params.get('swap')\\n    wait = module.params.get('wait')\\n    wait_timeout = int(module.params.get('wait_timeout'))\\n    watchdog = int(module.params.get('watchdog'))\\n    kwargs = dict()\\n    check_items = dict(\\n        alert_bwin_enabled=alert_bwin_enabled,\\n        alert_bwin_threshold=alert_bwin_threshold,\\n        alert_bwout_enabled=alert_bwout_enabled,\\n        alert_bwout_threshold=alert_bwout_threshold,\\n        alert_bwquota_enabled=alert_bwquota_enabled,\\n        alert_bwquota_threshold=alert_bwquota_threshold,\\n        alert_cpu_enabled=alert_cpu_enabled,\\n        alert_cpu_threshold=alert_cpu_threshold,\\n        alert_diskio_enabled=alert_diskio_enabled,\\n        alert_diskio_threshold=alert_diskio_threshold,\\n        backupweeklyday=backupweeklyday,\\n        backupwindow=backupwindow,\\n    )\\n    for key, value in check_items.items():\\n        if value is not None:\\n            kwargs[key] = value\\n    if not api_key:\\n        try:\\n            api_key = os.environ['LINODE_API_KEY']\\n        except KeyError as e:\\n            module.fail_json(msg='Unable to load %s' % e.message)\\n    try:\\n        api = linode_api.Api(api_key)\\n        api.test_echo()\\n    except Exception as e:\\n        module.fail_json(msg='%s' % e.value[0]['ERRORMESSAGE'])\\n    linodeServers(module, api, state, name,\\n                  displaygroup, plan,\\n                  additional_disks, distribution, datacenter, kernel_id, linode_id,\\n                  payment_term, password, private_ip, ssh_pub_key, swap, wait,\\n                  wait_timeout, watchdog, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Linode: Mark 'name' as required. Fixes #29785 (#44699)",
    "fixed_code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            state=dict(type='str', default='present',\\n                       choices=['absent', 'active', 'deleted', 'present', 'restarted', 'started', 'stopped']),\\n            api_key=dict(type='str', no_log=True),\\n            name=dict(type='str', required=True),\\n            alert_bwin_enabled=dict(type='bool'),\\n            alert_bwin_threshold=dict(type='int'),\\n            alert_bwout_enabled=dict(type='bool'),\\n            alert_bwout_threshold=dict(type='int'),\\n            alert_bwquota_enabled=dict(type='bool'),\\n            alert_bwquota_threshold=dict(type='int'),\\n            alert_cpu_enabled=dict(type='bool'),\\n            alert_cpu_threshold=dict(type='int'),\\n            alert_diskio_enabled=dict(type='bool'),\\n            alert_diskio_threshold=dict(type='int'),\\n            backupsenabled=dict(type='int'),\\n            backupweeklyday=dict(type='int'),\\n            backupwindow=dict(type='int'),\\n            displaygroup=dict(type='str', default=''),\\n            plan=dict(type='int'),\\n            additional_disks=dict(type='list'),\\n            distribution=dict(type='int'),\\n            datacenter=dict(type='int'),\\n            kernel_id=dict(type='int'),\\n            linode_id=dict(type='int', aliases=['lid']),\\n            payment_term=dict(type='int', default=1, choices=[1, 12, 24]),\\n            password=dict(type='str', no_log=True),\\n            private_ip=dict(type='bool'),\\n            ssh_pub_key=dict(type='str'),\\n            swap=dict(type='int', default=512),\\n            wait=dict(type='bool', default=True),\\n            wait_timeout=dict(default=300),\\n            watchdog=dict(type='bool', default=True),\\n        ),\\n    )\\n    if not HAS_LINODE:\\n        module.fail_json(msg='linode-python required for this module')\\n    state = module.params.get('state')\\n    api_key = module.params.get('api_key')\\n    name = module.params.get('name')\\n    alert_bwin_enabled = module.params.get('alert_bwin_enabled')\\n    alert_bwin_threshold = module.params.get('alert_bwin_threshold')\\n    alert_bwout_enabled = module.params.get('alert_bwout_enabled')\\n    alert_bwout_threshold = module.params.get('alert_bwout_threshold')\\n    alert_bwquota_enabled = module.params.get('alert_bwquota_enabled')\\n    alert_bwquota_threshold = module.params.get('alert_bwquota_threshold')\\n    alert_cpu_enabled = module.params.get('alert_cpu_enabled')\\n    alert_cpu_threshold = module.params.get('alert_cpu_threshold')\\n    alert_diskio_enabled = module.params.get('alert_diskio_enabled')\\n    alert_diskio_threshold = module.params.get('alert_diskio_threshold')\\n    backupsenabled = module.params.get('backupsenabled')\\n    backupweeklyday = module.params.get('backupweeklyday')\\n    backupwindow = module.params.get('backupwindow')\\n    displaygroup = module.params.get('displaygroup')\\n    plan = module.params.get('plan')\\n    additional_disks = module.params.get('additional_disks')\\n    distribution = module.params.get('distribution')\\n    datacenter = module.params.get('datacenter')\\n    kernel_id = module.params.get('kernel_id')\\n    linode_id = module.params.get('linode_id')\\n    payment_term = module.params.get('payment_term')\\n    password = module.params.get('password')\\n    private_ip = module.params.get('private_ip')\\n    ssh_pub_key = module.params.get('ssh_pub_key')\\n    swap = module.params.get('swap')\\n    wait = module.params.get('wait')\\n    wait_timeout = int(module.params.get('wait_timeout'))\\n    watchdog = int(module.params.get('watchdog'))\\n    kwargs = dict()\\n    check_items = dict(\\n        alert_bwin_enabled=alert_bwin_enabled,\\n        alert_bwin_threshold=alert_bwin_threshold,\\n        alert_bwout_enabled=alert_bwout_enabled,\\n        alert_bwout_threshold=alert_bwout_threshold,\\n        alert_bwquota_enabled=alert_bwquota_enabled,\\n        alert_bwquota_threshold=alert_bwquota_threshold,\\n        alert_cpu_enabled=alert_cpu_enabled,\\n        alert_cpu_threshold=alert_cpu_threshold,\\n        alert_diskio_enabled=alert_diskio_enabled,\\n        alert_diskio_threshold=alert_diskio_threshold,\\n        backupweeklyday=backupweeklyday,\\n        backupwindow=backupwindow,\\n    )\\n    for key, value in check_items.items():\\n        if value is not None:\\n            kwargs[key] = value\\n    if not api_key:\\n        try:\\n            api_key = os.environ['LINODE_API_KEY']\\n        except KeyError as e:\\n            module.fail_json(msg='Unable to load %s' % e.message)\\n    try:\\n        api = linode_api.Api(api_key)\\n        api.test_echo()\\n    except Exception as e:\\n        module.fail_json(msg='%s' % e.value[0]['ERRORMESSAGE'])\\n    linodeServers(module, api, state, name,\\n                  displaygroup, plan,\\n                  additional_disks, distribution, datacenter, kernel_id, linode_id,\\n                  payment_term, password, private_ip, ssh_pub_key, swap, wait,\\n                  wait_timeout, watchdog, **kwargs)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        lines=dict(aliases=['commands'], type='list'),\\n        parents=dict(type='list'),\\n        before=dict(type='list'),\\n        after=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'strict', 'exact', 'none']),\\n        replace=dict(default='line', choices=['line', 'block']),\\n        multiline_delimiter=dict(default='@'),\\n        force=dict(default=False, type='bool'),\\n        config=dict(),\\n        defaults=dict(type='bool', default=False),\\n        backup=dict(type='bool', default=False),\\n        save=dict(type='bool', default=False),\\n    )\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('lines', 'src')]\\n    required_if = [('match', 'strict', ['lines']),\\n                   ('match', 'exact', ['lines']),\\n                   ('replace', 'block', ['lines'])]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           required_if=required_if,\\n                           supports_check_mode=True)\\n    if module.params['force'] is True:\\n        module.params['match'] = 'none'\\n    result = {'changed': False}\\n    warnings = list()\\n    check_args(module, warnings)\\n    result['warnings'] = warnings\\n    if any((module.params['lines'], module.params['src'])):\\n        match = module.params['match']\\n        replace = module.params['replace']\\n        path = module.params['parents']\\n        candidate, want_banners = get_candidate(module)\\n        if match != 'none':\\n            config, have_banners = get_running_config(module)\\n            path = module.params['parents']\\n            configobjs = candidate.difference(config, path=path, match=match,\\n                                              replace=replace)\\n        else:\\n            configobjs = candidate.items\\n            have_banners = {}\\n        banners = diff_banners(want_banners, have_banners)\\n        if configobjs or banners:\\n            commands = dumps(configobjs, 'commands').split('\\n')\\n            if module.params['lines']:\\n                if module.params['before']:\\n                    commands[:0] = module.params['before']\\n                if module.params['after']:\\n                    commands.extend(module.params['after'])\\n            result['commands'] = commands\\n            result['banners'] = banners\\n            if not module.check_mode:\\n                if commands:\\n                    load_config(module, commands)\\n                if banners:\\n                    load_banners(module, banners)\\n            result['changed'] = True\\n    if module.params['backup']:\\n        result['__backup__'] = get_config(module=module)\\n    if module.params['save']:\\n        if not module.check_mode:\\n            run_commands(module, ['copy running-config startup-config\\r'])\\n        result['changed'] = True\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        src=dict(type='path'),\\n        lines=dict(aliases=['commands'], type='list'),\\n        parents=dict(type='list'),\\n        before=dict(type='list'),\\n        after=dict(type='list'),\\n        match=dict(default='line', choices=['line', 'strict', 'exact', 'none']),\\n        replace=dict(default='line', choices=['line', 'block']),\\n        multiline_delimiter=dict(default='@'),\\n        force=dict(default=False, type='bool'),\\n        config=dict(),\\n        defaults=dict(type='bool', default=False),\\n        backup=dict(type='bool', default=False),\\n        save=dict(type='bool', default=False),\\n    )\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('lines', 'src')]\\n    required_if = [('match', 'strict', ['lines']),\\n                   ('match', 'exact', ['lines']),\\n                   ('replace', 'block', ['lines'])]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           required_if=required_if,\\n                           supports_check_mode=True)\\n    if module.params['force'] is True:\\n        module.params['match'] = 'none'\\n    result = {'changed': False}\\n    warnings = list()\\n    check_args(module, warnings)\\n    result['warnings'] = warnings\\n    if any((module.params['lines'], module.params['src'])):\\n        match = module.params['match']\\n        replace = module.params['replace']\\n        path = module.params['parents']\\n        candidate, want_banners = get_candidate(module)\\n        if match != 'none':\\n            config, have_banners = get_running_config(module)\\n            path = module.params['parents']\\n            configobjs = candidate.difference(config, path=path, match=match,\\n                                              replace=replace)\\n        else:\\n            configobjs = candidate.items\\n            have_banners = {}\\n        banners = diff_banners(want_banners, have_banners)\\n        if configobjs or banners:\\n            commands = dumps(configobjs, 'commands').split('\\n')\\n            if module.params['lines']:\\n                if module.params['before']:\\n                    commands[:0] = module.params['before']\\n                if module.params['after']:\\n                    commands.extend(module.params['after'])\\n            result['commands'] = commands\\n            result['banners'] = banners\\n            if not module.check_mode:\\n                if commands:\\n                    load_config(module, commands)\\n                if banners:\\n                    load_banners(module, banners)\\n            result['changed'] = True\\n    if module.params['backup']:\\n        result['__backup__'] = get_config(module=module)\\n    if module.params['save']:\\n        if not module.check_mode:\\n            run_commands(module, ['copy running-config startup-config\\r'])\\n        result['changed'] = True\\n    module.exit_json(**result)"
  },
  {
    "code": "def getI(self):\\n        from scipy.basic.linalg import inv\\n        return matrix(inv(self))\\n    A = property(getA, None, doc=\"base array\")\\n    T = property(getT, None, doc=\"transpose\")\\n    H = property(getH, None, doc=\"hermitian (conjugate) transpose\")\\n    I = property(getI, None, doc=\"inverse\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed some typecode initilization warnings.  Made core libraries accessible directly under scipy name space.",
    "fixed_code": "def getI(self):\\n        from scipy.linalg import inv\\n        return matrix(inv(self))\\n    A = property(getA, None, doc=\"base array\")\\n    T = property(getT, None, doc=\"transpose\")\\n    H = property(getH, None, doc=\"hermitian (conjugate) transpose\")\\n    I = property(getI, None, doc=\"inverse\")"
  },
  {
    "code": "def _load_collections(self, attr, ds):\\n        _ensure_default_collection(collection_list=ds)\\n        if not ds:  \\n            return None\\n        env = Environment()\\n        for collection_name in ds:\\n            if is_template(collection_name, env):\\n                display.warning('\"collections\" is not templatable, but we found: %s, '\\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\\n        return ds",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make sure collection is a list if a str is given (#69081)\\n\\n\\n\\nBecause we are doing work on modifying the collections value before\\nit is actually validated, we can validate it ourselves early to make\\nsure the user supplies either a string or list. Dicts are not valid.\\n\\nThe new validation allows us to simplify the _ensure_default_collection()\\nfunction. And since the field is now static, we no longer need to specify\\na default for it, which also allows us to simplify the function. Since\\nthe default is now removed, we can also remove the sanity/ignore.txt entry\\nfor collectionsearch.py.\\n\\nNew unit tests are added (and the existing one modified) that allow us to\\nmake sure that we throw a parser error if a user specifies something other\\nthan a string or list for the collections value everywhere it can be specified.\\n\\n\\nThe default is actually used, so restore it.",
    "fixed_code": "def _load_collections(self, attr, ds):\\n        ds = self.get_validated_value('collections', self._collections, ds, None)\\n        _ensure_default_collection(collection_list=ds)\\n        if not ds:  \\n            return None\\n        env = Environment()\\n        for collection_name in ds:\\n            if is_template(collection_name, env):\\n                display.warning('\"collections\" is not templatable, but we found: %s, '\\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\\n        return ds"
  },
  {
    "code": "def download_request(self, request, spider):\\n        agent = ScrapyAgent(reactor, self._httpclientfactory)\\n        d = agent.launchRequest(request)\\n        d.addCallback(self._agent_callback, request)\\n        d.addErrback(self._agent_errback, request)\\n        return d",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def download_request(self, request, spider):\\n        agent = ScrapyAgent(reactor, self._httpclientfactory)\\n        d = agent.launchRequest(request)\\n        d.addCallback(self._agent_callback, request)\\n        d.addErrback(self._agent_errback, request)\\n        return d"
  },
  {
    "code": "def pformat(object, indent=1, width=80, depth=None):\\n    return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #19132: The pprint module now supports compact mode.",
    "fixed_code": "def pformat(object, indent=1, width=80, depth=None, *, compact=False):\\n    return PrettyPrinter(indent=indent, width=width, depth=depth,\\n                         compact=compact).pformat(object)"
  },
  {
    "code": "def insert(self, loc, item):\\n        if isinstance(item, self._data._recognized_scalars):\\n            item = self._data._scalar_type(item)\\n        elif is_valid_nat_for_dtype(item, self.dtype):\\n            item = self._na_value\\n        elif is_scalar(item) and isna(item):\\n            raise TypeError(\\n                f\"cannot insert {type(self).__name__} with incompatible label\"\\n            )\\n        freq = None\\n        if isinstance(item, self._data._scalar_type) or item is NaT:\\n            self._data._check_compatible_with(item, setitem=True)\\n            if self.size and self.freq is not None:\\n                if item is NaT:\\n                    pass\\n                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\\n                    freq = self.freq\\n                elif (loc == len(self)) and item - self.freq == self[-1]:\\n                    freq = self.freq\\n            item = item.asm8\\n        try:\\n            new_i8s = np.concatenate(\\n                (self[:loc].asi8, [item.view(np.int64)], self[loc:].asi8)\\n            )\\n            arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\\n            return type(self)._simple_new(arr, name=self.name)\\n        except (AttributeError, TypeError):\\n            if isinstance(item, str):\\n                return self.astype(object).insert(loc, item)\\n            raise TypeError(\\n                f\"cannot insert {type(self).__name__} with incompatible label\"\\n            )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def insert(self, loc, item):\\n        if isinstance(item, self._data._recognized_scalars):\\n            item = self._data._scalar_type(item)\\n        elif is_valid_nat_for_dtype(item, self.dtype):\\n            item = self._na_value\\n        elif is_scalar(item) and isna(item):\\n            raise TypeError(\\n                f\"cannot insert {type(self).__name__} with incompatible label\"\\n            )\\n        freq = None\\n        if isinstance(item, self._data._scalar_type) or item is NaT:\\n            self._data._check_compatible_with(item, setitem=True)\\n            if self.size and self.freq is not None:\\n                if item is NaT:\\n                    pass\\n                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\\n                    freq = self.freq\\n                elif (loc == len(self)) and item - self.freq == self[-1]:\\n                    freq = self.freq\\n            item = item.asm8\\n        try:\\n            new_i8s = np.concatenate(\\n                (self[:loc].asi8, [item.view(np.int64)], self[loc:].asi8)\\n            )\\n            arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\\n            return type(self)._simple_new(arr, name=self.name)\\n        except (AttributeError, TypeError):\\n            if isinstance(item, str):\\n                return self.astype(object).insert(loc, item)\\n            raise TypeError(\\n                f\"cannot insert {type(self).__name__} with incompatible label\"\\n            )"
  },
  {
    "code": "def __init__(self, settings):\\n        self._pool = HTTPConnectionPool(reactor, persistent=True)\\n        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\\n        self._pool._factory.noisy = False\\n        self._sslMethod = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\\n        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\\n        try:\\n            self._contextFactory = self._contextFactoryClass(method=self._sslMethod)\\n        except TypeError:\\n            self._contextFactory = self._contextFactoryClass()\\n            msg =  % (\\n                settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\\n            warnings.warn(msg)\\n        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\\n        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\\n        self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')\\n        self._disconnect_timeout = 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, settings):\\n        self._pool = HTTPConnectionPool(reactor, persistent=True)\\n        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\\n        self._pool._factory.noisy = False\\n        self._sslMethod = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\\n        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\\n        try:\\n            self._contextFactory = self._contextFactoryClass(method=self._sslMethod)\\n        except TypeError:\\n            self._contextFactory = self._contextFactoryClass()\\n            msg =  % (\\n                settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\\n            warnings.warn(msg)\\n        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\\n        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\\n        self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')\\n        self._disconnect_timeout = 1"
  },
  {
    "code": "def _sparse_array_op(left, right, op, name):\\n    sparse_op = lambda a, b: _sparse_op(a, b, name)\\n    if left.sp_index.equals(right.sp_index):\\n        result = op(left.sp_values, right.sp_values)\\n        result_index = left.sp_index\\n    else:\\n        result, result_index = sparse_op(left, right)\\n    try:\\n        fill_value = op(left.fill_value, right.fill_value)\\n    except:\\n        fill_value = nan\\n    return SparseArray(result, sparse_index=result_index,\\n                       fill_value=fill_value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH/BUG: Sparse now supports comparison op\\n\\nAuthor: sinhrks <sinhrks@gmail.com>\\n\\nCloses #12971 from sinhrks/sparse_bool_test and squashes the following commits:\\n\\nd57807c [sinhrks] ENH/BUG: Sparse now supports comparison op",
    "fixed_code": "def _sparse_array_op(left, right, op, name):\\n    if left.sp_index.equals(right.sp_index):\\n        result = op(left.sp_values, right.sp_values)\\n        result_index = left.sp_index\\n    else:\\n        sparse_op = getattr(splib, 'sparse_%s' % name)\\n        result, result_index = sparse_op(left.sp_values, left.sp_index,\\n                                         left.fill_value, right.sp_values,\\n                                         right.sp_index, right.fill_value)\\n    try:\\n        fill_value = op(left.fill_value, right.fill_value)\\n    except:\\n        fill_value = nan\\n    return _wrap_result(name, result, result_index, fill_value)"
  },
  {
    "code": "def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):\\n        if not bucket_name:\\n            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)\\n        prefix = re.split(r'[*]', wildcard_key, 1)[0]\\n        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)\\n        if klist:\\n            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]\\n            if key_matches:\\n                return self.get_key(key_matches[0], bucket_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5057] Provide bucket name to functions in S3 Hook when none is specified (#5674)\\n\\nNote: The order of arguments has changed for `check_for_prefix`.\\nThe `bucket_name` is now optional. It falls back to the `connection schema` attribute.\\n- refactor code\\n- complete docs",
    "fixed_code": "def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):\\n        if not bucket_name:\\n            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)\\n        prefix = re.split(r'[*]', wildcard_key, 1)[0]\\n        key_list = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)\\n        if key_list:\\n            key_matches = [k for k in key_list if fnmatch.fnmatch(k, wildcard_key)]\\n            if key_matches:\\n                return self.get_key(key_matches[0], bucket_name)\\n        return None\\n    @provide_bucket_name"
  },
  {
    "code": "def _parse_format_specifier(format_spec):\\n    m = _parse_format_specifier_regex.match(format_spec)\\n    if m is None:\\n        raise ValueError(\"Invalid format specifier: \" + format_spec)\\n    format_dict = m.groupdict()\\n    fill = format_dict['fill']\\n    align = format_dict['align']\\n    if format_dict.pop('zeropad') is not None:\\n        if fill is not None and fill != '0':\\n            raise ValueError(\"Fill character conflicts with '0'\"\\n                             \" in format specifier: \" + format_spec)\\n        if align is not None and align != '=':\\n            raise ValueError(\"Alignment conflicts with '0' in \"\\n                             \"format specifier: \" + format_spec)\\n        fill = '0'\\n        align = '='\\n    format_dict['fill'] = fill or ' '\\n    format_dict['align'] = align or '<'\\n    if format_dict['sign'] is None:\\n        format_dict['sign'] = '-'\\n    format_dict['minimumwidth'] = int(format_dict['minimumwidth'] or '0')\\n    if format_dict['precision'] is not None:\\n        format_dict['precision'] = int(format_dict['precision'])\\n    if format_dict['precision'] == 0:\\n        if format_dict['type'] is None or format_dict['type'] in 'gG':\\n            format_dict['precision'] = 1\\n    format_dict['unicode'] = isinstance(format_spec, unicode)\\n    return format_dict",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_format_specifier(format_spec):\\n    m = _parse_format_specifier_regex.match(format_spec)\\n    if m is None:\\n        raise ValueError(\"Invalid format specifier: \" + format_spec)\\n    format_dict = m.groupdict()\\n    fill = format_dict['fill']\\n    align = format_dict['align']\\n    if format_dict.pop('zeropad') is not None:\\n        if fill is not None and fill != '0':\\n            raise ValueError(\"Fill character conflicts with '0'\"\\n                             \" in format specifier: \" + format_spec)\\n        if align is not None and align != '=':\\n            raise ValueError(\"Alignment conflicts with '0' in \"\\n                             \"format specifier: \" + format_spec)\\n        fill = '0'\\n        align = '='\\n    format_dict['fill'] = fill or ' '\\n    format_dict['align'] = align or '<'\\n    if format_dict['sign'] is None:\\n        format_dict['sign'] = '-'\\n    format_dict['minimumwidth'] = int(format_dict['minimumwidth'] or '0')\\n    if format_dict['precision'] is not None:\\n        format_dict['precision'] = int(format_dict['precision'])\\n    if format_dict['precision'] == 0:\\n        if format_dict['type'] is None or format_dict['type'] in 'gG':\\n            format_dict['precision'] = 1\\n    format_dict['unicode'] = isinstance(format_spec, unicode)\\n    return format_dict"
  },
  {
    "code": "def _missing_double(self, vec):\\n        v = vec.view(dtype='u1,u1,u2,u4')\\n        miss = (v['f1'] == 0) & (v['f2'] == 0) & (v['f3'] == 0)\\n        miss1 = ((v['f0'] >= 0x41) & (v['f0'] <= 0x5a)) |\\\\n                (v['f0'] == 0x5f) | (v['f0'] == 0x2e)\\n        miss &= miss1\\n        return miss",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add truncated float support to read_sas, #11713",
    "fixed_code": "def _missing_double(self, vec):\\n        v = vec.view(dtype='u1,u1,u2,u4')\\n        miss = (v['f1'] == 0) & (v['f2'] == 0) & (v['f3'] == 0)\\n        miss1 = (((v['f0'] >= 0x41) & (v['f0'] <= 0x5a)) |\\n                 (v['f0'] == 0x5f) | (v['f0'] == 0x2e))\\n        miss &= miss1\\n        return miss"
  },
  {
    "code": "def get_combinator_sql(self, combinator, all):\\n\\t\\tfeatures = self.connection.features\\n\\t\\tcompilers = [\\n\\t\\t\\tquery.get_compiler(self.using, self.connection)\\n\\t\\t\\tfor query in self.query.combined_queries if not query.is_empty()\\n\\t\\t]\\n\\t\\tif not features.supports_slicing_ordering_in_compound:\\n\\t\\t\\tfor query, compiler in zip(self.query.combined_queries, compilers):\\n\\t\\t\\t\\tif query.low_mark or query.high_mark:\\n\\t\\t\\t\\t\\traise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')\\n\\t\\t\\t\\tif compiler.get_order_by():\\n\\t\\t\\t\\t\\traise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')\\n\\t\\tparts = ()\\n\\t\\tfor compiler in compilers:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif (not compiler.query.values_select and not compiler.query.annotations and\\n\\t\\t\\t\\t\\t\\tself.query.values_select):\\n\\t\\t\\t\\t\\tcompiler.query.set_values(self.query.values_select)\\n\\t\\t\\t\\tparts += (compiler.as_sql(),)\\n\\t\\t\\texcept EmptyResultSet:\\n\\t\\t\\t\\tif combinator == 'union' or (combinator == 'difference' and parts):\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\traise\\n\\t\\tif not parts:\\n\\t\\t\\traise EmptyResultSet\\n\\t\\tcombinator_sql = self.connection.ops.set_operators[combinator]\\n\\t\\tif all and combinator == 'union':\\n\\t\\t\\tcombinator_sql += ' ALL'\\n\\t\\tbraces = '({})' if features.supports_slicing_ordering_in_compound else '{}'\\n\\t\\tsql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))\\n\\t\\tresult = [' {} '.format(combinator_sql).join(sql_parts)]\\n\\t\\tparams = []\\n\\t\\tfor part in args_parts:\\n\\t\\t\\tparams.extend(part)\\n\\t\\treturn result, params",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #29286 -- Fixed column mismatch crash with QuerySet.values() or values_list() after combining an annotated and unannotated queryset with union(), difference(), or intersection().\\n\\nRegression in a0c03c62a8ac586e5be5b21393c925afa581efaf.\\n\\nThanks Tim Graham and Carlton Gibson for reviews.",
    "fixed_code": "def get_combinator_sql(self, combinator, all):\\n\\t\\tfeatures = self.connection.features\\n\\t\\tcompilers = [\\n\\t\\t\\tquery.get_compiler(self.using, self.connection)\\n\\t\\t\\tfor query in self.query.combined_queries if not query.is_empty()\\n\\t\\t]\\n\\t\\tif not features.supports_slicing_ordering_in_compound:\\n\\t\\t\\tfor query, compiler in zip(self.query.combined_queries, compilers):\\n\\t\\t\\t\\tif query.low_mark or query.high_mark:\\n\\t\\t\\t\\t\\traise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')\\n\\t\\t\\t\\tif compiler.get_order_by():\\n\\t\\t\\t\\t\\traise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')\\n\\t\\tparts = ()\\n\\t\\tfor compiler in compilers:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tif not compiler.query.values_select and self.query.values_select:\\n\\t\\t\\t\\t\\tcompiler.query.set_values((*self.query.values_select, *self.query.annotation_select))\\n\\t\\t\\t\\tparts += (compiler.as_sql(),)\\n\\t\\t\\texcept EmptyResultSet:\\n\\t\\t\\t\\tif combinator == 'union' or (combinator == 'difference' and parts):\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\traise\\n\\t\\tif not parts:\\n\\t\\t\\traise EmptyResultSet\\n\\t\\tcombinator_sql = self.connection.ops.set_operators[combinator]\\n\\t\\tif all and combinator == 'union':\\n\\t\\t\\tcombinator_sql += ' ALL'\\n\\t\\tbraces = '({})' if features.supports_slicing_ordering_in_compound else '{}'\\n\\t\\tsql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))\\n\\t\\tresult = [' {} '.format(combinator_sql).join(sql_parts)]\\n\\t\\tparams = []\\n\\t\\tfor part in args_parts:\\n\\t\\t\\tparams.extend(part)\\n\\t\\treturn result, params"
  },
  {
    "code": "def slave(self):\\n        return self._shortcmd('SLAVE')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def slave(self):\\n        return self._shortcmd('SLAVE')"
  },
  {
    "code": "def rollback(e):\\n\\t\\t\\twith self.get_db() as db:\\n\\t\\t\\t\\tif os.path.exists(incomplete_path):\\n\\t\\t\\t\\t\\tinvalid_path = self.get_image_filepath(image_id, 'invalid')\\n\\t\\t\\t\\t\\tLOG.debug(_(\"Fetch of cache file failed (%(e)s), rolling \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"back by moving '%(incomplete_path)s' to \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"'%(invalid_path)s'\") % locals())\\n\\t\\t\\t\\t\\tos.rename(incomplete_path, invalid_path)\\n\\t\\t\\t\\tdb.execute(, (image_id, ))\\n\\t\\t\\t\\tdb.commit()\\n\\t\\ttry:\\n\\t\\t\\twith open(incomplete_path, 'wb') as cache_file:\\n\\t\\t\\t\\tyield cache_file\\n\\t\\texcept Exception as e:\\n\\t\\t\\trollback(e)\\n\\t\\t\\traise\\n\\t\\telse:\\n\\t\\t\\tcommit()\\n\\t\\tfinally:\\n\\t\\t\\tif os.path.exists(incomplete_path):\\n\\t\\t\\t\\trollback('incomplete fetch')\\n\\t@contextmanager",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rollback(e):\\n\\t\\t\\twith self.get_db() as db:\\n\\t\\t\\t\\tif os.path.exists(incomplete_path):\\n\\t\\t\\t\\t\\tinvalid_path = self.get_image_filepath(image_id, 'invalid')\\n\\t\\t\\t\\t\\tLOG.debug(_(\"Fetch of cache file failed (%(e)s), rolling \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"back by moving '%(incomplete_path)s' to \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"'%(invalid_path)s'\") % locals())\\n\\t\\t\\t\\t\\tos.rename(incomplete_path, invalid_path)\\n\\t\\t\\t\\tdb.execute(, (image_id, ))\\n\\t\\t\\t\\tdb.commit()\\n\\t\\ttry:\\n\\t\\t\\twith open(incomplete_path, 'wb') as cache_file:\\n\\t\\t\\t\\tyield cache_file\\n\\t\\texcept Exception as e:\\n\\t\\t\\trollback(e)\\n\\t\\t\\traise\\n\\t\\telse:\\n\\t\\t\\tcommit()\\n\\t\\tfinally:\\n\\t\\t\\tif os.path.exists(incomplete_path):\\n\\t\\t\\t\\trollback('incomplete fetch')\\n\\t@contextmanager"
  },
  {
    "code": "def _skts_alias_dictionary():\\n    alias_dict = {}\\n    M_aliases = [\"M\", \"MTH\", \"MONTH\", \"MONTHLY\"]\\n    B_aliases = [\"B\", \"BUS\", \"BUSINESS\", \"BUSINESSLY\", 'WEEKDAY']\\n    D_aliases = [\"D\", \"DAY\", \"DLY\", \"DAILY\"]\\n    H_aliases = [\"H\", \"HR\", \"HOUR\", \"HRLY\", \"HOURLY\"]\\n    T_aliases = [\"T\", \"MIN\", \"MINUTE\", \"MINUTELY\"]\\n    S_aliases = [\"S\", \"SEC\", \"SECOND\", \"SECONDLY\"]\\n    U_aliases = [\"U\", \"UND\", \"UNDEF\", \"UNDEFINED\"]\\n    for k in M_aliases:\\n        alias_dict[k] = 'M'\\n    for k in B_aliases:\\n        alias_dict[k] = 'B'\\n    for k in D_aliases:\\n        alias_dict[k] = 'D'\\n    for k in H_aliases:\\n        alias_dict[k] = 'H'\\n    for k in T_aliases:\\n        alias_dict[k] = 'Min'\\n    for k in S_aliases:\\n        alias_dict[k] = 'S'\\n    for k in U_aliases:\\n        alias_dict[k] = None\\n    A_prefixes = [\"A\", \"Y\", \"ANN\", \"ANNUAL\", \"ANNUALLY\", \"YR\", \"YEAR\",\\n                  \"YEARLY\"]\\n    Q_prefixes = [\"Q\", \"QTR\", \"QUARTER\", \"QUARTERLY\", \"Q-E\",\\n                  \"QTR-E\", \"QUARTER-E\", \"QUARTERLY-E\"]\\n    month_names = [\\n        [ \"DEC\", \"DECEMBER\" ],\\n        [ \"JAN\", \"JANUARY\" ],\\n        [ \"FEB\", \"FEBRUARY\" ],\\n        [ \"MAR\", \"MARCH\" ],\\n        [ \"APR\", \"APRIL\" ],\\n        [ \"MAY\", \"MAY\" ],\\n        [ \"JUN\", \"JUNE\" ],\\n        [ \"JUL\", \"JULY\" ],\\n        [ \"AUG\", \"AUGUST\" ],\\n        [ \"SEP\", \"SEPTEMBER\" ],\\n        [ \"OCT\", \"OCTOBER\" ],\\n        [ \"NOV\", \"NOVEMBER\" ] ]\\n    seps = [\"@\", \"-\"]\\n    for k in A_prefixes:\\n        alias_dict[k] = 'A'\\n        for m_tup in month_names:\\n            for sep in seps:\\n                m1, m2 = m_tup\\n                alias_dict[k + sep + m1] = 'A@' + m1\\n                alias_dict[k + sep + m2] = 'A@' + m1\\n    for k in Q_prefixes:\\n        alias_dict[k] = 'Q'\\n        for m_tup in month_names:\\n            for sep in seps:\\n                m1, m2 = m_tup\\n                alias_dict[k + sep + m1] = 'Q@' + m1\\n                alias_dict[k + sep + m2] = 'Q@' + m1\\n    W_prefixes = [\"W\", \"WK\", \"WEEK\", \"WEEKLY\"]\\n    day_names = [\\n        [ \"SUN\", \"SUNDAY\" ],\\n        [ \"MON\", \"MONDAY\" ],\\n        [ \"TUE\", \"TUESDAY\" ],\\n        [ \"WED\", \"WEDNESDAY\" ],\\n        [ \"THU\", \"THURSDAY\" ],\\n        [ \"FRI\", \"FRIDAY\" ],\\n        [ \"SAT\", \"SATURDAY\" ] ]\\n    for k in W_prefixes:\\n        alias_dict[k] = 'W'\\n        for d_tup in day_names:\\n            for sep in [\"@\", \"-\"]:\\n                d1, d2 = d_tup\\n                alias_dict[k + sep + d1] = 'W@' + d1\\n                alias_dict[k + sep + d2] = 'W@' + d1\\n    return alias_dict\\n_reverse_interval_code_map = {}\\nfor k, v in _interval_code_map.iteritems():\\n    _reverse_interval_code_map[v] = k\\n_reso_interval_map = {\\n    \"year\"    : \"A\",\\n    \"quarter\" : \"Q\",\\n    \"month\"   : \"M\",\\n    \"day\"     : \"D\",\\n    \"hour\"    : \"H\",\\n    \"minute\"  : \"Min\",\\n    \"second\"  : \"S\",\\n}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: legacy time rule support and refactoring, better alias handling. misc tests, #1041",
    "fixed_code": "def _skts_alias_dictionary():\\n    alias_dict = {}\\n    M_aliases = [\"M\", \"MTH\", \"MONTH\", \"MONTHLY\"]\\n    B_aliases = [\"B\", \"BUS\", \"BUSINESS\", \"BUSINESSLY\", 'WEEKDAY']\\n    D_aliases = [\"D\", \"DAY\", \"DLY\", \"DAILY\"]\\n    H_aliases = [\"H\", \"HR\", \"HOUR\", \"HRLY\", \"HOURLY\"]\\n    T_aliases = [\"T\", \"MIN\", \"MINUTE\", \"MINUTELY\"]\\n    S_aliases = [\"S\", \"SEC\", \"SECOND\", \"SECONDLY\"]\\n    U_aliases = [\"U\", \"UND\", \"UNDEF\", \"UNDEFINED\"]\\n    for k in M_aliases:\\n        alias_dict[k] = 'M'\\n    for k in B_aliases:\\n        alias_dict[k] = 'B'\\n    for k in D_aliases:\\n        alias_dict[k] = 'D'\\n    for k in H_aliases:\\n        alias_dict[k] = 'H'\\n    for k in T_aliases:\\n        alias_dict[k] = 'Min'\\n    for k in S_aliases:\\n        alias_dict[k] = 'S'\\n    for k in U_aliases:\\n        alias_dict[k] = None\\n    A_prefixes = [\"A\", \"Y\", \"ANN\", \"ANNUAL\", \"ANNUALLY\", \"YR\", \"YEAR\",\\n                  \"YEARLY\"]\\n    Q_prefixes = [\"Q\", \"QTR\", \"QUARTER\", \"QUARTERLY\", \"Q-E\",\\n                  \"QTR-E\", \"QUARTER-E\", \"QUARTERLY-E\"]\\n    month_names = [\\n        [ \"DEC\", \"DECEMBER\" ],\\n        [ \"JAN\", \"JANUARY\" ],\\n        [ \"FEB\", \"FEBRUARY\" ],\\n        [ \"MAR\", \"MARCH\" ],\\n        [ \"APR\", \"APRIL\" ],\\n        [ \"MAY\", \"MAY\" ],\\n        [ \"JUN\", \"JUNE\" ],\\n        [ \"JUL\", \"JULY\" ],\\n        [ \"AUG\", \"AUGUST\" ],\\n        [ \"SEP\", \"SEPTEMBER\" ],\\n        [ \"OCT\", \"OCTOBER\" ],\\n        [ \"NOV\", \"NOVEMBER\" ] ]\\n    seps = [\"@\", \"-\"]\\n    for k in A_prefixes:\\n        alias_dict[k] = 'A'\\n        for m_tup in month_names:\\n            for sep in seps:\\n                m1, m2 = m_tup\\n                alias_dict[k + sep + m1] = 'A-' + m1\\n                alias_dict[k + sep + m2] = 'A-' + m1\\n    for k in Q_prefixes:\\n        alias_dict[k] = 'Q'\\n        for m_tup in month_names:\\n            for sep in seps:\\n                m1, m2 = m_tup\\n                alias_dict[k + sep + m1] = 'Q-' + m1\\n                alias_dict[k + sep + m2] = 'Q-' + m1\\n    W_prefixes = [\"W\", \"WK\", \"WEEK\", \"WEEKLY\"]\\n    day_names = [\\n        [ \"SUN\", \"SUNDAY\" ],\\n        [ \"MON\", \"MONDAY\" ],\\n        [ \"TUE\", \"TUESDAY\" ],\\n        [ \"WED\", \"WEDNESDAY\" ],\\n        [ \"THU\", \"THURSDAY\" ],\\n        [ \"FRI\", \"FRIDAY\" ],\\n        [ \"SAT\", \"SATURDAY\" ] ]\\n    for k in W_prefixes:\\n        alias_dict[k] = 'W'\\n        for d_tup in day_names:\\n            for sep in [\"@\", \"-\"]:\\n                d1, d2 = d_tup\\n                alias_dict[k + sep + d1] = 'W-' + d1\\n                alias_dict[k + sep + d2] = 'W-' + d1\\n    return alias_dict\\n_reverse_interval_code_map = {}\\nfor k, v in _interval_code_map.iteritems():\\n    _reverse_interval_code_map[v] = k\\n_reso_interval_map = {\\n    \"year\"    : \"A\",\\n    \"quarter\" : \"Q\",\\n    \"month\"   : \"M\",\\n    \"day\"     : \"D\",\\n    \"hour\"    : \"H\",\\n    \"minute\"  : \"T\",\\n    \"second\"  : \"S\",\\n}"
  },
  {
    "code": "def init_vocab(\\n\\tnlp: \"Language\",\\n\\t*,\\n\\tdata: Optional[Path] = None,\\n\\tlookups: Optional[Lookups] = None,\\n\\tvectors: Optional[str] = None,\\n) -> None:\\n\\tif lookups:\\n\\t\\tnlp.vocab.lookups = lookups\\n\\t\\tlogger.info(\"Added vocab lookups: %s\", \", \".join(lookups.tables))\\n\\tdata_path = ensure_path(data)\\n\\tif data_path is not None:\\n\\t\\tlex_attrs = srsly.read_jsonl(data_path)\\n\\t\\tfor lexeme in nlp.vocab:\\n\\t\\t\\tlexeme.rank = OOV_RANK\\n\\t\\tfor attrs in lex_attrs:\\n\\t\\t\\tif \"settings\" in attrs:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tlexeme = nlp.vocab[attrs[\"orth\"]]\\n\\t\\t\\tlexeme.set_attrs(**attrs)\\n\\t\\tif len(nlp.vocab):\\n\\t\\t\\toov_prob = min(lex.prob for lex in nlp.vocab) - 1\\n\\t\\telse:\\n\\t\\t\\toov_prob = DEFAULT_OOV_PROB\\n\\t\\tnlp.vocab.cfg.update({\"oov_prob\": oov_prob})\\n\\t\\tlogger.info(\"Added %d lexical entries to the vocab\", len(nlp.vocab))\\n\\tlogger.info(\"Created vocabulary\")\\n\\tif vectors is not None:\\n\\t\\tload_vectors_into_model(nlp, vectors)\\n\\t\\tlogger.info(\"Added vectors: %s\", vectors)\\n\\tsourced_vectors_hashes = nlp.meta.pop(\"_sourced_vectors_hashes\", {})\\n\\tvectors_hash = hash(nlp.vocab.vectors.to_bytes(exclude=[\"strings\"]))\\n\\tfor sourced_component, sourced_vectors_hash in sourced_vectors_hashes.items():\\n\\t\\tif vectors_hash != sourced_vectors_hash:\\n\\t\\t\\twarnings.warn(Warnings.W113.format(name=sourced_component))\\n\\tlogger.info(\"Finished initializing nlp object\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def init_vocab(\\n\\tnlp: \"Language\",\\n\\t*,\\n\\tdata: Optional[Path] = None,\\n\\tlookups: Optional[Lookups] = None,\\n\\tvectors: Optional[str] = None,\\n) -> None:\\n\\tif lookups:\\n\\t\\tnlp.vocab.lookups = lookups\\n\\t\\tlogger.info(\"Added vocab lookups: %s\", \", \".join(lookups.tables))\\n\\tdata_path = ensure_path(data)\\n\\tif data_path is not None:\\n\\t\\tlex_attrs = srsly.read_jsonl(data_path)\\n\\t\\tfor lexeme in nlp.vocab:\\n\\t\\t\\tlexeme.rank = OOV_RANK\\n\\t\\tfor attrs in lex_attrs:\\n\\t\\t\\tif \"settings\" in attrs:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tlexeme = nlp.vocab[attrs[\"orth\"]]\\n\\t\\t\\tlexeme.set_attrs(**attrs)\\n\\t\\tif len(nlp.vocab):\\n\\t\\t\\toov_prob = min(lex.prob for lex in nlp.vocab) - 1\\n\\t\\telse:\\n\\t\\t\\toov_prob = DEFAULT_OOV_PROB\\n\\t\\tnlp.vocab.cfg.update({\"oov_prob\": oov_prob})\\n\\t\\tlogger.info(\"Added %d lexical entries to the vocab\", len(nlp.vocab))\\n\\tlogger.info(\"Created vocabulary\")\\n\\tif vectors is not None:\\n\\t\\tload_vectors_into_model(nlp, vectors)\\n\\t\\tlogger.info(\"Added vectors: %s\", vectors)\\n\\tsourced_vectors_hashes = nlp.meta.pop(\"_sourced_vectors_hashes\", {})\\n\\tvectors_hash = hash(nlp.vocab.vectors.to_bytes(exclude=[\"strings\"]))\\n\\tfor sourced_component, sourced_vectors_hash in sourced_vectors_hashes.items():\\n\\t\\tif vectors_hash != sourced_vectors_hash:\\n\\t\\t\\twarnings.warn(Warnings.W113.format(name=sourced_component))\\n\\tlogger.info(\"Finished initializing nlp object\")"
  },
  {
    "code": "def decr_version(self, key, delta=1, version=None):\\n        return self.incr_version(key, -delta, version)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25668 -- Misc spelling errors",
    "fixed_code": "def decr_version(self, key, delta=1, version=None):\\n        return self.incr_version(key, -delta, version)"
  },
  {
    "code": "def register(self, **kwargs):\\n\\t\\tconsumer_id = kwargs['consumer-id']\\n\\t\\texisting_consumer = load_consumer_id(self.context)\\n\\t\\tif existing_consumer:\\n\\t\\t\\tm = _('This system has already been registered as a consumer. Please '\\n\\t\\t\\t\\t  'use the unregister command to remove the consumer before attempting '\\n\\t\\t\\t\\t  'to re-register.')\\n\\t\\t\\tself.prompt.render_failure_message(m)\\n\\t\\t\\treturn\\n\\t\\tname = kwargs.get('display-name', consumer_id)\\n\\t\\tdescription = kwargs.get('description')\\n\\t\\tnotes = kwargs.get('note')\\n\\t\\tif notes:\\n\\t\\t\\tnotes = args_to_notes_dict(notes, include_none=False)\\n\\t\\tid_cert_dir = self.context.config['filesystem']['id_cert_dir']\\n\\t\\tif not os.access(id_cert_dir, os.W_OK):\\n\\t\\t\\tmsg = _(\"Write permission is required for %(d)s to perform this operation.\")\\n\\t\\t\\tself.prompt.render_failure_message(msg % {'d': id_cert_dir})\\n\\t\\t\\treturn exceptions.CODE_PERMISSIONS_EXCEPTION\\n\\t\\tpath = self.context.config['authentication']['rsa_key']\\n\\t\\tkey = RSA.gen_key(2048, 65535, no_passphrase_callback)\\n\\t\\tkey.save_key(path, None)\\n\\t\\tpath = self.context.config['authentication']['rsa_pub']\\n\\t\\tkey.save_pub_key(path)\\n\\t\\tfp = open(path)\\n\\t\\ttry:\\n\\t\\t\\trsa_pub = fp.read()\\n\\t\\tfinally:\\n\\t\\t\\tfp.close()\\n\\t\\treply = self.context.server.consumer.register(\\n\\t\\t\\tconsumer_id,\\n\\t\\t\\tname=name,\\n\\t\\t\\tdescription=description,\\n\\t\\t\\tnotes=notes,\\n\\t\\t\\trsa_pub=rsa_pub)\\n\\t\\tcertificate = reply.response_body['certificate']\\n\\t\\tid_cert_name = self.context.config['filesystem']['id_cert_filename']\\n\\t\\tcert_filename = os.path.join(id_cert_dir, id_cert_name)\\n\\t\\tfp = open(cert_filename, 'w')\\n\\t\\ttry:\\n\\t\\t\\tfp.write(certificate)\\n\\t\\tfinally:\\n\\t\\t\\tfp.close()\\n\\t\\tupdate_server_key(self)\\n\\t\\tself.prompt.render_success_message('Consumer [%s] successfully registered' % consumer_id)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def register(self, **kwargs):\\n\\t\\tconsumer_id = kwargs['consumer-id']\\n\\t\\texisting_consumer = load_consumer_id(self.context)\\n\\t\\tif existing_consumer:\\n\\t\\t\\tm = _('This system has already been registered as a consumer. Please '\\n\\t\\t\\t\\t  'use the unregister command to remove the consumer before attempting '\\n\\t\\t\\t\\t  'to re-register.')\\n\\t\\t\\tself.prompt.render_failure_message(m)\\n\\t\\t\\treturn\\n\\t\\tname = kwargs.get('display-name', consumer_id)\\n\\t\\tdescription = kwargs.get('description')\\n\\t\\tnotes = kwargs.get('note')\\n\\t\\tif notes:\\n\\t\\t\\tnotes = args_to_notes_dict(notes, include_none=False)\\n\\t\\tid_cert_dir = self.context.config['filesystem']['id_cert_dir']\\n\\t\\tif not os.access(id_cert_dir, os.W_OK):\\n\\t\\t\\tmsg = _(\"Write permission is required for %(d)s to perform this operation.\")\\n\\t\\t\\tself.prompt.render_failure_message(msg % {'d': id_cert_dir})\\n\\t\\t\\treturn exceptions.CODE_PERMISSIONS_EXCEPTION\\n\\t\\tpath = self.context.config['authentication']['rsa_key']\\n\\t\\tkey = RSA.gen_key(2048, 65535, no_passphrase_callback)\\n\\t\\tkey.save_key(path, None)\\n\\t\\tpath = self.context.config['authentication']['rsa_pub']\\n\\t\\tkey.save_pub_key(path)\\n\\t\\tfp = open(path)\\n\\t\\ttry:\\n\\t\\t\\trsa_pub = fp.read()\\n\\t\\tfinally:\\n\\t\\t\\tfp.close()\\n\\t\\treply = self.context.server.consumer.register(\\n\\t\\t\\tconsumer_id,\\n\\t\\t\\tname=name,\\n\\t\\t\\tdescription=description,\\n\\t\\t\\tnotes=notes,\\n\\t\\t\\trsa_pub=rsa_pub)\\n\\t\\tcertificate = reply.response_body['certificate']\\n\\t\\tid_cert_name = self.context.config['filesystem']['id_cert_filename']\\n\\t\\tcert_filename = os.path.join(id_cert_dir, id_cert_name)\\n\\t\\tfp = open(cert_filename, 'w')\\n\\t\\ttry:\\n\\t\\t\\tfp.write(certificate)\\n\\t\\tfinally:\\n\\t\\t\\tfp.close()\\n\\t\\tupdate_server_key(self)\\n\\t\\tself.prompt.render_success_message('Consumer [%s] successfully registered' % consumer_id)"
  },
  {
    "code": "def Array(typecode_or_type, size_or_initializer, **kwds):\\n    ''\\n    lock = kwds.pop('lock', None)\\n    if kwds:\\n        raise ValueError('unrecognized keyword argument(s): %s' % list(kwds.keys()))\\n    obj = RawArray(typecode_or_type, size_or_initializer)\\n    if lock is None:\\n        lock = RLock()\\n    assert hasattr(lock, 'acquire')\\n    return synchronized(obj, lock)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merge r68708 to py3k, fixes 4449",
    "fixed_code": "def Array(typecode_or_type, size_or_initializer, **kwds):\\n    ''\\n    lock = kwds.pop('lock', None)\\n    if kwds:\\n        raise ValueError('unrecognized keyword argument(s): %s' % list(kwds.keys()))\\n    obj = RawArray(typecode_or_type, size_or_initializer)\\n    if lock is False:\\n        return obj\\n    if lock in (True, None):\\n        lock = RLock()\\n    if not hasattr(lock, 'acquire'):\\n        raise AttributeError(\"'%r' has no method 'acquire'\" % lock)\\n    return synchronized(obj, lock)"
  },
  {
    "code": "def __init__(\\n        self,\\n        *,\\n        conn_id: str | None = None,\\n        database: str | None = None,\\n        hook_params: dict | None = None,\\n        **kwargs,\\n    ):\\n        super().__init__(**kwargs)\\n        self.conn_id = conn_id\\n        self.database = database\\n        self.hook_params = {} if hook_params is None else hook_params\\n    @cached_property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Common sql bugfixes and improvements (#26761)\\n\\n\\n\\n\\nThe job_id is automatically generated by hook.insert_job()\\nif an empty string is passed, so job_id generation in the\\noperator is removed in favor of the existing code.\\n\\n\\n\\nSQL query building is moved to the init() method of the column\\nand table check operators to lessen the amount of duplicate code\\nin the child operator. It also has the added effect of, ideally,\\npassing a more complete query to OpenLineage.\\n\\nIn doing the above, the column check operator had to be reworked and\\nnow matches the logic of the table check operator in terms of\\nreturning multiple rows and only sending one query to the database.\\n\\n\\n\\nAdds a new parameter, retry_on_failure, and a new function to\\ndetermine if operators should retry or not on test failure.\\n\\n\\n\\n\\n\\n\\n\\nUpdates tests to reflect changes in operator code, and fixed\\nbugs in operators as well. Mainly moving the code to check for\\nfailed tests into the column and table check operators as it works\\nslightly differently for each and doesn't make much sense as a\\ntop-level function.\\n\\n\\n\\n\\n\\n\\nThe job_id is automatically generated by hook.insert_job()\\nif an empty string is passed, so job_id generation in the\\noperator is removed in favor of the existing code.\\n\\n\\n\\nSQL query building is moved to the init() method of the column\\nand table check operators to lessen the amount of duplicate code\\nin the child operator. It also has the added effect of, ideally,\\npassing a more complete query to OpenLineage.\\n\\nIn doing the above, the column check operator had to be reworked and\\nnow matches the logic of the table check operator in terms of\\nreturning multiple rows and only sending one query to the database.\\n\\n\\n\\nAdds a new parameter, retry_on_failure, and a new function to\\ndetermine if operators should retry or not on test failure.\\n\\n\\n\\n\\n\\n\\n\\nUpdates tests to reflect changes in operator code, and fixed\\nbugs in operators as well. Mainly moving the code to check for\\nfailed tests into the column and table check operators as it works\\nslightly differently for each and doesn't make much sense as a\\ntop-level function.\\n\\n\\n\\n\\n\\n\\n\\nAdds \"where\" option in checks dictionaries for column and table\\noperators, which may be renamed. This allows for check-level\\npartitioning, whereas the partition_clause param will always be\\nfor all checks. New tests are added for this addition.\\n\\n\\nCleans up operator and adds testing for new generator function.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe new argument, defaulting to true, will convert Nones returned\\nfrom the query to 0s so numeric calculations can be performed\\ncorrectly. This allows empty tables to be handled as a row of zeroes.\\n\\nAdditional documentation is also supplied\\n\\n\\n\\n\\nThe job_id is automatically generated by hook.insert_job()\\nif an empty string is passed, so job_id generation in the\\noperator is removed in favor of the existing code.\\n\\n\\n\\nSQL query building is moved to the init() method of the column\\nand table check operators to lessen the amount of duplicate code\\nin the child operator. It also has the added effect of, ideally,\\npassing a more complete query to OpenLineage.\\n\\nIn doing the above, the column check operator had to be reworked and\\nnow matches the logic of the table check operator in terms of\\nreturning multiple rows and only sending one query to the database.\\n\\n\\n\\nAdds a new parameter, retry_on_failure, and a new function to\\ndetermine if operators should retry or not on test failure.\\n\\n\\n\\n\\n\\n\\n\\nUpdates tests to reflect changes in operator code, and fixed\\nbugs in operators as well. Mainly moving the code to check for\\nfailed tests into the column and table check operators as it works\\nslightly differently for each and doesn't make much sense as a\\ntop-level function.\\n\\n\\n\\n\\n\\n\\n\\nAdds \"where\" option in checks dictionaries for column and table\\noperators, which may be renamed. This allows for check-level\\npartitioning, whereas the partition_clause param will always be\\nfor all checks. New tests are added for this addition.\\n\\n\\nCleans up operator and adds testing for new generator function.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe new argument, defaulting to true, will convert Nones returned\\nfrom the query to 0s so numeric calculations can be performed\\ncorrectly. This allows empty tables to be handled as a row of zeroes.\\n\\nAdditional documentation is also supplied",
    "fixed_code": "def __init__(\\n        self,\\n        *,\\n        conn_id: str | None = None,\\n        database: str | None = None,\\n        hook_params: dict | None = None,\\n        retry_on_failure: bool = True,\\n        **kwargs,\\n    ):\\n        super().__init__(**kwargs)\\n        self.conn_id = conn_id\\n        self.database = database\\n        self.hook_params = {} if hook_params is None else hook_params\\n        self.retry_on_failure = retry_on_failure\\n    @cached_property"
  },
  {
    "code": "def atomize(self, declarations) -> Generator[tuple[str, str], None, None]:\\n        for prop, value in declarations:\\n            attr = \"expand_\" + prop.replace(\"-\", \"_\")\\n            try:\\n                expand = getattr(self, attr)\\n            except AttributeError:\\n                yield prop, value\\n            else:\\n                for prop, value in expand(prop, value):\\n                    yield prop, value\\n    expand_border = _border_expander()\\n    expand_border_top = _border_expander(\"top\")\\n    expand_border_right = _border_expander(\"right\")\\n    expand_border_bottom = _border_expander(\"bottom\")\\n    expand_border_left = _border_expander(\"left\")\\n    expand_border_color = _side_expander(\"border-{:s}-color\")\\n    expand_border_style = _side_expander(\"border-{:s}-style\")\\n    expand_border_width = _side_expander(\"border-{:s}-width\")\\n    expand_margin = _side_expander(\"margin-{:s}\")\\n    expand_padding = _side_expander(\"padding-{:s}\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def atomize(self, declarations) -> Generator[tuple[str, str], None, None]:\\n        for prop, value in declarations:\\n            attr = \"expand_\" + prop.replace(\"-\", \"_\")\\n            try:\\n                expand = getattr(self, attr)\\n            except AttributeError:\\n                yield prop, value\\n            else:\\n                for prop, value in expand(prop, value):\\n                    yield prop, value\\n    expand_border = _border_expander()\\n    expand_border_top = _border_expander(\"top\")\\n    expand_border_right = _border_expander(\"right\")\\n    expand_border_bottom = _border_expander(\"bottom\")\\n    expand_border_left = _border_expander(\"left\")\\n    expand_border_color = _side_expander(\"border-{:s}-color\")\\n    expand_border_style = _side_expander(\"border-{:s}-style\")\\n    expand_border_width = _side_expander(\"border-{:s}-width\")\\n    expand_margin = _side_expander(\"margin-{:s}\")\\n    expand_padding = _side_expander(\"padding-{:s}\")"
  },
  {
    "code": "def repeat(self, repeats, axis=None):\\n        nv.validate_repeat(tuple(), dict(axis=axis))\\n        freq = self.freq if is_period_dtype(self) else None\\n        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)\\n    @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: TDI/DTI _shallow_copy creating invalid arrays (#30764)",
    "fixed_code": "def repeat(self, repeats, axis=None):\\n        nv.validate_repeat(tuple(), dict(axis=axis))\\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\\n        return self._shallow_copy(result)\\n    @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)"
  },
  {
    "code": "def get_template_env(self) -> jinja2.Environment:\\n        return (\\n            self.dag.get_template_env()\\n            if self.has_dag()\\n            else airflow.templates.SandboxedEnvironment(cache_size=0)\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_template_env(self) -> jinja2.Environment:\\n        return (\\n            self.dag.get_template_env()\\n            if self.has_dag()\\n            else airflow.templates.SandboxedEnvironment(cache_size=0)\\n        )"
  },
  {
    "code": "def __init__(self, conn_id, sql, parameters=None, success=None, failure=None, fail_on_empty=False,\\n                 allow_null=True, *args, **kwargs):\\n        self.conn_id = conn_id\\n        self.sql = sql\\n        self.parameters = parameters\\n        self.success = success\\n        self.failure = failure\\n        self.fail_on_empty = fail_on_empty\\n        self.allow_null = allow_null\\n        super().__init__(*args, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5073] Change SQLSensor to not treat NULL as success criteria (#5913)\\n\\nRemove allow_null parameter to decrease clutter.",
    "fixed_code": "def __init__(self, conn_id, sql, parameters=None, success=None, failure=None, fail_on_empty=False,\\n                 *args, **kwargs):\\n        self.conn_id = conn_id\\n        self.sql = sql\\n        self.parameters = parameters\\n        self.success = success\\n        self.failure = failure\\n        self.fail_on_empty = fail_on_empty\\n        super().__init__(*args, **kwargs)"
  },
  {
    "code": "def connections_import(args):\\n    if os.path.exists(args.file):\\n        _import_helper(args.file)\\n    else:\\n        raise SystemExit(\"Missing connections file.\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix CLI connections import and migrate logic from secrets to Connection model (#15425)\\n\\n\\n\\nIn connections_import, each connection was deserialized and stored into a\\nConnection model instance rather than a dictionary, so an erroneous call to the\\ndictionary methods .items() resulted in an AttributeError. With this fix,\\nconnection information is loaded from dictionaries directly into the\\nConnection constructor and committed to the DB.",
    "fixed_code": "def connections_import(args):\\n    if os.path.exists(args.file):\\n        _import_helper(args.file)\\n    else:\\n        raise SystemExit(\"Missing connections file.\")"
  },
  {
    "code": "def _interpolate_scipy_wrapper(x, y, new_x, method, fill_value=None,\\n                               bounds_error=False, order=None, **kwargs):\\n    try:\\n        from scipy import interpolate\\n        from pandas import DatetimeIndex  \\n    except ImportError:\\n        raise ImportError('{0} interpolation requires Scipy'.format(method))\\n    new_x = np.asarray(new_x)\\n    alt_methods = {\\n        'barycentric': interpolate.barycentric_interpolate,\\n        'krogh': interpolate.krogh_interpolate,\\n        'from_derivatives': _from_derivatives,\\n        'piecewise_polynomial': _from_derivatives,\\n    }\\n    if getattr(x, 'is_all_dates', False):\\n        x, new_x = x._values.astype('i8'), new_x.astype('i8')\\n    if method == 'pchip':\\n        try:\\n            alt_methods['pchip'] = interpolate.pchip_interpolate\\n        except AttributeError:\\n            raise ImportError(\"Your version of Scipy does not support \"\\n                              \"PCHIP interpolation.\")\\n    elif method == 'akima':\\n        try:\\n            from scipy.interpolate import Akima1DInterpolator  \\n            alt_methods['akima'] = _akima_interpolate\\n        except ImportError:\\n            raise ImportError(\"Your version of Scipy does not support \"\\n                              \"Akima interpolation.\")\\n    interp1d_methods = ['nearest', 'zero', 'slinear', 'quadratic', 'cubic',\\n                        'polynomial']\\n    if method in interp1d_methods:\\n        if method == 'polynomial':\\n            method = order\\n        terp = interpolate.interp1d(x, y, kind=method, fill_value=fill_value,\\n                                    bounds_error=bounds_error)\\n        new_y = terp(new_x)\\n    elif method == 'spline':\\n        if not order:\\n            raise ValueError(\"order needs to be specified and greater than 0\")\\n        terp = interpolate.UnivariateSpline(x, y, k=order, **kwargs)\\n        new_y = terp(new_x)\\n    else:\\n        if not x.flags.writeable:\\n            x = x.copy()\\n        if not y.flags.writeable:\\n            y = y.copy()\\n        if not new_x.flags.writeable:\\n            new_x = new_x.copy()\\n        method = alt_methods[method]\\n        new_y = method(x, y, new_x, **kwargs)\\n    return new_y",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _interpolate_scipy_wrapper(x, y, new_x, method, fill_value=None,\\n                               bounds_error=False, order=None, **kwargs):\\n    try:\\n        from scipy import interpolate\\n        from pandas import DatetimeIndex  \\n    except ImportError:\\n        raise ImportError('{0} interpolation requires Scipy'.format(method))\\n    new_x = np.asarray(new_x)\\n    alt_methods = {\\n        'barycentric': interpolate.barycentric_interpolate,\\n        'krogh': interpolate.krogh_interpolate,\\n        'from_derivatives': _from_derivatives,\\n        'piecewise_polynomial': _from_derivatives,\\n    }\\n    if getattr(x, 'is_all_dates', False):\\n        x, new_x = x._values.astype('i8'), new_x.astype('i8')\\n    if method == 'pchip':\\n        try:\\n            alt_methods['pchip'] = interpolate.pchip_interpolate\\n        except AttributeError:\\n            raise ImportError(\"Your version of Scipy does not support \"\\n                              \"PCHIP interpolation.\")\\n    elif method == 'akima':\\n        try:\\n            from scipy.interpolate import Akima1DInterpolator  \\n            alt_methods['akima'] = _akima_interpolate\\n        except ImportError:\\n            raise ImportError(\"Your version of Scipy does not support \"\\n                              \"Akima interpolation.\")\\n    interp1d_methods = ['nearest', 'zero', 'slinear', 'quadratic', 'cubic',\\n                        'polynomial']\\n    if method in interp1d_methods:\\n        if method == 'polynomial':\\n            method = order\\n        terp = interpolate.interp1d(x, y, kind=method, fill_value=fill_value,\\n                                    bounds_error=bounds_error)\\n        new_y = terp(new_x)\\n    elif method == 'spline':\\n        if not order:\\n            raise ValueError(\"order needs to be specified and greater than 0\")\\n        terp = interpolate.UnivariateSpline(x, y, k=order, **kwargs)\\n        new_y = terp(new_x)\\n    else:\\n        if not x.flags.writeable:\\n            x = x.copy()\\n        if not y.flags.writeable:\\n            y = y.copy()\\n        if not new_x.flags.writeable:\\n            new_x = new_x.copy()\\n        method = alt_methods[method]\\n        new_y = method(x, y, new_x, **kwargs)\\n    return new_y"
  },
  {
    "code": "def run(self):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tDocServer.base = http.server.HTTPServer\\n\\t\\t\\t\\tDocServer.handler = DocHandler\\n\\t\\t\\t\\tDocHandler.MessageClass = email.message.Message\\n\\t\\t\\t\\tDocHandler.urlhandler = staticmethod(self.urlhandler)\\n\\t\\t\\t\\tdocsvr = DocServer(self.port, self.ready)\\n\\t\\t\\t\\tself.docserver = docsvr\\n\\t\\t\\t\\tdocsvr.serve_until_quit()\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tself.error = e",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-31128: Allow pydoc to bind to arbitrary hostnames (#3011)\\n\\nNew -n flag allow overriding localhost with custom value,\\nfor example to run from containers.",
    "fixed_code": "def run(self):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tDocServer.base = http.server.HTTPServer\\n\\t\\t\\t\\tDocServer.handler = DocHandler\\n\\t\\t\\t\\tDocHandler.MessageClass = email.message.Message\\n\\t\\t\\t\\tDocHandler.urlhandler = staticmethod(self.urlhandler)\\n\\t\\t\\t\\tdocsvr = DocServer(self.host, self.port, self.ready)\\n\\t\\t\\t\\tself.docserver = docsvr\\n\\t\\t\\t\\tdocsvr.serve_until_quit()\\n\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\tself.error = e"
  },
  {
    "code": "def retry_http_digest_auth(self, req, auth):\\n        token, challenge = auth.split(' ', 1)\\n        chal = parse_keqv_list(filter(None, parse_http_list(challenge)))\\n        auth = self.get_authorization(req, chal)\\n        if auth:\\n            auth_val = 'Digest %s' % auth\\n            if req.headers.get(self.auth_header, None) == auth_val:\\n                return None\\n            req.add_unredirected_header(self.auth_header, auth_val)\\n            resp = self.parent.open(req)\\n            return resp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix for issue5102, timeout value propages between redirects, proxy, digest and auth handlers. Fixed tests to reflect the same.",
    "fixed_code": "def retry_http_digest_auth(self, req, auth):\\n        token, challenge = auth.split(' ', 1)\\n        chal = parse_keqv_list(filter(None, parse_http_list(challenge)))\\n        auth = self.get_authorization(req, chal)\\n        if auth:\\n            auth_val = 'Digest %s' % auth\\n            if req.headers.get(self.auth_header, None) == auth_val:\\n                return None\\n            req.add_unredirected_header(self.auth_header, auth_val)\\n            resp = self.parent.open(req, timeout=req.timeout)\\n            return resp"
  },
  {
    "code": "def __init__(self, values, placement, ndim: int):\\n        self.ndim = ndim\\n        self.mgr_locs = placement\\n        self.values = values\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PERF/REF: require BlockPlacement in Block.__init__ (#40361)",
    "fixed_code": "def __init__(self, values, placement: BlockPlacement, ndim: int):\\n        assert isinstance(ndim, int)\\n        assert isinstance(placement, BlockPlacement)\\n        self.ndim = ndim\\n        self._mgr_locs = placement\\n        self.values = values\\n    @property"
  },
  {
    "code": "def _load_post_and_files(self):\\n        if self.method != 'POST':\\n            self._post, self._files = QueryDict('', encoding=self._encoding), MultiValueDict()\\n            return\\n        if self._read_started and not hasattr(self, '_body'):\\n            self._mark_post_parse_error()\\n            return\\n        if self.META.get('CONTENT_TYPE', '').startswith('multipart/form-data'):\\n            if hasattr(self, '_body'):\\n                data = BytesIO(self._body)\\n            else:\\n                data = self\\n            try:\\n                self._post, self._files = self.parse_file_upload(self.META, data)\\n            except:\\n                self._mark_post_parse_error()\\n                raise\\n        elif self.META.get('CONTENT_TYPE', '').startswith('application/x-www-form-urlencoded'):\\n            self._post, self._files = QueryDict(self.body, encoding=self._encoding), MultiValueDict()\\n        else:\\n            self._post, self._files = QueryDict('', encoding=self._encoding), MultiValueDict()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _load_post_and_files(self):\\n        if self.method != 'POST':\\n            self._post, self._files = QueryDict('', encoding=self._encoding), MultiValueDict()\\n            return\\n        if self._read_started and not hasattr(self, '_body'):\\n            self._mark_post_parse_error()\\n            return\\n        if self.META.get('CONTENT_TYPE', '').startswith('multipart/form-data'):\\n            if hasattr(self, '_body'):\\n                data = BytesIO(self._body)\\n            else:\\n                data = self\\n            try:\\n                self._post, self._files = self.parse_file_upload(self.META, data)\\n            except:\\n                self._mark_post_parse_error()\\n                raise\\n        elif self.META.get('CONTENT_TYPE', '').startswith('application/x-www-form-urlencoded'):\\n            self._post, self._files = QueryDict(self.body, encoding=self._encoding), MultiValueDict()\\n        else:\\n            self._post, self._files = QueryDict('', encoding=self._encoding), MultiValueDict()"
  },
  {
    "code": "def set_state(\\n        tasks: Iterable[BaseOperator],\\n        execution_date: datetime.datetime,\\n        upstream: bool = False,\\n        downstream: bool = False,\\n        future: bool = False,\\n        past: bool = False,\\n        state: str = State.SUCCESS,\\n        commit: bool = False,\\n        session=None):  \\n    if not tasks:\\n        return []\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"Received non-localized date {}\".format(execution_date))\\n    task_dags = {task.dag for task in tasks}\\n    if len(task_dags) > 1:\\n        raise ValueError(\"Received tasks from multiple DAGs: {}\".format(task_dags))\\n    dag = next(iter(task_dags))\\n    if dag is None:\\n        raise ValueError(\"Received tasks with no DAG\")\\n    dates = get_execution_dates(dag, execution_date, future, past)\\n    task_ids = list(find_task_relatives(tasks, downstream, upstream))\\n    confirmed_dates = verify_dag_run_integrity(dag, dates)\\n    sub_dag_run_ids = get_subdag_runs(dag, session, state, task_ids, commit, confirmed_dates)\\n    qry_dag = get_all_dag_task_query(dag, session, state, task_ids, confirmed_dates)\\n    if commit:\\n        tis_altered = qry_dag.with_for_update().all()\\n        if sub_dag_run_ids:\\n            qry_sub_dag = all_subdag_tasks_query(sub_dag_run_ids, session, state, confirmed_dates)\\n            tis_altered += qry_sub_dag.with_for_update().all()\\n        for task_instance in tis_altered:\\n            task_instance.state = state\\n    else:\\n        tis_altered = qry_dag.all()\\n        if sub_dag_run_ids:\\n            qry_sub_dag = all_subdag_tasks_query(sub_dag_run_ids, session, state, confirmed_dates)\\n            tis_altered += qry_sub_dag.all()\\n    return tis_altered",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_state(\\n        tasks: Iterable[BaseOperator],\\n        execution_date: datetime.datetime,\\n        upstream: bool = False,\\n        downstream: bool = False,\\n        future: bool = False,\\n        past: bool = False,\\n        state: str = State.SUCCESS,\\n        commit: bool = False,\\n        session=None):  \\n    if not tasks:\\n        return []\\n    if not timezone.is_localized(execution_date):\\n        raise ValueError(\"Received non-localized date {}\".format(execution_date))\\n    task_dags = {task.dag for task in tasks}\\n    if len(task_dags) > 1:\\n        raise ValueError(\"Received tasks from multiple DAGs: {}\".format(task_dags))\\n    dag = next(iter(task_dags))\\n    if dag is None:\\n        raise ValueError(\"Received tasks with no DAG\")\\n    dates = get_execution_dates(dag, execution_date, future, past)\\n    task_ids = list(find_task_relatives(tasks, downstream, upstream))\\n    confirmed_dates = verify_dag_run_integrity(dag, dates)\\n    sub_dag_run_ids = get_subdag_runs(dag, session, state, task_ids, commit, confirmed_dates)\\n    qry_dag = get_all_dag_task_query(dag, session, state, task_ids, confirmed_dates)\\n    if commit:\\n        tis_altered = qry_dag.with_for_update().all()\\n        if sub_dag_run_ids:\\n            qry_sub_dag = all_subdag_tasks_query(sub_dag_run_ids, session, state, confirmed_dates)\\n            tis_altered += qry_sub_dag.with_for_update().all()\\n        for task_instance in tis_altered:\\n            task_instance.state = state\\n    else:\\n        tis_altered = qry_dag.all()\\n        if sub_dag_run_ids:\\n            qry_sub_dag = all_subdag_tasks_query(sub_dag_run_ids, session, state, confirmed_dates)\\n            tis_altered += qry_sub_dag.all()\\n    return tis_altered"
  },
  {
    "code": "def _reset_internal_locks(self):\\n        self._cond.__init__(Lock())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-40089: Add _at_fork_reinit() method to locks (GH-19195)\\n\\nAdd a private _at_fork_reinit() method to _thread.Lock,\\n_thread.RLock, threading.RLock and threading.Condition classes:\\nreinitialize the lock after fork in the child process; reset the lock\\nto the unlocked state.\\n\\nRename also the private _reset_internal_locks() method of\\nthreading.Event to _at_fork_reinit().\\n\\n  from the limited C API.\\n  _at_fork_reinit() on self._tstate_lock rather than creating a new\\n  Python lock object.",
    "fixed_code": "def _reset_internal_locks(self, is_alive):\\n        self._started._at_fork_reinit()\\n        if is_alive:\\n            self._tstate_lock._at_fork_reinit()\\n            self._tstate_lock.acquire()\\n        else:\\n            self._is_stopped = True\\n            self._tstate_lock = None"
  },
  {
    "code": "def exec_config(fmg, paramgram):\\n    datagram = {\\n        \"scope\": {\\n            \"name\": paramgram[\"device_unique_name\"]\\n        },\\n        \"adom\": paramgram[\"adom\"],\\n        \"flags\": \"none\"\\n    }\\n    url = \"/securityconsole/install/device\"\\n    response = fmg.execute(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_device_config (#52765)",
    "fixed_code": "def exec_config(fmgr, paramgram):\\n    datagram = {\\n        \"scope\": {\\n            \"name\": paramgram[\"device_unique_name\"]\\n        },\\n        \"adom\": paramgram[\"adom\"],\\n        \"flags\": \"none\"\\n    }\\n    url = \"/securityconsole/install/device\"\\n    response = fmgr.process_request(url, datagram, FMGRMethods.EXEC)\\n    return response"
  },
  {
    "code": "def _homogenize(data, index, columns, dtype=None):\\n    from pandas.core.series import _sanitize_array\\n    homogenized = {}\\n    if dtype is not None:\\n        dtype = np.dtype(dtype)\\n    oindex = None\\n    for k in columns:\\n        if k not in data:\\n            if dtype is not None and issubclass(dtype.type, np.integer):\\n                continue\\n            v = np.empty(len(index), dtype=dtype)\\n            v.fill(nan)\\n        else:\\n            v = data[k]\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index, copy=False)\\n        else:\\n            if isinstance(v, dict):\\n                if oindex is None:\\n                    oindex = index.astype('O')\\n                v = lib.fast_multiget(v, oindex, default=np.nan)\\n            v = _sanitize_array(v, index, dtype=dtype, copy=False,\\n                                raise_cast_failure=False)\\n        homogenized[k] = v\\n    return homogenized",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Handle subclasses of dicts in DataFrame constructor, with tests",
    "fixed_code": "def _homogenize(data, index, columns, dtype=None):\\n    from pandas.core.series import _sanitize_array\\n    homogenized = {}\\n    if dtype is not None:\\n        dtype = np.dtype(dtype)\\n    oindex = None\\n    for k in columns:\\n        if k not in data:\\n            if dtype is not None and issubclass(dtype.type, np.integer):\\n                continue\\n            v = np.empty(len(index), dtype=dtype)\\n            v.fill(nan)\\n        else:\\n            v = data[k]\\n        if isinstance(v, Series):\\n            if dtype is not None:\\n                v = v.astype(dtype)\\n            if v.index is not index:\\n                v = v.reindex(index, copy=False)\\n        else:\\n            if isinstance(v, dict):\\n                if oindex is None:\\n                    oindex = index.astype('O')\\n                if type(v) != dict:\\n                    v = dict(v)\\n                v = lib.fast_multiget(v, oindex, default=np.nan)\\n            v = _sanitize_array(v, index, dtype=dtype, copy=False,\\n                                raise_cast_failure=False)\\n        homogenized[k] = v\\n    return homogenized"
  },
  {
    "code": "def deprecated(self):\\n        pattern = re.compile('.. deprecated:: ')\\n        return (self.name.startswith('pandas.Panel')\\n                or bool(pattern.search(self.summary))\\n                or bool(pattern.search(self.extended_summary)))\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def deprecated(self):\\n        pattern = re.compile('.. deprecated:: ')\\n        return (self.name.startswith('pandas.Panel')\\n                or bool(pattern.search(self.summary))\\n                or bool(pattern.search(self.extended_summary)))\\n    @property"
  },
  {
    "code": "def _boto3_conn(self, regions):\\n        ''\\n        credentials = self._get_credentials()\\n        iam_role_arn = self.iam_role_arn\\n        if not regions:\\n            try:\\n                client = self._get_connection(credentials)\\n                resp = client.describe_regions()\\n                regions = [x['RegionName'] for x in resp.get('Regions', [])]\\n            except botocore.exceptions.NoRegionError:\\n                pass\\n        if not regions:\\n            session = boto3.Session()\\n            regions = session.get_available_regions('ec2')\\n        if not regions:\\n            raise AnsibleError('Unable to get regions list from available methods, you must specify the \"regions\" option to continue.')\\n        for region in regions:\\n            connection = self._get_connection(credentials, region)\\n            try:\\n                if iam_role_arn is not None:\\n                    assumed_credentials = self._boto3_assume_role(credentials, region)\\n                else:\\n                    assumed_credentials = credentials\\n                connection = boto3.session.Session(profile_name=self.boto_profile).client('ec2', region, **assumed_credentials)\\n            except (botocore.exceptions.ProfileNotFound, botocore.exceptions.PartialCredentialsError) as e:\\n                if self.boto_profile:\\n                    try:\\n                        connection = boto3.session.Session(profile_name=self.boto_profile).client('ec2', region)\\n                    except (botocore.exceptions.ProfileNotFound, botocore.exceptions.PartialCredentialsError) as e:\\n                        raise AnsibleError(\"Insufficient credentials found: %s\" % to_native(e))\\n                else:\\n                    raise AnsibleError(\"Insufficient credentials found: %s\" % to_native(e))\\n            yield connection, region",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _boto3_conn(self, regions):\\n        ''\\n        credentials = self._get_credentials()\\n        iam_role_arn = self.iam_role_arn\\n        if not regions:\\n            try:\\n                client = self._get_connection(credentials)\\n                resp = client.describe_regions()\\n                regions = [x['RegionName'] for x in resp.get('Regions', [])]\\n            except botocore.exceptions.NoRegionError:\\n                pass\\n        if not regions:\\n            session = boto3.Session()\\n            regions = session.get_available_regions('ec2')\\n        if not regions:\\n            raise AnsibleError('Unable to get regions list from available methods, you must specify the \"regions\" option to continue.')\\n        for region in regions:\\n            connection = self._get_connection(credentials, region)\\n            try:\\n                if iam_role_arn is not None:\\n                    assumed_credentials = self._boto3_assume_role(credentials, region)\\n                else:\\n                    assumed_credentials = credentials\\n                connection = boto3.session.Session(profile_name=self.boto_profile).client('ec2', region, **assumed_credentials)\\n            except (botocore.exceptions.ProfileNotFound, botocore.exceptions.PartialCredentialsError) as e:\\n                if self.boto_profile:\\n                    try:\\n                        connection = boto3.session.Session(profile_name=self.boto_profile).client('ec2', region)\\n                    except (botocore.exceptions.ProfileNotFound, botocore.exceptions.PartialCredentialsError) as e:\\n                        raise AnsibleError(\"Insufficient credentials found: %s\" % to_native(e))\\n                else:\\n                    raise AnsibleError(\"Insufficient credentials found: %s\" % to_native(e))\\n            yield connection, region"
  },
  {
    "code": "def _strip_annotations(t):\\n    if isinstance(t, _AnnotatedAlias):\\n        return _strip_annotations(t.__origin__)\\n    if isinstance(t, _GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return t.copy_with(stripped_args)\\n    if isinstance(t, GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return GenericAlias(t.__origin__, stripped_args)\\n    if isinstance(t, types.Union):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return functools.reduce(operator.or_, stripped_args)\\n    return t",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _strip_annotations(t):\\n    if isinstance(t, _AnnotatedAlias):\\n        return _strip_annotations(t.__origin__)\\n    if isinstance(t, _GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return t.copy_with(stripped_args)\\n    if isinstance(t, GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return GenericAlias(t.__origin__, stripped_args)\\n    if isinstance(t, types.Union):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return functools.reduce(operator.or_, stripped_args)\\n    return t"
  },
  {
    "code": "def run_server(self):\\n\\t\\tif cfg.CONF.pydev_worker_debug_host:\\n\\t\\t\\tutils.setup_remote_pydev_debug(cfg.CONF.pydev_worker_debug_host,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   cfg.CONF.pydev_worker_debug_port)\\n\\t\\teventlet.wsgi.HttpProtocol.default_request_version = \"HTTP/1.0\"\\n\\t\\ttry:\\n\\t\\t\\teventlet.hubs.use_hub('poll')\\n\\t\\texcept Exception:\\n\\t\\t\\tmsg = _(\"eventlet 'poll' hub is not available on this platform\")\\n\\t\\t\\traise exception.WorkerCreationFailure(reason=msg)\\n\\t\\tself.pool = self.create_pool()\\n\\t\\ttry:\\n\\t\\t\\teventlet.wsgi.server(self.sock,\\n\\t\\t\\t\\t\\t\\t\\t\\t self.application,\\n\\t\\t\\t\\t\\t\\t\\t\\t log=WritableLogger(self.logger),\\n\\t\\t\\t\\t\\t\\t\\t\\t custom_pool=self.pool)\\n\\t\\texcept socket.error as err:\\n\\t\\t\\tif err[0] != errno.EINVAL:\\n\\t\\t\\t\\traise\\n\\t\\tself.pool.waitall()",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Do not send traceback to clients by default\\n\\neventlet.wsgi.server contains a \"debug\" param which is True by default.\\nThis sends tracebacks to the client on 500 errors, which is not\\ndesirable for security reasons.\\n\\nSet this to be False by default.\\n\\nFixes bug 1192132",
    "fixed_code": "def run_server(self):\\n\\t\\tif cfg.CONF.pydev_worker_debug_host:\\n\\t\\t\\tutils.setup_remote_pydev_debug(cfg.CONF.pydev_worker_debug_host,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   cfg.CONF.pydev_worker_debug_port)\\n\\t\\teventlet.wsgi.HttpProtocol.default_request_version = \"HTTP/1.0\"\\n\\t\\ttry:\\n\\t\\t\\teventlet.hubs.use_hub('poll')\\n\\t\\texcept Exception:\\n\\t\\t\\tmsg = _(\"eventlet 'poll' hub is not available on this platform\")\\n\\t\\t\\traise exception.WorkerCreationFailure(reason=msg)\\n\\t\\tself.pool = self.create_pool()\\n\\t\\ttry:\\n\\t\\t\\teventlet.wsgi.server(self.sock,\\n\\t\\t\\t\\t\\t\\t\\t\\t self.application,\\n\\t\\t\\t\\t\\t\\t\\t\\t log=WritableLogger(self.logger),\\n\\t\\t\\t\\t\\t\\t\\t\\t custom_pool=self.pool,\\n\\t\\t\\t\\t\\t\\t\\t\\t debug=False)\\n\\t\\texcept socket.error as err:\\n\\t\\t\\tif err[0] != errno.EINVAL:\\n\\t\\t\\t\\traise\\n\\t\\tself.pool.waitall()"
  },
  {
    "code": "def _join_on(self, other, on, lsuffix, rsuffix):\\n        if isinstance(other, Series):\\n            assert(other.name is not None)\\n            other = DataFrame({other.name : other})\\n        if len(other.index) == 0:\\n            return self\\n        new_data = self._data.join_on(other._data, self[on], axis=1,\\n                                      lsuffix=lsuffix, rsuffix=rsuffix)\\n        return self._constructor(new_data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _join_on(self, other, on, lsuffix, rsuffix):\\n        if isinstance(other, Series):\\n            assert(other.name is not None)\\n            other = DataFrame({other.name : other})\\n        if len(other.index) == 0:\\n            return self\\n        new_data = self._data.join_on(other._data, self[on], axis=1,\\n                                      lsuffix=lsuffix, rsuffix=rsuffix)\\n        return self._constructor(new_data)"
  },
  {
    "code": "def tell(self):\\n        if self._base_content_is_iter:\\n            raise Exception(\"This %s instance cannot tell its position\" % self.__class__)\\n        return sum([len(str(chunk)) for chunk in self._container])\\nclass HttpResponseRedirect(HttpResponse):\\n    status_code = 302",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #18561 -- Made HttpResponse.tell() support non-ascii chars",
    "fixed_code": "def tell(self):\\n        if self._base_content_is_iter:\\n            raise Exception(\"This %s instance cannot tell its position\" % self.__class__)\\n        return sum([len(chunk) for chunk in self])\\nclass HttpResponseRedirect(HttpResponse):\\n    status_code = 302"
  },
  {
    "code": "def sequence_to_dt64ns(\\n    data,\\n    dtype=None,\\n    copy=False,\\n    tz=None,\\n    dayfirst=False,\\n    yearfirst=False,\\n    ambiguous=\"raise\",\\n):\\n    inferred_freq = None\\n    dtype = _validate_dt64_dtype(dtype)\\n    tz = timezones.maybe_get_tz(tz)\\n    if not hasattr(data, \"dtype\"):\\n        if np.ndim(data) == 0:\\n            data = list(data)\\n        data = np.asarray(data)\\n        copy = False\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    if isinstance(data, ABCPandasArray):\\n        data = data.to_numpy()\\n    if hasattr(data, \"freq\"):\\n        inferred_freq = data.freq\\n    tz = validate_tz_from_dtype(dtype, tz)\\n    if isinstance(data, ABCIndexClass):\\n        if data.nlevels > 1:\\n            raise TypeError(\"Cannot create a DatetimeArray from a MultiIndex.\")\\n        data = data._data\\n    data, copy = maybe_convert_dtype(data, copy)\\n    data_dtype = getattr(data, \"dtype\", None)\\n    if is_object_dtype(data_dtype) or is_string_dtype(data_dtype):\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\\n            data = data.astype(np.int64)\\n        else:\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data, dayfirst=dayfirst, yearfirst=yearfirst\\n            )\\n            if tz and inferred_tz:\\n                data = tzconversion.tz_convert_from_utc(data.view(\"i8\"), tz)\\n                data = data.view(DT64NS_DTYPE)\\n            elif inferred_tz:\\n                tz = inferred_tz\\n        data_dtype = data.dtype\\n    if is_datetime64tz_dtype(data_dtype):\\n        tz = _maybe_infer_tz(tz, data.tz)\\n        result = data._data\\n    elif is_datetime64_dtype(data_dtype):\\n        data = getattr(data, \"_data\", data)\\n        if data.dtype != DT64NS_DTYPE:\\n            data = conversion.ensure_datetime64ns(data)\\n        if tz is not None:\\n            tz = timezones.maybe_get_tz(tz)\\n            data = tzconversion.tz_localize_to_utc(\\n                data.view(\"i8\"), tz, ambiguous=ambiguous\\n            )\\n            data = data.view(DT64NS_DTYPE)\\n        assert data.dtype == DT64NS_DTYPE, data.dtype\\n        result = data\\n    else:\\n        if tz:\\n            tz = timezones.maybe_get_tz(tz)\\n        if data.dtype != INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        result = data.view(DT64NS_DTYPE)\\n    if copy:\\n        result = result.copy()\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \"M8[ns]\", result.dtype\\n    validate_tz_from_dtype(dtype, tz)\\n    return result, tz, inferred_freq",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: Fix Index construction from Sparse[\"datetime64[ns]\"] (#38332)",
    "fixed_code": "def sequence_to_dt64ns(\\n    data,\\n    dtype=None,\\n    copy=False,\\n    tz=None,\\n    dayfirst=False,\\n    yearfirst=False,\\n    ambiguous=\"raise\",\\n):\\n    inferred_freq = None\\n    dtype = _validate_dt64_dtype(dtype)\\n    tz = timezones.maybe_get_tz(tz)\\n    if not hasattr(data, \"dtype\"):\\n        if np.ndim(data) == 0:\\n            data = list(data)\\n        data = np.asarray(data)\\n        copy = False\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    if isinstance(data, ABCPandasArray):\\n        data = data.to_numpy()\\n    if hasattr(data, \"freq\"):\\n        inferred_freq = data.freq\\n    tz = validate_tz_from_dtype(dtype, tz)\\n    if isinstance(data, ABCIndexClass):\\n        if data.nlevels > 1:\\n            raise TypeError(\"Cannot create a DatetimeArray from a MultiIndex.\")\\n        data = data._data\\n    data, copy = maybe_convert_dtype(data, copy)\\n    data_dtype = getattr(data, \"dtype\", None)\\n    if (\\n        is_object_dtype(data_dtype)\\n        or is_string_dtype(data_dtype)\\n        or is_sparse(data_dtype)\\n    ):\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\\n            data = data.astype(np.int64)\\n        else:\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data, dayfirst=dayfirst, yearfirst=yearfirst\\n            )\\n            if tz and inferred_tz:\\n                data = tzconversion.tz_convert_from_utc(data.view(\"i8\"), tz)\\n                data = data.view(DT64NS_DTYPE)\\n            elif inferred_tz:\\n                tz = inferred_tz\\n        data_dtype = data.dtype\\n    if is_datetime64tz_dtype(data_dtype):\\n        tz = _maybe_infer_tz(tz, data.tz)\\n        result = data._data\\n    elif is_datetime64_dtype(data_dtype):\\n        data = getattr(data, \"_data\", data)\\n        if data.dtype != DT64NS_DTYPE:\\n            data = conversion.ensure_datetime64ns(data)\\n        if tz is not None:\\n            tz = timezones.maybe_get_tz(tz)\\n            data = tzconversion.tz_localize_to_utc(\\n                data.view(\"i8\"), tz, ambiguous=ambiguous\\n            )\\n            data = data.view(DT64NS_DTYPE)\\n        assert data.dtype == DT64NS_DTYPE, data.dtype\\n        result = data\\n    else:\\n        if tz:\\n            tz = timezones.maybe_get_tz(tz)\\n        if data.dtype != INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        result = data.view(DT64NS_DTYPE)\\n    if copy:\\n        result = result.copy()\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \"M8[ns]\", result.dtype\\n    validate_tz_from_dtype(dtype, tz)\\n    return result, tz, inferred_freq"
  },
  {
    "code": "def _fit(self, X, y, alpha, C, loss, learning_rate, coef_init=None,\\n             intercept_init=None, sample_weight=None):\\n        if hasattr(self, \"classes_\"):\\n            self.classes_ = None\\n        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\")\\n        n_samples, n_features = X.shape\\n        classes = np.unique(y)\\n        if self.warm_start and self.coef_ is not None:\\n            if coef_init is None:\\n                coef_init = self.coef_\\n            if intercept_init is None:\\n                intercept_init = self.intercept_\\n        else:\\n            self.coef_ = None\\n            self.intercept_ = None\\n        if self.average > 0:\\n            self.standard_coef_ = self.coef_\\n            self.standard_intercept_ = self.intercept_\\n            self.average_coef_ = None\\n            self.average_intercept_ = None\\n        self.t_ = None\\n        self._partial_fit(X, y, alpha, C, loss, learning_rate, self.n_iter,\\n                          classes, sample_weight, coef_init, intercept_init)\\n        return self\\n    def _fit_binary(self, X, y, alpha, C, sample_weight,\\n                    learning_rate, n_iter):\\n        coef, intercept = fit_binary(self, 1, X, y, alpha, C,\\n                                     learning_rate, n_iter,\\n                                     self._expanded_class_weight[1],\\n                                     self._expanded_class_weight[0],\\n                                     sample_weight)\\n        self.t_ += n_iter * X.shape[0]\\n        if self.average > 0:\\n            if self.average <= self.t_ - 1:\\n                self.coef_ = self.average_coef_.reshape(1, -1)\\n                self.intercept_ = self.average_intercept_\\n            else:\\n                self.coef_ = self.standard_coef_.reshape(1, -1)\\n                self.standard_intercept_ = np.atleast_1d(intercept)\\n                self.intercept_ = self.standard_intercept_\\n        else:\\n            self.coef_ = coef.reshape(1, -1)\\n            self.intercept_ = np.atleast_1d(intercept)\\n    def _fit_multiclass(self, X, y, alpha, C, learning_rate,\\n                        sample_weight, n_iter):\\n        result = Parallel(n_jobs=self.n_jobs, backend=\"threading\",\\n                          verbose=self.verbose)(\\n            delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate,\\n                                n_iter, self._expanded_class_weight[i], 1.,\\n                                sample_weight)\\n            for i in range(len(self.classes_)))\\n        for i, (_, intercept) in enumerate(result):\\n            self.intercept_[i] = intercept\\n        self.t_ += n_iter * X.shape[0]\\n        if self.average > 0:\\n            if self.average <= self.t_ - 1.0:\\n                self.coef_ = self.average_coef_\\n                self.intercept_ = self.average_intercept_\\n            else:\\n                self.coef_ = self.standard_coef_\\n                self.standard_intercept_ = np.atleast_1d(intercept)\\n                self.intercept_ = self.standard_intercept_",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fit(self, X, y, alpha, C, loss, learning_rate, coef_init=None,\\n             intercept_init=None, sample_weight=None):\\n        if hasattr(self, \"classes_\"):\\n            self.classes_ = None\\n        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\")\\n        n_samples, n_features = X.shape\\n        classes = np.unique(y)\\n        if self.warm_start and self.coef_ is not None:\\n            if coef_init is None:\\n                coef_init = self.coef_\\n            if intercept_init is None:\\n                intercept_init = self.intercept_\\n        else:\\n            self.coef_ = None\\n            self.intercept_ = None\\n        if self.average > 0:\\n            self.standard_coef_ = self.coef_\\n            self.standard_intercept_ = self.intercept_\\n            self.average_coef_ = None\\n            self.average_intercept_ = None\\n        self.t_ = None\\n        self._partial_fit(X, y, alpha, C, loss, learning_rate, self.n_iter,\\n                          classes, sample_weight, coef_init, intercept_init)\\n        return self\\n    def _fit_binary(self, X, y, alpha, C, sample_weight,\\n                    learning_rate, n_iter):\\n        coef, intercept = fit_binary(self, 1, X, y, alpha, C,\\n                                     learning_rate, n_iter,\\n                                     self._expanded_class_weight[1],\\n                                     self._expanded_class_weight[0],\\n                                     sample_weight)\\n        self.t_ += n_iter * X.shape[0]\\n        if self.average > 0:\\n            if self.average <= self.t_ - 1:\\n                self.coef_ = self.average_coef_.reshape(1, -1)\\n                self.intercept_ = self.average_intercept_\\n            else:\\n                self.coef_ = self.standard_coef_.reshape(1, -1)\\n                self.standard_intercept_ = np.atleast_1d(intercept)\\n                self.intercept_ = self.standard_intercept_\\n        else:\\n            self.coef_ = coef.reshape(1, -1)\\n            self.intercept_ = np.atleast_1d(intercept)\\n    def _fit_multiclass(self, X, y, alpha, C, learning_rate,\\n                        sample_weight, n_iter):\\n        result = Parallel(n_jobs=self.n_jobs, backend=\"threading\",\\n                          verbose=self.verbose)(\\n            delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate,\\n                                n_iter, self._expanded_class_weight[i], 1.,\\n                                sample_weight)\\n            for i in range(len(self.classes_)))\\n        for i, (_, intercept) in enumerate(result):\\n            self.intercept_[i] = intercept\\n        self.t_ += n_iter * X.shape[0]\\n        if self.average > 0:\\n            if self.average <= self.t_ - 1.0:\\n                self.coef_ = self.average_coef_\\n                self.intercept_ = self.average_intercept_\\n            else:\\n                self.coef_ = self.standard_coef_\\n                self.standard_intercept_ = np.atleast_1d(intercept)\\n                self.intercept_ = self.standard_intercept_"
  },
  {
    "code": "def average_name(self, var):\\n\\treturn var.op.name + \"/\" + self._name",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Ensure ExponentialMovingAverage returns correct average_name when used in scope Fixes #2740 Change: 126251978",
    "fixed_code": "def average_name(self, var):\\n\\tif var in self._averages:\\n\\t  return self._averages[var].op.name\\n\\treturn ops.get_default_graph().unique_name(\\n\\t\\tvar.op.name + \"/\" + self._name, mark_as_used=False)"
  },
  {
    "code": "def _get_job(self, project_name, job_id):\\n        job_name = 'projects/{}/jobs/{}'.format(project_name, job_id)\\n        request = self._cloudml.projects().jobs().get(name=job_name)\\n        while True:\\n            try:\\n                return request.execute()\\n            except errors.HttpError as e:\\n                if e.resp.status == 429:\\n                    time.sleep(30)\\n                else:\\n                    logging.error('Failed to get CloudML job: {}'.format(e))\\n                    raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-1401] Standardize cloud ml operator arguments\\n\\nStandardize on project_id, to be consistent with\\nother cloud operators,\\nbetter-supporting default arguments.\\n\\nThis is one of multiple commits that will be\\nrequired to resolve\\nAIRFLOW-1401.\\n\\nCloses #2439 from peterjdolan/cloudml_project_id",
    "fixed_code": "def _get_job(self, project_id, job_id):\\n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\\n        request = self._cloudml.projects().jobs().get(name=job_name)\\n        while True:\\n            try:\\n                return request.execute()\\n            except errors.HttpError as e:\\n                if e.resp.status == 429:\\n                    time.sleep(30)\\n                else:\\n                    logging.error('Failed to get CloudML job: {}'.format(e))\\n                    raise"
  },
  {
    "code": "def purge():\\n    \"Clear the regular expression caches\"\\n    _compile.cache_clear()\\n    _compile_repl.cache_clear()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def purge():\\n    \"Clear the regular expression caches\"\\n    _compile.cache_clear()\\n    _compile_repl.cache_clear()"
  },
  {
    "code": "def add_lazy_relation(cls, field, relation, operation):\\n    if relation == RECURSIVE_RELATIONSHIP_CONSTANT:\\n        app_label = cls._meta.app_label\\n        model_name = cls.__name__\\n    else:\\n        if isinstance(relation, six.string_types):\\n            try:\\n                app_label, model_name = relation.split(\".\")\\n            except ValueError:\\n                app_label = cls._meta.app_label\\n                model_name = relation\\n        else:\\n            app_label = relation._meta.app_label\\n            model_name = relation._meta.object_name\\n    model = get_model(app_label, model_name,\\n                      seed_cache=False, only_installed=False)\\n    if model:\\n        operation(field, model, cls)\\n    else:\\n        key = (app_label, model_name)\\n        value = (cls, field, operation)\\n        pending_lookups.setdefault(key, []).append(value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Rest of the _meta.app_cache stuff. Schema tests work now.",
    "fixed_code": "def add_lazy_relation(cls, field, relation, operation):\\n    if relation == RECURSIVE_RELATIONSHIP_CONSTANT:\\n        app_label = cls._meta.app_label\\n        model_name = cls.__name__\\n    else:\\n        if isinstance(relation, six.string_types):\\n            try:\\n                app_label, model_name = relation.split(\".\")\\n            except ValueError:\\n                app_label = cls._meta.app_label\\n                model_name = relation\\n        else:\\n            app_label = relation._meta.app_label\\n            model_name = relation._meta.object_name\\n    model = cls._meta.app_cache.get_model(app_label, model_name,\\n                      seed_cache=False, only_installed=False)\\n    if model:\\n        operation(field, model, cls)\\n    else:\\n        key = (app_label, model_name)\\n        value = (cls, field, operation)\\n        cls._meta.app_cache.pending_lookups.setdefault(key, []).append(value)"
  },
  {
    "code": "def clean_interp_method(method, **kwargs):\\n    order = kwargs.get('order')\\n    valid = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\\n             'quadratic', 'cubic', 'barycentric', 'polynomial', 'krogh',\\n             'piecewise_polynomial', 'pchip', 'akima', 'spline',\\n             'from_derivatives']\\n    if method in ('spline', 'polynomial') and order is None:\\n        raise ValueError(\"You must specify the order of the spline or \"\\n                         \"polynomial.\")\\n    if method not in valid:\\n        raise ValueError(\"method must be one of {0}.\"\\n                         \"Got '{1}' instead.\".format(valid, method))\\n    return method",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def clean_interp_method(method, **kwargs):\\n    order = kwargs.get('order')\\n    valid = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\\n             'quadratic', 'cubic', 'barycentric', 'polynomial', 'krogh',\\n             'piecewise_polynomial', 'pchip', 'akima', 'spline',\\n             'from_derivatives']\\n    if method in ('spline', 'polynomial') and order is None:\\n        raise ValueError(\"You must specify the order of the spline or \"\\n                         \"polynomial.\")\\n    if method not in valid:\\n        raise ValueError(\"method must be one of {0}.\"\\n                         \"Got '{1}' instead.\".format(valid, method))\\n    return method"
  },
  {
    "code": "def supports_json_field(self):\\n        with self.connection.cursor() as cursor:\\n            try:\\n                with transaction.atomic(self.connection.alias):\\n                    cursor.execute('SELECT JSON(\\'{\"a\": \"b\"}\\')')\\n            except OperationalError:\\n                return False\\n        return True\\n    can_introspect_json_field = property(operator.attrgetter('supports_json_field'))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def supports_json_field(self):\\n        with self.connection.cursor() as cursor:\\n            try:\\n                with transaction.atomic(self.connection.alias):\\n                    cursor.execute('SELECT JSON(\\'{\"a\": \"b\"}\\')')\\n            except OperationalError:\\n                return False\\n        return True\\n    can_introspect_json_field = property(operator.attrgetter('supports_json_field'))"
  },
  {
    "code": "def process_response(self, request, response):\\n\\t\\tif (self.sts_seconds and request.is_secure() and\\n\\t\\t\\t\\t'strict-transport-security' not in response):\\n\\t\\t\\tsts_header = \"max-age=%s\" % self.sts_seconds\\n\\t\\t\\tif self.sts_include_subdomains:\\n\\t\\t\\t\\tsts_header = sts_header + \"; includeSubDomains\"\\n\\t\\t\\tif self.sts_preload:\\n\\t\\t\\t\\tsts_header = sts_header + \"; preload\"\\n\\t\\t\\tresponse[\"strict-transport-security\"] = sts_header\\n\\t\\tif self.content_type_nosniff:\\n\\t\\t\\tresponse.setdefault('x-content-type-options', 'nosniff')\\n\\t\\tif self.xss_filter:\\n\\t\\t\\tresponse.setdefault('x-xss-protection', '1; mode=block')\\n\\t\\treturn response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Capitalized SecurityMiddleware headers for consistency with other headers.\\n\\n(No behavior change since HTTP headers are case insensitive.)",
    "fixed_code": "def process_response(self, request, response):\\n\\t\\tif (self.sts_seconds and request.is_secure() and\\n\\t\\t\\t\\t'Strict-Transport-Security' not in response):\\n\\t\\t\\tsts_header = \"max-age=%s\" % self.sts_seconds\\n\\t\\t\\tif self.sts_include_subdomains:\\n\\t\\t\\t\\tsts_header = sts_header + \"; includeSubDomains\"\\n\\t\\t\\tif self.sts_preload:\\n\\t\\t\\t\\tsts_header = sts_header + \"; preload\"\\n\\t\\t\\tresponse['Strict-Transport-Security'] = sts_header\\n\\t\\tif self.content_type_nosniff:\\n\\t\\t\\tresponse.setdefault('X-Content-Type-Options', 'nosniff')\\n\\t\\tif self.xss_filter:\\n\\t\\t\\tresponse.setdefault('X-XSS-Protection', '1; mode=block')\\n\\t\\treturn response"
  },
  {
    "code": "def map_config_to_obj(module):\\n    output = run_commands(module, ['show banner %s' % module.params['banner']])\\n    obj = {'banner': module.params['banner'], 'state': 'absent'}\\n    if output:\\n        if module.params['transport'] == 'cli':\\n            obj['text'] = output[0]\\n        else:\\n            if module.params['banner'] == 'login':\\n                banner_response_key = 'loginBanner'\\n            else:\\n                banner_response_key = 'motd'\\n            if isinstance(output[0], dict) and banner_response_key in output[0].keys():\\n                obj['text'] = output[0]\\n        obj['state'] = 'present'\\n    return obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_config_to_obj(module):\\n    output = run_commands(module, ['show banner %s' % module.params['banner']])\\n    obj = {'banner': module.params['banner'], 'state': 'absent'}\\n    if output:\\n        if module.params['transport'] == 'cli':\\n            obj['text'] = output[0]\\n        else:\\n            if module.params['banner'] == 'login':\\n                banner_response_key = 'loginBanner'\\n            else:\\n                banner_response_key = 'motd'\\n            if isinstance(output[0], dict) and banner_response_key in output[0].keys():\\n                obj['text'] = output[0]\\n        obj['state'] = 'present'\\n    return obj"
  },
  {
    "code": "def _try_cast(arr, dtype: Optional[DtypeObj], copy: bool, raise_cast_failure: bool):\\n    if isinstance(arr, np.ndarray):\\n        if maybe_castable(arr) and not copy and dtype is None:\\n            return arr\\n    if isinstance(dtype, ExtensionDtype) and (dtype.kind != \"M\" or is_sparse(dtype)):\\n        array_type = dtype.construct_array_type()._from_sequence\\n        subarr = array_type(arr, dtype=dtype, copy=copy)\\n        return subarr\\n    try:\\n        if is_integer_dtype(dtype):\\n            maybe_cast_to_integer_array(arr, dtype)\\n            subarr = arr\\n        else:\\n            subarr = maybe_cast_to_datetime(arr, dtype)\\n        if is_object_dtype(dtype) and (\\n            is_list_like(subarr)\\n            and not (is_iterator(subarr) or isinstance(subarr, np.ndarray))\\n        ):\\n            subarr = construct_1d_object_array_from_listlike(subarr)\\n        elif not is_extension_array_dtype(subarr):\\n            subarr = construct_1d_ndarray_preserving_na(subarr, dtype, copy=copy)\\n    except OutOfBoundsDatetime:\\n        raise\\n    except (ValueError, TypeError):\\n        if dtype is not None and raise_cast_failure:\\n            raise\\n        else:\\n            subarr = np.array(arr, dtype=object, copy=copy)\\n    return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: collected dtypes.cast and construction simplifications (#38629)",
    "fixed_code": "def _try_cast(arr, dtype: Optional[DtypeObj], copy: bool, raise_cast_failure: bool):\\n    if isinstance(arr, np.ndarray):\\n        if maybe_castable(arr) and not copy and dtype is None:\\n            return arr\\n    if isinstance(dtype, ExtensionDtype) and (dtype.kind != \"M\" or is_sparse(dtype)):\\n        array_type = dtype.construct_array_type()._from_sequence\\n        subarr = array_type(arr, dtype=dtype, copy=copy)\\n        return subarr\\n    if is_object_dtype(dtype) and not isinstance(arr, np.ndarray):\\n        subarr = construct_1d_object_array_from_listlike(arr)\\n        return subarr\\n    try:\\n        if is_integer_dtype(dtype):\\n            maybe_cast_to_integer_array(arr, dtype)\\n            subarr = arr\\n        else:\\n            subarr = maybe_cast_to_datetime(arr, dtype)\\n        if not isinstance(subarr, (ABCExtensionArray, ABCIndex)):\\n            subarr = construct_1d_ndarray_preserving_na(subarr, dtype, copy=copy)\\n    except OutOfBoundsDatetime:\\n        raise\\n    except (ValueError, TypeError):\\n        if dtype is not None and raise_cast_failure:\\n            raise\\n        else:\\n            subarr = np.array(arr, dtype=object, copy=copy)\\n    return subarr"
  },
  {
    "code": "def _meets_requirements(self, version, requirements, parent):\\n\\t\\top_map = {\\n\\t\\t\\t'!=': operator.ne,\\n\\t\\t\\t'==': operator.eq,\\n\\t\\t\\t'=': operator.eq,\\n\\t\\t\\t'>=': operator.ge,\\n\\t\\t\\t'>': operator.gt,\\n\\t\\t\\t'<=': operator.le,\\n\\t\\t\\t'<': operator.lt,\\n\\t\\t}\\n\\t\\tfor req in list(requirements.split(',')):\\n\\t\\t\\top_pos = 2 if len(req) > 1 and req[1] == '=' else 1\\n\\t\\t\\top = op_map.get(req[:op_pos])\\n\\t\\t\\trequirement = req[op_pos:]\\n\\t\\t\\tif not op:\\n\\t\\t\\t\\trequirement = req\\n\\t\\t\\t\\top = operator.eq\\n\\t\\t\\t\\tif parent and version == '*' and requirement != '*':\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\telif requirement == '*' or version == '*':\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\tif not op(LooseVersion(version), LooseVersion(requirement)):\\n\\t\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\t@staticmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "galaxy - Fix collection install dep resolver for bad versions (#67405)",
    "fixed_code": "def _meets_requirements(self, version, requirements, parent):\\n\\t\\top_map = {\\n\\t\\t\\t'!=': operator.ne,\\n\\t\\t\\t'==': operator.eq,\\n\\t\\t\\t'=': operator.eq,\\n\\t\\t\\t'>=': operator.ge,\\n\\t\\t\\t'>': operator.gt,\\n\\t\\t\\t'<=': operator.le,\\n\\t\\t\\t'<': operator.lt,\\n\\t\\t}\\n\\t\\tfor req in list(requirements.split(',')):\\n\\t\\t\\top_pos = 2 if len(req) > 1 and req[1] == '=' else 1\\n\\t\\t\\top = op_map.get(req[:op_pos])\\n\\t\\t\\trequirement = req[op_pos:]\\n\\t\\t\\tif not op:\\n\\t\\t\\t\\trequirement = req\\n\\t\\t\\t\\top = operator.eq\\n\\t\\t\\tif parent and version == '*' and requirement != '*':\\n\\t\\t\\t\\tdisplay.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"install does not have a version set, the collection may not work.\"\\n\\t\\t\\t\\t\\t\\t\\t\\t% (to_text(self), req, parent))\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telif requirement == '*' or version == '*':\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif not op(LooseVersion(version), LooseVersion(requirement)):\\n\\t\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\t@staticmethod"
  },
  {
    "code": "def _maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = tslib.iNaT\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, (np.datetime64, np.timedelta64)):\\n        if isnull(fill_value):\\n            fill_value = tslib.iNaT\\n        else:\\n            if issubclass(dtype.type, np.datetime64):\\n                try:\\n                    fill_value = lib.Timestamp(fill_value).value\\n                except:\\n                    fill_value = tslib.iNaT\\n            else:\\n                fill_value = tslib.iNaT\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    else:\\n        dtype = np.object_\\n    if issubclass(np.dtype(dtype).type, compat.string_types):\\n        dtype = np.object_\\n    return dtype, fill_value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Regression in merging Categorical and object dtypes (GH9426)",
    "fixed_code": "def _maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = tslib.iNaT\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, (np.datetime64, np.timedelta64)):\\n        if isnull(fill_value):\\n            fill_value = tslib.iNaT\\n        else:\\n            if issubclass(dtype.type, np.datetime64):\\n                try:\\n                    fill_value = lib.Timestamp(fill_value).value\\n                except:\\n                    fill_value = tslib.iNaT\\n            else:\\n                fill_value = tslib.iNaT\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    else:\\n        dtype = np.object_\\n    if is_categorical_dtype(dtype):\\n        dtype = dtype\\n    elif issubclass(np.dtype(dtype).type, compat.string_types):\\n        dtype = np.object_\\n    return dtype, fill_value"
  },
  {
    "code": "def g_lasso(X, alpha, cov_init=None, mode='cd', tol=1e-4,\\n            max_iter=100, verbose=False, return_costs=False):\\n    _, n_features = X.shape\\n    mle = empirical_covariance(X)\\n    if alpha == 0:\\n        return mle\\n    if cov_init is None:\\n        covariance_ = mle.copy()\\n    else:\\n        covariance_ = cov_init.copy()\\n        covariance_.flat[::n_features + 1] = mle.flat[::n_features + 1]\\n    indices = np.arange(n_features)\\n    precision_ = linalg.inv(covariance_)\\n    costs = list()\\n    for i in xrange(max_iter):\\n        for idx in xrange(n_features):\\n            sub_covariance = covariance_[indices != idx].T[indices != idx]\\n            row = mle[idx, indices != idx]\\n            if mode == 'cd':\\n                coefs = -precision_[indices != idx, idx]/precision_[idx, idx]\\n                coefs, _, _ = cd_fast.enet_coordinate_descent_gram(coefs,\\n                                            alpha, 0, sub_covariance,\\n                                            row, row, max_iter, tol)\\n            else:\\n                _, _, coefs = lars_path(sub_covariance, row,\\n                                        Xy=row, Gram=sub_covariance,\\n                                        alpha_min=alpha/(n_features-1),\\n                                        copy_Gram=True,\\n                                        method='lars')\\n                coefs = coefs[:, -1]\\n            precision_[idx, idx] = 1./(covariance_[idx, idx] -\\n                        np.dot(covariance_[indices != idx, idx], coefs))\\n            precision_[indices != idx, idx] = -precision_[idx, idx]*coefs\\n            precision_[idx, indices != idx] = -precision_[idx, idx]*coefs\\n            coefs = np.dot(sub_covariance, coefs)\\n            covariance_[idx, indices != idx] = coefs\\n            covariance_[indices != idx, idx] = coefs\\n        d_gap = _dual_gap(mle, precision_, alpha)\\n        if verbose or return_costs:\\n            cost = _objective(mle, precision_, alpha)\\n            if verbose:\\n                print '[g_lasso] Iteration % 3i, cost %.4f, dual gap %.3e' % (\\n                                                    i, cost, d_gap)\\n            if return_costs:\\n                costs.append((cost, d_gap))\\n        if np.abs(d_gap) < tol:\\n            break\\n    else:\\n        warnings.warn('g_lasso: did not converge after %i iteration:'\\n                        'dual gap: %.3e' % (max_iter, d_gap))\\n    if return_costs:\\n        return covariance_, precision_, costs\\n    return covariance_, precision_",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def g_lasso(X, alpha, cov_init=None, mode='cd', tol=1e-4,\\n            max_iter=100, verbose=False, return_costs=False):\\n    _, n_features = X.shape\\n    mle = empirical_covariance(X)\\n    if alpha == 0:\\n        return mle\\n    if cov_init is None:\\n        covariance_ = mle.copy()\\n    else:\\n        covariance_ = cov_init.copy()\\n        covariance_.flat[::n_features + 1] = mle.flat[::n_features + 1]\\n    indices = np.arange(n_features)\\n    precision_ = linalg.inv(covariance_)\\n    costs = list()\\n    for i in xrange(max_iter):\\n        for idx in xrange(n_features):\\n            sub_covariance = covariance_[indices != idx].T[indices != idx]\\n            row = mle[idx, indices != idx]\\n            if mode == 'cd':\\n                coefs = -precision_[indices != idx, idx]/precision_[idx, idx]\\n                coefs, _, _ = cd_fast.enet_coordinate_descent_gram(coefs,\\n                                            alpha, 0, sub_covariance,\\n                                            row, row, max_iter, tol)\\n            else:\\n                _, _, coefs = lars_path(sub_covariance, row,\\n                                        Xy=row, Gram=sub_covariance,\\n                                        alpha_min=alpha/(n_features-1),\\n                                        copy_Gram=True,\\n                                        method='lars')\\n                coefs = coefs[:, -1]\\n            precision_[idx, idx] = 1./(covariance_[idx, idx] -\\n                        np.dot(covariance_[indices != idx, idx], coefs))\\n            precision_[indices != idx, idx] = -precision_[idx, idx]*coefs\\n            precision_[idx, indices != idx] = -precision_[idx, idx]*coefs\\n            coefs = np.dot(sub_covariance, coefs)\\n            covariance_[idx, indices != idx] = coefs\\n            covariance_[indices != idx, idx] = coefs\\n        d_gap = _dual_gap(mle, precision_, alpha)\\n        if verbose or return_costs:\\n            cost = _objective(mle, precision_, alpha)\\n            if verbose:\\n                print '[g_lasso] Iteration % 3i, cost %.4f, dual gap %.3e' % (\\n                                                    i, cost, d_gap)\\n            if return_costs:\\n                costs.append((cost, d_gap))\\n        if np.abs(d_gap) < tol:\\n            break\\n    else:\\n        warnings.warn('g_lasso: did not converge after %i iteration:'\\n                        'dual gap: %.3e' % (max_iter, d_gap))\\n    if return_costs:\\n        return covariance_, precision_, costs\\n    return covariance_, precision_"
  },
  {
    "code": "def nested_to_record(ds, prefix=\"\", sep=\".\", level=0):\\n    singleton = False\\n    if isinstance(ds, dict):\\n        ds = [ds]\\n        singleton = True\\n    new_ds = []\\n    for d in ds:\\n        new_d = copy.deepcopy(d)\\n        for k, v in d.items():\\n            if not isinstance(k, compat.string_types):\\n                k = str(k)\\n            if level == 0:\\n                newkey = k\\n            else:\\n                newkey = prefix + sep + k\\n            if not isinstance(v, dict):\\n                if level != 0:  \\n                    v = new_d.pop(k)\\n                    new_d[newkey] = v\\n                if v is None:  \\n                    new_d.pop(k)\\n                continue\\n            else:\\n                v = new_d.pop(k)\\n                new_d.update(nested_to_record(v, newkey, sep, level + 1))\\n        new_ds.append(new_d)\\n    if singleton:\\n        return new_ds[0]\\n    return new_ds",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def nested_to_record(ds, prefix=\"\", sep=\".\", level=0):\\n    singleton = False\\n    if isinstance(ds, dict):\\n        ds = [ds]\\n        singleton = True\\n    new_ds = []\\n    for d in ds:\\n        new_d = copy.deepcopy(d)\\n        for k, v in d.items():\\n            if not isinstance(k, compat.string_types):\\n                k = str(k)\\n            if level == 0:\\n                newkey = k\\n            else:\\n                newkey = prefix + sep + k\\n            if not isinstance(v, dict):\\n                if level != 0:  \\n                    v = new_d.pop(k)\\n                    new_d[newkey] = v\\n                if v is None:  \\n                    new_d.pop(k)\\n                continue\\n            else:\\n                v = new_d.pop(k)\\n                new_d.update(nested_to_record(v, newkey, sep, level + 1))\\n        new_ds.append(new_d)\\n    if singleton:\\n        return new_ds[0]\\n    return new_ds"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"delete\"], type=\"str\", default=\"add\"),\\n        blind_add=dict(choices=[\"enable\", \"disable\"], type=\"str\", default=\"disable\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_username=dict(required=False, type=\"str\"),\\n        device_password=dict(required=False, type=\"str\", no_log=True),\\n        device_unique_name=dict(required=True, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_username\": module.params[\"device_username\"],\\n        \"device_password\": module.params[\"device_password\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"mode\": module.params[\"mode\"]\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"mode\"] == \"add\":\\n            if module.params[\"blind_add\"] == \"disable\":\\n                exists_results = get_device(fmgr, paramgram)\\n                fmgr.govern_response(module=module, results=exists_results, good_codes=(0, -3), changed=False,\\n                                     ansible_facts=fmgr.construct_ansible_facts(exists_results,\\n                                                                                module.params, paramgram))\\n            discover_results = discover_device(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=discover_results, stop_on_success=False,\\n                                 ansible_facts=fmgr.construct_ansible_facts(discover_results,\\n                                                                            module.params, paramgram))\\n            if discover_results[0] == 0:\\n                results = add_device(fmgr, paramgram)\\n                fmgr.govern_response(module=module, results=discover_results, stop_on_success=True,\\n                                     changed_if_success=True,\\n                                     ansible_facts=fmgr.construct_ansible_facts(discover_results,\\n                                                                                module.params, paramgram))\\n        if paramgram[\"mode\"] == \"delete\":\\n            results = delete_device(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results,\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"delete\"], type=\"str\", default=\"add\"),\\n        blind_add=dict(choices=[\"enable\", \"disable\"], type=\"str\", default=\"disable\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_username=dict(required=False, type=\"str\"),\\n        device_password=dict(required=False, type=\"str\", no_log=True),\\n        device_unique_name=dict(required=True, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_username\": module.params[\"device_username\"],\\n        \"device_password\": module.params[\"device_password\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"adom\": module.params[\"adom\"],\\n        \"mode\": module.params[\"mode\"]\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"mode\"] == \"add\":\\n            if module.params[\"blind_add\"] == \"disable\":\\n                exists_results = get_device(fmgr, paramgram)\\n                fmgr.govern_response(module=module, results=exists_results, good_codes=(0, -3), changed=False,\\n                                     ansible_facts=fmgr.construct_ansible_facts(exists_results,\\n                                                                                module.params, paramgram))\\n            discover_results = discover_device(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=discover_results, stop_on_success=False,\\n                                 ansible_facts=fmgr.construct_ansible_facts(discover_results,\\n                                                                            module.params, paramgram))\\n            if discover_results[0] == 0:\\n                results = add_device(fmgr, paramgram)\\n                fmgr.govern_response(module=module, results=discover_results, stop_on_success=True,\\n                                     changed_if_success=True,\\n                                     ansible_facts=fmgr.construct_ansible_facts(discover_results,\\n                                                                                module.params, paramgram))\\n        if paramgram[\"mode\"] == \"delete\":\\n            results = delete_device(fmgr, paramgram)\\n            fmgr.govern_response(module=module, results=results,\\n                                 ansible_facts=fmgr.construct_ansible_facts(results, module.params, paramgram))\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def get_loc(self, key, method=None, tolerance=None):\\n        if is_bool(key):\\n            raise KeyError(key)\\n        if is_float(key) and np.isnan(key):\\n            nan_idxs = self._nan_idxs\\n            if not len(nan_idxs):\\n                raise KeyError(key)\\n            elif len(nan_idxs) == 1:\\n                return nan_idxs[0]\\n            return nan_idxs\\n        return super().get_loc(key, method=method, tolerance=tolerance)\\n    @cache_readonly",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_loc(self, key, method=None, tolerance=None):\\n        if is_bool(key):\\n            raise KeyError(key)\\n        if is_float(key) and np.isnan(key):\\n            nan_idxs = self._nan_idxs\\n            if not len(nan_idxs):\\n                raise KeyError(key)\\n            elif len(nan_idxs) == 1:\\n                return nan_idxs[0]\\n            return nan_idxs\\n        return super().get_loc(key, method=method, tolerance=tolerance)\\n    @cache_readonly"
  },
  {
    "code": "def execute(self, context=None):\\n        self.log.info('Executing SQL check: %s', self.sql)\\n        records = self.get_db_hook().get_first(self.sql)\\n        self.log.info('Record: %s', records)\\n        if not records:\\n            raise AirflowException(\"The query returned None\")\\n        elif not all([bool(r) for r in records]):\\n            raise AirflowException(\"Test failed.\\nQuery:\\n{query}\\nResults:\\n{records!s}\".format(\\n                query=self.sql, records=records))\\n        self.log.info(\"Success.\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(self, context=None):\\n        self.log.info('Executing SQL check: %s', self.sql)\\n        records = self.get_db_hook().get_first(self.sql)\\n        self.log.info('Record: %s', records)\\n        if not records:\\n            raise AirflowException(\"The query returned None\")\\n        elif not all([bool(r) for r in records]):\\n            raise AirflowException(\"Test failed.\\nQuery:\\n{query}\\nResults:\\n{records!s}\".format(\\n                query=self.sql, records=records))\\n        self.log.info(\"Success.\")"
  },
  {
    "code": "def _get_handle(path, mode, encoding=None):\\n    if py3compat.PY3:  \\n        if encoding:\\n            f = open(path, mode, encoding=encoding)\\n        else:\\n            f = open(path, mode, errors='replace')\\n    else:\\n        f = open(path, mode)\\n    return f\\nif py3compat.PY3:",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: handle decompression in python-parser, e.g. for regular expression delimiting. close #2535",
    "fixed_code": "def _get_handle(path, mode, encoding=None, compression=None):\\n    if compression is not None:\\n        if encoding is not None:\\n            raise ValueError('encoding + compression not yet supported')\\n        if compression == 'gzip':\\n            import gzip\\n            return gzip.GzipFile(path, 'rb')\\n        elif compression == 'bz2':\\n            import bz2\\n            return bz2.BZ2File(path, 'rb')\\n        else:\\n            raise ValueError('Unrecognized compression type: %s' %\\n                             compression)\\n    if py3compat.PY3:  \\n        if encoding:\\n            f = open(path, mode, encoding=encoding)\\n        else:\\n            f = open(path, mode, errors='replace')\\n    else:\\n        f = open(path, mode)\\n    return f\\nif py3compat.PY3:"
  },
  {
    "code": "def sanitize_array(\\n    data,\\n    index: Index | None,\\n    dtype: DtypeObj | None = None,\\n    copy: bool = False,\\n    raise_cast_failure: bool = True,\\n    *,\\n    allow_2d: bool = False,\\n) -> ArrayLike:\\n    if isinstance(data, ma.MaskedArray):\\n        data = sanitize_masked_array(data)\\n    if isinstance(dtype, PandasDtype):\\n        dtype = dtype.numpy_dtype\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray) and data.ndim == 0:\\n        if dtype is None:\\n            dtype = data.dtype\\n        data = lib.item_from_zerodim(data)\\n    elif isinstance(data, range):\\n        data = range_to_ndarray(data)\\n        copy = False\\n    if not is_list_like(data):\\n        if index is None:\\n            raise ValueError(\"index must be specified when data is not list-like\")\\n        data = construct_1d_arraylike_from_scalar(data, len(index), dtype)\\n        return data\\n    if isinstance(data, np.ndarray):\\n        if isinstance(data, np.matrix):\\n            data = data.A\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except IntCastingNaNError:\\n                warnings.warn(\\n                    \"In a future version, passing float-dtype values containing NaN \"\\n                    \"and an integer dtype will raise IntCastingNaNError \"\\n                    \"(subclass of ValueError) instead of silently ignoring the \"\\n                    \"passed dtype. To retain the old behavior, call Series(arr) or \"\\n                    \"DataFrame(arr) without passing a dtype.\",\\n                    FutureWarning,\\n                    stacklevel=find_stack_level(),\\n                )\\n                subarr = np.array(data, copy=copy)\\n            except ValueError:\\n                if not raise_cast_failure:\\n                    warnings.warn(\\n                        \"In a future version, passing float-dtype values and an \"\\n                        \"integer dtype to DataFrame will retain floating dtype \"\\n                        \"if they cannot be cast losslessly (matching Series behavior). \"\\n                        \"To retain the old behavior, use DataFrame(data).astype(dtype)\",\\n                        FutureWarning,\\n                        stacklevel=find_stack_level(),\\n                    )\\n                    dtype = cast(np.dtype, dtype)\\n                    return np.array(data, dtype=dtype, copy=copy)\\n                subarr = np.array(data, copy=copy)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n    else:\\n        if isinstance(data, (set, frozenset)):\\n            raise TypeError(f\"'{type(data).__name__}' type is unordered\")\\n        if hasattr(data, \"__array__\"):\\n            data = np.asarray(data)\\n        else:\\n            data = list(data)\\n        if dtype is not None or len(data) == 0:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n            if subarr.dtype == object:\\n                subarr = cast(np.ndarray, subarr)\\n                subarr = maybe_infer_to_datetimelike(subarr)\\n    subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)\\n    if isinstance(subarr, np.ndarray):\\n        dtype = cast(np.dtype, dtype)\\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\\n    return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Series(floatlike, dtype=intlike) inconsistent with non-ndarray data (#45142)",
    "fixed_code": "def sanitize_array(\\n    data,\\n    index: Index | None,\\n    dtype: DtypeObj | None = None,\\n    copy: bool = False,\\n    raise_cast_failure: bool = True,\\n    *,\\n    allow_2d: bool = False,\\n) -> ArrayLike:\\n    if isinstance(data, ma.MaskedArray):\\n        data = sanitize_masked_array(data)\\n    if isinstance(dtype, PandasDtype):\\n        dtype = dtype.numpy_dtype\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray) and data.ndim == 0:\\n        if dtype is None:\\n            dtype = data.dtype\\n        data = lib.item_from_zerodim(data)\\n    elif isinstance(data, range):\\n        data = range_to_ndarray(data)\\n        copy = False\\n    if not is_list_like(data):\\n        if index is None:\\n            raise ValueError(\"index must be specified when data is not list-like\")\\n        data = construct_1d_arraylike_from_scalar(data, len(index), dtype)\\n        return data\\n    if isinstance(data, np.ndarray):\\n        if isinstance(data, np.matrix):\\n            data = data.A\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except IntCastingNaNError:\\n                warnings.warn(\\n                    \"In a future version, passing float-dtype values containing NaN \"\\n                    \"and an integer dtype will raise IntCastingNaNError \"\\n                    \"(subclass of ValueError) instead of silently ignoring the \"\\n                    \"passed dtype. To retain the old behavior, call Series(arr) or \"\\n                    \"DataFrame(arr) without passing a dtype.\",\\n                    FutureWarning,\\n                    stacklevel=find_stack_level(),\\n                )\\n                subarr = np.array(data, copy=copy)\\n            except ValueError:\\n                if not raise_cast_failure:\\n                    warnings.warn(\\n                        \"In a future version, passing float-dtype values and an \"\\n                        \"integer dtype to DataFrame will retain floating dtype \"\\n                        \"if they cannot be cast losslessly (matching Series behavior). \"\\n                        \"To retain the old behavior, use DataFrame(data).astype(dtype)\",\\n                        FutureWarning,\\n                        stacklevel=find_stack_level(),\\n                    )\\n                    dtype = cast(np.dtype, dtype)\\n                    return np.array(data, dtype=dtype, copy=copy)\\n                subarr = np.array(data, copy=copy)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n    else:\\n        if isinstance(data, (set, frozenset)):\\n            raise TypeError(f\"'{type(data).__name__}' type is unordered\")\\n        if hasattr(data, \"__array__\"):\\n            data = np.asarray(data)\\n        else:\\n            data = list(data)\\n        if dtype is not None or len(data) == 0:\\n            try:\\n                subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n            except ValueError:\\n                casted = np.array(data, copy=False)\\n                if casted.dtype.kind == \"f\" and is_integer_dtype(dtype):\\n                    return sanitize_array(\\n                        casted,\\n                        index,\\n                        dtype,\\n                        copy=False,\\n                        raise_cast_failure=raise_cast_failure,\\n                        allow_2d=allow_2d,\\n                    )\\n                else:\\n                    raise\\n        else:\\n            subarr = maybe_convert_platform(data)\\n            if subarr.dtype == object:\\n                subarr = cast(np.ndarray, subarr)\\n                subarr = maybe_infer_to_datetimelike(subarr)\\n    subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)\\n    if isinstance(subarr, np.ndarray):\\n        dtype = cast(np.dtype, dtype)\\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\\n    return subarr"
  },
  {
    "code": "def localize(value):\\n    if isinstance(value, (decimal.Decimal, float, int, long)):\\n        return number_format(value)\\n    elif isinstance(value, datetime.datetime):\\n        return date_format(value, 'DATETIME_FORMAT')\\n    elif isinstance(value, datetime.date):\\n        return date_format(value)\\n    elif isinstance(value, datetime.time):\\n        return time_format(value, 'TIME_FORMAT')\\n    else:\\n        return value\\ndef localize_input(value, default=None):\\n    if isinstance(value, (decimal.Decimal, float, int, long)):\\n        return number_format(value)\\n    elif isinstance(value, datetime.datetime):\\n        value = datetime_safe.new_datetime(value)\\n        format = smart_str(default or get_format('DATETIME_INPUT_FORMATS')[0])\\n        return value.strftime(format)\\n    elif isinstance(value, datetime.date):\\n        value = datetime_safe.new_date(value)\\n        format = smart_str(default or get_format('DATE_INPUT_FORMATS')[0])\\n        return value.strftime(format)\\n    elif isinstance(value, datetime.time):\\n        format = smart_str(default or get_format('TIME_INPUT_FORMATS')[0])\\n        return value.strftime(format)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def localize(value):\\n    if isinstance(value, (decimal.Decimal, float, int, long)):\\n        return number_format(value)\\n    elif isinstance(value, datetime.datetime):\\n        return date_format(value, 'DATETIME_FORMAT')\\n    elif isinstance(value, datetime.date):\\n        return date_format(value)\\n    elif isinstance(value, datetime.time):\\n        return time_format(value, 'TIME_FORMAT')\\n    else:\\n        return value\\ndef localize_input(value, default=None):\\n    if isinstance(value, (decimal.Decimal, float, int, long)):\\n        return number_format(value)\\n    elif isinstance(value, datetime.datetime):\\n        value = datetime_safe.new_datetime(value)\\n        format = smart_str(default or get_format('DATETIME_INPUT_FORMATS')[0])\\n        return value.strftime(format)\\n    elif isinstance(value, datetime.date):\\n        value = datetime_safe.new_date(value)\\n        format = smart_str(default or get_format('DATE_INPUT_FORMATS')[0])\\n        return value.strftime(format)\\n    elif isinstance(value, datetime.time):\\n        format = smart_str(default or get_format('TIME_INPUT_FORMATS')[0])\\n        return value.strftime(format)\\n    return value"
  },
  {
    "code": "def fit(self, X, Y, maxit=100):\\n        X = np.asanyarray(X, dtype=np.float64)\\n        Y = np.asanyarray(Y, dtype=np.float64)\\n        nsamples = X.shape[0]\\n        alpha = self.alpha * nsamples\\n        if self.coef_ is None:\\n            self.coef_ = np.zeros(X.shape[1], dtype=np.float64)\\n        self.coef_, self.dual_gap_, self.eps_ = \\\\n                    lasso_coordinate_descent(self.coef_, alpha, X, Y, maxit, 10, self.tol)\\n        if self.dual_gap_ > self.eps_:\\n            warnings.warn('Objective did not converge, you might want to increase the number of interations')\\n        return self",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fit(self, X, Y, maxit=100):\\n        X = np.asanyarray(X, dtype=np.float64)\\n        Y = np.asanyarray(Y, dtype=np.float64)\\n        nsamples = X.shape[0]\\n        alpha = self.alpha * nsamples\\n        if self.coef_ is None:\\n            self.coef_ = np.zeros(X.shape[1], dtype=np.float64)\\n        self.coef_, self.dual_gap_, self.eps_ = \\\\n                    lasso_coordinate_descent(self.coef_, alpha, X, Y, maxit, 10, self.tol)\\n        if self.dual_gap_ > self.eps_:\\n            warnings.warn('Objective did not converge, you might want to increase the number of interations')\\n        return self"
  },
  {
    "code": "def get_next_page(self):\\n\\t\\tif self.next_page is not None:\\n\\t\\t\\tnext_page = resolve_url(self.next_page)\\n\\t\\telif settings.LOGOUT_REDIRECT_URL:\\n\\t\\t\\tnext_page = resolve_url(settings.LOGOUT_REDIRECT_URL)\\n\\t\\telse:\\n\\t\\t\\tnext_page = self.next_page\\n\\t\\tif (\\n\\t\\t\\tself.redirect_field_name in self.request.POST\\n\\t\\t\\tor self.redirect_field_name in self.request.GET\\n\\t\\t):\\n\\t\\t\\tnext_page = self.request.POST.get(\\n\\t\\t\\t\\tself.redirect_field_name, self.request.GET.get(self.redirect_field_name)\\n\\t\\t\\t)\\n\\t\\t\\turl_is_safe = url_has_allowed_host_and_scheme(\\n\\t\\t\\t\\turl=next_page,\\n\\t\\t\\t\\tallowed_hosts=self.get_success_url_allowed_hosts(),\\n\\t\\t\\t\\trequire_https=self.request.is_secure(),\\n\\t\\t\\t)\\n\\t\\t\\tif not url_is_safe:\\n\\t\\t\\t\\tnext_page = self.request.path\\n\\t\\treturn next_page",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #33648 -- Prevented extra redirect in LogoutView on invalid next page when LOGOUT_REDIRECT_URL is set.",
    "fixed_code": "def get_next_page(self):\\n\\t\\tif self.next_page is not None:\\n\\t\\t\\tnext_page = resolve_url(self.next_page)\\n\\t\\telif settings.LOGOUT_REDIRECT_URL:\\n\\t\\t\\tnext_page = resolve_url(settings.LOGOUT_REDIRECT_URL)\\n\\t\\telse:\\n\\t\\t\\tnext_page = self.next_page\\n\\t\\tif (\\n\\t\\t\\tself.redirect_field_name in self.request.POST\\n\\t\\t\\tor self.redirect_field_name in self.request.GET\\n\\t\\t):\\n\\t\\t\\tnext_page = self.request.POST.get(\\n\\t\\t\\t\\tself.redirect_field_name, self.request.GET.get(self.redirect_field_name)\\n\\t\\t\\t)\\n\\t\\t\\turl_is_safe = url_has_allowed_host_and_scheme(\\n\\t\\t\\t\\turl=next_page,\\n\\t\\t\\t\\tallowed_hosts=self.get_success_url_allowed_hosts(),\\n\\t\\t\\t\\trequire_https=self.request.is_secure(),\\n\\t\\t\\t)\\n\\t\\t\\tif not url_is_safe:\\n\\t\\t\\t\\tif settings.LOGOUT_REDIRECT_URL:\\n\\t\\t\\t\\t\\tnext_page = resolve_url(settings.LOGOUT_REDIRECT_URL)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnext_page = self.request.path\\n\\t\\treturn next_page"
  },
  {
    "code": "def _transform(self, a):\\n\\t\\twith np.errstate(invalid=\"ignore\"):\\n\\t\\t\\tmasked = np.abs(a) > self.linthresh\\n\\t\\tsign = np.sign(a[masked])\\n\\t\\tlog = (self._linscale_adj +\\n\\t\\t\\t   np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\\n\\t\\tlog *= sign * self.linthresh\\n\\t\\ta[masked] = log\\n\\t\\ta[~masked] *= self._linscale_adj\\n\\t\\treturn a",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _transform(self, a):\\n\\t\\twith np.errstate(invalid=\"ignore\"):\\n\\t\\t\\tmasked = np.abs(a) > self.linthresh\\n\\t\\tsign = np.sign(a[masked])\\n\\t\\tlog = (self._linscale_adj +\\n\\t\\t\\t   np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\\n\\t\\tlog *= sign * self.linthresh\\n\\t\\ta[masked] = log\\n\\t\\ta[~masked] *= self._linscale_adj\\n\\t\\treturn a"
  },
  {
    "code": "def check_supervised_y_2d(name, Estimator):\\n    if \"MultiTask\" in name:\\n        return\\n    rnd = np.random.RandomState(0)\\n    X = rnd.uniform(size=(10, 3))\\n    y = np.arange(10) % 3\\n    estimator = Estimator()\\n    set_testing_parameters(estimator)\\n    set_random_state(estimator)\\n    estimator.fit(X, y)\\n    y_pred = estimator.predict(X)\\n    set_random_state(estimator)\\n    with warnings.catch_warnings(record=True) as w:\\n        warnings.simplefilter(\"always\", DataConversionWarning)\\n        warnings.simplefilter(\"ignore\", RuntimeWarning)\\n        estimator.fit(X, y[:, np.newaxis])\\n    y_pred_2d = estimator.predict(X)\\n    msg = \"expected 1 DataConversionWarning, got: %s\" % (\\n        \", \".join([str(w_x) for w_x in w]))\\n    if name not in MULTI_OUTPUT:\\n        assert_greater(len(w), 0, msg)\\n        assert_true(\"DataConversionWarning('A column-vector y\"\\n                    \" was passed when a 1d array was expected\" in msg)\\n    assert_array_almost_equal(y_pred.ravel(), y_pred_2d.ravel())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_supervised_y_2d(name, Estimator):\\n    if \"MultiTask\" in name:\\n        return\\n    rnd = np.random.RandomState(0)\\n    X = rnd.uniform(size=(10, 3))\\n    y = np.arange(10) % 3\\n    estimator = Estimator()\\n    set_testing_parameters(estimator)\\n    set_random_state(estimator)\\n    estimator.fit(X, y)\\n    y_pred = estimator.predict(X)\\n    set_random_state(estimator)\\n    with warnings.catch_warnings(record=True) as w:\\n        warnings.simplefilter(\"always\", DataConversionWarning)\\n        warnings.simplefilter(\"ignore\", RuntimeWarning)\\n        estimator.fit(X, y[:, np.newaxis])\\n    y_pred_2d = estimator.predict(X)\\n    msg = \"expected 1 DataConversionWarning, got: %s\" % (\\n        \", \".join([str(w_x) for w_x in w]))\\n    if name not in MULTI_OUTPUT:\\n        assert_greater(len(w), 0, msg)\\n        assert_true(\"DataConversionWarning('A column-vector y\"\\n                    \" was passed when a 1d array was expected\" in msg)\\n    assert_array_almost_equal(y_pred.ravel(), y_pred_2d.ravel())"
  },
  {
    "code": "def execute_module(self):\\n        result = {'changed': False}\\n        existing_lacp_interfaces_facts = self.get_lacp_interfaces_facts()\\n        config_xmls = self.set_config(existing_lacp_interfaces_facts)\\n        with locked_config(self._module):\\n            for config_xml in to_list(config_xmls):\\n                diff = load_config(self._module, config_xml, [])\\n            commit = not self._module.check_mode\\n            if diff:\\n                if commit:\\n                    commit_configuration(self._module)\\n                else:\\n                    discard_changes(self._module)\\n                result['changed'] = True\\n                if self._module._diff:\\n                    result['diff'] = {'prepared': diff}\\n        result['xml'] = config_xmls\\n        changed_lacp_interfaces_facts = self.get_lacp_interfaces_facts()\\n        result['before'] = existing_lacp_interfaces_facts\\n        if result['changed']:\\n            result['after'] = changed_lacp_interfaces_facts\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute_module(self):\\n        result = {'changed': False}\\n        existing_lacp_interfaces_facts = self.get_lacp_interfaces_facts()\\n        config_xmls = self.set_config(existing_lacp_interfaces_facts)\\n        with locked_config(self._module):\\n            for config_xml in to_list(config_xmls):\\n                diff = load_config(self._module, config_xml, [])\\n            commit = not self._module.check_mode\\n            if diff:\\n                if commit:\\n                    commit_configuration(self._module)\\n                else:\\n                    discard_changes(self._module)\\n                result['changed'] = True\\n                if self._module._diff:\\n                    result['diff'] = {'prepared': diff}\\n        result['xml'] = config_xmls\\n        changed_lacp_interfaces_facts = self.get_lacp_interfaces_facts()\\n        result['before'] = existing_lacp_interfaces_facts\\n        if result['changed']:\\n            result['after'] = changed_lacp_interfaces_facts\\n        return result"
  },
  {
    "code": "def _reindex_non_unique(self, target):\\n\\t\\ttarget = ensure_index(target)\\n\\t\\tindexer, missing = self.get_indexer_non_unique(target)\\n\\t\\tcheck = indexer != -1\\n\\t\\tnew_labels = self.take(indexer[check])\\n\\t\\tnew_indexer = None\\n\\t\\tif len(missing):\\n\\t\\t\\tlength = np.arange(len(indexer))\\n\\t\\t\\tmissing = ensure_platform_int(missing)\\n\\t\\t\\tmissing_labels = target.take(missing)\\n\\t\\t\\tmissing_indexer = ensure_int64(length[~check])\\n\\t\\t\\tcur_labels = self.take(indexer[check]).values\\n\\t\\t\\tcur_indexer = ensure_int64(length[check])\\n\\t\\t\\tnew_labels = np.empty(tuple([len(indexer)]), dtype=object)\\n\\t\\t\\tnew_labels[cur_indexer] = cur_labels\\n\\t\\t\\tnew_labels[missing_indexer] = missing_labels\\n\\t\\t\\tif target.is_unique:\\n\\t\\t\\t\\tnew_indexer = np.arange(len(indexer))\\n\\t\\t\\t\\tnew_indexer[cur_indexer] = np.arange(len(cur_labels))\\n\\t\\t\\t\\tnew_indexer[missing_indexer] = -1\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindexer[~check] = -1\\n\\t\\t\\t\\tnew_indexer = np.arange(len(self.take(indexer)))\\n\\t\\t\\t\\tnew_indexer[~check] = -1\\n\\t\\tif isinstance(self, ABCMultiIndex):\\n\\t\\t\\tnew_index = type(self).from_tuples(new_labels, names=self.names)\\n\\t\\telse:\\n\\t\\t\\tnew_index = Index(new_labels, name=self.name)\\n\\t\\treturn new_index, indexer, new_indexer",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Series.loc[[]] with non-unique MultiIndex #13691 (#37161)",
    "fixed_code": "def _reindex_non_unique(self, target):\\n\\t\\ttarget = ensure_index(target)\\n\\t\\tif len(target) == 0:\\n\\t\\t\\treturn self[:0], np.array([], dtype=np.intp), None\\n\\t\\tindexer, missing = self.get_indexer_non_unique(target)\\n\\t\\tcheck = indexer != -1\\n\\t\\tnew_labels = self.take(indexer[check])\\n\\t\\tnew_indexer = None\\n\\t\\tif len(missing):\\n\\t\\t\\tlength = np.arange(len(indexer))\\n\\t\\t\\tmissing = ensure_platform_int(missing)\\n\\t\\t\\tmissing_labels = target.take(missing)\\n\\t\\t\\tmissing_indexer = ensure_int64(length[~check])\\n\\t\\t\\tcur_labels = self.take(indexer[check]).values\\n\\t\\t\\tcur_indexer = ensure_int64(length[check])\\n\\t\\t\\tnew_labels = np.empty(tuple([len(indexer)]), dtype=object)\\n\\t\\t\\tnew_labels[cur_indexer] = cur_labels\\n\\t\\t\\tnew_labels[missing_indexer] = missing_labels\\n\\t\\t\\tif target.is_unique:\\n\\t\\t\\t\\tnew_indexer = np.arange(len(indexer))\\n\\t\\t\\t\\tnew_indexer[cur_indexer] = np.arange(len(cur_labels))\\n\\t\\t\\t\\tnew_indexer[missing_indexer] = -1\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindexer[~check] = -1\\n\\t\\t\\t\\tnew_indexer = np.arange(len(self.take(indexer)))\\n\\t\\t\\t\\tnew_indexer[~check] = -1\\n\\t\\tif isinstance(self, ABCMultiIndex):\\n\\t\\t\\tnew_index = type(self).from_tuples(new_labels, names=self.names)\\n\\t\\telse:\\n\\t\\t\\tnew_index = Index(new_labels, name=self.name)\\n\\t\\treturn new_index, indexer, new_indexer"
  },
  {
    "code": "def upgrade():\\n    conn = op.get_bind()\\n    inspector = Inspector.from_engine(conn)\\n    tables = inspector.get_table_names()\\n    if 'connection' not in tables:\\n        op.create_table(\\n            'connection',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('conn_id', sa.String(length=250), nullable=True),\\n            sa.Column('conn_type', sa.String(length=500), nullable=True),\\n            sa.Column('host', sa.String(length=500), nullable=True),\\n            sa.Column('schema', sa.String(length=500), nullable=True),\\n            sa.Column('login', sa.String(length=500), nullable=True),\\n            sa.Column('password', sa.String(length=500), nullable=True),\\n            sa.Column('port', sa.Integer(), nullable=True),\\n            sa.Column('extra', sa.String(length=5000), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'dag' not in tables:\\n        op.create_table(\\n            'dag',\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.Column('is_paused', sa.Boolean(), nullable=True),\\n            sa.Column('is_subdag', sa.Boolean(), nullable=True),\\n            sa.Column('is_active', sa.Boolean(), nullable=True),\\n            sa.Column('last_scheduler_run', sa.DateTime(), nullable=True),\\n            sa.Column('last_pickled', sa.DateTime(), nullable=True),\\n            sa.Column('last_expired', sa.DateTime(), nullable=True),\\n            sa.Column('scheduler_lock', sa.Boolean(), nullable=True),\\n            sa.Column('pickle_id', sa.Integer(), nullable=True),\\n            sa.Column('fileloc', sa.String(length=2000), nullable=True),\\n            sa.Column('owners', sa.String(length=2000), nullable=True),\\n            sa.PrimaryKeyConstraint('dag_id')\\n        )\\n    if 'dag_pickle' not in tables:\\n        op.create_table(\\n            'dag_pickle',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('pickle', sa.PickleType(), nullable=True),\\n            sa.Column('created_dttm', sa.DateTime(), nullable=True),\\n            sa.Column('pickle_hash', sa.BigInteger(), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'import_error' not in tables:\\n        op.create_table(\\n            'import_error',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('timestamp', sa.DateTime(), nullable=True),\\n            sa.Column('filename', sa.String(length=1024), nullable=True),\\n            sa.Column('stacktrace', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'job' not in tables:\\n        op.create_table(\\n            'job',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=True),\\n            sa.Column('state', sa.String(length=20), nullable=True),\\n            sa.Column('job_type', sa.String(length=30), nullable=True),\\n            sa.Column('start_date', sa.DateTime(), nullable=True),\\n            sa.Column('end_date', sa.DateTime(), nullable=True),\\n            sa.Column('latest_heartbeat', sa.DateTime(), nullable=True),\\n            sa.Column('executor_class', sa.String(length=500), nullable=True),\\n            sa.Column('hostname', sa.String(length=500), nullable=True),\\n            sa.Column('unixname', sa.String(length=1000), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n        op.create_index(\\n            'job_type_heart',\\n            'job',\\n            ['job_type', 'latest_heartbeat'],\\n            unique=False\\n        )\\n    if 'log' not in tables:\\n        op.create_table(\\n            'log',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('dttm', sa.DateTime(), nullable=True),\\n            sa.Column('dag_id', sa.String(length=250), nullable=True),\\n            sa.Column('task_id', sa.String(length=250), nullable=True),\\n            sa.Column('event', sa.String(length=30), nullable=True),\\n            sa.Column('execution_date', sa.DateTime(), nullable=True),\\n            sa.Column('owner', sa.String(length=500), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'sla_miss' not in tables:\\n        op.create_table(\\n            'sla_miss',\\n            sa.Column('task_id', sa.String(length=250), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.Column('execution_date', sa.DateTime(), nullable=False),\\n            sa.Column('email_sent', sa.Boolean(), nullable=True),\\n            sa.Column('timestamp', sa.DateTime(), nullable=True),\\n            sa.Column('description', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('task_id', 'dag_id', 'execution_date')\\n        )\\n    if 'slot_pool' not in tables:\\n        op.create_table(\\n            'slot_pool',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('pool', sa.String(length=50), nullable=True),\\n            sa.Column('slots', sa.Integer(), nullable=True),\\n            sa.Column('description', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('id'),\\n            sa.UniqueConstraint('pool')\\n        )\\n    if 'task_instance' not in tables:\\n        op.create_table(\\n            'task_instance',\\n            sa.Column('task_id', sa.String(length=250), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.Column('execution_date', sa.DateTime(), nullable=False),\\n            sa.Column('start_date', sa.DateTime(), nullable=True),\\n            sa.Column('end_date', sa.DateTime(), nullable=True),\\n            sa.Column('duration', sa.Integer(), nullable=True),\\n            sa.Column('state', sa.String(length=20), nullable=True),\\n            sa.Column('try_number', sa.Integer(), nullable=True),\\n            sa.Column('hostname', sa.String(length=1000), nullable=True),\\n            sa.Column('unixname', sa.String(length=1000), nullable=True),\\n            sa.Column('job_id', sa.Integer(), nullable=True),\\n            sa.Column('pool', sa.String(length=50), nullable=True),\\n            sa.Column('queue', sa.String(length=50), nullable=True),\\n            sa.Column('priority_weight', sa.Integer(), nullable=True),\\n            sa.PrimaryKeyConstraint('task_id', 'dag_id', 'execution_date')\\n        )\\n        op.create_index(\\n            'ti_dag_state',\\n            'task_instance',\\n            ['dag_id', 'state'],\\n            unique=False\\n        )\\n        op.create_index(\\n            'ti_pool',\\n            'task_instance',\\n            ['pool', 'state', 'priority_weight'],\\n            unique=False\\n        )\\n        op.create_index(\\n            'ti_state_lkp',\\n            'task_instance',\\n            ['dag_id', 'task_id', 'execution_date', 'state'],\\n            unique=False\\n        )\\n    if 'user' not in tables:\\n        op.create_table(\\n            'user',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('username', sa.String(length=250), nullable=True),\\n            sa.Column('email', sa.String(length=500), nullable=True),\\n            sa.PrimaryKeyConstraint('id'),\\n            sa.UniqueConstraint('username')\\n        )\\n    if 'variable' not in tables:\\n        op.create_table(\\n            'variable',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('key', sa.String(length=250), nullable=True),\\n            sa.Column('val', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('id'),\\n            sa.UniqueConstraint('key')\\n        )\\n    if 'chart' not in tables:\\n        op.create_table(\\n            'chart',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('label', sa.String(length=200), nullable=True),\\n            sa.Column('conn_id', sa.String(length=250), nullable=False),\\n            sa.Column('user_id', sa.Integer(), nullable=True),\\n            sa.Column('chart_type', sa.String(length=100), nullable=True),\\n            sa.Column('sql_layout', sa.String(length=50), nullable=True),\\n            sa.Column('sql', sa.Text(), nullable=True),\\n            sa.Column('y_log_scale', sa.Boolean(), nullable=True),\\n            sa.Column('show_datatable', sa.Boolean(), nullable=True),\\n            sa.Column('show_sql', sa.Boolean(), nullable=True),\\n            sa.Column('height', sa.Integer(), nullable=True),\\n            sa.Column('default_params', sa.String(length=5000), nullable=True),\\n            sa.Column('x_is_date', sa.Boolean(), nullable=True),\\n            sa.Column('iteration_no', sa.Integer(), nullable=True),\\n            sa.Column('last_modified', sa.DateTime(), nullable=True),\\n            sa.ForeignKeyConstraint(['user_id'], ['user.id'], ),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'xcom' not in tables:\\n        op.create_table(\\n            'xcom',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('key', sa.String(length=512), nullable=True),\\n            sa.Column('value', sa.PickleType(), nullable=True),\\n            sa.Column(\\n                'timestamp',\\n                sa.DateTime(),\\n                default=func.now(),\\n                nullable=False),\\n            sa.Column('execution_date', sa.DateTime(), nullable=False),\\n            sa.Column('task_id', sa.String(length=250), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.PrimaryKeyConstraint('id')\\n        )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "AIRFLOW-5701: Don't clear xcom explicitly before execution (#6370)",
    "fixed_code": "def upgrade():\\n    conn = op.get_bind()\\n    inspector = Inspector.from_engine(conn)\\n    tables = inspector.get_table_names()\\n    if 'connection' not in tables:\\n        op.create_table(\\n            'connection',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('conn_id', sa.String(length=250), nullable=True),\\n            sa.Column('conn_type', sa.String(length=500), nullable=True),\\n            sa.Column('host', sa.String(length=500), nullable=True),\\n            sa.Column('schema', sa.String(length=500), nullable=True),\\n            sa.Column('login', sa.String(length=500), nullable=True),\\n            sa.Column('password', sa.String(length=500), nullable=True),\\n            sa.Column('port', sa.Integer(), nullable=True),\\n            sa.Column('extra', sa.String(length=5000), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'dag' not in tables:\\n        op.create_table(\\n            'dag',\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.Column('is_paused', sa.Boolean(), nullable=True),\\n            sa.Column('is_subdag', sa.Boolean(), nullable=True),\\n            sa.Column('is_active', sa.Boolean(), nullable=True),\\n            sa.Column('last_scheduler_run', sa.DateTime(), nullable=True),\\n            sa.Column('last_pickled', sa.DateTime(), nullable=True),\\n            sa.Column('last_expired', sa.DateTime(), nullable=True),\\n            sa.Column('scheduler_lock', sa.Boolean(), nullable=True),\\n            sa.Column('pickle_id', sa.Integer(), nullable=True),\\n            sa.Column('fileloc', sa.String(length=2000), nullable=True),\\n            sa.Column('owners', sa.String(length=2000), nullable=True),\\n            sa.PrimaryKeyConstraint('dag_id')\\n        )\\n    if 'dag_pickle' not in tables:\\n        op.create_table(\\n            'dag_pickle',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('pickle', sa.PickleType(), nullable=True),\\n            sa.Column('created_dttm', sa.DateTime(), nullable=True),\\n            sa.Column('pickle_hash', sa.BigInteger(), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'import_error' not in tables:\\n        op.create_table(\\n            'import_error',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('timestamp', sa.DateTime(), nullable=True),\\n            sa.Column('filename', sa.String(length=1024), nullable=True),\\n            sa.Column('stacktrace', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'job' not in tables:\\n        op.create_table(\\n            'job',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=True),\\n            sa.Column('state', sa.String(length=20), nullable=True),\\n            sa.Column('job_type', sa.String(length=30), nullable=True),\\n            sa.Column('start_date', sa.DateTime(), nullable=True),\\n            sa.Column('end_date', sa.DateTime(), nullable=True),\\n            sa.Column('latest_heartbeat', sa.DateTime(), nullable=True),\\n            sa.Column('executor_class', sa.String(length=500), nullable=True),\\n            sa.Column('hostname', sa.String(length=500), nullable=True),\\n            sa.Column('unixname', sa.String(length=1000), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n        op.create_index(\\n            'job_type_heart',\\n            'job',\\n            ['job_type', 'latest_heartbeat'],\\n            unique=False\\n        )\\n    if 'log' not in tables:\\n        op.create_table(\\n            'log',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('dttm', sa.DateTime(), nullable=True),\\n            sa.Column('dag_id', sa.String(length=250), nullable=True),\\n            sa.Column('task_id', sa.String(length=250), nullable=True),\\n            sa.Column('event', sa.String(length=30), nullable=True),\\n            sa.Column('execution_date', sa.DateTime(), nullable=True),\\n            sa.Column('owner', sa.String(length=500), nullable=True),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'sla_miss' not in tables:\\n        op.create_table(\\n            'sla_miss',\\n            sa.Column('task_id', sa.String(length=250), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.Column('execution_date', sa.DateTime(), nullable=False),\\n            sa.Column('email_sent', sa.Boolean(), nullable=True),\\n            sa.Column('timestamp', sa.DateTime(), nullable=True),\\n            sa.Column('description', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('task_id', 'dag_id', 'execution_date')\\n        )\\n    if 'slot_pool' not in tables:\\n        op.create_table(\\n            'slot_pool',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('pool', sa.String(length=50), nullable=True),\\n            sa.Column('slots', sa.Integer(), nullable=True),\\n            sa.Column('description', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('id'),\\n            sa.UniqueConstraint('pool')\\n        )\\n    if 'task_instance' not in tables:\\n        op.create_table(\\n            'task_instance',\\n            sa.Column('task_id', sa.String(length=250), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.Column('execution_date', sa.DateTime(), nullable=False),\\n            sa.Column('start_date', sa.DateTime(), nullable=True),\\n            sa.Column('end_date', sa.DateTime(), nullable=True),\\n            sa.Column('duration', sa.Integer(), nullable=True),\\n            sa.Column('state', sa.String(length=20), nullable=True),\\n            sa.Column('try_number', sa.Integer(), nullable=True),\\n            sa.Column('hostname', sa.String(length=1000), nullable=True),\\n            sa.Column('unixname', sa.String(length=1000), nullable=True),\\n            sa.Column('job_id', sa.Integer(), nullable=True),\\n            sa.Column('pool', sa.String(length=50), nullable=True),\\n            sa.Column('queue', sa.String(length=50), nullable=True),\\n            sa.Column('priority_weight', sa.Integer(), nullable=True),\\n            sa.PrimaryKeyConstraint('task_id', 'dag_id', 'execution_date')\\n        )\\n        op.create_index(\\n            'ti_dag_state',\\n            'task_instance',\\n            ['dag_id', 'state'],\\n            unique=False\\n        )\\n        op.create_index(\\n            'ti_pool',\\n            'task_instance',\\n            ['pool', 'state', 'priority_weight'],\\n            unique=False\\n        )\\n        op.create_index(\\n            'ti_state_lkp',\\n            'task_instance',\\n            ['dag_id', 'task_id', 'execution_date', 'state'],\\n            unique=False\\n        )\\n    if 'user' not in tables:\\n        op.create_table(\\n            'user',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('username', sa.String(length=250), nullable=True),\\n            sa.Column('email', sa.String(length=500), nullable=True),\\n            sa.PrimaryKeyConstraint('id'),\\n            sa.UniqueConstraint('username')\\n        )\\n    if 'variable' not in tables:\\n        op.create_table(\\n            'variable',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('key', sa.String(length=250), nullable=True),\\n            sa.Column('val', sa.Text(), nullable=True),\\n            sa.PrimaryKeyConstraint('id'),\\n            sa.UniqueConstraint('key')\\n        )\\n    if 'chart' not in tables:\\n        op.create_table(\\n            'chart',\\n            sa.Column('id', sa.Integer(), nullable=False),\\n            sa.Column('label', sa.String(length=200), nullable=True),\\n            sa.Column('conn_id', sa.String(length=250), nullable=False),\\n            sa.Column('user_id', sa.Integer(), nullable=True),\\n            sa.Column('chart_type', sa.String(length=100), nullable=True),\\n            sa.Column('sql_layout', sa.String(length=50), nullable=True),\\n            sa.Column('sql', sa.Text(), nullable=True),\\n            sa.Column('y_log_scale', sa.Boolean(), nullable=True),\\n            sa.Column('show_datatable', sa.Boolean(), nullable=True),\\n            sa.Column('show_sql', sa.Boolean(), nullable=True),\\n            sa.Column('height', sa.Integer(), nullable=True),\\n            sa.Column('default_params', sa.String(length=5000), nullable=True),\\n            sa.Column('x_is_date', sa.Boolean(), nullable=True),\\n            sa.Column('iteration_no', sa.Integer(), nullable=True),\\n            sa.Column('last_modified', sa.DateTime(), nullable=True),\\n            sa.ForeignKeyConstraint(['user_id'], ['user.id'], ),\\n            sa.PrimaryKeyConstraint('id')\\n        )\\n    if 'xcom' not in tables:\\n        op.create_table(\\n            'xcom',\\n            sa.Column('key', sa.String(length=512), nullable=True),\\n            sa.Column('value', sa.PickleType(), nullable=True),\\n            sa.Column(\\n                'timestamp',\\n                sa.DateTime(),\\n                default=func.now(),\\n                nullable=False),\\n            sa.Column('execution_date', sa.DateTime(), nullable=False),\\n            sa.Column('task_id', sa.String(length=250), nullable=False),\\n            sa.Column('dag_id', sa.String(length=250), nullable=False),\\n            sa.PrimaryKeyConstraint('dag_id', 'task_id', 'execution_date', 'key')\\n        )"
  },
  {
    "code": "def _output_field_or_none(self):\\n        try:\\n            return self.output_field\\n        except FieldError:\\n            if not self._output_field_resolved_to_none:\\n                raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _output_field_or_none(self):\\n        try:\\n            return self.output_field\\n        except FieldError:\\n            if not self._output_field_resolved_to_none:\\n                raise"
  },
  {
    "code": "def decode(input, output, header = 0):\\n    if a2b_qp is not None:\\n        data = input.read()\\n        odata = a2b_qp(data, header = header)\\n        output.write(odata)\\n        return\\n    new = ''\\n    while 1:\\n        line = input.readline()\\n        if not line: break\\n        i, n = 0, len(line)\\n        if n > 0 and line[n-1] == '\\n':\\n            partial = 0; n = n-1\\n            while n > 0 and line[n-1] in \" \\t\\r\":\\n                n = n-1\\n        else:\\n            partial = 1\\n        while i < n:\\n            c = line[i]\\n            if c == '_' and header:\\n                new = new + ' '; i = i+1\\n            elif c != ESCAPE:\\n                new = new + c; i = i+1\\n            elif i+1 == n and not partial:\\n                partial = 1; break\\n            elif i+1 < n and line[i+1] == ESCAPE:\\n                new = new + ESCAPE; i = i+2\\n            elif i+2 < n and ishex(line[i+1]) and ishex(line[i+2]):\\n                new = new + chr(unhex(line[i+1:i+3])); i = i+3\\n            else: \\n                new = new + c; i = i+1\\n        if not partial:\\n            output.write(new + '\\n')\\n            new = ''\\n    if new:\\n        output.write(new)\\ndef decodestring(s, header = 0):\\n    if a2b_qp is not None:\\n        return a2b_qp(s, header = header)\\n    from io import StringIO\\n    infp = StringIO(s)\\n    outfp = StringIO()\\n    decode(infp, outfp, header = header)\\n    return outfp.getvalue()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix quopri to operate consistently on bytes.",
    "fixed_code": "def decode(input, output, header = 0):\\n    if a2b_qp is not None:\\n        data = input.read()\\n        odata = a2b_qp(data, header = header)\\n        output.write(odata)\\n        return\\n    new = b''\\n    while 1:\\n        line = input.readline()\\n        if not line: break\\n        i, n = 0, len(line)\\n        if n > 0 and line[n-1:n] == b'\\n':\\n            partial = 0; n = n-1\\n            while n > 0 and line[n-1:n] in b\" \\t\\r\":\\n                n = n-1\\n        else:\\n            partial = 1\\n        while i < n:\\n            c = line[i:i+1]\\n            if c == b'_' and header:\\n                new = new + b' '; i = i+1\\n            elif c != ESCAPE:\\n                new = new + c; i = i+1\\n            elif i+1 == n and not partial:\\n                partial = 1; break\\n            elif i+1 < n and line[i+1] == ESCAPE:\\n                new = new + ESCAPE; i = i+2\\n            elif i+2 < n and ishex(line[i+1:i+2]) and ishex(line[i+2:i+3]):\\n                new = new + bytes((unhex(line[i+1:i+3]),)); i = i+3\\n            else: \\n                new = new + c; i = i+1\\n        if not partial:\\n            output.write(new + b'\\n')\\n            new = b''\\n    if new:\\n        output.write(new)\\ndef decodestring(s, header = 0):\\n    if a2b_qp is not None:\\n        return a2b_qp(s, header = header)\\n    from io import BytesIO\\n    infp = BytesIO(s)\\n    outfp = BytesIO()\\n    decode(infp, outfp, header = header)\\n    return outfp.getvalue()"
  },
  {
    "code": "def __getitem__(self, key):\\n        try:\\n            s = self._series[key]\\n            s.name = key\\n            return s\\n        except (TypeError, KeyError):\\n            if isinstance(key, slice):\\n                date_rng = self.index[key]\\n                return self.reindex(date_rng)\\n            elif isinstance(key, (np.ndarray, list)):\\n                return self._getitem_array(key)\\n            else:  \\n                raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, key):\\n        try:\\n            s = self._series[key]\\n            s.name = key\\n            return s\\n        except (TypeError, KeyError):\\n            if isinstance(key, slice):\\n                date_rng = self.index[key]\\n                return self.reindex(date_rng)\\n            elif isinstance(key, (np.ndarray, list)):\\n                return self._getitem_array(key)\\n            else:  \\n                raise"
  },
  {
    "code": "def check_for_wildcard_key(self,\\n                               wildcard_key, bucket_name=None, delimiter=''):\\n        return self.get_wildcard_key(wildcard_key=wildcard_key,\\n                                     bucket_name=bucket_name,\\n                                     delimiter=delimiter) is not None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5057] Provide bucket name to functions in S3 Hook when none is specified (#5674)\\n\\nNote: The order of arguments has changed for `check_for_prefix`.\\nThe `bucket_name` is now optional. It falls back to the `connection schema` attribute.\\n- refactor code\\n- complete docs",
    "fixed_code": "def check_for_wildcard_key(self,\\n                               wildcard_key, bucket_name=None, delimiter=''):\\n        return self.get_wildcard_key(wildcard_key=wildcard_key,\\n                                     bucket_name=bucket_name,\\n                                     delimiter=delimiter) is not None\\n    @provide_bucket_name"
  },
  {
    "code": "def delete_version(self, project_name, model_name, version_name):\\n        full_name = 'projects/{}/models/{}/versions/{}'.format(\\n            project_name, model_name, version_name)\\n        delete_request = self._cloudml.projects().models().versions().delete(\\n            name=full_name)\\n        response = delete_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-1401] Standardize cloud ml operator arguments\\n\\nStandardize on project_id, to be consistent with\\nother cloud operators,\\nbetter-supporting default arguments.\\n\\nThis is one of multiple commits that will be\\nrequired to resolve\\nAIRFLOW-1401.\\n\\nCloses #2439 from peterjdolan/cloudml_project_id",
    "fixed_code": "def delete_version(self, project_id, model_name, version_name):\\n        full_name = 'projects/{}/models/{}/versions/{}'.format(\\n            project_id, model_name, version_name)\\n        delete_request = self._cloudml.projects().models().versions().delete(\\n            name=full_name)\\n        response = delete_request.execute()\\n        get_request = self._cloudml.projects().operations().get(\\n            name=response['name'])\\n        return _poll_with_exponential_delay(\\n            request=get_request,\\n            max_n=9,\\n            is_done_func=lambda resp: resp.get('done', False),\\n            is_error_func=lambda resp: resp.get('error', None) is not None)"
  },
  {
    "code": "def _parse_sitemap(self, response):\\n\\t\\tif response.url.endswith('/robots.txt'):\\n\\t\\t\\tfor url in sitemap_urls_from_robots(response.body):\\n\\t\\t\\t\\tyield Request(url, callback=self._parse_sitemap)\\n\\t\\telse:\\n\\t\\t\\tbody = self._get_sitemap_body(response)\\n\\t\\t\\tif body is None:\\n\\t\\t\\t\\tlogger.warning(\"Ignoring invalid sitemap: %(response)s\",\\n\\t\\t\\t\\t\\t\\t\\t   {'response': response}, extra={'spider': self})\\n\\t\\t\\t\\treturn\\n\\t\\t\\ts = Sitemap(body)\\n\\t\\t\\tif s.type == 'sitemapindex':\\n\\t\\t\\t\\tfor loc in iterloc(s, self.sitemap_alternate_links):\\n\\t\\t\\t\\t\\tif any(x.search(loc) for x in self._follow):\\n\\t\\t\\t\\t\\t\\tyield Request(loc, callback=self._parse_sitemap)\\n\\t\\t\\telif s.type == 'urlset':\\n\\t\\t\\t\\tfor loc in iterloc(s):\\n\\t\\t\\t\\t\\tfor r, c in self._cbs:\\n\\t\\t\\t\\t\\t\\tif r.search(loc):\\n\\t\\t\\t\\t\\t\\t\\tyield Request(loc, callback=c)\\n\\t\\t\\t\\t\\t\\t\\tbreak",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix SitemapSpider to extract sitemap urls from robots.txt properly\\n\\nThis will fix #1766.",
    "fixed_code": "def _parse_sitemap(self, response):\\n\\t\\tif response.url.endswith('/robots.txt'):\\n\\t\\t\\tfor url in sitemap_urls_from_robots(response.text):\\n\\t\\t\\t\\tyield Request(url, callback=self._parse_sitemap)\\n\\t\\telse:\\n\\t\\t\\tbody = self._get_sitemap_body(response)\\n\\t\\t\\tif body is None:\\n\\t\\t\\t\\tlogger.warning(\"Ignoring invalid sitemap: %(response)s\",\\n\\t\\t\\t\\t\\t\\t\\t   {'response': response}, extra={'spider': self})\\n\\t\\t\\t\\treturn\\n\\t\\t\\ts = Sitemap(body)\\n\\t\\t\\tif s.type == 'sitemapindex':\\n\\t\\t\\t\\tfor loc in iterloc(s, self.sitemap_alternate_links):\\n\\t\\t\\t\\t\\tif any(x.search(loc) for x in self._follow):\\n\\t\\t\\t\\t\\t\\tyield Request(loc, callback=self._parse_sitemap)\\n\\t\\t\\telif s.type == 'urlset':\\n\\t\\t\\t\\tfor loc in iterloc(s):\\n\\t\\t\\t\\t\\tfor r, c in self._cbs:\\n\\t\\t\\t\\t\\t\\tif r.search(loc):\\n\\t\\t\\t\\t\\t\\t\\tyield Request(loc, callback=c)\\n\\t\\t\\t\\t\\t\\t\\tbreak"
  },
  {
    "code": "def to_timestamp(self, freq='s', how='start'):\\n        how = _validate_end_alias(how)\\n        if freq is None:\\n            base, mult = _gfc(self.freq)\\n            val = self\\n        else:\\n            base, mult = _gfc(freq)\\n            val = self.asfreq(freq, how)\\n        dt64 = plib.period_ordinal_to_dt64(val.ordinal, base)\\n        return Timestamp(dt64)\\n    year = _period_field_accessor('year', 0)\\n    month = _period_field_accessor('month', 3)\\n    day = _period_field_accessor('day', 4)\\n    hour = _period_field_accessor('hour', 5)\\n    minute = _period_field_accessor('minute', 6)\\n    second = _period_field_accessor('second', 7)\\n    weekofyear = _period_field_accessor('week', 8)\\n    week = weekofyear\\n    dayofweek = _period_field_accessor('dayofweek', 10)\\n    weekday = dayofweek\\n    dayofyear = _period_field_accessor('dayofyear', 9)\\n    quarter = _period_field_accessor('quarter', 2)\\n    qyear = _period_field_accessor('qyear', 1)\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: change default frequency for to_timestamp",
    "fixed_code": "def to_timestamp(self, freq=None, how='start'):\\n        how = _validate_end_alias(how)\\n        if freq is None:\\n            base, mult = _gfc(self.freq)\\n            freq = _freq_mod.get_to_timestamp_base(base)\\n        base, mult = _gfc(freq)\\n        val = self.asfreq(freq, how)\\n        dt64 = plib.period_ordinal_to_dt64(val.ordinal, base)\\n        return Timestamp(dt64)\\n    year = _period_field_accessor('year', 0)\\n    month = _period_field_accessor('month', 3)\\n    day = _period_field_accessor('day', 4)\\n    hour = _period_field_accessor('hour', 5)\\n    minute = _period_field_accessor('minute', 6)\\n    second = _period_field_accessor('second', 7)\\n    weekofyear = _period_field_accessor('week', 8)\\n    week = weekofyear\\n    dayofweek = _period_field_accessor('dayofweek', 10)\\n    weekday = dayofweek\\n    dayofyear = _period_field_accessor('dayofyear', 9)\\n    quarter = _period_field_accessor('quarter', 2)\\n    qyear = _period_field_accessor('qyear', 1)\\n    @classmethod"
  },
  {
    "code": "def get_filepath_or_buffer(filepath_or_buffer, encoding=None,\\n                           compression=None):\\n    if _is_url(filepath_or_buffer):\\n        req = _urlopen(str(filepath_or_buffer))\\n        if compression == 'infer':\\n            content_encoding = req.headers.get('Content-Encoding', None)\\n            if content_encoding == 'gzip':\\n                compression = 'gzip'\\n        to_return = list(maybe_read_encoded_stream(req, encoding, compression)) + \\\\n                    [compression]\\n        return tuple(to_return)\\n    if _is_s3_url(filepath_or_buffer):\\n        try:\\n            import boto\\n        except:\\n            raise ImportError(\"boto is required to handle s3 files\")\\n        parsed_url = parse_url(filepath_or_buffer)\\n        try:\\n            conn = boto.connect_s3()\\n        except boto.exception.NoAuthHandlerFound:\\n            conn = boto.connect_s3(anon=True)\\n        b = conn.get_bucket(parsed_url.netloc)\\n        k = boto.s3.key.Key(b)\\n        k.key = parsed_url.path\\n        filepath_or_buffer = BytesIO(k.get_contents_as_string(\\n            encoding=encoding))\\n        return filepath_or_buffer, None, compression\\n    return _expand_user(filepath_or_buffer), None, compression",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_filepath_or_buffer(filepath_or_buffer, encoding=None,\\n                           compression=None):\\n    if _is_url(filepath_or_buffer):\\n        req = _urlopen(str(filepath_or_buffer))\\n        if compression == 'infer':\\n            content_encoding = req.headers.get('Content-Encoding', None)\\n            if content_encoding == 'gzip':\\n                compression = 'gzip'\\n        to_return = list(maybe_read_encoded_stream(req, encoding, compression)) + \\\\n                    [compression]\\n        return tuple(to_return)\\n    if _is_s3_url(filepath_or_buffer):\\n        try:\\n            import boto\\n        except:\\n            raise ImportError(\"boto is required to handle s3 files\")\\n        parsed_url = parse_url(filepath_or_buffer)\\n        try:\\n            conn = boto.connect_s3()\\n        except boto.exception.NoAuthHandlerFound:\\n            conn = boto.connect_s3(anon=True)\\n        b = conn.get_bucket(parsed_url.netloc)\\n        k = boto.s3.key.Key(b)\\n        k.key = parsed_url.path\\n        filepath_or_buffer = BytesIO(k.get_contents_as_string(\\n            encoding=encoding))\\n        return filepath_or_buffer, None, compression\\n    return _expand_user(filepath_or_buffer), None, compression"
  },
  {
    "code": "def _convert_to_indexer(self, obj, axis=0, is_setter=False):\\n        labels = self.obj._get_axis(axis)\\n        is_int_index = _is_integer_index(labels)\\n        if com.is_integer(obj) and not is_int_index:\\n            return obj\\n        try:\\n            return labels.get_loc(obj)\\n        except (KeyError, TypeError):\\n            pass\\n        if isinstance(obj, slice):\\n            ltype = labels.inferred_type\\n            float_slice = (labels.inferred_type == 'floating'\\n                           and _is_float_slice(obj))\\n            int_slice = _is_index_slice(obj)\\n            null_slice = obj.start is None and obj.stop is None\\n            position_slice = (int_slice\\n                              and not ltype == 'integer'\\n                              and not isinstance(labels, MultiIndex)\\n                              and not float_slice)\\n            start, stop = obj.start, obj.stop\\n            try:\\n                if position_slice and 'mixed' in ltype:\\n                    if start is not None:\\n                        i = labels.get_loc(start)\\n                    if stop is not None:\\n                        j = labels.get_loc(stop)\\n                    position_slice = False\\n            except KeyError:\\n                if ltype == 'mixed-integer-float':\\n                    raise\\n            if null_slice or position_slice:\\n                indexer = obj\\n            else:\\n                try:\\n                    indexer = labels.slice_indexer(start, stop, obj.step)\\n                except Exception:\\n                    if _is_index_slice(obj):\\n                        if ltype == 'integer':\\n                            raise\\n                        indexer = obj\\n                    else:\\n                        raise\\n            return indexer\\n        elif _is_list_like(obj):\\n            if com._is_bool_indexer(obj):\\n                obj = _check_bool_indexer(labels, obj)\\n                inds, = obj.nonzero()\\n                return inds\\n            else:\\n                if isinstance(obj, Index):\\n                    objarr = obj.values\\n                else:\\n                    objarr = _asarray_tuplesafe(obj)\\n                if _is_integer_dtype(objarr) and not is_int_index:\\n                    if labels.inferred_type != 'integer':\\n                        objarr = np.where(objarr < 0,\\n                                          len(labels) + objarr, objarr)\\n                    return objarr\\n                if (isinstance(labels, MultiIndex) and\\n                        not isinstance(objarr[0], tuple)):\\n                    level = 0\\n                    _, indexer = labels.reindex(objarr, level=level)\\n                    check = labels.levels[0].get_indexer(objarr)\\n                else:\\n                    level = None\\n                    if labels.is_unique:\\n                        indexer = check = labels.get_indexer(objarr)\\n                    else:\\n                        indexer, missing = labels.get_indexer_non_unique(objarr)\\n                        check = indexer\\n                mask = check == -1\\n                if mask.any():\\n                    raise KeyError('%s not in index' % objarr[mask])\\n                return indexer\\n        else:\\n            try:\\n                return labels.get_loc(obj)\\n            except (KeyError):\\n                if np.isscalar(obj) and is_setter:\\n                    return { 'key' : obj }\\n                raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_to_indexer(self, obj, axis=0, is_setter=False):\\n        labels = self.obj._get_axis(axis)\\n        is_int_index = _is_integer_index(labels)\\n        if com.is_integer(obj) and not is_int_index:\\n            return obj\\n        try:\\n            return labels.get_loc(obj)\\n        except (KeyError, TypeError):\\n            pass\\n        if isinstance(obj, slice):\\n            ltype = labels.inferred_type\\n            float_slice = (labels.inferred_type == 'floating'\\n                           and _is_float_slice(obj))\\n            int_slice = _is_index_slice(obj)\\n            null_slice = obj.start is None and obj.stop is None\\n            position_slice = (int_slice\\n                              and not ltype == 'integer'\\n                              and not isinstance(labels, MultiIndex)\\n                              and not float_slice)\\n            start, stop = obj.start, obj.stop\\n            try:\\n                if position_slice and 'mixed' in ltype:\\n                    if start is not None:\\n                        i = labels.get_loc(start)\\n                    if stop is not None:\\n                        j = labels.get_loc(stop)\\n                    position_slice = False\\n            except KeyError:\\n                if ltype == 'mixed-integer-float':\\n                    raise\\n            if null_slice or position_slice:\\n                indexer = obj\\n            else:\\n                try:\\n                    indexer = labels.slice_indexer(start, stop, obj.step)\\n                except Exception:\\n                    if _is_index_slice(obj):\\n                        if ltype == 'integer':\\n                            raise\\n                        indexer = obj\\n                    else:\\n                        raise\\n            return indexer\\n        elif _is_list_like(obj):\\n            if com._is_bool_indexer(obj):\\n                obj = _check_bool_indexer(labels, obj)\\n                inds, = obj.nonzero()\\n                return inds\\n            else:\\n                if isinstance(obj, Index):\\n                    objarr = obj.values\\n                else:\\n                    objarr = _asarray_tuplesafe(obj)\\n                if _is_integer_dtype(objarr) and not is_int_index:\\n                    if labels.inferred_type != 'integer':\\n                        objarr = np.where(objarr < 0,\\n                                          len(labels) + objarr, objarr)\\n                    return objarr\\n                if (isinstance(labels, MultiIndex) and\\n                        not isinstance(objarr[0], tuple)):\\n                    level = 0\\n                    _, indexer = labels.reindex(objarr, level=level)\\n                    check = labels.levels[0].get_indexer(objarr)\\n                else:\\n                    level = None\\n                    if labels.is_unique:\\n                        indexer = check = labels.get_indexer(objarr)\\n                    else:\\n                        indexer, missing = labels.get_indexer_non_unique(objarr)\\n                        check = indexer\\n                mask = check == -1\\n                if mask.any():\\n                    raise KeyError('%s not in index' % objarr[mask])\\n                return indexer\\n        else:\\n            try:\\n                return labels.get_loc(obj)\\n            except (KeyError):\\n                if np.isscalar(obj) and is_setter:\\n                    return { 'key' : obj }\\n                raise"
  },
  {
    "code": "def _register_keras_layer_context_function(func):\\n  global _KERAS_LAYER_CONTEXT_FUNCTION\\n  _KERAS_LAYER_CONTEXT_FUNCTION = func",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _register_keras_layer_context_function(func):\\n  global _KERAS_LAYER_CONTEXT_FUNCTION\\n  _KERAS_LAYER_CONTEXT_FUNCTION = func"
  },
  {
    "code": "def __init__(self, gcp_conn_id: str = 'google_cloud_default', delegate_to: str = None) -> None:\\n        super().__init__(gcp_conn_id, delegate_to=delegate_to)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5446] Rewrite Google KMS Hook to Google Cloud Python (#6065)",
    "fixed_code": "def __init__(\\n        self,\\n        gcp_conn_id: str = \"google_cloud_default\",\\n        delegate_to: Optional[str] = None\\n    ) -> None:\\n        super().__init__(gcp_conn_id=gcp_conn_id, delegate_to=delegate_to)\\n        self._conn = None"
  },
  {
    "code": "def to_string(self):\\n        series = self.series\\n        if len(series) == 0:\\n            return ''\\n        fmt_index, have_header = self._get_formatted_index()\\n        fmt_values = self._get_formatted_values()\\n        maxlen = max(len(x) for x in fmt_index)\\n        pad_space = min(maxlen, 60)\\n        result = ['%s   %s' % (k.ljust(pad_space), v)\\n                  for (k, v) in izip(fmt_index[1:], fmt_values)]\\n        if self.header and have_header:\\n            result.insert(0, fmt_index[0])\\n        footer = self._get_footer()\\n        if footer:\\n            result.append(footer)\\n        return '\\n'.join(result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_string(self):\\n        series = self.series\\n        if len(series) == 0:\\n            return ''\\n        fmt_index, have_header = self._get_formatted_index()\\n        fmt_values = self._get_formatted_values()\\n        maxlen = max(len(x) for x in fmt_index)\\n        pad_space = min(maxlen, 60)\\n        result = ['%s   %s' % (k.ljust(pad_space), v)\\n                  for (k, v) in izip(fmt_index[1:], fmt_values)]\\n        if self.header and have_header:\\n            result.insert(0, fmt_index[0])\\n        footer = self._get_footer()\\n        if footer:\\n            result.append(footer)\\n        return '\\n'.join(result)"
  },
  {
    "code": "def importfile(path):\\n    magic = importlib.util.MAGIC_NUMBER\\n    with open(path, 'rb') as file:\\n        is_bytecode = magic == file.read(len(magic))\\n    filename = os.path.basename(path)\\n    name, ext = os.path.splitext(filename)\\n    if is_bytecode:\\n        loader = importlib._bootstrap.SourcelessFileLoader(name, path)\\n    else:\\n        loader = importlib._bootstrap.SourceFileLoader(name, path)\\n    spec = importlib.util.spec_from_file_location(name, path, loader=loader)\\n    _spec = importlib._bootstrap._SpecMethods(spec)\\n    try:\\n        return _spec.load()\\n    except:\\n        raise ErrorDuringImport(path, sys.exc_info())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def importfile(path):\\n    magic = importlib.util.MAGIC_NUMBER\\n    with open(path, 'rb') as file:\\n        is_bytecode = magic == file.read(len(magic))\\n    filename = os.path.basename(path)\\n    name, ext = os.path.splitext(filename)\\n    if is_bytecode:\\n        loader = importlib._bootstrap.SourcelessFileLoader(name, path)\\n    else:\\n        loader = importlib._bootstrap.SourceFileLoader(name, path)\\n    spec = importlib.util.spec_from_file_location(name, path, loader=loader)\\n    _spec = importlib._bootstrap._SpecMethods(spec)\\n    try:\\n        return _spec.load()\\n    except:\\n        raise ErrorDuringImport(path, sys.exc_info())"
  },
  {
    "code": "def _execute(self, *new_args, **kwargs):\\n\\t\\tencoding = getattr(sys.stdout, 'encoding') or 'utf-8'\\n\\t\\targs = (sys.executable, '-m', 'scrapy.cmdline') + new_args\\n\\t\\tproc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\\n\\t\\tcomm = proc.communicate()[0].strip()\\n\\t\\treturn comm.decode(encoding)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _execute(self, *new_args, **kwargs):\\n\\t\\tencoding = getattr(sys.stdout, 'encoding') or 'utf-8'\\n\\t\\targs = (sys.executable, '-m', 'scrapy.cmdline') + new_args\\n\\t\\tproc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\\n\\t\\tcomm = proc.communicate()[0].strip()\\n\\t\\treturn comm.decode(encoding)"
  },
  {
    "code": "def getfileinfo(name):\\n    finfo = FInfo()\\n    with io.open(name, 'rb') as fp:\\n        data = fp.read(512)\\n        if 0 not in data:\\n            finfo.Type = 'TEXT'\\n        fp.seek(0, 2)\\n        dsize = fp.tell()\\n    dir, file = os.path.split(name)\\n    file = file.replace(':', '-', 1)\\n    return file, finfo, dsize, 0\\nclass openrsrc:",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getfileinfo(name):\\n    finfo = FInfo()\\n    with io.open(name, 'rb') as fp:\\n        data = fp.read(512)\\n        if 0 not in data:\\n            finfo.Type = 'TEXT'\\n        fp.seek(0, 2)\\n        dsize = fp.tell()\\n    dir, file = os.path.split(name)\\n    file = file.replace(':', '-', 1)\\n    return file, finfo, dsize, 0\\nclass openrsrc:"
  },
  {
    "code": "def decode(obj):\\n    typ = obj.get('typ')\\n    if typ is None:\\n        return obj\\n    elif typ == 'timestamp':\\n        return Timestamp(obj['value'], tz=obj['tz'], offset=obj['offset'])\\n    elif typ == 'period':\\n        return Period(ordinal=obj['ordinal'], freq=obj['freq'])\\n    elif typ == 'index':\\n        dtype = dtype_for(obj['dtype'])\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        return globals()[obj['klass']](data, dtype=dtype, name=obj['name'])\\n    elif typ == 'multi_index':\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        data = [tuple(x) for x in data]\\n        return globals()[obj['klass']].from_tuples(data, names=obj['names'])\\n    elif typ == 'period_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        d = dict(name=obj['name'], freq=obj['freq'])\\n        return globals()[obj['klass']](data, **d)\\n    elif typ == 'datetime_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        d = dict(name=obj['name'], freq=obj['freq'], verify_integrity=False)\\n        result = globals()[obj['klass']](data, **d)\\n        tz = obj['tz']\\n        if tz is not None:\\n            result = result.tz_localize('UTC').tz_convert(tz)\\n        return result\\n    elif typ == 'series':\\n        dtype = dtype_for(obj['dtype'])\\n        index = obj['index']\\n        return globals()[obj['klass']](unconvert(obj['data'], dtype,\\n                                                 obj['compress']),\\n                                       index=index, name=obj['name'])\\n    elif typ == 'block_manager':\\n        axes = obj['axes']",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decode(obj):\\n    typ = obj.get('typ')\\n    if typ is None:\\n        return obj\\n    elif typ == 'timestamp':\\n        return Timestamp(obj['value'], tz=obj['tz'], offset=obj['offset'])\\n    elif typ == 'period':\\n        return Period(ordinal=obj['ordinal'], freq=obj['freq'])\\n    elif typ == 'index':\\n        dtype = dtype_for(obj['dtype'])\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        return globals()[obj['klass']](data, dtype=dtype, name=obj['name'])\\n    elif typ == 'multi_index':\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        data = [tuple(x) for x in data]\\n        return globals()[obj['klass']].from_tuples(data, names=obj['names'])\\n    elif typ == 'period_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        d = dict(name=obj['name'], freq=obj['freq'])\\n        return globals()[obj['klass']](data, **d)\\n    elif typ == 'datetime_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        d = dict(name=obj['name'], freq=obj['freq'], verify_integrity=False)\\n        result = globals()[obj['klass']](data, **d)\\n        tz = obj['tz']\\n        if tz is not None:\\n            result = result.tz_localize('UTC').tz_convert(tz)\\n        return result\\n    elif typ == 'series':\\n        dtype = dtype_for(obj['dtype'])\\n        index = obj['index']\\n        return globals()[obj['klass']](unconvert(obj['data'], dtype,\\n                                                 obj['compress']),\\n                                       index=index, name=obj['name'])\\n    elif typ == 'block_manager':\\n        axes = obj['axes']"
  },
  {
    "code": "def eval(self):\\n        values = [ self.convert_value(v) for v in self.value ]\\n        if self.op in ['=','!=']:\\n            if self.is_in_table:\\n                if len(values) <= 61:\\n                    self.condition = \"(%s)\" % ' | '.join([ \"(%s == %s)\" % (self.field,v[0]) for v in values])\\n                else:\\n                    self.filter = set([ v[1] for v in values ])\\n            else:\\n                self.filter = set([ v[1] for v in values ])\\n        else:\\n            if self.is_in_table:\\n                self.condition = '(%s %s %s)' % (self.field, self.op, values[0][0])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def eval(self):\\n        values = [ self.convert_value(v) for v in self.value ]\\n        if self.op in ['=','!=']:\\n            if self.is_in_table:\\n                if len(values) <= 61:\\n                    self.condition = \"(%s)\" % ' | '.join([ \"(%s == %s)\" % (self.field,v[0]) for v in values])\\n                else:\\n                    self.filter = set([ v[1] for v in values ])\\n            else:\\n                self.filter = set([ v[1] for v in values ])\\n        else:\\n            if self.is_in_table:\\n                self.condition = '(%s %s %s)' % (self.field, self.op, values[0][0])"
  },
  {
    "code": "def spawn(self, context, instance, image_meta, network_info,\\n\\t\\t\\t  block_device_mapping=None):\\n\\t\\tself._vmops.spawn(context, instance, image_meta, network_info)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix vmwareapi driver spawn() signature\\n\\nFixes bug #1045393\\n\\ncommit 99fb9d2a changed the signature of the spawn() method but not\\nall drivers were updated for the change.\\n\\nThe injected_files and admin_password instance attributes were ignored\\nby the hyperv driver before, so we can just ignore them in spawn() now\\nthat they are being passed as function arguments.",
    "fixed_code": "def spawn(self, context, instance, image_meta, injected_files,\\n\\t\\t\\t  admin_password, network_info=None, block_device_info=None):\\n\\t\\tself._vmops.spawn(context, instance, image_meta, network_info)"
  },
  {
    "code": "def open(self, mode='rb'):\\n        self._require_file()\\n        if hasattr(self, '_file') and self._file is not None:\\n            self.file.open(mode)\\n        else:\\n            self.file = self.storage.open(self.name, mode)\\n    open.alters_data = True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def open(self, mode='rb'):\\n        self._require_file()\\n        if hasattr(self, '_file') and self._file is not None:\\n            self.file.open(mode)\\n        else:\\n            self.file = self.storage.open(self.name, mode)\\n    open.alters_data = True"
  },
  {
    "code": "def _run_module_code(code, init_globals=None, run_name=None,\\n                    mod_name=None, mod_fname=None,\\n                    mod_loader=None, alter_sys=False):\\n    if alter_sys:\\n        temp_module = imp.new_module(mod_name)\\n        mod_globals = temp_module.__dict__\\n        saved_argv0 = sys.argv[0]\\n        sentinel = object()\\n        module_mod_name = sys.modules.get(mod_name, sentinel)\\n        module_run_name = sys.modules.get(run_name, sentinel)\\n        sys.argv[0] = mod_fname\\n        sys.modules[mod_name] = temp_module\\n        if run_name != mod_name:\\n            sys.modules[run_name] = temp_module\\n        try:\\n            _run_code(code, mod_globals, init_globals, run_name,\\n                      mod_name, mod_fname, mod_loader)\\n        finally:\\n            sys.argv[0] = saved_argv0\\n            if module_mod_name is not sentinel:\\n                sys.modules[mod_name] = module_mod_name\\n            else:\\n                del sys.modules[mod_name]\\n            if run_name != mod_name:\\n                if module_run_name is not sentinel:\\n                    sys.modules[run_name] = module_run_name\\n                else:\\n                    del sys.modules[run_name]\\n        return mod_globals.copy()\\n    else:\\n        return _run_code(code, {}, init_globals, run_name,\\n                         mod_name, mod_fname, mod_loader)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert the __module_name__ changes made in rev 47142. We'll revisit this in Python 2.6",
    "fixed_code": "def _run_module_code(code, init_globals=None,\\n                    mod_name=None, mod_fname=None,\\n                    mod_loader=None, alter_sys=False):\\n    if alter_sys:\\n        temp_module = imp.new_module(mod_name)\\n        mod_globals = temp_module.__dict__\\n        saved_argv0 = sys.argv[0]\\n        restore_module = mod_name in sys.modules\\n        if restore_module:\\n            saved_module = sys.modules[mod_name]\\n        sys.argv[0] = mod_fname\\n        sys.modules[mod_name] = temp_module\\n        try:\\n            _run_code(code, mod_globals, init_globals,\\n                      mod_name, mod_fname, mod_loader)\\n        finally:\\n            sys.argv[0] = saved_argv0\\n        if restore_module:\\n            sys.modules[mod_name] = saved_module\\n        else:\\n            del sys.modules[mod_name]\\n        return mod_globals.copy()\\n    else:\\n        return _run_code(code, {}, init_globals,\\n                         mod_name, mod_fname, mod_loader)"
  },
  {
    "code": "def _getitem_multilevel(self, key):\\n        loc = self.columns.get_loc(key)\\n        if isinstance(loc, (slice, np.ndarray)):\\n            new_columns = self.columns[loc]\\n            result_columns = maybe_droplevels(new_columns, key)\\n            if self._is_mixed_type:\\n                result = self.reindex(columns=new_columns)\\n                result.columns = result_columns\\n            else:\\n                new_values = self.values[:, loc]\\n                result = self._constructor(\\n                    new_values, index=self.index, columns=result_columns\\n                )\\n                result = result.__finalize__(self)\\n            if len(result.columns) == 1:\\n                top = result.columns[0]\\n                if isinstance(top, tuple):\\n                    top = top[0]\\n                if top == \"\":\\n                    result = result[\"\"]\\n                    if isinstance(result, Series):\\n                        result = self._constructor_sliced(\\n                            result, index=self.index, name=key\\n                        )\\n            result._set_is_copy(self)\\n            return result\\n        else:\\n            return self._ixs(loc, axis=1)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _getitem_multilevel(self, key):\\n        loc = self.columns.get_loc(key)\\n        if isinstance(loc, (slice, np.ndarray)):\\n            new_columns = self.columns[loc]\\n            result_columns = maybe_droplevels(new_columns, key)\\n            if self._is_mixed_type:\\n                result = self.reindex(columns=new_columns)\\n                result.columns = result_columns\\n            else:\\n                new_values = self.values[:, loc]\\n                result = self._constructor(\\n                    new_values, index=self.index, columns=result_columns\\n                )\\n                result = result.__finalize__(self)\\n            if len(result.columns) == 1:\\n                top = result.columns[0]\\n                if isinstance(top, tuple):\\n                    top = top[0]\\n                if top == \"\":\\n                    result = result[\"\"]\\n                    if isinstance(result, Series):\\n                        result = self._constructor_sliced(\\n                            result, index=self.index, name=key\\n                        )\\n            result._set_is_copy(self)\\n            return result\\n        else:\\n            return self._ixs(loc, axis=1)"
  },
  {
    "code": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed urlize after smart_urlquote rewrite\\n\\nRefs #22267.",
    "fixed_code": "def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\\n    safe_input = isinstance(text, SafeData)"
  },
  {
    "code": "class Worker(object):\\n\\tdef __init__(self, worker_id, last_active=None):\\n\\t\\tself.id = worker_id\\n\\t\\tself.reference = None  \\n\\t\\tself.last_active = last_active  \\n\\t\\tself.started = time.time()  \\n\\t\\tself.tasks = set()  \\n\\t\\tself.info = {}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "A Fix the TypeError whenever prune is called before update has been invoked in worker.",
    "fixed_code": "class Worker(object):\\n\\tdef __init__(self, worker_id, last_active=time.time()):\\n\\t\\tself.id = worker_id\\n\\t\\tself.reference = None  \\n\\t\\tself.last_active = last_active  \\n\\t\\tself.started = time.time()  \\n\\t\\tself.tasks = set()  \\n\\t\\tself.info = {}"
  },
  {
    "code": "def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\\n    placemarks = []\\n    try:\\n        klass = apps.get_model(label, model)\\n    except LookupError:\\n        raise Http404('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model))\\n    if field_name:\\n        try:\\n            field = klass._meta.get_field(field_name)\\n            if not isinstance(field, GeometryField):\\n                raise FieldDoesNotExist\\n        except FieldDoesNotExist:\\n            raise Http404('Invalid geometry field.')\\n    connection = connections[using]\\n    if connection.features.has_AsKML_function:\\n        placemarks = klass._default_manager.using(using).annotate(kml=AsKML(field_name))\\n    else:\\n        placemarks = []\\n        if connection.features.has_Transform_function:\\n            qs = klass._default_manager.using(using).annotate(\\n                **{'%s_4326' % field_name: Transform(field_name, 4326)})\\n            field_name += '_4326'\\n        else:\\n            qs = klass._default_manager.using(using).all()\\n        for mod in qs:\\n            mod.kml = getattr(mod, field_name).kml\\n            placemarks.append(mod)\\n    if compress:\\n        render = render_to_kmz\\n    else:\\n        render = render_to_kml\\n    return render('gis/kml/placemarks.kml', {'places': placemarks})",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\\n    placemarks = []\\n    try:\\n        klass = apps.get_model(label, model)\\n    except LookupError:\\n        raise Http404('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model))\\n    if field_name:\\n        try:\\n            field = klass._meta.get_field(field_name)\\n            if not isinstance(field, GeometryField):\\n                raise FieldDoesNotExist\\n        except FieldDoesNotExist:\\n            raise Http404('Invalid geometry field.')\\n    connection = connections[using]\\n    if connection.features.has_AsKML_function:\\n        placemarks = klass._default_manager.using(using).annotate(kml=AsKML(field_name))\\n    else:\\n        placemarks = []\\n        if connection.features.has_Transform_function:\\n            qs = klass._default_manager.using(using).annotate(\\n                **{'%s_4326' % field_name: Transform(field_name, 4326)})\\n            field_name += '_4326'\\n        else:\\n            qs = klass._default_manager.using(using).all()\\n        for mod in qs:\\n            mod.kml = getattr(mod, field_name).kml\\n            placemarks.append(mod)\\n    if compress:\\n        render = render_to_kmz\\n    else:\\n        render = render_to_kml\\n    return render('gis/kml/placemarks.kml', {'places': placemarks})"
  },
  {
    "code": "def check_for_file(self, file_path):\\n        try:\\n            files = self.get_conn().glob(file_path, details=False, invalidate_cache=True)\\n            return len(files) == 1\\n        except FileNotFoundError:\\n            return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add LocalToAzureDataLakeStorageOperator (#10814)",
    "fixed_code": "def check_for_file(self, file_path: str) -> bool:\\n        try:\\n            files = self.get_conn().glob(file_path, details=False, invalidate_cache=True)\\n            return len(files) == 1\\n        except FileNotFoundError:\\n            return False"
  },
  {
    "code": "def pgettext(context, message):\\n    msg_with_ctxt = \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message)\\n    result = gettext(msg_with_ctxt)\\n    if CONTEXT_SEPARATOR in result:\\n        result = message\\n    return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #28304 -- Kept SafeData type for pgettext-translated strings",
    "fixed_code": "def pgettext(context, message):\\n    msg_with_ctxt = \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message)\\n    result = gettext(msg_with_ctxt)\\n    if CONTEXT_SEPARATOR in result:\\n        result = message\\n    elif isinstance(message, SafeData):\\n        result = mark_safe(result)\\n    return result"
  },
  {
    "code": "def _add_new_block(self, col, value):\\n        new_block = make_block(value, [col], self.columns)\\n        self.blocks.append(new_block)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _add_new_block(self, col, value):\\n        new_block = make_block(value, [col], self.columns)\\n        self.blocks.append(new_block)"
  },
  {
    "code": "def get_template_env(self) -> jinja2.Environment:\\n        return self.dag.get_template_env() if self.has_dag() else jinja2.Environment(cache_size=0)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Sandbox templates (#15912)\\n\\nTemplates _shouldn't_ ever be taken from untrusted user input (and it's hard to\\ndo so without just edit the dag file, at which point you can run whatever\\npython code you like _anyway_), but this is a reasonable safety measure just in\\ncase someone does something \"clever\".",
    "fixed_code": "def get_template_env(self) -> jinja2.Environment:\\n        return (\\n            self.dag.get_template_env()\\n            if self.has_dag()\\n            else airflow.templates.SandboxedEnvironment(cache_size=0)\\n        )"
  },
  {
    "code": "def name(self):\\n        if self._column is None:\\n            return None \\n        else:\\n            return self._column\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def name(self):\\n        if self._column is None:\\n            return None \\n        else:\\n            return self._column\\n    @property"
  },
  {
    "code": "def parallel_coordinates(\\n    frame,\\n    class_column,\\n    cols=None,\\n    ax=None,\\n    color=None,\\n    use_columns=False,\\n    xticks=None,\\n    colormap=None,\\n    axvlines=True,\\n    axvlines_kwds=None,\\n    sort_labels=False,\\n    **kwargs\\n):\\n    plot_backend = _get_plot_backend(\"matplotlib\")\\n    return plot_backend.parallel_coordinates(\\n        frame=frame,\\n        class_column=class_column,\\n        cols=cols,\\n        ax=ax,\\n        color=color,\\n        use_columns=use_columns,\\n        xticks=xticks,\\n        colormap=colormap,\\n        axvlines=axvlines,\\n        axvlines_kwds=axvlines_kwds,\\n        sort_labels=sort_labels,\\n        **kwargs\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make subdirs in tests/io/data (#29513)",
    "fixed_code": "def parallel_coordinates(\\n    frame,\\n    class_column,\\n    cols=None,\\n    ax=None,\\n    color=None,\\n    use_columns=False,\\n    xticks=None,\\n    colormap=None,\\n    axvlines=True,\\n    axvlines_kwds=None,\\n    sort_labels=False,\\n    **kwargs\\n):\\n    plot_backend = _get_plot_backend(\"matplotlib\")\\n    return plot_backend.parallel_coordinates(\\n        frame=frame,\\n        class_column=class_column,\\n        cols=cols,\\n        ax=ax,\\n        color=color,\\n        use_columns=use_columns,\\n        xticks=xticks,\\n        colormap=colormap,\\n        axvlines=axvlines,\\n        axvlines_kwds=axvlines_kwds,\\n        sort_labels=sort_labels,\\n        **kwargs\\n    )"
  },
  {
    "code": "def _use_inf_as_null(key):\\n    ''\\n    flag = get_option(key)\\n    if flag == True:\\n        globals()['_isnull'] = _isnull_old\\n    else:\\n        globals()['_isnull'] = _isnull_new",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _use_inf_as_null(key):\\n    ''\\n    flag = get_option(key)\\n    if flag == True:\\n        globals()['_isnull'] = _isnull_old\\n    else:\\n        globals()['_isnull'] = _isnull_new"
  },
  {
    "code": "def modify_firewall_config(self):\\n        net_firewall_config_obj = netapp_utils.zapi.NaElement(\"net-firewall-config-modify\")\\n        net_firewall_config_obj.add_new_child('node-name', self.parameters['node'])\\n        net_firewall_config_obj.add_new_child('is-enabled', self.parameters['enable'])\\n        net_firewall_config_obj.add_new_child('is-logging', self.parameters['logging'])\\n        try:\\n            self.server.invoke_successfully(net_firewall_config_obj, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg=\"Error modifying Firewall Config: %s\" % (to_native(error)), exception=traceback.format_exc())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Update to na_ontap_firewall_policy (#51976)",
    "fixed_code": "def modify_firewall_config(self, modify):\\n        net_firewall_config_obj = netapp_utils.zapi.NaElement(\"net-firewall-config-modify\")\\n        net_firewall_config_obj.add_new_child('node-name', self.parameters['node'])\\n        if modify.get('enable'):\\n            net_firewall_config_obj.add_new_child('is-enabled', self.change_status_to_bool(self.parameters['enable']))\\n        if modify.get('logging'):\\n            net_firewall_config_obj.add_new_child('is-logging', self.change_status_to_bool(self.parameters['logging']))\\n        try:\\n            self.server.invoke_successfully(net_firewall_config_obj, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg=\"Error modifying Firewall Config: %s\" % (to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def get_template_sources(self, template_name, template_dirs=None):\\n\\t\\tif not template_dirs:\\n\\t\\t\\ttemplate_dirs = app_template_dirs\\n\\t\\tfor template_dir in template_dirs:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tyield safe_join(template_dir, template_name)\\n\\t\\t\\texcept UnicodeDecodeError:\\n\\t\\t\\t\\traise\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Raised SuspiciousFileOperation in safe_join.\\n\\nAdded a test for the condition safe_join is designed to prevent.\\n\\nPreviously, a generic ValueError was raised. It was impossible to tell\\nan intentional exception raised to implement safe_join's contract from\\nan unintentional exception caused by incorrect inputs or unexpected\\nconditions. That resulted in bizarre exception catching patterns, which\\nthis patch removes.\\n\\nSince safe_join is a private API and since the change is unlikely to\\ncreate security issues for users who use it anyway -- at worst, an\\nuncaught SuspiciousFileOperation exception will bubble up -- it isn't\\ndocumented.",
    "fixed_code": "def get_template_sources(self, template_name, template_dirs=None):\\n\\t\\tif not template_dirs:\\n\\t\\t\\ttemplate_dirs = app_template_dirs\\n\\t\\tfor template_dir in template_dirs:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tyield safe_join(template_dir, template_name)\\n\\t\\t\\texcept SuspiciousFileOperation:\\n\\t\\t\\t\\tpass"
  },
  {
    "code": "def verify_version(client, required_versions):\\n    version_str = client.get_system_setting('version')\\n    if not verify_version_str(version_str, required_versions):\\n        supported_versions = ', '.join(\\n            ['.'.join([str(p) for p in required_version])\\n             for required_version in required_versions])\\n        raise ModuleFailException(\\n            'unsupported NSO version {0}. {1} or later supported'.format(\\n                version_str, supported_versions))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def verify_version(client, required_versions):\\n    version_str = client.get_system_setting('version')\\n    if not verify_version_str(version_str, required_versions):\\n        supported_versions = ', '.join(\\n            ['.'.join([str(p) for p in required_version])\\n             for required_version in required_versions])\\n        raise ModuleFailException(\\n            'unsupported NSO version {0}. {1} or later supported'.format(\\n                version_str, supported_versions))"
  },
  {
    "code": "def execute(self, context: 'Context') -> List[str]:\\n        hook = GCSHook(\\n            gcp_conn_id=self.gcp_conn_id,\\n            delegate_to=self.delegate_to,\\n            impersonation_chain=self.google_impersonation_chain,\\n        )\\n        self.log.info(\\n            'Getting list of the files. Bucket: %s; Delimiter: %s; Prefix: %s',\\n            self.bucket,\\n            self.delimiter,\\n            self.prefix,\\n        )\\n        files = hook.list(bucket_name=self.bucket, prefix=self.prefix, delimiter=self.delimiter)\\n        s3_hook = S3Hook(\\n            aws_conn_id=self.dest_aws_conn_id, verify=self.dest_verify, extra_args=self.dest_s3_extra_args\\n        )\\n        if not self.replace:\\n            bucket_name, prefix = S3Hook.parse_s3_url(self.dest_s3_key)\\n            existing_files = s3_hook.list_keys(bucket_name, prefix=prefix)\\n            existing_files = existing_files if existing_files is not None else []\\n            existing_files = [file.replace(prefix, '', 1) for file in existing_files]\\n            files = list(set(files) - set(existing_files))\\n        if files:\\n            for file in files:\\n                with hook.provide_file(object_name=file, bucket_name=self.bucket) as local_tmp_file:\\n                    dest_key = self.dest_s3_key + file\\n                    self.log.info(\"Saving file to %s\", dest_key)\\n                    s3_hook.load_file(\\n                        filename=local_tmp_file.name,\\n                        key=dest_key,\\n                        replace=self.replace,\\n                        acl_policy=self.s3_acl_policy,\\n                    )\\n            self.log.info(\"All done, uploaded %d files to S3\", len(files))\\n        else:\\n            self.log.info(\"In sync, no files needed to be uploaded to S3\")\\n        return files",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Bug-fix GCSToS3Operator (#22071)",
    "fixed_code": "def execute(self, context: 'Context') -> List[str]:\\n        hook = GCSHook(\\n            gcp_conn_id=self.gcp_conn_id,\\n            delegate_to=self.delegate_to,\\n            impersonation_chain=self.google_impersonation_chain,\\n        )\\n        self.log.info(\\n            'Getting list of the files. Bucket: %s; Delimiter: %s; Prefix: %s',\\n            self.bucket,\\n            self.delimiter,\\n            self.prefix,\\n        )\\n        files = hook.list(bucket_name=self.bucket, prefix=self.prefix, delimiter=self.delimiter)\\n        s3_hook = S3Hook(\\n            aws_conn_id=self.dest_aws_conn_id, verify=self.dest_verify, extra_args=self.dest_s3_extra_args\\n        )\\n        if not self.keep_directory_structure and self.prefix:\\n            self.dest_s3_key = os.path.join(self.dest_s3_key, self.prefix)\\n        if not self.replace:\\n            bucket_name, prefix = S3Hook.parse_s3_url(self.dest_s3_key)\\n            existing_files = s3_hook.list_keys(bucket_name, prefix=prefix)\\n            existing_files = existing_files if existing_files is not None else []\\n            existing_files = [file.replace(prefix, '', 1) for file in existing_files]\\n            files = list(set(files) - set(existing_files))\\n        if files:\\n            for file in files:\\n                with hook.provide_file(object_name=file, bucket_name=self.bucket) as local_tmp_file:\\n                    dest_key = os.path.join(self.dest_s3_key, file)\\n                    self.log.info(\"Saving file to %s\", dest_key)\\n                    s3_hook.load_file(\\n                        filename=local_tmp_file.name,\\n                        key=dest_key,\\n                        replace=self.replace,\\n                        acl_policy=self.s3_acl_policy,\\n                    )\\n            self.log.info(\"All done, uploaded %d files to S3\", len(files))\\n        else:\\n            self.log.info(\"In sync, no files needed to be uploaded to S3\")\\n        return files"
  },
  {
    "code": "def getsection(self, section: str) -> Optional[Dict[str, Union[str, int, float, bool]]]:\\n        if section not in self._sections and section not in self.airflow_defaults._sections:  \\n            return None\\n        _section = copy.deepcopy(self.airflow_defaults._sections[section])  \\n        if section in self._sections:  \\n            _section.update(copy.deepcopy(self._sections[section]))  \\n        section_prefix = f'AIRFLOW__{section.upper()}__'\\n        for env_var in sorted(os.environ.keys()):\\n            if env_var.startswith(section_prefix):\\n                key = env_var.replace(section_prefix, '')\\n                if key.endswith(\"_CMD\"):\\n                    key = key[:-4]\\n                key = key.lower()\\n                _section[key] = self._get_env_var_option(section, key)\\n        for key, val in _section.items():\\n            try:\\n                val = int(val)\\n            except ValueError:\\n                try:\\n                    val = float(val)\\n                except ValueError:\\n                    if val.lower() in ('t', 'true'):\\n                        val = True\\n                    elif val.lower() in ('f', 'false'):\\n                        val = False\\n            _section[key] = val\\n        return _section",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Configuration.getsection copes with sections that only exist in user config (#12816)\\n\\nIf you try to run `airflow config list` with an old config you upgraded\\nfrom 1.8, it would fail for any sections that have been removed from the\\ndefault cofing -- `ldap` for instance.\\n\\nThis would also be a problem if the user makes a typo in a config\\nsection, or is using the airflow config for storing their own\\ninformation.\\n\\nWhile I was changing this code, I also removed the use of private\\nmethods/variable access in favour of public API",
    "fixed_code": "def getsection(self, section: str) -> Optional[Dict[str, Union[str, int, float, bool]]]:\\n        if not self.has_section(section) and not self.airflow_defaults.has_section(section):\\n            return None\\n        if self.airflow_defaults.has_section(section):\\n            _section = OrderedDict(self.airflow_defaults.items(section))\\n        else:\\n            _section = OrderedDict()\\n        if self.has_section(section):\\n            _section.update(OrderedDict(self.items(section)))\\n        section_prefix = f'AIRFLOW__{section.upper()}__'\\n        for env_var in sorted(os.environ.keys()):\\n            if env_var.startswith(section_prefix):\\n                key = env_var.replace(section_prefix, '')\\n                if key.endswith(\"_CMD\"):\\n                    key = key[:-4]\\n                key = key.lower()\\n                _section[key] = self._get_env_var_option(section, key)\\n        for key, val in _section.items():\\n            try:\\n                val = int(val)\\n            except ValueError:\\n                try:\\n                    val = float(val)\\n                except ValueError:\\n                    if val.lower() in ('t', 'true'):\\n                        val = True\\n                    elif val.lower() in ('f', 'false'):\\n                        val = False\\n            _section[key] = val\\n        return _section"
  },
  {
    "code": "def get_env(self, context):\\n        env = self.env\\n        if env is None:\\n            env = os.environ.copy()\\n        airflow_context_vars = context_to_airflow_vars(context, in_env_var_format=True)\\n        self.log.debug(\\n            'Exporting the following env vars:\\n%s',\\n            '\\n'.join(f\"{k}={v}\" for k, v in airflow_context_vars.items()),\\n        )\\n        env.update(airflow_context_vars)\\n        return env",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Adding feature in bash operator to append the user defined env variable to system env variable (#18944)",
    "fixed_code": "def get_env(self, context):\\n        system_env = os.environ.copy()\\n        env = self.env\\n        if env is None:\\n            env = system_env\\n        else:\\n            if self.append_env:\\n                system_env.update(env)\\n                env = system_env\\n        airflow_context_vars = context_to_airflow_vars(context, in_env_var_format=True)\\n        self.log.debug(\\n            'Exporting the following env vars:\\n%s',\\n            '\\n'.join(f\"{k}={v}\" for k, v in airflow_context_vars.items()),\\n        )\\n        env.update(airflow_context_vars)\\n        return env"
  },
  {
    "code": "def __new__(metacls, cls, bases, classdict, **kwds):\\n        member_type, first_enum = metacls._get_mixins_(bases)\\n        __new__, save_new, use_args = metacls._find_new_(classdict, member_type,\\n                                                        first_enum)\\n        enum_members = {k: classdict[k] for k in classdict._member_names}\\n        for name in classdict._member_names:\\n            del classdict[name]\\n        _order_ = classdict.pop('_order_', None)\\n        invalid_names = set(enum_members) & {'mro', }\\n        if invalid_names:\\n            raise ValueError('Invalid enum member name: {0}'.format(\\n                ','.join(invalid_names)))\\n        if '__doc__' not in classdict:\\n            classdict['__doc__'] = 'An enumeration.'\\n        enum_class = super().__new__(metacls, cls, bases, classdict)\\n        enum_class._member_names_ = []               \\n        enum_class._member_map_ = OrderedDict()      \\n        enum_class._member_type_ = member_type\\n        base_attributes = {a for b in enum_class.mro() for a in b.__dict__}\\n        enum_class._value2member_map_ = {}\\n        if '__reduce_ex__' not in classdict:\\n            if member_type is not object:\\n                methods = ('__getnewargs_ex__', '__getnewargs__',\\n                        '__reduce_ex__', '__reduce__')\\n                if not any(m in member_type.__dict__ for m in methods):\\n                    _make_class_unpicklable(enum_class)\\n        for member_name in classdict._member_names:\\n            value = enum_members[member_name]\\n            if not isinstance(value, tuple):\\n                args = (value, )\\n            else:\\n                args = value\\n            if member_type is tuple:   \\n                args = (args, )     \\n            if not use_args:\\n                enum_member = __new__(enum_class)\\n                if not hasattr(enum_member, '_value_'):\\n                    enum_member._value_ = value\\n            else:\\n                enum_member = __new__(enum_class, *args)\\n                if not hasattr(enum_member, '_value_'):\\n                    if member_type is object:\\n                        enum_member._value_ = value\\n                    else:\\n                        enum_member._value_ = member_type(*args)\\n            value = enum_member._value_\\n            enum_member._name_ = member_name\\n            enum_member.__objclass__ = enum_class\\n            enum_member.__init__(*args)\\n            for name, canonical_member in enum_class._member_map_.items():\\n                if canonical_member._value_ == enum_member._value_:\\n                    enum_member = canonical_member\\n                    break\\n            else:\\n                enum_class._member_names_.append(member_name)\\n            if member_name not in base_attributes:\\n                setattr(enum_class, member_name, enum_member)\\n            enum_class._member_map_[member_name] = enum_member\\n            try:\\n                enum_class._value2member_map_[value] = enum_member\\n            except TypeError:\\n                pass\\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\\n            class_method = getattr(enum_class, name)\\n            obj_method = getattr(member_type, name, None)\\n            enum_method = getattr(first_enum, name, None)\\n            if obj_method is not None and obj_method is class_method:\\n                setattr(enum_class, name, enum_method)\\n        if Enum is not None:\\n            if save_new:\\n                enum_class.__new_member__ = __new__\\n            enum_class.__new__ = Enum.__new__\\n        if _order_ is not None:\\n            if isinstance(_order_, str):\\n                _order_ = _order_.replace(',', ' ').split()\\n            if _order_ != enum_class._member_names_:\\n                raise TypeError('member order does not match _order_')\\n        return enum_class",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "issue23591: add docs; code cleanup; more tests",
    "fixed_code": "def __new__(metacls, cls, bases, classdict):\\n        member_type, first_enum = metacls._get_mixins_(bases)\\n        __new__, save_new, use_args = metacls._find_new_(classdict, member_type,\\n                                                        first_enum)\\n        enum_members = {k: classdict[k] for k in classdict._member_names}\\n        for name in classdict._member_names:\\n            del classdict[name]\\n        _order_ = classdict.pop('_order_', None)\\n        invalid_names = set(enum_members) & {'mro', }\\n        if invalid_names:\\n            raise ValueError('Invalid enum member name: {0}'.format(\\n                ','.join(invalid_names)))\\n        if '__doc__' not in classdict:\\n            classdict['__doc__'] = 'An enumeration.'\\n        enum_class = super().__new__(metacls, cls, bases, classdict)\\n        enum_class._member_names_ = []               \\n        enum_class._member_map_ = OrderedDict()      \\n        enum_class._member_type_ = member_type\\n        base_attributes = {a for b in enum_class.mro() for a in b.__dict__}\\n        enum_class._value2member_map_ = {}\\n        if '__reduce_ex__' not in classdict:\\n            if member_type is not object:\\n                methods = ('__getnewargs_ex__', '__getnewargs__',\\n                        '__reduce_ex__', '__reduce__')\\n                if not any(m in member_type.__dict__ for m in methods):\\n                    _make_class_unpicklable(enum_class)\\n        for member_name in classdict._member_names:\\n            value = enum_members[member_name]\\n            if not isinstance(value, tuple):\\n                args = (value, )\\n            else:\\n                args = value\\n            if member_type is tuple:   \\n                args = (args, )     \\n            if not use_args:\\n                enum_member = __new__(enum_class)\\n                if not hasattr(enum_member, '_value_'):\\n                    enum_member._value_ = value\\n            else:\\n                enum_member = __new__(enum_class, *args)\\n                if not hasattr(enum_member, '_value_'):\\n                    if member_type is object:\\n                        enum_member._value_ = value\\n                    else:\\n                        enum_member._value_ = member_type(*args)\\n            value = enum_member._value_\\n            enum_member._name_ = member_name\\n            enum_member.__objclass__ = enum_class\\n            enum_member.__init__(*args)\\n            for name, canonical_member in enum_class._member_map_.items():\\n                if canonical_member._value_ == enum_member._value_:\\n                    enum_member = canonical_member\\n                    break\\n            else:\\n                enum_class._member_names_.append(member_name)\\n            if member_name not in base_attributes:\\n                setattr(enum_class, member_name, enum_member)\\n            enum_class._member_map_[member_name] = enum_member\\n            try:\\n                enum_class._value2member_map_[value] = enum_member\\n            except TypeError:\\n                pass\\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\\n            class_method = getattr(enum_class, name)\\n            obj_method = getattr(member_type, name, None)\\n            enum_method = getattr(first_enum, name, None)\\n            if obj_method is not None and obj_method is class_method:\\n                setattr(enum_class, name, enum_method)\\n        if Enum is not None:\\n            if save_new:\\n                enum_class.__new_member__ = __new__\\n            enum_class.__new__ = Enum.__new__\\n        if _order_ is not None:\\n            if isinstance(_order_, str):\\n                _order_ = _order_.replace(',', ' ').split()\\n            if _order_ != enum_class._member_names_:\\n                raise TypeError('member order does not match _order_')\\n        return enum_class"
  },
  {
    "code": "def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\\n\\t\\treturn f\"django_date_trunc(%s, {sql}, %s, %s)\", (\\n\\t\\t\\tlookup_type.lower(),\\n\\t\\t\\t*params,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\\n\\t\\treturn f\"django_date_trunc(%s, {sql}, %s, %s)\", (\\n\\t\\t\\tlookup_type.lower(),\\n\\t\\t\\t*params,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)"
  },
  {
    "code": "def mail(self, sender, options=[]):\\n        optionlist = ''\\n        if options and self.does_esmtp:\\n            optionlist = ' ' + ' '.join(options)\\n        self.putcmd(\"mail\", \"FROM:%s%s\" % (quoteaddr(sender), optionlist))\\n        return self.getreply()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#22027: Add RFC6531 support to smtplib.\\n\\nInitial patch by Milan Oberkirch.",
    "fixed_code": "def mail(self, sender, options=[]):\\n        optionlist = ''\\n        if options and self.does_esmtp:\\n            if any(x.lower()=='smtputf8' for x in options):\\n                if self.has_extn('smtputf8'):\\n                    self.command_encoding = 'utf-8'\\n                else:\\n                    raise SMTPNotSupportedError(\\n                        'SMTPUTF8 not supported by server')\\n            optionlist = ' ' + ' '.join(options)\\n        self.putcmd(\"mail\", \"FROM:%s%s\" % (quoteaddr(sender), optionlist))\\n        return self.getreply()"
  },
  {
    "code": "def _convert_types(values, na_values):\\n    try:\\n        values = lib.maybe_convert_numeric(values, na_values)\\n    except Exception:\\n        lib.sanitize_objects(values, na_values)\\n    if values.dtype == np.object_:\\n        return lib.maybe_convert_bool(values)\\n    return values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: don't need to convert numeric types already handled by converters, GH #546",
    "fixed_code": "def _convert_types(values, na_values):\\n    if issubclass(values.dtype.type, (np.number, np.bool_)):\\n        return values\\n    try:\\n        values = lib.maybe_convert_numeric(values, na_values)\\n    except Exception:\\n        lib.sanitize_objects(values, na_values)\\n    if values.dtype == np.object_:\\n        return lib.maybe_convert_bool(values)\\n    return values"
  },
  {
    "code": "def get_conn(self) -> Connection:\\n\\t\\tconn_md = self.get_connection(getattr(self, self.conn_name_attr))\\n\\t\\tcreds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\\n\\t\\tdatabase_url = (\\n\\t\\t\\tf\"{conn_md.extra_dejson.get('dialect_driver', 'drill+sadrill')}://{creds}\"\\n\\t\\t\\tf\"{conn_md.host}:{conn_md.port}/\"\\n\\t\\t\\tf'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\\n\\t\\t)\\n\\t\\tif \"?\" in database_url:\\n\\t\\t\\traise ValueError(\"Drill database_url should not contain a '?'\")\\n\\t\\tengine = create_engine(database_url)\\n\\t\\tself.log.info(\\n\\t\\t\\t\"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\\n\\t\\t)\\n\\t\\treturn engine.raw_connection()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_conn(self) -> Connection:\\n\\t\\tconn_md = self.get_connection(getattr(self, self.conn_name_attr))\\n\\t\\tcreds = f\"{conn_md.login}:{conn_md.password}@\" if conn_md.login else \"\"\\n\\t\\tdatabase_url = (\\n\\t\\t\\tf\"{conn_md.extra_dejson.get('dialect_driver', 'drill+sadrill')}://{creds}\"\\n\\t\\t\\tf\"{conn_md.host}:{conn_md.port}/\"\\n\\t\\t\\tf'{conn_md.extra_dejson.get(\"storage_plugin\", \"dfs\")}'\\n\\t\\t)\\n\\t\\tif \"?\" in database_url:\\n\\t\\t\\traise ValueError(\"Drill database_url should not contain a '?'\")\\n\\t\\tengine = create_engine(database_url)\\n\\t\\tself.log.info(\\n\\t\\t\\t\"Connected to the Drillbit at %s:%s as user %s\", conn_md.host, conn_md.port, conn_md.login\\n\\t\\t)\\n\\t\\treturn engine.raw_connection()"
  },
  {
    "code": "def maketrans(fromstr, tostr):\\n    if len(fromstr) != len(tostr):\\n        raise ValueError, \"maketrans arguments must have same length\"\\n    global _idmapL\\n    if not _idmapL:\\n        _idmapL = map(None, _idmap)\\n    L = _idmapL[:]\\n    fromstr = map(ord, fromstr)\\n    for i in range(len(fromstr)):\\n        L[fromstr[i]] = tostr[i]\\n    return ''.join(L)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Replace all map(None, a) with list(a).",
    "fixed_code": "def maketrans(fromstr, tostr):\\n    if len(fromstr) != len(tostr):\\n        raise ValueError, \"maketrans arguments must have same length\"\\n    global _idmapL\\n    if not _idmapL:\\n        _idmapL = list(_idmap)\\n    L = _idmapL[:]\\n    fromstr = map(ord, fromstr)\\n    for i in range(len(fromstr)):\\n        L[fromstr[i]] = tostr[i]\\n    return ''.join(L)"
  },
  {
    "code": "def _set_list(self, length, items):\\n        ndim = self._cs.dims\\n        hasz = self._cs.hasz  \\n        cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz)\\n        for i, c in enumerate(items):\\n            cs[i] = c\\n        ptr = self._init_func(cs.ptr)\\n        if ptr:\\n            capi.destroy_geom(self.ptr)\\n            self.ptr = ptr\\n            self._post_init()\\n        else:\\n            raise GEOSException('Geometry resulting from slice deletion was invalid.')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #30552 -- Fixed loss of SRID when calling reverse() on LineString/Point.\\n\\nThanks Mariusz Felisiak for contributing the Point part.",
    "fixed_code": "def _set_list(self, length, items):\\n        ndim = self._cs.dims\\n        hasz = self._cs.hasz  \\n        srid = self.srid\\n        cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz)\\n        for i, c in enumerate(items):\\n            cs[i] = c\\n        ptr = self._init_func(cs.ptr)\\n        if ptr:\\n            capi.destroy_geom(self.ptr)\\n            self.ptr = ptr\\n            if srid is not None:\\n                self.srid = srid\\n            self._post_init()\\n        else:\\n            raise GEOSException('Geometry resulting from slice deletion was invalid.')"
  },
  {
    "code": "def setdiff1d(ar1, ar2, assume_unique=False):\\n    if not assume_unique:\\n        ar1 = unique(ar1)\\n        ar2 = unique(ar2)\\n    aux = in1d(ar1, ar2, assume_unique=True)\\n    if aux.size == 0:\\n        return aux\\n    else:\\n        return np.asarray(ar1)[aux == 0]\\n@deprecate_with_doc('')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setdiff1d(ar1, ar2, assume_unique=False):\\n    if not assume_unique:\\n        ar1 = unique(ar1)\\n        ar2 = unique(ar2)\\n    aux = in1d(ar1, ar2, assume_unique=True)\\n    if aux.size == 0:\\n        return aux\\n    else:\\n        return np.asarray(ar1)[aux == 0]\\n@deprecate_with_doc('')"
  },
  {
    "code": "def get_quote_yahoo(symbols):\\n    if isinstance(symbols, compat.string_types):\\n        sym_list = symbols\\n    else:\\n        sym_list = '+'.join(symbols)\\n    request = ''.join(compat.itervalues(_yahoo_codes))  \\n    header = list(_yahoo_codes.keys())\\n    data = defaultdict(list)\\n    url_str = _YAHOO_QUOTE_URL + 's=%s&f=%s' % (sym_list, request)\\n    with urlopen(url_str) as url:\\n        lines = url.readlines()\\n    for line in lines:\\n        fields = line.decode('utf-8').strip().split(',')\\n        for i, field in enumerate(fields):\\n            if field[-2:] == '%\"':\\n                v = float(field.strip('\"%'))\\n            elif field[0] == '\"':\\n                v = field.strip('\"')\\n            else:\\n                try:\\n                    v = float(field)\\n                except ValueError:\\n                    v = field\\n            data[header[i]].append(v)\\n    idx = data.pop('symbol')\\n    return DataFrame(data, index=idx)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_quote_yahoo(symbols):\\n    if isinstance(symbols, compat.string_types):\\n        sym_list = symbols\\n    else:\\n        sym_list = '+'.join(symbols)\\n    request = ''.join(compat.itervalues(_yahoo_codes))  \\n    header = list(_yahoo_codes.keys())\\n    data = defaultdict(list)\\n    url_str = _YAHOO_QUOTE_URL + 's=%s&f=%s' % (sym_list, request)\\n    with urlopen(url_str) as url:\\n        lines = url.readlines()\\n    for line in lines:\\n        fields = line.decode('utf-8').strip().split(',')\\n        for i, field in enumerate(fields):\\n            if field[-2:] == '%\"':\\n                v = float(field.strip('\"%'))\\n            elif field[0] == '\"':\\n                v = field.strip('\"')\\n            else:\\n                try:\\n                    v = float(field)\\n                except ValueError:\\n                    v = field\\n            data[header[i]].append(v)\\n    idx = data.pop('symbol')\\n    return DataFrame(data, index=idx)"
  },
  {
    "code": "def to_offset(freqstr):\\n    if freqstr is None:\\n        return None\\n    if isinstance(freqstr, DateOffset):\\n        return freqstr\\n    if isinstance(freqstr, tuple):\\n        name = freqstr[0]\\n        stride = freqstr[1]\\n        if isinstance(stride, basestring):\\n            name, stride = stride, name\\n        name, _ = _base_and_stride(name)\\n    else:\\n        name, stride = _base_and_stride(freqstr)\\n    offset = getOffset(name)\\n    return offset * stride",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: legacy time rule support and refactoring, better alias handling. misc tests, #1041",
    "fixed_code": "def to_offset(freqstr):\\n    if freqstr is None:\\n        return None\\n    if isinstance(freqstr, DateOffset):\\n        return freqstr\\n    if isinstance(freqstr, tuple):\\n        name = freqstr[0]\\n        stride = freqstr[1]\\n        if isinstance(stride, basestring):\\n            name, stride = stride, name\\n        name, _ = _base_and_stride(name)\\n    else:\\n        name, stride = _base_and_stride(freqstr)\\n    offset = get_offset(name)\\n    return offset * stride"
  },
  {
    "code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        targetlbvserver=dict(type='str'),\\n        targetvserverexpr=dict(type='str'),\\n        comment=dict(type='str'),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'targetlbvserver',\\n        'targetvserverexpr',\\n        'comment',\\n    ]\\n    readonly_attrs = [\\n        'hits',\\n        'referencecount',\\n        'undefhits',\\n        'builtin',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'targetvserverexpr',\\n    ]\\n    transforms = {\\n    }\\n    json_encodes = ['targetvserverexpr']\\n    csaction_proxy = ConfigProxy(\\n        actual=csaction(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n        json_encodes=json_encodes,\\n    )\\n    try:\\n        ensure_feature_is_enabled(client, 'CS')\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.add()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not action_identical(client, module, csaction_proxy):\\n                immutables_changed = get_immutables_intersection(csaction_proxy, diff_list(client, module, csaction_proxy).keys())\\n                if immutables_changed != []:\\n                    module.fail_json(\\n                        msg='Cannot update immutable attributes %s' % (immutables_changed,),\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n                if not module.check_mode:\\n                    csaction_proxy.update()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            log('Sanity checks for state present')\\n            if not module.check_mode:\\n                if not action_exists(client, module):\\n                    module.fail_json(msg='Content switching action does not exist', **module_result)\\n                if not action_identical(client, module, csaction_proxy):\\n                    module.fail_json(\\n                        msg='Content switching action differs from configured',\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if action_exists(client, module):\\n                    module.fail_json(msg='Content switching action still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Various small fixes (#27766)",
    "fixed_code": "def main():\\n    module_specific_arguments = dict(\\n        name=dict(type='str'),\\n        targetlbvserver=dict(type='str'),\\n        targetvserverexpr=dict(type='str'),\\n        comment=dict(type='str'),\\n    )\\n    argument_spec = dict()\\n    argument_spec.update(netscaler_common_arguments)\\n    argument_spec.update(module_specific_arguments)\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n    )\\n    module_result = dict(\\n        changed=False,\\n        failed=False,\\n        loglines=loglines\\n    )\\n    if not PYTHON_SDK_IMPORTED:\\n        module.fail_json(msg='Could not load nitro python sdk')\\n    client = get_nitro_client(module)\\n    try:\\n        client.login()\\n    except nitro_exception as e:\\n        msg = \"nitro exception during login. errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg)\\n    except Exception as e:\\n        if str(type(e)) == \"<class 'requests.exceptions.ConnectionError'>\":\\n            module.fail_json(msg='Connection error %s' % str(e))\\n        elif str(type(e)) == \"<class 'requests.exceptions.SSLError'>\":\\n            module.fail_json(msg='SSL Error %s' % str(e))\\n        else:\\n            module.fail_json(msg='Unexpected error during login %s' % str(e))\\n    readwrite_attrs = [\\n        'name',\\n        'targetlbvserver',\\n        'targetvserverexpr',\\n        'comment',\\n    ]\\n    readonly_attrs = [\\n        'hits',\\n        'referencecount',\\n        'undefhits',\\n        'builtin',\\n    ]\\n    immutable_attrs = [\\n        'name',\\n        'targetvserverexpr',\\n    ]\\n    transforms = {\\n    }\\n    csaction_proxy = ConfigProxy(\\n        actual=csaction(),\\n        client=client,\\n        attribute_values_dict=module.params,\\n        readwrite_attrs=readwrite_attrs,\\n        readonly_attrs=readonly_attrs,\\n        immutable_attrs=immutable_attrs,\\n        transforms=transforms,\\n    )\\n    try:\\n        ensure_feature_is_enabled(client, 'CS')\\n        if module.params['state'] == 'present':\\n            log('Applying actions for state present')\\n            if not action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.add()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            elif not action_identical(client, module, csaction_proxy):\\n                immutables_changed = get_immutables_intersection(csaction_proxy, diff_list(client, module, csaction_proxy).keys())\\n                if immutables_changed != []:\\n                    module.fail_json(\\n                        msg='Cannot update immutable attributes %s' % (immutables_changed,),\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n                if not module.check_mode:\\n                    csaction_proxy.update()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            log('Sanity checks for state present')\\n            if not module.check_mode:\\n                if not action_exists(client, module):\\n                    module.fail_json(msg='Content switching action does not exist', **module_result)\\n                if not action_identical(client, module, csaction_proxy):\\n                    module.fail_json(\\n                        msg='Content switching action differs from configured',\\n                        diff=diff_list(client, module, csaction_proxy),\\n                        **module_result\\n                    )\\n        elif module.params['state'] == 'absent':\\n            log('Applying actions for state absent')\\n            if action_exists(client, module):\\n                if not module.check_mode:\\n                    csaction_proxy.delete()\\n                    if module.params['save_config']:\\n                        client.save_config()\\n                module_result['changed'] = True\\n            else:\\n                module_result['changed'] = False\\n            if not module.check_mode:\\n                log('Sanity checks for state absent')\\n                if action_exists(client, module):\\n                    module.fail_json(msg='Content switching action still exists', **module_result)\\n    except nitro_exception as e:\\n        msg = \"nitro exception errorcode=%s, message=%s\" % (str(e.errorcode), e.message)\\n        module.fail_json(msg=msg, **module_result)\\n    client.logout()\\n    module.exit_json(**module_result)"
  },
  {
    "code": "def _Num(self, t):\\n        self.write(\"(\")\\n        self.write(repr(t.n))\\n        self.write(\")\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _Num(self, t):\\n        self.write(\"(\")\\n        self.write(repr(t.n))\\n        self.write(\")\")"
  },
  {
    "code": "def _possibly_convert_objects(values, convert_dates=True,\\n                              convert_numeric=True,\\n                              convert_timedeltas=True):\\n    if isinstance(values, (list, tuple)):\\n        values = np.array(values, dtype=np.object_)\\n    if not hasattr(values, 'dtype'):\\n        values = np.array([values], dtype=np.object_)\\n    if convert_dates and values.dtype == np.object_:\\n        if convert_dates == 'coerce':\\n            new_values = _possibly_cast_to_datetime(\\n                values, 'M8[ns]', coerce=True)\\n            if not isnull(new_values).all():\\n                values = new_values\\n        else:\\n            values = lib.maybe_convert_objects(\\n                values, convert_datetime=convert_dates)\\n    if convert_timedeltas and values.dtype == np.object_:\\n        if convert_timedeltas == 'coerce':\\n            from pandas.tseries.timedeltas import \\\\n                 _possibly_cast_to_timedelta\\n            values = _possibly_cast_to_timedelta(values, coerce=True)\\n            if not isnull(new_values).all():\\n                values = new_values\\n        else:\\n            values = lib.maybe_convert_objects(\\n                values, convert_timedelta=convert_timedeltas)\\n    if values.dtype == np.object_:\\n        if convert_numeric:\\n            try:\\n                new_values = lib.maybe_convert_numeric(\\n                    values, set(), coerce_numeric=True)\\n                if not isnull(new_values).all():\\n                    values = new_values\\n            except:\\n                pass\\n        else:\\n            values = lib.maybe_convert_objects(values)\\n    return values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _possibly_convert_objects(values, convert_dates=True,\\n                              convert_numeric=True,\\n                              convert_timedeltas=True):\\n    if isinstance(values, (list, tuple)):\\n        values = np.array(values, dtype=np.object_)\\n    if not hasattr(values, 'dtype'):\\n        values = np.array([values], dtype=np.object_)\\n    if convert_dates and values.dtype == np.object_:\\n        if convert_dates == 'coerce':\\n            new_values = _possibly_cast_to_datetime(\\n                values, 'M8[ns]', coerce=True)\\n            if not isnull(new_values).all():\\n                values = new_values\\n        else:\\n            values = lib.maybe_convert_objects(\\n                values, convert_datetime=convert_dates)\\n    if convert_timedeltas and values.dtype == np.object_:\\n        if convert_timedeltas == 'coerce':\\n            from pandas.tseries.timedeltas import \\\\n                 _possibly_cast_to_timedelta\\n            values = _possibly_cast_to_timedelta(values, coerce=True)\\n            if not isnull(new_values).all():\\n                values = new_values\\n        else:\\n            values = lib.maybe_convert_objects(\\n                values, convert_timedelta=convert_timedeltas)\\n    if values.dtype == np.object_:\\n        if convert_numeric:\\n            try:\\n                new_values = lib.maybe_convert_numeric(\\n                    values, set(), coerce_numeric=True)\\n                if not isnull(new_values).all():\\n                    values = new_values\\n            except:\\n                pass\\n        else:\\n            values = lib.maybe_convert_objects(values)\\n    return values"
  },
  {
    "code": "def foreign_related_fields(self):\\n        return tuple(rhs_field for lhs_field, rhs_field in self.related_fields if rhs_field)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def foreign_related_fields(self):\\n        return tuple(rhs_field for lhs_field, rhs_field in self.related_fields if rhs_field)"
  },
  {
    "code": "def _EndRecData64(fpin, offset, endrec):\\n    locatorSize = struct.calcsize(structEndArchive64Locator)\\n    fpin.seek(offset - locatorSize, 2)\\n    data = fpin.read(locatorSize)\\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\\n    if sig != stringEndArchive64Locator:\\n        return endrec\\n    if diskno != 0 or disks != 1:\\n        raise BadZipfile(\"zipfiles that span multiple disks are not supported\")\\n    endArchiveSize = struct.calcsize(structEndArchive64)\\n    fpin.seek(offset - locatorSize - endArchiveSize, 2)\\n    data = fpin.read(endArchiveSize)\\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\\n            dircount, dircount2, dirsize, diroffset = \\\\n            struct.unpack(structEndArchive64, data)\\n    if sig != stringEndArchive64:\\n        return endrec\\n    endrec[1] = disk_num\\n    endrec[2] = disk_dir\\n    endrec[3] = dircount\\n    endrec[4] = dircount2\\n    endrec[5] = dirsize\\n    endrec[6] = diroffset\\n    return endrec",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 64688 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r64688 | martin.v.loewis | 2008-07-03 14:51:14 +0200 (Do, 03 Jul 2008) | 9 lines\\n\\n  Patch #1622: Correct interpretation of various ZIP header fields.\\n\\n  Also fixes\\n  - Issue #1526: Allow more than 64k files to be added to Zip64 file.\\n\\n  - Issue #1746: Correct handling of zipfile archive comments (previously\\n    archives with comments over 4k were flagged as invalid). Allow writing\\n    Zip files with archives by setting the 'comment' attribute of a ZipFile.\\n........",
    "fixed_code": "def _EndRecData64(fpin, offset, endrec):\\n    fpin.seek(offset - sizeEndCentDir64Locator, 2)\\n    data = fpin.read(sizeEndCentDir64Locator)\\n    sig, diskno, reloff, disks = struct.unpack(structEndCentDir64Locator, data)\\n    if sig != magicEndCentDir64Locator:\\n        return endrec\\n    if diskno != 0 or disks != 1:\\n        raise BadZipfile(\"zipfiles that span multiple disks are not supported\")\\n    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)\\n    data = fpin.read(sizeEndCentDir64)\\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\\n            dircount, dircount2, dirsize, diroffset = \\\\n            struct.unpack(structEndCentDir64, data)\\n    if sig != magicEndCentDir64:\\n        return endrec\\n    endrec[_ECD_DISK_NUMBER] = disk_num\\n    endrec[_ECD_DISK_START] = disk_dir\\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\\n    endrec[_ECD_SIZE] = dirsize\\n    endrec[_ECD_OFFSET] = diroffset\\n    return endrec"
  },
  {
    "code": "def maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = iNaT\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, np.datetime64):\\n        fill_value = tslibs.Timestamp(fill_value).value\\n    elif issubclass(dtype.type, np.timedelta64):\\n        fill_value = tslibs.Timedelta(fill_value).value\\n    elif is_datetime64tz_dtype(dtype):\\n        if isna(fill_value):\\n            fill_value = NaT\\n    elif is_extension_array_dtype(dtype) and isna(fill_value):\\n        fill_value = dtype.na_value\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        else:\\n            fill_value = np.bool_(fill_value)\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n        elif issubclass(dtype.type, np.floating):\\n            if _check_lossless_cast(fill_value, dtype):\\n                fill_value = dtype.type(fill_value)\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    elif fill_value is None:\\n        if is_float_dtype(dtype) or is_complex_dtype(dtype):\\n            fill_value = np.nan\\n        elif is_integer_dtype(dtype):\\n            dtype = np.float64\\n            fill_value = np.nan\\n        elif is_datetime_or_timedelta_dtype(dtype):\\n            fill_value = iNaT\\n        else:\\n            dtype = np.object_\\n            fill_value = np.nan\\n    else:\\n        dtype = np.object_\\n    if is_extension_array_dtype(dtype):\\n        pass\\n    elif is_datetime64tz_dtype(dtype):\\n        pass\\n    elif issubclass(np.dtype(dtype).type, (bytes, str)):\\n        dtype = np.object_\\n    return dtype, fill_value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Change maybe_promote fill_value to dt64/td64 NaT instead of iNaT (#28725)",
    "fixed_code": "def maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = fill_value.dtype.type(\"NaT\", \"ns\")\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, np.datetime64):\\n        fill_value = tslibs.Timestamp(fill_value).to_datetime64()\\n    elif issubclass(dtype.type, np.timedelta64):\\n        fv = tslibs.Timedelta(fill_value)\\n        if fv is NaT:\\n            fill_value = np.timedelta64(\"NaT\", \"ns\")\\n        else:\\n            fill_value = fv.to_timedelta64()\\n    elif is_datetime64tz_dtype(dtype):\\n        if isna(fill_value):\\n            fill_value = NaT\\n    elif is_extension_array_dtype(dtype) and isna(fill_value):\\n        fill_value = dtype.na_value\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        else:\\n            fill_value = np.bool_(fill_value)\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n        elif issubclass(dtype.type, np.floating):\\n            if _check_lossless_cast(fill_value, dtype):\\n                fill_value = dtype.type(fill_value)\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    elif fill_value is None:\\n        if is_float_dtype(dtype) or is_complex_dtype(dtype):\\n            fill_value = np.nan\\n        elif is_integer_dtype(dtype):\\n            dtype = np.float64\\n            fill_value = np.nan\\n        elif is_datetime_or_timedelta_dtype(dtype):\\n            fill_value = dtype.type(\"NaT\", \"ns\")\\n        else:\\n            dtype = np.object_\\n            fill_value = np.nan\\n    else:\\n        dtype = np.object_\\n    if is_extension_array_dtype(dtype):\\n        pass\\n    elif is_datetime64tz_dtype(dtype):\\n        pass\\n    elif issubclass(np.dtype(dtype).type, (bytes, str)):\\n        dtype = np.object_\\n    return dtype, fill_value"
  },
  {
    "code": "def ogr(self):\\n        \"Returns the OGR Geometry for this Geometry.\"\\n        if not gdal.HAS_GDAL:\\n            raise GEOSException('GDAL required to convert to an OGRGeometry.')\\n        if self.srid:\\n            try:\\n                return gdal.OGRGeometry(self.wkb, self.srid)\\n            except gdal.SRSException:\\n                pass\\n        return gdal.OGRGeometry(self.wkb)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26753 -- Made GDAL a required dependency for contrib.gis\\n\\nThanks Tim Graham for the review.",
    "fixed_code": "def ogr(self):\\n        \"Returns the OGR Geometry for this Geometry.\"\\n        if self.srid:\\n            try:\\n                return gdal.OGRGeometry(self.wkb, self.srid)\\n            except gdal.SRSException:\\n                pass\\n        return gdal.OGRGeometry(self.wkb)\\n    @property"
  },
  {
    "code": "def call(self, inputs, states, constants=None, **kwargs):\\n\\tstate_size = (self.state_size[::-1]\\n\\t\\t\\t\\t  if self.reverse_state_order else self.state_size)\\n\\tnested_states = nest.pack_sequence_as(state_size, nest.flatten(states))\\n\\tnew_nested_states = []\\n\\tfor cell, states in zip(self.cells, nested_states):\\n\\t  states = states if nest.is_sequence(states) else [states]\\n\\t  is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None\\n\\t  states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\\n\\t  if generic_utils.has_arg(cell.call, 'constants'):\\n\\t\\tinputs, states = cell.call(inputs, states, constants=constants,\\n\\t\\t\\t\\t\\t\\t\\t\\t   **kwargs)\\n\\t  else:\\n\\t\\tinputs, states = cell.call(inputs, states, **kwargs)\\n\\t  new_nested_states.append(states)\\n\\treturn inputs, nest.pack_sequence_as(state_size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t nest.flatten(new_nested_states))\\n  @tf_utils.shape_type_conversion",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Forward \"training\" param from StackedRNNCell to cells wrapped by it.\\n\\n#32586",
    "fixed_code": "def call(self, inputs, states, constants=None, training=None, **kwargs):\\n\\tstate_size = (self.state_size[::-1]\\n\\t\\t\\t\\t  if self.reverse_state_order else self.state_size)\\n\\tnested_states = nest.pack_sequence_as(state_size, nest.flatten(states))\\n\\tnew_nested_states = []\\n\\tfor cell, states in zip(self.cells, nested_states):\\n\\t  states = states if nest.is_sequence(states) else [states]\\n\\t  is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None\\n\\t  states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\\n\\t  if generic_utils.has_arg(cell.call, 'training'):\\n\\t\\tkwargs['training'] = training\\n\\t  else:\\n\\t\\tkwargs.pop('training', None)\\n\\t  if generic_utils.has_arg(cell.call, 'constants'):\\n\\t\\tinputs, states = cell.call(inputs, states, constants=constants,\\n\\t\\t\\t\\t\\t\\t\\t\\t   **kwargs)\\n\\t  else:\\n\\t\\tinputs, states = cell.call(inputs, states, **kwargs)\\n\\t  new_nested_states.append(states)\\n\\treturn inputs, nest.pack_sequence_as(state_size,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t nest.flatten(new_nested_states))\\n  @tf_utils.shape_type_conversion"
  },
  {
    "code": "def maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = fill_value.dtype.type(\"NaT\", \"ns\")\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, np.datetime64):\\n        fill_value = tslibs.Timestamp(fill_value).to_datetime64()\\n    elif issubclass(dtype.type, np.timedelta64):\\n        fv = tslibs.Timedelta(fill_value)\\n        if fv is NaT:\\n            fill_value = np.timedelta64(\"NaT\", \"ns\")\\n        else:\\n            fill_value = fv.to_timedelta64()\\n    elif is_datetime64tz_dtype(dtype):\\n        if isna(fill_value):\\n            fill_value = NaT\\n    elif is_extension_array_dtype(dtype) and isna(fill_value):\\n        fill_value = dtype.na_value\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        else:\\n            fill_value = np.bool_(fill_value)\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n        elif issubclass(dtype.type, np.floating):\\n            if _check_lossless_cast(fill_value, dtype):\\n                fill_value = dtype.type(fill_value)\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    elif fill_value is None:\\n        if is_float_dtype(dtype) or is_complex_dtype(dtype):\\n            fill_value = np.nan\\n        elif is_integer_dtype(dtype):\\n            dtype = np.float64\\n            fill_value = np.nan\\n        elif is_datetime_or_timedelta_dtype(dtype):\\n            fill_value = dtype.type(\"NaT\", \"ns\")\\n        else:\\n            dtype = np.object_\\n            fill_value = np.nan\\n    else:\\n        dtype = np.object_\\n    if is_extension_array_dtype(dtype):\\n        pass\\n    elif is_datetime64tz_dtype(dtype):\\n        pass\\n    elif issubclass(np.dtype(dtype).type, (bytes, str)):\\n        dtype = np.object_\\n    return dtype, fill_value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_promote(dtype, fill_value=np.nan):\\n    if isinstance(fill_value, np.ndarray):\\n        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):\\n            fill_value = fill_value.dtype.type(\"NaT\", \"ns\")\\n        else:\\n            if fill_value.dtype == np.object_:\\n                dtype = np.dtype(np.object_)\\n            fill_value = np.nan\\n    if issubclass(dtype.type, np.datetime64):\\n        fill_value = tslibs.Timestamp(fill_value).to_datetime64()\\n    elif issubclass(dtype.type, np.timedelta64):\\n        fv = tslibs.Timedelta(fill_value)\\n        if fv is NaT:\\n            fill_value = np.timedelta64(\"NaT\", \"ns\")\\n        else:\\n            fill_value = fv.to_timedelta64()\\n    elif is_datetime64tz_dtype(dtype):\\n        if isna(fill_value):\\n            fill_value = NaT\\n    elif is_extension_array_dtype(dtype) and isna(fill_value):\\n        fill_value = dtype.na_value\\n    elif is_float(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            dtype = np.float64\\n    elif is_bool(fill_value):\\n        if not issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        else:\\n            fill_value = np.bool_(fill_value)\\n    elif is_integer(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, np.integer):\\n            arr = np.asarray(fill_value)\\n            if arr != arr.astype(dtype):\\n                dtype = arr.dtype\\n        elif issubclass(dtype.type, np.floating):\\n            if _check_lossless_cast(fill_value, dtype):\\n                fill_value = dtype.type(fill_value)\\n    elif is_complex(fill_value):\\n        if issubclass(dtype.type, np.bool_):\\n            dtype = np.object_\\n        elif issubclass(dtype.type, (np.integer, np.floating)):\\n            dtype = np.complex128\\n    elif fill_value is None:\\n        if is_float_dtype(dtype) or is_complex_dtype(dtype):\\n            fill_value = np.nan\\n        elif is_integer_dtype(dtype):\\n            dtype = np.float64\\n            fill_value = np.nan\\n        elif is_datetime_or_timedelta_dtype(dtype):\\n            fill_value = dtype.type(\"NaT\", \"ns\")\\n        else:\\n            dtype = np.object_\\n            fill_value = np.nan\\n    else:\\n        dtype = np.object_\\n    if is_extension_array_dtype(dtype):\\n        pass\\n    elif is_datetime64tz_dtype(dtype):\\n        pass\\n    elif issubclass(np.dtype(dtype).type, (bytes, str)):\\n        dtype = np.object_\\n    return dtype, fill_value"
  },
  {
    "code": "def coord_transform(self):\\n        \"Returns the coordinate transformation object.\"\\n        SpatialRefSys = self.spatial_backend.spatial_ref_sys()\\n        try:\\n            target_srs = SpatialRefSys.objects.get(srid=self.geo_col.srid).srs\\n            return CoordTransform(self.source_srs, target_srs)\\n        except Exception, msg:\\n            raise LayerMapError('Could not translate between the data source and model geometry: %s' % msg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed `LayerMapping` to work with PostGIS geography fields; removed `LayerMapping.geometry_column` and replaced with `LayerMapping.geometry_field` because getting the `geometry_columns` entry was completely unnecessary, and only the geometry field instance is needed; cleaned up and fleshed out the `geogapp` tests.",
    "fixed_code": "def coord_transform(self):\\n        \"Returns the coordinate transformation object.\"\\n        SpatialRefSys = self.spatial_backend.spatial_ref_sys()\\n        try:\\n            target_srs = SpatialRefSys.objects.get(srid=self.geo_field.srid).srs\\n            return CoordTransform(self.source_srs, target_srs)\\n        except Exception, msg:\\n            raise LayerMapError('Could not translate between the data source and model geometry: %s' % msg)"
  },
  {
    "code": "def reset(self):\\n        self.__buf = BytesIO()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reset(self):\\n        self.__buf = BytesIO()"
  },
  {
    "code": "def _create_customized_form_field_behaviours_schema_validator():\\n    schema = json.loads(\\n        importlib_resources.read_text('airflow', 'customized_form_field_behaviours.schema.json')\\n    )\\n    cls = jsonschema.validators.validator_for(schema)\\n    validator = cls(schema)\\n    return validator",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Move away from legacy importlib.resources API (#19091)",
    "fixed_code": "def _create_customized_form_field_behaviours_schema_validator():\\n    with resource_files(\"airflow\").joinpath(\"customized_form_field_behaviours.schema.json\").open(\"rb\") as f:\\n        schema = json.load(f)\\n    cls = jsonschema.validators.validator_for(schema)\\n    validator = cls(schema)\\n    return validator"
  },
  {
    "code": "def get_scorer(scoring):\\n    if isinstance(scoring, six.string_types):\\n        try:\\n            scorer = SCORERS[scoring]\\n        except KeyError:\\n            raise ValueError('%r is not a valid scoring value. '\\n                             'Valid options are %s'\\n                             % (scoring, sorted(SCORERS.keys())))\\n    else:\\n        scorer = scoring\\n    return scorer",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_scorer(scoring):\\n    if isinstance(scoring, six.string_types):\\n        try:\\n            scorer = SCORERS[scoring]\\n        except KeyError:\\n            raise ValueError('%r is not a valid scoring value. '\\n                             'Valid options are %s'\\n                             % (scoring, sorted(SCORERS.keys())))\\n    else:\\n        scorer = scoring\\n    return scorer"
  },
  {
    "code": "def is_bool_indexer(key):\\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)):\\n        if key.dtype == np.object_:\\n            key = np.asarray(_values_from_object(key))\\n            if not lib.is_bool_array(key):\\n                if isna(key).any():\\n                    raise ValueError('cannot index with vector containing '\\n                                     'NA / NaN values')\\n                return False\\n            return True\\n        elif key.dtype == np.bool_:\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  \\n            return False\\n    return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_bool_indexer(key):\\n    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)):\\n        if key.dtype == np.object_:\\n            key = np.asarray(_values_from_object(key))\\n            if not lib.is_bool_array(key):\\n                if isna(key).any():\\n                    raise ValueError('cannot index with vector containing '\\n                                     'NA / NaN values')\\n                return False\\n            return True\\n        elif key.dtype == np.bool_:\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  \\n            return False\\n    return False"
  },
  {
    "code": "def itemsize(self):\\n        return self._data.itemsize",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: deprecate IntervalIndex.itemsize and remove IntervalArray.itemsize (#22149)",
    "fixed_code": "def itemsize(self):\\n        msg = ('IntervalIndex.itemsize is deprecated and will be removed in '\\n               'a future version')\\n        warnings.warn(msg, FutureWarning, stacklevel=2)\\n        with warnings.catch_warnings():\\n            warnings.simplefilter('ignore')\\n            return self.left.itemsize + self.right.itemsize"
  },
  {
    "code": "def _create_deprecated_modules_files():\\n    for (new_module_name, deprecated_path,\\n         correct_import_path, _) in _DEPRECATED_MODULES:\\n        relative_dots = deprecated_path.count(\".\") * \".\"\\n        deprecated_content = _FILE_CONTENT_TEMPLATE.format(\\n            new_module_name=new_module_name,\\n            relative_dots=relative_dots,\\n            deprecated_path=deprecated_path,\\n            correct_import_path=correct_import_path)\\n        with _get_deprecated_path(deprecated_path).open('w') as f:\\n            f.write(deprecated_content)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _create_deprecated_modules_files():\\n    for (new_module_name, deprecated_path,\\n         correct_import_path, _) in _DEPRECATED_MODULES:\\n        relative_dots = deprecated_path.count(\".\") * \".\"\\n        deprecated_content = _FILE_CONTENT_TEMPLATE.format(\\n            new_module_name=new_module_name,\\n            relative_dots=relative_dots,\\n            deprecated_path=deprecated_path,\\n            correct_import_path=correct_import_path)\\n        with _get_deprecated_path(deprecated_path).open('w') as f:\\n            f.write(deprecated_content)"
  },
  {
    "code": "def __init__(self, obj, keys=None, axis=0, level=None,\\n                 grouper=None, exclusions=None, selection=None, as_index=True,\\n                 sort=True, group_keys=True):\\n        self._selection = selection\\n        if isinstance(obj, NDFrame):\\n            obj._consolidate_inplace()\\n        self.obj = obj\\n        self.axis = axis\\n        self.level = level\\n        if not as_index:\\n            if not isinstance(obj, DataFrame):\\n                raise TypeError('as_index=False only valid with DataFrame')\\n            if axis != 0:\\n                raise ValueError('as_index=False only valid for axis=0')\\n        self.as_index = as_index\\n        self.keys = keys\\n        self.sort = sort\\n        self.group_keys = group_keys\\n        if grouper is None:\\n            grouper, exclusions = _get_grouper(obj, keys, axis=axis,\\n                                               level=level, sort=sort)\\n        self.grouper = grouper\\n        self.exclusions = set(exclusions) if exclusions else set()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, obj, keys=None, axis=0, level=None,\\n                 grouper=None, exclusions=None, selection=None, as_index=True,\\n                 sort=True, group_keys=True):\\n        self._selection = selection\\n        if isinstance(obj, NDFrame):\\n            obj._consolidate_inplace()\\n        self.obj = obj\\n        self.axis = axis\\n        self.level = level\\n        if not as_index:\\n            if not isinstance(obj, DataFrame):\\n                raise TypeError('as_index=False only valid with DataFrame')\\n            if axis != 0:\\n                raise ValueError('as_index=False only valid for axis=0')\\n        self.as_index = as_index\\n        self.keys = keys\\n        self.sort = sort\\n        self.group_keys = group_keys\\n        if grouper is None:\\n            grouper, exclusions = _get_grouper(obj, keys, axis=axis,\\n                                               level=level, sort=sort)\\n        self.grouper = grouper\\n        self.exclusions = set(exclusions) if exclusions else set()"
  },
  {
    "code": "def __lt__(self, other):\\n        return (self.priority, self.slot) < (other.priority, other.slot)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __lt__(self, other):\\n        return (self.priority, self.slot) < (other.priority, other.slot)"
  },
  {
    "code": "def generate_pages(current_page, num_of_pages,\\n                   search=None, showPaused=None, window=7):\\n    void_link = 'javascript:void(0)'\\n    first_node = \\n    previous_node = \\n    next_node = \\n    last_node = \\n    page_node = \\n    output = ['<ul class=\"pagination\" style=\"margin-top:0px;\">']\\n    is_disabled = 'disabled' if current_page <= 0 else ''\\n    output.append(first_node.format(href_link=\"?{}\"\\n                                    .format(get_params(page=0,\\n                                                       search=search,\\n                                                       showPaused=showPaused)),\\n                                    disabled=is_disabled))\\n    page_link = void_link\\n    if current_page > 0:\\n        page_link = '?{}'.format(get_params(page=(current_page - 1),\\n                                            search=search,\\n                                            showPaused=showPaused))\\n    output.append(previous_node.format(href_link=page_link,\\n                                       disabled=is_disabled))\\n    mid = int(window / 2)\\n    last_page = num_of_pages - 1\\n    if current_page <= mid or num_of_pages < window:\\n        pages = [i for i in range(0, min(num_of_pages, window))]\\n    elif mid < current_page < last_page - mid:\\n        pages = [i for i in range(current_page - mid, current_page + mid + 1)]\\n    else:\\n        pages = [i for i in range(num_of_pages - window, last_page + 1)]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_pages(current_page, num_of_pages,\\n                   search=None, showPaused=None, window=7):\\n    void_link = 'javascript:void(0)'\\n    first_node = \\n    previous_node = \\n    next_node = \\n    last_node = \\n    page_node = \\n    output = ['<ul class=\"pagination\" style=\"margin-top:0px;\">']\\n    is_disabled = 'disabled' if current_page <= 0 else ''\\n    output.append(first_node.format(href_link=\"?{}\"\\n                                    .format(get_params(page=0,\\n                                                       search=search,\\n                                                       showPaused=showPaused)),\\n                                    disabled=is_disabled))\\n    page_link = void_link\\n    if current_page > 0:\\n        page_link = '?{}'.format(get_params(page=(current_page - 1),\\n                                            search=search,\\n                                            showPaused=showPaused))\\n    output.append(previous_node.format(href_link=page_link,\\n                                       disabled=is_disabled))\\n    mid = int(window / 2)\\n    last_page = num_of_pages - 1\\n    if current_page <= mid or num_of_pages < window:\\n        pages = [i for i in range(0, min(num_of_pages, window))]\\n    elif mid < current_page < last_page - mid:\\n        pages = [i for i in range(current_page - mid, current_page + mid + 1)]\\n    else:\\n        pages = [i for i in range(num_of_pages - window, last_page + 1)]"
  },
  {
    "code": "def parse(self, fp):\\n\\t\\ttry:\\n\\t\\t\\tself._fp = fp\\n\\t\\t\\tself._fp.seek(-32, os.SEEK_END)\\n\\t\\t\\ttrailer = self._fp.read(32)\\n\\t\\t\\tif len(trailer) != 32:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\t\\t(\\n\\t\\t\\t\\toffset_size, self._ref_size, num_objects, top_object,\\n\\t\\t\\t\\toffset_table_offset\\n\\t\\t\\t) = struct.unpack('>6xBBQQQ', trailer)\\n\\t\\t\\tself._fp.seek(offset_table_offset)\\n\\t\\t\\tself._object_offsets = self._read_ints(num_objects, offset_size)\\n\\t\\t\\tself._objects = [_undefined] * num_objects\\n\\t\\t\\treturn self._read_object(top_object)\\n\\t\\texcept (OSError, IndexError, struct.error, OverflowError,\\n\\t\\t\\t\\tUnicodeDecodeError):\\n\\t\\t\\traise InvalidFileException()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-42103: Improve validation of Plist files. (GH-22882)\\n\\n  with extremely large number of objects or collection sizes.\\n(cherry picked from commit 34637a0ce21e7261b952fbd9d006474cc29b681f)",
    "fixed_code": "def parse(self, fp):\\n\\t\\ttry:\\n\\t\\t\\tself._fp = fp\\n\\t\\t\\tself._fp.seek(-32, os.SEEK_END)\\n\\t\\t\\ttrailer = self._fp.read(32)\\n\\t\\t\\tif len(trailer) != 32:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\t\\t(\\n\\t\\t\\t\\toffset_size, self._ref_size, num_objects, top_object,\\n\\t\\t\\t\\toffset_table_offset\\n\\t\\t\\t) = struct.unpack('>6xBBQQQ', trailer)\\n\\t\\t\\tself._fp.seek(offset_table_offset)\\n\\t\\t\\tself._object_offsets = self._read_ints(num_objects, offset_size)\\n\\t\\t\\tself._objects = [_undefined] * num_objects\\n\\t\\t\\treturn self._read_object(top_object)\\n\\t\\texcept (OSError, IndexError, struct.error, OverflowError,\\n\\t\\t\\t\\tValueError):\\n\\t\\t\\traise InvalidFileException()"
  },
  {
    "code": "def cast(self, dtype):\\n        new_blocks = []\\n        for block in self.blocks:\\n            newb = make_block(block.values.astype(dtype), block.ref_locs,\\n                              block.ref_columns, _columns=block._columns)\\n            new_blocks.append(newb)\\n        new_mgr = BlockManager(new_blocks, self.index, self.columns)\\n        return new_mgr.consolidate()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "unit tests pass...winning!",
    "fixed_code": "def cast(self, dtype):\\n        new_blocks = []\\n        for block in self.blocks:\\n            newb = make_block(block.values.astype(dtype), block.columns,\\n                              block.ref_columns)\\n            new_blocks.append(newb)\\n        new_mgr = BlockManager(new_blocks, self.index, self.columns)\\n        return new_mgr.consolidate()"
  },
  {
    "code": "def load(data):\\n    if hasattr(data, 'read') and hasattr(data.read, '__call__'):\\n        data = data.read()\\n    if isinstance(data, basestring):\\n        try:\\n            try:\\n                return json.loads(data)\\n            except:\\n                return safe_load(data)\\n        except:\\n            raise AnsibleParserError(\"data was not valid yaml\")\\n    raise AnsibleInternalError(\"expected file or string, got %s\" % type(data))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load(data):\\n    if hasattr(data, 'read') and hasattr(data.read, '__call__'):\\n        data = data.read()\\n    if isinstance(data, basestring):\\n        try:\\n            try:\\n                return json.loads(data)\\n            except:\\n                return safe_load(data)\\n        except:\\n            raise AnsibleParserError(\"data was not valid yaml\")\\n    raise AnsibleInternalError(\"expected file or string, got %s\" % type(data))"
  },
  {
    "code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    try:\\n        return compiler(source, filename, symbol)\\n    except SyntaxError:  \\n        pass\\n    with warnings.catch_warnings():\\n        warnings.simplefilter(\"error\")\\n        try:\\n            compiler(source + \"\\n\", filename, symbol)\\n        except SyntaxError as e:\\n            if \"incomplete input\" in str(e):\\n                return None\\n            raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-96052: codeop: fix handling compiler warnings in incomplete input (GH-96132)\\n\\nPreviously codeop.compile_command() emitted compiler warnings (SyntaxWarning or\\nDeprecationWarning) and raised a SyntaxError for incomplete input containing\\na potentially incorrect code. Now it always returns None for incomplete input\\nwithout emitting any warnings.",
    "fixed_code": "def _maybe_compile(compiler, source, filename, symbol):\\n    for line in source.split(\"\\n\"):\\n        line = line.strip()\\n        if line and line[0] != '\\n            break               \\n    else:\\n        if symbol != \"eval\":\\n            source = \"pass\"     \\n    with warnings.catch_warnings():\\n        warnings.simplefilter(\"ignore\", (SyntaxWarning, DeprecationWarning))\\n        try:\\n            compiler(source, filename, symbol)\\n        except SyntaxError:  \\n            try:\\n                compiler(source + \"\\n\", filename, symbol)\\n                return None\\n            except SyntaxError as e:\\n                if \"incomplete input\" in str(e):\\n                    return None\\n    return compiler(source, filename, symbol)"
  },
  {
    "code": "def select_as_coordinates(self, key, where=None, **kwargs):\\n        return self.get_storer(key).read_coordinates(where = where, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: New keywords ``iterator=boolean``, and ``chunksize=number_in_a_chunk`` are      provided to support iteration on ``select`` and ``select_as_multiple`` (GH3076_)",
    "fixed_code": "def select_as_coordinates(self, key, where=None, start=None, stop=None, **kwargs):\\n        return self.get_storer(key).read_coordinates(where=where, start=start, stop=stop, **kwargs)"
  },
  {
    "code": "def _check_formset(self, cls):\\n        if not issubclass(cls.formset, BaseModelFormSet):\\n            return must_inherit_from(parent='BaseModelFormSet', option='formset',\\n                                     obj=cls, id='admin.E205')\\n        else:\\n            return []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #17642 -- Added min_num support to modelformsets, inlines, and the admin.\\n\\nThanks Stephen Burrows for work on the patch as well.\\n\\nForwardport of 2914f66983a92fcae55673c517dd8d01e8c238c4 from stable/1.7.x",
    "fixed_code": "def _check_formset(self, cls):\\n        if not issubclass(cls.formset, BaseModelFormSet):\\n            return must_inherit_from(parent='BaseModelFormSet', option='formset',\\n                                     obj=cls, id='admin.E206')\\n        else:\\n            return []"
  },
  {
    "code": "def deconstruct(self):\\n        name, path, args, kwargs = super(ArrayField, self).deconstruct()\\n        if path == 'django.contrib.postgres.fields.array.ArrayField':\\n            path = 'django.contrib.postgres.fields.ArrayField'\\n        kwargs.update({\\n            'base_field': self.base_field,\\n            'size': self.size,\\n        })\\n        return name, path, args, kwargs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def deconstruct(self):\\n        name, path, args, kwargs = super(ArrayField, self).deconstruct()\\n        if path == 'django.contrib.postgres.fields.array.ArrayField':\\n            path = 'django.contrib.postgres.fields.ArrayField'\\n        kwargs.update({\\n            'base_field': self.base_field,\\n            'size': self.size,\\n        })\\n        return name, path, args, kwargs"
  },
  {
    "code": "def _set_content(self, value):\\n        if hasattr(value, '__iter__') and not isinstance(value, (bytes, six.text_type)):\\n            self._container = value\\n            self._base_content_is_iter = True\\n        else:\\n            self._container = [value]\\n            self._base_content_is_iter = False\\n    content = property(_get_content, _set_content)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[py3] Ported django.http according to PEP 3333.\\n\\nPerfomed some style cleanup while I was in the area.",
    "fixed_code": "def _set_content(self, value):\\n        if hasattr(value, '__iter__') and not isinstance(value, six.string_types):\\n            self._container = value\\n            self._base_content_is_iter = True\\n        else:\\n            self._container = [value]\\n            self._base_content_is_iter = False\\n    content = property(_get_content, _set_content)"
  },
  {
    "code": "def _valid_locales(locales, normalize):\\n    if normalize:\\n        normalizer = lambda x: locale.normalize(x.strip())\\n    else:\\n        normalizer = lambda x: x.strip()\\n    return list(filter(can_set_locale, map(normalizer, locales)))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: remove extant uses of built-in filter function (#35717)",
    "fixed_code": "def _valid_locales(locales, normalize):\\n    return [\\n        loc\\n        for loc in (\\n            locale.normalize(loc.strip()) if normalize else loc.strip()\\n            for loc in locales\\n        )\\n        if can_set_locale(loc)\\n    ]"
  },
  {
    "code": "def update_available_resource(self, context):\\n\\t\\tresources = self.driver.get_available_resource()\\n\\t\\tif not resources:\\n\\t\\t\\tLOG.audit(_(\"Virt driver does not support \"\\n\\t\\t\\t\\t\"'get_available_resource'  Compute tracking is disabled.\"))\\n\\t\\t\\tself.compute_node = None\\n\\t\\t\\tself.claims = {}\\n\\t\\t\\treturn\\n\\t\\tself._verify_resources(resources)\\n\\t\\tself._report_hypervisor_resource_view(resources)\\n\\t\\tself._purge_expired_claims()\\n\\t\\tinstances = db.instance_get_all_by_host(context, self.host)\\n\\t\\tself._update_usage_from_instances(resources, instances)\\n\\t\\tself._report_final_resource_view(resources)\\n\\t\\tself._sync_compute_node(context, resources)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_available_resource(self, context):\\n\\t\\tresources = self.driver.get_available_resource()\\n\\t\\tif not resources:\\n\\t\\t\\tLOG.audit(_(\"Virt driver does not support \"\\n\\t\\t\\t\\t\"'get_available_resource'  Compute tracking is disabled.\"))\\n\\t\\t\\tself.compute_node = None\\n\\t\\t\\tself.claims = {}\\n\\t\\t\\treturn\\n\\t\\tself._verify_resources(resources)\\n\\t\\tself._report_hypervisor_resource_view(resources)\\n\\t\\tself._purge_expired_claims()\\n\\t\\tinstances = db.instance_get_all_by_host(context, self.host)\\n\\t\\tself._update_usage_from_instances(resources, instances)\\n\\t\\tself._report_final_resource_view(resources)\\n\\t\\tself._sync_compute_node(context, resources)"
  },
  {
    "code": "def __contains__(self, key):\\n        try:\\n            return key in self._engine\\n        except TypeError:\\n            return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __contains__(self, key):\\n        try:\\n            return key in self._engine\\n        except TypeError:\\n            return False"
  },
  {
    "code": "def get_all_data(self, call=True, put=True):\\n        to_ret = Series({'calls': call, 'puts': put})\\n        to_ret = to_ret[to_ret].index\\n        try:\\n            months = self.months\\n        except AttributeError:\\n            months = self._get_expiry_months()\\n        all_data = []\\n        for name in to_ret:\\n            for month in months:\\n                m2 = month.month\\n                y2 = month.year\\n                m1 = _two_char_month(m2)\\n                nam = name + str(m1) + str(y2)[2:]\\n                try:  \\n                    frame = getattr(self, nam)\\n                except AttributeError:\\n                    meth_name = 'get_{0}_data'.format(name[:-1])\\n                    frame = getattr(self, meth_name)(expiry=month)\\n                all_data.append(frame)\\n        return concat(all_data).sortlevel()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix pandas.io.data.Options for change in format of Yahoo Option page\\n\\nENH: Automatically choose next expiry if expiry date isn't valid\\n\\nCOMPAT: Remove dictionary comprehension to pass 2.6 test\\n\\nBUG: Add check that tables were downloaded.\\n\\nENH: Replace third_saturday function with Pandas offset\\n\\nBUG: Check to make sure enough option tables are downloaded.\\n\\nTST: Add sample options page during market open.\\n\\nDOC: Add bug fix report to v0.15.1.txt for data.Options\\n\\nBUG: Ensure that the value used for chopping is within the range of strikes.\\n\\nTST: Add test for requesting out of range chop\\n\\nBUG: Fix missing underlying price and quote time in first process data\\n\\nBUG: Fix AM/PM on quote time\\n\\nENH: Refactor to expose available expiry dates\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nDOC: Update documentation for Options given Yahoo change.\\n\\nBUG: Undo accidental deletion of privat emethods\\n\\nENH: Add ability to use list as expiry parameter.\\n\\nAlso has year & month return all data for the specified year and month.\\n\\nTST: Remove tests for warnings that are no longer in use.\\n\\nDOC: Update docstrings of Options class.",
    "fixed_code": "def get_all_data(self, call=True, put=True):\\n        try:\\n            expiry_dates = self.expiry_dates\\n        except AttributeError:\\n            expiry_dates, _ = self._get_expiry_dates_and_links()\\n        return self._get_data_in_date_range(dates=expiry_dates, call=call, put=put)"
  },
  {
    "code": "def _isnull_old(obj):\\n    ''\\n    if lib.isscalar(obj):\\n        return lib.checknull_old(obj)\\n    if isinstance(obj, (ABCSeries, np.ndarray)):\\n        return _isnull_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj.apply(_isnull_old)\\n    elif isinstance(obj, list) or hasattr(obj, '__array__'):\\n        return _isnull_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None\\n_isnull = _isnull_new",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix segfault on isnull(MultiIndex)\\n\\nNow raises NotImplementedError b/c not yet clear what it should return.",
    "fixed_code": "def _isnull_old(obj):\\n    ''\\n    if lib.isscalar(obj):\\n        return lib.checknull_old(obj)\\n    elif isinstance(obj, pd.MultiIndex):\\n        raise NotImplementedError(\"isnull is not defined for MultiIndex\")\\n    elif isinstance(obj, (ABCSeries, np.ndarray)):\\n        return _isnull_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj.apply(_isnull_old)\\n    elif isinstance(obj, list) or hasattr(obj, '__array__'):\\n        return _isnull_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None\\n_isnull = _isnull_new"
  },
  {
    "code": "def getframeinfo(frame, context=1):\\n    if istraceback(frame):\\n        lineno = frame.tb_lineno\\n        frame = frame.tb_frame\\n    else:\\n        lineno = frame.f_lineno\\n    if not isframe(frame):\\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\\n    filename = getsourcefile(frame) or getfile(frame)\\n    if context > 0:\\n        start = lineno - 1 - context//2\\n        try:\\n            lines, lnum = findsource(frame)\\n        except OSError:\\n            lines = index = None\\n        else:\\n            start = max(0, min(start, len(lines) - context))\\n            lines = lines[start:start+context]\\n            index = lineno - 1 - start\\n    else:\\n        lines = index = None\\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-88116: Enhance the inspect frame APIs to use the extended position information (GH-91531)",
    "fixed_code": "def getframeinfo(frame, context=1):\\n    if istraceback(frame):\\n        positions = _get_code_position_from_tb(frame)\\n        lineno = frame.tb_lineno\\n        frame = frame.tb_frame\\n    else:\\n        lineno = frame.f_lineno\\n        positions = _get_code_position(frame.f_code, frame.f_lasti)\\n    if positions[0] is None:\\n        frame, *positions = (frame, lineno, *positions[1:])\\n    else:\\n        frame, *positions = (frame, *positions)\\n    lineno = positions[0]\\n    if not isframe(frame):\\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\\n    filename = getsourcefile(frame) or getfile(frame)\\n    if context > 0:\\n        start = lineno - 1 - context//2\\n        try:\\n            lines, lnum = findsource(frame)\\n        except OSError:\\n            lines = index = None\\n        else:\\n            start = max(0, min(start, len(lines) - context))\\n            lines = lines[start:start+context]\\n            index = lineno - 1 - start\\n    else:\\n        lines = index = None\\n    return Traceback(filename, lineno, frame.f_code.co_name, lines,\\n                     index, positions=dis.Positions(*positions))"
  },
  {
    "code": "def prepare_test_settings(self, alias):\\n        try:\\n            conn = self.databases[alias]\\n        except KeyError:\\n            raise ConnectionDoesNotExist(\"The connection %s doesn't exist\" % alias)\\n        test_settings = conn.setdefault('TEST', {})\\n        for key, value in six.iteritems(conn):\\n            if key.startswith('TEST_'):\\n                new_key = key[5:]\\n                new_key = self.TEST_SETTING_RENAMES.get(new_key, new_key)\\n                if new_key in test_settings:\\n                    raise ImproperlyConfigured(\"Connection %s has both %s and TEST[%s] specified.\" %\\n                                               (alias, key, new_key))\\n                warnings.warn(\"In Django 1.9 the %s connection setting will be moved \"\\n                              \"to a %s entry in the TEST setting\" % (key, new_key),\\n                              RemovedInDjango19Warning, stacklevel=2)\\n                test_settings[new_key] = value\\n        for key in list(conn.keys()):\\n            if key.startswith('TEST_'):\\n                del conn[key]\\n        for key, new_key in six.iteritems(self.TEST_SETTING_RENAMES):\\n            if key in test_settings:\\n                warnings.warn(\"Test setting %s was renamed to %s; specified value (%s) ignored\" %\\n                              (key, new_key, test_settings[key]), stacklevel=2)\\n        for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']:\\n            test_settings.setdefault(key, None)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #22811 -- Allowed setting both the old and new TEST database settings.\\n\\nAn ImproperlyConfigured exception will be raised they mismatch.",
    "fixed_code": "def prepare_test_settings(self, alias):\\n        try:\\n            conn = self.databases[alias]\\n        except KeyError:\\n            raise ConnectionDoesNotExist(\"The connection %s doesn't exist\" % alias)\\n        test_dict_set = 'TEST' in conn\\n        test_settings = conn.setdefault('TEST', {})\\n        old_test_settings = {}\\n        for key, value in six.iteritems(conn):\\n            if key.startswith('TEST_'):\\n                new_key = key[5:]\\n                new_key = self.TEST_SETTING_RENAMES.get(new_key, new_key)\\n                old_test_settings[new_key] = value\\n        if old_test_settings:\\n            if test_dict_set:\\n                if test_settings != old_test_settings:\\n                    raise ImproperlyConfigured(\\n                        \"Connection '%s' has mismatched TEST and TEST_* \"\\n                        \"database settings.\" % alias)\\n            else:\\n                test_settings = old_test_settings\\n                for key, _ in six.iteritems(old_test_settings):\\n                    warnings.warn(\"In Django 1.9 the %s connection setting will be moved \"\\n                                  \"to a %s entry in the TEST setting\" %\\n                                  (self.TEST_SETTING_RENAMES_REVERSE.get(key, key), key),\\n                                  RemovedInDjango19Warning, stacklevel=2)\\n        for key in list(conn.keys()):\\n            if key.startswith('TEST_'):\\n                del conn[key]\\n        for key, new_key in six.iteritems(self.TEST_SETTING_RENAMES):\\n            if key in test_settings:\\n                warnings.warn(\"Test setting %s was renamed to %s; specified value (%s) ignored\" %\\n                              (key, new_key, test_settings[key]), stacklevel=2)\\n        for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']:\\n            test_settings.setdefault(key, None)"
  },
  {
    "code": "def detect_modules(self):\\n        if not cross_compiling:\\n            add_dir_to_list(self.compiler.library_dirs, '/usr/local/lib')\\n            add_dir_to_list(self.compiler.include_dirs, '/usr/local/include')\\n        if cross_compiling:\\n            self.add_gcc_paths()\\n        self.add_multiarch_paths()\\n        for env_var, arg_name, dir_list in (\\n                ('LDFLAGS', '-R', self.compiler.runtime_library_dirs),\\n                ('LDFLAGS', '-L', self.compiler.library_dirs),\\n                ('CPPFLAGS', '-I', self.compiler.include_dirs)):\\n            env_val = sysconfig.get_config_var(env_var)\\n            if env_val:\\n                env_val = re.sub(r'(^|\\s+)-(-|(?!%s))' % arg_name[1],\\n                                 ' ', env_val)\\n                parser = optparse.OptionParser()\\n                parser.allow_interspersed_args = True\\n                parser.error = lambda msg: None\\n                parser.add_option(arg_name, dest=\"dirs\", action=\"append\")\\n                options = parser.parse_args(env_val.split())[0]\\n                if options.dirs:\\n                    for directory in reversed(options.dirs):\\n                        add_dir_to_list(dir_list, directory)\\n        if os.path.normpath(sys.base_prefix) != '/usr' \\\\n                and not sysconfig.get_config_var('PYTHONFRAMEWORK'):\\n            add_dir_to_list(self.compiler.library_dirs,\\n                            sysconfig.get_config_var(\"LIBDIR\"))\\n            add_dir_to_list(self.compiler.include_dirs,\\n                            sysconfig.get_config_var(\"INCLUDEDIR\"))\\n        if not cross_compiling:\\n            lib_dirs = self.compiler.library_dirs + [\\n                '/lib64', '/usr/lib64',\\n                '/lib', '/usr/lib',\\n                ]\\n            inc_dirs = self.compiler.include_dirs + ['/usr/include']\\n        else:\\n            lib_dirs = self.compiler.library_dirs[:]\\n            inc_dirs = self.compiler.include_dirs[:]\\n        exts = []\\n        missing = []\\n        config_h = sysconfig.get_config_h_filename()\\n        with open(config_h) as file:\\n            config_h_vars = sysconfig.parse_config_h(file)\\n        srcdir = sysconfig.get_config_var('srcdir')\\n        if host_platform in ['osf1', 'unixware7', 'openunix8']:\\n            lib_dirs += ['/usr/ccs/lib']\\n        if host_platform == 'hp-ux11':\\n            lib_dirs += ['/usr/lib/hpux64', '/usr/lib/hpux32']\\n        if host_platform == 'darwin':\\n            cflags, ldflags = sysconfig.get_config_vars(\\n                    'CFLAGS', 'LDFLAGS')\\n            for item in cflags.split():\\n                if item.startswith('-I'):\\n                    inc_dirs.append(item[2:])\\n            for item in ldflags.split():\\n                if item.startswith('-L'):\\n                    lib_dirs.append(item[2:])\\n        math_libs = ['m']\\n        if host_platform == 'darwin':\\n            math_libs = []\\n        exts.append( Extension('array', ['arraymodule.c']) )\\n        exts.append( Extension('cmath', ['cmathmodule.c', '_math.c'],\\n                               depends=['_math.h'],\\n                               libraries=math_libs) )\\n        exts.append( Extension('math',  ['mathmodule.c', '_math.c'],\\n                               depends=['_math.h'],\\n                               libraries=math_libs) )\\n        time_libs = []\\n        lib = sysconfig.get_config_var('TIMEMODULE_LIB')\\n        if lib:\\n            time_libs.append(lib)\\n        exts.append( Extension('time', ['timemodule.c'],\\n                               libraries=time_libs) )\\n        exts.append( Extension('_datetime', ['_datetimemodule.c']) )\\n        exts.append( Extension(\"_random\", [\"_randommodule.c\"]) )\\n        exts.append( Extension(\"_bisect\", [\"_bisectmodule.c\"]) )\\n        exts.append( Extension(\"_heapq\", [\"_heapqmodule.c\"]) )\\n        exts.append( Extension(\"_pickle\", [\"_pickle.c\"]) )\\n        exts.append( Extension(\"atexit\", [\"atexitmodule.c\"]) )\\n        exts.append( Extension(\"_json\", [\"_json.c\"]) )\\n        exts.append( Extension('_testcapi', ['_testcapimodule.c'],\\n                               depends=['testcapi_long.h']) )\\n        exts.append( Extension('_testbuffer', ['_testbuffer.c']) )\\n        exts.append( Extension('_testimportmultiple', ['_testimportmultiple.c']) )\\n        exts.append( Extension('_lsprof', ['_lsprof.c', 'rotatingtree.c']) )\\n        exts.append( Extension('unicodedata', ['unicodedata.c']) )\\n        libs = []\\n        if (config_h_vars.get('FLOCK_NEEDS_LIBBSD', False)):\\n            libs = ['bsd']\\n        exts.append( Extension('fcntl', ['fcntlmodule.c'], libraries=libs) )\\n        exts.append( Extension('pwd', ['pwdmodule.c']) )\\n        exts.append( Extension('grp', ['grpmodule.c']) )\\n        if (config_h_vars.get('HAVE_GETSPNAM', False) or\\n                config_h_vars.get('HAVE_GETSPENT', False)):\\n            exts.append( Extension('spwd', ['spwdmodule.c']) )\\n        else:\\n            missing.append('spwd')\\n        exts.append( Extension('select', ['selectmodule.c']) )\\n        exts.append( Extension('parser', ['parsermodule.c']) )\\n        exts.append( Extension('mmap', ['mmapmodule.c']) )\\n        exts.append( Extension('syslog', ['syslogmodule.c']) )\\n        exts.append( Extension('audioop', ['audioop.c']) )\\n        do_readline = self.compiler.find_library_file(lib_dirs, 'readline')\\n        readline_termcap_library = \"\"\\n        curses_library = \"\"\\n        tmpfile = os.path.join(self.build_temp, 'readline_termcap_lib')\\n        if not os.path.exists(self.build_temp):\\n            os.makedirs(self.build_temp)\\n        if do_readline:\\n            if cross_compiling:\\n                ret = os.system(\"%s -d %s | grep '(NEEDED)' > %s\" \\\\n                                % (sysconfig.get_config_var('READELF'),\\n                                   do_readline, tmpfile))\\n            elif find_executable('ldd'):\\n                ret = os.system(\"ldd %s > %s\" % (do_readline, tmpfile))\\n            else:\\n                ret = 256\\n            if ret >> 8 == 0:\\n                with open(tmpfile) as fp:\\n                    for ln in fp:\\n                        if 'curses' in ln:\\n                            readline_termcap_library = re.sub(\\n                                r'.*lib(n?cursesw?)\\.so.*', r'\\1', ln\\n                            ).rstrip()\\n                            break\\n                        if 'tinfo' in ln:\\n                            readline_termcap_library = 'tinfo'\\n                            break\\n            if os.path.exists(tmpfile):\\n                os.unlink(tmpfile)\\n        if 'curses' in readline_termcap_library:\\n            curses_library = readline_termcap_library\\n        elif self.compiler.find_library_file(lib_dirs, 'ncursesw'):\\n            curses_library = 'ncursesw'\\n        elif self.compiler.find_library_file(lib_dirs, 'ncurses'):\\n            curses_library = 'ncurses'\\n        elif self.compiler.find_library_file(lib_dirs, 'curses'):\\n            curses_library = 'curses'\\n        if host_platform == 'darwin':\\n            os_release = int(os.uname()[2].split('.')[0])\\n            dep_target = sysconfig.get_config_var('MACOSX_DEPLOYMENT_TARGET')\\n            if dep_target and dep_target.split('.') < ['10', '5']:\\n                os_release = 8\\n            if os_release < 9:\\n                if find_file('readline/rlconf.h', inc_dirs, []) is None:\\n                    do_readline = False\\n        if do_readline:\\n            if host_platform == 'darwin' and os_release < 9:\\n                readline_extra_link_args = ('-Wl,-search_paths_first',)\\n            else:\\n                readline_extra_link_args = ()\\n            readline_libs = ['readline']\\n            if readline_termcap_library:\\n                pass \\n            elif curses_library:\\n                readline_libs.append(curses_library)\\n            elif self.compiler.find_library_file(lib_dirs +\\n                                                     ['/usr/lib/termcap'],\\n                                                     'termcap'):\\n                readline_libs.append('termcap')\\n            exts.append( Extension('readline', ['readline.c'],\\n                                   library_dirs=['/usr/lib/termcap'],\\n                                   extra_link_args=readline_extra_link_args,\\n                                   libraries=readline_libs) )\\n        else:\\n            missing.append('readline')\\n        if self.compiler.find_library_file(lib_dirs, 'crypt'):\\n            libs = ['crypt']\\n        else:\\n            libs = []\\n        exts.append( Extension('_crypt', ['_cryptmodule.c'], libraries=libs) )\\n        exts.append( Extension('_csv', ['_csv.c']) )\\n        exts.append( Extension('_posixsubprocess', ['_posixsubprocess.c']) )\\n        exts.append( Extension('_socket', ['socketmodule.c'],\\n                               depends = ['socketmodule.h']) )\\n        search_for_ssl_incs_in = [\\n                              '/usr/local/ssl/include',\\n                              '/usr/contrib/ssl/include/'\\n                             ]\\n        ssl_incs = find_file('openssl/ssl.h', inc_dirs,\\n                             search_for_ssl_incs_in\\n                             )\\n        if ssl_incs is not None:\\n            krb5_h = find_file('krb5.h', inc_dirs,\\n                               ['/usr/kerberos/include'])\\n            if krb5_h:\\n                ssl_incs += krb5_h\\n        ssl_libs = find_library_file(self.compiler, 'ssl',lib_dirs,\\n                                     ['/usr/local/ssl/lib',\\n                                      '/usr/contrib/ssl/lib/'\\n                                     ] )\\n        if (ssl_incs is not None and\\n            ssl_libs is not None):\\n            exts.append( Extension('_ssl', ['_ssl.c'],\\n                                   include_dirs = ssl_incs,\\n                                   library_dirs = ssl_libs,\\n                                   libraries = ['ssl', 'crypto'],\\n                                   depends = ['socketmodule.h']), )\\n        else:\\n            missing.append('_ssl')\\n        openssl_ver = 0\\n        openssl_ver_re = re.compile(\\n            '^\\s*\\n        opensslv_h = find_file('openssl/opensslv.h', [],\\n                inc_dirs + search_for_ssl_incs_in)\\n        if opensslv_h:\\n            name = os.path.join(opensslv_h[0], 'openssl/opensslv.h')\\n            if host_platform == 'darwin' and is_macosx_sdk_path(name):\\n                name = os.path.join(macosx_sdk_root(), name[1:])\\n            try:\\n                with open(name, 'r') as incfile:\\n                    for line in incfile:\\n                        m = openssl_ver_re.match(line)\\n                        if m:\\n                            openssl_ver = eval(m.group(1))\\n            except IOError as msg:\\n                print(\"IOError while reading opensshv.h:\", msg)\\n                pass\\n        min_openssl_ver = 0x00907000\\n        have_any_openssl = ssl_incs is not None and ssl_libs is not None\\n        have_usable_openssl = (have_any_openssl and\\n                               openssl_ver >= min_openssl_ver)\\n        if have_any_openssl:\\n            if have_usable_openssl:\\n                exts.append( Extension('_hashlib', ['_hashopenssl.c'],\\n                                       depends = ['hashlib.h'],\\n                                       include_dirs = ssl_incs,\\n                                       library_dirs = ssl_libs,\\n                                       libraries = ['ssl', 'crypto']) )\\n            else:\\n                print(\"warning: openssl 0x%08x is too old for _hashlib\" %\\n                      openssl_ver)\\n                missing.append('_hashlib')\\n        exts.append( Extension('_sha256', ['sha256module.c'],\\n                               depends=['hashlib.h']) )\\n        exts.append( Extension('_sha512', ['sha512module.c'],\\n                               depends=['hashlib.h']) )\\n        exts.append( Extension('_md5', ['md5module.c'],\\n                               depends=['hashlib.h']) )\\n        exts.append( Extension('_sha1', ['sha1module.c'],\\n                               depends=['hashlib.h']) )\\n        sha3_depends = ['hashlib.h']\\n        keccak = os.path.join(os.getcwd(), srcdir, 'Modules', '_sha3',\\n                              'keccak')\\n        for pattern in ('*.c', '*.h', '*.macros'):\\n            sha3_depends.extend(glob(os.path.join(keccak, pattern)))\\n        exts.append(Extension(\"_sha3\", [\"_sha3/sha3module.c\"],\\n                              depends=sha3_depends))\\n        max_db_ver = (5, 3)\\n        min_db_ver = (3, 3)\\n        db_setup_debug = False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def detect_modules(self):\\n        if not cross_compiling:\\n            add_dir_to_list(self.compiler.library_dirs, '/usr/local/lib')\\n            add_dir_to_list(self.compiler.include_dirs, '/usr/local/include')\\n        if cross_compiling:\\n            self.add_gcc_paths()\\n        self.add_multiarch_paths()\\n        for env_var, arg_name, dir_list in (\\n                ('LDFLAGS', '-R', self.compiler.runtime_library_dirs),\\n                ('LDFLAGS', '-L', self.compiler.library_dirs),\\n                ('CPPFLAGS', '-I', self.compiler.include_dirs)):\\n            env_val = sysconfig.get_config_var(env_var)\\n            if env_val:\\n                env_val = re.sub(r'(^|\\s+)-(-|(?!%s))' % arg_name[1],\\n                                 ' ', env_val)\\n                parser = optparse.OptionParser()\\n                parser.allow_interspersed_args = True\\n                parser.error = lambda msg: None\\n                parser.add_option(arg_name, dest=\"dirs\", action=\"append\")\\n                options = parser.parse_args(env_val.split())[0]\\n                if options.dirs:\\n                    for directory in reversed(options.dirs):\\n                        add_dir_to_list(dir_list, directory)\\n        if os.path.normpath(sys.base_prefix) != '/usr' \\\\n                and not sysconfig.get_config_var('PYTHONFRAMEWORK'):\\n            add_dir_to_list(self.compiler.library_dirs,\\n                            sysconfig.get_config_var(\"LIBDIR\"))\\n            add_dir_to_list(self.compiler.include_dirs,\\n                            sysconfig.get_config_var(\"INCLUDEDIR\"))\\n        if not cross_compiling:\\n            lib_dirs = self.compiler.library_dirs + [\\n                '/lib64', '/usr/lib64',\\n                '/lib', '/usr/lib',\\n                ]\\n            inc_dirs = self.compiler.include_dirs + ['/usr/include']\\n        else:\\n            lib_dirs = self.compiler.library_dirs[:]\\n            inc_dirs = self.compiler.include_dirs[:]\\n        exts = []\\n        missing = []\\n        config_h = sysconfig.get_config_h_filename()\\n        with open(config_h) as file:\\n            config_h_vars = sysconfig.parse_config_h(file)\\n        srcdir = sysconfig.get_config_var('srcdir')\\n        if host_platform in ['osf1', 'unixware7', 'openunix8']:\\n            lib_dirs += ['/usr/ccs/lib']\\n        if host_platform == 'hp-ux11':\\n            lib_dirs += ['/usr/lib/hpux64', '/usr/lib/hpux32']\\n        if host_platform == 'darwin':\\n            cflags, ldflags = sysconfig.get_config_vars(\\n                    'CFLAGS', 'LDFLAGS')\\n            for item in cflags.split():\\n                if item.startswith('-I'):\\n                    inc_dirs.append(item[2:])\\n            for item in ldflags.split():\\n                if item.startswith('-L'):\\n                    lib_dirs.append(item[2:])\\n        math_libs = ['m']\\n        if host_platform == 'darwin':\\n            math_libs = []\\n        exts.append( Extension('array', ['arraymodule.c']) )\\n        exts.append( Extension('cmath', ['cmathmodule.c', '_math.c'],\\n                               depends=['_math.h'],\\n                               libraries=math_libs) )\\n        exts.append( Extension('math',  ['mathmodule.c', '_math.c'],\\n                               depends=['_math.h'],\\n                               libraries=math_libs) )\\n        time_libs = []\\n        lib = sysconfig.get_config_var('TIMEMODULE_LIB')\\n        if lib:\\n            time_libs.append(lib)\\n        exts.append( Extension('time', ['timemodule.c'],\\n                               libraries=time_libs) )\\n        exts.append( Extension('_datetime', ['_datetimemodule.c']) )\\n        exts.append( Extension(\"_random\", [\"_randommodule.c\"]) )\\n        exts.append( Extension(\"_bisect\", [\"_bisectmodule.c\"]) )\\n        exts.append( Extension(\"_heapq\", [\"_heapqmodule.c\"]) )\\n        exts.append( Extension(\"_pickle\", [\"_pickle.c\"]) )\\n        exts.append( Extension(\"atexit\", [\"atexitmodule.c\"]) )\\n        exts.append( Extension(\"_json\", [\"_json.c\"]) )\\n        exts.append( Extension('_testcapi', ['_testcapimodule.c'],\\n                               depends=['testcapi_long.h']) )\\n        exts.append( Extension('_testbuffer', ['_testbuffer.c']) )\\n        exts.append( Extension('_testimportmultiple', ['_testimportmultiple.c']) )\\n        exts.append( Extension('_lsprof', ['_lsprof.c', 'rotatingtree.c']) )\\n        exts.append( Extension('unicodedata', ['unicodedata.c']) )\\n        libs = []\\n        if (config_h_vars.get('FLOCK_NEEDS_LIBBSD', False)):\\n            libs = ['bsd']\\n        exts.append( Extension('fcntl', ['fcntlmodule.c'], libraries=libs) )\\n        exts.append( Extension('pwd', ['pwdmodule.c']) )\\n        exts.append( Extension('grp', ['grpmodule.c']) )\\n        if (config_h_vars.get('HAVE_GETSPNAM', False) or\\n                config_h_vars.get('HAVE_GETSPENT', False)):\\n            exts.append( Extension('spwd', ['spwdmodule.c']) )\\n        else:\\n            missing.append('spwd')\\n        exts.append( Extension('select', ['selectmodule.c']) )\\n        exts.append( Extension('parser', ['parsermodule.c']) )\\n        exts.append( Extension('mmap', ['mmapmodule.c']) )\\n        exts.append( Extension('syslog', ['syslogmodule.c']) )\\n        exts.append( Extension('audioop', ['audioop.c']) )\\n        do_readline = self.compiler.find_library_file(lib_dirs, 'readline')\\n        readline_termcap_library = \"\"\\n        curses_library = \"\"\\n        tmpfile = os.path.join(self.build_temp, 'readline_termcap_lib')\\n        if not os.path.exists(self.build_temp):\\n            os.makedirs(self.build_temp)\\n        if do_readline:\\n            if cross_compiling:\\n                ret = os.system(\"%s -d %s | grep '(NEEDED)' > %s\" \\\\n                                % (sysconfig.get_config_var('READELF'),\\n                                   do_readline, tmpfile))\\n            elif find_executable('ldd'):\\n                ret = os.system(\"ldd %s > %s\" % (do_readline, tmpfile))\\n            else:\\n                ret = 256\\n            if ret >> 8 == 0:\\n                with open(tmpfile) as fp:\\n                    for ln in fp:\\n                        if 'curses' in ln:\\n                            readline_termcap_library = re.sub(\\n                                r'.*lib(n?cursesw?)\\.so.*', r'\\1', ln\\n                            ).rstrip()\\n                            break\\n                        if 'tinfo' in ln:\\n                            readline_termcap_library = 'tinfo'\\n                            break\\n            if os.path.exists(tmpfile):\\n                os.unlink(tmpfile)\\n        if 'curses' in readline_termcap_library:\\n            curses_library = readline_termcap_library\\n        elif self.compiler.find_library_file(lib_dirs, 'ncursesw'):\\n            curses_library = 'ncursesw'\\n        elif self.compiler.find_library_file(lib_dirs, 'ncurses'):\\n            curses_library = 'ncurses'\\n        elif self.compiler.find_library_file(lib_dirs, 'curses'):\\n            curses_library = 'curses'\\n        if host_platform == 'darwin':\\n            os_release = int(os.uname()[2].split('.')[0])\\n            dep_target = sysconfig.get_config_var('MACOSX_DEPLOYMENT_TARGET')\\n            if dep_target and dep_target.split('.') < ['10', '5']:\\n                os_release = 8\\n            if os_release < 9:\\n                if find_file('readline/rlconf.h', inc_dirs, []) is None:\\n                    do_readline = False\\n        if do_readline:\\n            if host_platform == 'darwin' and os_release < 9:\\n                readline_extra_link_args = ('-Wl,-search_paths_first',)\\n            else:\\n                readline_extra_link_args = ()\\n            readline_libs = ['readline']\\n            if readline_termcap_library:\\n                pass \\n            elif curses_library:\\n                readline_libs.append(curses_library)\\n            elif self.compiler.find_library_file(lib_dirs +\\n                                                     ['/usr/lib/termcap'],\\n                                                     'termcap'):\\n                readline_libs.append('termcap')\\n            exts.append( Extension('readline', ['readline.c'],\\n                                   library_dirs=['/usr/lib/termcap'],\\n                                   extra_link_args=readline_extra_link_args,\\n                                   libraries=readline_libs) )\\n        else:\\n            missing.append('readline')\\n        if self.compiler.find_library_file(lib_dirs, 'crypt'):\\n            libs = ['crypt']\\n        else:\\n            libs = []\\n        exts.append( Extension('_crypt', ['_cryptmodule.c'], libraries=libs) )\\n        exts.append( Extension('_csv', ['_csv.c']) )\\n        exts.append( Extension('_posixsubprocess', ['_posixsubprocess.c']) )\\n        exts.append( Extension('_socket', ['socketmodule.c'],\\n                               depends = ['socketmodule.h']) )\\n        search_for_ssl_incs_in = [\\n                              '/usr/local/ssl/include',\\n                              '/usr/contrib/ssl/include/'\\n                             ]\\n        ssl_incs = find_file('openssl/ssl.h', inc_dirs,\\n                             search_for_ssl_incs_in\\n                             )\\n        if ssl_incs is not None:\\n            krb5_h = find_file('krb5.h', inc_dirs,\\n                               ['/usr/kerberos/include'])\\n            if krb5_h:\\n                ssl_incs += krb5_h\\n        ssl_libs = find_library_file(self.compiler, 'ssl',lib_dirs,\\n                                     ['/usr/local/ssl/lib',\\n                                      '/usr/contrib/ssl/lib/'\\n                                     ] )\\n        if (ssl_incs is not None and\\n            ssl_libs is not None):\\n            exts.append( Extension('_ssl', ['_ssl.c'],\\n                                   include_dirs = ssl_incs,\\n                                   library_dirs = ssl_libs,\\n                                   libraries = ['ssl', 'crypto'],\\n                                   depends = ['socketmodule.h']), )\\n        else:\\n            missing.append('_ssl')\\n        openssl_ver = 0\\n        openssl_ver_re = re.compile(\\n            '^\\s*\\n        opensslv_h = find_file('openssl/opensslv.h', [],\\n                inc_dirs + search_for_ssl_incs_in)\\n        if opensslv_h:\\n            name = os.path.join(opensslv_h[0], 'openssl/opensslv.h')\\n            if host_platform == 'darwin' and is_macosx_sdk_path(name):\\n                name = os.path.join(macosx_sdk_root(), name[1:])\\n            try:\\n                with open(name, 'r') as incfile:\\n                    for line in incfile:\\n                        m = openssl_ver_re.match(line)\\n                        if m:\\n                            openssl_ver = eval(m.group(1))\\n            except IOError as msg:\\n                print(\"IOError while reading opensshv.h:\", msg)\\n                pass\\n        min_openssl_ver = 0x00907000\\n        have_any_openssl = ssl_incs is not None and ssl_libs is not None\\n        have_usable_openssl = (have_any_openssl and\\n                               openssl_ver >= min_openssl_ver)\\n        if have_any_openssl:\\n            if have_usable_openssl:\\n                exts.append( Extension('_hashlib', ['_hashopenssl.c'],\\n                                       depends = ['hashlib.h'],\\n                                       include_dirs = ssl_incs,\\n                                       library_dirs = ssl_libs,\\n                                       libraries = ['ssl', 'crypto']) )\\n            else:\\n                print(\"warning: openssl 0x%08x is too old for _hashlib\" %\\n                      openssl_ver)\\n                missing.append('_hashlib')\\n        exts.append( Extension('_sha256', ['sha256module.c'],\\n                               depends=['hashlib.h']) )\\n        exts.append( Extension('_sha512', ['sha512module.c'],\\n                               depends=['hashlib.h']) )\\n        exts.append( Extension('_md5', ['md5module.c'],\\n                               depends=['hashlib.h']) )\\n        exts.append( Extension('_sha1', ['sha1module.c'],\\n                               depends=['hashlib.h']) )\\n        sha3_depends = ['hashlib.h']\\n        keccak = os.path.join(os.getcwd(), srcdir, 'Modules', '_sha3',\\n                              'keccak')\\n        for pattern in ('*.c', '*.h', '*.macros'):\\n            sha3_depends.extend(glob(os.path.join(keccak, pattern)))\\n        exts.append(Extension(\"_sha3\", [\"_sha3/sha3module.c\"],\\n                              depends=sha3_depends))\\n        max_db_ver = (5, 3)\\n        min_db_ver = (3, 3)\\n        db_setup_debug = False"
  },
  {
    "code": "def volume_modify_attributes(self, params):\\n        if self.volume_style == 'flexGroup' or self.parameters['is_infinite']:\\n            vol_mod_iter = netapp_utils.zapi.NaElement('volume-modify-iter-async')\\n        else:\\n            vol_mod_iter = netapp_utils.zapi.NaElement('volume-modify-iter')\\n        attributes = netapp_utils.zapi.NaElement('attributes')\\n        vol_mod_attributes = netapp_utils.zapi.NaElement('volume-attributes')\\n        vol_space_attributes = netapp_utils.zapi.NaElement('volume-space-attributes')\\n        if self.parameters.get('space_guarantee'):\\n            self.create_volume_attribute(vol_space_attributes, vol_mod_attributes,\\n                                         'space-guarantee', self.parameters['space_guarantee'])\\n        if self.parameters.get('percent_snapshot_space') is not None:\\n            self.create_volume_attribute(vol_space_attributes, vol_mod_attributes,\\n                                         'percentage-snapshot-reserve', str(self.parameters['percent_snapshot_space']))\\n        if self.parameters.get('space_slo'):\\n            self.create_volume_attribute(vol_space_attributes, vol_mod_attributes, 'space-slo', self.parameters['space_slo'])\\n        vol_snapshot_attributes = netapp_utils.zapi.NaElement('volume-snapshot-attributes')\\n        if self.parameters.get('snapshot_policy'):\\n            self.create_volume_attribute(vol_snapshot_attributes, vol_mod_attributes,\\n                                         'snapshot-policy', self.parameters['snapshot_policy'])\\n        if self.parameters.get('snapdir_access'):\\n            self.create_volume_attribute(vol_snapshot_attributes, vol_mod_attributes,\\n                                         'snapdir-access-enabled', self.parameters['snapdir_access'])\\n        if self.parameters.get('policy'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-export-attributes',\\n                                         'policy', self.parameters['policy'])\\n        if self.parameters.get('unix_permissions'):\\n            vol_security_attributes = netapp_utils.zapi.NaElement('volume-security-attributes')\\n            self.create_volume_attribute(vol_security_attributes, 'volume-security-unix-attributes',\\n                                         'permissions', self.parameters['unix_permissions'])\\n            vol_mod_attributes.add_child_elem(vol_security_attributes)\\n        if self.parameters.get('atime_update'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-performance-attributes',\\n                                         'is-atime-update-enabled', self.parameters['atime_update'])\\n        if self.parameters.get('qos_policy_group'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volumes-qos-attributes',\\n                                         'policy-group-name', self.parameters['qos_policy_group'])\\n        if self.parameters.get('qos_adaptive_policy_group'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volumes-qos-attributes',\\n                                         'adaptive-policy-group-name', self.parameters['qos_adaptive_policy_group'])\\n        if params and params.get('tiering_policy'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-comp-aggr-attributes',\\n                                         'tiering-policy', self.parameters['tiering_policy'])\\n        if self.parameters.get('nvfail_enabled') is not None:\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-state-attributes', 'is-nvfail-enabled', str(self.parameters['nvfail_enabled']))\\n        attributes.add_child_elem(vol_mod_attributes)\\n        query = netapp_utils.zapi.NaElement('query')\\n        vol_query_attributes = netapp_utils.zapi.NaElement('volume-attributes')\\n        self.create_volume_attribute(vol_query_attributes, 'volume-id-attributes',\\n                                     'name', self.parameters['name'])\\n        query.add_child_elem(vol_query_attributes)\\n        vol_mod_iter.add_child_elem(attributes)\\n        vol_mod_iter.add_child_elem(query)\\n        try:\\n            result = self.server.invoke_successfully(vol_mod_iter, enable_tunneling=True)\\n            failures = result.get_child_by_name('failure-list')\\n            if self.volume_style == 'flexGroup' or self.parameters['is_infinite']:\\n                success = result.get_child_by_name('success-list')\\n                success = success.get_child_by_name('volume-modify-iter-async-info')\\n                results = dict()\\n                for key in ('status', 'jobid'):\\n                    if success.get_child_by_name(key):\\n                        results[key] = success[key]\\n                status = results.get('status')\\n                if status == 'in_progress' and 'jobid' in results:\\n                    if self.parameters['time_out'] == 0:\\n                        return\\n                    error = self.check_job_status(results['jobid'])\\n                    if error is None:\\n                        return\\n                    else:\\n                        self.module.fail_json(msg='Error when modify volume: %s' % error)\\n                self.module.fail_json(msg='Unexpected error when modify volume: results is: %s' % repr(results))\\n            if failures is not None:\\n                if failures.get_child_by_name('volume-modify-iter-info') is not None:\\n                    return_info = 'volume-modify-iter-info'\\n                    error_msg = failures.get_child_by_name(return_info).get_child_content('error-message')\\n                    self.module.fail_json(msg=\"Error modifying volume %s: %s\"\\n                                          % (self.parameters['name'], error_msg),\\n                                          exception=traceback.format_exc())\\n                elif failures.get_child_by_name('volume-modify-iter-async-info') is not None:\\n                    return_info = 'volume-modify-iter-async-info'\\n                    error_msg = failures.get_child_by_name(return_info).get_child_content('error-message')\\n                    self.module.fail_json(msg=\"Error modifying volume %s: %s\"\\n                                          % (self.parameters['name'], error_msg),\\n                                          exception=traceback.format_exc())\\n            self.ems_log_event(\"volume-modify\")\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error modifying volume %s: %s'\\n                                  % (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def volume_modify_attributes(self, params):\\n        if self.volume_style == 'flexGroup' or self.parameters['is_infinite']:\\n            vol_mod_iter = netapp_utils.zapi.NaElement('volume-modify-iter-async')\\n        else:\\n            vol_mod_iter = netapp_utils.zapi.NaElement('volume-modify-iter')\\n        attributes = netapp_utils.zapi.NaElement('attributes')\\n        vol_mod_attributes = netapp_utils.zapi.NaElement('volume-attributes')\\n        vol_space_attributes = netapp_utils.zapi.NaElement('volume-space-attributes')\\n        if self.parameters.get('space_guarantee'):\\n            self.create_volume_attribute(vol_space_attributes, vol_mod_attributes,\\n                                         'space-guarantee', self.parameters['space_guarantee'])\\n        if self.parameters.get('percent_snapshot_space') is not None:\\n            self.create_volume_attribute(vol_space_attributes, vol_mod_attributes,\\n                                         'percentage-snapshot-reserve', str(self.parameters['percent_snapshot_space']))\\n        if self.parameters.get('space_slo'):\\n            self.create_volume_attribute(vol_space_attributes, vol_mod_attributes, 'space-slo', self.parameters['space_slo'])\\n        vol_snapshot_attributes = netapp_utils.zapi.NaElement('volume-snapshot-attributes')\\n        if self.parameters.get('snapshot_policy'):\\n            self.create_volume_attribute(vol_snapshot_attributes, vol_mod_attributes,\\n                                         'snapshot-policy', self.parameters['snapshot_policy'])\\n        if self.parameters.get('snapdir_access'):\\n            self.create_volume_attribute(vol_snapshot_attributes, vol_mod_attributes,\\n                                         'snapdir-access-enabled', self.parameters['snapdir_access'])\\n        if self.parameters.get('policy'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-export-attributes',\\n                                         'policy', self.parameters['policy'])\\n        if self.parameters.get('unix_permissions'):\\n            vol_security_attributes = netapp_utils.zapi.NaElement('volume-security-attributes')\\n            self.create_volume_attribute(vol_security_attributes, 'volume-security-unix-attributes',\\n                                         'permissions', self.parameters['unix_permissions'])\\n            vol_mod_attributes.add_child_elem(vol_security_attributes)\\n        if self.parameters.get('atime_update'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-performance-attributes',\\n                                         'is-atime-update-enabled', self.parameters['atime_update'])\\n        if self.parameters.get('qos_policy_group'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volumes-qos-attributes',\\n                                         'policy-group-name', self.parameters['qos_policy_group'])\\n        if self.parameters.get('qos_adaptive_policy_group'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volumes-qos-attributes',\\n                                         'adaptive-policy-group-name', self.parameters['qos_adaptive_policy_group'])\\n        if params and params.get('tiering_policy'):\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-comp-aggr-attributes',\\n                                         'tiering-policy', self.parameters['tiering_policy'])\\n        if self.parameters.get('nvfail_enabled') is not None:\\n            self.create_volume_attribute(vol_mod_attributes, 'volume-state-attributes', 'is-nvfail-enabled', str(self.parameters['nvfail_enabled']))\\n        attributes.add_child_elem(vol_mod_attributes)\\n        query = netapp_utils.zapi.NaElement('query')\\n        vol_query_attributes = netapp_utils.zapi.NaElement('volume-attributes')\\n        self.create_volume_attribute(vol_query_attributes, 'volume-id-attributes',\\n                                     'name', self.parameters['name'])\\n        query.add_child_elem(vol_query_attributes)\\n        vol_mod_iter.add_child_elem(attributes)\\n        vol_mod_iter.add_child_elem(query)\\n        try:\\n            result = self.server.invoke_successfully(vol_mod_iter, enable_tunneling=True)\\n            failures = result.get_child_by_name('failure-list')\\n            if self.volume_style == 'flexGroup' or self.parameters['is_infinite']:\\n                success = result.get_child_by_name('success-list')\\n                success = success.get_child_by_name('volume-modify-iter-async-info')\\n                results = dict()\\n                for key in ('status', 'jobid'):\\n                    if success.get_child_by_name(key):\\n                        results[key] = success[key]\\n                status = results.get('status')\\n                if status == 'in_progress' and 'jobid' in results:\\n                    if self.parameters['time_out'] == 0:\\n                        return\\n                    error = self.check_job_status(results['jobid'])\\n                    if error is None:\\n                        return\\n                    else:\\n                        self.module.fail_json(msg='Error when modify volume: %s' % error)\\n                self.module.fail_json(msg='Unexpected error when modify volume: results is: %s' % repr(results))\\n            if failures is not None:\\n                if failures.get_child_by_name('volume-modify-iter-info') is not None:\\n                    return_info = 'volume-modify-iter-info'\\n                    error_msg = failures.get_child_by_name(return_info).get_child_content('error-message')\\n                    self.module.fail_json(msg=\"Error modifying volume %s: %s\"\\n                                          % (self.parameters['name'], error_msg),\\n                                          exception=traceback.format_exc())\\n                elif failures.get_child_by_name('volume-modify-iter-async-info') is not None:\\n                    return_info = 'volume-modify-iter-async-info'\\n                    error_msg = failures.get_child_by_name(return_info).get_child_content('error-message')\\n                    self.module.fail_json(msg=\"Error modifying volume %s: %s\"\\n                                          % (self.parameters['name'], error_msg),\\n                                          exception=traceback.format_exc())\\n            self.ems_log_event(\"volume-modify\")\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error modifying volume %s: %s'\\n                                  % (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def renew_from_kt(principal, keytab):\\n    renewal_lifetime = \"%sm\" % configuration.conf.getint('kerberos', 'reinit_frequency')\\n    cmd_principal = principal or configuration.conf.get('kerberos', 'principal').replace(\\n        \"_HOST\", socket.getfqdn()\\n    )\\n    cmdv = [\\n        configuration.conf.get('kerberos', 'kinit_path'),\\n        \"-r\", renewal_lifetime,\\n        \"-k\",  \\n        \"-t\", keytab,  \\n        \"-c\", configuration.conf.get('kerberos', 'ccache'),  \\n        cmd_principal\\n    ]\\n    log.info(\"Reinitting kerberos from keytab: \" + \" \".join(cmdv))\\n    subp = subprocess.Popen(cmdv,\\n                            stdout=subprocess.PIPE,\\n                            stderr=subprocess.PIPE,\\n                            close_fds=True,\\n                            bufsize=-1,\\n                            universal_newlines=True)\\n    subp.wait()\\n    if subp.returncode != 0:\\n        log.error(\"Couldn't reinit from keytab! `kinit' exited with %s.\\n%s\\n%s\" % (\\n            subp.returncode,\\n            \"\\n\".join(subp.stdout.readlines()),\\n            \"\\n\".join(subp.stderr.readlines())))\\n        sys.exit(subp.returncode)\\n    global NEED_KRB181_WORKAROUND\\n    if NEED_KRB181_WORKAROUND is None:\\n        NEED_KRB181_WORKAROUND = detect_conf_var()\\n    if NEED_KRB181_WORKAROUND:\\n        time.sleep(1.5)\\n        perform_krb181_workaround(principal)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def renew_from_kt(principal, keytab):\\n    renewal_lifetime = \"%sm\" % configuration.conf.getint('kerberos', 'reinit_frequency')\\n    cmd_principal = principal or configuration.conf.get('kerberos', 'principal').replace(\\n        \"_HOST\", socket.getfqdn()\\n    )\\n    cmdv = [\\n        configuration.conf.get('kerberos', 'kinit_path'),\\n        \"-r\", renewal_lifetime,\\n        \"-k\",  \\n        \"-t\", keytab,  \\n        \"-c\", configuration.conf.get('kerberos', 'ccache'),  \\n        cmd_principal\\n    ]\\n    log.info(\"Reinitting kerberos from keytab: \" + \" \".join(cmdv))\\n    subp = subprocess.Popen(cmdv,\\n                            stdout=subprocess.PIPE,\\n                            stderr=subprocess.PIPE,\\n                            close_fds=True,\\n                            bufsize=-1,\\n                            universal_newlines=True)\\n    subp.wait()\\n    if subp.returncode != 0:\\n        log.error(\"Couldn't reinit from keytab! `kinit' exited with %s.\\n%s\\n%s\" % (\\n            subp.returncode,\\n            \"\\n\".join(subp.stdout.readlines()),\\n            \"\\n\".join(subp.stderr.readlines())))\\n        sys.exit(subp.returncode)\\n    global NEED_KRB181_WORKAROUND\\n    if NEED_KRB181_WORKAROUND is None:\\n        NEED_KRB181_WORKAROUND = detect_conf_var()\\n    if NEED_KRB181_WORKAROUND:\\n        time.sleep(1.5)\\n        perform_krb181_workaround(principal)"
  },
  {
    "code": "def __init__(self, dataflow, project_number, name, location, poll_sleep=10,\\n                 job_id=None, num_retries=None):\\n        self._dataflow = dataflow\\n        self._project_number = project_number\\n        self._job_name = name\\n        self._job_location = location\\n        self._job_id = job_id\\n        self._num_retries = num_retries\\n        self._job = self._get_job()\\n        self._poll_sleep = poll_sleep",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "AIRFLOW-3791: Dataflow - Support check status if pipeline spans on multiple jobs (#4633)\\n\\nSupport to check if job is already running before starting java job\\nIn case dataflow creates more than one job, we need to track all jobs for status\\n\\nSupport to check if job is already running before starting java job\\nIn case dataflow creates more than one job, we need to track all jobs for status\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nchange default for check if running\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name\\n\\nmerge redundant code of _get_job_id_from_name",
    "fixed_code": "def __init__(self, dataflow, project_number, name, location, poll_sleep=10,\\n                 job_id=None, num_retries=None, multiple_jobs=None):\\n        self._dataflow = dataflow\\n        self._project_number = project_number\\n        self._job_name = name\\n        self._job_location = location\\n        self._multiple_jobs = multiple_jobs\\n        self._job_id = job_id\\n        self._num_retries = num_retries\\n        if self._num_retries is None:\\n            self._num_retries = 0\\n        self._poll_sleep = poll_sleep\\n        self._jobs = self._get_jobs()"
  },
  {
    "code": "def logout(request, next_page=None,\\n           template_name='registration/logged_out.html',\\n           redirect_field_name=REDIRECT_FIELD_NAME,\\n           extra_context=None):\\n    auth_logout(request)\\n    if next_page is not None:\\n        next_page = resolve_url(next_page)\\n    elif settings.LOGOUT_REDIRECT_URL:\\n        next_page = resolve_url(settings.LOGOUT_REDIRECT_URL)\\n    if (redirect_field_name in request.POST or\\n            redirect_field_name in request.GET):\\n        next_page = request.POST.get(redirect_field_name,\\n                                     request.GET.get(redirect_field_name))\\n        if not is_safe_url(url=next_page, host=request.get_host()):\\n            next_page = request.path\\n    if next_page:\\n        return HttpResponseRedirect(next_page)\\n    current_site = get_current_site(request)\\n    context = {\\n        'site': current_site,\\n        'site_name': current_site.name,\\n        'title': _('Logged out')\\n    }\\n    if extra_context is not None:\\n        context.update(extra_context)\\n    return TemplateResponse(request, template_name, context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def logout(request, next_page=None,\\n           template_name='registration/logged_out.html',\\n           redirect_field_name=REDIRECT_FIELD_NAME,\\n           extra_context=None):\\n    auth_logout(request)\\n    if next_page is not None:\\n        next_page = resolve_url(next_page)\\n    elif settings.LOGOUT_REDIRECT_URL:\\n        next_page = resolve_url(settings.LOGOUT_REDIRECT_URL)\\n    if (redirect_field_name in request.POST or\\n            redirect_field_name in request.GET):\\n        next_page = request.POST.get(redirect_field_name,\\n                                     request.GET.get(redirect_field_name))\\n        if not is_safe_url(url=next_page, host=request.get_host()):\\n            next_page = request.path\\n    if next_page:\\n        return HttpResponseRedirect(next_page)\\n    current_site = get_current_site(request)\\n    context = {\\n        'site': current_site,\\n        'site_name': current_site.name,\\n        'title': _('Logged out')\\n    }\\n    if extra_context is not None:\\n        context.update(extra_context)\\n    return TemplateResponse(request, template_name, context)"
  },
  {
    "code": "def process_lhs(self, compiler, connection, lhs=None):\\n        lhs = lhs or self.lhs\\n        if hasattr(lhs, 'resolve_expression'):\\n            lhs = lhs.resolve_expression(compiler.query)\\n        return compiler.compile(lhs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_lhs(self, compiler, connection, lhs=None):\\n        lhs = lhs or self.lhs\\n        if hasattr(lhs, 'resolve_expression'):\\n            lhs = lhs.resolve_expression(compiler.query)\\n        return compiler.compile(lhs)"
  },
  {
    "code": "def join_path(self, *args):\\n\\t\\tparts = []\\n\\t\\tfor arg in args:\\n\\t\\t\\targ = self._unquote(arg).replace('/', '\\\\')\\n\\t\\t\\tparts.extend([a for a in arg.split('\\\\') if a])\\n\\t\\tpath = '\\\\'.join(parts)\\n\\t\\tif path.startswith('~'):\\n\\t\\t\\treturn path\\n\\t\\treturn path",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix UNC path support in the powershell shell plugin (#66604)",
    "fixed_code": "def join_path(self, *args):\\n\\t\\tparts = [ntpath.normpath(self._unquote(arg)) for arg in args]\\n\\t\\treturn ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])"
  },
  {
    "code": "def _round_half_even(self, prec, expdiff, context):\\n        tmp = Decimal( (self._sign, self._int[:prec], self._exp - expdiff))\\n        half = (self._int[prec] == 5)\\n        if half:\\n            for digit in self._int[prec+1:]:\\n                if digit != 0:\\n                    half = 0\\n                    break\\n        if half:\\n            if self._int[prec-1] & 1 == 0:\\n                return tmp\\n        return self._round_half_up(prec, expdiff, context, tmp)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _round_half_even(self, prec):\\n        if _exact_half(self._int, prec) and \\\\n                (prec == 0 or self._int[prec-1] in '02468'):\\n            return -1\\n        else:\\n            return self._round_half_up(prec)"
  },
  {
    "code": "def _maybe_right_yaxis(self, ax):\\n        _types = (list, tuple, np.ndarray)\\n        sec_true = isinstance(self.secondary_y, bool) and self.secondary_y\\n        list_sec = isinstance(self.secondary_y, _types)\\n        has_sec = list_sec and len(self.secondary_y) > 0\\n        all_sec = list_sec and len(self.secondary_y) == self.nseries\\n        if (sec_true or has_sec) and not hasattr(ax, 'right_ax'):\\n            orig_ax, new_ax = ax, ax.twinx()\\n            orig_ax.right_ax, new_ax.left_ax = new_ax, orig_ax\\n            new_ax.right_ax = new_ax\\n            if len(orig_ax.get_lines()) == 0:  \\n                orig_ax.get_yaxis().set_visible(False)\\n            if len(new_ax.get_lines()) == 0:\\n                new_ax.get_yaxis().set_visible(False)\\n            if sec_true or all_sec:\\n                ax = new_ax\\n        else:\\n            ax.get_yaxis().set_visible(True)\\n        return ax",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_right_yaxis(self, ax):\\n        _types = (list, tuple, np.ndarray)\\n        sec_true = isinstance(self.secondary_y, bool) and self.secondary_y\\n        list_sec = isinstance(self.secondary_y, _types)\\n        has_sec = list_sec and len(self.secondary_y) > 0\\n        all_sec = list_sec and len(self.secondary_y) == self.nseries\\n        if (sec_true or has_sec) and not hasattr(ax, 'right_ax'):\\n            orig_ax, new_ax = ax, ax.twinx()\\n            orig_ax.right_ax, new_ax.left_ax = new_ax, orig_ax\\n            new_ax.right_ax = new_ax\\n            if len(orig_ax.get_lines()) == 0:  \\n                orig_ax.get_yaxis().set_visible(False)\\n            if len(new_ax.get_lines()) == 0:\\n                new_ax.get_yaxis().set_visible(False)\\n            if sec_true or all_sec:\\n                ax = new_ax\\n        else:\\n            ax.get_yaxis().set_visible(True)\\n        return ax"
  },
  {
    "code": "def index(self, req, flavor_id):\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tauthorize(context)\\n\\t\\ttry:\\n\\t\\t\\tflavor = flavors.get_flavor_by_flavor_id(flavor_id, ctxt=context)\\n\\t\\texcept exception.FlavorNotFound:\\n\\t\\t\\texplanation = _(\"Flavor not found.\")\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=explanation)\\n\\t\\tif flavor['is_public']:\\n\\t\\t\\texplanation = _(\"Access list not available for public flavors.\")\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=explanation)\\n\\t\\treturn _marshall_flavor_access(flavor_id)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def index(self, req, flavor_id):\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tauthorize(context)\\n\\t\\ttry:\\n\\t\\t\\tflavor = flavors.get_flavor_by_flavor_id(flavor_id, ctxt=context)\\n\\t\\texcept exception.FlavorNotFound:\\n\\t\\t\\texplanation = _(\"Flavor not found.\")\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=explanation)\\n\\t\\tif flavor['is_public']:\\n\\t\\t\\texplanation = _(\"Access list not available for public flavors.\")\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=explanation)\\n\\t\\treturn _marshall_flavor_access(flavor_id)"
  },
  {
    "code": "def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,\\n               cluster_all=True, max_iterations=300):\\n    if bandwidth is None:\\n        bandwidth = estimate_bandwidth(X)\\n    if seeds is None:\\n        if bin_seeding:\\n            seeds = get_bin_seeds(X, bandwidth)\\n        else:\\n            seeds = X\\n    n_samples, n_features = X.shape\\n    stop_thresh = 1e-3 * bandwidth  \\n    center_intensity_dict = {}\\n    nbrs = NearestNeighbors(radius=bandwidth).fit(X)\\n    for my_mean in seeds:\\n        completed_iterations = 0\\n        while True:\\n            i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth,\\n                                           return_distance=False)[0]\\n            points_within = X[i_nbrs]\\n            if len(points_within) == 0:\\n                break  \\n            my_old_mean = my_mean  \\n            my_mean = np.mean(points_within, axis=0)\\n            if (extmath.norm(my_mean - my_old_mean) < stop_thresh or\\n                    completed_iterations == max_iterations):\\n                center_intensity_dict[tuple(my_mean)] = len(points_within)\\n                break\\n            completed_iterations += 1\\n    sorted_by_intensity = sorted(center_intensity_dict.items(),\\n                                 key=lambda tup: tup[1], reverse=True)\\n    sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\\n    unique = np.ones(len(sorted_centers), dtype=np.bool)\\n    nbrs = NearestNeighbors(radius=bandwidth).fit(sorted_centers)\\n    for i, center in enumerate(sorted_centers):\\n        if unique[i]:\\n            neighbor_idxs = nbrs.radius_neighbors([center],\\n                                                  return_distance=False)[0]\\n            unique[neighbor_idxs] = 0\\n            unique[i] = 1  \\n    cluster_centers = sorted_centers[unique]\\n    nbrs = NearestNeighbors(n_neighbors=1).fit(cluster_centers)\\n    labels = np.zeros(n_samples, dtype=np.int)\\n    distances, idxs = nbrs.kneighbors(X)\\n    if cluster_all:\\n        labels = idxs.flatten()\\n    else:\\n        labels.fill(-1)\\n        bool_selector = distances.flatten() <= bandwidth\\n        labels[bool_selector] = idxs.flatten()[bool_selector]\\n    return cluster_centers, labels",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX/ENH mean shift clustering",
    "fixed_code": "def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,\\n               min_bin_freq=1, cluster_all=True, max_iterations=300):\\n    if bandwidth is None:\\n        bandwidth = estimate_bandwidth(X)\\n    if seeds is None:\\n        if bin_seeding:\\n            seeds = get_bin_seeds(X, bandwidth, min_bin_freq)\\n        else:\\n            seeds = X\\n    n_samples, n_features = X.shape\\n    stop_thresh = 1e-3 * bandwidth  \\n    center_intensity_dict = {}\\n    nbrs = NearestNeighbors(radius=bandwidth).fit(X)\\n    for my_mean in seeds:\\n        completed_iterations = 0\\n        while True:\\n            i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth,\\n                                           return_distance=False)[0]\\n            points_within = X[i_nbrs]\\n            if len(points_within) == 0:\\n                break  \\n            my_old_mean = my_mean  \\n            my_mean = np.mean(points_within, axis=0)\\n            if (extmath.norm(my_mean - my_old_mean) < stop_thresh or\\n                    completed_iterations == max_iterations):\\n                center_intensity_dict[tuple(my_mean)] = len(points_within)\\n                break\\n            completed_iterations += 1\\n    sorted_by_intensity = sorted(center_intensity_dict.items(),\\n                                 key=lambda tup: tup[1], reverse=True)\\n    sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\\n    unique = np.ones(len(sorted_centers), dtype=np.bool)\\n    nbrs = NearestNeighbors(radius=bandwidth).fit(sorted_centers)\\n    for i, center in enumerate(sorted_centers):\\n        if unique[i]:\\n            neighbor_idxs = nbrs.radius_neighbors([center],\\n                                                  return_distance=False)[0]\\n            unique[neighbor_idxs] = 0\\n            unique[i] = 1  \\n    cluster_centers = sorted_centers[unique]\\n    nbrs = NearestNeighbors(n_neighbors=1).fit(cluster_centers)\\n    labels = np.zeros(n_samples, dtype=np.int)\\n    distances, idxs = nbrs.kneighbors(X)\\n    if cluster_all:\\n        labels = idxs.flatten()\\n    else:\\n        labels.fill(-1)\\n        bool_selector = distances.flatten() <= bandwidth\\n        labels[bool_selector] = idxs.flatten()[bool_selector]\\n    return cluster_centers, labels"
  },
  {
    "code": "def get_urls(self):\\n        return [\\n            url(r'^(.+)/password/$', self.admin_site.admin_view(self.user_change_password), name='auth_user_password_change'),\\n        ] + super(UserAdmin, self).get_urls()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_urls(self):\\n        return [\\n            url(r'^(.+)/password/$', self.admin_site.admin_view(self.user_change_password), name='auth_user_password_change'),\\n        ] + super(UserAdmin, self).get_urls()"
  },
  {
    "code": "def _delegate_method(self, name, *args, **kwargs):\\n        from pandas import Series\\n        method = getattr(self.values, name)\\n        result = method(*args, **kwargs)\\n        if not com.is_list_like(result):\\n            return result\\n        result = Series(result, index=self.index)\\n        result.is_copy = (\"modifications to a method of a datetimelike object are not \"\\n                          \"supported and are discarded. Change values on the original.\")\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _delegate_method(self, name, *args, **kwargs):\\n        from pandas import Series\\n        method = getattr(self.values, name)\\n        result = method(*args, **kwargs)\\n        if not com.is_list_like(result):\\n            return result\\n        result = Series(result, index=self.index)\\n        result.is_copy = (\"modifications to a method of a datetimelike object are not \"\\n                          \"supported and are discarded. Change values on the original.\")\\n        return result"
  },
  {
    "code": "def parse(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              chunksize=None):\\n        choose = {True:self._parse_xlsx,\\n                  False:self._parse_xls}\\n        return choose[self.use_xlsx](sheetname, header=header,\\n                                     skiprows=skiprows, index_col=index_col,\\n                                     parse_dates=parse_dates,\\n                                     date_parser=date_parser,\\n                                     na_values=na_values, chunksize=chunksize)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: parsing numbers with commas in read_csv/table/clipboard/fwf #796 VB: read_csv with/without thousands separator parsing",
    "fixed_code": "def parse(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              thousands=None, chunksize=None):\\n        choose = {True:self._parse_xlsx,\\n                  False:self._parse_xls}\\n        return choose[self.use_xlsx](sheetname, header=header,\\n                                     skiprows=skiprows, index_col=index_col,\\n                                     parse_dates=parse_dates,\\n                                     date_parser=date_parser,\\n                                     na_values=na_values,\\n                                     thousands=thousands,\\n                                     chunksize=chunksize)"
  },
  {
    "code": "def time_is_year_start(self, tz):\\n        self.ts.is_quarter_end",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fastpaths for Timestamp properties (#18539)",
    "fixed_code": "def time_is_year_start(self, tz, freq):\\n        self.ts.is_quarter_end"
  },
  {
    "code": "def __init__(self, isa=None, private=False, default=None):\\n       self.isa = isa\\n       self.private = private\\n       self.value = None\\n       self.default = default",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, isa=None, private=False, default=None):\\n       self.isa = isa\\n       self.private = private\\n       self.value = None\\n       self.default = default"
  },
  {
    "code": "def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\\n        if (\\n            self._for_loop_depths\\n            and self._for_loop_depths[-1] == self.depth\\n            and leaf.type == token.NAME\\n            and leaf.value == \"in\"\\n        ):\\n            self.depth -= 1\\n            self._for_loop_depths.pop()\\n            return True\\n        return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\\n        if (\\n            self._for_loop_depths\\n            and self._for_loop_depths[-1] == self.depth\\n            and leaf.type == token.NAME\\n            and leaf.value == \"in\"\\n        ):\\n            self.depth -= 1\\n            self._for_loop_depths.pop()\\n            return True\\n        return False"
  },
  {
    "code": "def get_features_used(node: Node) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {syms.typedargslist, syms.arglist}:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n    return features",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix feature detection for positional-only arguments in lambdas (#2532)",
    "fixed_code": "def get_features_used(node: Node) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    for n in node.pre_order():\\n        if n.type == token.STRING:\\n            value_head = n.value[:2]  \\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif n.type == token.NUMBER:\\n            if \"_\" in n.value:  \\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n    return features"
  },
  {
    "code": "def _possibly_cast_to_datetime(value, dtype, coerce = False):\\n    if dtype is not None:\\n        if isinstance(dtype, basestring):\\n            dtype = np.dtype(dtype)\\n        is_datetime64  = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            from pandas.tseries.tools import to_datetime\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            value = _possibly_cast_to_timedelta(value)\\n                    except:\\n                        pass\\n    else:\\n        if isinstance(value, np.ndarray) and not (issubclass(value.dtype.type, np.integer) or value.dtype == np.object_):\\n            pass\\n        else:\\n            v = value\\n            if not is_list_like(v):\\n                v = [ v ]\\n            if len(v):\\n                inferred_type = lib.infer_dtype(v)\\n                if inferred_type in ['datetime','datetime64']:\\n                    try:\\n                        value = tslib.array_to_datetime(np.array(v))\\n                    except:\\n                        pass\\n                elif inferred_type in ['timedelta','timedelta64']:\\n                    value = _possibly_cast_to_timedelta(value)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _possibly_cast_to_datetime(value, dtype, coerce = False):\\n    if dtype is not None:\\n        if isinstance(dtype, basestring):\\n            dtype = np.dtype(dtype)\\n        is_datetime64  = is_datetime64_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_timedelta64:\\n            if np.isscalar(value):\\n                if value == tslib.iNaT or isnull(value):\\n                    value = tslib.iNaT\\n            else:\\n                value = np.array(value)\\n                if value.ndim == 0:\\n                    value = tslib.iNaT\\n                elif np.prod(value.shape) and value.dtype != dtype:\\n                    try:\\n                        if is_datetime64:\\n                            from pandas.tseries.tools import to_datetime\\n                            value = to_datetime(value, coerce=coerce).values\\n                        elif is_timedelta64:\\n                            value = _possibly_cast_to_timedelta(value)\\n                    except:\\n                        pass\\n    else:\\n        if isinstance(value, np.ndarray) and not (issubclass(value.dtype.type, np.integer) or value.dtype == np.object_):\\n            pass\\n        else:\\n            v = value\\n            if not is_list_like(v):\\n                v = [ v ]\\n            if len(v):\\n                inferred_type = lib.infer_dtype(v)\\n                if inferred_type in ['datetime','datetime64']:\\n                    try:\\n                        value = tslib.array_to_datetime(np.array(v))\\n                    except:\\n                        pass\\n                elif inferred_type in ['timedelta','timedelta64']:\\n                    value = _possibly_cast_to_timedelta(value)\\n    return value"
  },
  {
    "code": "def _change_state_for_executable_task_instances(self, task_instances,\\n                                                    acceptable_states, session=None):\\n        if len(task_instances) == 0:\\n            session.commit()\\n            return []\\n        TI = models.TaskInstance\\n        filter_for_ti_state_change = (\\n            [and_(\\n                TI.dag_id == ti.dag_id,\\n                TI.task_id == ti.task_id,\\n                TI.execution_date == ti.execution_date)\\n                for ti in task_instances])\\n        ti_query = (\\n            session\\n            .query(TI)\\n            .filter(or_(*filter_for_ti_state_change)))\\n        if acceptable_states:\\n            if None in acceptable_states:\\n                if all(x is None for x in acceptable_states):\\n                    ti_query = ti_query.filter(TI.state == None)  \\n                else:\\n                    not_none_acceptable_states = [state for state in acceptable_states if state]\\n                    ti_query = ti_query.filter(\\n                        or_(TI.state == None,  \\n                            TI.state.in_(not_none_acceptable_states))\\n                    )\\n            else:\\n                ti_query = ti_query.filter(TI.state.in_(acceptable_states))\\n        tis_to_set_to_queued = (\\n            ti_query\\n            .with_for_update()\\n            .all())\\n        if len(tis_to_set_to_queued) == 0:\\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\\n            session.commit()\\n            return []\\n        filter_for_tis = TI.filter_for_tis(tis_to_set_to_queued)\\n        session.query(TI).filter(filter_for_tis).update(\\n            {TI.state: State.QUEUED, TI.queued_dttm: timezone.utcnow()}, synchronize_session=False\\n        )\\n        session.commit()\\n        simple_task_instances = [SimpleTaskInstance(ti) for ti in tis_to_set_to_queued]\\n        task_instance_str = \"\\n\\t\".join([repr(x) for x in tis_to_set_to_queued])\\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\\n                      len(tis_to_set_to_queued), task_instance_str)\\n        return simple_task_instances",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6907] Simplify SchedulerJob (#7527)",
    "fixed_code": "def _change_state_for_executable_task_instances(self, task_instances, session=None):\\n        if len(task_instances) == 0:\\n            session.commit()\\n            return []\\n        TI = models.TaskInstance\\n        tis_to_set_to_queued = (\\n            session\\n            .query(TI)\\n            .filter(TI.filter_for_tis(task_instances))\\n            .filter(TI.state == State.SCHEDULED)\\n            .with_for_update()\\n            .all()\\n        )\\n        if len(tis_to_set_to_queued) == 0:\\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\\n            session.commit()\\n            return []\\n        filter_for_tis = TI.filter_for_tis(tis_to_set_to_queued)\\n        session.query(TI).filter(filter_for_tis).update(\\n            {TI.state: State.QUEUED, TI.queued_dttm: timezone.utcnow()}, synchronize_session=False\\n        )\\n        session.commit()\\n        simple_task_instances = [SimpleTaskInstance(ti) for ti in tis_to_set_to_queued]\\n        task_instance_str = \"\\n\\t\".join([repr(x) for x in tis_to_set_to_queued])\\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\\n                      len(tis_to_set_to_queued), task_instance_str)\\n        return simple_task_instances"
  },
  {
    "code": "def equals(self, other):\\n        if self.is_(other):\\n            return True\\n        return array_equivalent(_values_from_object(self),\\n                                _values_from_object(other))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/CLN: datetimelike Index.equals may return True with non-Index\\n\\ncloses #13107\\n\\nAuthor: sinhrks <sinhrks@gmail.com>\\n\\nCloses #13986 from sinhrks/dti_equals and squashes the following commits:\\n\\n580151a [sinhrks] BUG/CLN: move .equals to DatetimeOpsMixin",
    "fixed_code": "def equals(self, other):\\n        if self is other:\\n            return True\\n        if not isinstance(other, Index):\\n            return False\\n        try:\\n            if not isinstance(other, Float64Index):\\n                other = self._constructor(other)\\n            if (not is_dtype_equal(self.dtype, other.dtype) or\\n                    self.shape != other.shape):\\n                return False\\n            left, right = self._values, other._values\\n            return ((left == right) | (self._isnan & other._isnan)).all()\\n        except (TypeError, ValueError):\\n            return False"
  },
  {
    "code": "def generate_take_cython_file():\\n    directory = os.path.dirname(os.path.realpath(__file__))\\n    filename = 'generated.pyx'\\n    path = os.path.join(directory, filename)\\n    with open(path, 'w') as f:\\n        print(warning_to_new_contributors, file=f)\\n        print(header, file=f)\\n        print(generate_ensure_dtypes(), file=f)\\n        for template in templates_1d:\\n            print(generate_from_template(template), file=f)\\n        for template in take_templates:\\n            print(generate_take_template(template), file=f)\\n        for template in put_2d:\\n            print(generate_put_template(template), file=f)\\n        for template in groupbys:\\n            print(generate_put_template(template, use_ints=False), file=f)\\n        for template in groupby_selection:\\n            print(generate_put_selection_template(template, use_ints=True),\\n                  file=f)\\n        for template in groupby_min_max:\\n            print(generate_put_min_max_template(template, use_ints=True),\\n                  file=f)\\n        for template in nobool_1d_templates:\\n            print(generate_from_template(template, exclude=['bool']), file=f)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_take_cython_file():\\n    directory = os.path.dirname(os.path.realpath(__file__))\\n    filename = 'generated.pyx'\\n    path = os.path.join(directory, filename)\\n    with open(path, 'w') as f:\\n        print(warning_to_new_contributors, file=f)\\n        print(header, file=f)\\n        print(generate_ensure_dtypes(), file=f)\\n        for template in templates_1d:\\n            print(generate_from_template(template), file=f)\\n        for template in take_templates:\\n            print(generate_take_template(template), file=f)\\n        for template in put_2d:\\n            print(generate_put_template(template), file=f)\\n        for template in groupbys:\\n            print(generate_put_template(template, use_ints=False), file=f)\\n        for template in groupby_selection:\\n            print(generate_put_selection_template(template, use_ints=True),\\n                  file=f)\\n        for template in groupby_min_max:\\n            print(generate_put_min_max_template(template, use_ints=True),\\n                  file=f)\\n        for template in nobool_1d_templates:\\n            print(generate_from_template(template, exclude=['bool']), file=f)"
  },
  {
    "code": "def canonicalize(block):\\n\\t\\t\\treturn (block.mgr_locs.as_array.tolist(), block.dtype.name)\\n\\t\\tself_blocks = sorted(self.blocks, key=canonicalize)\\n\\t\\tother_blocks = sorted(other.blocks, key=canonicalize)\\n\\t\\treturn all(\\n\\t\\t\\tblock.equals(oblock) for block, oblock in zip(self_blocks, other_blocks)\\n\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def canonicalize(block):\\n\\t\\t\\treturn (block.mgr_locs.as_array.tolist(), block.dtype.name)\\n\\t\\tself_blocks = sorted(self.blocks, key=canonicalize)\\n\\t\\tother_blocks = sorted(other.blocks, key=canonicalize)\\n\\t\\treturn all(\\n\\t\\t\\tblock.equals(oblock) for block, oblock in zip(self_blocks, other_blocks)\\n\\t\\t)"
  },
  {
    "code": "def stringify_path(\\n    filepath_or_buffer: FilePathOrBuffer[AnyStr],\\n    convert_file_like: bool = False,\\n) -> FileOrBuffer[AnyStr]:\\n    if not convert_file_like and is_file_like(filepath_or_buffer):\\n        return cast(FileOrBuffer[AnyStr], filepath_or_buffer)\\n    if isinstance(filepath_or_buffer, os.PathLike):\\n        filepath_or_buffer = filepath_or_buffer.__fspath__()\\n    return _expand_user(filepath_or_buffer)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def stringify_path(\\n    filepath_or_buffer: FilePathOrBuffer[AnyStr],\\n    convert_file_like: bool = False,\\n) -> FileOrBuffer[AnyStr]:\\n    if not convert_file_like and is_file_like(filepath_or_buffer):\\n        return cast(FileOrBuffer[AnyStr], filepath_or_buffer)\\n    if isinstance(filepath_or_buffer, os.PathLike):\\n        filepath_or_buffer = filepath_or_buffer.__fspath__()\\n    return _expand_user(filepath_or_buffer)"
  },
  {
    "code": "def values(self) -> ArrayLike:\\n        return self._data\\n    @cache_readonly\\n    @doc(IndexOpsMixin.array)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def values(self) -> ArrayLike:\\n        return self._data\\n    @cache_readonly\\n    @doc(IndexOpsMixin.array)"
  },
  {
    "code": "def get_prep_lookup(self, lookup_type, value):\\n\\t\\tif hasattr(value, '_prepare'):\\n\\t\\t\\treturn value._prepare(self)\\n\\t\\tif lookup_type in {\\n\\t\\t\\t'iexact', 'contains', 'icontains',\\n\\t\\t\\t'startswith', 'istartswith', 'endswith', 'iendswith',\\n\\t\\t\\t'isnull', 'search', 'regex', 'iregex',\\n\\t\\t}:\\n\\t\\t\\treturn value\\n\\t\\telif lookup_type in ('exact', 'gt', 'gte', 'lt', 'lte'):\\n\\t\\t\\treturn self.get_prep_value(value)\\n\\t\\telif lookup_type in ('range', 'in'):\\n\\t\\t\\treturn [self.get_prep_value(v) for v in value]\\n\\t\\treturn self.get_prep_value(value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_prep_lookup(self, lookup_type, value):\\n\\t\\tif hasattr(value, '_prepare'):\\n\\t\\t\\treturn value._prepare(self)\\n\\t\\tif lookup_type in {\\n\\t\\t\\t'iexact', 'contains', 'icontains',\\n\\t\\t\\t'startswith', 'istartswith', 'endswith', 'iendswith',\\n\\t\\t\\t'isnull', 'search', 'regex', 'iregex',\\n\\t\\t}:\\n\\t\\t\\treturn value\\n\\t\\telif lookup_type in ('exact', 'gt', 'gte', 'lt', 'lte'):\\n\\t\\t\\treturn self.get_prep_value(value)\\n\\t\\telif lookup_type in ('range', 'in'):\\n\\t\\t\\treturn [self.get_prep_value(v) for v in value]\\n\\t\\treturn self.get_prep_value(value)"
  },
  {
    "code": "def _worker(executor_reference, work_queue, initializer, initargs):\\n    if initializer is not None:\\n        try:\\n            initializer(*initargs)\\n        except BaseException:\\n            _base.LOGGER.critical('Exception in initializer:', exc_info=True)\\n            executor = executor_reference()\\n            if executor is not None:\\n                executor._initializer_failed()\\n            return\\n    try:\\n        while True:\\n            work_item = work_queue.get(block=True)\\n            if work_item is not None:\\n                work_item.run()\\n                del work_item\\n                continue\\n            executor = executor_reference()\\n            if _shutdown or executor is None or executor._shutdown:\\n                if executor is not None:\\n                    executor._shutdown = True\\n                work_queue.put(None)\\n                return\\n            del executor\\n    except BaseException:\\n        _base.LOGGER.critical('Exception in worker', exc_info=True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-24882: Let ThreadPoolExecutor reuse idle threads before creating new thread (#6375)\\n\\n\\n\\n\\nAdjust the shutdown test so that, after submitting three jobs\\nto the executor, the test checks for less than three threads,\\ninstead of looking for exactly three threads.\\n\\nIf idle threads are being recycled properly, then we should have\\nless than three threads.\\n\\n\\nAs suggested by reviewer tomMoral, swapped lock-protected counter\\nwith a semaphore to track the number of unused threads.\\n\\nAdjusted test_threads_terminate to wait for completiton of the\\nprevious future before submitting a new one (and checking the\\nnumber of threads used).\\n\\nAlso added a new test to confirm the thread pool can be saturated.",
    "fixed_code": "def _worker(executor_reference, work_queue, initializer, initargs):\\n    if initializer is not None:\\n        try:\\n            initializer(*initargs)\\n        except BaseException:\\n            _base.LOGGER.critical('Exception in initializer:', exc_info=True)\\n            executor = executor_reference()\\n            if executor is not None:\\n                executor._initializer_failed()\\n            return\\n    try:\\n        while True:\\n            work_item = work_queue.get(block=True)\\n            if work_item is not None:\\n                work_item.run()\\n                del work_item\\n                executor = executor_reference()\\n                if executor is not None:\\n                    executor._idle_semaphore.release()\\n                del executor\\n                continue\\n            executor = executor_reference()\\n            if _shutdown or executor is None or executor._shutdown:\\n                if executor is not None:\\n                    executor._shutdown = True\\n                work_queue.put(None)\\n                return\\n            del executor\\n    except BaseException:\\n        _base.LOGGER.critical('Exception in worker', exc_info=True)"
  },
  {
    "code": "def update(self, current, values=None, force=False):\\n\\t\\tvalues = values or []\\n\\t\\tfor k, v in values:\\n\\t\\t\\tif k not in self.sum_values:\\n\\t\\t\\t\\tself.sum_values[k] = [v * (current - self.seen_so_far),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  current - self.seen_so_far]\\n\\t\\t\\t\\tself.unique_values.append(k)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.sum_values[k][0] += v * (current - self.seen_so_far)\\n\\t\\t\\t\\tself.sum_values[k][1] += (current - self.seen_so_far)\\n\\t\\tself.seen_so_far = current\\n\\t\\tnow = time.time()\\n\\t\\tinfo = ' - %.0fs' % (now - self.start)\\n\\t\\tif self.verbose == 1:\\n\\t\\t\\tif (not force and (now - self.last_update) < self.interval and\\n\\t\\t\\t\\t\\t(self.target is not None and current < self.target)):\\n\\t\\t\\t\\treturn\\n\\t\\t\\tprev_total_width = self.total_width\\n\\t\\t\\tif self._dynamic_display:\\n\\t\\t\\t\\tsys.stdout.write('\\b' * prev_total_width)\\n\\t\\t\\t\\tsys.stdout.write('\\r')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tsys.stdout.write('\\n')\\n\\t\\t\\tif self.target is not None:\\n\\t\\t\\t\\tnumdigits = int(np.floor(np.log10(self.target))) + 1\\n\\t\\t\\t\\tbarstr = '%%%dd/%d [' % (numdigits, self.target)\\n\\t\\t\\t\\tbar = barstr % current\\n\\t\\t\\t\\tprog = float(current) / self.target\\n\\t\\t\\t\\tprog_width = int(self.width * prog)\\n\\t\\t\\t\\tif prog_width > 0:\\n\\t\\t\\t\\t\\tbar += ('=' * (prog_width - 1))\\n\\t\\t\\t\\t\\tif current < self.target:\\n\\t\\t\\t\\t\\t\\tbar += '>'\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tbar += '='\\n\\t\\t\\t\\tbar += ('.' * (self.width - prog_width))\\n\\t\\t\\t\\tbar += ']'\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbar = '%7d/Unknown' % current\\n\\t\\t\\tself.total_width = len(bar)\\n\\t\\t\\tsys.stdout.write(bar)\\n\\t\\t\\tif current:\\n\\t\\t\\t\\ttime_per_unit = (now - self.start) / current\\n\\t\\t\\telse:\\n\\t\\t\\t\\ttime_per_unit = 0\\n\\t\\t\\tif self.target is not None and current < self.target:\\n\\t\\t\\t\\teta = time_per_unit * (self.target - current)\\n\\t\\t\\t\\tif eta > 3600:\\n\\t\\t\\t\\t\\teta_format = '%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)\\n\\t\\t\\t\\telif eta > 60:\\n\\t\\t\\t\\t\\teta_format = '%d:%02d' % (eta // 60, eta % 60)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\teta_format = '%ds' % eta\\n\\t\\t\\t\\tinfo = ' - ETA: %s' % eta_format\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif time_per_unit >= 1:\\n\\t\\t\\t\\t\\tinfo += ' %.0fs/step' % time_per_unit\\n\\t\\t\\t\\telif time_per_unit >= 1e-3:\\n\\t\\t\\t\\t\\tinfo += ' %.0fms/step' % (time_per_unit * 1e3)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tinfo += ' %.0fus/step' % (time_per_unit * 1e6)\\n\\t\\t\\tfor k in self.unique_values:\\n\\t\\t\\t\\tinfo += ' - %s:' % k\\n\\t\\t\\t\\tif isinstance(self.sum_values[k], list):\\n\\t\\t\\t\\t\\tavg = np.mean(\\n\\t\\t\\t\\t\\t\\tself.sum_values[k][0] / max(1, self.sum_values[k][1]))\\n\\t\\t\\t\\t\\tif abs(avg) > 1e-3:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4f' % avg\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4e' % avg\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tinfo += ' %s' % self.sum_values[k]\\n\\t\\t\\tself.total_width += len(info)\\n\\t\\t\\tif prev_total_width > self.total_width:\\n\\t\\t\\t\\tinfo += (' ' * (prev_total_width - self.total_width))\\n\\t\\t\\tif self.target is not None and current >= self.target:\\n\\t\\t\\t\\tinfo += '\\n'\\n\\t\\t\\tsys.stdout.write(info)\\n\\t\\t\\tsys.stdout.flush()\\n\\t\\telif self.verbose == 2:\\n\\t\\t\\tif self.target is None or current >= self.target:\\n\\t\\t\\t\\tfor k in self.unique_values:\\n\\t\\t\\t\\t\\tinfo += ' - %s:' % k\\n\\t\\t\\t\\t\\tavg = np.mean(\\n\\t\\t\\t\\t\\t\\tself.sum_values[k][0] / max(1, self.sum_values[k][1]))\\n\\t\\t\\t\\t\\tif avg > 1e-3:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4f' % avg\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4e' % avg\\n\\t\\t\\t\\tinfo += '\\n'\\n\\t\\t\\t\\tsys.stdout.write(info)\\n\\t\\t\\t\\tsys.stdout.flush()\\n\\t\\tself.last_update = now",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update(self, current, values=None, force=False):\\n\\t\\tvalues = values or []\\n\\t\\tfor k, v in values:\\n\\t\\t\\tif k not in self.sum_values:\\n\\t\\t\\t\\tself.sum_values[k] = [v * (current - self.seen_so_far),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  current - self.seen_so_far]\\n\\t\\t\\t\\tself.unique_values.append(k)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.sum_values[k][0] += v * (current - self.seen_so_far)\\n\\t\\t\\t\\tself.sum_values[k][1] += (current - self.seen_so_far)\\n\\t\\tself.seen_so_far = current\\n\\t\\tnow = time.time()\\n\\t\\tinfo = ' - %.0fs' % (now - self.start)\\n\\t\\tif self.verbose == 1:\\n\\t\\t\\tif (not force and (now - self.last_update) < self.interval and\\n\\t\\t\\t\\t\\t(self.target is not None and current < self.target)):\\n\\t\\t\\t\\treturn\\n\\t\\t\\tprev_total_width = self.total_width\\n\\t\\t\\tif self._dynamic_display:\\n\\t\\t\\t\\tsys.stdout.write('\\b' * prev_total_width)\\n\\t\\t\\t\\tsys.stdout.write('\\r')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tsys.stdout.write('\\n')\\n\\t\\t\\tif self.target is not None:\\n\\t\\t\\t\\tnumdigits = int(np.floor(np.log10(self.target))) + 1\\n\\t\\t\\t\\tbarstr = '%%%dd/%d [' % (numdigits, self.target)\\n\\t\\t\\t\\tbar = barstr % current\\n\\t\\t\\t\\tprog = float(current) / self.target\\n\\t\\t\\t\\tprog_width = int(self.width * prog)\\n\\t\\t\\t\\tif prog_width > 0:\\n\\t\\t\\t\\t\\tbar += ('=' * (prog_width - 1))\\n\\t\\t\\t\\t\\tif current < self.target:\\n\\t\\t\\t\\t\\t\\tbar += '>'\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tbar += '='\\n\\t\\t\\t\\tbar += ('.' * (self.width - prog_width))\\n\\t\\t\\t\\tbar += ']'\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbar = '%7d/Unknown' % current\\n\\t\\t\\tself.total_width = len(bar)\\n\\t\\t\\tsys.stdout.write(bar)\\n\\t\\t\\tif current:\\n\\t\\t\\t\\ttime_per_unit = (now - self.start) / current\\n\\t\\t\\telse:\\n\\t\\t\\t\\ttime_per_unit = 0\\n\\t\\t\\tif self.target is not None and current < self.target:\\n\\t\\t\\t\\teta = time_per_unit * (self.target - current)\\n\\t\\t\\t\\tif eta > 3600:\\n\\t\\t\\t\\t\\teta_format = '%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)\\n\\t\\t\\t\\telif eta > 60:\\n\\t\\t\\t\\t\\teta_format = '%d:%02d' % (eta // 60, eta % 60)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\teta_format = '%ds' % eta\\n\\t\\t\\t\\tinfo = ' - ETA: %s' % eta_format\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif time_per_unit >= 1:\\n\\t\\t\\t\\t\\tinfo += ' %.0fs/step' % time_per_unit\\n\\t\\t\\t\\telif time_per_unit >= 1e-3:\\n\\t\\t\\t\\t\\tinfo += ' %.0fms/step' % (time_per_unit * 1e3)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tinfo += ' %.0fus/step' % (time_per_unit * 1e6)\\n\\t\\t\\tfor k in self.unique_values:\\n\\t\\t\\t\\tinfo += ' - %s:' % k\\n\\t\\t\\t\\tif isinstance(self.sum_values[k], list):\\n\\t\\t\\t\\t\\tavg = np.mean(\\n\\t\\t\\t\\t\\t\\tself.sum_values[k][0] / max(1, self.sum_values[k][1]))\\n\\t\\t\\t\\t\\tif abs(avg) > 1e-3:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4f' % avg\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4e' % avg\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tinfo += ' %s' % self.sum_values[k]\\n\\t\\t\\tself.total_width += len(info)\\n\\t\\t\\tif prev_total_width > self.total_width:\\n\\t\\t\\t\\tinfo += (' ' * (prev_total_width - self.total_width))\\n\\t\\t\\tif self.target is not None and current >= self.target:\\n\\t\\t\\t\\tinfo += '\\n'\\n\\t\\t\\tsys.stdout.write(info)\\n\\t\\t\\tsys.stdout.flush()\\n\\t\\telif self.verbose == 2:\\n\\t\\t\\tif self.target is None or current >= self.target:\\n\\t\\t\\t\\tfor k in self.unique_values:\\n\\t\\t\\t\\t\\tinfo += ' - %s:' % k\\n\\t\\t\\t\\t\\tavg = np.mean(\\n\\t\\t\\t\\t\\t\\tself.sum_values[k][0] / max(1, self.sum_values[k][1]))\\n\\t\\t\\t\\t\\tif avg > 1e-3:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4f' % avg\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tinfo += ' %.4e' % avg\\n\\t\\t\\t\\tinfo += '\\n'\\n\\t\\t\\t\\tsys.stdout.write(info)\\n\\t\\t\\t\\tsys.stdout.flush()\\n\\t\\tself.last_update = now"
  },
  {
    "code": "def _log_preprocess(X):\\n    X = _make_nonnegative(X, min_value=1)\\n    if issparse(X):\\n        X = np.asarray(X.todense())\\n    L = np.log(X)\\n    row_avg = L.mean(axis=1)[:, np.newaxis]\\n    col_avg = L.mean(axis=0)\\n    avg = L.mean()\\n    return L - row_avg - col_avg + avg",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _log_preprocess(X):\\n    X = _make_nonnegative(X, min_value=1)\\n    if issparse(X):\\n        X = np.asarray(X.todense())\\n    L = np.log(X)\\n    row_avg = L.mean(axis=1)[:, np.newaxis]\\n    col_avg = L.mean(axis=0)\\n    avg = L.mean()\\n    return L - row_avg - col_avg + avg"
  },
  {
    "code": "def get_device_facts(self):\\n        device_facts = {}\\n        device_facts['devices'] = {}\\n        lspci = self.module.get_bin_path('lspci')\\n        if lspci:\\n            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\\n        else:\\n            pcidata = None\\n        try:\\n            block_devs = os.listdir(\"/sys/block\")\\n        except OSError:\\n            return device_facts\\n        devs_wwn = {}\\n        try:\\n            devs_by_id = os.listdir(\"/dev/disk/by-id\")\\n        except OSError:\\n            pass\\n        else:\\n            for link_name in devs_by_id:\\n                if link_name.startswith(\"wwn-\"):\\n                    try:\\n                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\\n                    except OSError:\\n                        continue\\n                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\\n        links = self.get_all_device_links()\\n        device_facts['device_links'] = links\\n        for block in block_devs:\\n            virtual = 1\\n            sysfs_no_links = 0\\n            try:\\n                path = os.readlink(os.path.join(\"/sys/block/\", block))\\n            except OSError:\\n                e = sys.exc_info()[1]\\n                if e.errno == errno.EINVAL:\\n                    path = block\\n                    sysfs_no_links = 1\\n                else:\\n                    continue\\n            sysdir = os.path.join(\"/sys/block\", path)\\n            if sysfs_no_links == 1:\\n                for folder in os.listdir(sysdir):\\n                    if \"device\" in folder:\\n                        virtual = 0\\n                        break\\n            d = {}\\n            d['virtual'] = virtual\\n            d['links'] = {}\\n            for (link_type, link_values) in iteritems(links):\\n                d['links'][link_type] = link_values.get(block, [])\\n            diskname = os.path.basename(sysdir)\\n            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\\n                d[key] = get_file_content(sysdir + \"/device/\" + key)\\n            sg_inq = self.module.get_bin_path('sg_inq')\\n            serial_path = \"/sys/block/%s/device/serial\" % (block)\\n            if sg_inq:\\n                serial = self._get_sg_inq_serial(sg_inq, block)\\n                if serial:\\n                    d['serial'] = serial\\n            else:\\n                serial = get_file_content(serial_path)\\n                if serial:\\n                    d['serial'] = serial\\n            for key, test in [('removable', '/removable'),\\n                              ('support_discard', '/queue/discard_granularity'),\\n                              ]:\\n                d[key] = get_file_content(sysdir + test)\\n            if diskname in devs_wwn:\\n                d['wwn'] = devs_wwn[diskname]\\n            d['partitions'] = {}\\n            for folder in os.listdir(sysdir):\\n                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\\n                if m:\\n                    part = {}\\n                    partname = m.group(1)\\n                    part_sysdir = sysdir + \"/\" + partname\\n                    part['links'] = {}\\n                    for (link_type, link_values) in iteritems(links):\\n                        part['links'][link_type] = link_values.get(partname, [])\\n                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\\n                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\\n                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\\n                    if not part['sectorsize']:\\n                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\\n                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\\n                    part['uuid'] = get_partition_uuid(partname)\\n                    self.get_holders(part, part_sysdir)\\n                    d['partitions'][partname] = part\\n            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\\n            d['scheduler_mode'] = \"\"\\n            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\\n            if scheduler is not None:\\n                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\\n                if m:\\n                    d['scheduler_mode'] = m.group(2)\\n            d['sectors'] = get_file_content(sysdir + \"/size\")\\n            if not d['sectors']:\\n                d['sectors'] = 0\\n            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\\n            if not d['sectorsize']:\\n                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\\n            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\\n            d['host'] = \"\"\\n            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\\n            if m and pcidata:\\n                pciid = m.group(1)\\n                did = re.escape(pciid)\\n                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\\n                if m:\\n                    d['host'] = m.group(1)\\n            self.get_holders(d, sysdir)\\n            device_facts['devices'][diskname] = d\\n        return device_facts",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_device_facts(self):\\n        device_facts = {}\\n        device_facts['devices'] = {}\\n        lspci = self.module.get_bin_path('lspci')\\n        if lspci:\\n            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\\n        else:\\n            pcidata = None\\n        try:\\n            block_devs = os.listdir(\"/sys/block\")\\n        except OSError:\\n            return device_facts\\n        devs_wwn = {}\\n        try:\\n            devs_by_id = os.listdir(\"/dev/disk/by-id\")\\n        except OSError:\\n            pass\\n        else:\\n            for link_name in devs_by_id:\\n                if link_name.startswith(\"wwn-\"):\\n                    try:\\n                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\\n                    except OSError:\\n                        continue\\n                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\\n        links = self.get_all_device_links()\\n        device_facts['device_links'] = links\\n        for block in block_devs:\\n            virtual = 1\\n            sysfs_no_links = 0\\n            try:\\n                path = os.readlink(os.path.join(\"/sys/block/\", block))\\n            except OSError:\\n                e = sys.exc_info()[1]\\n                if e.errno == errno.EINVAL:\\n                    path = block\\n                    sysfs_no_links = 1\\n                else:\\n                    continue\\n            sysdir = os.path.join(\"/sys/block\", path)\\n            if sysfs_no_links == 1:\\n                for folder in os.listdir(sysdir):\\n                    if \"device\" in folder:\\n                        virtual = 0\\n                        break\\n            d = {}\\n            d['virtual'] = virtual\\n            d['links'] = {}\\n            for (link_type, link_values) in iteritems(links):\\n                d['links'][link_type] = link_values.get(block, [])\\n            diskname = os.path.basename(sysdir)\\n            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\\n                d[key] = get_file_content(sysdir + \"/device/\" + key)\\n            sg_inq = self.module.get_bin_path('sg_inq')\\n            serial_path = \"/sys/block/%s/device/serial\" % (block)\\n            if sg_inq:\\n                serial = self._get_sg_inq_serial(sg_inq, block)\\n                if serial:\\n                    d['serial'] = serial\\n            else:\\n                serial = get_file_content(serial_path)\\n                if serial:\\n                    d['serial'] = serial\\n            for key, test in [('removable', '/removable'),\\n                              ('support_discard', '/queue/discard_granularity'),\\n                              ]:\\n                d[key] = get_file_content(sysdir + test)\\n            if diskname in devs_wwn:\\n                d['wwn'] = devs_wwn[diskname]\\n            d['partitions'] = {}\\n            for folder in os.listdir(sysdir):\\n                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\\n                if m:\\n                    part = {}\\n                    partname = m.group(1)\\n                    part_sysdir = sysdir + \"/\" + partname\\n                    part['links'] = {}\\n                    for (link_type, link_values) in iteritems(links):\\n                        part['links'][link_type] = link_values.get(partname, [])\\n                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\\n                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\\n                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\\n                    if not part['sectorsize']:\\n                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\\n                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\\n                    part['uuid'] = get_partition_uuid(partname)\\n                    self.get_holders(part, part_sysdir)\\n                    d['partitions'][partname] = part\\n            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\\n            d['scheduler_mode'] = \"\"\\n            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\\n            if scheduler is not None:\\n                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\\n                if m:\\n                    d['scheduler_mode'] = m.group(2)\\n            d['sectors'] = get_file_content(sysdir + \"/size\")\\n            if not d['sectors']:\\n                d['sectors'] = 0\\n            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\\n            if not d['sectorsize']:\\n                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\\n            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\\n            d['host'] = \"\"\\n            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\\n            if m and pcidata:\\n                pciid = m.group(1)\\n                did = re.escape(pciid)\\n                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\\n                if m:\\n                    d['host'] = m.group(1)\\n            self.get_holders(d, sysdir)\\n            device_facts['devices'][diskname] = d\\n        return device_facts"
  },
  {
    "code": "def time_lstrip(self):\\n        self.s.str.lstrip(\"A\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_lstrip(self, dtype):\\n        self.s.str.lstrip(\"A\")"
  },
  {
    "code": "def get_absolute_url(self):\\n        return iri_to_uri(get_script_prefix().rstrip('/') + self.url)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #22261 -- Fixed resolving namespaced URLs for flatpages.",
    "fixed_code": "def get_absolute_url(self):\\n        from .views import flatpage\\n        for url in (self.url.lstrip('/'), self.url):\\n            try:\\n                return reverse(flatpage, kwargs={'url': url})\\n            except NoReverseMatch:\\n                pass\\n        return iri_to_uri(get_script_prefix().rstrip('/') + self.url)"
  },
  {
    "code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    else:\\n        return left_ax, None, right_indexer",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "improves merge performance when key space exceeds i8 bounds",
    "fixed_code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    return left_ax, None, right_indexer"
  },
  {
    "code": "def test_parse_cookiecutter_args():\\n\\targs = main.parse_cookiecutter_args(['project/'])\\n\\tassert args.input_dir == 'project/'\\n\\tassert args.checkout is None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_parse_cookiecutter_args():\\n\\targs = main.parse_cookiecutter_args(['project/'])\\n\\tassert args.input_dir == 'project/'\\n\\tassert args.checkout is None"
  },
  {
    "code": "def _delegate_method(self, name, *args, **kwargs):\\n        method = getattr(self.values, name)\\n        result = method(*args, **kwargs)\\n        if not com.is_list_like(result):\\n            return result\\n        result = Series(result, index=self.index)\\n        result.is_copy = (\"modifications to a method of a datetimelike object are not \"\\n                          \"supported and are discarded. Change values on the original.\")\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH/DOC: reimplement Series delegates/accessors using descriptors\\n\\nThis PR implements `Series.str`, `Series.dt` and `Series.cat` as descriptors\\ninstead of properties. This means that the API docs can refer to methods like\\n`Series.str.lower` instead of `StringMethods.lower` and tab-completion like\\n`Series.str.<tab>` also works, even on the base class.\\n\\nCC jorisvandenbossche jreback",
    "fixed_code": "def _delegate_method(self, name, *args, **kwargs):\\n        from pandas import Series\\n        method = getattr(self.values, name)\\n        result = method(*args, **kwargs)\\n        if not com.is_list_like(result):\\n            return result\\n        result = Series(result, index=self.index)\\n        result.is_copy = (\"modifications to a method of a datetimelike object are not \"\\n                          \"supported and are discarded. Change values on the original.\")\\n        return result"
  },
  {
    "code": "def set_sequences(self, sequences):\\n        f = open(os.path.join(self._path, '.mh_sequences'), 'r+')\\n        try:\\n            os.close(os.open(f.name, os.O_WRONLY | os.O_TRUNC))\\n            for name, keys in sequences.items():\\n                if len(keys) == 0:\\n                    continue\\n                f.write('%s:' % name)\\n                prev = None\\n                completing = False\\n                for key in sorted(set(keys)):\\n                    if key - 1 == prev:\\n                        if not completing:\\n                            completing = True\\n                            f.write('-')\\n                    elif completing:\\n                        completing = False\\n                        f.write('%s %s' % (prev, key))\\n                    else:\\n                        f.write(' %s' % key)\\n                    prev = key\\n                if completing:\\n                    f.write(str(prev) + '\\n')\\n                else:\\n                    f.write('\\n')\\n        finally:\\n            _sync_close(f)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Correct Issue#1561: test_mailbox failed on Windows. Open all text files with newline='', this is the only way to have consistent offsets.",
    "fixed_code": "def set_sequences(self, sequences):\\n        f = open(os.path.join(self._path, '.mh_sequences'), 'r+', newline='')\\n        try:\\n            os.close(os.open(f.name, os.O_WRONLY | os.O_TRUNC))\\n            for name, keys in sequences.items():\\n                if len(keys) == 0:\\n                    continue\\n                f.write('%s:' % name)\\n                prev = None\\n                completing = False\\n                for key in sorted(set(keys)):\\n                    if key - 1 == prev:\\n                        if not completing:\\n                            completing = True\\n                            f.write('-')\\n                    elif completing:\\n                        completing = False\\n                        f.write('%s %s' % (prev, key))\\n                    else:\\n                        f.write(' %s' % key)\\n                    prev = key\\n                if completing:\\n                    f.write(str(prev) + '\\n')\\n                else:\\n                    f.write('\\n')\\n        finally:\\n            _sync_close(f)"
  },
  {
    "code": "def loads(self, data):\\n        return simplejson.loads(data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #18023 -- Removed bundled simplejson.\\n\\nAnd started the deprecation path for django.utils.simplejson.\\n\\nThanks Alex Ogier, Clueless, and other contributors for their\\nwork on the patch.",
    "fixed_code": "def loads(self, data):\\n        return json.loads(data)"
  },
  {
    "code": "def _parse_latex_header_span(\\n    cell: dict[str, Any],\\n    multirow_align: str,\\n    multicol_align: str,\\n    wrap: bool = False,\\n    convert_css: bool = False,\\n) -> str:\\n    r\\n    display_val = _parse_latex_cell_styles(\\n        cell[\"cellstyle\"], cell[\"display_value\"], convert_css\\n    )\\n    if \"attributes\" in cell:\\n        attrs = cell[\"attributes\"]\\n        if 'colspan=\"' in attrs:\\n            colspan = attrs[attrs.find('colspan=\"') + 9 :]  \\n            colspan = int(colspan[: colspan.find('\"')])\\n            if \"naive-l\" == multicol_align:\\n                out = f\"{{{display_val}}}\" if wrap else f\"{display_val}\"\\n                blanks = \" & {}\" if wrap else \" &\"\\n                return out + blanks * (colspan - 1)\\n            elif \"naive-r\" == multicol_align:\\n                out = f\"{{{display_val}}}\" if wrap else f\"{display_val}\"\\n                blanks = \"{} & \" if wrap else \"& \"\\n                return blanks * (colspan - 1) + out\\n            return f\"\\\\multicolumn{{{colspan}}}{{{multicol_align}}}{{{display_val}}}\"\\n        elif 'rowspan=\"' in attrs:\\n            if multirow_align == \"naive\":\\n                return display_val\\n            rowspan = attrs[attrs.find('rowspan=\"') + 9 :]\\n            rowspan = int(rowspan[: rowspan.find('\"')])\\n            return f\"\\\\multirow[{multirow_align}]{{{rowspan}}}{{*}}{{{display_val}}}\"\\n    if wrap:\\n        return f\"{{{display_val}}}\"\\n    else:\\n        return display_val",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_latex_header_span(\\n    cell: dict[str, Any],\\n    multirow_align: str,\\n    multicol_align: str,\\n    wrap: bool = False,\\n    convert_css: bool = False,\\n) -> str:\\n    r\\n    display_val = _parse_latex_cell_styles(\\n        cell[\"cellstyle\"], cell[\"display_value\"], convert_css\\n    )\\n    if \"attributes\" in cell:\\n        attrs = cell[\"attributes\"]\\n        if 'colspan=\"' in attrs:\\n            colspan = attrs[attrs.find('colspan=\"') + 9 :]  \\n            colspan = int(colspan[: colspan.find('\"')])\\n            if \"naive-l\" == multicol_align:\\n                out = f\"{{{display_val}}}\" if wrap else f\"{display_val}\"\\n                blanks = \" & {}\" if wrap else \" &\"\\n                return out + blanks * (colspan - 1)\\n            elif \"naive-r\" == multicol_align:\\n                out = f\"{{{display_val}}}\" if wrap else f\"{display_val}\"\\n                blanks = \"{} & \" if wrap else \"& \"\\n                return blanks * (colspan - 1) + out\\n            return f\"\\\\multicolumn{{{colspan}}}{{{multicol_align}}}{{{display_val}}}\"\\n        elif 'rowspan=\"' in attrs:\\n            if multirow_align == \"naive\":\\n                return display_val\\n            rowspan = attrs[attrs.find('rowspan=\"') + 9 :]\\n            rowspan = int(rowspan[: rowspan.find('\"')])\\n            return f\"\\\\multirow[{multirow_align}]{{{rowspan}}}{{*}}{{{display_val}}}\"\\n    if wrap:\\n        return f\"{{{display_val}}}\"\\n    else:\\n        return display_val"
  },
  {
    "code": "def time_rpartition(self):\\n        self.s.str.rpartition(\"A\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_rpartition(self, dtype):\\n        self.s.str.rpartition(\"A\")"
  },
  {
    "code": "def newgroups(self, date, time, file=None):\\n        return self.longcmd('NEWGROUPS ' + date + ' ' + time, file)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def newgroups(self, date, *, file=None):\\n        if not isinstance(date, (datetime.date, datetime.date)):\\n            raise TypeError(\\n                \"the date parameter must be a date or datetime object, \"\\n                \"not '{:40}'\".format(date.__class__.__name__))\\n        date_str, time_str = _unparse_datetime(date, self.nntp_version < 2)\\n        cmd = 'NEWGROUPS {0} {1}'.format(date_str, time_str)\\n        resp, lines = self._longcmdstring(cmd, file)\\n        return resp, self._grouplist(lines)"
  },
  {
    "code": "def send_email_smtp(\\n    to: Union[str, Iterable[str]],\\n    subject: str,\\n    html_content: str,\\n    files: Optional[List[str]] = None,\\n    dryrun: bool = False,\\n    cc: Optional[Union[str, Iterable[str]]] = None,\\n    bcc: Optional[Union[str, Iterable[str]]] = None,\\n    mime_subtype: str = 'mixed',\\n    mime_charset: str = 'utf-8',\\n    conn_id: str = \"smtp_default\",\\n    from_email: str = None,\\n    custom_headers: Optional[Dict[str, Any]] = None,\\n    **kwargs,\\n):\\n    smtp_mail_from = conf.get('smtp', 'SMTP_MAIL_FROM')\\n    mail_from = smtp_mail_from or from_email\\n    msg, recipients = build_mime_message(\\n        mail_from=mail_from,\\n        to=to,\\n        subject=subject,\\n        html_content=html_content,\\n        files=files,\\n        cc=cc,\\n        bcc=bcc,\\n        mime_subtype=mime_subtype,\\n        mime_charset=mime_charset,\\n        custom_headers=custom_headers,\\n    )\\n    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix mypy errors in `tests/utils` (#20093)",
    "fixed_code": "def send_email_smtp(\\n    to: Union[str, Iterable[str]],\\n    subject: str,\\n    html_content: str,\\n    files: Optional[List[str]] = None,\\n    dryrun: bool = False,\\n    cc: Optional[Union[str, Iterable[str]]] = None,\\n    bcc: Optional[Union[str, Iterable[str]]] = None,\\n    mime_subtype: str = 'mixed',\\n    mime_charset: str = 'utf-8',\\n    conn_id: str = \"smtp_default\",\\n    from_email: Optional[str] = None,\\n    custom_headers: Optional[Dict[str, Any]] = None,\\n    **kwargs,\\n):\\n    smtp_mail_from = conf.get('smtp', 'SMTP_MAIL_FROM')\\n    if smtp_mail_from:\\n        mail_from = smtp_mail_from\\n    else:\\n        mail_from = from_email\\n    msg, recipients = build_mime_message(\\n        mail_from=mail_from,\\n        to=to,\\n        subject=subject,\\n        html_content=html_content,\\n        files=files,\\n        cc=cc,\\n        bcc=bcc,\\n        mime_subtype=mime_subtype,\\n        mime_charset=mime_charset,\\n        custom_headers=custom_headers,\\n    )\\n    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)"
  },
  {
    "code": "def users_list(args):\\n    appbuilder = cached_appbuilder()\\n    users = appbuilder.sm.get_all_users()\\n    fields = ['id', 'username', 'email', 'first_name', 'last_name', 'roles']\\n    users = [[user.__getattribute__(field) for field in fields] for user in users]\\n    msg = tabulate(users, [field.capitalize().replace('_', ' ') for field in fields],\\n                   tablefmt=\"fancy_grid\")\\n    print(msg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def users_list(args):\\n    appbuilder = cached_appbuilder()\\n    users = appbuilder.sm.get_all_users()\\n    fields = ['id', 'username', 'email', 'first_name', 'last_name', 'roles']\\n    users = [[user.__getattribute__(field) for field in fields] for user in users]\\n    msg = tabulate(users, [field.capitalize().replace('_', ' ') for field in fields],\\n                   tablefmt=\"fancy_grid\")\\n    print(msg)"
  },
  {
    "code": "def formfield(self, **kwargs):\\n        defaults = {'form_class': forms.FileField, 'max_length': self.max_length}\\n        defaults.update(kwargs)\\n        return super().formfield(**defaults)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def formfield(self, **kwargs):\\n        defaults = {'form_class': forms.FileField, 'max_length': self.max_length}\\n        defaults.update(kwargs)\\n        return super().formfield(**defaults)"
  },
  {
    "code": "def _hash_add(cls, fields, globals):\\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\\n    return _hash_fn(flds, globals)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _hash_add(cls, fields, globals):\\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\\n    return _hash_fn(flds, globals)"
  },
  {
    "code": "def _is_safe_url(url, allowed_hosts, require_https=False):\\n\\tif url.startswith('///'):\\n\\t\\treturn False\\n\\ttry:\\n\\t\\turl_info = _urlparse(url)\\n\\texcept ValueError:  \\n\\t\\treturn False\\n\\tif not url_info.netloc and url_info.scheme:\\n\\t\\treturn False\\n\\tif unicodedata.category(url[0])[0] == 'C':\\n\\t\\treturn False\\n\\tscheme = url_info.scheme\\n\\tif not url_info.scheme and url_info.netloc:\\n\\t\\tscheme = 'http'\\n\\tvalid_schemes = ['https'] if require_https else ['http', 'https']\\n\\treturn ((not url_info.netloc or url_info.netloc in allowed_hosts) and\\n\\t\\t\\t(not scheme or scheme in valid_schemes))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _is_safe_url(url, allowed_hosts, require_https=False):\\n\\tif url.startswith('///'):\\n\\t\\treturn False\\n\\ttry:\\n\\t\\turl_info = _urlparse(url)\\n\\texcept ValueError:  \\n\\t\\treturn False\\n\\tif not url_info.netloc and url_info.scheme:\\n\\t\\treturn False\\n\\tif unicodedata.category(url[0])[0] == 'C':\\n\\t\\treturn False\\n\\tscheme = url_info.scheme\\n\\tif not url_info.scheme and url_info.netloc:\\n\\t\\tscheme = 'http'\\n\\tvalid_schemes = ['https'] if require_https else ['http', 'https']\\n\\treturn ((not url_info.netloc or url_info.netloc in allowed_hosts) and\\n\\t\\t\\t(not scheme or scheme in valid_schemes))"
  },
  {
    "code": "def __init__(self, n_components=None, algorithm='parallel', whiten=True,\\n                 fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,\\n                 w_init=None, random_state=None, compute_sources=True):\\n        super(FastICA, self).__init__()\\n        self.n_components = n_components\\n        self.algorithm = algorithm\\n        self.whiten = whiten\\n        self.fun = fun\\n        self.fun_args = fun_args\\n        self.max_iter = max_iter\\n        self.tol = tol\\n        self.w_init = w_init\\n        self.random_state = random_state",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, n_components=None, algorithm='parallel', whiten=True,\\n                 fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,\\n                 w_init=None, random_state=None, compute_sources=True):\\n        super(FastICA, self).__init__()\\n        self.n_components = n_components\\n        self.algorithm = algorithm\\n        self.whiten = whiten\\n        self.fun = fun\\n        self.fun_args = fun_args\\n        self.max_iter = max_iter\\n        self.tol = tol\\n        self.w_init = w_init\\n        self.random_state = random_state"
  },
  {
    "code": "def replace(self, to_replace, value=None, method='pad', axis=0,\\n                inplace=False, limit=None):\\n        self._consolidate_inplace()\\n        axis = self._get_axis_number(axis)\\n        if value is None:\\n            return self._interpolate(to_replace, method, axis, inplace, limit)\\n        else:\\n            if len(self.columns) == 0:\\n                return self\\n            new_data = self._data\\n            if isinstance(to_replace, (dict, Series)):\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for c, src in to_replace.iteritems():\\n                        if c in value and c in self:\\n                            new_data = new_data.replace(src, value[c],\\n                                                        filter=[ c ],\\n                                                        inplace=inplace)\\n                elif not isinstance(value, (list, np.ndarray)):\\n                    new_data = self._data\\n                    for k, src in to_replace.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(src, value,\\n                                                        filter = [ k ],\\n                                                        inplace=inplace)\\n                else:\\n                    raise ValueError('Fill value must be scalar or dict or Series')\\n            elif isinstance(to_replace, (list, np.ndarray)):\\n                if isinstance(value, (list, np.ndarray)):\\n                    if len(to_replace) != len(value):\\n                        raise ValueError('Replacement lists must match '\\n                                         'in length. Expecting %d got %d ' %\\n                                         (len(to_replace), len(value)))\\n                    new_data = self._data.replace_list(to_replace, value,\\n                                                       inplace=inplace)\\n                else:  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace)\\n            else:\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for k, v in value.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(to_replace, v,\\n                                                        filter=[ k ],\\n                                                        inplace=inplace)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace)\\n                else:\\n                    raise ValueError('Invalid to_replace type: %s' %\\n                                     type(to_replace))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add regex functionality to DataFrame.replace\\n\\nadd default of None to to_replace\\n\\nadd ability to pass regex as to_replace regex\\n\\nRemove cruft\\n\\nmore tests and add ability to pass regex and value\\n\\nMake exceptions more clear; push examples to missing_data.rst\\n\\nremove interpolation call\\n\\nmake inplace work across axes in interpolate method\\n\\nability to use nested dicts for regexs and others\\n\\nmostly doc updates\\n\\nformatting\\n\\ninfer_types correction\\n\\nrls notes",
    "fixed_code": "def replace(self, to_replace=None, value=None, method='pad', axis=0,\\n                inplace=False, limit=None, regex=False, infer_types=False):\\n        if not isinstance(regex, bool) and to_replace is not None:\\n            raise AssertionError(\"'to_replace' must be 'None' if 'regex' is \"\\n                                 \"not a bool\")\\n        self._consolidate_inplace()\\n        axis = self._get_axis_number(axis)\\n        method = com._clean_fill_method(method)\\n        if value is None:\\n            if not isinstance(to_replace, (dict, Series)):\\n                if not isinstance(regex, (dict, Series)):\\n                    raise TypeError('If \"to_replace\" and \"value\" are both None'\\n                                    ' then regex must be a mapping')\\n                to_replace = regex\\n                regex = True\\n            items = to_replace.items()\\n            keys, values = itertools.izip(*items)\\n            are_mappings = [isinstance(v, (dict, Series)) for v in values]\\n            if any(are_mappings):\\n                if not all(are_mappings):\\n                    raise TypeError(\"If a nested mapping is passed, all values\"\\n                                    \" of the top level mapping must be \"\\n                                    \"mappings\")\\n                to_rep_dict = {}\\n                value_dict = {}\\n                for k, v in items:\\n                    to_rep_dict[k] = v.keys()\\n                    value_dict[k] = v.values()\\n                to_replace, value = to_rep_dict, value_dict\\n            else:\\n                to_replace, value = keys, values\\n            return self.replace(to_replace, value, method=method, axis=axis,\\n                                inplace=inplace, limit=limit, regex=regex,\\n                                infer_types=infer_types)\\n        else:\\n            if not len(self.columns):\\n                return self\\n            new_data = self._data\\n            if isinstance(to_replace, (dict, Series)):\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for c, src in to_replace.iteritems():\\n                        if c in value and c in self:\\n                            new_data = new_data.replace(src, value[c],\\n                                                        filter=[ c ],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data\\n                    for k, src in to_replace.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(src, value,\\n                                                        filter = [ k ],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                else:\\n                    raise TypeError('Fill value must be scalar, dict, or '\\n                                    'Series')\\n            elif isinstance(to_replace, (list, np.ndarray)):\\n                if isinstance(value, (list, np.ndarray)):\\n                    if len(to_replace) != len(value):\\n                        raise ValueError('Replacement lists must match '\\n                                         'in length. Expecting %d got %d ' %\\n                                         (len(to_replace), len(value)))\\n                    new_data = self._data.replace_list(to_replace, value,\\n                                                       inplace=inplace,\\n                                                       regex=regex)\\n                else:  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n            elif to_replace is None:\\n                if not (_re_compilable(regex) or\\n                        isinstance(regex, (list, dict, np.ndarray, Series))):\\n                    raise TypeError(\"'regex' must be a string or a compiled \"\\n                                    \"regular expression or a list or dict of \"\\n                                    \"strings or regular expressions, you \"\\n                                    \"passed a {0}\".format(type(regex)))\\n                return self.replace(regex, value, method=method, axis=axis,\\n                                    inplace=inplace, limit=limit, regex=True,\\n                                    infer_types=infer_types)\\n            else:\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for k, v in value.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(to_replace, v,\\n                                                        filter=[ k ],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n                else:\\n                    raise TypeError('Invalid \"to_replace\" type: '\\n                                    '{0}'.format(type(to_replace)))  \\n        if infer_types:\\n            new_data = new_data.convert()\\n        if inplace:\\n            self._data = new_data\\n        else:\\n            return self._constructor(new_data)"
  },
  {
    "code": "def to_locale(language):\\n    p = language.find('-')\\n    if p >= 0:\\n        if len(language[p + 1:]) > 2:\\n            return language[:p].lower() + '_' + language[p + 1].upper() + language[p + 2:].lower()\\n        return language[:p].lower() + '_' + language[p + 1:].upper()\\n    else:\\n        return language.lower()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #28546 -- Fixed translation's to_locale() with langauge subtags.\\n\\nThanks Brent Hand for the initial patch.",
    "fixed_code": "def to_locale(language):\\n    language = language.lower()\\n    parts = language.split('-')\\n    try:\\n        country = parts[1]\\n    except IndexError:\\n        return language\\n    else:\\n        parts[1] = country.title() if len(country) > 2 else country.upper()\\n    return parts[0] + '_' + '-'.join(parts[1:])"
  },
  {
    "code": "def gettext(message):\\n    return TECHNICAL_ID_MAP.get(message, message)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Translating safe strings should return a safe result.",
    "fixed_code": "def gettext(message):\\n    result = TECHNICAL_ID_MAP.get(message, message)\\n    if isinstance(message, SafeData):\\n        return mark_safe(result)\\n    return result"
  },
  {
    "code": "def fmgr_get_custom(fmgr, paramgram):\\n    if paramgram[\"custom_dict\"] is not None:\\n        datagram = paramgram[\"custom_dict\"]\\n    else:\\n        datagram = dict()\\n    url = paramgram[\"custom_endpoint\"]\\n    response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    return response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fmgr_get_custom(fmgr, paramgram):\\n    if paramgram[\"custom_dict\"] is not None:\\n        datagram = paramgram[\"custom_dict\"]\\n    else:\\n        datagram = dict()\\n    url = paramgram[\"custom_endpoint\"]\\n    response = fmgr.process_request(url, datagram, FMGRMethods.GET)\\n    return response"
  },
  {
    "code": "def sql_schema(self):\\n        from sqlalchemy.schema import CreateTable\\n        return str(CreateTable(self.table))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sql_schema(self):\\n        from sqlalchemy.schema import CreateTable\\n        return str(CreateTable(self.table))"
  },
  {
    "code": "def _lock_file(f, dotlock=True):\\n    dotlock_done = False\\n    try:\\n        if fcntl:\\n            try:\\n                fcntl.lockf(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\\n            except OSError as e:\\n                if e.errno in (errno.EAGAIN, errno.EACCES, errno.EROFS):\\n                    raise ExternalClashError('lockf: lock unavailable: %s' %\\n                                             f.name)\\n                else:\\n                    raise\\n        if dotlock:\\n            try:\\n                pre_lock = _create_temporary(f.name + '.lock')\\n                pre_lock.close()\\n            except OSError as e:\\n                if e.errno in (errno.EACCES, errno.EROFS):\\n                    return  \\n                else:\\n                    raise\\n            try:\\n                try:\\n                    os.link(pre_lock.name, f.name + '.lock')\\n                    dotlock_done = True\\n                except (AttributeError, PermissionError):\\n                    os.rename(pre_lock.name, f.name + '.lock')\\n                    dotlock_done = True\\n                else:\\n                    os.unlink(pre_lock.name)\\n            except FileExistsError:\\n                os.remove(pre_lock.name)\\n                raise ExternalClashError('dot lock unavailable: %s' %\\n                                         f.name)\\n    except:\\n        if fcntl:\\n            fcntl.lockf(f, fcntl.LOCK_UN)\\n        if dotlock_done:\\n            os.remove(f.name + '.lock')\\n        raise",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _lock_file(f, dotlock=True):\\n    dotlock_done = False\\n    try:\\n        if fcntl:\\n            try:\\n                fcntl.lockf(f, fcntl.LOCK_EX | fcntl.LOCK_NB)\\n            except OSError as e:\\n                if e.errno in (errno.EAGAIN, errno.EACCES, errno.EROFS):\\n                    raise ExternalClashError('lockf: lock unavailable: %s' %\\n                                             f.name)\\n                else:\\n                    raise\\n        if dotlock:\\n            try:\\n                pre_lock = _create_temporary(f.name + '.lock')\\n                pre_lock.close()\\n            except OSError as e:\\n                if e.errno in (errno.EACCES, errno.EROFS):\\n                    return  \\n                else:\\n                    raise\\n            try:\\n                try:\\n                    os.link(pre_lock.name, f.name + '.lock')\\n                    dotlock_done = True\\n                except (AttributeError, PermissionError):\\n                    os.rename(pre_lock.name, f.name + '.lock')\\n                    dotlock_done = True\\n                else:\\n                    os.unlink(pre_lock.name)\\n            except FileExistsError:\\n                os.remove(pre_lock.name)\\n                raise ExternalClashError('dot lock unavailable: %s' %\\n                                         f.name)\\n    except:\\n        if fcntl:\\n            fcntl.lockf(f, fcntl.LOCK_UN)\\n        if dotlock_done:\\n            os.remove(f.name + '.lock')\\n        raise"
  },
  {
    "code": "def install_local_project(path):\\n    path = os.path.abspath(path)\\n    if os.path.isdir(path):\\n        logger.info('Installing from source directory: %s', path)\\n        return _run_install_from_dir(path)\\n    elif _is_archive_file(path):\\n        logger.info('Installing from archive: %s', path)\\n        _unpacked_dir = tempfile.mkdtemp()\\n        shutil.unpack_archive(path, _unpacked_dir)\\n        return _run_install_from_archive(_unpacked_dir)\\n    else:\\n        logger.warning('No projects to install.')\\n        return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def install_local_project(path):\\n    path = os.path.abspath(path)\\n    if os.path.isdir(path):\\n        logger.info('Installing from source directory: %s', path)\\n        return _run_install_from_dir(path)\\n    elif _is_archive_file(path):\\n        logger.info('Installing from archive: %s', path)\\n        _unpacked_dir = tempfile.mkdtemp()\\n        shutil.unpack_archive(path, _unpacked_dir)\\n        return _run_install_from_archive(_unpacked_dir)\\n    else:\\n        logger.warning('No projects to install.')\\n        return False"
  },
  {
    "code": "def quit(self):\\n        try:\\n            resp = self._shortcmd('QUIT')\\n        finally:\\n            self._close()\\n        return resp",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quit(self):\\n        try:\\n            resp = self._shortcmd('QUIT')\\n        finally:\\n            self._close()\\n        return resp"
  },
  {
    "code": "def test(args, dag=None):\\n    logging.getLogger('airflow.task').propagate = True\\n    dag = dag or get_dag(args)\\n    task = dag.get_task(task_id=args.task_id)\\n    if args.task_params:\\n        passed_in_params = json.loads(args.task_params)\\n        task.params.update(passed_in_params)\\n    ti = TaskInstance(task, args.execution_date)\\n    try:\\n        if args.dry_run:\\n            ti.dry_run()\\n        else:\\n            ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)\\n    except Exception:\\n        if args.post_mortem:\\n            try:\\n                debugger = importlib.import_module(\"ipdb\")\\n            except ImportError:\\n                debugger = importlib.import_module(\"pdb\")\\n            debugger.post_mortem()\\n        else:\\n            raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def test(args, dag=None):\\n    logging.getLogger('airflow.task').propagate = True\\n    dag = dag or get_dag(args)\\n    task = dag.get_task(task_id=args.task_id)\\n    if args.task_params:\\n        passed_in_params = json.loads(args.task_params)\\n        task.params.update(passed_in_params)\\n    ti = TaskInstance(task, args.execution_date)\\n    try:\\n        if args.dry_run:\\n            ti.dry_run()\\n        else:\\n            ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)\\n    except Exception:  \\n        if args.post_mortem:\\n            try:\\n                debugger = importlib.import_module(\"ipdb\")\\n            except ImportError:\\n                debugger = importlib.import_module(\"pdb\")\\n            debugger.post_mortem()\\n        else:\\n            raise"
  },
  {
    "code": "def dump_svmlight_file(X, y, f, zero_based=True, comment=None, query_id=None):\\n    if comment is not None:\\n        if isinstance(comment, bytes):\\n            comment.decode(\"ascii\")     \\n        else:\\n            comment = comment.encode(\"utf-8\")\\n        if \"\\0\" in comment:\\n            raise ValueError(\"comment string contains NUL byte\")\\n    y = np.asarray(y)\\n    if y.ndim != 1:\\n        raise ValueError(\"expected y of shape (n_samples,), got %r\"\\n                         % (y.shape,))\\n    X = atleast2d_or_csr(X)\\n    if X.shape[0] != y.shape[0]:\\n        raise ValueError(\"X.shape[0] and y.shape[0] should be the same, \"\\n                         \"got: %r and %r instead.\" % (X.shape[0], y.shape[0]))\\n    if query_id is not None:\\n        query_id = np.asarray(query_id)\\n        if query_id.shape[0] != y.shape[0]:\\n            raise ValueError(\"expected query_id of shape (n_samples,), got %r\"\\n                             % (query_id.shape,))\\n    one_based = not zero_based\\n    if hasattr(f, \"write\"):\\n        _dump_svmlight(X, y, f, one_based, comment, query_id)\\n    else:\\n        with open(f, \"wb\") as f:\\n            _dump_svmlight(X, y, f, one_based, comment, query_id)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def dump_svmlight_file(X, y, f, zero_based=True, comment=None, query_id=None):\\n    if comment is not None:\\n        if isinstance(comment, bytes):\\n            comment.decode(\"ascii\")     \\n        else:\\n            comment = comment.encode(\"utf-8\")\\n        if \"\\0\" in comment:\\n            raise ValueError(\"comment string contains NUL byte\")\\n    y = np.asarray(y)\\n    if y.ndim != 1:\\n        raise ValueError(\"expected y of shape (n_samples,), got %r\"\\n                         % (y.shape,))\\n    X = atleast2d_or_csr(X)\\n    if X.shape[0] != y.shape[0]:\\n        raise ValueError(\"X.shape[0] and y.shape[0] should be the same, \"\\n                         \"got: %r and %r instead.\" % (X.shape[0], y.shape[0]))\\n    if query_id is not None:\\n        query_id = np.asarray(query_id)\\n        if query_id.shape[0] != y.shape[0]:\\n            raise ValueError(\"expected query_id of shape (n_samples,), got %r\"\\n                             % (query_id.shape,))\\n    one_based = not zero_based\\n    if hasattr(f, \"write\"):\\n        _dump_svmlight(X, y, f, one_based, comment, query_id)\\n    else:\\n        with open(f, \"wb\") as f:\\n            _dump_svmlight(X, y, f, one_based, comment, query_id)"
  },
  {
    "code": "def JSONObject(s_and_end, strict, scan_once, object_hook, object_pairs_hook,\\n               memo=None, _w=WHITESPACE.match, _ws=WHITESPACE_STR):\\n    s, end = s_and_end\\n    pairs = []\\n    pairs_append = pairs.append\\n    if memo is None:\\n        memo = {}\\n    memo_get = memo.setdefault\\n    nextchar = s[end:end + 1]\\n    if nextchar != '\"':\\n        if nextchar in _ws:\\n            end = _w(s, end).end()\\n            nextchar = s[end:end + 1]\\n        if nextchar == '}':\\n            if object_pairs_hook is not None:\\n                result = object_pairs_hook(pairs)\\n                return result, end\\n            pairs = {}\\n            if object_hook is not None:\\n                pairs = object_hook(pairs)\\n            return pairs, end + 1\\n        elif nextchar != '\"':\\n            raise ValueError(errmsg(\\n                \"Expecting property name enclosed in double quotes\", s, end))\\n    end += 1\\n    while True:\\n        key, end = scanstring(s, end, strict)\\n        key = memo_get(key, key)\\n        if s[end:end + 1] != ':':\\n            end = _w(s, end).end()\\n            if s[end:end + 1] != ':':\\n                raise ValueError(errmsg(\"Expecting ':' delimiter\", s, end))\\n        end += 1\\n        try:\\n            if s[end] in _ws:\\n                end += 1\\n                if s[end] in _ws:\\n                    end = _w(s, end + 1).end()\\n        except IndexError:\\n            pass\\n        try:\\n            value, end = scan_once(s, end)\\n        except StopIteration:\\n            raise ValueError(errmsg(\"Expecting object\", s, end))\\n        pairs_append((key, value))\\n        try:\\n            nextchar = s[end]\\n            if nextchar in _ws:\\n                end = _w(s, end + 1).end()\\n                nextchar = s[end]\\n        except IndexError:\\n            nextchar = ''\\n        end += 1\\n        if nextchar == '}':\\n            break\\n        elif nextchar != ',':\\n            raise ValueError(errmsg(\"Expecting ',' delimiter\", s, end - 1))\\n        end = _w(s, end).end()\\n        nextchar = s[end:end + 1]\\n        end += 1\\n        if nextchar != '\"':\\n            raise ValueError(errmsg(\\n                \"Expecting property name enclosed in double quotes\", s, end - 1))\\n    if object_pairs_hook is not None:\\n        result = object_pairs_hook(pairs)\\n        return result, end\\n    pairs = dict(pairs)\\n    if object_hook is not None:\\n        pairs = object_hook(pairs)\\n    return pairs, end",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#17368: Fix an off-by-one error in the Python JSON decoder that caused a failure while decoding empty object literals when object_pairs_hook was specified.",
    "fixed_code": "def JSONObject(s_and_end, strict, scan_once, object_hook, object_pairs_hook,\\n               memo=None, _w=WHITESPACE.match, _ws=WHITESPACE_STR):\\n    s, end = s_and_end\\n    pairs = []\\n    pairs_append = pairs.append\\n    if memo is None:\\n        memo = {}\\n    memo_get = memo.setdefault\\n    nextchar = s[end:end + 1]\\n    if nextchar != '\"':\\n        if nextchar in _ws:\\n            end = _w(s, end).end()\\n            nextchar = s[end:end + 1]\\n        if nextchar == '}':\\n            if object_pairs_hook is not None:\\n                result = object_pairs_hook(pairs)\\n                return result, end + 1\\n            pairs = {}\\n            if object_hook is not None:\\n                pairs = object_hook(pairs)\\n            return pairs, end + 1\\n        elif nextchar != '\"':\\n            raise ValueError(errmsg(\\n                \"Expecting property name enclosed in double quotes\", s, end))\\n    end += 1\\n    while True:\\n        key, end = scanstring(s, end, strict)\\n        key = memo_get(key, key)\\n        if s[end:end + 1] != ':':\\n            end = _w(s, end).end()\\n            if s[end:end + 1] != ':':\\n                raise ValueError(errmsg(\"Expecting ':' delimiter\", s, end))\\n        end += 1\\n        try:\\n            if s[end] in _ws:\\n                end += 1\\n                if s[end] in _ws:\\n                    end = _w(s, end + 1).end()\\n        except IndexError:\\n            pass\\n        try:\\n            value, end = scan_once(s, end)\\n        except StopIteration:\\n            raise ValueError(errmsg(\"Expecting object\", s, end))\\n        pairs_append((key, value))\\n        try:\\n            nextchar = s[end]\\n            if nextchar in _ws:\\n                end = _w(s, end + 1).end()\\n                nextchar = s[end]\\n        except IndexError:\\n            nextchar = ''\\n        end += 1\\n        if nextchar == '}':\\n            break\\n        elif nextchar != ',':\\n            raise ValueError(errmsg(\"Expecting ',' delimiter\", s, end - 1))\\n        end = _w(s, end).end()\\n        nextchar = s[end:end + 1]\\n        end += 1\\n        if nextchar != '\"':\\n            raise ValueError(errmsg(\\n                \"Expecting property name enclosed in double quotes\", s, end - 1))\\n    if object_pairs_hook is not None:\\n        result = object_pairs_hook(pairs)\\n        return result, end\\n    pairs = dict(pairs)\\n    if object_hook is not None:\\n        pairs = object_hook(pairs)\\n    return pairs, end"
  },
  {
    "code": "def _unstack_multiple(data, clocs, fill_value=None):\\n\\tif len(clocs) == 0:\\n\\t\\treturn data\\n\\tindex = data.index\\n\\tclocs = [index._get_level_number(i) for i in clocs]\\n\\trlocs = [i for i in range(index.nlevels) if i not in clocs]\\n\\tclevels = [index.levels[i] for i in clocs]\\n\\tccodes = [index.codes[i] for i in clocs]\\n\\tcnames = [index.names[i] for i in clocs]\\n\\trlevels = [index.levels[i] for i in rlocs]\\n\\trcodes = [index.codes[i] for i in rlocs]\\n\\trnames = [index.names[i] for i in rlocs]\\n\\tshape = [len(x) for x in clevels]\\n\\tgroup_index = get_group_index(ccodes, shape, sort=False, xnull=False)\\n\\tcomp_ids, obs_ids = compress_group_index(group_index, sort=False)\\n\\trecons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\\n\\tif rlocs == []:\\n\\t\\tdummy_index = Index(obs_ids, name=\"__placeholder__\")\\n\\telse:\\n\\t\\tdummy_index = MultiIndex(\\n\\t\\t\\tlevels=rlevels + [obs_ids],\\n\\t\\t\\tcodes=rcodes + [comp_ids],\\n\\t\\t\\tnames=rnames + [\"__placeholder__\"],\\n\\t\\t\\tverify_integrity=False,\\n\\t\\t)\\n\\tif isinstance(data, Series):\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tnew_levels = clevels\\n\\t\\tnew_names = cnames\\n\\t\\tnew_codes = recons_codes\\n\\telse:\\n\\t\\tif isinstance(data.columns, MultiIndex):\\n\\t\\t\\tresult = data\\n\\t\\t\\tfor i in range(len(clocs)):\\n\\t\\t\\t\\tval = clocs[i]\\n\\t\\t\\t\\tresult = result.unstack(val, fill_value=fill_value)\\n\\t\\t\\t\\tclocs = [v if i > v else v - 1 for v in clocs]\\n\\t\\t\\treturn result\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tif isinstance(unstacked, Series):\\n\\t\\t\\tunstcols = unstacked.index\\n\\t\\telse:\\n\\t\\t\\tunstcols = unstacked.columns\\n\\t\\tnew_levels = [unstcols.levels[0]] + clevels\\n\\t\\tnew_names = [data.columns.name] + cnames\\n\\t\\tnew_codes = [unstcols.codes[0]]\\n\\t\\tfor rec in recons_codes:\\n\\t\\t\\tnew_codes.append(rec.take(unstcols.codes[-1]))\\n\\tnew_columns = MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)\\n\\tif isinstance(unstacked, Series):\\n\\t\\tunstacked.index = new_columns\\n\\telse:\\n\\t\\tunstacked.columns = new_columns\\n\\treturn unstacked",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _unstack_multiple(data, clocs, fill_value=None):\\n\\tif len(clocs) == 0:\\n\\t\\treturn data\\n\\tindex = data.index\\n\\tclocs = [index._get_level_number(i) for i in clocs]\\n\\trlocs = [i for i in range(index.nlevels) if i not in clocs]\\n\\tclevels = [index.levels[i] for i in clocs]\\n\\tccodes = [index.codes[i] for i in clocs]\\n\\tcnames = [index.names[i] for i in clocs]\\n\\trlevels = [index.levels[i] for i in rlocs]\\n\\trcodes = [index.codes[i] for i in rlocs]\\n\\trnames = [index.names[i] for i in rlocs]\\n\\tshape = [len(x) for x in clevels]\\n\\tgroup_index = get_group_index(ccodes, shape, sort=False, xnull=False)\\n\\tcomp_ids, obs_ids = compress_group_index(group_index, sort=False)\\n\\trecons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\\n\\tif rlocs == []:\\n\\t\\tdummy_index = Index(obs_ids, name=\"__placeholder__\")\\n\\telse:\\n\\t\\tdummy_index = MultiIndex(\\n\\t\\t\\tlevels=rlevels + [obs_ids],\\n\\t\\t\\tcodes=rcodes + [comp_ids],\\n\\t\\t\\tnames=rnames + [\"__placeholder__\"],\\n\\t\\t\\tverify_integrity=False,\\n\\t\\t)\\n\\tif isinstance(data, Series):\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tnew_levels = clevels\\n\\t\\tnew_names = cnames\\n\\t\\tnew_codes = recons_codes\\n\\telse:\\n\\t\\tif isinstance(data.columns, MultiIndex):\\n\\t\\t\\tresult = data\\n\\t\\t\\tfor i in range(len(clocs)):\\n\\t\\t\\t\\tval = clocs[i]\\n\\t\\t\\t\\tresult = result.unstack(val, fill_value=fill_value)\\n\\t\\t\\t\\tclocs = [v if i > v else v - 1 for v in clocs]\\n\\t\\t\\treturn result\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tif isinstance(unstacked, Series):\\n\\t\\t\\tunstcols = unstacked.index\\n\\t\\telse:\\n\\t\\t\\tunstcols = unstacked.columns\\n\\t\\tnew_levels = [unstcols.levels[0]] + clevels\\n\\t\\tnew_names = [data.columns.name] + cnames\\n\\t\\tnew_codes = [unstcols.codes[0]]\\n\\t\\tfor rec in recons_codes:\\n\\t\\t\\tnew_codes.append(rec.take(unstcols.codes[-1]))\\n\\tnew_columns = MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)\\n\\tif isinstance(unstacked, Series):\\n\\t\\tunstacked.index = new_columns\\n\\telse:\\n\\t\\tunstacked.columns = new_columns\\n\\treturn unstacked"
  },
  {
    "code": "def time_match(self, dtype):\\n        self.s.str.match(\"A\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_match(self, dtype):\\n        self.s.str.match(\"A\")"
  },
  {
    "code": "def main():\\n    rhsm = Rhsm(None)\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            state=dict(default='present',\\n                       choices=['present', 'absent']),\\n            username=dict(default=None,\\n                          required=False),\\n            password=dict(default=None,\\n                          required=False,\\n                          no_log=True),\\n            server_hostname=dict(default=None,\\n                                 required=False),\\n            server_insecure=dict(default=None,\\n                                 required=False),\\n            rhsm_baseurl=dict(default=None,\\n                              required=False),\\n            rhsm_repo_ca_cert=dict(default=None, required=False),\\n            auto_attach=dict(aliases=['autosubscribe'], default=False, type='bool'),\\n            activationkey=dict(default=None,\\n                               required=False,\\n                               no_log=True),\\n            org_id=dict(default=None,\\n                        required=False),\\n            environment=dict(default=None,\\n                             required=False, type='str'),\\n            pool=dict(default='^$',\\n                      required=False,\\n                      type='str'),\\n            pool_ids=dict(default=[],\\n                          required=False,\\n                          type='list'),\\n            consumer_type=dict(default=None,\\n                               required=False),\\n            consumer_name=dict(default=None,\\n                               required=False),\\n            consumer_id=dict(default=None,\\n                             required=False),\\n            force_register=dict(default=False,\\n                                type='bool'),\\n            server_proxy_hostname=dict(default=None,\\n                                       required=False),\\n            server_proxy_port=dict(default=None,\\n                                   required=False),\\n            server_proxy_user=dict(default=None,\\n                                   required=False),\\n            server_proxy_password=dict(default=None,\\n                                       required=False,\\n                                       no_log=True),\\n            release=dict(default=None, required=False)\\n        ),\\n        required_together=[['username', 'password'],\\n                           ['server_proxy_hostname', 'server_proxy_port'],\\n                           ['server_proxy_user', 'server_proxy_password']],\\n        mutually_exclusive=[['activationkey', 'username'],\\n                            ['activationkey', 'consumer_id'],\\n                            ['activationkey', 'environment'],\\n                            ['activationkey', 'autosubscribe'],\\n                            ['force', 'consumer_id'],\\n                            ['pool', 'pool_ids']],\\n        required_if=[['state', 'present', ['username', 'activationkey'], True]],\\n    )\\n    rhsm.module = module\\n    state = module.params['state']\\n    username = module.params['username']\\n    password = module.params['password']\\n    server_hostname = module.params['server_hostname']\\n    server_insecure = module.params['server_insecure']\\n    rhsm_baseurl = module.params['rhsm_baseurl']\\n    rhsm_repo_ca_cert = module.params['rhsm_repo_ca_cert']\\n    auto_attach = module.params['auto_attach']\\n    activationkey = module.params['activationkey']\\n    org_id = module.params['org_id']\\n    if activationkey and not org_id:\\n        module.fail_json(msg='org_id is required when using activationkey')\\n    environment = module.params['environment']\\n    pool = module.params['pool']\\n    pool_ids = {}\\n    for value in module.params['pool_ids']:\\n        if isinstance(value, dict):\\n            if len(value) != 1:\\n                module.fail_json(msg='Unable to parse pool_ids option.')\\n            pool_id, quantity = list(value.items())[0]\\n        else:\\n            pool_id, quantity = value, 1\\n        pool_ids[pool_id] = str(quantity)\\n    consumer_type = module.params[\"consumer_type\"]\\n    consumer_name = module.params[\"consumer_name\"]\\n    consumer_id = module.params[\"consumer_id\"]\\n    force_register = module.params[\"force_register\"]\\n    server_proxy_hostname = module.params['server_proxy_hostname']\\n    server_proxy_port = module.params['server_proxy_port']\\n    server_proxy_user = module.params['server_proxy_user']\\n    server_proxy_password = module.params['server_proxy_password']\\n    release = module.params['release']\\n    global SUBMAN_CMD\\n    SUBMAN_CMD = module.get_bin_path('subscription-manager', True)\\n    if state == 'present':\\n        if rhsm.is_registered and not force_register:\\n            if pool != '^$' or pool_ids:\\n                try:\\n                    if pool_ids:\\n                        result = rhsm.update_subscriptions_by_pool_ids(pool_ids)\\n                    else:\\n                        result = rhsm.update_subscriptions(pool)\\n                except Exception as e:\\n                    module.fail_json(msg=\"Failed to update subscriptions for '%s': %s\" % (server_hostname, to_native(e)))\\n                else:\\n                    module.exit_json(**result)\\n            else:\\n                module.exit_json(changed=False, msg=\"System already registered.\")\\n        else:\\n            try:\\n                rhsm.enable()\\n                rhsm.configure(**module.params)\\n                rhsm.register(username, password, auto_attach, activationkey, org_id,\\n                              consumer_type, consumer_name, consumer_id, force_register,\\n                              environment, rhsm_baseurl, server_insecure, server_hostname,\\n                              server_proxy_hostname, server_proxy_port, server_proxy_user, server_proxy_password, release)\\n                if pool_ids:\\n                    subscribed_pool_ids = rhsm.subscribe_by_pool_ids(pool_ids)\\n                elif pool != '^$':\\n                    subscribed_pool_ids = rhsm.subscribe(pool)\\n                else:\\n                    subscribed_pool_ids = []\\n            except Exception as e:\\n                module.fail_json(msg=\"Failed to register with '%s': %s\" % (server_hostname, to_native(e)))\\n            else:\\n                module.exit_json(changed=True,\\n                                 msg=\"System successfully registered to '%s'.\" % server_hostname,\\n                                 subscribed_pool_ids=subscribed_pool_ids)\\n    if state == 'absent':\\n        if not rhsm.is_registered:\\n            module.exit_json(changed=False, msg=\"System already unregistered.\")\\n        else:\\n            try:\\n                rhsm.unsubscribe()\\n                rhsm.unregister()\\n            except Exception as e:\\n                module.fail_json(msg=\"Failed to unregister: %s\" % to_native(e))\\n            else:\\n                module.exit_json(changed=True, msg=\"System successfully unregistered from %s.\" % server_hostname)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Added support for syspurpose to redhat_subscribtion module (#59850)",
    "fixed_code": "def main():\\n    rhsm = Rhsm(None)\\n    module = AnsibleModule(\\n        argument_spec={\\n            'state': {'default': 'present', 'choices': ['present', 'absent']},\\n            'username': {},\\n            'password': {'no_log': True},\\n            'server_hostname': {},\\n            'server_insecure': {},\\n            'rhsm_baseurl': {},\\n            'rhsm_repo_ca_cert': {},\\n            'auto_attach': {'aliases': ['autosubscribe'], 'type': 'bool'},\\n            'activationkey': {'no_log': True},\\n            'org_id': {},\\n            'environment': {},\\n            'pool': {'default': '^$'},\\n            'pool_ids': {'default': [], 'type': 'list'},\\n            'consumer_type': {},\\n            'consumer_name': {},\\n            'consumer_id': {},\\n            'force_register': {'default': False, 'type': 'bool'},\\n            'server_proxy_hostname': {},\\n            'server_proxy_port': {},\\n            'server_proxy_user': {},\\n            'server_proxy_password': {'no_log': True},\\n            'release': {},\\n            'syspurpose': {\\n                'type': 'dict',\\n                'options': {\\n                    'role': {},\\n                    'usage': {},\\n                    'service_level_agreement': {},\\n                    'addons': {'type': 'list'},\\n                    'sync': {'type': 'bool', 'default': False}\\n                }\\n            }\\n        },\\n        required_together=[['username', 'password'],\\n                           ['server_proxy_hostname', 'server_proxy_port'],\\n                           ['server_proxy_user', 'server_proxy_password']],\\n        mutually_exclusive=[['activationkey', 'username'],\\n                            ['activationkey', 'consumer_id'],\\n                            ['activationkey', 'environment'],\\n                            ['activationkey', 'autosubscribe'],\\n                            ['force', 'consumer_id'],\\n                            ['pool', 'pool_ids']],\\n        required_if=[['state', 'present', ['username', 'activationkey'], True]],\\n    )\\n    rhsm.module = module\\n    state = module.params['state']\\n    username = module.params['username']\\n    password = module.params['password']\\n    server_hostname = module.params['server_hostname']\\n    server_insecure = module.params['server_insecure']\\n    rhsm_baseurl = module.params['rhsm_baseurl']\\n    rhsm_repo_ca_cert = module.params['rhsm_repo_ca_cert']\\n    auto_attach = module.params['auto_attach']\\n    activationkey = module.params['activationkey']\\n    org_id = module.params['org_id']\\n    if activationkey and not org_id:\\n        module.fail_json(msg='org_id is required when using activationkey')\\n    environment = module.params['environment']\\n    pool = module.params['pool']\\n    pool_ids = {}\\n    for value in module.params['pool_ids']:\\n        if isinstance(value, dict):\\n            if len(value) != 1:\\n                module.fail_json(msg='Unable to parse pool_ids option.')\\n            pool_id, quantity = list(value.items())[0]\\n        else:\\n            pool_id, quantity = value, 1\\n        pool_ids[pool_id] = str(quantity)\\n    consumer_type = module.params[\"consumer_type\"]\\n    consumer_name = module.params[\"consumer_name\"]\\n    consumer_id = module.params[\"consumer_id\"]\\n    force_register = module.params[\"force_register\"]\\n    server_proxy_hostname = module.params['server_proxy_hostname']\\n    server_proxy_port = module.params['server_proxy_port']\\n    server_proxy_user = module.params['server_proxy_user']\\n    server_proxy_password = module.params['server_proxy_password']\\n    release = module.params['release']\\n    syspurpose = module.params['syspurpose']\\n    global SUBMAN_CMD\\n    SUBMAN_CMD = module.get_bin_path('subscription-manager', True)\\n    syspurpose_changed = False\\n    if syspurpose is not None:\\n        try:\\n            syspurpose_changed = SysPurpose().update_syspurpose(syspurpose)\\n        except Exception as err:\\n            module.fail_json(msg=\"Failed to update syspurpose attributes: %s\" % to_native(err))\\n    if state == 'present':\\n        if rhsm.is_registered and not force_register:\\n            if syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:\\n                try:\\n                    rhsm.sync_syspurpose()\\n                except Exception as e:\\n                    module.fail_json(msg=\"Failed to synchronize syspurpose attributes: %s\" % to_native(e))\\n            if pool != '^$' or pool_ids:\\n                try:\\n                    if pool_ids:\\n                        result = rhsm.update_subscriptions_by_pool_ids(pool_ids)\\n                    else:\\n                        result = rhsm.update_subscriptions(pool)\\n                except Exception as e:\\n                    module.fail_json(msg=\"Failed to update subscriptions for '%s': %s\" % (server_hostname, to_native(e)))\\n                else:\\n                    module.exit_json(**result)\\n            else:\\n                if syspurpose_changed is True:\\n                    module.exit_json(changed=True, msg=\"Syspurpose attributes changed.\")\\n                else:\\n                    module.exit_json(changed=False, msg=\"System already registered.\")\\n        else:\\n            try:\\n                rhsm.enable()\\n                rhsm.configure(**module.params)\\n                rhsm.register(username, password, auto_attach, activationkey, org_id,\\n                              consumer_type, consumer_name, consumer_id, force_register,\\n                              environment, rhsm_baseurl, server_insecure, server_hostname,\\n                              server_proxy_hostname, server_proxy_port, server_proxy_user, server_proxy_password, release)\\n                if syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:\\n                    rhsm.sync_syspurpose()\\n                if pool_ids:\\n                    subscribed_pool_ids = rhsm.subscribe_by_pool_ids(pool_ids)\\n                elif pool != '^$':\\n                    subscribed_pool_ids = rhsm.subscribe(pool)\\n                else:\\n                    subscribed_pool_ids = []\\n            except Exception as e:\\n                module.fail_json(msg=\"Failed to register with '%s': %s\" % (server_hostname, to_native(e)))\\n            else:\\n                module.exit_json(changed=True,\\n                                 msg=\"System successfully registered to '%s'.\" % server_hostname,\\n                                 subscribed_pool_ids=subscribed_pool_ids)\\n    if state == 'absent':\\n        if not rhsm.is_registered:\\n            module.exit_json(changed=False, msg=\"System already unregistered.\")\\n        else:\\n            try:\\n                rhsm.unsubscribe()\\n                rhsm.unregister()\\n            except Exception as e:\\n                module.fail_json(msg=\"Failed to unregister: %s\" % to_native(e))\\n            else:\\n                module.exit_json(changed=True, msg=\"System successfully unregistered from %s.\" % server_hostname)"
  },
  {
    "code": "def http_error_302(self, req, fp, code, msg, headers):\\n        if \"location\" in headers:\\n            newurl = headers[\"location\"]\\n        elif \"uri\" in headers:\\n            newurl = headers[\"uri\"]\\n        else:\\n            return\\n        urlparts = urlparse(newurl)\\n        if not urlparts.path:\\n            urlparts = list(urlparts)\\n            urlparts[2] = \"/\"\\n        newurl = urlunparse(urlparts)\\n        newurl = urljoin(req.full_url, newurl)\\n        new = self.redirect_request(req, fp, code, msg, headers, newurl)\\n        if new is None:\\n            return\\n        if hasattr(req, 'redirect_dict'):\\n            visited = new.redirect_dict = req.redirect_dict\\n            if (visited.get(newurl, 0) >= self.max_repeats or\\n                len(visited) >= self.max_redirections):\\n                raise HTTPError(req.full_url, code,\\n                                self.inf_msg + msg, headers, fp)\\n        else:\\n            visited = new.redirect_dict = req.redirect_dict = {}\\n        visited[newurl] = visited.get(newurl, 0) + 1\\n        fp.read()\\n        fp.close()\\n        return self.parent.open(new)\\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\\n    inf_msg = \"The HTTP server returned a redirect error that would \" \\\\n              \"lead to an infinite loop.\\n\" \\\\n              \"The last 30x error message was:\\n\"",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix for issue5102, timeout value propages between redirects, proxy, digest and auth handlers. Fixed tests to reflect the same.",
    "fixed_code": "def http_error_302(self, req, fp, code, msg, headers):\\n        if \"location\" in headers:\\n            newurl = headers[\"location\"]\\n        elif \"uri\" in headers:\\n            newurl = headers[\"uri\"]\\n        else:\\n            return\\n        urlparts = urlparse(newurl)\\n        if not urlparts.path:\\n            urlparts = list(urlparts)\\n            urlparts[2] = \"/\"\\n        newurl = urlunparse(urlparts)\\n        newurl = urljoin(req.full_url, newurl)\\n        new = self.redirect_request(req, fp, code, msg, headers, newurl)\\n        if new is None:\\n            return\\n        if hasattr(req, 'redirect_dict'):\\n            visited = new.redirect_dict = req.redirect_dict\\n            if (visited.get(newurl, 0) >= self.max_repeats or\\n                len(visited) >= self.max_redirections):\\n                raise HTTPError(req.full_url, code,\\n                                self.inf_msg + msg, headers, fp)\\n        else:\\n            visited = new.redirect_dict = req.redirect_dict = {}\\n        visited[newurl] = visited.get(newurl, 0) + 1\\n        fp.read()\\n        fp.close()\\n        return self.parent.open(new, timeout=req.timeout)\\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\\n    inf_msg = \"The HTTP server returned a redirect error that would \" \\\\n              \"lead to an infinite loop.\\n\" \\\\n              \"The last 30x error message was:\\n\""
  },
  {
    "code": "def rolling_corr(arg1, arg2, window, min_periods=None, time_rule=None):\\n    num = rolling_cov(arg1, arg2, window, min_periods, time_rule)\\n    den  = (rolling_std(arg1, window, min_periods, time_rule) *\\n            rolling_std(arg2, window, min_periods, time_rule))\\n    return num / den",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "refactoring EW moments to have uniform interface, unit test coverage",
    "fixed_code": "def rolling_corr(arg1, arg2, window, min_periods=None, time_rule=None):\\n    X, Y = _prep_binary(arg1, arg2)\\n    num = rolling_cov(X, Y, window, min_periods, time_rule)\\n    den  = (rolling_std(X, window, min_periods, time_rule) *\\n            rolling_std(Y, window, min_periods, time_rule))\\n    return num / den"
  },
  {
    "code": "def _code_to_hash_pyc(code, source_hash, checked=True):\\n    \"Produce the data for a hash-based pyc.\"\\n    data = bytearray(MAGIC_NUMBER)\\n    flags = 0b1 | checked << 1\\n    data.extend(_pack_uint32(flags))\\n    assert len(source_hash) == 8\\n    data.extend(source_hash)\\n    data.extend(marshal.dumps(code))\\n    return data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _code_to_hash_pyc(code, source_hash, checked=True):\\n    \"Produce the data for a hash-based pyc.\"\\n    data = bytearray(MAGIC_NUMBER)\\n    flags = 0b1 | checked << 1\\n    data.extend(_pack_uint32(flags))\\n    assert len(source_hash) == 8\\n    data.extend(source_hash)\\n    data.extend(marshal.dumps(code))\\n    return data"
  },
  {
    "code": "def __truediv__(self, other):\\n\\t\\tif isinstance(other, self.__class__):\\n\\t\\t\\treturn self.standard / other.standard\\n\\t\\tif isinstance(other, NUMERIC_TYPES):\\n\\t\\t\\treturn self.__class__(default_unit=self._default_unit,\\n\\t\\t\\t\\t**{self.STANDARD_UNIT: (self.standard / other)})\\n\\t\\telse:\\n\\t\\t\\traise TypeError('%(class)s must be divided with number or %(class)s' % {\"class\": pretty_name(self)})",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __truediv__(self, other):\\n\\t\\tif isinstance(other, self.__class__):\\n\\t\\t\\treturn self.standard / other.standard\\n\\t\\tif isinstance(other, NUMERIC_TYPES):\\n\\t\\t\\treturn self.__class__(default_unit=self._default_unit,\\n\\t\\t\\t\\t**{self.STANDARD_UNIT: (self.standard / other)})\\n\\t\\telse:\\n\\t\\t\\traise TypeError('%(class)s must be divided with number or %(class)s' % {\"class\": pretty_name(self)})"
  },
  {
    "code": "def process_list_block(docstring, starting_point, section_end,\\n\\t\\t\\t\\t\\t   leading_spaces, marker):\\n\\tending_point = docstring.find('\\n', starting_point)\\n\\tblock = docstring[starting_point:(None if ending_point == -1 else\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  ending_point - 1)]\\n\\tdocstring_slice = docstring[starting_point:section_end].replace(block, marker)\\n\\tdocstring = (docstring[:starting_point]\\n\\t\\t\\t\\t + docstring_slice\\n\\t\\t\\t\\t + docstring[section_end:])\\n\\tlines = block.split('\\n')\\n\\tlines = [re.sub('^' + ' ' * leading_spaces, '', line) for line in lines]\\n\\ttop_level_regex = r'^\\t([^\\s\\\\\\(]+):(.*)'\\n\\ttop_level_replacement = r'- __\\1__:\\2'\\n\\tlines = [re.sub(top_level_regex, top_level_replacement, line) for line in lines]\\n\\tlines = [re.sub(r'^\\t', '', line) for line in lines]\\n\\tindent = 0\\n\\ttext_block = False\\n\\tfor i in range(len(lines)):\\n\\t\\tline = lines[i]\\n\\t\\tspaces = re.search(r'\\S', line)\\n\\t\\tif spaces:\\n\\t\\t\\tif line[spaces.start()] == '-':\\n\\t\\t\\t\\tindent = spaces.start() + 1\\n\\t\\t\\t\\tif text_block:\\n\\t\\t\\t\\t\\ttext_block = False\\n\\t\\t\\t\\t\\tlines[i] = '\\n' + line\\n\\t\\t\\telif spaces.start() < indent:\\n\\t\\t\\t\\ttext_block = True\\n\\t\\t\\t\\tindent = spaces.start()\\n\\t\\t\\t\\tlines[i] = '\\n' + line\\n\\t\\telse:\\n\\t\\t\\ttext_block = False\\n\\t\\t\\tindent = 0\\n\\tblock = '\\n'.join(lines)\\n\\treturn docstring, block",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix Arguments display in Docs (#12007)",
    "fixed_code": "def process_list_block(docstring, starting_point, section_end,\\n\\t\\t\\t\\t\\t   leading_spaces, marker):\\n\\tending_point = docstring.find('\\n', starting_point)\\n\\tblock = docstring[starting_point:(ending_point - 1 if ending_point > -1 else\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  section_end)]\\n\\tdocstring_slice = docstring[starting_point:section_end].replace(block, marker)\\n\\tdocstring = (docstring[:starting_point]\\n\\t\\t\\t\\t + docstring_slice\\n\\t\\t\\t\\t + docstring[section_end:])\\n\\tlines = block.split('\\n')\\n\\tlines = [re.sub('^' + ' ' * leading_spaces, '', line) for line in lines]\\n\\ttop_level_regex = r'^\\t([^\\s\\\\\\(]+):(.*)'\\n\\ttop_level_replacement = r'- __\\1__:\\2'\\n\\tlines = [re.sub(top_level_regex, top_level_replacement, line) for line in lines]\\n\\tlines = [re.sub(r'^\\t', '', line) for line in lines]\\n\\tindent = 0\\n\\ttext_block = False\\n\\tfor i in range(len(lines)):\\n\\t\\tline = lines[i]\\n\\t\\tspaces = re.search(r'\\S', line)\\n\\t\\tif spaces:\\n\\t\\t\\tif line[spaces.start()] == '-':\\n\\t\\t\\t\\tindent = spaces.start() + 1\\n\\t\\t\\t\\tif text_block:\\n\\t\\t\\t\\t\\ttext_block = False\\n\\t\\t\\t\\t\\tlines[i] = '\\n' + line\\n\\t\\t\\telif spaces.start() < indent:\\n\\t\\t\\t\\ttext_block = True\\n\\t\\t\\t\\tindent = spaces.start()\\n\\t\\t\\t\\tlines[i] = '\\n' + line\\n\\t\\telse:\\n\\t\\t\\ttext_block = False\\n\\t\\t\\tindent = 0\\n\\tblock = '\\n'.join(lines)\\n\\treturn docstring, block"
  },
  {
    "code": "def strftime(self, date_format) -> Index:\\n        arr = self._data.strftime(date_format)\\n        return Index(arr, name=self.name, dtype=object)\\n    @doc(DatetimeArray.tz_convert)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def strftime(self, date_format) -> Index:\\n        arr = self._data.strftime(date_format)\\n        return Index(arr, name=self.name, dtype=object)\\n    @doc(DatetimeArray.tz_convert)"
  },
  {
    "code": "def __delitem__(self, key):\\n        if key == DEFAULTSECT:\\n            raise ValueError(\"Cannot remove the default section.\")\\n        if not self.has_section(key):\\n            raise KeyError(key)\\n        self.remove_section(key)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "configparser: the name of the DEFAULT section is now customizable",
    "fixed_code": "def __delitem__(self, key):\\n        if key == self._default_section:\\n            raise ValueError(\"Cannot remove the default section.\")\\n        if not self.has_section(key):\\n            raise KeyError(key)\\n        self.remove_section(key)"
  },
  {
    "code": "def authenticate(request=None, **credentials):\\n    for backend, backend_path in _get_backends(return_tuples=True):\\n        backend_signature = inspect.signature(backend.authenticate)\\n        try:\\n            backend_signature.bind(request, **credentials)\\n        except TypeError:\\n            continue\\n        try:\\n            user = backend.authenticate(request, **credentials)\\n        except PermissionDenied:\\n            break\\n        if user is None:\\n            continue\\n        user.backend = backend_path\\n        return user\\n    user_login_failed.send(sender=__name__, credentials=_clean_credentials(credentials), request=request)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def authenticate(request=None, **credentials):\\n    for backend, backend_path in _get_backends(return_tuples=True):\\n        backend_signature = inspect.signature(backend.authenticate)\\n        try:\\n            backend_signature.bind(request, **credentials)\\n        except TypeError:\\n            continue\\n        try:\\n            user = backend.authenticate(request, **credentials)\\n        except PermissionDenied:\\n            break\\n        if user is None:\\n            continue\\n        user.backend = backend_path\\n        return user\\n    user_login_failed.send(sender=__name__, credentials=_clean_credentials(credentials), request=request)"
  },
  {
    "code": "def as_sql(self, compiler, connection):\\n\\t\\tlhs, params = compiler.compile(self.lhs)\\n\\t\\treturn \"(%s -> '%s')\" % (lhs, self.key_name), params",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[1.11.x] Fixed CVE-2019-14234 -- Protected JSONField/HStoreField key and index lookups against SQL injection.\\n\\nThanks to Sage M. Abdullah for the report and initial patch.\\nThanks Florian Apolloner for reviews.",
    "fixed_code": "def as_sql(self, compiler, connection):\\n\\t\\tlhs, params = compiler.compile(self.lhs)\\n\\t\\treturn '(%s -> %%s)' % lhs, [self.key_name] + params"
  },
  {
    "code": "def _isna_old(obj):\\n    if is_scalar(obj):\\n        return libmissing.checknull_old(obj)\\n    elif isinstance(obj, ABCMultiIndex):\\n        raise NotImplementedError(\"isna is not defined for MultiIndex\")\\n    elif isinstance(obj, type):\\n        return False\\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\\n        return _isna_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj._constructor(obj._data.isna(func=_isna_old))\\n    elif isinstance(obj, list):\\n        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\\n    elif hasattr(obj, \"__array__\"):\\n        return _isna_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _isna_old(obj):\\n    if is_scalar(obj):\\n        return libmissing.checknull_old(obj)\\n    elif isinstance(obj, ABCMultiIndex):\\n        raise NotImplementedError(\"isna is not defined for MultiIndex\")\\n    elif isinstance(obj, type):\\n        return False\\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\\n        return _isna_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj._constructor(obj._data.isna(func=_isna_old))\\n    elif isinstance(obj, list):\\n        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\\n    elif hasattr(obj, \"__array__\"):\\n        return _isna_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None"
  },
  {
    "code": "def _has_bool_dtype(x):\\n    try:\\n        return x.dtype == bool\\n    except AttributeError:\\n        try:\\n            return 'bool' in x.dtypes\\n        except AttributeError:\\n            return isinstance(x, (bool, np.bool_))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": ".ne fails if comparing a list of columns containing column name 'dtype' #22383 (#22416)",
    "fixed_code": "def _has_bool_dtype(x):\\n    try:\\n        if isinstance(x, ABCDataFrame):\\n            return 'bool' in x.dtypes\\n        else:\\n            return x.dtype == bool\\n    except AttributeError:\\n        return isinstance(x, (bool, np.bool_))"
  },
  {
    "code": "def state_absent(module, existing, proposed, candidate):\\n    commands = []\\n    parents = ['interface {0}'.format(module.params['interface'].capitalize())]\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in existing_commands.items():\\n        if 'ip ospf bfd' in key:\\n            if 'default' not in value:\\n                commands.append('no ip ospf bfd')\\n            continue\\n        if 'ip ospf passive-interface' in key and value is not None:\\n            commands.append('default ip ospf passive-interface')\\n            continue\\n        if value:\\n            if key.startswith('ip ospf message-digest-key'):\\n                if 'options' not in key:\\n                    if existing['message_digest_encryption_type'] == '3des':\\n                        encryption_type = '3'\\n                    elif existing['message_digest_encryption_type'] == 'cisco_type_7':\\n                        encryption_type = '7'\\n                    command = 'no {0} {1} {2} {3} {4}'.format(\\n                        key,\\n                        existing['message_digest_key_id'],\\n                        existing['message_digest_algorithm_type'],\\n                        encryption_type,\\n                        existing['message_digest_password'])\\n                    commands.append(command)\\n            elif key in ['ip ospf authentication message-digest', 'ip ospf network']:\\n                if value:\\n                    commands.append('no {0}'.format(key))\\n            elif key == 'ip router ospf':\\n                command = 'no {0} {1} area {2}'.format(key, proposed['ospf'], proposed['area'])\\n                if command not in commands:\\n                    commands.append(command)\\n            else:\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n    candidate.add(commands, parents=parents)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def state_absent(module, existing, proposed, candidate):\\n    commands = []\\n    parents = ['interface {0}'.format(module.params['interface'].capitalize())]\\n    existing_commands = apply_key_map(PARAM_TO_COMMAND_KEYMAP, existing)\\n    for key, value in existing_commands.items():\\n        if 'ip ospf bfd' in key:\\n            if 'default' not in value:\\n                commands.append('no ip ospf bfd')\\n            continue\\n        if 'ip ospf passive-interface' in key and value is not None:\\n            commands.append('default ip ospf passive-interface')\\n            continue\\n        if value:\\n            if key.startswith('ip ospf message-digest-key'):\\n                if 'options' not in key:\\n                    if existing['message_digest_encryption_type'] == '3des':\\n                        encryption_type = '3'\\n                    elif existing['message_digest_encryption_type'] == 'cisco_type_7':\\n                        encryption_type = '7'\\n                    command = 'no {0} {1} {2} {3} {4}'.format(\\n                        key,\\n                        existing['message_digest_key_id'],\\n                        existing['message_digest_algorithm_type'],\\n                        encryption_type,\\n                        existing['message_digest_password'])\\n                    commands.append(command)\\n            elif key in ['ip ospf authentication message-digest', 'ip ospf network']:\\n                if value:\\n                    commands.append('no {0}'.format(key))\\n            elif key == 'ip router ospf':\\n                command = 'no {0} {1} area {2}'.format(key, proposed['ospf'], proposed['area'])\\n                if command not in commands:\\n                    commands.append(command)\\n            else:\\n                existing_value = existing_commands.get(key)\\n                commands.append('no {0} {1}'.format(key, existing_value))\\n    candidate.add(commands, parents=parents)"
  },
  {
    "code": "def is_dict_like(obj):\\n    for attr in (\"__getitem__\", \"keys\", \"__contains__\"):\\n        if not hasattr(obj, attr):\\n            return False\\n    return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_dict_like(obj):\\n    for attr in (\"__getitem__\", \"keys\", \"__contains__\"):\\n        if not hasattr(obj, attr):\\n            return False\\n    return True"
  },
  {
    "code": "def sigquit_handler(sig, frame):\\n    print(\"Dumping stack traces for all threads in PID {}\".format(os.getpid()))\\n    id_to_name = {th.ident: th.name for th in threading.enumerate()}\\n    code = []\\n    for thread_id, stack in sys._current_frames().items():\\n        code.append(\"\\n                    .format(id_to_name.get(thread_id, \"\"), thread_id))\\n        for filename, line_number, name, line in traceback.extract_stack(stack):\\n            code.append('File: \"{}\", line {}, in {}'\\n                        .format(filename, line_number, name))\\n            if line:\\n                code.append(\"  {}\".format(line.strip()))\\n    print(\"\\n\".join(code))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def sigquit_handler(sig, frame):  \\n    print(\"Dumping stack traces for all threads in PID {}\".format(os.getpid()))\\n    id_to_name = {th.ident: th.name for th in threading.enumerate()}\\n    code = []\\n    for thread_id, stack in sys._current_frames().items():  \\n        code.append(\"\\n                    .format(id_to_name.get(thread_id, \"\"), thread_id))\\n        for filename, line_number, name, line in traceback.extract_stack(stack):\\n            code.append('File: \"{}\", line {}, in {}'\\n                        .format(filename, line_number, name))\\n            if line:\\n                code.append(\"  {}\".format(line.strip()))\\n    print(\"\\n\".join(code))"
  },
  {
    "code": "def conf(self):\\n\\t\\traw = request.args.get(\"raw\") == \"true\"\\n\\t\\ttitle = \"Airflow Configuration\"\\n\\t\\texpose_config = conf.get(\"webserver\", \"expose_config\").lower()\\n\\t\\tif raw:\\n\\t\\t\\tif expose_config == \"non-sensitive-only\":",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use single source of truth for sensitive config items (#31820)\\n\\nPreviously we had them defined both in constant and in config.yml.\\n\\nNow just config.yml\\n\\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "fixed_code": "def conf(self):\\n\\t\\traw = request.args.get(\"raw\") == \"true\"\\n\\t\\ttitle = \"Airflow Configuration\"\\n\\t\\texpose_config = conf.get(\"webserver\", \"expose_config\").lower()\\n\\t\\tif raw:\\n\\t\\t\\tif expose_config == \"non-sensitive-only\":\\n\\t\\t\\t\\tupdater = configupdater.ConfigUpdater()\\n\\t\\t\\t\\tupdater.read(AIRFLOW_CONFIG)\\n\\t\\t\\t\\tfor sect, key in conf.sensitive_config_values:\\n\\t\\t\\t\\t\\tif updater.has_option(sect, key):\\n\\t\\t\\t\\t\\t\\tupdater[sect][key].value = \"< hidden >\"\\n\\t\\t\\t\\tconfig = str(updater)\\n\\t\\t\\telif expose_config in {\"true\", \"t\", \"1\"}:\\n\\t\\t\\t\\twith open(AIRFLOW_CONFIG) as file:\\n\\t\\t\\t\\t\\tconfig = file.read()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tconfig = (\\n\\t\\t\\t\\t\\t\"\\n\\t\\t\\t\\t\\t\"most likely for security reasons.\"\\n\\t\\t\\t\\t)\\n\\t\\t\\treturn Response(\\n\\t\\t\\t\\tresponse=config,\\n\\t\\t\\t\\tstatus=200,\\n\\t\\t\\t\\tmimetype=\"application/text\",\\n\\t\\t\\t\\theaders={\"Deprecation\": \"Endpoint will be removed in Airflow 3.0, use the REST API instead.\"},\\n\\t\\t\\t)\\n\\t\\tif expose_config in {\"non-sensitive-only\", \"true\", \"t\", \"1\"}:\\n\\t\\t\\tdisplay_sensitive = expose_config != \"non-sensitive-only\"\\n\\t\\t\\ttable = [\\n\\t\\t\\t\\t(section, key, str(value), source)\\n\\t\\t\\t\\tfor section, parameters in conf.as_dict(True, display_sensitive).items()\\n\\t\\t\\t\\tfor key, (value, source) in parameters.items()\\n\\t\\t\\t]\\n\\t\\t\\treturn self.render_template(\\n\\t\\t\\t\\ttemplate=\"airflow/config.html\",\\n\\t\\t\\t\\ttitle=title,\\n\\t\\t\\t\\ttable=table,\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\treturn self.render_template(\\n\\t\\t\\t\\t\"airflow/config.html\",\\n\\t\\t\\t\\ttitle=title,\\n\\t\\t\\t\\thide_config_msg=(\\n\\t\\t\\t\\t\\t\"Your Airflow administrator chose not to expose the configuration, \"\\n\\t\\t\\t\\t\\t\"most likely for security reasons.\"\\n\\t\\t\\t\\t),\\n\\t\\t\\t)"
  },
  {
    "code": "def _validate_searchsorted_value(self, value):\\n        if not is_list_like(value):\\n            value = self._validate_scalar(value, True)\\n        else:\\n            value = self._validate_listlike(value)\\n        return self._unbox(value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _validate_searchsorted_value(self, value):\\n        if not is_list_like(value):\\n            value = self._validate_scalar(value, True)\\n        else:\\n            value = self._validate_listlike(value)\\n        return self._unbox(value)"
  },
  {
    "code": "def collapse_addresses(addresses):\\n    i = 0\\n    addrs = []\\n    ips = []\\n    nets = []\\n    for ip in addresses:\\n        if isinstance(ip, _BaseAddress):\\n            if ips and ips[-1]._version != ip._version:\\n                raise TypeError(\"%s and %s are not of the same version\" % (\\n                                 ip, ips[-1]))\\n            ips.append(ip)\\n        elif ip._prefixlen == ip._max_prefixlen:\\n            if ips and ips[-1]._version != ip._version:\\n                raise TypeError(\"%s and %s are not of the same version\" % (\\n                                 ip, ips[-1]))\\n            try:\\n                ips.append(ip.ip)\\n            except AttributeError:\\n                ips.append(ip.network_address)\\n        else:\\n            if nets and nets[-1]._version != ip._version:\\n                raise TypeError(\"%s and %s are not of the same version\" % (\\n                                 ip, nets[-1]))\\n            nets.append(ip)\\n    ips = sorted(set(ips))\\n    while i < len(ips):\\n        (first, last) = _find_address_range(ips[i:])\\n        i = ips.index(last) + 1\\n        addrs.extend(summarize_address_range(first, last))\\n    return _collapse_addresses_internal(addrs + nets)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #23266: Much faster implementation of ipaddress.collapse_addresses() when there are many non-consecutive addresses.",
    "fixed_code": "def collapse_addresses(addresses):\\n    i = 0\\n    addrs = []\\n    ips = []\\n    nets = []\\n    for ip in addresses:\\n        if isinstance(ip, _BaseAddress):\\n            if ips and ips[-1]._version != ip._version:\\n                raise TypeError(\"%s and %s are not of the same version\" % (\\n                                 ip, ips[-1]))\\n            ips.append(ip)\\n        elif ip._prefixlen == ip._max_prefixlen:\\n            if ips and ips[-1]._version != ip._version:\\n                raise TypeError(\"%s and %s are not of the same version\" % (\\n                                 ip, ips[-1]))\\n            try:\\n                ips.append(ip.ip)\\n            except AttributeError:\\n                ips.append(ip.network_address)\\n        else:\\n            if nets and nets[-1]._version != ip._version:\\n                raise TypeError(\"%s and %s are not of the same version\" % (\\n                                 ip, nets[-1]))\\n            nets.append(ip)\\n    ips = sorted(ips)\\n    while i < len(ips):\\n        (first, last, items) = _find_address_range(ips[i:])\\n        i = items + i\\n        addrs.extend(summarize_address_range(first, last))\\n    return _collapse_addresses_internal(addrs + nets)"
  },
  {
    "code": "def diff(arr, n, axis=0):\\n    n = int(n)\\n    dtype = arr.dtype\\n    if issubclass(dtype.type, np.integer):\\n        dtype = np.float64\\n    elif issubclass(dtype.type, np.bool_):\\n        dtype = np.object_\\n    out_arr = np.empty(arr.shape, dtype=dtype)\\n    na_indexer = [slice(None)] * arr.ndim\\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n    out_arr[tuple(na_indexer)] = np.nan\\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\\n        f = _diff_special[arr.dtype.name]\\n        f(arr, out_arr, n, axis)\\n    else:\\n        res_indexer = [slice(None)] * arr.ndim\\n        res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n        res_indexer = tuple(res_indexer)\\n        lag_indexer = [slice(None)] * arr.ndim\\n        lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n        lag_indexer = tuple(lag_indexer)\\n        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n    return out_arr",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def diff(arr, n, axis=0):\\n    n = int(n)\\n    dtype = arr.dtype\\n    if issubclass(dtype.type, np.integer):\\n        dtype = np.float64\\n    elif issubclass(dtype.type, np.bool_):\\n        dtype = np.object_\\n    out_arr = np.empty(arr.shape, dtype=dtype)\\n    na_indexer = [slice(None)] * arr.ndim\\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\\n    out_arr[tuple(na_indexer)] = np.nan\\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\\n        f = _diff_special[arr.dtype.name]\\n        f(arr, out_arr, n, axis)\\n    else:\\n        res_indexer = [slice(None)] * arr.ndim\\n        res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\\n        res_indexer = tuple(res_indexer)\\n        lag_indexer = [slice(None)] * arr.ndim\\n        lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\\n        lag_indexer = tuple(lag_indexer)\\n        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\\n    return out_arr"
  },
  {
    "code": "def _background_gradient(\\n        s,\\n        cmap=\"PuBu\",\\n        low: float = 0,\\n        high: float = 0,\\n        text_color_threshold: float = 0.408,\\n        vmin: float | None = None,\\n        vmax: float | None = None,\\n    ):\\n        if (\\n            not isinstance(text_color_threshold, (float, int))\\n            or not 0 <= text_color_threshold <= 1\\n        ):\\n            msg = \"`text_color_threshold` must be a value from 0 to 1.\"\\n            raise ValueError(msg)\\n        with _mpl(Styler.background_gradient) as (plt, colors):\\n            smin = np.nanmin(s.to_numpy()) if vmin is None else vmin\\n            smax = np.nanmax(s.to_numpy()) if vmax is None else vmax\\n            rng = smax - smin\\n            norm = colors.Normalize(smin - (rng * low), smax + (rng * high))\\n            rgbas = plt.cm.get_cmap(cmap)(norm(s.to_numpy(dtype=float)))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add a gradient map to background gradient (#39930)",
    "fixed_code": "def _background_gradient(\\n    data,\\n    cmap=\"PuBu\",\\n    low: float = 0,\\n    high: float = 0,\\n    text_color_threshold: float = 0.408,\\n    vmin: float | None = None,\\n    vmax: float | None = None,\\n    gmap: Sequence | np.ndarray | FrameOrSeries | None = None,\\n):\\n    if gmap is None:  \\n        gmap = data.to_numpy(dtype=float)\\n    else:  \\n        gmap = _validate_apply_axis_arg(gmap, \"gmap\", float, data)\\n    with _mpl(Styler.background_gradient) as (plt, colors):\\n        smin = np.nanmin(gmap) if vmin is None else vmin\\n        smax = np.nanmax(gmap) if vmax is None else vmax\\n        rng = smax - smin\\n        norm = colors.Normalize(smin - (rng * low), smax + (rng * high))\\n        rgbas = plt.cm.get_cmap(cmap)(norm(gmap))"
  },
  {
    "code": "def check_for_file(self, file_path):\\n        try:\\n            files = self.connection.glob(file_path, details=False, invalidate_cache=True)\\n            return len(files) == 1\\n        except FileNotFoundError:\\n            return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6890] AzureDataLakeHook: Move DB call out of __init__ (#7513)",
    "fixed_code": "def check_for_file(self, file_path):\\n        try:\\n            files = self.get_conn().glob(file_path, details=False, invalidate_cache=True)\\n            return len(files) == 1\\n        except FileNotFoundError:\\n            return False"
  },
  {
    "code": "def datetime_extract_sql(self, lookup_type, field_name, tzname):\\n\\t\\treturn \"django_datetime_extract('%s', %s, %s, %s)\" % (\\n\\t\\t\\tlookup_type.lower(),\\n\\t\\t\\tfield_name,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Refs CVE-2022-34265 -- Properly escaped Extract() and Trunc() parameters.",
    "fixed_code": "def datetime_extract_sql(self, lookup_type, sql, params, tzname):\\n\\t\\treturn f\"django_datetime_extract(%s, {sql}, %s, %s)\", (\\n\\t\\t\\tlookup_type.lower(),\\n\\t\\t\\t*params,\\n\\t\\t\\t*self._convert_tznames_to_sql(tzname),\\n\\t\\t)"
  },
  {
    "code": "def run_hook(hook_name, project_dir, context):\\n\\tscript = find_hooks().get(hook_name)\\n\\tif script is None:\\n\\t\\tlogging.debug('No hooks found')\\n\\t\\treturn EXIT_SUCCESS\\n\\treturn run_script_with_context(script, project_dir, context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "using custom exception instead of exit status and cleanup project dir",
    "fixed_code": "def run_hook(hook_name, project_dir, context):\\n\\tscript = find_hooks().get(hook_name)\\n\\tif script is None:\\n\\t\\tlogging.debug('No hooks found')\\n\\t\\treturn\\n\\trun_script_with_context(script, project_dir, context)"
  },
  {
    "code": "def verify(self, address):\\n        self.putcmd(\"vrfy\", _addr_only(address))\\n        return self.getreply()\\n    vrfy = verify",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def verify(self, address):\\n        self.putcmd(\"vrfy\", _addr_only(address))\\n        return self.getreply()\\n    vrfy = verify"
  },
  {
    "code": "def _convert_datetimes(sas_datetimes: pd.Series, unit: str) -> pd.Series:\\n    try:\\n        return pd.to_datetime(sas_datetimes, unit=unit, origin=\"1960-01-01\")\\n    except OutOfBoundsDatetime:\\n        s_series = sas_datetimes.apply(_parse_datetime, unit=unit)\\n        s_series = cast(pd.Series, s_series)\\n        return s_series",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_datetimes(sas_datetimes: pd.Series, unit: str) -> pd.Series:\\n    try:\\n        return pd.to_datetime(sas_datetimes, unit=unit, origin=\"1960-01-01\")\\n    except OutOfBoundsDatetime:\\n        s_series = sas_datetimes.apply(_parse_datetime, unit=unit)\\n        s_series = cast(pd.Series, s_series)\\n        return s_series"
  },
  {
    "code": "def dump(self):\\n        for item in self.contents.values():\\n            yield from item.dump(indent=0)\\n@dataclasses.dataclass\\nclass ABIItem:\\n    kind: str\\n    name: str\\n    added: str = None\\n    contents: list = dataclasses.field(default_factory=list)\\n    abi_only: bool = False\\n    ifdef: str = None\\n    struct_abi_kind: str = None\\n    members: list = None\\n    doc: str = None\\n    windows: bool = False\\n    KINDS = frozenset({\\n        'struct', 'function', 'macro', 'data', 'const', 'typedef', 'ifdef',\\n    })\\n    def dump(self, indent=0):\\n        yield f\"{'    ' * indent}{self.kind} {self.name}\"\\n        if self.added:\\n            yield f\"{'    ' * (indent+1)}added {self.added}\"\\n        if self.ifdef:\\n            yield f\"{'    ' * (indent+1)}ifdef {self.ifdef}\"\\n        if self.abi_only:\\n            yield f\"{'    ' * (indent+1)}abi_only\"",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-91324: Convert the stable ABI manifest to TOML (GH-92026)",
    "fixed_code": "def dump(self):\\n        for item in self.contents.values():\\n            fields = dataclasses.fields(item)\\n            yield f\"[{item.kind}.{item.name}]\"\\n            for field in fields:\\n                if field.name in {'name', 'value', 'kind'}:\\n                    continue\\n                value = getattr(item, field.name)\\n                if value == field.default:\\n                    pass\\n                elif value is True:\\n                    yield f\"    {field.name} = true\"\\n                elif value:\\n                    yield f\"    {field.name} = {value!r}\""
  },
  {
    "code": "def pad_2d(values, limit=None, mask=None):\\n    if is_float_dtype(values):\\n        _method = _algos.pad_2d_inplace_float64\\n    elif is_datetime64_dtype(values):\\n        _method = _pad_2d_datetime\\n    elif values.dtype == np.object_:\\n        _method = _algos.pad_2d_inplace_object\\n    else: \\n        raise ValueError('Invalid dtype for padding')\\n    if mask is None:\\n        mask = isnull(values)\\n    mask = mask.view(np.uint8)\\n    _method(values, mask, limit=limit)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pad_2d(values, limit=None, mask=None):\\n    if is_float_dtype(values):\\n        _method = _algos.pad_2d_inplace_float64\\n    elif is_datetime64_dtype(values):\\n        _method = _pad_2d_datetime\\n    elif values.dtype == np.object_:\\n        _method = _algos.pad_2d_inplace_object\\n    else: \\n        raise ValueError('Invalid dtype for padding')\\n    if mask is None:\\n        mask = isnull(values)\\n    mask = mask.view(np.uint8)\\n    _method(values, mask, limit=limit)"
  },
  {
    "code": "def score(self, X, y, **fit_params):\\n        check_is_fitted(self)\\n        return self.estimator_.score(self.transform(X), y, **fit_params)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def score(self, X, y, **fit_params):\\n        check_is_fitted(self)\\n        return self.estimator_.score(self.transform(X), y, **fit_params)"
  },
  {
    "code": "def __neg__(self):\\n        return self.__class__(-self.n, **self.kwds)\\n    @apply_wraps",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: offset normalize option may not work in addition/subtraction",
    "fixed_code": "def __neg__(self):\\n        return self.__class__(-self.n, normalize=self.normalize, **self.kwds)\\n    @apply_wraps"
  },
  {
    "code": "def password_reset_confirm(request, uidb64=None, token=None,\\n\\t\\t\\t\\t\\t\\t   template_name='registration/password_reset_confirm.html',\\n\\t\\t\\t\\t\\t\\t   token_generator=default_token_generator,\\n\\t\\t\\t\\t\\t\\t   set_password_form=SetPasswordForm,\\n\\t\\t\\t\\t\\t\\t   post_reset_redirect=None,\\n\\t\\t\\t\\t\\t\\t   current_app=None, extra_context=None):\\n\\tUserModel = get_user_model()\\n\\tassert uidb64 is not None and token is not None  \\n\\tif post_reset_redirect is None:\\n\\t\\tpost_reset_redirect = reverse('password_reset_complete')\\n\\telse:\\n\\t\\tpost_reset_redirect = resolve_url(post_reset_redirect)\\n\\ttry:\\n\\t\\tuid = force_text(urlsafe_base64_decode(uidb64))\\n\\t\\tuser = UserModel._default_manager.get(pk=uid)\\n\\texcept (TypeError, ValueError, OverflowError, UserModel.DoesNotExist):\\n\\t\\tuser = None\\n\\tif user is not None and token_generator.check_token(user, token):\\n\\t\\tvalidlink = True\\n\\t\\ttitle = _('Enter new password')\\n\\t\\tif request.method == 'POST':\\n\\t\\t\\tform = set_password_form(user, request.POST)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\tform.save()\\n\\t\\t\\t\\treturn HttpResponseRedirect(post_reset_redirect)\\n\\t\\telse:\\n\\t\\t\\tform = set_password_form(user)\\n\\telse:\\n\\t\\tvalidlink = False\\n\\t\\tform = None\\n\\t\\ttitle = _('Password reset unsuccessful')\\n\\tcontext = {\\n\\t\\t'form': form,\\n\\t\\t'title': title,\\n\\t\\t'validlink': validlink,\\n\\t}\\n\\tif extra_context is not None:\\n\\t\\tcontext.update(extra_context)\\n\\tif current_app is not None:\\n\\t\\trequest.current_app = current_app\\n\\treturn TemplateResponse(request, template_name, context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def password_reset_confirm(request, uidb64=None, token=None,\\n\\t\\t\\t\\t\\t\\t   template_name='registration/password_reset_confirm.html',\\n\\t\\t\\t\\t\\t\\t   token_generator=default_token_generator,\\n\\t\\t\\t\\t\\t\\t   set_password_form=SetPasswordForm,\\n\\t\\t\\t\\t\\t\\t   post_reset_redirect=None,\\n\\t\\t\\t\\t\\t\\t   current_app=None, extra_context=None):\\n\\tUserModel = get_user_model()\\n\\tassert uidb64 is not None and token is not None  \\n\\tif post_reset_redirect is None:\\n\\t\\tpost_reset_redirect = reverse('password_reset_complete')\\n\\telse:\\n\\t\\tpost_reset_redirect = resolve_url(post_reset_redirect)\\n\\ttry:\\n\\t\\tuid = force_text(urlsafe_base64_decode(uidb64))\\n\\t\\tuser = UserModel._default_manager.get(pk=uid)\\n\\texcept (TypeError, ValueError, OverflowError, UserModel.DoesNotExist):\\n\\t\\tuser = None\\n\\tif user is not None and token_generator.check_token(user, token):\\n\\t\\tvalidlink = True\\n\\t\\ttitle = _('Enter new password')\\n\\t\\tif request.method == 'POST':\\n\\t\\t\\tform = set_password_form(user, request.POST)\\n\\t\\t\\tif form.is_valid():\\n\\t\\t\\t\\tform.save()\\n\\t\\t\\t\\treturn HttpResponseRedirect(post_reset_redirect)\\n\\t\\telse:\\n\\t\\t\\tform = set_password_form(user)\\n\\telse:\\n\\t\\tvalidlink = False\\n\\t\\tform = None\\n\\t\\ttitle = _('Password reset unsuccessful')\\n\\tcontext = {\\n\\t\\t'form': form,\\n\\t\\t'title': title,\\n\\t\\t'validlink': validlink,\\n\\t}\\n\\tif extra_context is not None:\\n\\t\\tcontext.update(extra_context)\\n\\tif current_app is not None:\\n\\t\\trequest.current_app = current_app\\n\\treturn TemplateResponse(request, template_name, context)"
  },
  {
    "code": "def __pos__(self, context=None):\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n        sign = self._sign\\n        if not self:\\n            sign = 0\\n        if context is None:\\n            context = getcontext()\\n        if context._rounding_decision == ALWAYS_ROUND:\\n            ans = self._fix(context)\\n        else:\\n            ans = Decimal(self)\\n        ans._sign = sign\\n        return ans",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def __pos__(self, context=None):\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n        if not self:\\n            ans = self.copy_abs()\\n        else:\\n            ans = Decimal(self)\\n        if context is None:\\n            context = getcontext()\\n        return ans._fix(context)"
  },
  {
    "code": "def __eq__(self, other):\\n        return self.path == other.path",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __eq__(self, other):\\n        return self.path == other.path"
  },
  {
    "code": "def cmd_pasv(self, arg):\\n\\t\\twith socket.create_server((self.socket.getsockname()[0], 0)) as sock:\\n\\t\\t\\tsock.settimeout(TIMEOUT)\\n\\t\\t\\tip, port = sock.getsockname()[:2]\\n\\t\\t\\tip = ip.replace('.', ','); p1 = port / 256; p2 = port % 256\\n\\t\\t\\tself.push('227 entering passive mode (%s,%d,%d)' %(ip, p1, p2))\\n\\t\\t\\tconn, addr = sock.accept()\\n\\t\\t\\tself.dtp = self.dtp_handler(conn, baseclass=self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-43285 Make ftplib not trust the PASV response. (GH-24838)\\n\\nbpo-43285: Make ftplib not trust the PASV response.\\n\\nThe IPv4 address value returned from the server in response to the PASV command\\nshould not be trusted.  This prevents a malicious FTP server from using the\\nresponse to probe IPv4 address and port combinations on the client network.\\n\\nInstead of using the returned address, we use the IP address we're\\nalready connected to.  This is the strategy other ftp clients adopted,\\nand matches the only strategy available for the modern IPv6 EPSV command\\nwhere the server response must return a port number and nothing else.\\n\\nFor the rare user who _wants_ this ugly behavior, set a `trust_server_pasv_ipv4_address`\\nattribute on your `ftplib.FTP` instance to True.",
    "fixed_code": "def cmd_pasv(self, arg):\\n\\t\\twith socket.create_server((self.socket.getsockname()[0], 0)) as sock:\\n\\t\\t\\tsock.settimeout(TIMEOUT)\\n\\t\\t\\tport = sock.getsockname()[1]\\n\\t\\t\\tip = self.fake_pasv_server_ip\\n\\t\\t\\tip = ip.replace('.', ','); p1 = port / 256; p2 = port % 256\\n\\t\\t\\tself.push('227 entering passive mode (%s,%d,%d)' %(ip, p1, p2))\\n\\t\\t\\tconn, addr = sock.accept()\\n\\t\\t\\tself.dtp = self.dtp_handler(conn, baseclass=self)"
  },
  {
    "code": "def maybe_cast_to_datetime(value, dtype, errors='raise'):\\n    from pandas.core.tools.timedeltas import to_timedelta\\n    from pandas.core.tools.datetimes import to_datetime\\n    if dtype is not None:\\n        if isinstance(dtype, string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_datetime64tz = is_datetime64tz_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_datetime64tz or is_timedelta64:\\n            msg = (\"The '{dtype}' dtype has no unit. \"\\n                   \"Please pass in '{dtype}[ns]' instead.\")\\n            if is_datetime64 and not is_dtype_equal(dtype, _NS_DTYPE):\\n                if dtype.name in ('datetime64', 'datetime64[ns]'):\\n                    if dtype.name == 'datetime64':\\n                        raise ValueError(msg.format(dtype=dtype.name))\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\"cannot convert datetimelike to \"\\n                                    \"dtype [{dtype}]\".format(dtype=dtype))\\n            elif is_datetime64tz:\\n                if is_scalar(value) and isna(value):\\n                    value = [value]\\n            elif is_timedelta64 and not is_dtype_equal(dtype, _TD_DTYPE):\\n                if dtype.name in ('timedelta64', 'timedelta64[ns]'):\\n                    if dtype.name == 'timedelta64':\\n                        raise ValueError(msg.format(dtype=dtype.name))\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\"cannot convert timedeltalike to \"\\n                                    \"dtype [{dtype}]\".format(dtype=dtype))\\n            if is_scalar(value):\\n                if value == iNaT or isna(value):\\n                    value = iNaT\\n            else:\\n                value = np.array(value, copy=False)\\n                if value.ndim == 0:\\n                    value = iNaT\\n                elif np.prod(value.shape) or not is_dtype_equal(value.dtype,\\n                                                                dtype):\\n                    try:\\n                        if is_datetime64:\\n                            value = to_datetime(value, errors=errors)._values\\n                        elif is_datetime64tz:\\n                            is_dt_string = is_string_dtype(value)\\n                            value = to_datetime(value, errors=errors)\\n                            if is_dt_string:\\n                                value = value.tz_localize(dtype.tz)\\n                            else:\\n                                value = (value.tz_localize('UTC')\\n                                         .tz_convert(dtype.tz))\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, errors=errors)._values\\n                    except (AttributeError, ValueError, TypeError):\\n                        pass\\n        elif is_datetime64_dtype(value) and not is_datetime64_dtype(dtype):\\n            if is_object_dtype(dtype):\\n                if value.dtype != _NS_DTYPE:\\n                    value = value.astype(_NS_DTYPE)\\n                ints = np.asarray(value).view('i8')\\n                return tslib.ints_to_pydatetime(ints)\\n            raise TypeError('Cannot cast datetime64 to {dtype}'\\n                            .format(dtype=dtype))\\n    else:\\n        is_array = isinstance(value, np.ndarray)\\n        if is_array and value.dtype.kind in ['M', 'm']:\\n            dtype = value.dtype\\n            if dtype.kind == 'M' and dtype != _NS_DTYPE:\\n                value = value.astype(_NS_DTYPE)\\n            elif dtype.kind == 'm' and dtype != _TD_DTYPE:\\n                value = to_timedelta(value)\\n        elif not (is_array and not (issubclass(value.dtype.type, np.integer) or\\n                                    value.dtype == np.object_)):\\n            value = maybe_infer_to_datetimelike(value)\\n    return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_cast_to_datetime(value, dtype, errors='raise'):\\n    from pandas.core.tools.timedeltas import to_timedelta\\n    from pandas.core.tools.datetimes import to_datetime\\n    if dtype is not None:\\n        if isinstance(dtype, string_types):\\n            dtype = np.dtype(dtype)\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_datetime64tz = is_datetime64tz_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n        if is_datetime64 or is_datetime64tz or is_timedelta64:\\n            msg = (\"The '{dtype}' dtype has no unit. \"\\n                   \"Please pass in '{dtype}[ns]' instead.\")\\n            if is_datetime64 and not is_dtype_equal(dtype, _NS_DTYPE):\\n                if dtype.name in ('datetime64', 'datetime64[ns]'):\\n                    if dtype.name == 'datetime64':\\n                        raise ValueError(msg.format(dtype=dtype.name))\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\"cannot convert datetimelike to \"\\n                                    \"dtype [{dtype}]\".format(dtype=dtype))\\n            elif is_datetime64tz:\\n                if is_scalar(value) and isna(value):\\n                    value = [value]\\n            elif is_timedelta64 and not is_dtype_equal(dtype, _TD_DTYPE):\\n                if dtype.name in ('timedelta64', 'timedelta64[ns]'):\\n                    if dtype.name == 'timedelta64':\\n                        raise ValueError(msg.format(dtype=dtype.name))\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\"cannot convert timedeltalike to \"\\n                                    \"dtype [{dtype}]\".format(dtype=dtype))\\n            if is_scalar(value):\\n                if value == iNaT or isna(value):\\n                    value = iNaT\\n            else:\\n                value = np.array(value, copy=False)\\n                if value.ndim == 0:\\n                    value = iNaT\\n                elif np.prod(value.shape) or not is_dtype_equal(value.dtype,\\n                                                                dtype):\\n                    try:\\n                        if is_datetime64:\\n                            value = to_datetime(value, errors=errors)._values\\n                        elif is_datetime64tz:\\n                            is_dt_string = is_string_dtype(value)\\n                            value = to_datetime(value, errors=errors)\\n                            if is_dt_string:\\n                                value = value.tz_localize(dtype.tz)\\n                            else:\\n                                value = (value.tz_localize('UTC')\\n                                         .tz_convert(dtype.tz))\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, errors=errors)._values\\n                    except (AttributeError, ValueError, TypeError):\\n                        pass\\n        elif is_datetime64_dtype(value) and not is_datetime64_dtype(dtype):\\n            if is_object_dtype(dtype):\\n                if value.dtype != _NS_DTYPE:\\n                    value = value.astype(_NS_DTYPE)\\n                ints = np.asarray(value).view('i8')\\n                return tslib.ints_to_pydatetime(ints)\\n            raise TypeError('Cannot cast datetime64 to {dtype}'\\n                            .format(dtype=dtype))\\n    else:\\n        is_array = isinstance(value, np.ndarray)\\n        if is_array and value.dtype.kind in ['M', 'm']:\\n            dtype = value.dtype\\n            if dtype.kind == 'M' and dtype != _NS_DTYPE:\\n                value = value.astype(_NS_DTYPE)\\n            elif dtype.kind == 'm' and dtype != _TD_DTYPE:\\n                value = to_timedelta(value)\\n        elif not (is_array and not (issubclass(value.dtype.type, np.integer) or\\n                                    value.dtype == np.object_)):\\n            value = maybe_infer_to_datetimelike(value)\\n    return value"
  },
  {
    "code": "def parse_manifest(file):\\n    manifest = Manifest()\\n    data = tomllib.load(file)\\n    for kind, itemclass in itemclasses.items():\\n        for name, item_data in data[kind].items():\\n            try:\\n                item = itemclass(name=name, kind=kind, **item_data)\\n                manifest.add(item)\\n            except BaseException as exc:\\n                exc.add_note(f'in {kind} {name}')\\n                raise\\n    return manifest\\ngenerators = []",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_manifest(file):\\n    manifest = Manifest()\\n    data = tomllib.load(file)\\n    for kind, itemclass in itemclasses.items():\\n        for name, item_data in data[kind].items():\\n            try:\\n                item = itemclass(name=name, kind=kind, **item_data)\\n                manifest.add(item)\\n            except BaseException as exc:\\n                exc.add_note(f'in {kind} {name}')\\n                raise\\n    return manifest\\ngenerators = []"
  },
  {
    "code": "def signature(obj):\\n    ''\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = signature(obj.__func__)\\n        return _signature_bound_method(sig)\\n    obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        wrapped_sig = signature(partialmethod.func)\\n        sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n        first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n        new_params = (first_wrapped_param,) + tuple(sig.parameters.values())\\n        return sig.replace(parameters=new_params)\\n    if _signature_is_builtin(obj):\\n        return Signature.from_builtin(obj)\\n    if isinstance(obj, types.FunctionType):\\n        return Signature.from_function(obj)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = signature(obj.func)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n        else:\\n            new = _get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = signature(new)\\n            else:\\n                init = _get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = signature(init)\\n        if sig is None:\\n            if type in obj.__mro__:\\n                return signature(type)\\n            else:\\n                return signature(object)\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n    if sig is not None:\\n        return _signature_bound_method(sig)\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def signature(obj):\\n    ''\\n    if not callable(obj):\\n        raise TypeError('{!r} is not a callable object'.format(obj))\\n    if isinstance(obj, types.MethodType):\\n        sig = signature(obj.__func__)\\n        return _signature_bound_method(sig)\\n    obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\\n    try:\\n        sig = obj.__signature__\\n    except AttributeError:\\n        pass\\n    else:\\n        if sig is not None:\\n            return sig\\n    try:\\n        partialmethod = obj._partialmethod\\n    except AttributeError:\\n        pass\\n    else:\\n        wrapped_sig = signature(partialmethod.func)\\n        sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\\n        first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\\n        new_params = (first_wrapped_param,) + tuple(sig.parameters.values())\\n        return sig.replace(parameters=new_params)\\n    if _signature_is_builtin(obj):\\n        return Signature.from_builtin(obj)\\n    if isinstance(obj, types.FunctionType):\\n        return Signature.from_function(obj)\\n    if isinstance(obj, functools.partial):\\n        wrapped_sig = signature(obj.func)\\n        return _signature_get_partial(wrapped_sig, obj)\\n    sig = None\\n    if isinstance(obj, type):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n        else:\\n            new = _get_user_defined_method(obj, '__new__')\\n            if new is not None:\\n                sig = signature(new)\\n            else:\\n                init = _get_user_defined_method(obj, '__init__')\\n                if init is not None:\\n                    sig = signature(init)\\n        if sig is None:\\n            if type in obj.__mro__:\\n                return signature(type)\\n            else:\\n                return signature(object)\\n    elif not isinstance(obj, _NonUserDefinedCallables):\\n        call = _get_user_defined_method(type(obj), '__call__')\\n        if call is not None:\\n            sig = signature(call)\\n    if sig is not None:\\n        return _signature_bound_method(sig)\\n    if isinstance(obj, types.BuiltinFunctionType):\\n        msg = 'no signature found for builtin function {!r}'.format(obj)\\n        raise ValueError(msg)\\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))"
  },
  {
    "code": "def _convert_datetimes(sas_datetimes: pd.Series, unit: str) -> pd.Series:\\n    try:\\n        return pd.to_datetime(sas_datetimes, unit=unit, origin=\"1960-01-01\")\\n    except OutOfBoundsDatetime:\\n        if unit == \"s\":\\n            s_series = sas_datetimes.apply(\\n                lambda sas_float: datetime(1960, 1, 1) + timedelta(seconds=sas_float)\\n            )\\n            s_series = cast(pd.Series, s_series)\\n            return s_series\\n        elif unit == \"d\":\\n            d_series = sas_datetimes.apply(\\n                lambda sas_float: datetime(1960, 1, 1) + timedelta(days=sas_float)\\n            )\\n            d_series = cast(pd.Series, d_series)\\n            return d_series\\n        else:\\n            raise ValueError(\"unit must be 'd' or 's'\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "SAS validate null dates (#39726)",
    "fixed_code": "def _convert_datetimes(sas_datetimes: pd.Series, unit: str) -> pd.Series:\\n    try:\\n        return pd.to_datetime(sas_datetimes, unit=unit, origin=\"1960-01-01\")\\n    except OutOfBoundsDatetime:\\n        s_series = sas_datetimes.apply(_parse_datetime, unit=unit)\\n        s_series = cast(pd.Series, s_series)\\n        return s_series"
  },
  {
    "code": "def sequence_to_td64ns(data, copy=False, unit=None, errors=\"raise\"):\\n    inferred_freq = None\\n    if unit is not None:\\n        unit = parse_timedelta_unit(unit)\\n    if not hasattr(data, \"dtype\"):\\n        if np.ndim(data) == 0:\\n            data = list(data)\\n        data = np.array(data, copy=False)\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    elif isinstance(data, (ABCTimedeltaIndex, TimedeltaArray)):\\n        inferred_freq = data.freq\\n        data = data._data\\n    elif isinstance(data, IntegerArray):\\n        data = data.to_numpy(\"int64\", na_value=tslibs.iNaT)\\n    if is_object_dtype(data.dtype) or is_string_dtype(data.dtype):\\n        data = objects_to_td64ns(data, unit=unit, errors=errors)\\n        copy = False\\n    elif is_integer_dtype(data.dtype):\\n        data, copy_made = ints_to_td64ns(data, unit=unit)\\n        copy = copy and not copy_made\\n    elif is_float_dtype(data.dtype):\\n        mask = np.isnan(data)\\n        m, p = precision_from_unit(unit or \"ns\")\\n        base = data.astype(np.int64)\\n        frac = data - base\\n        if p:\\n            frac = np.round(frac, p)\\n        data = (base * m + (frac * m).astype(np.int64)).view(\"timedelta64[ns]\")\\n        data[mask] = iNaT\\n        copy = False\\n    elif is_timedelta64_dtype(data.dtype):\\n        if data.dtype != TD64NS_DTYPE:\\n            data = data.astype(TD64NS_DTYPE)\\n            copy = False\\n    else:\\n        raise TypeError(f\"dtype {data.dtype} cannot be converted to timedelta64[ns]\")\\n    data = np.array(data, copy=copy)\\n    assert data.dtype == \"m8[ns]\", data\\n    return data, inferred_freq",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/API: tighter checks on DTI/TDI.equals (#36962)",
    "fixed_code": "def sequence_to_td64ns(data, copy=False, unit=None, errors=\"raise\"):\\n    inferred_freq = None\\n    if unit is not None:\\n        unit = parse_timedelta_unit(unit)\\n    if not hasattr(data, \"dtype\"):\\n        if np.ndim(data) == 0:\\n            data = list(data)\\n        data = np.array(data, copy=False)\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    elif isinstance(data, (ABCTimedeltaIndex, TimedeltaArray)):\\n        inferred_freq = data.freq\\n        data = data._data\\n    elif isinstance(data, IntegerArray):\\n        data = data.to_numpy(\"int64\", na_value=tslibs.iNaT)\\n    elif is_categorical_dtype(data.dtype):\\n        data = data.categories.take(data.codes, fill_value=NaT)._values\\n        copy = False\\n    if is_object_dtype(data.dtype) or is_string_dtype(data.dtype):\\n        data = objects_to_td64ns(data, unit=unit, errors=errors)\\n        copy = False\\n    elif is_integer_dtype(data.dtype):\\n        data, copy_made = ints_to_td64ns(data, unit=unit)\\n        copy = copy and not copy_made\\n    elif is_float_dtype(data.dtype):\\n        mask = np.isnan(data)\\n        m, p = precision_from_unit(unit or \"ns\")\\n        base = data.astype(np.int64)\\n        frac = data - base\\n        if p:\\n            frac = np.round(frac, p)\\n        data = (base * m + (frac * m).astype(np.int64)).view(\"timedelta64[ns]\")\\n        data[mask] = iNaT\\n        copy = False\\n    elif is_timedelta64_dtype(data.dtype):\\n        if data.dtype != TD64NS_DTYPE:\\n            data = data.astype(TD64NS_DTYPE)\\n            copy = False\\n    else:\\n        raise TypeError(f\"dtype {data.dtype} cannot be converted to timedelta64[ns]\")\\n    data = np.array(data, copy=copy)\\n    assert data.dtype == \"m8[ns]\", data\\n    return data, inferred_freq"
  },
  {
    "code": "def __add__(self, other):\\n        if isinstance(other, Index):\\n            return self.union(other)\\n        elif isinstance(other, (DateOffset, timedelta)):\\n            new_values = self.astype('O') + other\\n            return DatetimeIndex(new_values, tz=self.tz)\\n        else:\\n            return Index(self.view(np.ndarray) + other)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __add__(self, other):\\n        if isinstance(other, Index):\\n            return self.union(other)\\n        elif isinstance(other, (DateOffset, timedelta)):\\n            new_values = self.astype('O') + other\\n            return DatetimeIndex(new_values, tz=self.tz)\\n        else:\\n            return Index(self.view(np.ndarray) + other)"
  },
  {
    "code": "def _round_half_up(self, prec, expdiff, context, tmp = None):\\n        if tmp is None:\\n            tmp = Decimal( (self._sign,self._int[:prec], self._exp - expdiff))\\n        if self._int[prec] >= 5:\\n            tmp = tmp._increment(round=0, context=context)\\n            if len(tmp._int) > prec:\\n                return Decimal( (tmp._sign, tmp._int[:-1], tmp._exp + 1))\\n        return tmp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _round_half_up(self, prec):\\n        if self._int[prec] in '56789':\\n            return 1\\n        elif _all_zeros(self._int, prec):\\n            return 0\\n        else:\\n            return -1"
  },
  {
    "code": "def get_ordering_field_columns(self):\\n\\t\\tordering = self._get_default_ordering()\\n\\t\\tordering_fields = OrderedDict()\\n\\t\\tif ORDER_VAR not in self.params:\\n\\t\\t\\tfor field in ordering:\\n\\t\\t\\t\\tif isinstance(field, (Combinable, OrderBy)):\\n\\t\\t\\t\\t\\tif not isinstance(field, OrderBy):\\n\\t\\t\\t\\t\\t\\tfield = field.asc()\\n\\t\\t\\t\\t\\tif isinstance(field.expression, F):\\n\\t\\t\\t\\t\\t\\torder_type = 'desc' if field.descending else 'asc'\\n\\t\\t\\t\\t\\t\\tfield = field.expression.name\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\telif field.startswith('-'):\\n\\t\\t\\t\\t\\tfield = field[1:]\\n\\t\\t\\t\\t\\torder_type = 'desc'\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\torder_type = 'asc'\\n\\t\\t\\t\\tfor index, attr in enumerate(self.list_display):\\n\\t\\t\\t\\t\\tif self.get_ordering_field(attr) == field:\\n\\t\\t\\t\\t\\t\\tordering_fields[index] = order_type\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\tfor p in self.params[ORDER_VAR].split('.'):\\n\\t\\t\\t\\tnone, pfx, idx = p.rpartition('-')\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tidx = int(idx)\\n\\t\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\t\\tcontinue  \\n\\t\\t\\t\\tordering_fields[idx] = 'desc' if pfx == '-' else 'asc'\\n\\t\\treturn ordering_fields",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_ordering_field_columns(self):\\n\\t\\tordering = self._get_default_ordering()\\n\\t\\tordering_fields = OrderedDict()\\n\\t\\tif ORDER_VAR not in self.params:\\n\\t\\t\\tfor field in ordering:\\n\\t\\t\\t\\tif isinstance(field, (Combinable, OrderBy)):\\n\\t\\t\\t\\t\\tif not isinstance(field, OrderBy):\\n\\t\\t\\t\\t\\t\\tfield = field.asc()\\n\\t\\t\\t\\t\\tif isinstance(field.expression, F):\\n\\t\\t\\t\\t\\t\\torder_type = 'desc' if field.descending else 'asc'\\n\\t\\t\\t\\t\\t\\tfield = field.expression.name\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\telif field.startswith('-'):\\n\\t\\t\\t\\t\\tfield = field[1:]\\n\\t\\t\\t\\t\\torder_type = 'desc'\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\torder_type = 'asc'\\n\\t\\t\\t\\tfor index, attr in enumerate(self.list_display):\\n\\t\\t\\t\\t\\tif self.get_ordering_field(attr) == field:\\n\\t\\t\\t\\t\\t\\tordering_fields[index] = order_type\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\tfor p in self.params[ORDER_VAR].split('.'):\\n\\t\\t\\t\\tnone, pfx, idx = p.rpartition('-')\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tidx = int(idx)\\n\\t\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\t\\tcontinue  \\n\\t\\t\\t\\tordering_fields[idx] = 'desc' if pfx == '-' else 'asc'\\n\\t\\treturn ordering_fields"
  },
  {
    "code": "def get_value(self, series, key):\\n        try:\\n            return Index.get_value(self, series, key)\\n        except KeyError:\\n            try:\\n                loc = self._get_string_slice(key)\\n                return series[loc]\\n            except (TypeError, ValueError, KeyError):\\n                pass\\n            if isinstance(key, time):\\n                locs = self.indexer_at_time(key)\\n                return series.take(locs)\\n            try:\\n                if isinstance(key, basestring):\\n                    stamp = Timestamp(key, tz=self.tz)\\n                else:\\n                    stamp = Timestamp(key)\\n                return self._engine.get_value(series, stamp)\\n            except KeyError:\\n                raise KeyError(stamp)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_value(self, series, key):\\n        try:\\n            return Index.get_value(self, series, key)\\n        except KeyError:\\n            try:\\n                loc = self._get_string_slice(key)\\n                return series[loc]\\n            except (TypeError, ValueError, KeyError):\\n                pass\\n            if isinstance(key, time):\\n                locs = self.indexer_at_time(key)\\n                return series.take(locs)\\n            try:\\n                if isinstance(key, basestring):\\n                    stamp = Timestamp(key, tz=self.tz)\\n                else:\\n                    stamp = Timestamp(key)\\n                return self._engine.get_value(series, stamp)\\n            except KeyError:\\n                raise KeyError(stamp)"
  },
  {
    "code": "def _is_indexed_like(obj, other):\\n    if isinstance(obj, Series):\\n        if not isinstance(other, Series):\\n            return False\\n        return obj.index.equals(other.index)\\n    elif isinstance(obj, DataFrame):\\n        if not isinstance(other, DataFrame):\\n            return False\\n        return obj._indexed_same(other)\\n    return False\\nclass Grouping(object):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _is_indexed_like(obj, other):\\n    if isinstance(obj, Series):\\n        if not isinstance(other, Series):\\n            return False\\n        return obj.index.equals(other.index)\\n    elif isinstance(obj, DataFrame):\\n        if not isinstance(other, DataFrame):\\n            return False\\n        return obj._indexed_same(other)\\n    return False\\nclass Grouping(object):"
  },
  {
    "code": "def get_validation_errors(outfile, app=None):\\n    from django.db import models, connection\\n    from django.db.models.loading import get_app_errors\\n    from django.db.models.fields.related import RelatedObject\\n    from django.db.models.deletion import SET_NULL, SET_DEFAULT\\n    e = ModelErrorCollection(outfile)\\n    for (app_name, error) in get_app_errors().items():\\n        e.add(app_name, error)\\n    inc = set(models.get_models(app, include_swapped=True))\\n    no_inc = set(models.get_models(app))\\n    for cls in models.get_models(app, include_swapped=True):\\n        opts = cls._meta\\n        if opts.swapped:\\n            try:\\n                app_label, model_name = opts.swapped.split('.')\\n            except ValueError:\\n                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\\n                continue\\n            if not models.get_model(app_label, model_name):\\n                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\\n            continue\\n        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\\n            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\\n                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\\n        for f in opts.local_fields:\\n            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\\n                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\\n            if f.name.endswith('_'):\\n                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\\n            if (f.primary_key and f.null and\\n                    not connection.features.interprets_empty_strings_as_nulls):\\n                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\\n            if isinstance(f, models.CharField):\\n                try:\\n                    max_length = int(f.max_length)\\n                    if max_length <= 0:\\n                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n                except (ValueError, TypeError):\\n                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n            if isinstance(f, models.DecimalField):\\n                decimalp_ok, mdigits_ok = False, False\\n                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\\n                try:\\n                    decimal_places = int(f.decimal_places)\\n                    if decimal_places < 0:\\n                        e.add(opts, decimalp_msg % f.name)\\n                    else:\\n                        decimalp_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, decimalp_msg % f.name)\\n                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\\n                try:\\n                    max_digits = int(f.max_digits)\\n                    if max_digits <= 0:\\n                        e.add(opts,  mdigits_msg % f.name)\\n                    else:\\n                        mdigits_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, mdigits_msg % f.name)\\n                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\\n                if decimalp_ok and mdigits_ok:\\n                    if decimal_places > max_digits:\\n                        e.add(opts, invalid_values_msg % f.name)\\n            if isinstance(f, models.FileField) and not f.upload_to:\\n                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\\n            if isinstance(f, models.ImageField):\\n                try:\\n                    from PIL import Image\\n                except ImportError:\\n                    try:\\n                        import Image\\n                    except ImportError:\\n                        e.add(opts, '\"%s\": To use ImageFields, you need to install the Python Imaging Library. Get it at http://www.pythonware.com/products/pil/ .' % f.name)\\n            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\\n                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\\n            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\\n                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\\n            if f.choices:\\n                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\\n                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\\n                else:\\n                    for c in f.choices:\\n                        if not isinstance(c, (list, tuple)) or len(c) != 2:\\n                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-tuples.' % f.name)\\n            if f.db_index not in (None, True, False):\\n                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\\n            connection.validation.validate_field(e, opts, f)\\n            if f.rel and hasattr(f.rel, 'on_delete'):\\n                if f.rel.on_delete == SET_NULL and not f.null:\\n                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\\n                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\\n                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\\n            if f.rel:\\n                if f.rel.to not in models.get_models():\\n                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                    else:\\n                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n                if not f.rel.to._meta.get_field(f.rel.field_name).unique:\\n                    e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.rel.field_name, f.rel.to.__name__))\\n                rel_opts = f.rel.to._meta\\n                rel_name = RelatedObject(f.rel.to, cls, f).get_accessor_name()\\n                rel_query_name = f.related_query_name()\\n                if not f.rel.is_hidden():\\n                    for r in rel_opts.fields:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.local_many_to_many:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.get_all_related_many_to_many_objects():\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    for r in rel_opts.get_all_related_objects():\\n                        if r.field is not f:\\n                            if r.get_accessor_name() == rel_name:\\n                                e.add(opts, \"Accessor for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                            if r.get_accessor_name() == rel_query_name:\\n                                e.add(opts, \"Reverse query name for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        seen_intermediary_signatures = []\\n        for i, f in enumerate(opts.local_many_to_many):\\n            if f.rel.to not in models.get_models():\\n                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                else:\\n                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n            if f.unique:\\n                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\\n            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\\n                from_model, to_model = cls, f.rel.to\\n                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\\n                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\\n                seen_from, seen_to, seen_self = False, False, 0\\n                for inter_field in f.rel.through._meta.fields:\\n                    rel_to = getattr(inter_field.rel, 'to', None)\\n                    if from_model == to_model:  \\n                        if rel_to == from_model:\\n                            seen_self += 1\\n                        if seen_self > 2:\\n                            e.add(opts, \"Intermediary model %s has more than \"\\n                                \"two foreign keys to %s, which is ambiguous \"\\n                                \"and is not permitted.\" % (\\n                                    f.rel.through._meta.object_name,\\n                                    from_model._meta.object_name\\n                                )\\n                            )\\n                    else:\\n                        if rel_to == from_model:\\n                            if seen_from:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                         from_model._meta.object_name\\n                                     )\\n                                 )\\n                            else:\\n                                seen_from = True\\n                        elif rel_to == to_model:\\n                            if seen_to:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                        rel_to._meta.object_name\\n                                    )\\n                                )\\n                            else:\\n                                seen_to = True\\n                if f.rel.through not in models.get_models(include_auto_created=True):\\n                    e.add(opts, \"'%s' specifies an m2m relation through model \"\\n                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\\n                    )\\n                signature = (f.rel.to, cls, f.rel.through)\\n                if signature in seen_intermediary_signatures:\\n                    e.add(opts, \"The model %s has two manually-defined m2m \"\\n                        \"relations through the model %s, which is not \"\\n                        \"permitted. Please consider using an extra field on \"\\n                        \"your intermediary model instead.\" % (\\n                            cls._meta.object_name,\\n                            f.rel.through._meta.object_name\\n                        )\\n                    )\\n                else:\\n                    seen_intermediary_signatures.append(signature)\\n                if not f.rel.through._meta.auto_created:\\n                    seen_related_fk, seen_this_fk = False, False\\n                    for field in f.rel.through._meta.fields:\\n                        if field.rel:\\n                            if not seen_related_fk and field.rel.to == f.rel.to:\\n                                seen_related_fk = True\\n                            elif field.rel.to == cls:\\n                                seen_this_fk = True\\n                    if not seen_related_fk or not seen_this_fk:\\n                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\\n                            \"through model %s, which does not have foreign keys \"\\n                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\\n                                f.rel.to._meta.object_name, cls._meta.object_name)\\n                        )\\n            elif isinstance(f.rel.through, six.string_types):\\n                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\\n                    \"which has not been installed\" % (f.name, f.rel.through)\\n                )\\n            rel_opts = f.rel.to._meta\\n            rel_name = RelatedObject(f.rel.to, cls, f).get_accessor_name()\\n            rel_query_name = f.related_query_name()\\n            if rel_name is not None:\\n                for r in rel_opts.fields:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.local_many_to_many:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.get_all_related_many_to_many_objects():\\n                    if r.field is not f:\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                for r in rel_opts.get_all_related_objects():\\n                    if r.get_accessor_name() == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    if r.get_accessor_name() == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        if opts.ordering:\\n            for field_name in opts.ordering:\\n                if field_name == '?':\\n                    continue\\n                if field_name.startswith('-'):\\n                    field_name = field_name[1:]\\n                if opts.order_with_respect_to and field_name == '_order':\\n                    continue\\n                if '__' in field_name:\\n                    continue\\n                if field_name == 'pk':\\n                    continue\\n                try:\\n                    opts.get_field(field_name, many_to_many=False)\\n                except models.FieldDoesNotExist:\\n                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\\n        for ut in opts.unique_together:\\n            validate_local_fields(e, opts, \"unique_together\", ut)\\n        if not isinstance(opts.index_together, collections.Sequence):\\n            e.add(opts, '\"index_together\" must a sequence')\\n        else:\\n            for it in opts.index_together:\\n                validate_local_fields(e, opts, \"index_together\", it)\\n    return len(e.errors)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_validation_errors(outfile, app=None):\\n    from django.db import models, connection\\n    from django.db.models.loading import get_app_errors\\n    from django.db.models.fields.related import RelatedObject\\n    from django.db.models.deletion import SET_NULL, SET_DEFAULT\\n    e = ModelErrorCollection(outfile)\\n    for (app_name, error) in get_app_errors().items():\\n        e.add(app_name, error)\\n    inc = set(models.get_models(app, include_swapped=True))\\n    no_inc = set(models.get_models(app))\\n    for cls in models.get_models(app, include_swapped=True):\\n        opts = cls._meta\\n        if opts.swapped:\\n            try:\\n                app_label, model_name = opts.swapped.split('.')\\n            except ValueError:\\n                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\\n                continue\\n            if not models.get_model(app_label, model_name):\\n                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\\n            continue\\n        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\\n            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\\n                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\\n        for f in opts.local_fields:\\n            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\\n                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\\n            if f.name.endswith('_'):\\n                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\\n            if (f.primary_key and f.null and\\n                    not connection.features.interprets_empty_strings_as_nulls):\\n                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\\n            if isinstance(f, models.CharField):\\n                try:\\n                    max_length = int(f.max_length)\\n                    if max_length <= 0:\\n                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n                except (ValueError, TypeError):\\n                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\\n            if isinstance(f, models.DecimalField):\\n                decimalp_ok, mdigits_ok = False, False\\n                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\\n                try:\\n                    decimal_places = int(f.decimal_places)\\n                    if decimal_places < 0:\\n                        e.add(opts, decimalp_msg % f.name)\\n                    else:\\n                        decimalp_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, decimalp_msg % f.name)\\n                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\\n                try:\\n                    max_digits = int(f.max_digits)\\n                    if max_digits <= 0:\\n                        e.add(opts,  mdigits_msg % f.name)\\n                    else:\\n                        mdigits_ok = True\\n                except (ValueError, TypeError):\\n                    e.add(opts, mdigits_msg % f.name)\\n                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\\n                if decimalp_ok and mdigits_ok:\\n                    if decimal_places > max_digits:\\n                        e.add(opts, invalid_values_msg % f.name)\\n            if isinstance(f, models.FileField) and not f.upload_to:\\n                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\\n            if isinstance(f, models.ImageField):\\n                try:\\n                    from PIL import Image\\n                except ImportError:\\n                    try:\\n                        import Image\\n                    except ImportError:\\n                        e.add(opts, '\"%s\": To use ImageFields, you need to install the Python Imaging Library. Get it at http://www.pythonware.com/products/pil/ .' % f.name)\\n            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\\n                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\\n            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\\n                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\\n            if f.choices:\\n                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\\n                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\\n                else:\\n                    for c in f.choices:\\n                        if not isinstance(c, (list, tuple)) or len(c) != 2:\\n                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-tuples.' % f.name)\\n            if f.db_index not in (None, True, False):\\n                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\\n            connection.validation.validate_field(e, opts, f)\\n            if f.rel and hasattr(f.rel, 'on_delete'):\\n                if f.rel.on_delete == SET_NULL and not f.null:\\n                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\\n                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\\n                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\\n            if f.rel:\\n                if f.rel.to not in models.get_models():\\n                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                    else:\\n                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n                if not f.rel.to._meta.get_field(f.rel.field_name).unique:\\n                    e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.rel.field_name, f.rel.to.__name__))\\n                rel_opts = f.rel.to._meta\\n                rel_name = RelatedObject(f.rel.to, cls, f).get_accessor_name()\\n                rel_query_name = f.related_query_name()\\n                if not f.rel.is_hidden():\\n                    for r in rel_opts.fields:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.local_many_to_many:\\n                        if r.name == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                        if r.name == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    for r in rel_opts.get_all_related_many_to_many_objects():\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    for r in rel_opts.get_all_related_objects():\\n                        if r.field is not f:\\n                            if r.get_accessor_name() == rel_name:\\n                                e.add(opts, \"Accessor for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                            if r.get_accessor_name() == rel_query_name:\\n                                e.add(opts, \"Reverse query name for field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        seen_intermediary_signatures = []\\n        for i, f in enumerate(opts.local_many_to_many):\\n            if f.rel.to not in models.get_models():\\n                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\\n                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\\n                else:\\n                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\\n                if isinstance(f.rel.to, six.string_types):\\n                    continue\\n            if f.unique:\\n                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\\n            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\\n                from_model, to_model = cls, f.rel.to\\n                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\\n                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\\n                seen_from, seen_to, seen_self = False, False, 0\\n                for inter_field in f.rel.through._meta.fields:\\n                    rel_to = getattr(inter_field.rel, 'to', None)\\n                    if from_model == to_model:  \\n                        if rel_to == from_model:\\n                            seen_self += 1\\n                        if seen_self > 2:\\n                            e.add(opts, \"Intermediary model %s has more than \"\\n                                \"two foreign keys to %s, which is ambiguous \"\\n                                \"and is not permitted.\" % (\\n                                    f.rel.through._meta.object_name,\\n                                    from_model._meta.object_name\\n                                )\\n                            )\\n                    else:\\n                        if rel_to == from_model:\\n                            if seen_from:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                         from_model._meta.object_name\\n                                     )\\n                                 )\\n                            else:\\n                                seen_from = True\\n                        elif rel_to == to_model:\\n                            if seen_to:\\n                                e.add(opts, \"Intermediary model %s has more \"\\n                                    \"than one foreign key to %s, which is \"\\n                                    \"ambiguous and is not permitted.\" % (\\n                                        f.rel.through._meta.object_name,\\n                                        rel_to._meta.object_name\\n                                    )\\n                                )\\n                            else:\\n                                seen_to = True\\n                if f.rel.through not in models.get_models(include_auto_created=True):\\n                    e.add(opts, \"'%s' specifies an m2m relation through model \"\\n                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\\n                    )\\n                signature = (f.rel.to, cls, f.rel.through)\\n                if signature in seen_intermediary_signatures:\\n                    e.add(opts, \"The model %s has two manually-defined m2m \"\\n                        \"relations through the model %s, which is not \"\\n                        \"permitted. Please consider using an extra field on \"\\n                        \"your intermediary model instead.\" % (\\n                            cls._meta.object_name,\\n                            f.rel.through._meta.object_name\\n                        )\\n                    )\\n                else:\\n                    seen_intermediary_signatures.append(signature)\\n                if not f.rel.through._meta.auto_created:\\n                    seen_related_fk, seen_this_fk = False, False\\n                    for field in f.rel.through._meta.fields:\\n                        if field.rel:\\n                            if not seen_related_fk and field.rel.to == f.rel.to:\\n                                seen_related_fk = True\\n                            elif field.rel.to == cls:\\n                                seen_this_fk = True\\n                    if not seen_related_fk or not seen_this_fk:\\n                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\\n                            \"through model %s, which does not have foreign keys \"\\n                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\\n                                f.rel.to._meta.object_name, cls._meta.object_name)\\n                        )\\n            elif isinstance(f.rel.through, six.string_types):\\n                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\\n                    \"which has not been installed\" % (f.name, f.rel.through)\\n                )\\n            rel_opts = f.rel.to._meta\\n            rel_name = RelatedObject(f.rel.to, cls, f).get_accessor_name()\\n            rel_query_name = f.related_query_name()\\n            if rel_name is not None:\\n                for r in rel_opts.fields:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.local_many_to_many:\\n                    if r.name == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                    if r.name == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\\n                for r in rel_opts.get_all_related_many_to_many_objects():\\n                    if r.field is not f:\\n                        if r.get_accessor_name() == rel_name:\\n                            e.add(opts, \"Accessor for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                        if r.get_accessor_name() == rel_query_name:\\n                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with related m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                for r in rel_opts.get_all_related_objects():\\n                    if r.get_accessor_name() == rel_name:\\n                        e.add(opts, \"Accessor for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n                    if r.get_accessor_name() == rel_query_name:\\n                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with related field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.get_accessor_name(), f.name))\\n        if opts.ordering:\\n            for field_name in opts.ordering:\\n                if field_name == '?':\\n                    continue\\n                if field_name.startswith('-'):\\n                    field_name = field_name[1:]\\n                if opts.order_with_respect_to and field_name == '_order':\\n                    continue\\n                if '__' in field_name:\\n                    continue\\n                if field_name == 'pk':\\n                    continue\\n                try:\\n                    opts.get_field(field_name, many_to_many=False)\\n                except models.FieldDoesNotExist:\\n                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\\n        for ut in opts.unique_together:\\n            validate_local_fields(e, opts, \"unique_together\", ut)\\n        if not isinstance(opts.index_together, collections.Sequence):\\n            e.add(opts, '\"index_together\" must a sequence')\\n        else:\\n            for it in opts.index_together:\\n                validate_local_fields(e, opts, \"index_together\", it)\\n    return len(e.errors)"
  },
  {
    "code": "def _try_import():\\n    try:\\n        import feather\\n    except ImportError:\\n        raise ImportError(\"the feather-format library is not installed\\n\"\\n                          \"you can install via conda\\n\"\\n                          \"conda install feather-format -c conda-forge\\n\"\\n                          \"or via pip\\n\"\\n                          \"pip install -U feather-format\\n\")\\n    try:\\n        LooseVersion(feather.__version__) >= LooseVersion('0.3.1')\\n    except AttributeError:\\n        raise ImportError(\"the feather-format library must be >= \"\\n                          \"version 0.3.1\\n\"\\n                          \"you can install via conda\\n\"\\n                          \"conda install feather-format -c conda-forge\"\\n                          \"or via pip\\n\"\\n                          \"pip install -U feather-format\\n\")\\n    return feather",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Deprecate nthreads argument (#23112)\\n\\nThe nthreads argument is no longer supported since pyarrow 0.11.0 and\\nwas replaced with use_threads.\\nHence we deprecate the argument now as well so we can remove it in the\\nfuture.\\n\\nThis commit also:\\n- removes feather-format as a dependency and replaces it with usage of\\n  pyarrow directly.\\n- sets CI dependencies to respect the changes above.\\n\\nWe test backwards compatibility with pyarrow 0.9.0 as conda does not\\nprovide a pyarrow 0.10.0 and the conda-forge version has comatibility\\nissues with the rest of the installed packages.\\n\\nResolves #23053.\\nResolves #21639.",
    "fixed_code": "def _try_import():\\n    try:\\n        import pyarrow\\n        from pyarrow import feather\\n    except ImportError:\\n        raise ImportError(\"pyarrow is not installed\\n\"\\n                          \"you can install via conda\\n\"\\n                          \"conda install pyarrow -c conda-forge\\n\"\\n                          \"or via pip\\n\"\\n                          \"pip install -U pyarrow\\n\")\\n    if LooseVersion(pyarrow.__version__) < LooseVersion('0.4.1'):\\n        raise ImportError(\"pyarrow >= 0.4.1 required for feather support\\n\"\\n                          \"you can install via conda\\n\"\\n                          \"conda install pyarrow -c conda-forge\"\\n                          \"or via pip\\n\"\\n                          \"pip install -U pyarrow\\n\")\\n    return feather, pyarrow"
  },
  {
    "code": "def header_items(self):\\n        return [\\n            (to_native_str(k, errors='replace'),\\n             [to_native_str(x, errors='replace') for x in v])\\n            for k, v in self.request.headers.items()\\n        ]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def header_items(self):\\n        return [\\n            (to_native_str(k, errors='replace'),\\n             [to_native_str(x, errors='replace') for x in v])\\n            for k, v in self.request.headers.items()\\n        ]"
  },
  {
    "code": "def _rename_columns_inplace(self, mapper):\\n        self._data = self._data.rename_items(mapper, copydata=False)\\n        self._clear_caches()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: refactoring and micro-optimizations to support #437, uncovered panel bug",
    "fixed_code": "def _rename_columns_inplace(self, mapper):\\n        self._data = self._data.rename_items(mapper, copydata=False)\\n        self._clear_item_cache()"
  },
  {
    "code": "def read_csv(filepath_or_buffer,\\n             sep=',',\\n             dialect=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             verbose=False,\\n             delimiter=None,\\n             encoding=None,\\n             squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_table_doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_csv(filepath_or_buffer,\\n             sep=',',\\n             dialect=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             verbose=False,\\n             delimiter=None,\\n             encoding=None,\\n             squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_table_doc)"
  },
  {
    "code": "def _resolution(self):\\n        return Resolution.get_reso_from_freq(self.freqstr)\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: make Resolution an enum (#34462)",
    "fixed_code": "def _resolution(self) -> Optional[Resolution]:\\n        try:\\n            return Resolution.get_reso_from_freq(self.freqstr)\\n        except KeyError:\\n            return None\\n    @property"
  },
  {
    "code": "def _open(self):\\n\\t\\ts = self.fp.read(128)\\n\\t\\tif not _accept(s):\\n\\t\\t\\traise SyntaxError(\"not a PCX file\")\\n\\t\\tbbox = i16(s, 4), i16(s, 6), i16(s, 8) + 1, i16(s, 10) + 1\\n\\t\\tif bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:\\n\\t\\t\\traise SyntaxError(\"bad PCX image size\")\\n\\t\\tlogger.debug(\"BBox: %s %s %s %s\", *bbox)\\n\\t\\tversion = s[1]\\n\\t\\tbits = s[3]\\n\\t\\tplanes = s[65]\\n\\t\\tignored_stride = i16(s, 66)\\n\\t\\tlogger.debug(\\n\\t\\t\\t\"PCX version %s, bits %s, planes %s, stride %s\",\\n\\t\\t\\tversion,\\n\\t\\t\\tbits,\\n\\t\\t\\tplanes,\\n\\t\\t\\tignored_stride,\\n\\t\\t)\\n\\t\\tself.info[\"dpi\"] = i16(s, 12), i16(s, 14)\\n\\t\\tif bits == 1 and planes == 1:\\n\\t\\t\\tmode = rawmode = \"1\"\\n\\t\\telif bits == 1 and planes in (2, 4):\\n\\t\\t\\tmode = \"P\"\\n\\t\\t\\trawmode = \"P;%dL\" % planes\\n\\t\\t\\tself.palette = ImagePalette.raw(\"RGB\", s[16:64])\\n\\t\\telif version == 5 and bits == 8 and planes == 1:\\n\\t\\t\\tmode = rawmode = \"L\"\\n\\t\\t\\tself.fp.seek(-769, io.SEEK_END)\\n\\t\\t\\ts = self.fp.read(769)\\n\\t\\t\\tif len(s) == 769 and s[0] == 12:\\n\\t\\t\\t\\tfor i in range(256):\\n\\t\\t\\t\\t\\tif s[i * 3 + 1 : i * 3 + 4] != o8(i) * 3:\\n\\t\\t\\t\\t\\t\\tmode = rawmode = \"P\"\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\tif mode == \"P\":\\n\\t\\t\\t\\t\\tself.palette = ImagePalette.raw(\"RGB\", s[1:])\\n\\t\\t\\tself.fp.seek(128)\\n\\t\\telif version == 5 and bits == 8 and planes == 3:\\n\\t\\t\\tmode = \"RGB\"\\n\\t\\t\\trawmode = \"RGB;L\"\\n\\t\\telse:\\n\\t\\t\\traise OSError(\"unknown PCX mode\")\\n\\t\\tself.mode = mode\\n\\t\\tself._size = bbox[2] - bbox[0], bbox[3] - bbox[1]\\n\\t\\tstride = (self._size[0] * bits + 7) // 8\\n\\t\\tstride += stride % 2\\n\\t\\tbbox = (0, 0) + self.size\\n\\t\\tlogger.debug(\"size: %sx%s\", *self.size)\\n\\t\\tself.tile = [(\"pcx\", bbox, self.fp.tell(), (rawmode, planes * stride))]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _open(self):\\n\\t\\ts = self.fp.read(128)\\n\\t\\tif not _accept(s):\\n\\t\\t\\traise SyntaxError(\"not a PCX file\")\\n\\t\\tbbox = i16(s, 4), i16(s, 6), i16(s, 8) + 1, i16(s, 10) + 1\\n\\t\\tif bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:\\n\\t\\t\\traise SyntaxError(\"bad PCX image size\")\\n\\t\\tlogger.debug(\"BBox: %s %s %s %s\", *bbox)\\n\\t\\tversion = s[1]\\n\\t\\tbits = s[3]\\n\\t\\tplanes = s[65]\\n\\t\\tignored_stride = i16(s, 66)\\n\\t\\tlogger.debug(\\n\\t\\t\\t\"PCX version %s, bits %s, planes %s, stride %s\",\\n\\t\\t\\tversion,\\n\\t\\t\\tbits,\\n\\t\\t\\tplanes,\\n\\t\\t\\tignored_stride,\\n\\t\\t)\\n\\t\\tself.info[\"dpi\"] = i16(s, 12), i16(s, 14)\\n\\t\\tif bits == 1 and planes == 1:\\n\\t\\t\\tmode = rawmode = \"1\"\\n\\t\\telif bits == 1 and planes in (2, 4):\\n\\t\\t\\tmode = \"P\"\\n\\t\\t\\trawmode = \"P;%dL\" % planes\\n\\t\\t\\tself.palette = ImagePalette.raw(\"RGB\", s[16:64])\\n\\t\\telif version == 5 and bits == 8 and planes == 1:\\n\\t\\t\\tmode = rawmode = \"L\"\\n\\t\\t\\tself.fp.seek(-769, io.SEEK_END)\\n\\t\\t\\ts = self.fp.read(769)\\n\\t\\t\\tif len(s) == 769 and s[0] == 12:\\n\\t\\t\\t\\tfor i in range(256):\\n\\t\\t\\t\\t\\tif s[i * 3 + 1 : i * 3 + 4] != o8(i) * 3:\\n\\t\\t\\t\\t\\t\\tmode = rawmode = \"P\"\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\tif mode == \"P\":\\n\\t\\t\\t\\t\\tself.palette = ImagePalette.raw(\"RGB\", s[1:])\\n\\t\\t\\tself.fp.seek(128)\\n\\t\\telif version == 5 and bits == 8 and planes == 3:\\n\\t\\t\\tmode = \"RGB\"\\n\\t\\t\\trawmode = \"RGB;L\"\\n\\t\\telse:\\n\\t\\t\\traise OSError(\"unknown PCX mode\")\\n\\t\\tself.mode = mode\\n\\t\\tself._size = bbox[2] - bbox[0], bbox[3] - bbox[1]\\n\\t\\tstride = (self._size[0] * bits + 7) // 8\\n\\t\\tstride += stride % 2\\n\\t\\tbbox = (0, 0) + self.size\\n\\t\\tlogger.debug(\"size: %sx%s\", *self.size)\\n\\t\\tself.tile = [(\"pcx\", bbox, self.fp.tell(), (rawmode, planes * stride))]"
  },
  {
    "code": "def get_hook(self, *, hook_kwargs=None):\\n        (\\n            hook_class_name,\\n            conn_id_param,\\n            package_name,\\n            hook_name,\\n            connection_type,\\n        ) = ProvidersManager().hooks.get(self.conn_type, (None, None, None, None, None))\\n        if not hook_class_name:\\n            raise AirflowException(f'Unknown hook type \"{self.conn_type}\"')\\n        try:\\n            hook_class = import_string(hook_class_name)\\n        except ImportError:\\n            warnings.warn(\\n                \"Could not import %s when discovering %s %s\", hook_class_name, hook_name, package_name\\n            )\\n            raise\\n        if hook_kwargs is None:\\n            hook_kwargs = {}\\n        return hook_class(**{conn_id_param: self.conn_id}, **hook_kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_hook(self, *, hook_kwargs=None):\\n        (\\n            hook_class_name,\\n            conn_id_param,\\n            package_name,\\n            hook_name,\\n            connection_type,\\n        ) = ProvidersManager().hooks.get(self.conn_type, (None, None, None, None, None))\\n        if not hook_class_name:\\n            raise AirflowException(f'Unknown hook type \"{self.conn_type}\"')\\n        try:\\n            hook_class = import_string(hook_class_name)\\n        except ImportError:\\n            warnings.warn(\\n                \"Could not import %s when discovering %s %s\", hook_class_name, hook_name, package_name\\n            )\\n            raise\\n        if hook_kwargs is None:\\n            hook_kwargs = {}\\n        return hook_class(**{conn_id_param: self.conn_id}, **hook_kwargs)"
  },
  {
    "code": "def load_files(container_path, description=None, categories=None,\\n               load_content=True, shuffle=True, encoding=None,\\n               decode_error='strict', random_state=0):\\n    target = []\\n    target_names = []\\n    filenames = []\\n    folders = [f for f in sorted(listdir(container_path))\\n               if isdir(join(container_path, f))]\\n    if categories is not None:\\n        folders = [f for f in folders if f in categories]\\n    for label, folder in enumerate(folders):\\n        target_names.append(folder)\\n        folder_path = join(container_path, folder)\\n        documents = [join(folder_path, d)\\n                     for d in sorted(listdir(folder_path))]\\n        target.extend(len(documents) * [label])\\n        filenames.extend(documents)\\n    filenames = np.array(filenames)\\n    target = np.array(target)\\n    if shuffle:\\n        random_state = check_random_state(random_state)\\n        indices = np.arange(filenames.shape[0])\\n        random_state.shuffle(indices)\\n        filenames = filenames[indices]\\n        target = target[indices]\\n    if load_content:\\n        data = [open(filename, 'rb').read() for filename in filenames]\\n        if encoding is not None:\\n            data = [d.decode(encoding, decode_error) for d in data]\\n        return Bunch(data=data,\\n                     filenames=filenames,\\n                     target_names=target_names,\\n                     target=target,\\n                     DESCR=description)\\n    return Bunch(filenames=filenames,\\n                 target_names=target_names,\\n                 target=target,\\n                 DESCR=description)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Changed f=open() to with open() as f to eliminate ResourceWarnings. Fixes #3410.",
    "fixed_code": "def load_files(container_path, description=None, categories=None,\\n               load_content=True, shuffle=True, encoding=None,\\n               decode_error='strict', random_state=0):\\n    target = []\\n    target_names = []\\n    filenames = []\\n    folders = [f for f in sorted(listdir(container_path))\\n               if isdir(join(container_path, f))]\\n    if categories is not None:\\n        folders = [f for f in folders if f in categories]\\n    for label, folder in enumerate(folders):\\n        target_names.append(folder)\\n        folder_path = join(container_path, folder)\\n        documents = [join(folder_path, d)\\n                     for d in sorted(listdir(folder_path))]\\n        target.extend(len(documents) * [label])\\n        filenames.extend(documents)\\n    filenames = np.array(filenames)\\n    target = np.array(target)\\n    if shuffle:\\n        random_state = check_random_state(random_state)\\n        indices = np.arange(filenames.shape[0])\\n        random_state.shuffle(indices)\\n        filenames = filenames[indices]\\n        target = target[indices]\\n    if load_content:\\n        data = []\\n        for filename in filenames:\\n            with open(filename, 'rb') as f:\\n                data.append(f.read())\\n        if encoding is not None:\\n            data = [d.decode(encoding, decode_error) for d in data]\\n        return Bunch(data=data,\\n                     filenames=filenames,\\n                     target_names=target_names,\\n                     target=target,\\n                     DESCR=description)\\n    return Bunch(filenames=filenames,\\n                 target_names=target_names,\\n                 target=target,\\n                 DESCR=description)"
  },
  {
    "code": "def _formatparam(param, value=None, quote=True):\\n    if value is not None and len(value) > 0:\\n        if isinstance(value, tuple):\\n            param += '*'\\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\\n        else:\\n            try:\\n                value.encode('ascii')\\n            except UnicodeEncodeError:\\n                param += '*'\\n                value = utils.encode_rfc2231(value, 'utf-8', '')\\n        if quote or tspecials.search(value):\\n            return '%s=\"%s\"' % (param, utils.quote(value))\\n        else:\\n            return '%s=%s' % (param, value)\\n    else:\\n        return param",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#1693546: don't add quotes around RFC 2231 encoded values.\\n\\nThe RFC is bit hard to understand on this point, but the examples\\nclearly show that parameter values that are encoded according\\nto its charset/language rules don't have surrounding quotes, and\\nthe ABNF does not allow for quotes.  So when we produce such\\nencoded values, we no longer add quotes.",
    "fixed_code": "def _formatparam(param, value=None, quote=True):\\n    if value is not None and len(value) > 0:\\n        if isinstance(value, tuple):\\n            param += '*'\\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\\n            return '%s=%s' % (param, value)\\n        else:\\n            try:\\n                value.encode('ascii')\\n            except UnicodeEncodeError:\\n                param += '*'\\n                value = utils.encode_rfc2231(value, 'utf-8', '')\\n                return '%s=%s' % (param, value)\\n        if quote or tspecials.search(value):\\n            return '%s=\"%s\"' % (param, utils.quote(value))\\n        else:\\n            return '%s=%s' % (param, value)\\n    else:\\n        return param"
  },
  {
    "code": "def run_luks_remove_key(self, device, keyfile, passphrase,\\n                            force_remove_last_key=False):\\n        ''\\n        if not force_remove_last_key:\\n            result = self._run_command([self._cryptsetup_bin, 'luksDump', device])\\n            if result[RETURN_CODE] != 0:\\n                raise ValueError('Error while dumping LUKS header from %s'\\n                                 % (device, ))\\n            keyslot_count = 0\\n            keyslot_area = False\\n            keyslot_re = re.compile(r'^Key Slot [0-9]+: ENABLED')\\n            for line in result[STDOUT].splitlines():\\n                if line.startswith('Keyslots:'):\\n                    keyslot_area = True\\n                elif line.startswith('  '):\\n                    if keyslot_area and line[2] in '0123456789':\\n                        keyslot_count += 1\\n                elif line.startswith('\\t'):\\n                    pass\\n                elif keyslot_re.match(line):\\n                    keyslot_count += 1\\n                else:\\n                    keyslot_area = False\\n            if keyslot_count < 2:\\n                self._module.fail_json(msg=\"LUKS device %s has less than two active keyslots. \"\\n                                           \"To be able to remove a key, please set \"\\n                                           \"`force_remove_last_key` to `yes`.\" % device)\\n        args = [self._cryptsetup_bin, 'luksRemoveKey', device, '-q']\\n        if keyfile:\\n            args.extend(['--key-file', keyfile])\\n        result = self._run_command(args, data=passphrase)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while removing LUKS key from %s: %s'\\n                             % (device, result[STDERR]))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run_luks_remove_key(self, device, keyfile, passphrase,\\n                            force_remove_last_key=False):\\n        ''\\n        if not force_remove_last_key:\\n            result = self._run_command([self._cryptsetup_bin, 'luksDump', device])\\n            if result[RETURN_CODE] != 0:\\n                raise ValueError('Error while dumping LUKS header from %s'\\n                                 % (device, ))\\n            keyslot_count = 0\\n            keyslot_area = False\\n            keyslot_re = re.compile(r'^Key Slot [0-9]+: ENABLED')\\n            for line in result[STDOUT].splitlines():\\n                if line.startswith('Keyslots:'):\\n                    keyslot_area = True\\n                elif line.startswith('  '):\\n                    if keyslot_area and line[2] in '0123456789':\\n                        keyslot_count += 1\\n                elif line.startswith('\\t'):\\n                    pass\\n                elif keyslot_re.match(line):\\n                    keyslot_count += 1\\n                else:\\n                    keyslot_area = False\\n            if keyslot_count < 2:\\n                self._module.fail_json(msg=\"LUKS device %s has less than two active keyslots. \"\\n                                           \"To be able to remove a key, please set \"\\n                                           \"`force_remove_last_key` to `yes`.\" % device)\\n        args = [self._cryptsetup_bin, 'luksRemoveKey', device, '-q']\\n        if keyfile:\\n            args.extend(['--key-file', keyfile])\\n        result = self._run_command(args, data=passphrase)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while removing LUKS key from %s: %s'\\n                             % (device, result[STDERR]))"
  },
  {
    "code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\\n\\tfields = {}\\n\\tif cls.__module__ in sys.modules:\\n\\t\\tglobals = sys.modules[cls.__module__].__dict__\\n\\telse:\\n\\t\\tglobals = {}\\n\\tsetattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   unsafe_hash, frozen))\\n\\tany_frozen_base = False\\n\\thas_dataclass_bases = False\\n\\tfor b in cls.__mro__[-1:0:-1]:\\n\\t\\tbase_fields = getattr(b, _FIELDS, None)\\n\\t\\tif base_fields is not None:\\n\\t\\t\\thas_dataclass_bases = True\\n\\t\\t\\tfor f in base_fields.values():\\n\\t\\t\\t\\tfields[f.name] = f\\n\\t\\t\\tif getattr(b, _PARAMS).frozen:\\n\\t\\t\\t\\tany_frozen_base = True\\n\\tcls_annotations = cls.__dict__.get('__annotations__', {})\\n\\tcls_fields = [_get_field(cls, name, type)\\n\\t\\t\\t\\t  for name, type in cls_annotations.items()]\\n\\tfor f in cls_fields:\\n\\t\\tfields[f.name] = f\\n\\t\\tif isinstance(getattr(cls, f.name, None), Field):\\n\\t\\t\\tif f.default is MISSING:\\n\\t\\t\\t\\tdelattr(cls, f.name)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tsetattr(cls, f.name, f.default)\\n\\tfor name, value in cls.__dict__.items():\\n\\t\\tif isinstance(value, Field) and not name in cls_annotations:\\n\\t\\t\\traise TypeError(f'{name!r} is a field but has no type annotation')\\n\\tif has_dataclass_bases:\\n\\t\\tif any_frozen_base and not frozen:\\n\\t\\t\\traise TypeError('cannot inherit non-frozen dataclass from a '\\n\\t\\t\\t\\t\\t\\t\\t'frozen one')\\n\\t\\tif not any_frozen_base and frozen:\\n\\t\\t\\traise TypeError('cannot inherit frozen dataclass from a '\\n\\t\\t\\t\\t\\t\\t\\t'non-frozen one')\\n\\tsetattr(cls, _FIELDS, fields)\\n\\tclass_hash = cls.__dict__.get('__hash__', MISSING)\\n\\thas_explicit_hash = not (class_hash is MISSING or\\n\\t\\t\\t\\t\\t\\t\\t (class_hash is None and '__eq__' in cls.__dict__))\\n\\tif order and not eq:\\n\\t\\traise ValueError('eq must be true if order is true')\\n\\tif init:\\n\\t\\thas_post_init = hasattr(cls, _POST_INIT_NAME)\\n\\t\\tflds = [f for f in fields.values()\\n\\t\\t\\t\\tif f._field_type in (_FIELD, _FIELD_INITVAR)]\\n\\t\\t_set_new_attribute(cls, '__init__',\\n\\t\\t\\t\\t\\t\\t   _init_fn(flds,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tfrozen,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\thas_post_init,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t'__dataclass_self__' if 'self' in fields\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse 'self',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tglobals,\\n\\t\\t\\t\\t\\t\\t  ))\\n\\tfield_list = [f for f in fields.values() if f._field_type is _FIELD]\\n\\tif repr:\\n\\t\\tflds = [f for f in field_list if f.repr]\\n\\t\\t_set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n\\tif eq:\\n\\t\\tflds = [f for f in field_list if f.compare]\\n\\t\\tself_tuple = _tuple_str('self', flds)\\n\\t\\tother_tuple = _tuple_str('other', flds)\\n\\t\\t_set_new_attribute(cls, '__eq__',\\n\\t\\t\\t\\t\\t\\t   _cmp_fn('__eq__', '==',\\n\\t\\t\\t\\t\\t\\t\\t\\t   self_tuple, other_tuple,\\n\\t\\t\\t\\t\\t\\t\\t\\t   globals=globals))\\n\\tif order:\\n\\t\\tflds = [f for f in field_list if f.compare]\\n\\t\\tself_tuple = _tuple_str('self', flds)\\n\\t\\tother_tuple = _tuple_str('other', flds)\\n\\t\\tfor name, op in [('__lt__', '<'),\\n\\t\\t\\t\\t\\t\\t ('__le__', '<='),\\n\\t\\t\\t\\t\\t\\t ('__gt__', '>'),\\n\\t\\t\\t\\t\\t\\t ('__ge__', '>='),\\n\\t\\t\\t\\t\\t\\t ]:\\n\\t\\t\\tif _set_new_attribute(cls, name,\\n\\t\\t\\t\\t\\t\\t\\t\\t  _cmp_fn(name, op, self_tuple, other_tuple,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  globals=globals)):\\n\\t\\t\\t\\traise TypeError(f'Cannot overwrite attribute {name} '\\n\\t\\t\\t\\t\\t\\t\\t\\tf'in class {cls.__name__}. Consider using '\\n\\t\\t\\t\\t\\t\\t\\t\\t'functools.total_ordering')\\n\\tif frozen:\\n\\t\\tfor fn in _frozen_get_del_attr(cls, field_list, globals):\\n\\t\\t\\tif _set_new_attribute(cls, fn.__name__, fn):\\n\\t\\t\\t\\traise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n\\t\\t\\t\\t\\t\\t\\t\\tf'in class {cls.__name__}')\\n\\thash_action = _hash_action[bool(unsafe_hash),\\n\\t\\t\\t\\t\\t\\t\\t   bool(eq),\\n\\t\\t\\t\\t\\t\\t\\t   bool(frozen),\\n\\t\\t\\t\\t\\t\\t\\t   has_explicit_hash]\\n\\tif hash_action:\\n\\t\\tcls.__hash__ = hash_action(cls, field_list, globals)\\n\\tif not getattr(cls, '__doc__'):\\n\\t\\tcls.__doc__ = (cls.__name__ +\\n\\t\\t\\t\\t\\t   str(inspect.signature(cls)).replace(' -> NoneType', ''))\\n\\tif '__match_args__' not in cls.__dict__:\\n\\t\\tcls.__match_args__ = tuple(f.name for f in field_list if f.init)\\n\\tabc.update_abstractmethods(cls)\\n\\treturn cls",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\\n\\tfields = {}\\n\\tif cls.__module__ in sys.modules:\\n\\t\\tglobals = sys.modules[cls.__module__].__dict__\\n\\telse:\\n\\t\\tglobals = {}\\n\\tsetattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   unsafe_hash, frozen))\\n\\tany_frozen_base = False\\n\\thas_dataclass_bases = False\\n\\tfor b in cls.__mro__[-1:0:-1]:\\n\\t\\tbase_fields = getattr(b, _FIELDS, None)\\n\\t\\tif base_fields is not None:\\n\\t\\t\\thas_dataclass_bases = True\\n\\t\\t\\tfor f in base_fields.values():\\n\\t\\t\\t\\tfields[f.name] = f\\n\\t\\t\\tif getattr(b, _PARAMS).frozen:\\n\\t\\t\\t\\tany_frozen_base = True\\n\\tcls_annotations = cls.__dict__.get('__annotations__', {})\\n\\tcls_fields = [_get_field(cls, name, type)\\n\\t\\t\\t\\t  for name, type in cls_annotations.items()]\\n\\tfor f in cls_fields:\\n\\t\\tfields[f.name] = f\\n\\t\\tif isinstance(getattr(cls, f.name, None), Field):\\n\\t\\t\\tif f.default is MISSING:\\n\\t\\t\\t\\tdelattr(cls, f.name)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tsetattr(cls, f.name, f.default)\\n\\tfor name, value in cls.__dict__.items():\\n\\t\\tif isinstance(value, Field) and not name in cls_annotations:\\n\\t\\t\\traise TypeError(f'{name!r} is a field but has no type annotation')\\n\\tif has_dataclass_bases:\\n\\t\\tif any_frozen_base and not frozen:\\n\\t\\t\\traise TypeError('cannot inherit non-frozen dataclass from a '\\n\\t\\t\\t\\t\\t\\t\\t'frozen one')\\n\\t\\tif not any_frozen_base and frozen:\\n\\t\\t\\traise TypeError('cannot inherit frozen dataclass from a '\\n\\t\\t\\t\\t\\t\\t\\t'non-frozen one')\\n\\tsetattr(cls, _FIELDS, fields)\\n\\tclass_hash = cls.__dict__.get('__hash__', MISSING)\\n\\thas_explicit_hash = not (class_hash is MISSING or\\n\\t\\t\\t\\t\\t\\t\\t (class_hash is None and '__eq__' in cls.__dict__))\\n\\tif order and not eq:\\n\\t\\traise ValueError('eq must be true if order is true')\\n\\tif init:\\n\\t\\thas_post_init = hasattr(cls, _POST_INIT_NAME)\\n\\t\\tflds = [f for f in fields.values()\\n\\t\\t\\t\\tif f._field_type in (_FIELD, _FIELD_INITVAR)]\\n\\t\\t_set_new_attribute(cls, '__init__',\\n\\t\\t\\t\\t\\t\\t   _init_fn(flds,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tfrozen,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\thas_post_init,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t'__dataclass_self__' if 'self' in fields\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse 'self',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tglobals,\\n\\t\\t\\t\\t\\t\\t  ))\\n\\tfield_list = [f for f in fields.values() if f._field_type is _FIELD]\\n\\tif repr:\\n\\t\\tflds = [f for f in field_list if f.repr]\\n\\t\\t_set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\\n\\tif eq:\\n\\t\\tflds = [f for f in field_list if f.compare]\\n\\t\\tself_tuple = _tuple_str('self', flds)\\n\\t\\tother_tuple = _tuple_str('other', flds)\\n\\t\\t_set_new_attribute(cls, '__eq__',\\n\\t\\t\\t\\t\\t\\t   _cmp_fn('__eq__', '==',\\n\\t\\t\\t\\t\\t\\t\\t\\t   self_tuple, other_tuple,\\n\\t\\t\\t\\t\\t\\t\\t\\t   globals=globals))\\n\\tif order:\\n\\t\\tflds = [f for f in field_list if f.compare]\\n\\t\\tself_tuple = _tuple_str('self', flds)\\n\\t\\tother_tuple = _tuple_str('other', flds)\\n\\t\\tfor name, op in [('__lt__', '<'),\\n\\t\\t\\t\\t\\t\\t ('__le__', '<='),\\n\\t\\t\\t\\t\\t\\t ('__gt__', '>'),\\n\\t\\t\\t\\t\\t\\t ('__ge__', '>='),\\n\\t\\t\\t\\t\\t\\t ]:\\n\\t\\t\\tif _set_new_attribute(cls, name,\\n\\t\\t\\t\\t\\t\\t\\t\\t  _cmp_fn(name, op, self_tuple, other_tuple,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  globals=globals)):\\n\\t\\t\\t\\traise TypeError(f'Cannot overwrite attribute {name} '\\n\\t\\t\\t\\t\\t\\t\\t\\tf'in class {cls.__name__}. Consider using '\\n\\t\\t\\t\\t\\t\\t\\t\\t'functools.total_ordering')\\n\\tif frozen:\\n\\t\\tfor fn in _frozen_get_del_attr(cls, field_list, globals):\\n\\t\\t\\tif _set_new_attribute(cls, fn.__name__, fn):\\n\\t\\t\\t\\traise TypeError(f'Cannot overwrite attribute {fn.__name__} '\\n\\t\\t\\t\\t\\t\\t\\t\\tf'in class {cls.__name__}')\\n\\thash_action = _hash_action[bool(unsafe_hash),\\n\\t\\t\\t\\t\\t\\t\\t   bool(eq),\\n\\t\\t\\t\\t\\t\\t\\t   bool(frozen),\\n\\t\\t\\t\\t\\t\\t\\t   has_explicit_hash]\\n\\tif hash_action:\\n\\t\\tcls.__hash__ = hash_action(cls, field_list, globals)\\n\\tif not getattr(cls, '__doc__'):\\n\\t\\tcls.__doc__ = (cls.__name__ +\\n\\t\\t\\t\\t\\t   str(inspect.signature(cls)).replace(' -> NoneType', ''))\\n\\tif '__match_args__' not in cls.__dict__:\\n\\t\\tcls.__match_args__ = tuple(f.name for f in field_list if f.init)\\n\\tabc.update_abstractmethods(cls)\\n\\treturn cls"
  },
  {
    "code": "def decode(obj):\\n    typ = obj.get('typ')\\n    if typ is None:\\n        return obj\\n    elif typ == 'timestamp':\\n        return Timestamp(obj['value'], tz=obj['tz'], offset=obj['offset'])\\n    elif typ == 'period':\\n        return Period(ordinal=obj['ordinal'], freq=obj['freq'])\\n    elif typ == 'index':\\n        dtype = dtype_for(obj['dtype'])\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        return globals()[obj['klass']](data, dtype=dtype, name=obj['name'])\\n    elif typ == 'multi_index':\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        data = [tuple(x) for x in data]\\n        return globals()[obj['klass']].from_tuples(data, names=obj['names'])\\n    elif typ == 'period_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        return globals()[obj['klass']](data, name=obj['name'],\\n                                       freq=obj['freq'])\\n    elif typ == 'datetime_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        result = globals()[obj['klass']](data, freq=obj['freq'],\\n                                         name=obj['name'])\\n        tz = obj['tz']\\n        if tz is not None:\\n            result = result.tz_localize('UTC').tz_convert(tz)\\n        return result\\n    elif typ == 'series':\\n        dtype = dtype_for(obj['dtype'])\\n        index = obj['index']\\n        return globals()[obj['klass']](unconvert(obj['data'], dtype,\\n                                                 obj['compress']),\\n                                       index=index, name=obj['name'])\\n    elif typ == 'block_manager':\\n        axes = obj['axes']",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Bug in pd.read_msgpack with inferring a DateTimeIndex frequencey      incorrectly (GH5947)",
    "fixed_code": "def decode(obj):\\n    typ = obj.get('typ')\\n    if typ is None:\\n        return obj\\n    elif typ == 'timestamp':\\n        return Timestamp(obj['value'], tz=obj['tz'], offset=obj['offset'])\\n    elif typ == 'period':\\n        return Period(ordinal=obj['ordinal'], freq=obj['freq'])\\n    elif typ == 'index':\\n        dtype = dtype_for(obj['dtype'])\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        return globals()[obj['klass']](data, dtype=dtype, name=obj['name'])\\n    elif typ == 'multi_index':\\n        data = unconvert(obj['data'], np.typeDict[obj['dtype']],\\n                         obj.get('compress'))\\n        data = [tuple(x) for x in data]\\n        return globals()[obj['klass']].from_tuples(data, names=obj['names'])\\n    elif typ == 'period_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        d = dict(name=obj['name'], freq=obj['freq'])\\n        return globals()[obj['klass']](data, **d)\\n    elif typ == 'datetime_index':\\n        data = unconvert(obj['data'], np.int64, obj.get('compress'))\\n        d = dict(name=obj['name'], freq=obj['freq'], verify_integrity=False)\\n        result = globals()[obj['klass']](data, **d)\\n        tz = obj['tz']\\n        if tz is not None:\\n            result = result.tz_localize('UTC').tz_convert(tz)\\n        return result\\n    elif typ == 'series':\\n        dtype = dtype_for(obj['dtype'])\\n        index = obj['index']\\n        return globals()[obj['klass']](unconvert(obj['data'], dtype,\\n                                                 obj['compress']),\\n                                       index=index, name=obj['name'])\\n    elif typ == 'block_manager':\\n        axes = obj['axes']"
  },
  {
    "code": "def queue_task_instance(\\n            self,\\n            task_instance,\\n            mark_success=False,\\n            pickle_id=None,\\n            force=False,\\n            ignore_dependencies=False,\\n            ignore_depends_on_past=False,\\n            pool=None):\\n        pool = pool or task_instance.pool\\n        command = task_instance.command(\\n            local=True,\\n            mark_success=mark_success,\\n            force=force,\\n            ignore_dependencies=ignore_dependencies,\\n            ignore_depends_on_past=ignore_depends_on_past,\\n            pool=pool,\\n            pickle_id=pickle_id)\\n        self.queue_command(\\n            task_instance,\\n            command,\\n            priority=task_instance.task.priority_weight_total,\\n            queue=task_instance.task.queue)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def queue_task_instance(\\n            self,\\n            task_instance,\\n            mark_success=False,\\n            pickle_id=None,\\n            force=False,\\n            ignore_dependencies=False,\\n            ignore_depends_on_past=False,\\n            pool=None):\\n        pool = pool or task_instance.pool\\n        command = task_instance.command(\\n            local=True,\\n            mark_success=mark_success,\\n            force=force,\\n            ignore_dependencies=ignore_dependencies,\\n            ignore_depends_on_past=ignore_depends_on_past,\\n            pool=pool,\\n            pickle_id=pickle_id)\\n        self.queue_command(\\n            task_instance,\\n            command,\\n            priority=task_instance.task.priority_weight_total,\\n            queue=task_instance.task.queue)"
  },
  {
    "code": "def vlines(\\n\\t\\tx, ymin, ymax, colors=None, linestyles='solid', label='', *,\\n\\t\\tdata=None, **kwargs):\\n\\treturn gca().vlines(\\n\\t\\tx, ymin, ymax, colors=colors, linestyles=linestyles,\\n\\t\\tlabel=label, **({\"data\": data} if data is not None else {}),\\n\\t\\t**kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def vlines(\\n\\t\\tx, ymin, ymax, colors=None, linestyles='solid', label='', *,\\n\\t\\tdata=None, **kwargs):\\n\\treturn gca().vlines(\\n\\t\\tx, ymin, ymax, colors=colors, linestyles=linestyles,\\n\\t\\tlabel=label, **({\"data\": data} if data is not None else {}),\\n\\t\\t**kwargs)"
  },
  {
    "code": "def process_subdir(subdir):\\n    if subdir:\\n        subdir = subdir.replace('DAGS_FOLDER', DAGS_FOLDER)\\n        subdir = os.path.abspath(os.path.expanduser(subdir))\\n    return subdir",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_subdir(subdir):\\n    if subdir:\\n        subdir = subdir.replace('DAGS_FOLDER', DAGS_FOLDER)\\n        subdir = os.path.abspath(os.path.expanduser(subdir))\\n    return subdir"
  },
  {
    "code": "def header_items(self):\\n        return [\\n            (to_native_str(k), [to_native_str(x) for x in v])\\n            for k, v in self.request.headers.items()\\n        ]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not break cookie parsing on non-utf8 headers",
    "fixed_code": "def header_items(self):\\n        return [\\n            (to_native_str(k, errors='replace'),\\n             [to_native_str(x, errors='replace') for x in v])\\n            for k, v in self.request.headers.items()\\n        ]"
  },
  {
    "code": "def get_lzma_file() -> type[pandas.compat._compressors.LZMAFile]:\\n    if not pandas.compat._compressors.has_lzma:\\n        raise RuntimeError(\\n            \"lzma module not available. \"\\n            \"A Python re-install with the proper dependencies, \"\\n            \"might be required to solve this issue.\"\\n        )\\n    return pandas.compat._compressors.LZMAFile",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_lzma_file() -> type[pandas.compat._compressors.LZMAFile]:\\n    if not pandas.compat._compressors.has_lzma:\\n        raise RuntimeError(\\n            \"lzma module not available. \"\\n            \"A Python re-install with the proper dependencies, \"\\n            \"might be required to solve this issue.\"\\n        )\\n    return pandas.compat._compressors.LZMAFile"
  },
  {
    "code": "def __call__(self, request):\\n\\t\\tLOG.info(\"%(method)s %(url)s\" % {\"method\": request.method,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \"url\": request.url})\\n\\t\\ttry:\\n\\t\\t\\taction, args, accept = self.deserializer.deserialize(request)\\n\\t\\texcept exception.InvalidContentType:\\n\\t\\t\\tmsg = _(\"Unsupported Content-Type\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\texcept exception.MalformedRequestBody:\\n\\t\\t\\tmsg = _(\"Malformed request body\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\tproject_id = args.pop(\"project_id\", None)\\n\\t\\tif 'nova.context' in request.environ and project_id:\\n\\t\\t\\trequest.environ['nova.context'].project_id = project_id\\n\\t\\ttry:\\n\\t\\t\\taction_result = self.dispatch(request, action, args)\\n\\t\\texcept faults.Fault as ex:\\n\\t\\t\\tLOG.info(_(\"Fault thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = ex\\n\\t\\texcept webob.exc.HTTPException as ex:\\n\\t\\t\\tLOG.info(_(\"HTTP exception thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = faults.Fault(ex)\\n\\t\\tif type(action_result) is dict or action_result is None:\\n\\t\\t\\tresponse = self.serializer.serialize(action_result,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t accept,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t action=action)\\n\\t\\telse:\\n\\t\\t\\tresponse = action_result\\n\\t\\ttry:\\n\\t\\t\\tmsg_dict = dict(url=request.url, status=response.status_int)\\n\\t\\t\\tmsg = _(\"%(url)s returned with HTTP %(status)d\") % msg_dict\\n\\t\\texcept AttributeError, e:\\n\\t\\t\\tmsg_dict = dict(url=request.url, e=e)\\n\\t\\t\\tmsg = _(\"%(url)s returned a fault: %(e)s\" % msg_dict)\\n\\t\\tLOG.info(msg)\\n\\t\\treturn response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not overwrite project_id from request params\\n\\nPrevent project_id overwriting from OSAPI request parameters.\\nThe patch is actually very simple (nova/api/openstack/wsgi.py) but\\nneeds significant test adjustments (nova/tests/*) to pass.\\n\\nFixes bug 904072. Patch from Vish Ishaya and Mark McLoughlin.\\n\\n(cherry picked from commit c9c09bd60e7a0e0258d218a31d7878755bea1395)",
    "fixed_code": "def __call__(self, request):\\n\\t\\tLOG.info(\"%(method)s %(url)s\" % {\"method\": request.method,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \"url\": request.url})\\n\\t\\ttry:\\n\\t\\t\\taction, args, accept = self.deserializer.deserialize(request)\\n\\t\\texcept exception.InvalidContentType:\\n\\t\\t\\tmsg = _(\"Unsupported Content-Type\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\texcept exception.MalformedRequestBody:\\n\\t\\t\\tmsg = _(\"Malformed request body\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\tproject_id = args.pop(\"project_id\", None)\\n\\t\\tif ('nova.context' in request.environ and project_id\\n\\t\\t\\tand project_id != request.environ['nova.context'].project_id):\\n\\t\\t\\tmsg = _(\"Malformed request url\")\\n\\t\\t\\treturn faults.Fault(webob.exc.HTTPBadRequest(explanation=msg))\\n\\t\\ttry:\\n\\t\\t\\taction_result = self.dispatch(request, action, args)\\n\\t\\texcept faults.Fault as ex:\\n\\t\\t\\tLOG.info(_(\"Fault thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = ex\\n\\t\\texcept webob.exc.HTTPException as ex:\\n\\t\\t\\tLOG.info(_(\"HTTP exception thrown: %s\"), unicode(ex))\\n\\t\\t\\taction_result = faults.Fault(ex)\\n\\t\\tif type(action_result) is dict or action_result is None:\\n\\t\\t\\tresponse = self.serializer.serialize(action_result,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t accept,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t action=action)\\n\\t\\telse:\\n\\t\\t\\tresponse = action_result\\n\\t\\ttry:\\n\\t\\t\\tmsg_dict = dict(url=request.url, status=response.status_int)\\n\\t\\t\\tmsg = _(\"%(url)s returned with HTTP %(status)d\") % msg_dict\\n\\t\\texcept AttributeError, e:\\n\\t\\t\\tmsg_dict = dict(url=request.url, e=e)\\n\\t\\t\\tmsg = _(\"%(url)s returned a fault: %(e)s\" % msg_dict)\\n\\t\\tLOG.info(msg)\\n\\t\\treturn response"
  },
  {
    "code": "def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):\\n    from pandas.core.format import print_config\\n    from pandas.tseries.offsets import DateOffset\\n    from pandas.tseries.frequencies import (_get_rule_month, _month_numbers,\\n                                            _get_freq_str)\\n    if not isinstance(arg, basestring):\\n        return arg\\n    arg = arg.upper()\\n    default = datetime(1, 1, 1).replace(hour=0, minute=0,\\n                                      second=0, microsecond=0)\\n    if len(arg) in [4, 6]:\\n        m = ypat.match(arg)\\n        if m:\\n            ret = default.replace(year=int(m.group(1)))\\n            return ret, ret, 'year'\\n        add_century = False\\n        if len(arg) == 4:\\n            add_century = True\\n            qpats = [(qpat1, 1), (qpat2, 0)]\\n        else:\\n            qpats = [(qpat1full, 1), (qpat2full, 0)]\\n        for pat, yfirst in qpats:\\n            qparse = pat.match(arg)\\n            if qparse is not None:\\n                if yfirst:\\n                    yi, qi = 1, 2\\n                else:\\n                    yi, qi = 2, 1\\n                q = int(qparse.group(yi))\\n                y_str = qparse.group(qi)\\n                y = int(y_str)\\n                if add_century:\\n                    y += 2000\\n                if freq is not None:\\n                    mnum = _month_numbers[_get_rule_month(freq)] + 1\\n                    month = (mnum + (q - 1) * 3) % 12 + 1\\n                    if month > mnum:\\n                        y -= 1\\n                else:\\n                    month = (q - 1) * 3 + 1\\n                ret = default.replace(year=y, month=month)\\n                return ret, ret, 'quarter'\\n        is_mo_str = freq is not None and freq == 'M'\\n        is_mo_off = getattr(freq, 'rule_code', None) == 'M'\\n        is_monthly = is_mo_str or is_mo_off\\n        if len(arg) == 6 and is_monthly:\\n            try:\\n                ret = _try_parse_monthly(arg)\\n                if ret is not None:\\n                    return ret, ret, 'month'\\n            except Exception:\\n                pass\\n    mresult = _attempt_monthly(arg)\\n    if mresult:\\n        return mresult\\n    if dayfirst is None:\\n        dayfirst = print_config.date_dayfirst\\n    if yearfirst is None:\\n        yearfirst = print_config.date_yearfirst\\n    try:\\n        parsed = parse(arg, dayfirst=dayfirst, yearfirst=yearfirst)\\n    except Exception, e:\\n        raise DateParseError(e)\\n    if parsed is None:\\n        raise DateParseError(\"Could not parse %s\" % arg)\\n    repl = {}\\n    reso = 'year'\\n    stopped = False\\n    for attr in [\"year\", \"month\", \"day\", \"hour\",\\n                 \"minute\", \"second\", \"microsecond\"]:\\n        can_be_zero = ['hour', 'minute', 'second', 'microsecond']\\n        value = getattr(parsed, attr)\\n        if value is not None and value != 0:  \\n            repl[attr] = value\\n            if not stopped:\\n                reso = attr\\n        else:\\n            stopped = True\\n            break\\n    ret = default.replace(**repl)\\n    return ret, parsed, reso",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: tools.parse_time_string does not parse intraday strings correctly #2306",
    "fixed_code": "def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):\\n    from pandas.core.format import print_config\\n    from pandas.tseries.offsets import DateOffset\\n    from pandas.tseries.frequencies import (_get_rule_month, _month_numbers,\\n                                            _get_freq_str)\\n    if not isinstance(arg, basestring):\\n        return arg\\n    arg = arg.upper()\\n    default = datetime(1, 1, 1).replace(hour=0, minute=0,\\n                                      second=0, microsecond=0)\\n    if len(arg) in [4, 6]:\\n        m = ypat.match(arg)\\n        if m:\\n            ret = default.replace(year=int(m.group(1)))\\n            return ret, ret, 'year'\\n        add_century = False\\n        if len(arg) == 4:\\n            add_century = True\\n            qpats = [(qpat1, 1), (qpat2, 0)]\\n        else:\\n            qpats = [(qpat1full, 1), (qpat2full, 0)]\\n        for pat, yfirst in qpats:\\n            qparse = pat.match(arg)\\n            if qparse is not None:\\n                if yfirst:\\n                    yi, qi = 1, 2\\n                else:\\n                    yi, qi = 2, 1\\n                q = int(qparse.group(yi))\\n                y_str = qparse.group(qi)\\n                y = int(y_str)\\n                if add_century:\\n                    y += 2000\\n                if freq is not None:\\n                    mnum = _month_numbers[_get_rule_month(freq)] + 1\\n                    month = (mnum + (q - 1) * 3) % 12 + 1\\n                    if month > mnum:\\n                        y -= 1\\n                else:\\n                    month = (q - 1) * 3 + 1\\n                ret = default.replace(year=y, month=month)\\n                return ret, ret, 'quarter'\\n        is_mo_str = freq is not None and freq == 'M'\\n        is_mo_off = getattr(freq, 'rule_code', None) == 'M'\\n        is_monthly = is_mo_str or is_mo_off\\n        if len(arg) == 6 and is_monthly:\\n            try:\\n                ret = _try_parse_monthly(arg)\\n                if ret is not None:\\n                    return ret, ret, 'month'\\n            except Exception:\\n                pass\\n    mresult = _attempt_monthly(arg)\\n    if mresult:\\n        return mresult\\n    if dayfirst is None:\\n        dayfirst = print_config.date_dayfirst\\n    if yearfirst is None:\\n        yearfirst = print_config.date_yearfirst\\n    try:\\n        parsed = parse(arg, dayfirst=dayfirst, yearfirst=yearfirst)\\n    except Exception, e:\\n        raise DateParseError(e)\\n    if parsed is None:\\n        raise DateParseError(\"Could not parse %s\" % arg)\\n    repl = {}\\n    reso = 'year'\\n    for attr in [\"year\", \"month\", \"day\", \"hour\",\\n                 \"minute\", \"second\", \"microsecond\"]:\\n        value = getattr(parsed, attr)\\n        if value is not None and value != 0:  \\n            repl[attr] = value\\n            reso = attr\\n    ret = default.replace(**repl)\\n    return ret, parsed, reso"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            outputfile=dict(required=True),\\n            host=dict(required=False),\\n            username=dict(required=False),\\n            password=dict(required=False, no_log=True),\\n            enablePassword=dict(required=False, no_log=True),\\n            deviceType=dict(required=True),\\n            vlanArg1=dict(required=True),\\n            vlanArg2=dict(required=False),\\n            vlanArg3=dict(required=False),\\n            vlanArg4=dict(required=False),\\n            vlanArg5=dict(required=False),),\\n        supports_check_mode=False)\\n    outputfile = module.params['outputfile']\\n    output = \"\"\\n    output = output + str(vlanConfig(module, \"(config)\\n    file = open(outputfile, \"a\")\\n    file.write(output)\\n    file.close()\\n    errorMsg = cnos.checkOutputForError(output)\\n    if(errorMsg is None):\\n        module.exit_json(changed=True,\\n                         msg=\"VLAN configuration is accomplished\")\\n    else:\\n        module.fail_json(msg=errorMsg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactoring cnos_vlan in line with ios, eos etc. (#48924)",
    "fixed_code": "def main():\\n    element_spec = dict(\\n        vlan_id=dict(type='int'),\\n        name=dict(),\\n        interfaces=dict(type='list'),\\n        associated_interfaces=dict(type='list'),\\n        delay=dict(default=10, type='int'),\\n        state=dict(default='present',\\n                   choices=['present', 'absent', 'active', 'suspend'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['vlan_id'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec),\\n        purge=dict(default=False, type='bool')\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(cnos_argument_spec)\\n    required_one_of = [['vlan_id', 'aggregate']]\\n    mutually_exclusive = [['vlan_id', 'aggregate']]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           required_one_of=required_one_of,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands((want, have), module)\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    check_declarative_intent_params(want, module, result)\\n    module.exit_json(**result)"
  },
  {
    "code": "def run_single_file_mode(\\n    line_length: int,\\n    check: bool,\\n    fast: bool,\\n    quiet: bool,\\n    write_back: WriteBack,\\n    src: Path,\\n) -> int:\\n    report = Report(check=check, quiet=quiet)\\n    try:\\n        if not src.is_file() and str(src) == \"-\":\\n            changed = format_stdin_to_stdout(\\n                line_length=line_length, fast=fast, write_back=write_back\\n            )\\n        else:\\n            changed = Changed.NO\\n            cache: Cache = {}\\n            if write_back != WriteBack.DIFF:\\n                cache = read_cache()\\n                src = src.resolve()\\n                if src in cache and cache[src] == get_cache_info(src):\\n                    changed = Changed.CACHED\\n            if changed is not Changed.CACHED:\\n                changed = format_file_in_place(\\n                    src, line_length=line_length, fast=fast, write_back=write_back\\n                )\\n            if write_back != WriteBack.DIFF and changed is not Changed.NO:\\n                write_cache(cache, [src])\\n        report.done(src, changed)\\n    except Exception as exc:\\n        report.failed(src, str(exc))\\n    return report.return_code",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert `format_file_in_place()` and `format_stdin_to_stdout()` to return bools\\n\\n`Changed.CACHED` is meaningless for those two functions.",
    "fixed_code": "def run_single_file_mode(\\n    line_length: int,\\n    check: bool,\\n    fast: bool,\\n    quiet: bool,\\n    write_back: WriteBack,\\n    src: Path,\\n) -> int:\\n    report = Report(check=check, quiet=quiet)\\n    try:\\n        changed = Changed.NO\\n        if not src.is_file() and str(src) == \"-\":\\n            if format_stdin_to_stdout(\\n                line_length=line_length, fast=fast, write_back=write_back\\n            ):\\n                changed = Changed.YES\\n        else:\\n            cache: Cache = {}\\n            if write_back != WriteBack.DIFF:\\n                cache = read_cache()\\n                src = src.resolve()\\n                if src in cache and cache[src] == get_cache_info(src):\\n                    changed = Changed.CACHED\\n            if (\\n                changed is not Changed.CACHED\\n                and format_file_in_place(\\n                    src, line_length=line_length, fast=fast, write_back=write_back\\n                )\\n            ):\\n                changed = Changed.YES\\n            if write_back != WriteBack.DIFF and changed is not Changed.NO:\\n                write_cache(cache, [src])\\n        report.done(src, changed)\\n    except Exception as exc:\\n        report.failed(src, str(exc))\\n    return report.return_code"
  },
  {
    "code": "def map_config_to_obj(module):\\n    out = run_commands(module, ['show run all | inc nxapi'], check_rc=False)[0]\\n    match = re.search(r'no feature nxapi', out, re.M)\\n    if match or out == '':\\n        return {'state': 'absent'}\\n    out = str(out).strip()\\n    obj = {'state': 'present'}\\n    obj.update(parse_http(out))\\n    obj.update(parse_https(out))\\n    obj.update(parse_sandbox(out))\\n    obj.update(parse_ssl_strong_ciphers(out))\\n    obj.update(parse_ssl_protocols(out))\\n    return obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_config_to_obj(module):\\n    out = run_commands(module, ['show run all | inc nxapi'], check_rc=False)[0]\\n    match = re.search(r'no feature nxapi', out, re.M)\\n    if match or out == '':\\n        return {'state': 'absent'}\\n    out = str(out).strip()\\n    obj = {'state': 'present'}\\n    obj.update(parse_http(out))\\n    obj.update(parse_https(out))\\n    obj.update(parse_sandbox(out))\\n    obj.update(parse_ssl_strong_ciphers(out))\\n    obj.update(parse_ssl_protocols(out))\\n    return obj"
  },
  {
    "code": "def __init__(self, values, categories=None, ordered=False, name=None, fastpath=False,\\n                 levels=None):\\n        if fastpath:\\n            self._codes = _coerce_indexer_dtype(values, categories)\\n            self.name = name\\n            self.categories = categories\\n            self._ordered = ordered\\n            return\\n        if name is None:\\n            name = getattr(values, 'name', None)\\n        if not levels is None:\\n            warn(\"Creating a 'Categorical' with 'levels' is deprecated, use 'categories' instead\",\\n                 FutureWarning)\\n            if categories is None:\\n                categories = levels\\n            else:\\n                raise ValueError(\"Cannot pass in both 'categories' and (deprecated) 'levels', \"\\n                                 \"use only 'categories'\")\\n        if is_categorical_dtype(values):\\n            if isinstance(values, (ABCSeries, ABCCategoricalIndex)):\\n                values = values.values\\n            if ordered is None:\\n                ordered = values.ordered\\n            if categories is None:\\n                categories = values.categories\\n            values = values.__array__()\\n        elif isinstance(values, ABCIndexClass):\\n            pass\\n        else:\\n            values = _possibly_infer_to_datetimelike(values, convert_dates=True)\\n            if not isinstance(values, np.ndarray):\\n                values = _convert_to_list_like(values)\\n                from pandas.core.series import _sanitize_array\\n                dtype = 'object' if isnull(values).any() else None\\n                values = _sanitize_array(values, None, dtype=dtype)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove Categorical.name to make it more numpy.ndarray like\\n\\n`name` was initialy introduced to save the name of a Series/column during\\na groupby, when categorical was mostly a helper for that.\\n\\n\\nCloses: #10482",
    "fixed_code": "def __init__(self, values, categories=None, ordered=False, name=None, fastpath=False,\\n                 levels=None):\\n        if fastpath:\\n            self._codes = _coerce_indexer_dtype(values, categories)\\n            self.categories = categories\\n            self._ordered = ordered\\n            return\\n        if not name is None:\\n            msg = \"the 'name' keyword is removed, use 'name' with consumers of the \" \\\\n                  \"categorical instead (e.g. 'Series(cat, name=\\\"something\\\")'\"\\n            warn(msg, UserWarning, stacklevel=2)\\n        if not levels is None:\\n            warn(\"Creating a 'Categorical' with 'levels' is deprecated, use 'categories' instead\",\\n                 FutureWarning, stacklevel=2)\\n            if categories is None:\\n                categories = levels\\n            else:\\n                raise ValueError(\"Cannot pass in both 'categories' and (deprecated) 'levels', \"\\n                                 \"use only 'categories'\", stacklevel=2)\\n        if is_categorical_dtype(values):\\n            if isinstance(values, (ABCSeries, ABCCategoricalIndex)):\\n                values = values.values\\n            if ordered is None:\\n                ordered = values.ordered\\n            if categories is None:\\n                categories = values.categories\\n            values = values.__array__()\\n        elif isinstance(values, ABCIndexClass):\\n            pass\\n        else:\\n            values = _possibly_infer_to_datetimelike(values, convert_dates=True)\\n            if not isinstance(values, np.ndarray):\\n                values = _convert_to_list_like(values)\\n                from pandas.core.series import _sanitize_array\\n                dtype = 'object' if isnull(values).any() else None\\n                values = _sanitize_array(values, None, dtype=dtype)"
  },
  {
    "code": "def _align_method_SERIES(left, right, align_asobject=False):\\n    if isinstance(right, ABCSeries):\\n        if not left.index.equals(right.index):\\n            if align_asobject:\\n                left = left.astype(object)\\n                right = right.astype(object)\\n            left, right = left.align(right, copy=False)\\n            index, lidx, ridx = left.index.join(right.index, how='outer',\\n                                                return_indexers=True)\\n            left.index = index\\n            right.index = index\\n    return left, right",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix alignment in series ops (GH14227) (#14230)",
    "fixed_code": "def _align_method_SERIES(left, right, align_asobject=False):\\n    if isinstance(right, ABCSeries):\\n        if not left.index.equals(right.index):\\n            if align_asobject:\\n                left = left.astype(object)\\n                right = right.astype(object)\\n            left, right = left.align(right, copy=False)\\n    return left, right"
  },
  {
    "code": "def frange(start, stop, step):\\twhile start <= stop:\\t\\tyield start\\t\\tstart += stop",
    "label": 1,
    "bug_type": "varmisuse",
    "bug_description": "Fix SF #1345263, colorsys tests, bug in frange\\n\\nFix a typo that caused step to be ignored.\\n\\nWill backport.",
    "fixed_code": "def frange(start, stop, step):\\n\\twhile start <= stop:\\n\\t\\tyield start\\n\\t\\tstart += step\\nclass ColorsysTest(unittest.TestCase):"
  },
  {
    "code": "def unstack(self, level=-1):\\n        from pandas.core.reshape import _Unstacker\\n        unstacker = _Unstacker(self.values, self.index, level=level,\\n                               value_columns=self.columns)\\n        return unstacker.get_result()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: unstack/stack multiple levels per #370, use Series index per note in #373",
    "fixed_code": "def unstack(self, level=-1):\\n        from pandas.core.reshape import unstack\\n        if isinstance(level, (tuple, list)):\\n            result = self\\n            for lev in level:\\n                result = unstack(result, lev)\\n            return result\\n        else:\\n            return unstack(self, level)"
  },
  {
    "code": "def read_sql_table(table_name, con, schema=None, index_col=None,\\n                   coerce_float=True, parse_dates=None, columns=None,\\n                   chunksize=None):\\n    con = _engine_builder(con)\\n    if not _is_sqlalchemy_connectable(con):\\n        raise NotImplementedError(\"read_sql_table only supported for \"\\n                                  \"SQLAlchemy connectable.\")\\n    import sqlalchemy\\n    from sqlalchemy.schema import MetaData\\n    meta = MetaData(con, schema=schema)\\n    try:\\n        meta.reflect(only=[table_name], views=True)\\n    except sqlalchemy.exc.InvalidRequestError:\\n        raise ValueError(\"Table %s not found\" % table_name)\\n    pandas_sql = SQLDatabase(con, meta=meta)\\n    table = pandas_sql.read_table(\\n        table_name, index_col=index_col, coerce_float=coerce_float,\\n        parse_dates=parse_dates, columns=columns, chunksize=chunksize)\\n    if table is not None:\\n        return table\\n    else:\\n        raise ValueError(\"Table %s not found\" % table_name, con)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_sql_table(table_name, con, schema=None, index_col=None,\\n                   coerce_float=True, parse_dates=None, columns=None,\\n                   chunksize=None):\\n    con = _engine_builder(con)\\n    if not _is_sqlalchemy_connectable(con):\\n        raise NotImplementedError(\"read_sql_table only supported for \"\\n                                  \"SQLAlchemy connectable.\")\\n    import sqlalchemy\\n    from sqlalchemy.schema import MetaData\\n    meta = MetaData(con, schema=schema)\\n    try:\\n        meta.reflect(only=[table_name], views=True)\\n    except sqlalchemy.exc.InvalidRequestError:\\n        raise ValueError(\"Table %s not found\" % table_name)\\n    pandas_sql = SQLDatabase(con, meta=meta)\\n    table = pandas_sql.read_table(\\n        table_name, index_col=index_col, coerce_float=coerce_float,\\n        parse_dates=parse_dates, columns=columns, chunksize=chunksize)\\n    if table is not None:\\n        return table\\n    else:\\n        raise ValueError(\"Table %s not found\" % table_name, con)"
  },
  {
    "code": "def set_tick_params(self, which='major', reset=False, **kw):\\n\\t\\tcbook._check_in_list(['major', 'minor', 'both'], which=which)\\n\\t\\tkwtrans = self._translate_tick_kw(kw)\\n\\t\\tif reset:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.clear()\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.clear()\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\tself.reset_ticks()\\n\\t\\telse:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.majorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.minorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif 'labelcolor' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_color(kwtrans['labelcolor'])\\n\\t\\tself.stale = True\\n\\t@staticmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "set_tick_params(label1On=False) should also make offset text invisible.",
    "fixed_code": "def set_tick_params(self, which='major', reset=False, **kw):\\n\\t\\tcbook._check_in_list(['major', 'minor', 'both'], which=which)\\n\\t\\tkwtrans = self._translate_tick_kw(kw)\\n\\t\\tif reset:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.clear()\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.clear()\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\tself.reset_ticks()\\n\\t\\telse:\\n\\t\\t\\tif which in ['major', 'both']:\\n\\t\\t\\t\\tself._major_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.majorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif which in ['minor', 'both']:\\n\\t\\t\\t\\tself._minor_tick_kw.update(kwtrans)\\n\\t\\t\\t\\tfor tick in self.minorTicks:\\n\\t\\t\\t\\t\\ttick._apply_params(**kwtrans)\\n\\t\\t\\tif 'label1On' in kwtrans or 'label2On' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_visible(\\n\\t\\t\\t\\t\\tself._major_tick_kw.get('label1On', False)\\n\\t\\t\\t\\t\\tor self._major_tick_kw.get('label2On', False))\\n\\t\\t\\tif 'labelcolor' in kwtrans:\\n\\t\\t\\t\\tself.offsetText.set_color(kwtrans['labelcolor'])\\n\\t\\tself.stale = True\\n\\t@staticmethod"
  },
  {
    "code": "def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\\n           force_grouping=False):\\n    use_grouping = settings.USE_L10N and settings.USE_THOUSAND_SEPARATOR\\n    use_grouping = use_grouping or force_grouping\\n    use_grouping = use_grouping and grouping > 0\\n    if isinstance(number, int) and not use_grouping and not decimal_pos:\\n        return mark_safe(six.text_type(number))\\n    sign = ''\\n    if isinstance(number, Decimal):\\n        str_number = '{:f}'.format(number)\\n    else:\\n        str_number = six.text_type(number)\\n    if str_number[0] == '-':\\n        sign = '-'\\n        str_number = str_number[1:]\\n    if '.' in str_number:\\n        int_part, dec_part = str_number.split('.')\\n        if decimal_pos is not None:\\n            dec_part = dec_part[:decimal_pos]\\n    else:\\n        int_part, dec_part = str_number, ''\\n    if decimal_pos is not None:\\n        dec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\\n    if dec_part:\\n        dec_part = decimal_sep + dec_part\\n    if use_grouping:\\n        int_part_gd = ''\\n        for cnt, digit in enumerate(int_part[::-1]):\\n            if cnt and not cnt % grouping:\\n                int_part_gd += thousand_sep[::-1]\\n            int_part_gd += digit\\n        int_part = int_part_gd[::-1]\\n    return sign + int_part + dec_part",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #25920 -- Added support for non-uniform NUMBER_GROUPING.",
    "fixed_code": "def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\\n           force_grouping=False):\\n    use_grouping = settings.USE_L10N and settings.USE_THOUSAND_SEPARATOR\\n    use_grouping = use_grouping or force_grouping\\n    use_grouping = use_grouping and grouping != 0\\n    if isinstance(number, int) and not use_grouping and not decimal_pos:\\n        return mark_safe(six.text_type(number))\\n    sign = ''\\n    if isinstance(number, Decimal):\\n        str_number = '{:f}'.format(number)\\n    else:\\n        str_number = six.text_type(number)\\n    if str_number[0] == '-':\\n        sign = '-'\\n        str_number = str_number[1:]\\n    if '.' in str_number:\\n        int_part, dec_part = str_number.split('.')\\n        if decimal_pos is not None:\\n            dec_part = dec_part[:decimal_pos]\\n    else:\\n        int_part, dec_part = str_number, ''\\n    if decimal_pos is not None:\\n        dec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\\n    if dec_part:\\n        dec_part = decimal_sep + dec_part\\n    if use_grouping:\\n        try:\\n            intervals = list(grouping)\\n        except TypeError:\\n            intervals = [grouping, 0]\\n        active_interval = intervals.pop(0)\\n        int_part_gd = ''\\n        cnt = 0\\n        for digit in int_part[::-1]:\\n            if cnt and cnt == active_interval:\\n                if intervals:\\n                    active_interval = intervals.pop(0) or active_interval\\n                int_part_gd += thousand_sep[::-1]\\n                cnt = 0\\n            int_part_gd += digit\\n            cnt += 1\\n        int_part = int_part_gd[::-1]\\n    return sign + int_part + dec_part"
  },
  {
    "code": "def create_app(config=None, testing=False, app_name=\"Airflow\"):\\n    flask_app = Flask(__name__)\\n    flask_app.secret_key = conf.get('webserver', 'SECRET_KEY')\\n    flask_app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\\n    flask_app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\\n    flask_app.config['APP_NAME'] = app_name\\n    flask_app.config['TESTING'] = testing\\n    flask_app.config['SQLALCHEMY_DATABASE_URI'] = conf.get('core', 'SQL_ALCHEMY_CONN')\\n    flask_app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\\n    flask_app.config['SESSION_COOKIE_HTTPONLY'] = True\\n    flask_app.config['SESSION_COOKIE_SECURE'] = conf.getboolean('webserver', 'COOKIE_SECURE')\\n    cookie_samesite_config = conf.get('webserver', 'COOKIE_SAMESITE')\\n    if cookie_samesite_config == \"\":\\n        warnings.warn(\\n            \"Old deprecated value found for `cookie_samesite` option in `[webserver]` section. \"\\n            \"Using `Lax` instead. Change the value to `Lax` in airflow.cfg to remove this warning.\",\\n            DeprecationWarning,\\n        )\\n        cookie_samesite_config = \"Lax\"\\n    flask_app.config['SESSION_COOKIE_SAMESITE'] = cookie_samesite_config\\n    if config:\\n        flask_app.config.from_mapping(config)\\n    if 'SQLALCHEMY_ENGINE_OPTIONS' not in flask_app.config:\\n        flask_app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()\\n    flask_app.json_encoder = AirflowJsonEncoder\\n    csrf.init_app(flask_app)\\n    init_wsgi_middleware(flask_app)\\n    db = SQLA()\\n    db.session = settings.Session\\n    db.init_app(flask_app)\\n    init_dagbag(flask_app)\\n    init_api_experimental_auth(flask_app)\\n    Cache(app=flask_app, config={'CACHE_TYPE': 'filesystem', 'CACHE_DIR': '/tmp'})\\n    init_flash_views(flask_app)\\n    configure_logging()\\n    configure_manifest_files(flask_app)\\n    with flask_app.app_context():\\n        init_appbuilder(flask_app)\\n        init_appbuilder_views(flask_app)\\n        init_appbuilder_links(flask_app)\\n        init_plugins(flask_app)\\n        init_connection_form()\\n        init_error_handlers(flask_app)\\n        init_api_connexion(flask_app)\\n        init_api_experimental(flask_app)\\n        sync_appbuilder_roles(flask_app)\\n        init_jinja_globals(flask_app)\\n        init_xframe_protection(flask_app)\\n        init_permanent_session(flask_app)\\n    return flask_app",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def create_app(config=None, testing=False, app_name=\"Airflow\"):\\n    flask_app = Flask(__name__)\\n    flask_app.secret_key = conf.get('webserver', 'SECRET_KEY')\\n    flask_app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\\n    flask_app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\\n    flask_app.config['APP_NAME'] = app_name\\n    flask_app.config['TESTING'] = testing\\n    flask_app.config['SQLALCHEMY_DATABASE_URI'] = conf.get('core', 'SQL_ALCHEMY_CONN')\\n    flask_app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\\n    flask_app.config['SESSION_COOKIE_HTTPONLY'] = True\\n    flask_app.config['SESSION_COOKIE_SECURE'] = conf.getboolean('webserver', 'COOKIE_SECURE')\\n    cookie_samesite_config = conf.get('webserver', 'COOKIE_SAMESITE')\\n    if cookie_samesite_config == \"\":\\n        warnings.warn(\\n            \"Old deprecated value found for `cookie_samesite` option in `[webserver]` section. \"\\n            \"Using `Lax` instead. Change the value to `Lax` in airflow.cfg to remove this warning.\",\\n            DeprecationWarning,\\n        )\\n        cookie_samesite_config = \"Lax\"\\n    flask_app.config['SESSION_COOKIE_SAMESITE'] = cookie_samesite_config\\n    if config:\\n        flask_app.config.from_mapping(config)\\n    if 'SQLALCHEMY_ENGINE_OPTIONS' not in flask_app.config:\\n        flask_app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()\\n    flask_app.json_encoder = AirflowJsonEncoder\\n    csrf.init_app(flask_app)\\n    init_wsgi_middleware(flask_app)\\n    db = SQLA()\\n    db.session = settings.Session\\n    db.init_app(flask_app)\\n    init_dagbag(flask_app)\\n    init_api_experimental_auth(flask_app)\\n    Cache(app=flask_app, config={'CACHE_TYPE': 'filesystem', 'CACHE_DIR': '/tmp'})\\n    init_flash_views(flask_app)\\n    configure_logging()\\n    configure_manifest_files(flask_app)\\n    with flask_app.app_context():\\n        init_appbuilder(flask_app)\\n        init_appbuilder_views(flask_app)\\n        init_appbuilder_links(flask_app)\\n        init_plugins(flask_app)\\n        init_connection_form()\\n        init_error_handlers(flask_app)\\n        init_api_connexion(flask_app)\\n        init_api_experimental(flask_app)\\n        sync_appbuilder_roles(flask_app)\\n        init_jinja_globals(flask_app)\\n        init_xframe_protection(flask_app)\\n        init_permanent_session(flask_app)\\n    return flask_app"
  },
  {
    "code": "def errors(self) -> Optional[str]:\\n        pass\\n    @abstractproperty",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def errors(self) -> Optional[str]:\\n        pass\\n    @abstractproperty"
  },
  {
    "code": "def _eigen_decompose_covariance(self, X, y, sqrt_sw):\\n        n_samples, n_features = X.shape\\n        cov = np.empty((n_features + 1, n_features + 1), dtype=X.dtype)\\n        cov[:-1, :-1], X_mean = self._compute_covariance(X, sqrt_sw)\\n        if not self.fit_intercept:\\n            cov = cov[:-1, :-1]\\n        else:\\n            cov[-1] = 0\\n            cov[:, -1] = 0\\n            cov[-1, -1] = sqrt_sw.dot(sqrt_sw)\\n        nullspace_dim = max(0, X.shape[1] - X.shape[0])\\n        eigvals, V = linalg.eigh(cov)\\n        eigvals = eigvals[nullspace_dim:]\\n        V = V[:, nullspace_dim:]\\n        return X_mean, eigvals, V, X",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _eigen_decompose_covariance(self, X, y, sqrt_sw):\\n        n_samples, n_features = X.shape\\n        cov = np.empty((n_features + 1, n_features + 1), dtype=X.dtype)\\n        cov[:-1, :-1], X_mean = self._compute_covariance(X, sqrt_sw)\\n        if not self.fit_intercept:\\n            cov = cov[:-1, :-1]\\n        else:\\n            cov[-1] = 0\\n            cov[:, -1] = 0\\n            cov[-1, -1] = sqrt_sw.dot(sqrt_sw)\\n        nullspace_dim = max(0, X.shape[1] - X.shape[0])\\n        eigvals, V = linalg.eigh(cov)\\n        eigvals = eigvals[nullspace_dim:]\\n        V = V[:, nullspace_dim:]\\n        return X_mean, eigvals, V, X"
  },
  {
    "code": "def get_dump_object(self, obj):\\n        model = obj._meta.proxy_for_model if obj._deferred else obj.__class__\\n        data = OrderedDict([('model', force_text(model._meta))])\\n        if not self.use_natural_primary_keys or not hasattr(obj, 'natural_key'):\\n            data[\"pk\"] = force_text(obj._get_pk_val(), strings_only=True)\\n        data['fields'] = self._current\\n        return data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_dump_object(self, obj):\\n        model = obj._meta.proxy_for_model if obj._deferred else obj.__class__\\n        data = OrderedDict([('model', force_text(model._meta))])\\n        if not self.use_natural_primary_keys or not hasattr(obj, 'natural_key'):\\n            data[\"pk\"] = force_text(obj._get_pk_val(), strings_only=True)\\n        data['fields'] = self._current\\n        return data"
  },
  {
    "code": "def _rescale(self, exp, rounding):\\n        if self._is_special:\\n            return Decimal(self)\\n        if not self:\\n            return _dec_from_triple(self._sign, '0', exp)\\n        if self._exp >= exp:\\n            return _dec_from_triple(self._sign,\\n                                        self._int + '0'*(self._exp - exp), exp)\\n        digits = len(self._int) + self._exp - exp\\n        if digits < 0:\\n            self = _dec_from_triple(self._sign, '1', exp-1)\\n            digits = 0\\n        this_function = getattr(self, self._pick_rounding_function[rounding])\\n        changed = this_function(digits)\\n        coeff = self._int[:digits] or '0'\\n        if changed == 1:\\n            coeff = str(int(coeff)+1)\\n        return _dec_from_triple(self._sign, coeff, exp)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _rescale(self, exp, rounding):\\n        if self._is_special:\\n            return Decimal(self)\\n        if not self:\\n            return _dec_from_triple(self._sign, '0', exp)\\n        if self._exp >= exp:\\n            return _dec_from_triple(self._sign,\\n                                        self._int + '0'*(self._exp - exp), exp)\\n        digits = len(self._int) + self._exp - exp\\n        if digits < 0:\\n            self = _dec_from_triple(self._sign, '1', exp-1)\\n            digits = 0\\n        this_function = getattr(self, self._pick_rounding_function[rounding])\\n        changed = this_function(digits)\\n        coeff = self._int[:digits] or '0'\\n        if changed == 1:\\n            coeff = str(int(coeff)+1)\\n        return _dec_from_triple(self._sign, coeff, exp)"
  },
  {
    "code": "def uploader_fn(exit_event):\\n  params = Params()\\n  dongle_id = params.get(\"DongleId\", encoding='utf8')\\n  if dongle_id is None:\\n    cloudlog.info(\"uploader missing dongle_id\")\\n    raise Exception(\"uploader can't start without dongle id\")\\n  if TICI and not Path(\"/data/media\").is_mount():\\n    cloudlog.warning(\"NVME not mounted\")\\n  sm = messaging.SubMaster(['deviceState'])\\n  pm = messaging.PubMaster(['uploaderState'])\\n  uploader = Uploader(dongle_id, ROOT)\\n  backoff = 0.1\\n  while not exit_event.is_set():\\n    sm.update(0)\\n    offroad = params.get_bool(\"IsOffroad\")\\n    network_type = sm['deviceState'].networkType if not force_wifi else NetworkType.wifi\\n    if network_type == NetworkType.none:\\n      if allow_sleep:\\n        time.sleep(60 if offroad else 5)\\n      continue\\n    d = uploader.next_file_to_upload()\\n    if d is None:  \\n      if allow_sleep:\\n        time.sleep(60 if offroad else 5)\\n      continue\\n    key, fn = d\\n    cloudlog.debug(\"upload %r over %s\", d, network_type)\\n    success = uploader.upload(key, fn)\\n    if success:\\n      backoff = 0.1\\n    elif allow_sleep:\\n      cloudlog.info(\"upload backoff %r\", backoff)\\n      time.sleep(backoff + random.uniform(0, backoff))\\n      backoff = min(backoff*2, 120)\\n    pm.send(\"uploaderState\", uploader.get_msg())\\n    cloudlog.info(\"upload done, success=%r\", success)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def uploader_fn(exit_event):\\n  params = Params()\\n  dongle_id = params.get(\"DongleId\", encoding='utf8')\\n  if dongle_id is None:\\n    cloudlog.info(\"uploader missing dongle_id\")\\n    raise Exception(\"uploader can't start without dongle id\")\\n  if TICI and not Path(\"/data/media\").is_mount():\\n    cloudlog.warning(\"NVME not mounted\")\\n  sm = messaging.SubMaster(['deviceState'])\\n  pm = messaging.PubMaster(['uploaderState'])\\n  uploader = Uploader(dongle_id, ROOT)\\n  backoff = 0.1\\n  while not exit_event.is_set():\\n    sm.update(0)\\n    offroad = params.get_bool(\"IsOffroad\")\\n    network_type = sm['deviceState'].networkType if not force_wifi else NetworkType.wifi\\n    if network_type == NetworkType.none:\\n      if allow_sleep:\\n        time.sleep(60 if offroad else 5)\\n      continue\\n    d = uploader.next_file_to_upload()\\n    if d is None:  \\n      if allow_sleep:\\n        time.sleep(60 if offroad else 5)\\n      continue\\n    key, fn = d\\n    cloudlog.debug(\"upload %r over %s\", d, network_type)\\n    success = uploader.upload(key, fn)\\n    if success:\\n      backoff = 0.1\\n    elif allow_sleep:\\n      cloudlog.info(\"upload backoff %r\", backoff)\\n      time.sleep(backoff + random.uniform(0, backoff))\\n      backoff = min(backoff*2, 120)\\n    pm.send(\"uploaderState\", uploader.get_msg())\\n    cloudlog.info(\"upload done, success=%r\", success)"
  },
  {
    "code": "def combine_expression(self, connector, sub_expressions):\\n        if connector == '^':\\n            return 'POW(%s)' % ','.join(sub_expressions)\\n        return super(DatabaseOperations, self).combine_expression(connector, sub_expressions)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #27681 -- Fixed binary &/| operators for negative values on MySQL.",
    "fixed_code": "def combine_expression(self, connector, sub_expressions):\\n        if connector == '^':\\n            return 'POW(%s)' % ','.join(sub_expressions)\\n        elif connector in ('&', '|'):\\n            return 'CONVERT(%s, SIGNED)' % connector.join(sub_expressions)\\n        return super(DatabaseOperations, self).combine_expression(connector, sub_expressions)"
  },
  {
    "code": "def main():\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        nopassword=dict(type='bool'),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        privilege=dict(type='int'),\\n        view=dict(aliases=['role']),\\n        sshkey=dict(),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection']),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'aggregate')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    if module.params['password'] and not module.params['configured_password']:\\n        warnings.append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_users = [x['name'] for x in want]\\n        have_users = [x['name'] for x in have]\\n        for item in set(have_users).difference(want_users):\\n            if item != 'admin':\\n                commands.append(user_del_cmd(item))\\n    result['commands'] = commands\\n    for cmd in commands:\\n        if 'no username admin' in cmd:\\n            module.fail_json(msg='cannot delete the `admin` account')\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix ios_user issues (#44904)",
    "fixed_code": "def main():\\n    element_spec = dict(\\n        name=dict(),\\n        configured_password=dict(no_log=True),\\n        nopassword=dict(type='bool'),\\n        update_password=dict(default='always', choices=['on_create', 'always']),\\n        privilege=dict(type='int'),\\n        view=dict(aliases=['role']),\\n        sshkey=dict(),\\n        state=dict(default='present', choices=['present', 'absent'])\\n    )\\n    aggregate_spec = deepcopy(element_spec)\\n    aggregate_spec['name'] = dict(required=True)\\n    remove_default_spec(aggregate_spec)\\n    argument_spec = dict(\\n        aggregate=dict(type='list', elements='dict', options=aggregate_spec, aliases=['users', 'collection']),\\n        purge=dict(type='bool', default=False)\\n    )\\n    argument_spec.update(element_spec)\\n    argument_spec.update(ios_argument_spec)\\n    mutually_exclusive = [('name', 'aggregate')]\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           mutually_exclusive=mutually_exclusive,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    if module.params['password'] and not module.params['configured_password']:\\n        warnings.append(\\n            'The \"password\" argument is used to authenticate the current connection. ' +\\n            'To set a user password use \"configured_password\" instead.'\\n        )\\n    check_args(module, warnings)\\n    result = {'changed': False}\\n    if warnings:\\n        result['warnings'] = warnings\\n    want = map_params_to_obj(module)\\n    have = map_config_to_obj(module)\\n    commands = map_obj_to_commands(update_objects(want, have), module)\\n    if module.params['purge']:\\n        want_users = [x['name'] for x in want]\\n        have_users = [x['name'] for x in have]\\n        for item in set(have_users).difference(want_users):\\n            if item != 'admin':\\n                commands.append(user_del_cmd(item))\\n    result['commands'] = commands\\n    if commands:\\n        if not module.check_mode:\\n            load_config(module, commands)\\n        result['changed'] = True\\n    module.exit_json(**result)"
  },
  {
    "code": "def get_objs_combined_axis(\\n\\tobjs, intersect: bool = False, axis=0, sort: bool = True\\n) -> Index:\\n\\tobs_idxes = [obj._get_axis(axis) for obj in objs]\\n\\treturn _get_combined_index(obs_idxes, intersect=intersect, sort=sort)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: concat not copying index and columns when copy=True (#31119)",
    "fixed_code": "def get_objs_combined_axis(\\n\\tobjs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\\n) -> Index:\\n\\tobs_idxes = [obj._get_axis(axis) for obj in objs]\\n\\treturn _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)"
  },
  {
    "code": "def visit_Str(self, node, **kwargs):\\n        name = self.env.add_tmp(node.s)\\n        return self.term_type(name, self.env)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def visit_Str(self, node, **kwargs):\\n        name = self.env.add_tmp(node.s)\\n        return self.term_type(name, self.env)"
  },
  {
    "code": "def do_upload(self, key, fn):\\n    try:\\n      url_resp = self.api.get(\"v1.4/\" + self.dongle_id + \"/upload_url/\", timeout=10, path=key, access_token=self.api.get_token())\\n      if url_resp.status_code == 412:\\n        self.last_resp = url_resp\\n        return\\n      url_resp_json = json.loads(url_resp.text)\\n      url = url_resp_json['url']\\n      headers = url_resp_json['headers']\\n      cloudlog.debug(\"upload_url v1.4 %s %s\", url, str(headers))\\n      if fake_upload:\\n        cloudlog.debug(\"*** WARNING, THIS IS A FAKE UPLOAD TO %s ***\" % url)\\n        class FakeResponse():",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_upload(self, key, fn):\\n    try:\\n      url_resp = self.api.get(\"v1.4/\" + self.dongle_id + \"/upload_url/\", timeout=10, path=key, access_token=self.api.get_token())\\n      if url_resp.status_code == 412:\\n        self.last_resp = url_resp\\n        return\\n      url_resp_json = json.loads(url_resp.text)\\n      url = url_resp_json['url']\\n      headers = url_resp_json['headers']\\n      cloudlog.debug(\"upload_url v1.4 %s %s\", url, str(headers))\\n      if fake_upload:\\n        cloudlog.debug(\"*** WARNING, THIS IS A FAKE UPLOAD TO %s ***\" % url)\\n        class FakeResponse():"
  },
  {
    "code": "def _ensure_frozen(nd_array_like, copy=False):\\n    if not isinstance(nd_array_like, FrozenNDArray):\\n        arr = np.asarray(nd_array_like, dtype=np.int_)\\n        if copy:\\n            arr = arr.copy()\\n        nd_array_like = arr.view(FrozenNDArray)\\n    else:\\n        if copy:\\n            nd_array_like = nd_array_like.copy()\\n    return nd_array_like",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ensure_frozen(nd_array_like, copy=False):\\n    if not isinstance(nd_array_like, FrozenNDArray):\\n        arr = np.asarray(nd_array_like, dtype=np.int_)\\n        if copy:\\n            arr = arr.copy()\\n        nd_array_like = arr.view(FrozenNDArray)\\n    else:\\n        if copy:\\n            nd_array_like = nd_array_like.copy()\\n    return nd_array_like"
  },
  {
    "code": "def run_validators(self, value):\\n        super(SimpleArrayField, self).run_validators(value)\\n        errors = []\\n        for index, item in enumerate(value):\\n            try:\\n                self.base_field.run_validators(item)\\n            except ValidationError as error:\\n                errors.append(prefix_validation_error(\\n                    error,\\n                    prefix=self.error_messages['item_invalid'],\\n                    code='item_invalid',\\n                    params={'nth': index},\\n                ))\\n        if errors:\\n            raise ValidationError(errors)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run_validators(self, value):\\n        super(SimpleArrayField, self).run_validators(value)\\n        errors = []\\n        for index, item in enumerate(value):\\n            try:\\n                self.base_field.run_validators(item)\\n            except ValidationError as error:\\n                errors.append(prefix_validation_error(\\n                    error,\\n                    prefix=self.error_messages['item_invalid'],\\n                    code='item_invalid',\\n                    params={'nth': index},\\n                ))\\n        if errors:\\n            raise ValidationError(errors)"
  },
  {
    "code": "def generate_perturbed_logarithm_dataset(size):\\n    return (np.random.randint(-50, 50, size=size) +\\n            50. * np.log(1 + np.arange(size)))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_perturbed_logarithm_dataset(size):\\n    return (np.random.randint(-50, 50, size=size) +\\n            50. * np.log(1 + np.arange(size)))"
  },
  {
    "code": "def _assume_role_with_saml(self, sts_client: boto3.client) -> Dict[str, Any]:\\n        saml_config = self.extra_config['assume_role_with_saml']\\n        principal_arn = saml_config['principal_arn']\\n        idp_auth_method = saml_config['idp_auth_method']\\n        if idp_auth_method == 'http_spegno_auth':\\n            saml_assertion = self._fetch_saml_assertion_using_http_spegno_auth(saml_config)\\n        else:\\n            raise NotImplementedError(\\n                f'idp_auth_method={idp_auth_method} in Connection {self.conn.conn_id} Extra.'\\n                'Currently only \"http_spegno_auth\" is supported, and must be specified.'\\n            )\\n        self.log.info(\"Doing sts_client.assume_role_with_saml to role_arn=%s\", self.role_arn)\\n        assume_role_kwargs = self.extra_config.get(\"assume_role_kwargs\", {})\\n        return sts_client.assume_role_with_saml(\\n            RoleArn=self.role_arn,\\n            PrincipalArn=principal_arn,\\n            SAMLAssertion=saml_assertion,\\n            **assume_role_kwargs,\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _assume_role_with_saml(self, sts_client: boto3.client) -> Dict[str, Any]:\\n        saml_config = self.extra_config['assume_role_with_saml']\\n        principal_arn = saml_config['principal_arn']\\n        idp_auth_method = saml_config['idp_auth_method']\\n        if idp_auth_method == 'http_spegno_auth':\\n            saml_assertion = self._fetch_saml_assertion_using_http_spegno_auth(saml_config)\\n        else:\\n            raise NotImplementedError(\\n                f'idp_auth_method={idp_auth_method} in Connection {self.conn.conn_id} Extra.'\\n                'Currently only \"http_spegno_auth\" is supported, and must be specified.'\\n            )\\n        self.log.info(\"Doing sts_client.assume_role_with_saml to role_arn=%s\", self.role_arn)\\n        assume_role_kwargs = self.extra_config.get(\"assume_role_kwargs\", {})\\n        return sts_client.assume_role_with_saml(\\n            RoleArn=self.role_arn,\\n            PrincipalArn=principal_arn,\\n            SAMLAssertion=saml_assertion,\\n            **assume_role_kwargs,\\n        )"
  },
  {
    "code": "def _has_complex_internals(self) -> bool:\\n        return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _has_complex_internals(self) -> bool:\\n        return True"
  },
  {
    "code": "def standardize_weights(y,\\n\\t\\t\\t\\t\\t\\tsample_weight=None,\\n\\t\\t\\t\\t\\t\\tclass_weight=None,\\n\\t\\t\\t\\t\\t\\tsample_weight_mode=None):\\n\\tif sample_weight_mode is not None:\\n\\t\\tif sample_weight_mode != 'temporal':\\n\\t\\t\\traise ValueError('\"sample_weight_mode '\\n\\t\\t\\t\\t\\t\\t\\t 'should be None or \"temporal\". '\\n\\t\\t\\t\\t\\t\\t\\t 'Found: ' + str(sample_weight_mode))\\n\\t\\tif len(y.shape) < 3:\\n\\t\\t\\traise ValueError('Found a sample_weight array for '\\n\\t\\t\\t\\t\\t\\t\\t 'an input with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(y.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'Timestep-wise sample weighting (use of '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight_mode=\"temporal\") is restricted to '\\n\\t\\t\\t\\t\\t\\t\\t 'outputs that are at least 3D, i.e. that have '\\n\\t\\t\\t\\t\\t\\t\\t 'a time dimension.')\\n\\t\\tif sample_weight is not None and len(sample_weight.shape) != 2:\\n\\t\\t\\traise ValueError('Found a sample_weight array with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'In order to use timestep-wise sample weighting, '\\n\\t\\t\\t\\t\\t\\t\\t 'you should pass a 2D sample_weight array.')\\n\\telse:\\n\\t\\tif sample_weight is not None and len(sample_weight.shape) != 1:\\n\\t\\t\\traise ValueError('Found a sample_weight array with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'In order to use timestep-wise sample weights, '\\n\\t\\t\\t\\t\\t\\t\\t 'you should specify '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight_mode=\"temporal\" '\\n\\t\\t\\t\\t\\t\\t\\t 'in compile(). If you just mean to use '\\n\\t\\t\\t\\t\\t\\t\\t 'sample-wise weights, make sure your '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight array is 1D.')\\n\\tif sample_weight is not None:\\n\\t\\tif len(sample_weight.shape) > len(y.shape):\\n\\t\\t\\traise ValueError('Found a sample_weight with shape' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) + '.'\\n\\t\\t\\t\\t\\t\\t\\t 'Expected sample_weight with rank '\\n\\t\\t\\t\\t\\t\\t\\t 'less than or equal to ' + str(len(y.shape)))\\n\\t\\tif y.shape[:sample_weight.ndim] != sample_weight.shape:\\n\\t\\t\\traise ValueError('Found a sample_weight array with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) +\\n\\t\\t\\t\\t\\t\\t\\t ' for an input with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(y.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight cannot be broadcast.')\\n\\tclass_sample_weight = None\\n\\tif isinstance(class_weight, dict):\\n\\t\\tif len(y.shape) > 2:\\n\\t\\t\\traise ValueError('`class_weight` not supported for '\\n\\t\\t\\t\\t\\t\\t\\t '3+ dimensional targets.')\\n\\t\\tif len(y.shape) == 2:\\n\\t\\t\\tif y.shape[1] > 1:\\n\\t\\t\\t\\ty_classes = np.argmax(y, axis=1)\\n\\t\\t\\telif y.shape[1] == 1:\\n\\t\\t\\t\\ty_classes = np.reshape(y, y.shape[0])\\n\\t\\telse:\\n\\t\\t\\ty_classes = y\\n\\t\\tclass_sample_weight = np.asarray(\\n\\t\\t\\t[class_weight[cls] for cls in y_classes if cls in class_weight])\\n\\t\\tif len(class_sample_weight) != len(y_classes):\\n\\t\\t\\texisting_classes = set(y_classes)\\n\\t\\t\\texisting_class_weight = set(class_weight.keys())\\n\\t\\t\\traise ValueError('`class_weight` must contain '\\n\\t\\t\\t\\t\\t\\t\\t 'all classes in the data.'\\n\\t\\t\\t\\t\\t\\t\\t ' The classes %s exist in the data but not in '\\n\\t\\t\\t\\t\\t\\t\\t '`class_weight`.'\\n\\t\\t\\t\\t\\t\\t\\t % (existing_classes - existing_class_weight))\\n\\tif sample_weight is not None and class_sample_weight is not None:\\n\\t\\treturn sample_weight * class_sample_weight\\n\\tif sample_weight is not None:\\n\\t\\treturn sample_weight\\n\\tif class_sample_weight is not None:\\n\\t\\treturn class_sample_weight\\n\\tif sample_weight_mode is None:\\n\\t\\treturn np.ones((y.shape[0],), dtype=K.floatx())\\n\\telse:\\n\\t\\treturn np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def standardize_weights(y,\\n\\t\\t\\t\\t\\t\\tsample_weight=None,\\n\\t\\t\\t\\t\\t\\tclass_weight=None,\\n\\t\\t\\t\\t\\t\\tsample_weight_mode=None):\\n\\tif sample_weight_mode is not None:\\n\\t\\tif sample_weight_mode != 'temporal':\\n\\t\\t\\traise ValueError('\"sample_weight_mode '\\n\\t\\t\\t\\t\\t\\t\\t 'should be None or \"temporal\". '\\n\\t\\t\\t\\t\\t\\t\\t 'Found: ' + str(sample_weight_mode))\\n\\t\\tif len(y.shape) < 3:\\n\\t\\t\\traise ValueError('Found a sample_weight array for '\\n\\t\\t\\t\\t\\t\\t\\t 'an input with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(y.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'Timestep-wise sample weighting (use of '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight_mode=\"temporal\") is restricted to '\\n\\t\\t\\t\\t\\t\\t\\t 'outputs that are at least 3D, i.e. that have '\\n\\t\\t\\t\\t\\t\\t\\t 'a time dimension.')\\n\\t\\tif sample_weight is not None and len(sample_weight.shape) != 2:\\n\\t\\t\\traise ValueError('Found a sample_weight array with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'In order to use timestep-wise sample weighting, '\\n\\t\\t\\t\\t\\t\\t\\t 'you should pass a 2D sample_weight array.')\\n\\telse:\\n\\t\\tif sample_weight is not None and len(sample_weight.shape) != 1:\\n\\t\\t\\traise ValueError('Found a sample_weight array with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'In order to use timestep-wise sample weights, '\\n\\t\\t\\t\\t\\t\\t\\t 'you should specify '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight_mode=\"temporal\" '\\n\\t\\t\\t\\t\\t\\t\\t 'in compile(). If you just mean to use '\\n\\t\\t\\t\\t\\t\\t\\t 'sample-wise weights, make sure your '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight array is 1D.')\\n\\tif sample_weight is not None:\\n\\t\\tif len(sample_weight.shape) > len(y.shape):\\n\\t\\t\\traise ValueError('Found a sample_weight with shape' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) + '.'\\n\\t\\t\\t\\t\\t\\t\\t 'Expected sample_weight with rank '\\n\\t\\t\\t\\t\\t\\t\\t 'less than or equal to ' + str(len(y.shape)))\\n\\t\\tif y.shape[:sample_weight.ndim] != sample_weight.shape:\\n\\t\\t\\traise ValueError('Found a sample_weight array with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(sample_weight.shape) +\\n\\t\\t\\t\\t\\t\\t\\t ' for an input with shape ' +\\n\\t\\t\\t\\t\\t\\t\\t str(y.shape) + '. '\\n\\t\\t\\t\\t\\t\\t\\t 'sample_weight cannot be broadcast.')\\n\\tclass_sample_weight = None\\n\\tif isinstance(class_weight, dict):\\n\\t\\tif len(y.shape) > 2:\\n\\t\\t\\traise ValueError('`class_weight` not supported for '\\n\\t\\t\\t\\t\\t\\t\\t '3+ dimensional targets.')\\n\\t\\tif len(y.shape) == 2:\\n\\t\\t\\tif y.shape[1] > 1:\\n\\t\\t\\t\\ty_classes = np.argmax(y, axis=1)\\n\\t\\t\\telif y.shape[1] == 1:\\n\\t\\t\\t\\ty_classes = np.reshape(y, y.shape[0])\\n\\t\\telse:\\n\\t\\t\\ty_classes = y\\n\\t\\tclass_sample_weight = np.asarray(\\n\\t\\t\\t[class_weight[cls] for cls in y_classes if cls in class_weight])\\n\\t\\tif len(class_sample_weight) != len(y_classes):\\n\\t\\t\\texisting_classes = set(y_classes)\\n\\t\\t\\texisting_class_weight = set(class_weight.keys())\\n\\t\\t\\traise ValueError('`class_weight` must contain '\\n\\t\\t\\t\\t\\t\\t\\t 'all classes in the data.'\\n\\t\\t\\t\\t\\t\\t\\t ' The classes %s exist in the data but not in '\\n\\t\\t\\t\\t\\t\\t\\t '`class_weight`.'\\n\\t\\t\\t\\t\\t\\t\\t % (existing_classes - existing_class_weight))\\n\\tif sample_weight is not None and class_sample_weight is not None:\\n\\t\\treturn sample_weight * class_sample_weight\\n\\tif sample_weight is not None:\\n\\t\\treturn sample_weight\\n\\tif class_sample_weight is not None:\\n\\t\\treturn class_sample_weight\\n\\tif sample_weight_mode is None:\\n\\t\\treturn np.ones((y.shape[0],), dtype=K.floatx())\\n\\telse:\\n\\t\\treturn np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())"
  },
  {
    "code": "def ensure_timezone(self):\\n        if self.connection is None:\\n            return False\\n        conn_timezone_name = self.connection.get_parameter_status('TimeZone')\\n        timezone_name = self.timezone_name\\n        if timezone_name and conn_timezone_name != timezone_name:\\n            with self.connection.cursor() as cursor:\\n                cursor.execute(self.ops.set_time_zone_sql(), [timezone_name])\\n            return True\\n        return False",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ensure_timezone(self):\\n        if self.connection is None:\\n            return False\\n        conn_timezone_name = self.connection.get_parameter_status('TimeZone')\\n        timezone_name = self.timezone_name\\n        if timezone_name and conn_timezone_name != timezone_name:\\n            with self.connection.cursor() as cursor:\\n                cursor.execute(self.ops.set_time_zone_sql(), [timezone_name])\\n            return True\\n        return False"
  },
  {
    "code": "def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',\\n                  n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,\\n                  max_iter=1000, n_jobs=1, check_input=True, verbose=0):\\n    if check_input:\\n        if algorithm == 'lasso_cd':\\n            dictionary = check_array(dictionary, order='C', dtype='float64')\\n            X = check_array(X, order='C', dtype='float64')\\n        else:\\n            dictionary = check_array(dictionary)\\n            X = check_array(X)\\n    n_samples, n_features = X.shape\\n    n_components = dictionary.shape[0]\\n    if gram is None and algorithm != 'threshold':\\n        gram = np.dot(dictionary, dictionary.T)\\n    if cov is None and algorithm != 'lasso_cd':\\n        copy_cov = False\\n        cov = np.dot(dictionary, X.T)\\n    if algorithm in ('lars', 'omp'):\\n        regularization = n_nonzero_coefs\\n        if regularization is None:\\n            regularization = min(max(n_features / 10, 1), n_components)\\n    else:\\n        regularization = alpha\\n        if regularization is None:\\n            regularization = 1.\\n    if n_jobs == 1 or algorithm == 'threshold':\\n        code = _sparse_encode(X,\\n                              dictionary, gram, cov=cov,\\n                              algorithm=algorithm,\\n                              regularization=regularization, copy_cov=copy_cov,\\n                              init=init,\\n                              max_iter=max_iter,\\n                              check_input=False,\\n                              verbose=verbose)\\n        if code.ndim == 1:\\n            code = code[np.newaxis, :]\\n        return code\\n    code = np.empty((n_samples, n_components))\\n    slices = list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))\\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\\n        delayed(_sparse_encode)(\\n            X[this_slice], dictionary, gram,\\n            cov[:, this_slice] if cov is not None else None,\\n            algorithm,\\n            regularization=regularization, copy_cov=copy_cov,\\n            init=init[this_slice] if init is not None else None,\\n            max_iter=max_iter,\\n            check_input=False)\\n        for this_slice in slices)\\n    for this_slice, this_view in zip(slices, code_views):\\n        code[this_slice] = this_view\\n    return code",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',\\n                  n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,\\n                  max_iter=1000, n_jobs=1, check_input=True, verbose=0):\\n    if check_input:\\n        if algorithm == 'lasso_cd':\\n            dictionary = check_array(dictionary, order='C', dtype='float64')\\n            X = check_array(X, order='C', dtype='float64')\\n        else:\\n            dictionary = check_array(dictionary)\\n            X = check_array(X)\\n    n_samples, n_features = X.shape\\n    n_components = dictionary.shape[0]\\n    if gram is None and algorithm != 'threshold':\\n        gram = np.dot(dictionary, dictionary.T)\\n    if cov is None and algorithm != 'lasso_cd':\\n        copy_cov = False\\n        cov = np.dot(dictionary, X.T)\\n    if algorithm in ('lars', 'omp'):\\n        regularization = n_nonzero_coefs\\n        if regularization is None:\\n            regularization = min(max(n_features / 10, 1), n_components)\\n    else:\\n        regularization = alpha\\n        if regularization is None:\\n            regularization = 1.\\n    if n_jobs == 1 or algorithm == 'threshold':\\n        code = _sparse_encode(X,\\n                              dictionary, gram, cov=cov,\\n                              algorithm=algorithm,\\n                              regularization=regularization, copy_cov=copy_cov,\\n                              init=init,\\n                              max_iter=max_iter,\\n                              check_input=False,\\n                              verbose=verbose)\\n        if code.ndim == 1:\\n            code = code[np.newaxis, :]\\n        return code\\n    code = np.empty((n_samples, n_components))\\n    slices = list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))\\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\\n        delayed(_sparse_encode)(\\n            X[this_slice], dictionary, gram,\\n            cov[:, this_slice] if cov is not None else None,\\n            algorithm,\\n            regularization=regularization, copy_cov=copy_cov,\\n            init=init[this_slice] if init is not None else None,\\n            max_iter=max_iter,\\n            check_input=False)\\n        for this_slice in slices)\\n    for this_slice, this_view in zip(slices, code_views):\\n        code[this_slice] = this_view\\n    return code"
  },
  {
    "code": "def __new__(cls, name, bases, attrs):\\n        super_new = super(ModelBase, cls).__new__\\n        parents = [b for b in bases if isinstance(b, ModelBase) and\\n                not (b.__name__ == 'NewBase' and b.__mro__ == (b, object))]\\n        if not parents:\\n            return super_new(cls, name, bases, attrs)\\n        module = attrs.pop('__module__')\\n        new_class = super_new(cls, name, bases, {'__module__': module})\\n        attr_meta = attrs.pop('Meta', None)\\n        abstract = getattr(attr_meta, 'abstract', False)\\n        if not attr_meta:\\n            meta = getattr(new_class, 'Meta', None)\\n        else:\\n            meta = attr_meta\\n        base_meta = getattr(new_class, '_meta', None)\\n        if getattr(meta, 'app_label', None) is None:\\n            model_module = sys.modules[new_class.__module__]\\n            kwargs = {\"app_label\": model_module.__name__.split('.')[-2]}\\n        else:\\n            kwargs = {}\\n        new_class.add_to_class('_meta', Options(meta, **kwargs))\\n        if not abstract:\\n            new_class.add_to_class('DoesNotExist', subclass_exception(str('DoesNotExist'),\\n                    tuple(x.DoesNotExist\\n                          for x in parents if hasattr(x, '_meta') and not x._meta.abstract)\\n                    or (ObjectDoesNotExist,),\\n                    module, attached_to=new_class))\\n            new_class.add_to_class('MultipleObjectsReturned', subclass_exception(str('MultipleObjectsReturned'),\\n                    tuple(x.MultipleObjectsReturned\\n                          for x in parents if hasattr(x, '_meta') and not x._meta.abstract)\\n                    or (MultipleObjectsReturned,),\\n                    module, attached_to=new_class))\\n            if base_meta and not base_meta.abstract:\\n                if not hasattr(meta, 'ordering'):\\n                    new_class._meta.ordering = base_meta.ordering\\n                if not hasattr(meta, 'get_latest_by'):\\n                    new_class._meta.get_latest_by = base_meta.get_latest_by\\n        is_proxy = new_class._meta.proxy\\n        if is_proxy and base_meta and base_meta.swapped:\\n            raise TypeError(\"%s cannot proxy the swapped model '%s'.\" % (name, base_meta.swapped))\\n        if getattr(new_class, '_default_manager', None):\\n            if not is_proxy:\\n                new_class._default_manager = None\\n                new_class._base_manager = None\\n            else:\\n                new_class._default_manager = new_class._default_manager._copy_to_model(new_class)\\n                new_class._base_manager = new_class._base_manager._copy_to_model(new_class)\\n        m = get_model(new_class._meta.app_label, name,\\n                      seed_cache=False, only_installed=False)\\n        if m is not None:\\n            return m\\n        for obj_name, obj in attrs.items():\\n            new_class.add_to_class(obj_name, obj)\\n        new_fields = new_class._meta.local_fields + \\\\n                     new_class._meta.local_many_to_many + \\\\n                     new_class._meta.virtual_fields\\n        field_names = set([f.name for f in new_fields])\\n        if is_proxy:\\n            base = None\\n            for parent in [cls for cls in parents if hasattr(cls, '_meta')]:\\n                if parent._meta.abstract:\\n                    if parent._meta.fields:\\n                        raise TypeError(\"Abstract base class containing model fields not permitted for proxy model '%s'.\" % name)\\n                    else:\\n                        continue\\n                if base is not None:\\n                    raise TypeError(\"Proxy model '%s' has more than one non-abstract model base class.\" % name)\\n                else:\\n                    base = parent\\n            if base is None:\\n                    raise TypeError(\"Proxy model '%s' has no non-abstract model base class.\" % name)\\n            if (new_class._meta.local_fields or\\n                    new_class._meta.local_many_to_many):\\n                raise FieldError(\"Proxy model '%s' contains model fields.\" % name)\\n            new_class._meta.setup_proxy(base)\\n            new_class._meta.concrete_model = base._meta.concrete_model\\n        else:\\n            new_class._meta.concrete_model = new_class\\n        o2o_map = dict([(f.rel.to, f) for f in new_class._meta.local_fields\\n                if isinstance(f, OneToOneField)])\\n        for base in parents:\\n            original_base = base\\n            if not hasattr(base, '_meta'):\\n                continue\\n            parent_fields = base._meta.local_fields + base._meta.local_many_to_many\\n            for field in parent_fields:\\n                if field.name in field_names:\\n                    raise FieldError('Local field %r in class %r clashes '\\n                                     'with field of similar name from '\\n                                     'base class %r' %\\n                                        (field.name, name, base.__name__))\\n            if not base._meta.abstract:\\n                base = base._meta.concrete_model\\n                if base in o2o_map:\\n                    field = o2o_map[base]\\n                elif not is_proxy:\\n                    attr_name = '%s_ptr' % base._meta.model_name\\n                    field = OneToOneField(base, name=attr_name,\\n                            auto_created=True, parent_link=True)\\n                    new_class.add_to_class(attr_name, field)\\n                else:\\n                    field = None\\n                new_class._meta.parents[base] = field\\n            else:\\n                for field in parent_fields:\\n                    new_class.add_to_class(field.name, copy.deepcopy(field))\\n                new_class._meta.parents.update(base._meta.parents)\\n            new_class.copy_managers(base._meta.abstract_managers)\\n            if is_proxy:\\n                new_class.copy_managers(original_base._meta.concrete_managers)\\n            for field in base._meta.virtual_fields:\\n                if base._meta.abstract and field.name in field_names:\\n                    raise FieldError('Local field %r in class %r clashes '\\\\n                                     'with field of similar name from '\\\\n                                     'abstract base class %r' % \\\\n                                        (field.name, name, base.__name__))\\n                new_class.add_to_class(field.name, copy.deepcopy(field))\\n        if abstract:\\n            attr_meta.abstract = False\\n            new_class.Meta = attr_meta\\n            return new_class\\n        new_class._prepare()\\n        register_models(new_class._meta.app_label, new_class)\\n        return get_model(new_class._meta.app_label, name,\\n                         seed_cache=False, only_installed=False)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19688 -- Allow model subclassing with a custom metaclass using six.with_metaclass",
    "fixed_code": "def __new__(cls, name, bases, attrs):\\n        super_new = super(ModelBase, cls).__new__\\n        if name == 'NewBase' and attrs == {}:\\n            return super_new(cls, name, bases, attrs)\\n        parents = [b for b in bases if isinstance(b, ModelBase) and\\n                not (b.__name__ == 'NewBase' and b.__mro__ == (b, object))]\\n        if not parents:\\n            return super_new(cls, name, bases, attrs)\\n        module = attrs.pop('__module__')\\n        new_class = super_new(cls, name, bases, {'__module__': module})\\n        attr_meta = attrs.pop('Meta', None)\\n        abstract = getattr(attr_meta, 'abstract', False)\\n        if not attr_meta:\\n            meta = getattr(new_class, 'Meta', None)\\n        else:\\n            meta = attr_meta\\n        base_meta = getattr(new_class, '_meta', None)\\n        if getattr(meta, 'app_label', None) is None:\\n            model_module = sys.modules[new_class.__module__]\\n            kwargs = {\"app_label\": model_module.__name__.split('.')[-2]}\\n        else:\\n            kwargs = {}\\n        new_class.add_to_class('_meta', Options(meta, **kwargs))\\n        if not abstract:\\n            new_class.add_to_class('DoesNotExist', subclass_exception(str('DoesNotExist'),\\n                    tuple(x.DoesNotExist\\n                          for x in parents if hasattr(x, '_meta') and not x._meta.abstract)\\n                    or (ObjectDoesNotExist,),\\n                    module, attached_to=new_class))\\n            new_class.add_to_class('MultipleObjectsReturned', subclass_exception(str('MultipleObjectsReturned'),\\n                    tuple(x.MultipleObjectsReturned\\n                          for x in parents if hasattr(x, '_meta') and not x._meta.abstract)\\n                    or (MultipleObjectsReturned,),\\n                    module, attached_to=new_class))\\n            if base_meta and not base_meta.abstract:\\n                if not hasattr(meta, 'ordering'):\\n                    new_class._meta.ordering = base_meta.ordering\\n                if not hasattr(meta, 'get_latest_by'):\\n                    new_class._meta.get_latest_by = base_meta.get_latest_by\\n        is_proxy = new_class._meta.proxy\\n        if is_proxy and base_meta and base_meta.swapped:\\n            raise TypeError(\"%s cannot proxy the swapped model '%s'.\" % (name, base_meta.swapped))\\n        if getattr(new_class, '_default_manager', None):\\n            if not is_proxy:\\n                new_class._default_manager = None\\n                new_class._base_manager = None\\n            else:\\n                new_class._default_manager = new_class._default_manager._copy_to_model(new_class)\\n                new_class._base_manager = new_class._base_manager._copy_to_model(new_class)\\n        m = get_model(new_class._meta.app_label, name,\\n                      seed_cache=False, only_installed=False)\\n        if m is not None:\\n            return m\\n        for obj_name, obj in attrs.items():\\n            new_class.add_to_class(obj_name, obj)\\n        new_fields = new_class._meta.local_fields + \\\\n                     new_class._meta.local_many_to_many + \\\\n                     new_class._meta.virtual_fields\\n        field_names = set([f.name for f in new_fields])\\n        if is_proxy:\\n            base = None\\n            for parent in [cls for cls in parents if hasattr(cls, '_meta')]:\\n                if parent._meta.abstract:\\n                    if parent._meta.fields:\\n                        raise TypeError(\"Abstract base class containing model fields not permitted for proxy model '%s'.\" % name)\\n                    else:\\n                        continue\\n                if base is not None:\\n                    raise TypeError(\"Proxy model '%s' has more than one non-abstract model base class.\" % name)\\n                else:\\n                    base = parent\\n            if base is None:\\n                    raise TypeError(\"Proxy model '%s' has no non-abstract model base class.\" % name)\\n            if (new_class._meta.local_fields or\\n                    new_class._meta.local_many_to_many):\\n                raise FieldError(\"Proxy model '%s' contains model fields.\" % name)\\n            new_class._meta.setup_proxy(base)\\n            new_class._meta.concrete_model = base._meta.concrete_model\\n        else:\\n            new_class._meta.concrete_model = new_class\\n        o2o_map = dict([(f.rel.to, f) for f in new_class._meta.local_fields\\n                if isinstance(f, OneToOneField)])\\n        for base in parents:\\n            original_base = base\\n            if not hasattr(base, '_meta'):\\n                continue\\n            parent_fields = base._meta.local_fields + base._meta.local_many_to_many\\n            for field in parent_fields:\\n                if field.name in field_names:\\n                    raise FieldError('Local field %r in class %r clashes '\\n                                     'with field of similar name from '\\n                                     'base class %r' %\\n                                        (field.name, name, base.__name__))\\n            if not base._meta.abstract:\\n                base = base._meta.concrete_model\\n                if base in o2o_map:\\n                    field = o2o_map[base]\\n                elif not is_proxy:\\n                    attr_name = '%s_ptr' % base._meta.model_name\\n                    field = OneToOneField(base, name=attr_name,\\n                            auto_created=True, parent_link=True)\\n                    new_class.add_to_class(attr_name, field)\\n                else:\\n                    field = None\\n                new_class._meta.parents[base] = field\\n            else:\\n                for field in parent_fields:\\n                    new_class.add_to_class(field.name, copy.deepcopy(field))\\n                new_class._meta.parents.update(base._meta.parents)\\n            new_class.copy_managers(base._meta.abstract_managers)\\n            if is_proxy:\\n                new_class.copy_managers(original_base._meta.concrete_managers)\\n            for field in base._meta.virtual_fields:\\n                if base._meta.abstract and field.name in field_names:\\n                    raise FieldError('Local field %r in class %r clashes '\\\\n                                     'with field of similar name from '\\\\n                                     'abstract base class %r' % \\\\n                                        (field.name, name, base.__name__))\\n                new_class.add_to_class(field.name, copy.deepcopy(field))\\n        if abstract:\\n            attr_meta.abstract = False\\n            new_class.Meta = attr_meta\\n            return new_class\\n        new_class._prepare()\\n        register_models(new_class._meta.app_label, new_class)\\n        return get_model(new_class._meta.app_label, name,\\n                         seed_cache=False, only_installed=False)"
  },
  {
    "code": "def setup(self, request, *args, **kwargs):\\n        self.request = request\\n        self.args = args\\n        self.kwargs = kwargs",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #20456 -- Moved initialization of HEAD method based on GET to the View.setup() for generic views.\\n\\nThis will ease unit testing of views since setup will essentially do\\neverything needed to set the view instance up (other than instantiating\\nit). Credit for idea goes to Vincent Prouillet.",
    "fixed_code": "def setup(self, request, *args, **kwargs):\\n        if hasattr(self, 'get') and not hasattr(self, 'head'):\\n            self.head = self.get\\n        self.request = request\\n        self.args = args\\n        self.kwargs = kwargs"
  },
  {
    "code": "def update_add(x, increment):\\n\\top = tf_state_ops.assign_add(x, increment)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_add(x, increment):\\n\\top = tf_state_ops.assign_add(x, increment)\\n\\twith tf.control_dependencies([op]):\\n\\t\\treturn tf.identity(x)"
  },
  {
    "code": "def uuid3(namespace, name):\\n    from hashlib import md5\\n    hash = md5(namespace.bytes + bytes(name, \"utf-8\")).digest()\\n    return UUID(bytes=bytes_(hash[:16]), version=3)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def uuid3(namespace, name):\\n    from hashlib import md5\\n    hash = md5(namespace.bytes + bytes(name, \"utf-8\")).digest()\\n    return UUID(bytes=bytes_(hash[:16]), version=3)"
  },
  {
    "code": "def run_luks_add_key(self, device, keyfile, passphrase, new_keyfile,\\n                         new_passphrase):\\n        device';\\n            authentication done using 'keyfile' or 'passphrase'.\\n            Raises ValueError when command fails.\\n        luksAddKey', device]\\n        if keyfile:\\n            args.extend(['--key-file', keyfile])\\n        else:\\n            data.append(passphrase)\\n        if new_keyfile:\\n            args.append(new_keyfile)\\n        else:\\n            data.extend([new_passphrase, new_passphrase])\\n        result = self._run_command(args, data='\\n'.join(data) or None)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while adding new LUKS keyslot to %s: %s'\\n                             % (device, result[STDERR]))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run_luks_add_key(self, device, keyfile, passphrase, new_keyfile,\\n                         new_passphrase):\\n        device';\\n            authentication done using 'keyfile' or 'passphrase'.\\n            Raises ValueError when command fails.\\n        luksAddKey', device]\\n        if keyfile:\\n            args.extend(['--key-file', keyfile])\\n        else:\\n            data.append(passphrase)\\n        if new_keyfile:\\n            args.append(new_keyfile)\\n        else:\\n            data.extend([new_passphrase, new_passphrase])\\n        result = self._run_command(args, data='\\n'.join(data) or None)\\n        if result[RETURN_CODE] != 0:\\n            raise ValueError('Error while adding new LUKS keyslot to %s: %s'\\n                             % (device, result[STDERR]))"
  },
  {
    "code": "def _readmodule(module, path, inpackage=None):\\n    ''\\n    if inpackage:\\n        fullmodule = \"%s.%s\" % (inpackage, module)\\n    else:\\n        fullmodule = module\\n    if fullmodule in _modules:\\n        return _modules[fullmodule]\\n    dict = {}\\n    if module in sys.builtin_module_names and not inpackage:\\n        _modules[module] = dict\\n        return dict\\n    i = module.rfind('.')\\n    if i >= 0:\\n        package = module[:i]\\n        submodule = module[i+1:]\\n        parent = _readmodule(package, path, inpackage)\\n        if inpackage:\\n            package = \"%s.%s\" % (inpackage, package)\\n        return _readmodule(submodule, parent['__path__'], package)\\n    f = None\\n    if inpackage:\\n        f, file, (suff, mode, type) = imp.find_module(module, path)\\n    else:\\n        f, file, (suff, mode, type) = imp.find_module(module, path + sys.path)\\n    if type == imp.PKG_DIRECTORY:\\n        dict['__path__'] = [file]\\n        path = [file] + path\\n        f, file, (suff, mode, type) = imp.find_module('__init__', [file])\\n    _modules[fullmodule] = dict\\n    if type != imp.PY_SOURCE:\\n        f.close()\\n        return dict\\n    stack = [] \\n    g = tokenize.generate_tokens(f.readline)\\n    try:\\n        for tokentype, token, start, end, line in g:\\n            if tokentype == DEDENT:\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n            elif token == 'def':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, meth_name, start, end, line = g.next()\\n                if tokentype != NAME:\\n                    continue \\n                if stack:\\n                    cur_class = stack[-1][0]\\n                    if isinstance(cur_class, Class):\\n                        cur_class._addmethod(meth_name, lineno)\\n                else:\\n                    dict[meth_name] = Function(module, meth_name, file, lineno)\\n                stack.append((None, thisindent)) \\n            elif token == 'class':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, class_name, start, end, line = g.next()\\n                if tokentype != NAME:\\n                    continue \\n                tokentype, token, start, end, line = g.next()\\n                inherit = None\\n                if token == '(':\\n                    names = [] \\n                    level = 1\\n                    super = [] \\n                    while True:\\n                        tokentype, token, start, end, line = g.next()\\n                        if token in (')', ',') and level == 1:\\n                            n = \"\".join(super)\\n                            if n in dict:\\n                                n = dict[n]\\n                            else:\\n                                c = n.split('.')\\n                                if len(c) > 1:\\n                                    m = c[-2]\\n                                    c = c[-1]\\n                                    if m in _modules:\\n                                        d = _modules[m]\\n                                        if c in d:\\n                                            n = d[c]\\n                            names.append(n)\\n                            super = []\\n                        if token == '(':\\n                            level += 1\\n                        elif token == ')':\\n                            level -= 1\\n                            if level == 0:\\n                                break\\n                        elif token == ',' and level == 1:\\n                            pass\\n                        elif tokentype in (NAME, OP) and level == 1:\\n                            super.append(token)\\n                    inherit = names\\n                cur_class = Class(fullmodule, class_name, inherit, file, lineno)\\n                if not stack:\\n                    dict[class_name] = cur_class\\n                stack.append((cur_class, thisindent))\\n            elif token == 'import' and start[1] == 0:\\n                modules = _getnamelist(g)\\n                for mod, mod2 in modules:\\n                    try:\\n                        if not inpackage:\\n                            _readmodule(mod, path)\\n                        else:\\n                            try:\\n                                _readmodule(mod, path, inpackage)\\n                            except ImportError:\\n                                _readmodule(mod, [])\\n                    except:\\n                        pass\\n            elif token == 'from' and start[1] == 0:\\n                mod, token = _getname(g)\\n                if not mod or token != \"import\":\\n                    continue\\n                names = _getnamelist(g)\\n                try:\\n                    d = _readmodule(mod, path, inpackage)\\n                except:\\n                    continue\\n                for n, n2 in names:\\n                    if n in d:\\n                        dict[n2 or n] = d[n]\\n                    elif n == '*':\\n                        for n in d:\\n                            if n[0] != '_':\\n                                dict[n] = d[n]\\n    except StopIteration:\\n        pass\\n    f.close()\\n    return dict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Bug #1560617: in pyclbr, return full module name not only for classes, but also for functions.",
    "fixed_code": "def _readmodule(module, path, inpackage=None):\\n    ''\\n    if inpackage:\\n        fullmodule = \"%s.%s\" % (inpackage, module)\\n    else:\\n        fullmodule = module\\n    if fullmodule in _modules:\\n        return _modules[fullmodule]\\n    dict = {}\\n    if module in sys.builtin_module_names and not inpackage:\\n        _modules[module] = dict\\n        return dict\\n    i = module.rfind('.')\\n    if i >= 0:\\n        package = module[:i]\\n        submodule = module[i+1:]\\n        parent = _readmodule(package, path, inpackage)\\n        if inpackage:\\n            package = \"%s.%s\" % (inpackage, package)\\n        return _readmodule(submodule, parent['__path__'], package)\\n    f = None\\n    if inpackage:\\n        f, file, (suff, mode, type) = imp.find_module(module, path)\\n    else:\\n        f, file, (suff, mode, type) = imp.find_module(module, path + sys.path)\\n    if type == imp.PKG_DIRECTORY:\\n        dict['__path__'] = [file]\\n        path = [file] + path\\n        f, file, (suff, mode, type) = imp.find_module('__init__', [file])\\n    _modules[fullmodule] = dict\\n    if type != imp.PY_SOURCE:\\n        f.close()\\n        return dict\\n    stack = [] \\n    g = tokenize.generate_tokens(f.readline)\\n    try:\\n        for tokentype, token, start, end, line in g:\\n            if tokentype == DEDENT:\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n            elif token == 'def':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, meth_name, start, end, line = g.next()\\n                if tokentype != NAME:\\n                    continue \\n                if stack:\\n                    cur_class = stack[-1][0]\\n                    if isinstance(cur_class, Class):\\n                        cur_class._addmethod(meth_name, lineno)\\n                else:\\n                    dict[meth_name] = Function(fullmodule, meth_name, file, lineno)\\n                stack.append((None, thisindent)) \\n            elif token == 'class':\\n                lineno, thisindent = start\\n                while stack and stack[-1][1] >= thisindent:\\n                    del stack[-1]\\n                tokentype, class_name, start, end, line = g.next()\\n                if tokentype != NAME:\\n                    continue \\n                tokentype, token, start, end, line = g.next()\\n                inherit = None\\n                if token == '(':\\n                    names = [] \\n                    level = 1\\n                    super = [] \\n                    while True:\\n                        tokentype, token, start, end, line = g.next()\\n                        if token in (')', ',') and level == 1:\\n                            n = \"\".join(super)\\n                            if n in dict:\\n                                n = dict[n]\\n                            else:\\n                                c = n.split('.')\\n                                if len(c) > 1:\\n                                    m = c[-2]\\n                                    c = c[-1]\\n                                    if m in _modules:\\n                                        d = _modules[m]\\n                                        if c in d:\\n                                            n = d[c]\\n                            names.append(n)\\n                            super = []\\n                        if token == '(':\\n                            level += 1\\n                        elif token == ')':\\n                            level -= 1\\n                            if level == 0:\\n                                break\\n                        elif token == ',' and level == 1:\\n                            pass\\n                        elif tokentype in (NAME, OP) and level == 1:\\n                            super.append(token)\\n                    inherit = names\\n                cur_class = Class(fullmodule, class_name, inherit, file, lineno)\\n                if not stack:\\n                    dict[class_name] = cur_class\\n                stack.append((cur_class, thisindent))\\n            elif token == 'import' and start[1] == 0:\\n                modules = _getnamelist(g)\\n                for mod, mod2 in modules:\\n                    try:\\n                        if not inpackage:\\n                            _readmodule(mod, path)\\n                        else:\\n                            try:\\n                                _readmodule(mod, path, inpackage)\\n                            except ImportError:\\n                                _readmodule(mod, [])\\n                    except:\\n                        pass\\n            elif token == 'from' and start[1] == 0:\\n                mod, token = _getname(g)\\n                if not mod or token != \"import\":\\n                    continue\\n                names = _getnamelist(g)\\n                try:\\n                    d = _readmodule(mod, path, inpackage)\\n                except:\\n                    continue\\n                for n, n2 in names:\\n                    if n in d:\\n                        dict[n2 or n] = d[n]\\n                    elif n == '*':\\n                        for n in d:\\n                            if n[0] != '_':\\n                                dict[n] = d[n]\\n    except StopIteration:\\n        pass\\n    f.close()\\n    return dict"
  },
  {
    "code": "def nlargest(arr, n, keep='first'):\\n    arr = _ensure_data_view(arr)\\n    return nsmallest(-arr, n, keep=keep)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def nlargest(arr, n, keep='first'):\\n    arr = _ensure_data_view(arr)\\n    return nsmallest(-arr, n, keep=keep)"
  },
  {
    "code": "def __getitem__(self, idx):\\n\\t\\tif not isinstance(idx, six.integer_types + (slice,)):\\n\\t\\t\\traise TypeError\\n\\t\\treturn list(self.__iter__())[idx]\\n\\t@property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, idx):\\n\\t\\tif not isinstance(idx, six.integer_types + (slice,)):\\n\\t\\t\\traise TypeError\\n\\t\\treturn list(self.__iter__())[idx]\\n\\t@property"
  },
  {
    "code": "def __init__(\\n        self, *, batch_id: Union[int, str], livy_conn_id: str = 'livy_default', **kwargs: Any\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self._livy_conn_id = livy_conn_id\\n        self._batch_id = batch_id\\n        self._livy_hook: Optional[LivyHook] = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Extend HTTP extra_options to LivyHook and operator (#14816)\\n\\nThe LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.",
    "fixed_code": "def __init__(\\n        self,\\n        *,\\n        batch_id: Union[int, str],\\n        livy_conn_id: str = 'livy_default',\\n        extra_options: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.batch_id = batch_id\\n        self._livy_conn_id = livy_conn_id\\n        self._livy_hook: Optional[LivyHook] = None\\n        self._extra_options = extra_options or {}"
  },
  {
    "code": "def copy_location(new_node, old_node):\\n    for attr in 'lineno', 'col_offset', 'end_lineno', 'end_col_offset':\\n        if attr in old_node._attributes and attr in new_node._attributes:\\n            value = getattr(old_node, attr, None)\\n            if value is not None:\\n                setattr(new_node, attr, value)\\n    return new_node",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy_location(new_node, old_node):\\n    for attr in 'lineno', 'col_offset', 'end_lineno', 'end_col_offset':\\n        if attr in old_node._attributes and attr in new_node._attributes:\\n            value = getattr(old_node, attr, None)\\n            if value is not None:\\n                setattr(new_node, attr, value)\\n    return new_node"
  },
  {
    "code": "def cmd_pasv(self, arg):\\n\\t\\twith socket.create_server((self.socket.getsockname()[0], 0)) as sock:\\n\\t\\t\\tsock.settimeout(TIMEOUT)\\n\\t\\t\\tport = sock.getsockname()[1]\\n\\t\\t\\tip = self.fake_pasv_server_ip\\n\\t\\t\\tip = ip.replace('.', ','); p1 = port / 256; p2 = port % 256\\n\\t\\t\\tself.push('227 entering passive mode (%s,%d,%d)' %(ip, p1, p2))\\n\\t\\t\\tconn, addr = sock.accept()\\n\\t\\t\\tself.dtp = self.dtp_handler(conn, baseclass=self)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cmd_pasv(self, arg):\\n\\t\\twith socket.create_server((self.socket.getsockname()[0], 0)) as sock:\\n\\t\\t\\tsock.settimeout(TIMEOUT)\\n\\t\\t\\tport = sock.getsockname()[1]\\n\\t\\t\\tip = self.fake_pasv_server_ip\\n\\t\\t\\tip = ip.replace('.', ','); p1 = port / 256; p2 = port % 256\\n\\t\\t\\tself.push('227 entering passive mode (%s,%d,%d)' %(ip, p1, p2))\\n\\t\\t\\tconn, addr = sock.accept()\\n\\t\\t\\tself.dtp = self.dtp_handler(conn, baseclass=self)"
  },
  {
    "code": "def slave(self):\\n        return self.shortcmd('SLAVE')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def slave(self):\\n        return self._shortcmd('SLAVE')"
  },
  {
    "code": "def _args_from_interpreter_flags():\\n    flag_opt_map = {\\n        'debug': 'd',\\n        'dont_write_bytecode': 'B',\\n        'no_user_site': 's',\\n        'no_site': 'S',\\n        'ignore_environment': 'E',\\n        'verbose': 'v',\\n        'bytes_warning': 'b',\\n        'quiet': 'q',\\n    }\\n    args = _optim_args_from_interpreter_flags()\\n    for flag, opt in flag_opt_map.items():\\n        v = getattr(sys.flags, flag)\\n        if v > 0:\\n            args.append('-' + opt * v)\\n    for opt in sys.warnoptions:\\n        args.append('-W' + opt)\\n    xoptions = getattr(sys, '_xoptions', {})\\n    if 'dev' in xoptions:\\n        args.extend(('-X', 'dev'))\\n    for opt in ('faulthandler', 'tracemalloc', 'importtime',\\n                'showalloccount', 'showrefcount'):\\n        if opt in xoptions:\\n            value = xoptions[opt]\\n            if value is True:\\n                arg = opt\\n            else:\\n                arg = '%s=%s' % (opt, value)\\n            args.extend(('-X', arg))\\n    return args",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-32230: Set sys.warnoptions with -X dev (#4820)\\n\\nRather than supporting dev mode directly in the warnings module, this\\ninstead adjusts the initialisation code to add an extra 'default'\\nentry to sys.warnoptions when dev mode is enabled.\\n\\nThis ensures that dev mode behaves *exactly* as if `-Wdefault` had\\nbeen passed on the command line, including in the way it interacts\\nwith `sys.warnoptions`, and with other command line flags like `-bb`.\\n\\nFix also bpo-20361: have -b & -bb options take precedence over any\\nother warnings options.\\n\\nPatch written by Nick Coghlan, with minor modifications of Victor Stinner.",
    "fixed_code": "def _args_from_interpreter_flags():\\n    flag_opt_map = {\\n        'debug': 'd',\\n        'dont_write_bytecode': 'B',\\n        'no_user_site': 's',\\n        'no_site': 'S',\\n        'ignore_environment': 'E',\\n        'verbose': 'v',\\n        'bytes_warning': 'b',\\n        'quiet': 'q',\\n    }\\n    args = _optim_args_from_interpreter_flags()\\n    for flag, opt in flag_opt_map.items():\\n        v = getattr(sys.flags, flag)\\n        if v > 0:\\n            args.append('-' + opt * v)\\n    warnopts = sys.warnoptions[:]\\n    bytes_warning = sys.flags.bytes_warning\\n    xoptions = getattr(sys, '_xoptions', {})\\n    dev_mode = ('dev' in xoptions)\\n    if bytes_warning > 1:\\n        warnopts.remove(\"error::BytesWarning\")\\n    elif bytes_warning:\\n        warnopts.remove(\"default::BytesWarning\")\\n    if dev_mode:\\n        warnopts.remove('default')\\n    for opt in warnopts:\\n        args.append('-W' + opt)\\n    if dev_mode:\\n        args.extend(('-X', 'dev'))\\n    for opt in ('faulthandler', 'tracemalloc', 'importtime',\\n                'showalloccount', 'showrefcount'):\\n        if opt in xoptions:\\n            value = xoptions[opt]\\n            if value is True:\\n                arg = opt\\n            else:\\n                arg = '%s=%s' % (opt, value)\\n            args.extend(('-X', arg))\\n    return args"
  },
  {
    "code": "def __dir__():\\n            return list(globals().keys() | {'Tester', 'testing'})\\n    else:\\n        from .testing import Tester\\n    from numpy._pytesttester import PytestTester\\n    test = PytestTester(__name__)\\n    del PytestTester",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __dir__():\\n            return list(globals().keys() | {'Tester', 'testing'})\\n    else:\\n        from .testing import Tester\\n    from numpy._pytesttester import PytestTester\\n    test = PytestTester(__name__)\\n    del PytestTester"
  },
  {
    "code": "def main():\\n    fields = {\\n        \"host\": {\"required\": False, \"type\": \"str\"},\\n        \"username\": {\"required\": False, \"type\": \"str\"},\\n        \"password\": {\"required\": False, \"type\": \"str\", \"default\": \"\", \"no_log\": True},\\n        \"vdom\": {\"required\": False, \"type\": \"str\", \"default\": \"root\"},\\n        \"https\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"ssl_verify\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"system_global\": {\\n            \"required\": False, \"type\": \"dict\", \"default\": None,\\n            \"options\": {\\n                \"admin_concurrent\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_console_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_https_pki_required\": {\"required\": False, \"type\": \"str\",\\n                                             \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_https_ssl_versions\": {\"required\": False, \"type\": \"list\",\\n                                             \"choices\": [\"tlsv1-0\", \"tlsv1-1\", \"tlsv1-2\"]},\\n                \"admin_lockout_duration\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_lockout_threshold\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_login_max\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_maintainer\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_port\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_restrict_local\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_scp\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_server_cert\": {\"required\": False, \"type\": \"str\"},\\n                \"admin_sport\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_ssh_grace_time\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_ssh_password\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_ssh_port\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_ssh_v1\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_telnet_port\": {\"required\": False, \"type\": \"int\"},\\n                \"admintimeout\": {\"required\": False, \"type\": \"int\"},\\n                \"alias\": {\"required\": False, \"type\": \"str\"},\\n                \"allow_traffic_redirect\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"enable\", \"disable\"]},\\n                \"anti_replay\": {\"required\": False, \"type\": \"str\",\\n                                \"choices\": [\"disable\", \"loose\", \"strict\"]},\\n                \"arp_max_entry\": {\"required\": False, \"type\": \"int\"},\\n                \"asymroute\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"enable\", \"disable\"]},\\n                \"auth_cert\": {\"required\": False, \"type\": \"str\"},\\n                \"auth_http_port\": {\"required\": False, \"type\": \"int\"},\\n                \"auth_https_port\": {\"required\": False, \"type\": \"int\"},\\n                \"auth_keepalive\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"auth_session_limit\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"block-new\", \"logout-inactive\"]},\\n                \"auto_auth_extension_device\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"av_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"av_failopen\": {\"required\": False, \"type\": \"str\",\\n                                \"choices\": [\"pass\", \"off\", \"one-shot\"]},\\n                \"av_failopen_session\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"batch_cmdb\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"block_session_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"br_fdb_max_entry\": {\"required\": False, \"type\": \"int\"},\\n                \"cert_chain_max\": {\"required\": False, \"type\": \"int\"},\\n                \"cfg_revert_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"cfg_save\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"automatic\", \"manual\", \"revert\"]},\\n                \"check_protocol_header\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"loose\", \"strict\"]},\\n                \"check_reset_range\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"strict\", \"disable\"]},\\n                \"cli_audit_log\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"clt_cert_req\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"compliance_check\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"compliance_check_time\": {\"required\": False, \"type\": \"str\"},\\n                \"cpu_use_threshold\": {\"required\": False, \"type\": \"int\"},\\n                \"csr_ca_attribute\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"daily_restart\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"device_identification_active_scan_delay\": {\"required\": False, \"type\": \"int\"},\\n                \"device_idle_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"dh_params\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"1024\", \"1536\", \"2048\",\\n                                          \"3072\", \"4096\", \"6144\",\\n                                          \"8192\"]},\\n                \"dnsproxy_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"dst\": {\"required\": False, \"type\": \"str\",\\n                        \"choices\": [\"enable\", \"disable\"]},\\n                \"endpoint_control_fds_access\": {\"required\": False, \"type\": \"str\",\\n                                                \"choices\": [\"enable\", \"disable\"]},\\n                \"endpoint_control_portal_port\": {\"required\": False, \"type\": \"int\"},\\n                \"failtime\": {\"required\": False, \"type\": \"int\"},\\n                \"fds_statistics\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"fds_statistics_period\": {\"required\": False, \"type\": \"int\"},\\n                \"fgd_alert_subscription\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"advisory\", \"latest-threat\", \"latest-virus\",\\n                                                       \"latest-attack\", \"new-antivirus-db\", \"new-attack-db\"]},\\n                \"fortiextender\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"fortiextender_data_port\": {\"required\": False, \"type\": \"int\"},\\n                \"fortiextender_vlan_mode\": {\"required\": False, \"type\": \"str\",\\n                                            \"choices\": [\"enable\", \"disable\"]},\\n                \"fortiservice_port\": {\"required\": False, \"type\": \"int\"},\\n                \"gui_certificates\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_custom_language\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_date_format\": {\"required\": False, \"type\": \"str\",\\n                                    \"choices\": [\"yyyy/MM/dd\", \"dd/MM/yyyy\", \"MM/dd/yyyy\",\\n                                                \"yyyy-MM-dd\", \"dd-MM-yyyy\", \"MM-dd-yyyy\"]},\\n                \"gui_device_latitude\": {\"required\": False, \"type\": \"str\"},\\n                \"gui_device_longitude\": {\"required\": False, \"type\": \"str\"},\\n                \"gui_display_hostname\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_ipv6\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_lines_per_page\": {\"required\": False, \"type\": \"int\"},\\n                \"gui_theme\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"green\", \"red\", \"blue\",\\n                                          \"melongene\", \"mariner\"]},\\n                \"gui_wireless_opensecurity\": {\"required\": False, \"type\": \"str\",\\n                                              \"choices\": [\"enable\", \"disable\"]},\\n                \"honor_df\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"enable\", \"disable\"]},\\n                \"hostname\": {\"required\": False, \"type\": \"str\"},\\n                \"igmp_state_limit\": {\"required\": False, \"type\": \"int\"},\\n                \"interval\": {\"required\": False, \"type\": \"int\"},\\n                \"ip_src_port_range\": {\"required\": False, \"type\": \"str\"},\\n                \"ips_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"ipsec_asic_offload\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"ipsec_hmac_offload\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"ipsec_soft_dec_async\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"ipv6_accept_dad\": {\"required\": False, \"type\": \"int\"},\\n                \"ipv6_allow_anycast_probe\": {\"required\": False, \"type\": \"str\",\\n                                             \"choices\": [\"enable\", \"disable\"]},\\n                \"language\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"english\", \"french\", \"spanish\",\\n                                         \"portuguese\", \"japanese\", \"trach\",\\n                                         \"simch\", \"korean\"]},\\n                \"ldapconntimeout\": {\"required\": False, \"type\": \"int\"},\\n                \"lldp_transmission\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"log_ssl_connection\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"log_uuid\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"disable\", \"policy-only\", \"extended\"]},\\n                \"login_timestamp\": {\"required\": False, \"type\": \"str\",\\n                                    \"choices\": [\"enable\", \"disable\"]},\\n                \"long_vdom_name\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"management_vdom\": {\"required\": False, \"type\": \"str\"},\\n                \"max_dlpstat_memory\": {\"required\": False, \"type\": \"int\"},\\n                \"max_route_cache_size\": {\"required\": False, \"type\": \"int\"},\\n                \"mc_ttl_notchange\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"memory_use_threshold_extreme\": {\"required\": False, \"type\": \"int\"},\\n                \"memory_use_threshold_green\": {\"required\": False, \"type\": \"int\"},\\n                \"memory_use_threshold_red\": {\"required\": False, \"type\": \"int\"},\\n                \"miglog_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"miglogd_children\": {\"required\": False, \"type\": \"int\"},\\n                \"multi_factor_authentication\": {\"required\": False, \"type\": \"str\",\\n                                                \"choices\": [\"optional\", \"mandatory\"]},\\n                \"multicast_forward\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"ndp_max_entry\": {\"required\": False, \"type\": \"int\"},\\n                \"per_user_bwl\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"policy_auth_concurrent\": {\"required\": False, \"type\": \"int\"},\\n                \"post_login_banner\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"disable\", \"enable\"]},\\n                \"pre_login_banner\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"private_data_encryption\": {\"required\": False, \"type\": \"str\",\\n                                            \"choices\": [\"disable\", \"enable\"]},\\n                \"proxy_auth_lifetime\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"proxy_auth_lifetime_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"proxy_auth_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"proxy_cipher_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                       \"choices\": [\"disable\", \"enable\"]},\\n                \"proxy_kxp_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                    \"choices\": [\"disable\", \"enable\"]},\\n                \"proxy_re_authentication_mode\": {\"required\": False, \"type\": \"str\",\\n                                                 \"choices\": [\"session\", \"traffic\", \"absolute\"]},\\n                \"proxy_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"radius_port\": {\"required\": False, \"type\": \"int\"},\\n                \"reboot_upon_config_restore\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"refresh\": {\"required\": False, \"type\": \"int\"},\\n                \"remoteauthtimeout\": {\"required\": False, \"type\": \"int\"},\\n                \"reset_sessionless_tcp\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"enable\", \"disable\"]},\\n                \"restart_time\": {\"required\": False, \"type\": \"str\"},\\n                \"revision_backup_on_logout\": {\"required\": False, \"type\": \"str\",\\n                                              \"choices\": [\"enable\", \"disable\"]},\\n                \"revision_image_auto_backup\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"scanunit_count\": {\"required\": False, \"type\": \"int\"},\\n                \"security_rating_result_submission\": {\"required\": False, \"type\": \"str\",\\n                                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"security_rating_run_on_schedule\": {\"required\": False, \"type\": \"str\",\\n                                                    \"choices\": [\"enable\", \"disable\"]},\\n                \"send_pmtu_icmp\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"snat_route_change\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"special_file_23_support\": {\"required\": False, \"type\": \"str\",\\n                                            \"choices\": [\"disable\", \"enable\"]},\\n                \"ssd_trim_date\": {\"required\": False, \"type\": \"int\"},\\n                \"ssd_trim_freq\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"never\", \"hourly\", \"daily\",\\n                                              \"weekly\", \"monthly\"]},\\n                \"ssd_trim_hour\": {\"required\": False, \"type\": \"int\"},\\n                \"ssd_trim_min\": {\"required\": False, \"type\": \"int\"},\\n                \"ssd_trim_weekday\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"sunday\", \"monday\", \"tuesday\",\\n                                                 \"wednesday\", \"thursday\", \"friday\",\\n                                                 \"saturday\"]},\\n                \"ssh_cbc_cipher\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"ssh_hmac_md5\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"ssh_kex_sha1\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"ssl_min_proto_version\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"SSLv3\", \"TLSv1\", \"TLSv1-1\",\\n                                                      \"TLSv1-2\"]},\\n                \"ssl_static_key_ciphers\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"enable\", \"disable\"]},\\n                \"sslvpn_cipher_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"sslvpn_kxp_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"sslvpn_max_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"sslvpn_plugin_version_check\": {\"required\": False, \"type\": \"str\",\\n                                                \"choices\": [\"enable\", \"disable\"]},\\n                \"strict_dirty_session_check\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"strong_crypto\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"switch_controller\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"disable\", \"enable\"]},\\n                \"switch_controller_reserved_network\": {\"required\": False, \"type\": \"str\"},\\n                \"sys_perf_log_interval\": {\"required\": False, \"type\": \"int\"},\\n                \"tcp_halfclose_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"tcp_halfopen_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"tcp_option\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"tcp_timewait_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"tftp\": {\"required\": False, \"type\": \"str\",\\n                         \"choices\": [\"enable\", \"disable\"]},\\n                \"timezone\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"01\", \"02\", \"03\",\\n                                         \"04\", \"05\", \"81\",\\n                                         \"06\", \"07\", \"08\",\\n                                         \"09\", \"10\", \"11\",\\n                                         \"12\", \"13\", \"74\",\\n                                         \"14\", \"77\", \"15\",\\n                                         \"87\", \"16\", \"17\",\\n                                         \"18\", \"19\", \"20\",\\n                                         \"75\", \"21\", \"22\",\\n                                         \"23\", \"24\", \"80\",\\n                                         \"79\", \"25\", \"26\",\\n                                         \"27\", \"28\", \"78\",\\n                                         \"29\", \"30\", \"31\",\\n                                         \"32\", \"33\", \"34\",\\n                                         \"35\", \"36\", \"37\",\\n                                         \"38\", \"83\", \"84\",\\n                                         \"40\", \"85\", \"41\",\\n                                         \"42\", \"43\", \"39\",\\n                                         \"44\", \"46\", \"47\",\\n                                         \"51\", \"48\", \"45\",\\n                                         \"49\", \"50\", \"52\",\\n                                         \"53\", \"54\", \"55\",\\n                                         \"56\", \"57\", \"58\",\\n                                         \"59\", \"60\", \"62\",\\n                                         \"63\", \"61\", \"64\",\\n                                         \"65\", \"66\", \"67\",\\n                                         \"68\", \"69\", \"70\",\\n                                         \"71\", \"72\", \"00\",\\n                                         \"82\", \"73\", \"86\",\\n                                         \"76\"]},\\n                \"tp_mc_skip_policy\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"traffic_priority\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"tos\", \"dscp\"]},\\n                \"traffic_priority_level\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"low\", \"medium\", \"high\"]},\\n                \"two_factor_email_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_fac_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_ftk_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_ftm_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_sms_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"udp_idle_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"user_server_cert\": {\"required\": False, \"type\": \"str\"},\\n                \"vdom_admin\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"vip_arp_range\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"unlimited\", \"restricted\"]},\\n                \"virtual_server_count\": {\"required\": False, \"type\": \"int\"},\\n                \"virtual_server_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                         \"choices\": [\"disable\", \"enable\"]},\\n                \"wad_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"wad_csvc_cs_count\": {\"required\": False, \"type\": \"int\"},\\n                \"wad_csvc_db_count\": {\"required\": False, \"type\": \"int\"},\\n                \"wad_source_affinity\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"disable\", \"enable\"]},\\n                \"wad_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"wifi_ca_certificate\": {\"required\": False, \"type\": \"str\"},\\n                \"wifi_certificate\": {\"required\": False, \"type\": \"str\"},\\n                \"wimax_4g_usb\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"wireless_controller\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"wireless_controller_port\": {\"required\": False, \"type\": \"int\"}\\n            }\\n        }\\n    }\\n    module = AnsibleModule(argument_spec=fields,\\n                           supports_check_mode=False)\\n    legacy_mode = 'host' in module.params and module.params['host'] is not None and \\\\n                  'username' in module.params and module.params['username'] is not None and \\\\n                  'password' in module.params and module.params['password'] is not None\\n    if not legacy_mode:\\n        if module._socket_path:\\n            connection = Connection(module._socket_path)\\n            fos = FortiOSHandler(connection)\\n            is_error, has_changed, result = fortios_system(module.params, fos)\\n        else:\\n            module.fail_json(**FAIL_SOCKET_MSG)\\n    else:\\n        try:\\n            from fortiosapi import FortiOSAPI\\n        except ImportError:\\n            module.fail_json(msg=\"fortiosapi module is required\")\\n        fos = FortiOSAPI()\\n        login(module.params, fos)\\n        is_error, has_changed, result = fortios_system(module.params, fos)\\n        fos.logout()\\n    if not is_error:\\n        module.exit_json(changed=has_changed, meta=result)\\n    else:\\n        module.fail_json(msg=\"Error in repo\", meta=result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fortinet's new module for fortios_system_global (#64817)",
    "fixed_code": "def main():\\n    fields = {\\n        \"host\": {\"required\": False, \"type\": \"str\"},\\n        \"username\": {\"required\": False, \"type\": \"str\"},\\n        \"password\": {\"required\": False, \"type\": \"str\", \"default\": \"\", \"no_log\": True},\\n        \"vdom\": {\"required\": False, \"type\": \"str\", \"default\": \"root\"},\\n        \"https\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"ssl_verify\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"system_global\": {\\n            \"required\": False, \"type\": \"dict\", \"default\": None,\\n            \"options\": {\\n                \"admin_concurrent\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_console_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_hsts_max_age\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_https_pki_required\": {\"required\": False, \"type\": \"str\",\\n                                             \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_https_redirect\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_https_ssl_versions\": {\"required\": False, \"type\": \"list\",\\n                                             \"choices\": [\"tlsv1-0\", \"tlsv1-1\", \"tlsv1-2\"]},\\n                \"admin_lockout_duration\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_lockout_threshold\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_login_max\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_maintainer\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_port\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_restrict_local\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_scp\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_server_cert\": {\"required\": False, \"type\": \"str\"},\\n                \"admin_sport\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_ssh_grace_time\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_ssh_password\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_ssh_port\": {\"required\": False, \"type\": \"int\"},\\n                \"admin_ssh_v1\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"admin_telnet_port\": {\"required\": False, \"type\": \"int\"},\\n                \"admintimeout\": {\"required\": False, \"type\": \"int\"},\\n                \"alias\": {\"required\": False, \"type\": \"str\"},\\n                \"allow_traffic_redirect\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"enable\", \"disable\"]},\\n                \"anti_replay\": {\"required\": False, \"type\": \"str\",\\n                                \"choices\": [\"disable\", \"loose\", \"strict\"]},\\n                \"arp_max_entry\": {\"required\": False, \"type\": \"int\"},\\n                \"asymroute\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"enable\", \"disable\"]},\\n                \"auth_cert\": {\"required\": False, \"type\": \"str\"},\\n                \"auth_http_port\": {\"required\": False, \"type\": \"int\"},\\n                \"auth_https_port\": {\"required\": False, \"type\": \"int\"},\\n                \"auth_keepalive\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"auth_session_limit\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"block-new\", \"logout-inactive\"]},\\n                \"auto_auth_extension_device\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"av_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"av_failopen\": {\"required\": False, \"type\": \"str\",\\n                                \"choices\": [\"pass\", \"off\", \"one-shot\"]},\\n                \"av_failopen_session\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"batch_cmdb\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"block_session_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"br_fdb_max_entry\": {\"required\": False, \"type\": \"int\"},\\n                \"cert_chain_max\": {\"required\": False, \"type\": \"int\"},\\n                \"cfg_revert_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"cfg_save\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"automatic\", \"manual\", \"revert\"]},\\n                \"check_protocol_header\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"loose\", \"strict\"]},\\n                \"check_reset_range\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"strict\", \"disable\"]},\\n                \"cli_audit_log\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"clt_cert_req\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"compliance_check\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"compliance_check_time\": {\"required\": False, \"type\": \"str\"},\\n                \"cpu_use_threshold\": {\"required\": False, \"type\": \"int\"},\\n                \"csr_ca_attribute\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"daily_restart\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"device_identification_active_scan_delay\": {\"required\": False, \"type\": \"int\"},\\n                \"device_idle_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"dh_params\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"1024\", \"1536\", \"2048\",\\n                                          \"3072\", \"4096\", \"6144\",\\n                                          \"8192\"]},\\n                \"dnsproxy_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"dst\": {\"required\": False, \"type\": \"str\",\\n                        \"choices\": [\"enable\", \"disable\"]},\\n                \"endpoint_control_fds_access\": {\"required\": False, \"type\": \"str\",\\n                                                \"choices\": [\"enable\", \"disable\"]},\\n                \"endpoint_control_portal_port\": {\"required\": False, \"type\": \"int\"},\\n                \"failtime\": {\"required\": False, \"type\": \"int\"},\\n                \"fds_statistics\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"fds_statistics_period\": {\"required\": False, \"type\": \"int\"},\\n                \"fgd_alert_subscription\": {\"required\": False, \"type\": \"list\",\\n                                           \"choices\": [\"advisory\", \"latest-threat\", \"latest-virus\",\\n                                                       \"latest-attack\", \"new-antivirus-db\", \"new-attack-db\"]},\\n                \"fortiextender\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"fortiextender_data_port\": {\"required\": False, \"type\": \"int\"},\\n                \"fortiextender_vlan_mode\": {\"required\": False, \"type\": \"str\",\\n                                            \"choices\": [\"enable\", \"disable\"]},\\n                \"fortiservice_port\": {\"required\": False, \"type\": \"int\"},\\n                \"gui_certificates\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_custom_language\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_date_format\": {\"required\": False, \"type\": \"str\",\\n                                    \"choices\": [\"yyyy/MM/dd\", \"dd/MM/yyyy\", \"MM/dd/yyyy\",\\n                                                \"yyyy-MM-dd\", \"dd-MM-yyyy\", \"MM-dd-yyyy\"]},\\n                \"gui_device_latitude\": {\"required\": False, \"type\": \"str\"},\\n                \"gui_device_longitude\": {\"required\": False, \"type\": \"str\"},\\n                \"gui_display_hostname\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_ipv6\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"enable\", \"disable\"]},\\n                \"gui_lines_per_page\": {\"required\": False, \"type\": \"int\"},\\n                \"gui_theme\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"green\", \"red\", \"blue\",\\n                                          \"melongene\", \"mariner\"]},\\n                \"gui_wireless_opensecurity\": {\"required\": False, \"type\": \"str\",\\n                                              \"choices\": [\"enable\", \"disable\"]},\\n                \"honor_df\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"enable\", \"disable\"]},\\n                \"hostname\": {\"required\": False, \"type\": \"str\"},\\n                \"igmp_state_limit\": {\"required\": False, \"type\": \"int\"},\\n                \"interval\": {\"required\": False, \"type\": \"int\"},\\n                \"ip_src_port_range\": {\"required\": False, \"type\": \"str\"},\\n                \"ips_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"ipsec_asic_offload\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"ipsec_hmac_offload\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"ipsec_soft_dec_async\": {\"required\": False, \"type\": \"str\",\\n                                         \"choices\": [\"enable\", \"disable\"]},\\n                \"ipv6_accept_dad\": {\"required\": False, \"type\": \"int\"},\\n                \"ipv6_allow_anycast_probe\": {\"required\": False, \"type\": \"str\",\\n                                             \"choices\": [\"enable\", \"disable\"]},\\n                \"language\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"english\", \"french\", \"spanish\",\\n                                         \"portuguese\", \"japanese\", \"trach\",\\n                                         \"simch\", \"korean\"]},\\n                \"ldapconntimeout\": {\"required\": False, \"type\": \"int\"},\\n                \"lldp_transmission\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"log_ssl_connection\": {\"required\": False, \"type\": \"str\",\\n                                       \"choices\": [\"enable\", \"disable\"]},\\n                \"log_uuid\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"disable\", \"policy-only\", \"extended\"]},\\n                \"login_timestamp\": {\"required\": False, \"type\": \"str\",\\n                                    \"choices\": [\"enable\", \"disable\"]},\\n                \"long_vdom_name\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"management_vdom\": {\"required\": False, \"type\": \"str\"},\\n                \"max_dlpstat_memory\": {\"required\": False, \"type\": \"int\"},\\n                \"max_route_cache_size\": {\"required\": False, \"type\": \"int\"},\\n                \"mc_ttl_notchange\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"memory_use_threshold_extreme\": {\"required\": False, \"type\": \"int\"},\\n                \"memory_use_threshold_green\": {\"required\": False, \"type\": \"int\"},\\n                \"memory_use_threshold_red\": {\"required\": False, \"type\": \"int\"},\\n                \"miglog_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"miglogd_children\": {\"required\": False, \"type\": \"int\"},\\n                \"multi_factor_authentication\": {\"required\": False, \"type\": \"str\",\\n                                                \"choices\": [\"optional\", \"mandatory\"]},\\n                \"multicast_forward\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"ndp_max_entry\": {\"required\": False, \"type\": \"int\"},\\n                \"per_user_bwl\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"policy_auth_concurrent\": {\"required\": False, \"type\": \"int\"},\\n                \"post_login_banner\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"disable\", \"enable\"]},\\n                \"pre_login_banner\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"private_data_encryption\": {\"required\": False, \"type\": \"str\",\\n                                            \"choices\": [\"disable\", \"enable\"]},\\n                \"proxy_auth_lifetime\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"proxy_auth_lifetime_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"proxy_auth_timeout\": {\"required\": False, \"type\": \"int\"},\\n                \"proxy_cipher_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                       \"choices\": [\"disable\", \"enable\"]},\\n                \"proxy_kxp_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                    \"choices\": [\"disable\", \"enable\"]},\\n                \"proxy_re_authentication_mode\": {\"required\": False, \"type\": \"str\",\\n                                                 \"choices\": [\"session\", \"traffic\", \"absolute\"]},\\n                \"proxy_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"radius_port\": {\"required\": False, \"type\": \"int\"},\\n                \"reboot_upon_config_restore\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"refresh\": {\"required\": False, \"type\": \"int\"},\\n                \"remoteauthtimeout\": {\"required\": False, \"type\": \"int\"},\\n                \"reset_sessionless_tcp\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"enable\", \"disable\"]},\\n                \"restart_time\": {\"required\": False, \"type\": \"str\"},\\n                \"revision_backup_on_logout\": {\"required\": False, \"type\": \"str\",\\n                                              \"choices\": [\"enable\", \"disable\"]},\\n                \"revision_image_auto_backup\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"scanunit_count\": {\"required\": False, \"type\": \"int\"},\\n                \"security_rating_result_submission\": {\"required\": False, \"type\": \"str\",\\n                                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"security_rating_run_on_schedule\": {\"required\": False, \"type\": \"str\",\\n                                                    \"choices\": [\"enable\", \"disable\"]},\\n                \"send_pmtu_icmp\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"snat_route_change\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"special_file_23_support\": {\"required\": False, \"type\": \"str\",\\n                                            \"choices\": [\"disable\", \"enable\"]},\\n                \"ssd_trim_date\": {\"required\": False, \"type\": \"int\"},\\n                \"ssd_trim_freq\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"never\", \"hourly\", \"daily\",\\n                                              \"weekly\", \"monthly\"]},\\n                \"ssd_trim_hour\": {\"required\": False, \"type\": \"int\"},\\n                \"ssd_trim_min\": {\"required\": False, \"type\": \"int\"},\\n                \"ssd_trim_weekday\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"sunday\", \"monday\", \"tuesday\",\\n                                                 \"wednesday\", \"thursday\", \"friday\",\\n                                                 \"saturday\"]},\\n                \"ssh_cbc_cipher\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"ssh_hmac_md5\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"ssh_kex_sha1\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"ssl_min_proto_version\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"SSLv3\", \"TLSv1\", \"TLSv1-1\",\\n                                                      \"TLSv1-2\"]},\\n                \"ssl_static_key_ciphers\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"enable\", \"disable\"]},\\n                \"sslvpn_cipher_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"sslvpn_kxp_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                     \"choices\": [\"enable\", \"disable\"]},\\n                \"sslvpn_max_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"sslvpn_plugin_version_check\": {\"required\": False, \"type\": \"str\",\\n                                                \"choices\": [\"enable\", \"disable\"]},\\n                \"strict_dirty_session_check\": {\"required\": False, \"type\": \"str\",\\n                                               \"choices\": [\"enable\", \"disable\"]},\\n                \"strong_crypto\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"switch_controller\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"disable\", \"enable\"]},\\n                \"switch_controller_reserved_network\": {\"required\": False, \"type\": \"str\"},\\n                \"sys_perf_log_interval\": {\"required\": False, \"type\": \"int\"},\\n                \"tcp_halfclose_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"tcp_halfopen_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"tcp_option\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"tcp_timewait_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"tftp\": {\"required\": False, \"type\": \"str\",\\n                         \"choices\": [\"enable\", \"disable\"]},\\n                \"timezone\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"01\", \"02\", \"03\",\\n                                         \"04\", \"05\", \"81\",\\n                                         \"06\", \"07\", \"08\",\\n                                         \"09\", \"10\", \"11\",\\n                                         \"12\", \"13\", \"74\",\\n                                         \"14\", \"77\", \"15\",\\n                                         \"87\", \"16\", \"17\",\\n                                         \"18\", \"19\", \"20\",\\n                                         \"75\", \"21\", \"22\",\\n                                         \"23\", \"24\", \"80\",\\n                                         \"79\", \"25\", \"26\",\\n                                         \"27\", \"28\", \"78\",\\n                                         \"29\", \"30\", \"31\",\\n                                         \"32\", \"33\", \"34\",\\n                                         \"35\", \"36\", \"37\",\\n                                         \"38\", \"83\", \"84\",\\n                                         \"40\", \"85\", \"41\",\\n                                         \"42\", \"43\", \"39\",\\n                                         \"44\", \"46\", \"47\",\\n                                         \"51\", \"48\", \"45\",\\n                                         \"49\", \"50\", \"52\",\\n                                         \"53\", \"54\", \"55\",\\n                                         \"56\", \"57\", \"58\",\\n                                         \"59\", \"60\", \"62\",\\n                                         \"63\", \"61\", \"64\",\\n                                         \"65\", \"66\", \"67\",\\n                                         \"68\", \"69\", \"70\",\\n                                         \"71\", \"72\", \"00\",\\n                                         \"82\", \"73\", \"86\",\\n                                         \"76\"]},\\n                \"tp_mc_skip_policy\": {\"required\": False, \"type\": \"str\",\\n                                      \"choices\": [\"enable\", \"disable\"]},\\n                \"traffic_priority\": {\"required\": False, \"type\": \"str\",\\n                                     \"choices\": [\"tos\", \"dscp\"]},\\n                \"traffic_priority_level\": {\"required\": False, \"type\": \"str\",\\n                                           \"choices\": [\"low\", \"medium\", \"high\"]},\\n                \"two_factor_email_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_fac_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_ftk_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_ftm_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"two_factor_sms_expiry\": {\"required\": False, \"type\": \"int\"},\\n                \"udp_idle_timer\": {\"required\": False, \"type\": \"int\"},\\n                \"user_server_cert\": {\"required\": False, \"type\": \"str\"},\\n                \"vdom_admin\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"vip_arp_range\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"unlimited\", \"restricted\"]},\\n                \"virtual_server_count\": {\"required\": False, \"type\": \"int\"},\\n                \"virtual_server_hardware_acceleration\": {\"required\": False, \"type\": \"str\",\\n                                                         \"choices\": [\"disable\", \"enable\"]},\\n                \"wad_affinity\": {\"required\": False, \"type\": \"str\"},\\n                \"wad_csvc_cs_count\": {\"required\": False, \"type\": \"int\"},\\n                \"wad_csvc_db_count\": {\"required\": False, \"type\": \"int\"},\\n                \"wad_source_affinity\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"disable\", \"enable\"]},\\n                \"wad_worker_count\": {\"required\": False, \"type\": \"int\"},\\n                \"wifi_ca_certificate\": {\"required\": False, \"type\": \"str\"},\\n                \"wifi_certificate\": {\"required\": False, \"type\": \"str\"},\\n                \"wimax_4g_usb\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"enable\", \"disable\"]},\\n                \"wireless_controller\": {\"required\": False, \"type\": \"str\",\\n                                        \"choices\": [\"enable\", \"disable\"]},\\n                \"wireless_controller_port\": {\"required\": False, \"type\": \"int\"}\\n            }\\n        }\\n    }\\n    module = AnsibleModule(argument_spec=fields,\\n                           supports_check_mode=False)\\n    legacy_mode = 'host' in module.params and module.params['host'] is not None and \\\\n                  'username' in module.params and module.params['username'] is not None and \\\\n                  'password' in module.params and module.params['password'] is not None\\n    if not legacy_mode:\\n        if module._socket_path:\\n            connection = Connection(module._socket_path)\\n            fos = FortiOSHandler(connection)\\n            is_error, has_changed, result = fortios_system(module.params, fos)\\n        else:\\n            module.fail_json(**FAIL_SOCKET_MSG)\\n    else:\\n        try:\\n            from fortiosapi import FortiOSAPI\\n        except ImportError:\\n            module.fail_json(msg=\"fortiosapi module is required\")\\n        fos = FortiOSAPI()\\n        login(module.params, fos)\\n        is_error, has_changed, result = fortios_system(module.params, fos)\\n        fos.logout()\\n    if not is_error:\\n        module.exit_json(changed=has_changed, meta=result)\\n    else:\\n        module.fail_json(msg=\"Error in repo\", meta=result)"
  },
  {
    "code": "def _signature_fromstr(cls, obj, s):\\n    Parameter = cls._parameter_cls\\n    clean_signature, self_parameter, last_positional_only = \\\\n        _signature_strip_non_python_syntax(s)\\n    program = \"def foo\" + clean_signature + \": pass\"\\n    try:\\n        module = ast.parse(program)\\n    except SyntaxError:\\n        module = None\\n    if not isinstance(module, ast.Module):\\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\\n    f = module.body[0]\\n    parameters = []\\n    empty = Parameter.empty\\n    invalid = object()\\n    module = None\\n    module_dict = {}\\n    module_name = getattr(obj, '__module__', None)\\n    if module_name:\\n        module = sys.modules.get(module_name, None)\\n        if module:\\n            module_dict = module.__dict__\\n    sys_module_dict = sys.modules",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "inspect: Fix getfullargspec() to not to follow __wrapped__ chains\\n\\nInitial patch by Nick Coghlan.",
    "fixed_code": "def _signature_fromstr(cls, obj, s, skip_bound_arg=True):\\n    Parameter = cls._parameter_cls\\n    clean_signature, self_parameter, last_positional_only = \\\\n        _signature_strip_non_python_syntax(s)\\n    program = \"def foo\" + clean_signature + \": pass\"\\n    try:\\n        module = ast.parse(program)\\n    except SyntaxError:\\n        module = None\\n    if not isinstance(module, ast.Module):\\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\\n    f = module.body[0]\\n    parameters = []\\n    empty = Parameter.empty\\n    invalid = object()\\n    module = None\\n    module_dict = {}\\n    module_name = getattr(obj, '__module__', None)\\n    if module_name:\\n        module = sys.modules.get(module_name, None)\\n        if module:\\n            module_dict = module.__dict__\\n    sys_module_dict = sys.modules"
  },
  {
    "code": "def select(condlist, choicelist, default=0):\\n    if len(condlist) != len(choicelist):\\n        raise ValueError(\\n            'list of cases must be same length as list of conditions')\\n    if len(condlist) == 0:\\n        warnings.warn(\"select with an empty condition list is not possible\"\\n                      \"and will be deprecated\",\\n                      DeprecationWarning)\\n        return np.asarray(default)[()]\\n    choicelist = [np.asarray(choice) for choice in choicelist]\\n    choicelist.append(np.asarray(default))\\n    dtype = np.result_type(*choicelist)\\n    condlist = np.broadcast_arrays(*condlist)\\n    choicelist = np.broadcast_arrays(*choicelist)\\n    deprecated_ints = False\\n    for i in range(len(condlist)):\\n        cond = condlist[i]\\n        if cond.dtype.type is not np.bool_:\\n            if np.issubdtype(cond.dtype, np.integer):\\n                condlist[i] = condlist[i].astype(bool)\\n                deprecated_ints = True\\n            else:\\n                raise ValueError(\\n                    'invalid entry in choicelist: should be boolean ndarray')\\n    if deprecated_ints:\\n        msg = \"select condlists containing integer ndarrays is deprecated \" \\\\n            \"and will be removed in the future. Use `.astype(bool)` to \" \\\\n            \"convert to bools.\"\\n        warnings.warn(msg, DeprecationWarning)\\n    if choicelist[0].ndim == 0:\\n        result_shape = condlist[0].shape\\n    else:\\n        result_shape = np.broadcast_arrays(condlist[0], choicelist[0])[0].shape\\n    result = np.full(result_shape, choicelist[-1], dtype)\\n    choicelist = choicelist[-2::-1]\\n    condlist = condlist[::-1]\\n    for choice, cond in zip(choicelist, condlist):\\n        np.copyto(result, choice, where=cond)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def select(condlist, choicelist, default=0):\\n    if len(condlist) != len(choicelist):\\n        raise ValueError(\\n            'list of cases must be same length as list of conditions')\\n    if len(condlist) == 0:\\n        warnings.warn(\"select with an empty condition list is not possible\"\\n                      \"and will be deprecated\",\\n                      DeprecationWarning)\\n        return np.asarray(default)[()]\\n    choicelist = [np.asarray(choice) for choice in choicelist]\\n    choicelist.append(np.asarray(default))\\n    dtype = np.result_type(*choicelist)\\n    condlist = np.broadcast_arrays(*condlist)\\n    choicelist = np.broadcast_arrays(*choicelist)\\n    deprecated_ints = False\\n    for i in range(len(condlist)):\\n        cond = condlist[i]\\n        if cond.dtype.type is not np.bool_:\\n            if np.issubdtype(cond.dtype, np.integer):\\n                condlist[i] = condlist[i].astype(bool)\\n                deprecated_ints = True\\n            else:\\n                raise ValueError(\\n                    'invalid entry in choicelist: should be boolean ndarray')\\n    if deprecated_ints:\\n        msg = \"select condlists containing integer ndarrays is deprecated \" \\\\n            \"and will be removed in the future. Use `.astype(bool)` to \" \\\\n            \"convert to bools.\"\\n        warnings.warn(msg, DeprecationWarning)\\n    if choicelist[0].ndim == 0:\\n        result_shape = condlist[0].shape\\n    else:\\n        result_shape = np.broadcast_arrays(condlist[0], choicelist[0])[0].shape\\n    result = np.full(result_shape, choicelist[-1], dtype)\\n    choicelist = choicelist[-2::-1]\\n    condlist = condlist[::-1]\\n    for choice, cond in zip(choicelist, condlist):\\n        np.copyto(result, choice, where=cond)\\n    return result"
  },
  {
    "code": "def _check_object_id_field(self):\\n        try:\\n            self.model._meta.get_field(self.fk_field)\\n        except FieldDoesNotExist:\\n            return [\\n                checks.Error(\\n                    \"The GenericForeignKey object ID references the non-existent field '%s'.\" % self.fk_field,\\n                    hint=None,\\n                    obj=self,\\n                    id='contenttypes.E002',\\n                )\\n            ]\\n        else:\\n            return []",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_object_id_field(self):\\n        try:\\n            self.model._meta.get_field(self.fk_field)\\n        except FieldDoesNotExist:\\n            return [\\n                checks.Error(\\n                    \"The GenericForeignKey object ID references the non-existent field '%s'.\" % self.fk_field,\\n                    hint=None,\\n                    obj=self,\\n                    id='contenttypes.E002',\\n                )\\n            ]\\n        else:\\n            return []"
  },
  {
    "code": "def pairwise_distances_argmin(X, Y=None, axis=1, metric=\"euclidean\",\\n                              batch_size_x=500, batch_size_y=500,\\n                              **kwargs):\\n    dist_func = None\\n    if metric in PAIRWISE_DISTANCE_FUNCTIONS:\\n        dist_func = PAIRWISE_DISTANCE_FUNCTIONS[metric]\\n    elif not callable(metric):\\n        raise ValueError(\"'metric' must be a string or a callable\")\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        batch_size_x, batch_size_y = batch_size_y, batch_size_x\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(np.infty)\\n    for chunk_x in gen_batches(X.shape[0], batch_size_x):\\n        X_chunk = X[chunk_x, :]\\n        for chunk_y in gen_batches(Y.shape[0], batch_size_y):\\n            Y_chunk = Y[chunk_y, :]\\n            if dist_func is not None:\\n                if metric == 'euclidean':  \\n                    tvar = np.dot(X_chunk, Y_chunk.T)\\n                    tvar *= -2\\n                    tvar += (X_chunk * X_chunk).sum(axis=1)[:, np.newaxis]\\n                    tvar += (Y_chunk * Y_chunk).sum(axis=1)[np.newaxis, :]\\n                    np.maximum(tvar, 0, tvar)\\n                else:\\n                    tvar = dist_func(X_chunk, Y_chunk, **kwargs)\\n            else:\\n                tvar = np.empty((X_chunk.shape[0], Y_chunk.shape[0]),\\n                                dtype='float')\\n                for n_x in range(X_chunk.shape[0]):\\n                    start = 0\\n                    if X is Y:\\n                        start = n_x\\n                    for n_y in range(start, Y_chunk.shape[0]):\\n                        tvar[n_x, n_y] = metric(X_chunk[n_x], Y_chunk[n_y],\\n                                                **kwargs)\\n                        if X is Y:\\n                            tvar[n_y, n_x] = tvar[n_x, n_y]\\n            min_indices = tvar.argmin(axis=1)\\n            min_values = tvar[range(chunk_x.stop - chunk_x.start), min_indices]\\n            flags = values[chunk_x] > min_values\\n            indices[chunk_x] = np.where(\\n                flags, min_indices + chunk_y.start, indices[chunk_x])\\n            values[chunk_x] = np.where(\\n                flags, min_values, values[chunk_x])\\n    if metric == \"euclidean\" and not kwargs.get(\"squared\", False):\\n        values = np.sqrt(values)\\n    return indices, values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Code cleanup in pairwise_distances_argmin",
    "fixed_code": "def pairwise_distances_argmin(X, Y=None, axis=1, metric=\"euclidean\",\\n                              batch_size_x=500, batch_size_y=500,\\n                              **kwargs):\\n    dist_func = None\\n    if metric in PAIRWISE_DISTANCE_FUNCTIONS:\\n        dist_func = PAIRWISE_DISTANCE_FUNCTIONS[metric]\\n    elif not callable(metric):\\n        raise ValueError(\"'metric' must be a string or a callable\")\\n    X, Y = check_pairwise_arrays(X, Y)\\n    if axis == 0:\\n        X, Y = Y, X\\n        batch_size_x, batch_size_y = batch_size_y, batch_size_x\\n    indices = np.empty(X.shape[0], dtype='int32')\\n    values = np.empty(X.shape[0])\\n    values.fill(np.infty)\\n    for chunk_x in gen_batches(X.shape[0], batch_size_x):\\n        X_chunk = X[chunk_x, :]\\n        for chunk_y in gen_batches(Y.shape[0], batch_size_y):\\n            Y_chunk = Y[chunk_y, :]\\n            if dist_func is not None:\\n                if metric == 'euclidean':  \\n                    dist_chunk = np.dot(X_chunk, Y_chunk.T)\\n                    dist_chunk *= -2\\n                    dist_chunk += (X_chunk * X_chunk\\n                                   ).sum(axis=1)[:, np.newaxis]\\n                    dist_chunk += (Y_chunk * Y_chunk\\n                                   ).sum(axis=1)[np.newaxis, :]\\n                    np.maximum(dist_chunk, 0, dist_chunk)\\n                else:\\n                    dist_chunk = dist_func(X_chunk, Y_chunk, **kwargs)\\n            else:\\n                dist_chunk = np.empty((X_chunk.shape[0], Y_chunk.shape[0]),\\n                                dtype='float')\\n                for n_x in range(X_chunk.shape[0]):\\n                    start = 0\\n                    if X is Y:\\n                        start = n_x\\n                    for n_y in range(start, Y_chunk.shape[0]):\\n                        dist_chunk[n_x, n_y] = metric(X_chunk[n_x],\\n                                                      Y_chunk[n_y],\\n                                                      **kwargs)\\n                        if X is Y:\\n                            dist_chunk[n_y, n_x] = dist_chunk[n_x, n_y]\\n            min_indices = dist_chunk.argmin(axis=1)\\n            min_values = dist_chunk[range(chunk_x.stop - chunk_x.start),\\n                                    min_indices]\\n            flags = values[chunk_x] > min_values\\n            indices[chunk_x] = np.where(\\n                flags, min_indices + chunk_y.start, indices[chunk_x])\\n            values[chunk_x] = np.where(\\n                flags, min_values, values[chunk_x])\\n    if metric == \"euclidean\" and not kwargs.get(\"squared\", False):\\n        values = np.sqrt(values)\\n    return indices, values"
  },
  {
    "code": "def _mkstemp_inner(dir, pre, suf, flags):\\n    names = _get_candidate_names()\\n    for seq in range(TMP_MAX):\\n        name = next(names)\\n        file = _os.path.join(dir, pre + name + suf)\\n        try:\\n            fd = _os.open(file, flags, 0o600)\\n            _set_cloexec(fd)\\n            return (fd, _os.path.abspath(file))\\n        except FileExistsError:\\n            continue    \\n    raise FileExistsError(_errno.EEXIST,\\n                          \"No usable temporary file name found\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _mkstemp_inner(dir, pre, suf, flags):\\n    names = _get_candidate_names()\\n    for seq in range(TMP_MAX):\\n        name = next(names)\\n        file = _os.path.join(dir, pre + name + suf)\\n        try:\\n            fd = _os.open(file, flags, 0o600)\\n            _set_cloexec(fd)\\n            return (fd, _os.path.abspath(file))\\n        except FileExistsError:\\n            continue    \\n    raise FileExistsError(_errno.EEXIST,\\n                          \"No usable temporary file name found\")"
  },
  {
    "code": "def commit(self, body):\\n        resp = self.connection.projects().commit(\\n            projectId=self.project_id, body=body).execute()\\n        return resp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4014] Change DatastoreHook and add tests (#4842)\\n\\n- update default used version for connecting to the Admin API from v1beta1 to v1\\n- move the establishment of the connection to the function calls instead of the hook init\\n- change get_conn signature to be able to pass an is_admin arg to set an admin connection\\n- rename GoogleCloudBaseHook._authorize function to GoogleCloudBaseHook.authorize\\n- rename the `partialKeys` argument of function `allocate_ids` to `partial_keys`.\\n- add tests\\n- update docs\\n- refactor code\\n\\nMove version attribute from get_conn to __init__\\n\\n- revert renaming of authorize function\\n- improve docs\\n- refactor code",
    "fixed_code": "def commit(self, body):\\n        conn = self.get_conn()\\n        resp = conn.projects().commit(projectId=self.project_id, body=body).execute()\\n        return resp"
  },
  {
    "code": "def start_java_dataflow(self, job_name, variables, jar, job_class=None,\\n                            append_job_name=True, multiple_jobs=False):\\n        name = self._build_dataflow_job_name(job_name, append_job_name)\\n        variables['jobName'] = name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def start_java_dataflow(self, job_name, variables, jar, job_class=None,\\n                            append_job_name=True, multiple_jobs=False):\\n        name = self._build_dataflow_job_name(job_name, append_job_name)\\n        variables['jobName'] = name"
  },
  {
    "code": "def _meets_requirements(self, version, requirements, parent):\\n\\t\\top_map = {\\n\\t\\t\\t'!=': operator.ne,\\n\\t\\t\\t'==': operator.eq,\\n\\t\\t\\t'=': operator.eq,\\n\\t\\t\\t'>=': operator.ge,\\n\\t\\t\\t'>': operator.gt,\\n\\t\\t\\t'<=': operator.le,\\n\\t\\t\\t'<': operator.lt,\\n\\t\\t}\\n\\t\\tfor req in list(requirements.split(',')):\\n\\t\\t\\top_pos = 2 if len(req) > 1 and req[1] == '=' else 1\\n\\t\\t\\top = op_map.get(req[:op_pos])\\n\\t\\t\\trequirement = req[op_pos:]\\n\\t\\t\\tif not op:\\n\\t\\t\\t\\trequirement = req\\n\\t\\t\\t\\top = operator.eq\\n\\t\\t\\tif parent and version == '*' and requirement != '*':\\n\\t\\t\\t\\tdisplay.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"install does not have a version set, the collection may not work.\"\\n\\t\\t\\t\\t\\t\\t\\t\\t% (to_text(self), req, parent))\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telif requirement == '*' or version == '*':\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif not op(LooseVersion(version), LooseVersion(requirement)):\\n\\t\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\t@staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _meets_requirements(self, version, requirements, parent):\\n\\t\\top_map = {\\n\\t\\t\\t'!=': operator.ne,\\n\\t\\t\\t'==': operator.eq,\\n\\t\\t\\t'=': operator.eq,\\n\\t\\t\\t'>=': operator.ge,\\n\\t\\t\\t'>': operator.gt,\\n\\t\\t\\t'<=': operator.le,\\n\\t\\t\\t'<': operator.lt,\\n\\t\\t}\\n\\t\\tfor req in list(requirements.split(',')):\\n\\t\\t\\top_pos = 2 if len(req) > 1 and req[1] == '=' else 1\\n\\t\\t\\top = op_map.get(req[:op_pos])\\n\\t\\t\\trequirement = req[op_pos:]\\n\\t\\t\\tif not op:\\n\\t\\t\\t\\trequirement = req\\n\\t\\t\\t\\top = operator.eq\\n\\t\\t\\tif parent and version == '*' and requirement != '*':\\n\\t\\t\\t\\tdisplay.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\"install does not have a version set, the collection may not work.\"\\n\\t\\t\\t\\t\\t\\t\\t\\t% (to_text(self), req, parent))\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\telif requirement == '*' or version == '*':\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif not op(LooseVersion(version), LooseVersion(requirement)):\\n\\t\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\t@staticmethod"
  },
  {
    "code": "def aggregate(self, func, *args, **kwargs):\\n        raise NotImplementedError",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def aggregate(self, func, *args, **kwargs):\\n        raise NotImplementedError"
  },
  {
    "code": "def protocol_df_chunk_to_pandas(df: DataFrameXchg) -> pd.DataFrame:\\n    columns: dict[str, Any] = {}\\n    buffers = []  \\n    for name in df.column_names():\\n        if not isinstance(name, str):\\n            raise ValueError(f\"Column {name} is not a string\")\\n        if name in columns:\\n            raise ValueError(f\"Column {name} is not unique\")\\n        col = df.get_column_by_name(name)\\n        dtype = col.dtype[0]\\n        if dtype in (\\n            DtypeKind.INT,\\n            DtypeKind.UINT,\\n            DtypeKind.FLOAT,\\n            DtypeKind.BOOL,\\n        ):\\n            columns[name], buf = primitive_column_to_ndarray(col)\\n        elif dtype == DtypeKind.CATEGORICAL:\\n            columns[name], buf = categorical_column_to_series(col)\\n        elif dtype == DtypeKind.STRING:\\n            columns[name], buf = string_column_to_ndarray(col)\\n        elif dtype == DtypeKind.DATETIME:\\n            columns[name], buf = datetime_column_to_ndarray(col)\\n        else:\\n            raise NotImplementedError(f\"Data type {dtype} not handled yet\")\\n        buffers.append(buf)\\n    pandas_df = pd.DataFrame(columns)\\n    pandas_df.attrs[\"_INTERCHANGE_PROTOCOL_BUFFERS\"] = buffers\\n    return pandas_df",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def protocol_df_chunk_to_pandas(df: DataFrameXchg) -> pd.DataFrame:\\n    columns: dict[str, Any] = {}\\n    buffers = []  \\n    for name in df.column_names():\\n        if not isinstance(name, str):\\n            raise ValueError(f\"Column {name} is not a string\")\\n        if name in columns:\\n            raise ValueError(f\"Column {name} is not unique\")\\n        col = df.get_column_by_name(name)\\n        dtype = col.dtype[0]\\n        if dtype in (\\n            DtypeKind.INT,\\n            DtypeKind.UINT,\\n            DtypeKind.FLOAT,\\n            DtypeKind.BOOL,\\n        ):\\n            columns[name], buf = primitive_column_to_ndarray(col)\\n        elif dtype == DtypeKind.CATEGORICAL:\\n            columns[name], buf = categorical_column_to_series(col)\\n        elif dtype == DtypeKind.STRING:\\n            columns[name], buf = string_column_to_ndarray(col)\\n        elif dtype == DtypeKind.DATETIME:\\n            columns[name], buf = datetime_column_to_ndarray(col)\\n        else:\\n            raise NotImplementedError(f\"Data type {dtype} not handled yet\")\\n        buffers.append(buf)\\n    pandas_df = pd.DataFrame(columns)\\n    pandas_df.attrs[\"_INTERCHANGE_PROTOCOL_BUFFERS\"] = buffers\\n    return pandas_df"
  },
  {
    "code": "def send_commenting_notifications(self, changes):\\n\\t\\trelevant_comment_ids = []\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\\n\\t\\tif (not changes['new_comments']\\n\\t\\t\\t\\tand not changes['deleted_comments']\\n\\t\\t\\t\\tand not changes['resolved_comments']\\n\\t\\t\\t\\tand not changes['new_replies']):\\n\\t\\t\\treturn\\n\\t\\tsubscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\\n\\t\\tglobal_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\\n\\t\\treplies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\\n\\t\\tcomments = Comment.objects.filter(id__in=relevant_comment_ids)\\n\\t\\tthread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\\n\\t\\t\\tPrefetch('comment_replies', queryset=replies),\\n\\t\\t\\tPrefetch(COMMENTS_RELATION_NAME, queryset=comments)\\n\\t\\t).exclude(\\n\\t\\t\\tQ(comment_replies__isnull=True) & Q(**{('%s__isnull' % COMMENTS_RELATION_NAME): True})\\n\\t\\t)\\n\\t\\tif not (global_recipient_users or thread_users):\\n\\t\\t\\treturn\\n\\t\\tthread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\\n\\t\\tmailed_users = set()\\n\\t\\tfor current_user, current_threads in thread_users:\\n\\t\\t\\tif current_user in mailed_users:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tusers = [current_user]\\n\\t\\t\\tmailed_users.add(current_user)\\n\\t\\t\\tfor user, threads in thread_users:\\n\\t\\t\\t\\tif user not in mailed_users and threads == current_threads:\\n\\t\\t\\t\\t\\tusers.append(user)\\n\\t\\t\\t\\t\\tmailed_users.add(user)\\n\\t\\t\\tsend_notification(users, 'updated_comments', {\\n\\t\\t\\t\\t'page': self.page,\\n\\t\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t\\t'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'deleted_comments': [],\\n\\t\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t\\t\\tif comment.pk in threads\\n\\t\\t\\t\\t]\\n\\t\\t\\t})\\n\\t\\treturn send_notification(global_recipient_users, 'updated_comments', {\\n\\t\\t\\t'page': self.page,\\n\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t'new_comments': changes['new_comments'],\\n\\t\\t\\t'resolved_comments': changes['resolved_comments'],\\n\\t\\t\\t'deleted_comments': changes['deleted_comments'],\\n\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t]\\n\\t\\t})",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix commenting thread notifications being sent to non-thread users",
    "fixed_code": "def send_commenting_notifications(self, changes):\\n\\t\\trelevant_comment_ids = []\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\\n\\t\\trelevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\\n\\t\\tif (not changes['new_comments']\\n\\t\\t\\t\\tand not changes['deleted_comments']\\n\\t\\t\\t\\tand not changes['resolved_comments']\\n\\t\\t\\t\\tand not changes['new_replies']):\\n\\t\\t\\treturn\\n\\t\\tsubscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\\n\\t\\tglobal_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\\n\\t\\treplies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\\n\\t\\tcomments = Comment.objects.filter(id__in=relevant_comment_ids)\\n\\t\\tthread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).filter(\\n\\t\\t\\tQ(comment_replies__comment_id__in=relevant_comment_ids) | Q(**{('%s__pk__in' % COMMENTS_RELATION_NAME): relevant_comment_ids})\\n\\t\\t).prefetch_related(\\n\\t\\t\\tPrefetch('comment_replies', queryset=replies),\\n\\t\\t\\tPrefetch(COMMENTS_RELATION_NAME, queryset=comments)\\n\\t\\t)\\n\\t\\tif not (global_recipient_users or thread_users):\\n\\t\\t\\treturn\\n\\t\\tthread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\\n\\t\\tmailed_users = set()\\n\\t\\tfor current_user, current_threads in thread_users:\\n\\t\\t\\tif current_user in mailed_users:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tusers = [current_user]\\n\\t\\t\\tmailed_users.add(current_user)\\n\\t\\t\\tfor user, threads in thread_users:\\n\\t\\t\\t\\tif user not in mailed_users and threads == current_threads:\\n\\t\\t\\t\\t\\tusers.append(user)\\n\\t\\t\\t\\t\\tmailed_users.add(user)\\n\\t\\t\\tsend_notification(users, 'updated_comments', {\\n\\t\\t\\t\\t'page': self.page,\\n\\t\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t\\t'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\\n\\t\\t\\t\\t'deleted_comments': [],\\n\\t\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t\\t\\tif comment.pk in threads\\n\\t\\t\\t\\t]\\n\\t\\t\\t})\\n\\t\\treturn send_notification(global_recipient_users, 'updated_comments', {\\n\\t\\t\\t'page': self.page,\\n\\t\\t\\t'editor': self.request.user,\\n\\t\\t\\t'new_comments': changes['new_comments'],\\n\\t\\t\\t'resolved_comments': changes['resolved_comments'],\\n\\t\\t\\t'deleted_comments': changes['deleted_comments'],\\n\\t\\t\\t'replied_comments': [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t'comment': comment,\\n\\t\\t\\t\\t\\t'replies': replies,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tfor comment, replies in changes['new_replies']\\n\\t\\t\\t]\\n\\t\\t})"
  },
  {
    "code": "def mask_zero_div_zero(x, y, result):\\n\\tif not isinstance(result, np.ndarray):\\n\\t\\treturn result\\n\\tif is_scalar(y):\\n\\t\\ty = np.array(y)\\n\\tzmask = y == 0\\n\\tif isinstance(zmask, bool):\\n\\t\\treturn result\\n\\tif zmask.any():\\n\\t\\tzneg_mask = zmask & np.signbit(y)\\n\\t\\tzpos_mask = zmask & ~zneg_mask\\n\\t\\tnan_mask = zmask & (x == 0)\\n\\t\\twith np.errstate(invalid=\"ignore\"):\\n\\t\\t\\tneginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))\\n\\t\\t\\tposinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))\\n\\t\\tif nan_mask.any() or neginf_mask.any() or posinf_mask.any():\\n\\t\\t\\tresult = result.astype(\"float64\", copy=False)\\n\\t\\t\\tresult[nan_mask] = np.nan\\n\\t\\t\\tresult[posinf_mask] = np.inf\\n\\t\\t\\tresult[neginf_mask] = -np.inf\\n\\treturn result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mask_zero_div_zero(x, y, result):\\n\\tif not isinstance(result, np.ndarray):\\n\\t\\treturn result\\n\\tif is_scalar(y):\\n\\t\\ty = np.array(y)\\n\\tzmask = y == 0\\n\\tif isinstance(zmask, bool):\\n\\t\\treturn result\\n\\tif zmask.any():\\n\\t\\tzneg_mask = zmask & np.signbit(y)\\n\\t\\tzpos_mask = zmask & ~zneg_mask\\n\\t\\tnan_mask = zmask & (x == 0)\\n\\t\\twith np.errstate(invalid=\"ignore\"):\\n\\t\\t\\tneginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))\\n\\t\\t\\tposinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))\\n\\t\\tif nan_mask.any() or neginf_mask.any() or posinf_mask.any():\\n\\t\\t\\tresult = result.astype(\"float64\", copy=False)\\n\\t\\t\\tresult[nan_mask] = np.nan\\n\\t\\t\\tresult[posinf_mask] = np.inf\\n\\t\\t\\tresult[neginf_mask] = -np.inf\\n\\treturn result"
  },
  {
    "code": "def create_default_site(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs):\\n    try:\\n        Site = apps.get_model('sites', 'Site')\\n    except LookupError:\\n        return\\n    if not router.allow_migrate_model(using, Site):\\n        return\\n    if not Site.objects.using(using).exists():\\n        if verbosity >= 2:\\n            print(\"Creating example.com Site object\")\\n        Site(pk=settings.SITE_ID, domain=\"example.com\", name=\"example.com\").save(using=using)\\n        sequence_sql = connections[using].ops.sequence_reset_sql(no_style(), [Site])\\n        if sequence_sql:\\n            if verbosity >= 2:\\n                print(\"Resetting sequence\")\\n            with connections[using].cursor() as cursor:\\n                for command in sequence_sql:\\n                    cursor.execute(command)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24488 -- Made create_default_site() use default pk of 1.\\n\\nFixed create_default_site() to use a default value in case\\nsettings.SITE_ID isn't set; refs #23945.",
    "fixed_code": "def create_default_site(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs):\\n    try:\\n        Site = apps.get_model('sites', 'Site')\\n    except LookupError:\\n        return\\n    if not router.allow_migrate_model(using, Site):\\n        return\\n    if not Site.objects.using(using).exists():\\n        if verbosity >= 2:\\n            print(\"Creating example.com Site object\")\\n        Site(pk=getattr(settings, 'SITE_ID', 1), domain=\"example.com\", name=\"example.com\").save(using=using)\\n        sequence_sql = connections[using].ops.sequence_reset_sql(no_style(), [Site])\\n        if sequence_sql:\\n            if verbosity >= 2:\\n                print(\"Resetting sequence\")\\n            with connections[using].cursor() as cursor:\\n                for command in sequence_sql:\\n                    cursor.execute(command)"
  },
  {
    "code": "def ewma(arg, com=None, span=None, halflife=None, min_periods=0, freq=None, time_rule=None,\\n         adjust=True):\\n    com = _get_center_of_mass(com, span, halflife)\\n    arg = _conv_timerule(arg, freq, time_rule)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ewma(arg, com=None, span=None, halflife=None, min_periods=0, freq=None, time_rule=None,\\n         adjust=True):\\n    com = _get_center_of_mass(com, span, halflife)\\n    arg = _conv_timerule(arg, freq, time_rule)"
  },
  {
    "code": "def _convert_index(index):\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return index.values, 'integer', atom\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                            v.microsecond / 1E6) for v in values],\\n                            dtype=np.float64)\\n        return converted, 'datetime', _tables().Time64Col()\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                            dtype=np.int32)\\n        return converted, 'date', _tables().Time32Col()\\n    elif inferred_type =='string':\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return converted, 'string', _tables().StringCol(itemsize)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return np.asarray(values, dtype=np.int64), 'integer', atom\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return np.asarray(values, dtype=np.float64), 'float', atom\\n    else: \\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_index(index):\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return index.values, 'integer', atom\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return converted, 'datetime64', _tables().Int64Col()\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                            v.microsecond / 1E6) for v in values],\\n                            dtype=np.float64)\\n        return converted, 'datetime', _tables().Time64Col()\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                            dtype=np.int32)\\n        return converted, 'date', _tables().Time32Col()\\n    elif inferred_type =='string':\\n        converted = np.array(list(values), dtype=np.str_)\\n        itemsize = converted.dtype.itemsize\\n        return converted, 'string', _tables().StringCol(itemsize)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return np.asarray(values, dtype=np.int64), 'integer', atom\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return np.asarray(values, dtype=np.float64), 'float', atom\\n    else: \\n        atom = _tables().ObjectAtom()\\n        return np.asarray(values, dtype='O'), 'object', atom"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        supports_check_mode=True,\\n        argument_spec=dict(\\n            table=dict(type='str', default='filter', choices=['filter', 'nat', 'mangle', 'raw', 'security']),\\n            state=dict(type='str', default='present', choices=['absent', 'present']),\\n            action=dict(type='str', default='append', choices=['append', 'insert']),\\n            ip_version=dict(type='str', default='ipv4', choices=['ipv4', 'ipv6']),\\n            chain=dict(type='str'),\\n            rule_num=dict(type='str'),\\n            protocol=dict(type='str'),\\n            wait=dict(type='str'),\\n            source=dict(type='str'),\\n            to_source=dict(type='str'),\\n            destination=dict(type='str'),\\n            to_destination=dict(type='str'),\\n            match=dict(type='list', elements='str', default=[]),\\n            tcp_flags=dict(type='dict',\\n                           options=dict(\\n                                flags=dict(type='list', elements='str'),\\n                                flags_set=dict(type='list', elements='str'))\\n                           ),\\n            jump=dict(type='str'),\\n            gateway=dict(type='str'),\\n            log_prefix=dict(type='str'),\\n            log_level=dict(type='str',\\n                           choices=['0', '1', '2', '3', '4', '5', '6', '7',\\n                                    'emerg', 'alert', 'crit', 'error',\\n                                    'warning', 'notice', 'info', 'debug'],\\n                           default=None,\\n                           ),\\n            goto=dict(type='str'),\\n            in_interface=dict(type='str'),\\n            out_interface=dict(type='str'),\\n            fragment=dict(type='str'),\\n            set_counters=dict(type='str'),\\n            source_port=dict(type='str'),\\n            destination_port=dict(type='str'),\\n            destination_ports=dict(type='list', elements='str', default=[]),\\n            to_ports=dict(type='str'),\\n            set_dscp_mark=dict(type='str'),\\n            set_dscp_mark_class=dict(type='str'),\\n            comment=dict(type='str'),\\n            ctstate=dict(type='list', elements='str', default=[]),\\n            src_range=dict(type='str'),\\n            dst_range=dict(type='str'),\\n            match_set=dict(type='str'),\\n            match_set_flags=dict(type='str', choices=['src', 'dst', 'src,dst', 'dst,src']),\\n            limit=dict(type='str'),\\n            limit_burst=dict(type='str'),\\n            uid_owner=dict(type='str'),\\n            gid_owner=dict(type='str'),\\n            reject_with=dict(type='str'),\\n            icmp_type=dict(type='str'),\\n            syn=dict(type='str', default='ignore', choices=['ignore', 'match', 'negate']),\\n            flush=dict(type='bool', default=False),\\n            policy=dict(type='str', choices=['ACCEPT', 'DROP', 'QUEUE', 'RETURN']),\\n        ),\\n        mutually_exclusive=(\\n            ['set_dscp_mark', 'set_dscp_mark_class'],\\n            ['flush', 'policy'],\\n        ),\\n        required_if=[\\n            ['jump', 'TEE', ['gateway']],\\n            ['jump', 'tee', ['gateway']],\\n        ]\\n    )\\n    args = dict(\\n        changed=False,\\n        failed=False,\\n        ip_version=module.params['ip_version'],\\n        table=module.params['table'],\\n        chain=module.params['chain'],\\n        flush=module.params['flush'],\\n        rule=' '.join(construct_rule(module.params)),\\n        state=module.params['state'],\\n    )\\n    ip_version = module.params['ip_version']\\n    iptables_path = module.get_bin_path(BINS[ip_version], True)\\n    if args['flush'] is False and args['chain'] is None:\\n        module.fail_json(msg=\"Either chain or flush parameter must be specified.\")\\n    if module.params.get('log_prefix', None) or module.params.get('log_level', None):\\n        if module.params['jump'] is None:\\n            module.params['jump'] = 'LOG'\\n        elif module.params['jump'] != 'LOG':\\n            module.fail_json(msg=\"Logging options can only be used with the LOG jump target.\")\\n    iptables_version = LooseVersion(get_iptables_version(iptables_path, module))\\n    if iptables_version >= LooseVersion(IPTABLES_WAIT_SUPPORT_ADDED):\\n        if iptables_version < LooseVersion(IPTABLES_WAIT_WITH_SECONDS_SUPPORT_ADDED):\\n            module.params['wait'] = ''\\n    else:\\n        module.params['wait'] = None\\n    if args['flush'] is True:\\n        args['changed'] = True\\n        if not module.check_mode:\\n            flush_table(iptables_path, module, module.params)\\n    elif module.params['policy']:\\n        current_policy = get_chain_policy(iptables_path, module, module.params)\\n        if not current_policy:\\n            module.fail_json(msg='Can\\'t detect current policy')\\n        changed = current_policy != module.params['policy']\\n        args['changed'] = changed\\n        if changed and not module.check_mode:\\n            set_chain_policy(iptables_path, module, module.params)\\n    else:\\n        insert = (module.params['action'] == 'insert')\\n        rule_is_present = check_present(iptables_path, module, module.params)\\n        should_be_present = (args['state'] == 'present')\\n        args['changed'] = (rule_is_present != should_be_present)\\n        if args['changed'] is False:\\n            module.exit_json(**args)\\n        if not module.check_mode:\\n            if should_be_present:\\n                if insert:\\n                    insert_rule(iptables_path, module, module.params)\\n                else:\\n                    append_rule(iptables_path, module, module.params)\\n            else:\\n                remove_rule(iptables_path, module, module.params)\\n    module.exit_json(**args)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "iptables - added a ``chain_management`` parameter to control chain (#76378)\\n\\ncreation and deletion\\n\\nfixes #25099\\ncloses #32158",
    "fixed_code": "def main():\\n    module = AnsibleModule(\\n        supports_check_mode=True,\\n        argument_spec=dict(\\n            table=dict(type='str', default='filter', choices=['filter', 'nat', 'mangle', 'raw', 'security']),\\n            state=dict(type='str', default='present', choices=['absent', 'present']),\\n            action=dict(type='str', default='append', choices=['append', 'insert']),\\n            ip_version=dict(type='str', default='ipv4', choices=['ipv4', 'ipv6']),\\n            chain=dict(type='str'),\\n            rule_num=dict(type='str'),\\n            protocol=dict(type='str'),\\n            wait=dict(type='str'),\\n            source=dict(type='str'),\\n            to_source=dict(type='str'),\\n            destination=dict(type='str'),\\n            to_destination=dict(type='str'),\\n            match=dict(type='list', elements='str', default=[]),\\n            tcp_flags=dict(type='dict',\\n                           options=dict(\\n                                flags=dict(type='list', elements='str'),\\n                                flags_set=dict(type='list', elements='str'))\\n                           ),\\n            jump=dict(type='str'),\\n            gateway=dict(type='str'),\\n            log_prefix=dict(type='str'),\\n            log_level=dict(type='str',\\n                           choices=['0', '1', '2', '3', '4', '5', '6', '7',\\n                                    'emerg', 'alert', 'crit', 'error',\\n                                    'warning', 'notice', 'info', 'debug'],\\n                           default=None,\\n                           ),\\n            goto=dict(type='str'),\\n            in_interface=dict(type='str'),\\n            out_interface=dict(type='str'),\\n            fragment=dict(type='str'),\\n            set_counters=dict(type='str'),\\n            source_port=dict(type='str'),\\n            destination_port=dict(type='str'),\\n            destination_ports=dict(type='list', elements='str', default=[]),\\n            to_ports=dict(type='str'),\\n            set_dscp_mark=dict(type='str'),\\n            set_dscp_mark_class=dict(type='str'),\\n            comment=dict(type='str'),\\n            ctstate=dict(type='list', elements='str', default=[]),\\n            src_range=dict(type='str'),\\n            dst_range=dict(type='str'),\\n            match_set=dict(type='str'),\\n            match_set_flags=dict(type='str', choices=['src', 'dst', 'src,dst', 'dst,src']),\\n            limit=dict(type='str'),\\n            limit_burst=dict(type='str'),\\n            uid_owner=dict(type='str'),\\n            gid_owner=dict(type='str'),\\n            reject_with=dict(type='str'),\\n            icmp_type=dict(type='str'),\\n            syn=dict(type='str', default='ignore', choices=['ignore', 'match', 'negate']),\\n            flush=dict(type='bool', default=False),\\n            policy=dict(type='str', choices=['ACCEPT', 'DROP', 'QUEUE', 'RETURN']),\\n            chain_management=dict(type='bool', default=False),\\n        ),\\n        mutually_exclusive=(\\n            ['set_dscp_mark', 'set_dscp_mark_class'],\\n            ['flush', 'policy'],\\n        ),\\n        required_if=[\\n            ['jump', 'TEE', ['gateway']],\\n            ['jump', 'tee', ['gateway']],\\n        ]\\n    )\\n    args = dict(\\n        changed=False,\\n        failed=False,\\n        ip_version=module.params['ip_version'],\\n        table=module.params['table'],\\n        chain=module.params['chain'],\\n        flush=module.params['flush'],\\n        rule=' '.join(construct_rule(module.params)),\\n        state=module.params['state'],\\n        chain_management=module.params['chain_management'],\\n    )\\n    ip_version = module.params['ip_version']\\n    iptables_path = module.get_bin_path(BINS[ip_version], True)\\n    if args['flush'] is False and args['chain'] is None:\\n        module.fail_json(msg=\"Either chain or flush parameter must be specified.\")\\n    if module.params.get('log_prefix', None) or module.params.get('log_level', None):\\n        if module.params['jump'] is None:\\n            module.params['jump'] = 'LOG'\\n        elif module.params['jump'] != 'LOG':\\n            module.fail_json(msg=\"Logging options can only be used with the LOG jump target.\")\\n    iptables_version = LooseVersion(get_iptables_version(iptables_path, module))\\n    if iptables_version >= LooseVersion(IPTABLES_WAIT_SUPPORT_ADDED):\\n        if iptables_version < LooseVersion(IPTABLES_WAIT_WITH_SECONDS_SUPPORT_ADDED):\\n            module.params['wait'] = ''\\n    else:\\n        module.params['wait'] = None\\n    if args['flush'] is True:\\n        args['changed'] = True\\n        if not module.check_mode:\\n            flush_table(iptables_path, module, module.params)\\n    elif module.params['policy']:\\n        current_policy = get_chain_policy(iptables_path, module, module.params)\\n        if not current_policy:\\n            module.fail_json(msg='Can\\'t detect current policy')\\n        changed = current_policy != module.params['policy']\\n        args['changed'] = changed\\n        if changed and not module.check_mode:\\n            set_chain_policy(iptables_path, module, module.params)\\n    elif (args['state'] == 'absent') and not args['rule']:\\n        chain_is_present = check_chain_present(\\n            iptables_path, module, module.params\\n        )\\n        args['changed'] = chain_is_present\\n        if (chain_is_present and args['chain_management'] and not module.check_mode):\\n            delete_chain(iptables_path, module, module.params)\\n    else:\\n        insert = (module.params['action'] == 'insert')\\n        rule_is_present = check_rule_present(\\n            iptables_path, module, module.params\\n        )\\n        chain_is_present = rule_is_present or check_chain_present(\\n            iptables_path, module, module.params\\n        )\\n        should_be_present = (args['state'] == 'present')\\n        args['changed'] = (rule_is_present != should_be_present)\\n        if args['changed'] is False:\\n            module.exit_json(**args)\\n        if not module.check_mode:\\n            if should_be_present:\\n                if not chain_is_present and args['chain_management']:\\n                    create_chain(iptables_path, module, module.params)\\n                if insert:\\n                    insert_rule(iptables_path, module, module.params)\\n                else:\\n                    append_rule(iptables_path, module, module.params)\\n            else:\\n                remove_rule(iptables_path, module, module.params)\\n    module.exit_json(**args)"
  },
  {
    "code": "def can_hold_element(arr: ArrayLike, element: Any) -> bool:\\n    dtype = arr.dtype\\n    if not isinstance(dtype, np.dtype) or dtype.kind in [\"m\", \"M\"]:\\n        if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):\\n            arr = cast(\\n                \"PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray\", arr\\n            )\\n            try:\\n                arr._validate_setitem_value(element)\\n                return True\\n            except (ValueError, TypeError):\\n                return False\\n        return True\\n    tipo = maybe_infer_dtype_type(element)\\n    if dtype.kind in [\"i\", \"u\"]:\\n        if tipo is not None:\\n            if tipo.kind not in [\"i\", \"u\"]:\\n                if is_float(element) and element.is_integer():\\n                    return True\\n                return False\\n            elif dtype.itemsize < tipo.itemsize:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return is_integer(element) or (is_float(element) and element.is_integer())\\n    elif dtype.kind == \"f\":\\n        if tipo is not None:\\n            if tipo.kind not in [\"f\", \"i\", \"u\"]:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return lib.is_integer(element) or lib.is_float(element)\\n    elif dtype.kind == \"c\":\\n        if tipo is not None:\\n            return tipo.kind in [\"c\", \"f\", \"i\", \"u\"]\\n        return (\\n            lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element)\\n        )\\n    elif dtype.kind == \"b\":\\n        if tipo is not None:\\n            return tipo.kind == \"b\"\\n        return lib.is_bool(element)\\n    elif dtype == object:  \\n        return True\\n    elif dtype.kind == \"S\":\\n        if tipo is not None:\\n            return tipo.kind == \"S\" and tipo.itemsize <= dtype.itemsize\\n        return isinstance(element, bytes) and len(element) <= dtype.itemsize\\n    raise NotImplementedError(dtype)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Regression in loc setitem raising ValueError when setting array as cell value (#43450)",
    "fixed_code": "def can_hold_element(arr: ArrayLike, element: Any) -> bool:\\n    dtype = arr.dtype\\n    if not isinstance(dtype, np.dtype) or dtype.kind in [\"m\", \"M\"]:\\n        if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):\\n            arr = cast(\\n                \"PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray\", arr\\n            )\\n            try:\\n                arr._validate_setitem_value(element)\\n                return True\\n            except (ValueError, TypeError):\\n                return False\\n        return True\\n    if dtype == object:  \\n        return True\\n    tipo = maybe_infer_dtype_type(element)\\n    if dtype.kind in [\"i\", \"u\"]:\\n        if tipo is not None:\\n            if tipo.kind not in [\"i\", \"u\"]:\\n                if is_float(element) and element.is_integer():\\n                    return True\\n                return False\\n            elif dtype.itemsize < tipo.itemsize:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return is_integer(element) or (is_float(element) and element.is_integer())\\n    elif dtype.kind == \"f\":\\n        if tipo is not None:\\n            if tipo.kind not in [\"f\", \"i\", \"u\"]:\\n                return False\\n            elif not isinstance(tipo, np.dtype):\\n                return not element._mask.any()\\n            return True\\n        return lib.is_integer(element) or lib.is_float(element)\\n    elif dtype.kind == \"c\":\\n        if tipo is not None:\\n            return tipo.kind in [\"c\", \"f\", \"i\", \"u\"]\\n        return (\\n            lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element)\\n        )\\n    elif dtype.kind == \"b\":\\n        if tipo is not None:\\n            return tipo.kind == \"b\"\\n        return lib.is_bool(element)\\n    elif dtype.kind == \"S\":\\n        if tipo is not None:\\n            return tipo.kind == \"S\" and tipo.itemsize <= dtype.itemsize\\n        return isinstance(element, bytes) and len(element) <= dtype.itemsize\\n    raise NotImplementedError(dtype)"
  },
  {
    "code": "def _build_files_manifest(b_collection_path, namespace, name, ignore_patterns):\\n    b_ignore_patterns = [\\n        b'galaxy.yml',\\n        b'galaxy.yaml',\\n        b'.git',\\n        b'*.pyc',\\n        b'*.retry',\\n        b'tests/output',  \\n        to_bytes('{0}-{1}-*.tar.gz'.format(namespace, name)),  \\n    ]\\n    b_ignore_patterns += [to_bytes(p) for p in ignore_patterns]\\n    b_ignore_dirs = frozenset([b'CVS', b'.bzr', b'.hg', b'.git', b'.svn', b'__pycache__', b'.tox'])\\n    entry_template = {\\n        'name': None,\\n        'ftype': None,\\n        'chksum_type': None,\\n        'chksum_sha256': None,\\n        'format': MANIFEST_FORMAT\\n    }\\n    manifest = {\\n        'files': [\\n            {\\n                'name': '.',\\n                'ftype': 'dir',\\n                'chksum_type': None,\\n                'chksum_sha256': None,\\n                'format': MANIFEST_FORMAT,\\n            },\\n        ],\\n        'format': MANIFEST_FORMAT,\\n    }",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "galaxy build - ignore existing MANIFEST and FILES (#76479)",
    "fixed_code": "def _build_files_manifest(b_collection_path, namespace, name, ignore_patterns):\\n    b_ignore_patterns = [\\n        b'MANIFEST.json',\\n        b'FILES.json',\\n        b'galaxy.yml',\\n        b'galaxy.yaml',\\n        b'.git',\\n        b'*.pyc',\\n        b'*.retry',\\n        b'tests/output',  \\n        to_bytes('{0}-{1}-*.tar.gz'.format(namespace, name)),  \\n    ]\\n    b_ignore_patterns += [to_bytes(p) for p in ignore_patterns]\\n    b_ignore_dirs = frozenset([b'CVS', b'.bzr', b'.hg', b'.git', b'.svn', b'__pycache__', b'.tox'])\\n    entry_template = {\\n        'name': None,\\n        'ftype': None,\\n        'chksum_type': None,\\n        'chksum_sha256': None,\\n        'format': MANIFEST_FORMAT\\n    }\\n    manifest = {\\n        'files': [\\n            {\\n                'name': '.',\\n                'ftype': 'dir',\\n                'chksum_type': None,\\n                'chksum_sha256': None,\\n                'format': MANIFEST_FORMAT,\\n            },\\n        ],\\n        'format': MANIFEST_FORMAT,\\n    }"
  },
  {
    "code": "def searchsorted(self, value, side=\"left\", sorter=None):\\n\\t\\tif isinstance(value, (np.ndarray, Index)):\\n\\t\\t\\tvalue = np.array(value, dtype=_TD_DTYPE, copy=False)\\n\\t\\telse:\\n\\t\\t\\tvalue = Timedelta(value).asm8.view(_TD_DTYPE)\\n\\t\\treturn self.values.searchsorted(value, side=side, sorter=sorter)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: TimedeltaIndex.searchsorted accepting invalid types/dtypes (#30831)",
    "fixed_code": "def searchsorted(self, value, side=\"left\", sorter=None):\\n\\t\\tif isinstance(value, (np.ndarray, Index)):\\n\\t\\t\\tif not type(self._data)._is_recognized_dtype(value):\\n\\t\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\\t\"searchsorted requires compatible dtype or scalar, \"\\n\\t\\t\\t\\t\\tf\"not {type(value).__name__}\"\\n\\t\\t\\t\\t)\\n\\t\\t\\tvalue = type(self._data)(value)\\n\\t\\t\\tself._data._check_compatible_with(value)\\n\\t\\telif isinstance(value, self._data._recognized_scalars):\\n\\t\\t\\tself._data._check_compatible_with(value)\\n\\t\\t\\tvalue = self._data._scalar_type(value)\\n\\t\\telif not isinstance(value, TimedeltaArray):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"searchsorted requires compatible dtype or scalar, \"\\n\\t\\t\\t\\tf\"not {type(value).__name__}\"\\n\\t\\t\\t)\\n\\t\\treturn self._data.searchsorted(value, side=side, sorter=sorter)"
  },
  {
    "code": "def user_change_password(self, request, id, form_url=''):\\n        if not self.has_change_permission(request):\\n            raise PermissionDenied\\n        user = self.get_object(request, unquote(id))\\n        if user is None:\\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {\\n                'name': force_text(self.model._meta.verbose_name),\\n                'key': escape(id),\\n            })\\n        if request.method == 'POST':\\n            form = self.change_password_form(user, request.POST)\\n            if form.is_valid():\\n                form.save()\\n                change_message = self.construct_change_message(request, form, None)\\n                self.log_change(request, user, change_message)\\n                msg = ugettext('Password changed successfully.')\\n                messages.success(request, msg)\\n                update_session_auth_hash(request, form.user)\\n                return HttpResponseRedirect(\\n                    reverse(\\n                        '%s:auth_%s_change' % (\\n                            self.admin_site.name,\\n                            user._meta.model_name,\\n                        ),\\n                        args=(user.pk,),\\n                    )\\n                )\\n        else:\\n            form = self.change_password_form(user)\\n        fieldsets = [(None, {'fields': list(form.base_fields)})]\\n        adminForm = admin.helpers.AdminForm(form, fieldsets, {})\\n        context = {\\n            'title': _('Change password: %s') % escape(user.get_username()),\\n            'adminForm': adminForm,\\n            'form_url': form_url,\\n            'form': form,\\n            'is_popup': (IS_POPUP_VAR in request.POST or\\n                         IS_POPUP_VAR in request.GET),\\n            'add': True,\\n            'change': False,\\n            'has_delete_permission': False,\\n            'has_change_permission': True,\\n            'has_absolute_url': False,\\n            'opts': self.model._meta,\\n            'original': user,\\n            'save_as': False,\\n            'show_save': True,\\n        }\\n        context.update(admin.site.each_context(request))\\n        request.current_app = self.admin_site.name\\n        return TemplateResponse(request,\\n            self.change_user_password_template or\\n            'admin/auth/user/change_password.html',\\n            context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def user_change_password(self, request, id, form_url=''):\\n        if not self.has_change_permission(request):\\n            raise PermissionDenied\\n        user = self.get_object(request, unquote(id))\\n        if user is None:\\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {\\n                'name': force_text(self.model._meta.verbose_name),\\n                'key': escape(id),\\n            })\\n        if request.method == 'POST':\\n            form = self.change_password_form(user, request.POST)\\n            if form.is_valid():\\n                form.save()\\n                change_message = self.construct_change_message(request, form, None)\\n                self.log_change(request, user, change_message)\\n                msg = ugettext('Password changed successfully.')\\n                messages.success(request, msg)\\n                update_session_auth_hash(request, form.user)\\n                return HttpResponseRedirect(\\n                    reverse(\\n                        '%s:auth_%s_change' % (\\n                            self.admin_site.name,\\n                            user._meta.model_name,\\n                        ),\\n                        args=(user.pk,),\\n                    )\\n                )\\n        else:\\n            form = self.change_password_form(user)\\n        fieldsets = [(None, {'fields': list(form.base_fields)})]\\n        adminForm = admin.helpers.AdminForm(form, fieldsets, {})\\n        context = {\\n            'title': _('Change password: %s') % escape(user.get_username()),\\n            'adminForm': adminForm,\\n            'form_url': form_url,\\n            'form': form,\\n            'is_popup': (IS_POPUP_VAR in request.POST or\\n                         IS_POPUP_VAR in request.GET),\\n            'add': True,\\n            'change': False,\\n            'has_delete_permission': False,\\n            'has_change_permission': True,\\n            'has_absolute_url': False,\\n            'opts': self.model._meta,\\n            'original': user,\\n            'save_as': False,\\n            'show_save': True,\\n        }\\n        context.update(admin.site.each_context(request))\\n        request.current_app = self.admin_site.name\\n        return TemplateResponse(request,\\n            self.change_user_password_template or\\n            'admin/auth/user/change_password.html',\\n            context)"
  },
  {
    "code": "def copy(self, names=None, dtype=None, levels=None, labels=None,\\n             deep=False):\\n        new_index = np.ndarray.copy(self)\\n        if levels is not None:\\n            new_index = new_index.set_levels(levels)\\n        if labels is not None:\\n            new_index = new_index.set_labels(labels)\\n        if names is not None:\\n            new_index = new_index.set_names(names)\\n        if dtype:\\n            new_index = new_index.astype(dtype)\\n        return new_index",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Additional keyword arguments for Index.copy()\\n\\n  `dtype` on copy. MultiIndex can set `levels`, `labels`, and `names`.\\n  copies axes as well. Defaults to False.\\n  copy its index.",
    "fixed_code": "def copy(self, names=None, name=None, dtype=None, deep=False):\\n        if names is not None and name is not None:\\n            raise TypeError(\"Can only provide one of `names` and `name`\")\\n        if deep:\\n            from copy import deepcopy\\n            new_index = np.ndarray.__deepcopy__(self, {}).view(self.__class__)\\n            name = name or deepcopy(self.name)\\n        else:\\n            new_index = super(Index, self).copy()\\n        if name is not None:\\n            names = [name]\\n        if names:\\n            new_index = new_index.set_names(names)\\n        if dtype:\\n            new_index = new_index.astype(dtype)\\n        return new_index"
  },
  {
    "code": "def write(self, content):\\n        if self._base_content_is_iter:\\n            raise Exception(\"This %s instance is not writable\" % self.__class__)\\n        self._container.append(content)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #7581 -- Added streaming responses.\\n\\nThanks mrmachine and everyone else involved on this long-standing ticket.",
    "fixed_code": "def write(self, content):\\n        raise Exception(\"This %s instance is not writable\" % self.__class__.__name__)"
  },
  {
    "code": "def process_view(self, request, callback, callback_args, callback_kwargs):\\n\\t\\tif getattr(request, 'csrf_processing_done', False):\\n\\t\\t\\treturn None\\n\\t\\tif getattr(callback, 'csrf_exempt', False):\\n\\t\\t\\treturn None\\n\\t\\tif request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):\\n\\t\\t\\tif getattr(request, '_dont_enforce_csrf_checks', False):\\n\\t\\t\\t\\treturn self._accept(request)\\n\\t\\t\\tif request.is_secure():\\n\\t\\t\\t\\treferer = request.META.get('HTTP_REFERER')\\n\\t\\t\\t\\tif referer is None:\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_NO_REFERER)\\n\\t\\t\\t\\treferer = urlparse(referer)\\n\\t\\t\\t\\tif '' in (referer.scheme, referer.netloc):\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_MALFORMED_REFERER)\\n\\t\\t\\t\\tif referer.scheme != 'https':\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_INSECURE_REFERER)\\n\\t\\t\\t\\tgood_referer = (\\n\\t\\t\\t\\t\\tsettings.SESSION_COOKIE_DOMAIN\\n\\t\\t\\t\\t\\tif settings.CSRF_USE_SESSIONS\\n\\t\\t\\t\\t\\telse settings.CSRF_COOKIE_DOMAIN\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tif good_referer is not None:\\n\\t\\t\\t\\t\\tserver_port = request.get_port()\\n\\t\\t\\t\\t\\tif server_port not in ('443', '80'):\\n\\t\\t\\t\\t\\t\\tgood_referer = '%s:%s' % (good_referer, server_port)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tgood_referer = request.get_host()\\n\\t\\t\\t\\t\\texcept DisallowedHost:\\n\\t\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\tgood_hosts = list(settings.CSRF_TRUSTED_ORIGINS)\\n\\t\\t\\t\\tif good_referer is not None:\\n\\t\\t\\t\\t\\tgood_hosts.append(good_referer)\\n\\t\\t\\t\\tif not any(is_same_domain(referer.netloc, host) for host in good_hosts):\\n\\t\\t\\t\\t\\treason = REASON_BAD_REFERER % referer.geturl()\\n\\t\\t\\t\\t\\treturn self._reject(request, reason)\\n\\t\\t\\tcsrf_token = request.META.get('CSRF_COOKIE')\\n\\t\\t\\tif csrf_token is None:\\n\\t\\t\\t\\treturn self._reject(request, REASON_NO_CSRF_COOKIE)\\n\\t\\t\\trequest_csrf_token = \"\"\\n\\t\\t\\tif request.method == \"POST\":\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\trequest_csrf_token = request.POST.get('csrfmiddlewaretoken', '')\\n\\t\\t\\t\\texcept OSError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\tif request_csrf_token == \"\":\\n\\t\\t\\t\\trequest_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')\\n\\t\\t\\trequest_csrf_token = _sanitize_token(request_csrf_token)\\n\\t\\t\\tif not _compare_masked_tokens(request_csrf_token, csrf_token):\\n\\t\\t\\t\\treturn self._reject(request, REASON_BAD_TOKEN)\\n\\t\\treturn self._accept(request)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #28699 -- Fixed CSRF validation with remote user middleware.\\n\\nEnsured process_view() always accesses the CSRF token from the session\\nor cookie, rather than the request, as rotate_token() may have been called\\nby an authentication middleware during the process_request() phase.",
    "fixed_code": "def process_view(self, request, callback, callback_args, callback_kwargs):\\n\\t\\tif getattr(request, 'csrf_processing_done', False):\\n\\t\\t\\treturn None\\n\\t\\tif getattr(callback, 'csrf_exempt', False):\\n\\t\\t\\treturn None\\n\\t\\tif request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):\\n\\t\\t\\tif getattr(request, '_dont_enforce_csrf_checks', False):\\n\\t\\t\\t\\treturn self._accept(request)\\n\\t\\t\\tif request.is_secure():\\n\\t\\t\\t\\treferer = request.META.get('HTTP_REFERER')\\n\\t\\t\\t\\tif referer is None:\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_NO_REFERER)\\n\\t\\t\\t\\treferer = urlparse(referer)\\n\\t\\t\\t\\tif '' in (referer.scheme, referer.netloc):\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_MALFORMED_REFERER)\\n\\t\\t\\t\\tif referer.scheme != 'https':\\n\\t\\t\\t\\t\\treturn self._reject(request, REASON_INSECURE_REFERER)\\n\\t\\t\\t\\tgood_referer = (\\n\\t\\t\\t\\t\\tsettings.SESSION_COOKIE_DOMAIN\\n\\t\\t\\t\\t\\tif settings.CSRF_USE_SESSIONS\\n\\t\\t\\t\\t\\telse settings.CSRF_COOKIE_DOMAIN\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tif good_referer is not None:\\n\\t\\t\\t\\t\\tserver_port = request.get_port()\\n\\t\\t\\t\\t\\tif server_port not in ('443', '80'):\\n\\t\\t\\t\\t\\t\\tgood_referer = '%s:%s' % (good_referer, server_port)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tgood_referer = request.get_host()\\n\\t\\t\\t\\t\\texcept DisallowedHost:\\n\\t\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\tgood_hosts = list(settings.CSRF_TRUSTED_ORIGINS)\\n\\t\\t\\t\\tif good_referer is not None:\\n\\t\\t\\t\\t\\tgood_hosts.append(good_referer)\\n\\t\\t\\t\\tif not any(is_same_domain(referer.netloc, host) for host in good_hosts):\\n\\t\\t\\t\\t\\treason = REASON_BAD_REFERER % referer.geturl()\\n\\t\\t\\t\\t\\treturn self._reject(request, reason)\\n\\t\\t\\tcsrf_token = self._get_token(request)\\n\\t\\t\\tif csrf_token is None:\\n\\t\\t\\t\\treturn self._reject(request, REASON_NO_CSRF_COOKIE)\\n\\t\\t\\trequest_csrf_token = \"\"\\n\\t\\t\\tif request.method == \"POST\":\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\trequest_csrf_token = request.POST.get('csrfmiddlewaretoken', '')\\n\\t\\t\\t\\texcept OSError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\tif request_csrf_token == \"\":\\n\\t\\t\\t\\trequest_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')\\n\\t\\t\\trequest_csrf_token = _sanitize_token(request_csrf_token)\\n\\t\\t\\tif not _compare_masked_tokens(request_csrf_token, csrf_token):\\n\\t\\t\\t\\treturn self._reject(request, REASON_BAD_TOKEN)\\n\\t\\treturn self._accept(request)"
  },
  {
    "code": "def _get_field(cls, a_name, a_type):\\n    default = getattr(cls, a_name, MISSING)\\n    if isinstance(default, Field):\\n        f = default\\n    else:\\n        if isinstance(default, types.MemberDescriptorType):\\n            default = MISSING\\n        f = field(default=default)\\n    f._field_type = _FIELD\\n    f.name = a_name\\n    f.type = a_type\\n    typing = sys.modules.get('typing')\\n    if typing is not None:\\n        if (type(a_type) is typing._GenericAlias and\\n                a_type.__origin__ is typing.ClassVar):\\n            f._field_type = _FIELD_CLASSVAR\\n    if f._field_type is _FIELD:\\n        if a_type is InitVar:\\n            f._field_type = _FIELD_INITVAR\\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\\n        if f.default_factory is not MISSING:\\n            raise TypeError(f'field {f.name} cannot have a '\\n                            'default factory')\\n    if f._field_type is _FIELD and isinstance(f.default, (list, dict, set)):\\n        raise ValueError(f'mutable default {type(f.default)} for field '\\n                         f'{f.name} is not allowed: use default_factory')\\n    return f",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add 'Field' to dataclasses.__all__. (GH-6182)\\n\\n- Add missing 'Field' to __all__.\\n- Improve tests to catch this.",
    "fixed_code": "def _get_field(cls, a_name, a_type):\\n    default = getattr(cls, a_name, MISSING)\\n    if isinstance(default, Field):\\n        f = default\\n    else:\\n        if isinstance(default, types.MemberDescriptorType):\\n            default = MISSING\\n        f = field(default=default)\\n    f._field_type = _FIELD\\n    f.name = a_name\\n    f.type = a_type\\n    typing = sys.modules.get('typing')\\n    if typing is not None:\\n        if (type(a_type) is typing._GenericAlias and\\n                a_type.__origin__ is typing.ClassVar):\\n            f._field_type = _FIELD_CLASSVAR\\n    if f._field_type is _FIELD:\\n        if a_type is InitVar:\\n            f._field_type = _FIELD_INITVAR\\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\\n        if f.default_factory is not MISSING:\\n            raise TypeError(f'field {f.name} cannot have a '\\n                            'default factory')\\n    if f._field_type is _FIELD and isinstance(f.default, (list, dict, set)):\\n        raise ValueError(f'mutable default {type(f.default)} for field '\\n                         f'{f.name} is not allowed: use default_factory')\\n    return f"
  },
  {
    "code": "def __init__(\\n        self, *, aws_conn_id: Optional[str] = DEFAULT_CONN_ID, region: Optional[str] = None, **kwargs\\n    ):\\n        self.aws_conn_id = aws_conn_id\\n        self.region = region\\n        super().__init__(**kwargs)\\n    @cached_property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self, *, aws_conn_id: Optional[str] = DEFAULT_CONN_ID, region: Optional[str] = None, **kwargs\\n    ):\\n        self.aws_conn_id = aws_conn_id\\n        self.region = region\\n        super().__init__(**kwargs)\\n    @cached_property"
  },
  {
    "code": "def read_pickle(path, compression=\"infer\"):\\n\\tpath = stringify_path(path)\\n\\tf, fh = get_handle(path, \"rb\", compression=compression, is_text=False)\\n\\texcs_to_catch = (AttributeError, ImportError, ModuleNotFoundError)\\n\\ttry:\\n\\t\\twith warnings.catch_warnings(record=True):\\n\\t\\t\\twarnings.simplefilter(\"ignore\", Warning)\\n\\t\\t\\treturn pickle.load(f)\\n\\texcept excs_to_catch:\\n\\t\\treturn pc.load(f, encoding=None)\\n\\texcept UnicodeDecodeError:\\n\\t\\treturn pc.load(f, encoding=\"latin-1\")\\n\\tfinally:\\n\\t\\tf.close()\\n\\t\\tfor _f in fh:\\n\\t\\t\\t_f.close()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: func 'to_pickle' and 'read_pickle' where not accepting URL GH#30163 (#30301)",
    "fixed_code": "def read_pickle(\\n\\tfilepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\\n):\\n\\tfp_or_buf, _, compression, should_close = get_filepath_or_buffer(\\n\\t\\tfilepath_or_buffer, compression=compression\\n\\t)\\n\\tif not isinstance(fp_or_buf, str) and compression == \"infer\":\\n\\t\\tcompression = None\\n\\tf, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\\n\\texcs_to_catch = (AttributeError, ImportError, ModuleNotFoundError)\\n\\ttry:\\n\\t\\twith warnings.catch_warnings(record=True):\\n\\t\\t\\twarnings.simplefilter(\"ignore\", Warning)\\n\\t\\t\\treturn pickle.load(f)\\n\\texcept excs_to_catch:\\n\\t\\treturn pc.load(f, encoding=None)\\n\\texcept UnicodeDecodeError:\\n\\t\\treturn pc.load(f, encoding=\"latin-1\")\\n\\tfinally:\\n\\t\\tf.close()\\n\\t\\tfor _f in fh:\\n\\t\\t\\t_f.close()\\n\\t\\tif should_close:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tfp_or_buf.close()\\n\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\tpass"
  },
  {
    "code": "def select(self, key, where=None, start=None, stop=None, columns=None, **kwargs):\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError('No object named %s in the file' % key)\\n        return self._read_group(group, where=where, start=start, stop=stop, columns=columns, **kwargs)\\n    def select_as_coordinates(self, key, where=None, **kwargs):\\n        return self.get_storer(key).read_coordinates(where = where, **kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: New keywords ``iterator=boolean``, and ``chunksize=number_in_a_chunk`` are      provided to support iteration on ``select`` and ``select_as_multiple`` (GH3076_)",
    "fixed_code": "def select(self, key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, **kwargs):\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError('No object named %s in the file' % key)\\n        s = self._create_storer(group)\\n        s.infer_axes()"
  },
  {
    "code": "def get_datevalue(date, freq):\\n    if isinstance(date, Period):\\n        return date.asfreq(freq).ordinal\\n    elif isinstance(date, str):\\n        return Period(date, freq).ordinal\\n    elif isinstance(date, (int, float)) or \\\\n            (isinstance(date, np.ndarray) and (date.size == 1)):\\n        return date\\n    elif date is None:\\n        return None\\n    raise ValueError(\"Unrecognizable date '%s'\" % date)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: set_xlim for time series plots #1339",
    "fixed_code": "def get_datevalue(date, freq):\\n    if isinstance(date, Period):\\n        return date.asfreq(freq).ordinal\\n    elif isinstance(date, (str, datetime, pydt.date, pydt.time)):\\n        return Period(date, freq).ordinal\\n    elif isinstance(date, (int, float)) or \\\\n            (isinstance(date, np.ndarray) and (date.size == 1)):\\n        return date\\n    elif date is None:\\n        return None\\n    raise ValueError(\"Unrecognizable date '%s'\" % date)"
  },
  {
    "code": "def get_mgr_concatenation_plan(mgr, indexers):\\n    mgr_shape = list(mgr.shape)\\n    for ax, indexer in indexers.items():\\n        mgr_shape[ax] = len(indexer)\\n    mgr_shape = tuple(mgr_shape)\\n    if 0 in indexers:\\n        ax0_indexer = indexers.pop(0)\\n        blknos = algos.take_1d(mgr.blknos, ax0_indexer, fill_value=-1)\\n        blklocs = algos.take_1d(mgr.blklocs, ax0_indexer, fill_value=-1)\\n    else:\\n        if mgr._is_single_block:\\n            blk = mgr.blocks[0]\\n            return [(blk.mgr_locs, JoinUnit(blk, mgr_shape, indexers))]\\n        ax0_indexer = None\\n        blknos = mgr.blknos\\n        blklocs = mgr.blklocs\\n    plan = []\\n    for blkno, placements in libinternals.get_blkno_placements(blknos, group=False):\\n        assert placements.is_slice_like\\n        join_unit_indexers = indexers.copy()\\n        shape = list(mgr_shape)\\n        shape[0] = len(placements)\\n        shape = tuple(shape)\\n        if blkno == -1:\\n            unit = JoinUnit(None, shape)\\n        else:\\n            blk = mgr.blocks[blkno]\\n            ax0_blk_indexer = blklocs[placements.indexer]\\n            unit_no_ax0_reindexing = (\\n                len(placements) == len(blk.mgr_locs)\\n                and\\n                (\\n                    (\\n                        ax0_indexer is None\\n                        and blk.mgr_locs.is_slice_like\\n                        and blk.mgr_locs.as_slice.step == 1\\n                    )\\n                    or\\n                    (np.diff(ax0_blk_indexer) == 1).all()\\n                )\\n            )\\n            if unit_no_ax0_reindexing:\\n                join_unit_indexers.pop(0, None)\\n            else:\\n                join_unit_indexers[0] = ax0_blk_indexer\\n            unit = JoinUnit(blk, shape, join_unit_indexers)\\n        plan.append((placements, unit))\\n    return plan",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_mgr_concatenation_plan(mgr, indexers):\\n    mgr_shape = list(mgr.shape)\\n    for ax, indexer in indexers.items():\\n        mgr_shape[ax] = len(indexer)\\n    mgr_shape = tuple(mgr_shape)\\n    if 0 in indexers:\\n        ax0_indexer = indexers.pop(0)\\n        blknos = algos.take_1d(mgr.blknos, ax0_indexer, fill_value=-1)\\n        blklocs = algos.take_1d(mgr.blklocs, ax0_indexer, fill_value=-1)\\n    else:\\n        if mgr._is_single_block:\\n            blk = mgr.blocks[0]\\n            return [(blk.mgr_locs, JoinUnit(blk, mgr_shape, indexers))]\\n        ax0_indexer = None\\n        blknos = mgr.blknos\\n        blklocs = mgr.blklocs\\n    plan = []\\n    for blkno, placements in libinternals.get_blkno_placements(blknos, group=False):\\n        assert placements.is_slice_like\\n        join_unit_indexers = indexers.copy()\\n        shape = list(mgr_shape)\\n        shape[0] = len(placements)\\n        shape = tuple(shape)\\n        if blkno == -1:\\n            unit = JoinUnit(None, shape)\\n        else:\\n            blk = mgr.blocks[blkno]\\n            ax0_blk_indexer = blklocs[placements.indexer]\\n            unit_no_ax0_reindexing = (\\n                len(placements) == len(blk.mgr_locs)\\n                and\\n                (\\n                    (\\n                        ax0_indexer is None\\n                        and blk.mgr_locs.is_slice_like\\n                        and blk.mgr_locs.as_slice.step == 1\\n                    )\\n                    or\\n                    (np.diff(ax0_blk_indexer) == 1).all()\\n                )\\n            )\\n            if unit_no_ax0_reindexing:\\n                join_unit_indexers.pop(0, None)\\n            else:\\n                join_unit_indexers[0] = ax0_blk_indexer\\n            unit = JoinUnit(blk, shape, join_unit_indexers)\\n        plan.append((placements, unit))\\n    return plan"
  },
  {
    "code": "def parse_header(line):\\n    plist = _parse_header_params(b';' + line)\\n    key = plist.pop(0).lower().decode('ascii')\\n    pdict = {}\\n    for p in plist:\\n        i = p.find(b'=')\\n        if i >= 0:\\n            has_encoding = False\\n            name = p[:i].strip().lower().decode('ascii')\\n            if name.endswith('*'):\\n                name = name[:-1]\\n                has_encoding = True\\n            value = p[i + 1:].strip()\\n            if has_encoding:\\n                encoding, lang, value = value.split(b\"'\")\\n                if six.PY3:\\n                    value = unquote(value.decode(), encoding=encoding.decode())\\n                else:\\n                    value = unquote(value).decode(encoding)\\n            if len(value) >= 2 and value[:1] == value[-1:] == b'\"':\\n                value = value[1:-1]\\n                value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\\n            pdict[name] = value\\n    return key, pdict",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_header(line):\\n    plist = _parse_header_params(b';' + line)\\n    key = plist.pop(0).lower().decode('ascii')\\n    pdict = {}\\n    for p in plist:\\n        i = p.find(b'=')\\n        if i >= 0:\\n            has_encoding = False\\n            name = p[:i].strip().lower().decode('ascii')\\n            if name.endswith('*'):\\n                name = name[:-1]\\n                has_encoding = True\\n            value = p[i + 1:].strip()\\n            if has_encoding:\\n                encoding, lang, value = value.split(b\"'\")\\n                if six.PY3:\\n                    value = unquote(value.decode(), encoding=encoding.decode())\\n                else:\\n                    value = unquote(value).decode(encoding)\\n            if len(value) >= 2 and value[:1] == value[-1:] == b'\"':\\n                value = value[1:-1]\\n                value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\\n            pdict[name] = value\\n    return key, pdict"
  },
  {
    "code": "def get_host(self):\\n\\t\\thost = self._get_raw_host()\\n\\t\\tallowed_hosts = settings.ALLOWED_HOSTS\\n\\t\\tif settings.DEBUG and not allowed_hosts:\\n\\t\\t\\tallowed_hosts = ['localhost', '127.0.0.1', '[::1]']\\n\\t\\tdomain, port = split_domain_port(host)\\n\\t\\tif domain and validate_host(domain, allowed_hosts):\\n\\t\\t\\treturn host\\n\\t\\telse:\\n\\t\\t\\tmsg = \"Invalid HTTP_HOST header: %r.\" % host\\n\\t\\t\\tif domain:\\n\\t\\t\\t\\tmsg += \" You may need to add %r to ALLOWED_HOSTS.\" % domain\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmsg += \" The domain name provided is not valid according to RFC 1034/1035.\"\\n\\t\\t\\traise DisallowedHost(msg)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_host(self):\\n\\t\\thost = self._get_raw_host()\\n\\t\\tallowed_hosts = settings.ALLOWED_HOSTS\\n\\t\\tif settings.DEBUG and not allowed_hosts:\\n\\t\\t\\tallowed_hosts = ['localhost', '127.0.0.1', '[::1]']\\n\\t\\tdomain, port = split_domain_port(host)\\n\\t\\tif domain and validate_host(domain, allowed_hosts):\\n\\t\\t\\treturn host\\n\\t\\telse:\\n\\t\\t\\tmsg = \"Invalid HTTP_HOST header: %r.\" % host\\n\\t\\t\\tif domain:\\n\\t\\t\\t\\tmsg += \" You may need to add %r to ALLOWED_HOSTS.\" % domain\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmsg += \" The domain name provided is not valid according to RFC 1034/1035.\"\\n\\t\\t\\traise DisallowedHost(msg)"
  },
  {
    "code": "def fmgr_fwobj_ipv6(fmg, paramgram):\\n    if paramgram[\"mode\"] in ['set', 'add']:\\n        datagram = {\\n            \"comment\": paramgram[\"comment\"],\\n            \"name\": paramgram[\"name\"],\\n            \"color\": paramgram[\"color\"],\\n            \"dynamic_mapping\": [],\\n            \"visibility\": paramgram[\"visibility\"],\\n            \"type\": paramgram[\"ipv6\"]\\n        }\\n        if datagram[\"type\"] == \"group\":\\n            url = '/pm/config/adom/{adom}/obj/firewall/addrgrp6'.format(adom=paramgram[\"adom\"])\\n        else:\\n            url = '/pm/config/adom/{adom}/obj/firewall/address6'.format(adom=paramgram[\"adom\"])\\n        if datagram[\"type\"] == \"ip\":\\n            datagram[\"type\"] = \"ipprefix\"\\n            datagram[\"ip6\"] = paramgram[\"ipv6addr\"]\\n        if datagram[\"type\"] == \"iprange\":\\n            datagram[\"start-ip\"] = paramgram[\"start-ip\"]\\n            datagram[\"end-ip\"] = paramgram[\"end-ip\"]\\n        if datagram[\"type\"] == \"group\":\\n            datagram = None\\n            datagram = {\\n                \"comment\": paramgram[\"comment\"],\\n                \"name\": paramgram[\"group_name\"],\\n                \"color\": paramgram[\"color\"],\\n                \"visibility\": paramgram[\"visibility\"]\\n            }\\n            members = []\\n            group_members = paramgram[\"group_members\"].replace(\" \", \"\")\\n            try:\\n                for member in group_members.split(\",\"):\\n                    members.append(member)\\n            except Exception:\\n                pass\\n            datagram[\"member\"] = members\\n    if paramgram[\"mode\"] == \"delete\":\\n        if paramgram[\"ipv6\"] == \"group\":\\n            datagram = {}\\n            url = '/pm/config/adom/{adom}/obj/firewall/addrgrp6/{name}'.format(adom=paramgram[\"adom\"],\\n                                                                               name=paramgram[\"group_name\"])\\n        else:\\n            datagram = {}\\n            url = '/pm/config/adom/{adom}/obj/firewall/address6/{name}'.format(adom=paramgram[\"adom\"],\\n                                                                               name=paramgram[\"name\"])\\n    if paramgram[\"mode\"] == \"set\":\\n        response = fmg.set(url, datagram)\\n        return response\\n    if paramgram[\"mode\"] == \"add\":\\n        response = fmg.add(url, datagram)\\n        return response\\n    if paramgram[\"mode\"] == \"delete\":\\n        response = fmg.delete(url, datagram)\\n        return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_fwobj_address (#52773)",
    "fixed_code": "def fmgr_fwobj_ipv6(fmgr, paramgram):\\n    if paramgram[\"mode\"] in ['set', 'add']:\\n        datagram = {\\n            \"comment\": paramgram[\"comment\"],\\n            \"name\": paramgram[\"name\"],\\n            \"color\": paramgram[\"color\"],\\n            \"dynamic_mapping\": [],\\n            \"visibility\": paramgram[\"visibility\"],\\n            \"type\": paramgram[\"ipv6\"]\\n        }\\n        if datagram[\"type\"] == \"group\":\\n            url = '/pm/config/adom/{adom}/obj/firewall/addrgrp6'.format(adom=paramgram[\"adom\"])\\n        else:\\n            url = '/pm/config/adom/{adom}/obj/firewall/address6'.format(adom=paramgram[\"adom\"])\\n        if datagram[\"type\"] == \"ip\":\\n            datagram[\"type\"] = \"ipprefix\"\\n            datagram[\"ip6\"] = paramgram[\"ipv6addr\"]\\n        if datagram[\"type\"] == \"iprange\":\\n            datagram[\"start-ip\"] = paramgram[\"start-ip\"]\\n            datagram[\"end-ip\"] = paramgram[\"end-ip\"]\\n        if datagram[\"type\"] == \"group\":\\n            datagram = None\\n            datagram = {\\n                \"comment\": paramgram[\"comment\"],\\n                \"name\": paramgram[\"group_name\"],\\n                \"color\": paramgram[\"color\"],\\n                \"visibility\": paramgram[\"visibility\"]\\n            }\\n            members = []\\n            group_members = paramgram[\"group_members\"].replace(\" \", \"\")\\n            try:\\n                for member in group_members.split(\",\"):\\n                    members.append(member)\\n            except Exception:\\n                pass\\n            datagram[\"member\"] = members\\n    if paramgram[\"mode\"] == \"delete\":\\n        if paramgram[\"ipv6\"] == \"group\":\\n            datagram = {}\\n            url = '/pm/config/adom/{adom}/obj/firewall/addrgrp6/{name}'.format(adom=paramgram[\"adom\"],\\n                                                                               name=paramgram[\"group_name\"])\\n        else:\\n            datagram = {}\\n            url = '/pm/config/adom/{adom}/obj/firewall/address6/{name}'.format(adom=paramgram[\"adom\"],\\n                                                                               name=paramgram[\"name\"])\\n    response = fmgr.process_request(url, datagram, paramgram[\"mode\"])\\n    return response"
  },
  {
    "code": "def __invert__(self):\\n        return type(self)(self.queryset, negated=(not self.negated), **self.extra)\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __invert__(self):\\n        return type(self)(self.queryset, negated=(not self.negated), **self.extra)\\n    @property"
  },
  {
    "code": "def modify_service_processor_network(self, params=None):\\n        if self.parameters['is_enabled'] is False:\\n            if params.get('is_enabled') and len(params) > 1:\\n                self.module.fail_json(msg='Error: Cannot modify any other parameter for a service processor network if option \"is_enabled\" is set to false.')\\n            elif params.get('is_enabled') is None and len(params) > 0:\\n                self.module.fail_json(msg='Error: Cannot modify a service processor network if it is disabled.')\\n        sp_modify = netapp_utils.zapi.NaElement('service-processor-network-modify')\\n        sp_modify.add_new_child(\"node\", self.parameters['node'])\\n        sp_modify.add_new_child(\"address-type\", self.parameters['address_type'])\\n        sp_attributes = dict()\\n        for item_key in self.parameters:\\n            if item_key in self.na_helper.zapi_string_keys:\\n                zapi_key = self.na_helper.zapi_string_keys.get(item_key)\\n                sp_attributes[zapi_key] = self.parameters[item_key]\\n            elif item_key in self.na_helper.zapi_bool_keys:\\n                zapi_key = self.na_helper.zapi_bool_keys.get(item_key)\\n                sp_attributes[zapi_key] = self.na_helper.get_value_for_bool(from_zapi=False, value=self.parameters[item_key])\\n            elif item_key in self.na_helper.zapi_int_keys:\\n                zapi_key = self.na_helper.zapi_int_keys.get(item_key)\\n                sp_attributes[zapi_key] = self.na_helper.get_value_for_int(from_zapi=False, value=self.parameters[item_key])\\n        sp_modify.translate_struct(sp_attributes)\\n        try:\\n            self.server.invoke_successfully(sp_modify, enable_tunneling=True)\\n            if self.parameters.get('wait_for_completion'):\\n                retries = 10\\n                while self.get_sp_network_status() == 'in_progress' and retries > 0:\\n                    time.sleep(10)\\n                    retries = retries - 1\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error modifying service processor network: %s' % (to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def modify_service_processor_network(self, params=None):\\n        if self.parameters['is_enabled'] is False:\\n            if params.get('is_enabled') and len(params) > 1:\\n                self.module.fail_json(msg='Error: Cannot modify any other parameter for a service processor network if option \"is_enabled\" is set to false.')\\n            elif params.get('is_enabled') is None and len(params) > 0:\\n                self.module.fail_json(msg='Error: Cannot modify a service processor network if it is disabled.')\\n        sp_modify = netapp_utils.zapi.NaElement('service-processor-network-modify')\\n        sp_modify.add_new_child(\"node\", self.parameters['node'])\\n        sp_modify.add_new_child(\"address-type\", self.parameters['address_type'])\\n        sp_attributes = dict()\\n        for item_key in self.parameters:\\n            if item_key in self.na_helper.zapi_string_keys:\\n                zapi_key = self.na_helper.zapi_string_keys.get(item_key)\\n                sp_attributes[zapi_key] = self.parameters[item_key]\\n            elif item_key in self.na_helper.zapi_bool_keys:\\n                zapi_key = self.na_helper.zapi_bool_keys.get(item_key)\\n                sp_attributes[zapi_key] = self.na_helper.get_value_for_bool(from_zapi=False, value=self.parameters[item_key])\\n            elif item_key in self.na_helper.zapi_int_keys:\\n                zapi_key = self.na_helper.zapi_int_keys.get(item_key)\\n                sp_attributes[zapi_key] = self.na_helper.get_value_for_int(from_zapi=False, value=self.parameters[item_key])\\n        sp_modify.translate_struct(sp_attributes)\\n        try:\\n            self.server.invoke_successfully(sp_modify, enable_tunneling=True)\\n            if self.parameters.get('wait_for_completion'):\\n                retries = 10\\n                while self.get_sp_network_status() == 'in_progress' and retries > 0:\\n                    time.sleep(10)\\n                    retries = retries - 1\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error modifying service processor network: %s' % (to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def process_response(self, request, response):\\n\\t\\ttry:\\n\\t\\t\\taccessed = request.session.accessed\\n\\t\\t\\tmodified = request.session.modified\\n\\t\\t\\tempty = request.session.is_empty()\\n\\t\\texcept AttributeError:\\n\\t\\t\\tpass\\n\\t\\telse:\\n\\t\\t\\tif settings.SESSION_COOKIE_NAME in request.COOKIES and empty:\\n\\t\\t\\t\\tresponse.delete_cookie(settings.SESSION_COOKIE_NAME,\\n\\t\\t\\t\\t\\tdomain=settings.SESSION_COOKIE_DOMAIN)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif accessed:\\n\\t\\t\\t\\t\\tpatch_vary_headers(response, ('Cookie',))\\n\\t\\t\\t\\tif (modified or settings.SESSION_SAVE_EVERY_REQUEST) and not empty:\\n\\t\\t\\t\\t\\tif request.session.get_expire_at_browser_close():\\n\\t\\t\\t\\t\\t\\tmax_age = None\\n\\t\\t\\t\\t\\t\\texpires = None\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tmax_age = request.session.get_expiry_age()\\n\\t\\t\\t\\t\\t\\texpires_time = time.time() + max_age\\n\\t\\t\\t\\t\\t\\texpires = cookie_date(expires_time)\\n\\t\\t\\t\\t\\tif response.status_code != 500:\\n\\t\\t\\t\\t\\t\\trequest.session.save()\\n\\t\\t\\t\\t\\t\\tresponse.set_cookie(settings.SESSION_COOKIE_NAME,\\n\\t\\t\\t\\t\\t\\t\\t\\trequest.session.session_key, max_age=max_age,\\n\\t\\t\\t\\t\\t\\t\\t\\texpires=expires, domain=settings.SESSION_COOKIE_DOMAIN,\\n\\t\\t\\t\\t\\t\\t\\t\\tpath=settings.SESSION_COOKIE_PATH,\\n\\t\\t\\t\\t\\t\\t\\t\\tsecure=settings.SESSION_COOKIE_SECURE or None,\\n\\t\\t\\t\\t\\t\\t\\t\\thttponly=settings.SESSION_COOKIE_HTTPONLY or None)\\n\\t\\treturn response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_response(self, request, response):\\n\\t\\ttry:\\n\\t\\t\\taccessed = request.session.accessed\\n\\t\\t\\tmodified = request.session.modified\\n\\t\\t\\tempty = request.session.is_empty()\\n\\t\\texcept AttributeError:\\n\\t\\t\\tpass\\n\\t\\telse:\\n\\t\\t\\tif settings.SESSION_COOKIE_NAME in request.COOKIES and empty:\\n\\t\\t\\t\\tresponse.delete_cookie(settings.SESSION_COOKIE_NAME,\\n\\t\\t\\t\\t\\tdomain=settings.SESSION_COOKIE_DOMAIN)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif accessed:\\n\\t\\t\\t\\t\\tpatch_vary_headers(response, ('Cookie',))\\n\\t\\t\\t\\tif (modified or settings.SESSION_SAVE_EVERY_REQUEST) and not empty:\\n\\t\\t\\t\\t\\tif request.session.get_expire_at_browser_close():\\n\\t\\t\\t\\t\\t\\tmax_age = None\\n\\t\\t\\t\\t\\t\\texpires = None\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tmax_age = request.session.get_expiry_age()\\n\\t\\t\\t\\t\\t\\texpires_time = time.time() + max_age\\n\\t\\t\\t\\t\\t\\texpires = cookie_date(expires_time)\\n\\t\\t\\t\\t\\tif response.status_code != 500:\\n\\t\\t\\t\\t\\t\\trequest.session.save()\\n\\t\\t\\t\\t\\t\\tresponse.set_cookie(settings.SESSION_COOKIE_NAME,\\n\\t\\t\\t\\t\\t\\t\\t\\trequest.session.session_key, max_age=max_age,\\n\\t\\t\\t\\t\\t\\t\\t\\texpires=expires, domain=settings.SESSION_COOKIE_DOMAIN,\\n\\t\\t\\t\\t\\t\\t\\t\\tpath=settings.SESSION_COOKIE_PATH,\\n\\t\\t\\t\\t\\t\\t\\t\\tsecure=settings.SESSION_COOKIE_SECURE or None,\\n\\t\\t\\t\\t\\t\\t\\t\\thttponly=settings.SESSION_COOKIE_HTTPONLY or None)\\n\\t\\treturn response"
  },
  {
    "code": "def queue_command(self, key, command, priority=1, queue=None):\\n        if key not in self.queued_tasks and key not in self.running:\\n            self.logger.info(\"Adding to queue: {}\".format(command))\\n            self.queued_tasks[key] = (command, priority, queue)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Handle queued tasks from multiple jobs/executors\\n\\nWhen Scheduler is run with `\u2014num-runs`, there can be multiple\\nSchedulers and Executors all trying to run tasks. For queued tasks,\\nScheduler was previously only trying to run tasks that it itself had\\nqueued \u2014 but that doesn\u2019t work if the Scheduler is restarting. This PR\\nreverts that behavior and adds two types of \u201cbest effort\u201d executions \u2014\\nbefore running a TI, executors check if it is already running, and\\nbefore ending executors call sync() one last time",
    "fixed_code": "def queue_command(self, task_instance, command, priority=1, queue=None):\\n        key = task_instance.key\\n        if key not in self.queued_tasks and key not in self.running:\\n            self.logger.info(\"Adding to queue: {}\".format(command))\\n            self.queued_tasks[key] = (command, priority, queue, task_instance)"
  },
  {
    "code": "def test_types(self):\\n\\t\\tinstance = self.model(value=1)\\n\\t\\tself.assertIsInstance(instance.value, int)\\n\\t\\tinstance.save()\\n\\t\\tself.assertIsInstance(instance.value, int)\\n\\t\\tinstance = self.model.objects.get()\\n\\t\\tself.assertIsInstance(instance.value, int)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_types(self):\\n\\t\\tinstance = self.model(value=1)\\n\\t\\tself.assertIsInstance(instance.value, int)\\n\\t\\tinstance.save()\\n\\t\\tself.assertIsInstance(instance.value, int)\\n\\t\\tinstance = self.model.objects.get()\\n\\t\\tself.assertIsInstance(instance.value, int)"
  },
  {
    "code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in seq:\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or arg == \"\"\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "fix comment typos, use not arg instead of arg == \"\", add test coverage for inside of the final if needquotes: within subprocess.list2cmdline().",
    "fixed_code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in seq:\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)"
  },
  {
    "code": "def time_dayofweek(self, tz, freq):\\n        self.ts.dayofweek",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_dayofweek(self, tz, freq):\\n        self.ts.dayofweek"
  },
  {
    "code": "def get_token(request):\\n\\tif \"CSRF_COOKIE\" not in request.META:\\n\\t\\tcsrf_secret = _get_new_csrf_string()\\n\\t\\trequest.META[\"CSRF_COOKIE\"] = _salt_cipher_secret(csrf_secret)\\n\\telse:\\n\\t\\tcsrf_secret = _unsalt_cipher_token(request.META[\"CSRF_COOKIE\"])\\n\\trequest.META[\"CSRF_COOKIE_USED\"] = True\\n\\treturn _salt_cipher_secret(csrf_secret)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #31291 -- Renamed salt to mask for CSRF tokens.",
    "fixed_code": "def get_token(request):\\n\\tif \"CSRF_COOKIE\" not in request.META:\\n\\t\\tcsrf_secret = _get_new_csrf_string()\\n\\t\\trequest.META[\"CSRF_COOKIE\"] = _mask_cipher_secret(csrf_secret)\\n\\telse:\\n\\t\\tcsrf_secret = _unmask_cipher_token(request.META[\"CSRF_COOKIE\"])\\n\\trequest.META[\"CSRF_COOKIE_USED\"] = True\\n\\treturn _mask_cipher_secret(csrf_secret)"
  },
  {
    "code": "def _maybe_convert_for_string_atom(\\n    name: str,\\n    block: \"Block\",\\n    existing_col,\\n    min_itemsize,\\n    nan_rep,\\n    encoding,\\n    errors,\\n    block_columns: List[str],\\n):\\n    if not block.is_object:\\n        return block.values\\n    dtype_name = block.dtype.name\\n    inferred_type = lib.infer_dtype(block.values, skipna=False)\\n    if inferred_type == \"date\":\\n        raise TypeError(\"[date] is not implemented as a table column\")\\n    elif inferred_type == \"datetime\":\\n        raise TypeError(\\n            \"too many timezones in this block, create separate data columns\"\\n        )\\n    elif not (inferred_type == \"string\" or dtype_name == \"object\"):\\n        return block.values\\n    blocks: List[\"Block\"] = block.fillna(nan_rep, downcast=False)\\n    assert len(blocks) == 1\\n    block = blocks[0]\\n    data = block.values\\n    inferred_type = lib.infer_dtype(data, skipna=False)\\n    if inferred_type != \"string\":\\n        for i in range(block.shape[0]):\\n            col = block.iget(i)\\n            inferred_type = lib.infer_dtype(col, skipna=False)\\n            if inferred_type != \"string\":\\n                error_column_label = (\\n                    block_columns[i] if len(block_columns) > i else f\"No.{i}\"\\n                )\\n                raise TypeError(\\n                    f\"Cannot serialize the column [{error_column_label}]\\n\"\\n                    f\"because its data contents are not [string] but \"\\n                    f\"[{inferred_type}] object dtype\"\\n                )\\n    data_converted = _convert_string_array(data, encoding, errors).reshape(data.shape)\\n    assert data_converted.shape == block.shape, (data_converted.shape, block.shape)\\n    itemsize = data_converted.itemsize\\n    if isinstance(min_itemsize, dict):\\n        min_itemsize = int(min_itemsize.get(name) or min_itemsize.get(\"values\") or 0)\\n    itemsize = max(min_itemsize or 0, itemsize)\\n    if existing_col is not None:\\n        eci = existing_col.validate_col(itemsize)\\n        if eci > itemsize:\\n            itemsize = eci\\n    data_converted = data_converted.astype(f\"|S{itemsize}\", copy=False)\\n    return data_converted",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _maybe_convert_for_string_atom(\\n    name: str,\\n    block: \"Block\",\\n    existing_col,\\n    min_itemsize,\\n    nan_rep,\\n    encoding,\\n    errors,\\n    block_columns: List[str],\\n):\\n    if not block.is_object:\\n        return block.values\\n    dtype_name = block.dtype.name\\n    inferred_type = lib.infer_dtype(block.values, skipna=False)\\n    if inferred_type == \"date\":\\n        raise TypeError(\"[date] is not implemented as a table column\")\\n    elif inferred_type == \"datetime\":\\n        raise TypeError(\\n            \"too many timezones in this block, create separate data columns\"\\n        )\\n    elif not (inferred_type == \"string\" or dtype_name == \"object\"):\\n        return block.values\\n    blocks: List[\"Block\"] = block.fillna(nan_rep, downcast=False)\\n    assert len(blocks) == 1\\n    block = blocks[0]\\n    data = block.values\\n    inferred_type = lib.infer_dtype(data, skipna=False)\\n    if inferred_type != \"string\":\\n        for i in range(block.shape[0]):\\n            col = block.iget(i)\\n            inferred_type = lib.infer_dtype(col, skipna=False)\\n            if inferred_type != \"string\":\\n                error_column_label = (\\n                    block_columns[i] if len(block_columns) > i else f\"No.{i}\"\\n                )\\n                raise TypeError(\\n                    f\"Cannot serialize the column [{error_column_label}]\\n\"\\n                    f\"because its data contents are not [string] but \"\\n                    f\"[{inferred_type}] object dtype\"\\n                )\\n    data_converted = _convert_string_array(data, encoding, errors).reshape(data.shape)\\n    assert data_converted.shape == block.shape, (data_converted.shape, block.shape)\\n    itemsize = data_converted.itemsize\\n    if isinstance(min_itemsize, dict):\\n        min_itemsize = int(min_itemsize.get(name) or min_itemsize.get(\"values\") or 0)\\n    itemsize = max(min_itemsize or 0, itemsize)\\n    if existing_col is not None:\\n        eci = existing_col.validate_col(itemsize)\\n        if eci > itemsize:\\n            itemsize = eci\\n    data_converted = data_converted.astype(f\"|S{itemsize}\", copy=False)\\n    return data_converted"
  },
  {
    "code": "def splituser(host):\\n    global _userprog\\n    if _userprog is None:\\n        import re\\n        _userprog = re.compile('^(.*)@(.*)$')\\n    match = _userprog.match(host)\\n    if match: return match.group(1, 2)\\n    return None, host\\n_passwdprog = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def splituser(host):\\n    global _userprog\\n    if _userprog is None:\\n        import re\\n        _userprog = re.compile('^(.*)@(.*)$')\\n    match = _userprog.match(host)\\n    if match: return match.group(1, 2)\\n    return None, host\\n_passwdprog = None"
  },
  {
    "code": "def reset(self):\\n        pass",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Patch #1436130: codecs.lookup() now returns a CodecInfo object (a subclass of tuple) that provides incremental decoders and encoders (a way to use stateful codecs without the stream API). Functions codecs.getincrementaldecoder() and codecs.getincrementalencoder() have been added.",
    "fixed_code": "def reset(self):\\nclass IncrementalDecoder(object):"
  },
  {
    "code": "def xs(self, key):\\n        if key not in self.index:\\n            raise Exception('No cross-section for %s' % key)\\n        loc = self.index.indexMap[key]\\n        theSlice = self.values[loc, :].copy()\\n        xsIndex = self.columns\\n        result = Series(theSlice, index=xsIndex)\\n        if self.objects is not None and len(self.objects.columns) > 0:\\n            result = result.append(self.objects.xs(key))\\n        return result\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "get view when calling xs in DataMatrix",
    "fixed_code": "def xs(self, key, copy=True):\\n        if key not in self.index:\\n            raise Exception('No cross-section for %s' % key)\\n        loc = self.index.indexMap[key]\\n        xs = self.values[loc, :]\\n        if copy:\\n            xs = xs.copy()\\n        result = Series(xs, index=self.columns)\\n        if self.objects is not None and len(self.objects.columns) > 0:\\n            if not copy:\\n                raise Exception('cannot get view of mixed-type cross-section')\\n            result = result.append(self.objects.xs(key))\\n        return result\\n    @property"
  },
  {
    "code": "def auc_score(y_true, y_score):\\n    if len(np.unique(y_true)) != 2:\\n        raise ValueError(\"AUC is defined for binary classification only\")\\n    fpr, tpr, tresholds = roc_curve(y_true, y_score)\\n    return auc(fpr, tpr, reorder=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def auc_score(y_true, y_score):\\n    if len(np.unique(y_true)) != 2:\\n        raise ValueError(\"AUC is defined for binary classification only\")\\n    fpr, tpr, tresholds = roc_curve(y_true, y_score)\\n    return auc(fpr, tpr, reorder=True)"
  },
  {
    "code": "def write(self, data, offset, count):\\n\\t\\tself._run_command([\"write\"], data, str(offset), str(count))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def write(self, data, offset, count):\\n\\t\\tself._run_command([\"write\"], data, str(offset), str(count))"
  },
  {
    "code": "def end_integer(self):\\n        raw = self.get_data()\\n        if raw.startswith('0x') or raw.startswith('0X'):\\n            self.add_object(int(raw, 16))\\n        else:\\n            self.add_object(int(raw))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def end_integer(self):\\n        raw = self.get_data()\\n        if raw.startswith('0x') or raw.startswith('0X'):\\n            self.add_object(int(raw, 16))\\n        else:\\n            self.add_object(int(raw))"
  },
  {
    "code": "def is_excluded_dtype(dtype) -> bool:\\n\\t\\tis_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)\\n\\t\\treturn any(is_excluded(dtype) for is_excluded in is_excluded_checks)\\n\\treturn _is_dtype(arr_or_dtype, condition)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_excluded_dtype(dtype) -> bool:\\n\\t\\tis_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)\\n\\t\\treturn any(is_excluded(dtype) for is_excluded in is_excluded_checks)\\n\\treturn _is_dtype(arr_or_dtype, condition)"
  },
  {
    "code": "def __repr__(self):\\n        if self.values is None or len(self.columns) == 0:\\n            output = 'Empty DataMatrix\\nIndex: %s' % repr(self.index)\\n        elif 0 < len(self.index) < 1000 and self.values.shape[1] < 10:\\n            output = self.toString(to_stdout=False)\\n        else:\\n            output = str(self.__class__) + '\\n'\\n            output = output + self.info(to_stdout=False)\\n        return output",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "not weighting untransformed data in PanelOLS. changed DataFrame and DataMatrix.toString to take a buffer instead of a to_stdout flag",
    "fixed_code": "def __repr__(self):\\n        buffer = StringIO()\\n        if self.values is None or len(self.columns) == 0:\\n            buffer.write('Empty DataMatrix\\nIndex: %s' % repr(self.index))\\n        elif 0 < len(self.index) < 500 and self.values.shape[1] < 10:\\n            self.toString(buffer=buffer)\\n        else:\\n            print >> buffer, str(self.__class__)\\n            self.info(buffer=buffer)\\n        return buffer.getvalue()"
  },
  {
    "code": "def rolling_cov(arg1, arg2, window, min_periods=None, time_rule=None):\\n    X, Y = _prep_binary(arg1, arg2)\\n    mean = lambda x: rolling_mean(x, window, min_periods, time_rule)\\n    bias_adj = window / (window - 1)\\n    return (mean(X * Y) - mean(X) * mean(Y)) * bias_adj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rolling_cov(arg1, arg2, window, min_periods=None, time_rule=None):\\n    X, Y = _prep_binary(arg1, arg2)\\n    mean = lambda x: rolling_mean(x, window, min_periods, time_rule)\\n    bias_adj = window / (window - 1)\\n    return (mean(X * Y) - mean(X) * mean(Y)) * bias_adj"
  },
  {
    "code": "def _create_fn(name, args, body, globals=None, locals=None,\\n               return_type=MISSING):\\n    if locals is None:\\n        locals = {}\\n    return_annotation = ''\\n    if return_type is not MISSING:\\n        locals['_return_type'] = return_type\\n        return_annotation = '->_return_type'\\n    args = ','.join(args)\\n    body = '\\n'.join(f' {b}' for b in body)\\n    txt = f'def {name}({args}){return_annotation}:\\n{body}'\\n    exec(txt, globals, locals)\\n    return locals[name]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-32513: Make it easier to override dunders in dataclasses. (GH-5366)\\n\\nClass authors no longer need to specify repr=False if they want to provide a custom __repr__ for dataclasses. The same thing applies for the other dunder methods that the dataclass decorator adds. If dataclass finds that a dunder methods is defined in the class, it will not overwrite it.",
    "fixed_code": "def _create_fn(name, args, body, *, globals=None, locals=None,\\n               return_type=MISSING):\\n    if locals is None:\\n        locals = {}\\n    return_annotation = ''\\n    if return_type is not MISSING:\\n        locals['_return_type'] = return_type\\n        return_annotation = '->_return_type'\\n    args = ','.join(args)\\n    body = '\\n'.join(f' {b}' for b in body)\\n    txt = f'def {name}({args}){return_annotation}:\\n{body}'\\n    exec(txt, globals, locals)\\n    return locals[name]"
  },
  {
    "code": "def merge_provider_auth_provider_param(self, result, provider):\\n        if self.validate_params('auth_provider', provider):\\n            result['auth_provider'] = provider['auth_provider']\\n        elif self.validate_params('auth_provider', self.params):\\n            result['auth_provider'] = self.params['auth_provider']\\n        elif self.validate_params('F5_AUTH_PROVIDER', os.environ):\\n            result['auth_provider'] = os.environ['F5_AUTH_PROVIDER']\\n        else:\\n            result['auth_provider'] = None\\n        if result['auth_provider'] is not None and '__omit_place_holder__' in result['auth_provider']:\\n            result['auth_provider'] = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "removes args from the code and corrects few missed tests (#58175)",
    "fixed_code": "def merge_provider_auth_provider_param(self, result, provider):\\n        if self.validate_params('auth_provider', provider):\\n            result['auth_provider'] = provider['auth_provider']\\n        elif self.validate_params('F5_AUTH_PROVIDER', os.environ):\\n            result['auth_provider'] = os.environ['F5_AUTH_PROVIDER']\\n        else:\\n            result['auth_provider'] = None\\n        if result['auth_provider'] is not None and '__omit_place_holder__' in result['auth_provider']:\\n            result['auth_provider'] = None"
  },
  {
    "code": "def format_str(src_contents: str, line_length: int) -> FileContent:\\n    src_node = lib2to3_parse(src_contents)\\n    dst_contents = \"\"\\n    future_imports = get_future_imports(src_node)\\n    py36 = is_python36(src_node)\\n    lines = LineGenerator(remove_u_prefix=py36 or \"unicode_literals\" in future_imports)\\n    elt = EmptyLineTracker()\\n    empty_line = Line()\\n    after = 0\\n    for current_line in lines.visit(src_node):\\n        for _ in range(after):\\n            dst_contents += str(empty_line)\\n        before, after = elt.maybe_empty_lines(current_line)\\n        for _ in range(before):\\n            dst_contents += str(empty_line)\\n        for line in split_line(current_line, line_length=line_length, py36=py36):\\n            dst_contents += str(line)\\n    return dst_contents",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add support for pyi files (#210)\\n\\nFixes #207",
    "fixed_code": "def format_str(\\n    src_contents: str, line_length: int, *, is_pyi: bool = False\\n) -> FileContent:\\n    src_node = lib2to3_parse(src_contents)\\n    dst_contents = \"\"\\n    future_imports = get_future_imports(src_node)\\n    elt = EmptyLineTracker(is_pyi=is_pyi)\\n    py36 = is_python36(src_node)\\n    lines = LineGenerator(\\n        remove_u_prefix=py36 or \"unicode_literals\" in future_imports, is_pyi=is_pyi\\n    )\\n    empty_line = Line()\\n    after = 0\\n    for current_line in lines.visit(src_node):\\n        for _ in range(after):\\n            dst_contents += str(empty_line)\\n        before, after = elt.maybe_empty_lines(current_line)\\n        for _ in range(before):\\n            dst_contents += str(empty_line)\\n        for line in split_line(current_line, line_length=line_length, py36=py36):\\n            dst_contents += str(line)\\n    return dst_contents"
  },
  {
    "code": "def __call__(self, inputs, *args, **kwargs):\\n\\tscope = kwargs.pop('scope', None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __call__(self, inputs, *args, **kwargs):\\n\\tscope = kwargs.pop('scope', None)"
  },
  {
    "code": "def sum(self):\\n        try:\\n            return self._cython_agg_general('add')\\n        except Exception:\\n            return self.aggregate(np.sum)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: groupby bug with flex aggregate functions. and hierarchical columns weren't being passed on",
    "fixed_code": "def sum(self, axis=None):\\n        try:\\n            if self.axis == 0:\\n                return self._cython_agg_general('add')\\n            else:\\n                return self.aggregate(lambda x: x.sum(axis=self.axis))\\n        except Exception:\\n            return self.aggregate(lambda x: np.sum(x, axis=self.axis))"
  },
  {
    "code": "def _use_dynamic_x(ax, data):\\n    freq = _get_index_freq(data.index)\\n    ax_freq = _get_ax_freq(ax)\\n    if freq is None:  \\n        freq = ax_freq\\n    else:  \\n        if (ax_freq is None) and (len(ax.get_lines()) > 0):\\n            return False\\n    if freq is None:\\n        return False\\n    freq = _get_period_alias(freq)\\n    if freq is None:\\n        return False\\n    if isinstance(data.index, ABCDatetimeIndex):\\n        base = get_freq_code(freq)[0]\\n        x = data.index\\n        if base <= FreqGroup.FR_DAY:\\n            return x[:1].is_normalized\\n        return Period(x[0], freq).to_timestamp().tz_localize(x.tz) == x[0]\\n    return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _use_dynamic_x(ax, data):\\n    freq = _get_index_freq(data.index)\\n    ax_freq = _get_ax_freq(ax)\\n    if freq is None:  \\n        freq = ax_freq\\n    else:  \\n        if (ax_freq is None) and (len(ax.get_lines()) > 0):\\n            return False\\n    if freq is None:\\n        return False\\n    freq = _get_period_alias(freq)\\n    if freq is None:\\n        return False\\n    if isinstance(data.index, ABCDatetimeIndex):\\n        base = get_freq_code(freq)[0]\\n        x = data.index\\n        if base <= FreqGroup.FR_DAY:\\n            return x[:1].is_normalized\\n        return Period(x[0], freq).to_timestamp().tz_localize(x.tz) == x[0]\\n    return True"
  },
  {
    "code": "def convert_values(self, value, field, connection):\\n        if connection.ops.oracle:\\n            value = super(GeoQuery, self).convert_values(value, field or GeomField(), connection)\\n        if value is None:\\n            pass\\n        elif isinstance(field, DistanceField):\\n            value = Distance(**{field.distance_att : value})\\n        elif isinstance(field, AreaField):\\n            value = Area(**{field.area_att : value})\\n        elif isinstance(field, (GeomField, GeometryField)) and value:\\n            value = Geometry(value)\\n        return value",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def convert_values(self, value, field, connection):\\n        if connection.ops.oracle:\\n            value = super(GeoQuery, self).convert_values(value, field or GeomField(), connection)\\n        if value is None:\\n            pass\\n        elif isinstance(field, DistanceField):\\n            value = Distance(**{field.distance_att : value})\\n        elif isinstance(field, AreaField):\\n            value = Area(**{field.area_att : value})\\n        elif isinstance(field, (GeomField, GeometryField)) and value:\\n            value = Geometry(value)\\n        return value"
  },
  {
    "code": "def parse_header(line):\\n    plist = _parse_header_params(b';' + line)\\n    key = plist.pop(0).lower().decode('ascii')\\n    pdict = {}\\n    for p in plist:\\n        i = p.find(b'=')\\n        if i >= 0:\\n            has_encoding = False\\n            name = p[:i].strip().lower().decode('ascii')\\n            if name.endswith('*'):\\n                name = name[:-1]\\n                has_encoding = True\\n            value = p[i + 1:].strip()\\n            if has_encoding:\\n                encoding, lang, value = value.split(b\"'\")\\n                if six.PY3:\\n                    value = unquote(value.decode(), encoding=encoding.decode())\\n                else:\\n                    value = unquote(value).decode(encoding)\\n            if len(value) >= 2 and value[:1] == value[-1:] == b'\"':\\n                value = value[1:-1]\\n                value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\\n            pdict[name] = value\\n    return key, pdict",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24209 -- Prevented crash when parsing malformed RFC 2231 headers\\n\\nThanks Tom Christie for the report and review.",
    "fixed_code": "def parse_header(line):\\n    plist = _parse_header_params(b';' + line)\\n    key = plist.pop(0).lower().decode('ascii')\\n    pdict = {}\\n    for p in plist:\\n        i = p.find(b'=')\\n        if i >= 0:\\n            has_encoding = False\\n            name = p[:i].strip().lower().decode('ascii')\\n            if name.endswith('*'):\\n                name = name[:-1]\\n                if p.count(b\"'\") == 2:\\n                    has_encoding = True\\n            value = p[i + 1:].strip()\\n            if has_encoding:\\n                encoding, lang, value = value.split(b\"'\")\\n                if six.PY3:\\n                    value = unquote(value.decode(), encoding=encoding.decode())\\n                else:\\n                    value = unquote(value).decode(encoding)\\n            if len(value) >= 2 and value[:1] == value[-1:] == b'\"':\\n                value = value[1:-1]\\n                value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\\n            pdict[name] = value\\n    return key, pdict"
  },
  {
    "code": "def _upload_file_obj(self,\\n                         file_obj,\\n                         key,\\n                         bucket_name=None,\\n                         replace=False,\\n                         encrypt=False,\\n                         acl_policy=None):\\n        if not replace and self.check_for_key(key, bucket_name):\\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\\n        extra_args = {}\\n        if encrypt:\\n            extra_args['ServerSideEncryption'] = \"AES256\"\\n        if acl_policy:\\n            extra_args['ACL'] = acl_policy\\n        client = self.get_conn()\\n        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _upload_file_obj(self,\\n                         file_obj,\\n                         key,\\n                         bucket_name=None,\\n                         replace=False,\\n                         encrypt=False,\\n                         acl_policy=None):\\n        if not replace and self.check_for_key(key, bucket_name):\\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\\n        extra_args = {}\\n        if encrypt:\\n            extra_args['ServerSideEncryption'] = \"AES256\"\\n        if acl_policy:\\n            extra_args['ACL'] = acl_policy\\n        client = self.get_conn()\\n        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)"
  },
  {
    "code": "def __repr__(self):\\n        return '<{} \"{}\">'.format(self.__class__.__name__, self)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-88116: Enhance the inspect frame APIs to use the extended position information (GH-91531)",
    "fixed_code": "def __repr__(self):\\n        return ('Traceback(filename={!r}, lineno={!r}, function={!r}, '\\n               'code_context={!r}, index={!r}, positions={!r})'.format(\\n                self.filename, self.lineno, self.function, self.code_context,\\n                self.index, self.positions))"
  },
  {
    "code": "def print_context(ds, **kwargs):\\n        pprint(kwargs)\\n        print(ds)\\n        return 'Whatever you return gets printed in the logs'\\n    run_this = PythonOperator(\\n        task_id='print_the_context',\\n        python_callable=print_context,\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def print_context(ds, **kwargs):\\n        pprint(kwargs)\\n        print(ds)\\n        return 'Whatever you return gets printed in the logs'\\n    run_this = PythonOperator(\\n        task_id='print_the_context',\\n        python_callable=print_context,\\n    )"
  },
  {
    "code": "def patch_response_headers(response, cache_timeout=None):\\n    if cache_timeout is None:\\n        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\\n    if cache_timeout < 0:\\n        cache_timeout = 0 \\n    if settings.USE_ETAGS and not response.has_header('ETag'):\\n        response['ETag'] = '\"%s\"' % hashlib.md5(response.content).hexdigest()\\n    if not response.has_header('Last-Modified'):\\n        response['Last-Modified'] = http_date()\\n    if not response.has_header('Expires'):\\n        response['Expires'] = http_date(time.time() + cache_timeout)\\n    patch_cache_control(response, max_age=cache_timeout)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #16003 -- Restored compatibility of the admin when using USE_ETAGS. Thanks for the initial patch, pterk.",
    "fixed_code": "def patch_response_headers(response, cache_timeout=None):\\n    if cache_timeout is None:\\n        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\\n    if cache_timeout < 0:\\n        cache_timeout = 0 \\n    if settings.USE_ETAGS and not response.has_header('ETag'):\\n        if hasattr(response, 'render') and callable(response.render):\\n            response.add_post_render_callback(_set_response_etag)\\n        else:\\n            response = _set_response_etag(response)\\n    if not response.has_header('Last-Modified'):\\n        response['Last-Modified'] = http_date()\\n    if not response.has_header('Expires'):\\n        response['Expires'] = http_date(time.time() + cache_timeout)\\n    patch_cache_control(response, max_age=cache_timeout)"
  },
  {
    "code": "def apply(\\n\\t\\tself,\\n\\t\\tfunc,\\n\\t\\traw: bool = False,\\n\\t\\tengine: str = \"cython\",\\n\\t\\tengine_kwargs: Optional[Dict] = None,\\n\\t\\targs: Optional[Tuple] = None,\\n\\t\\tkwargs: Optional[Dict] = None,\\n\\t):\\n\\t\\tif args is None:\\n\\t\\t\\targs = ()\\n\\t\\tif kwargs is None:\\n\\t\\t\\tkwargs = {}\\n\\t\\tkwargs.pop(\"_level\", None)\\n\\t\\tkwargs.pop(\"floor\", None)\\n\\t\\twindow = self._get_window()\\n\\t\\toffset = calculate_center_offset(window) if self.center else 0\\n\\t\\tif not is_bool(raw):\\n\\t\\t\\traise ValueError(\"raw parameter must be `True` or `False`\")\\n\\t\\tif engine == \"cython\":\\n\\t\\t\\tif engine_kwargs is not None:\\n\\t\\t\\t\\traise ValueError(\"cython engine does not accept engine_kwargs\")\\n\\t\\t\\tapply_func = self._generate_cython_apply_func(\\n\\t\\t\\t\\targs, kwargs, raw, offset, func\\n\\t\\t\\t)\\n\\t\\telif engine == \"numba\":\\n\\t\\t\\tif raw is False:\\n\\t\\t\\t\\traise ValueError(\"raw must be `True` when using the numba engine\")\\n\\t\\t\\tif func in self._numba_func_cache:\\n\\t\\t\\t\\tapply_func = self._numba_func_cache[func]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tapply_func = generate_numba_apply_func(\\n\\t\\t\\t\\t\\targs, kwargs, func, engine_kwargs\\n\\t\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\traise ValueError(\"engine must be either 'numba' or 'cython'\")\\n\\t\\treturn self._apply(\\n\\t\\t\\tapply_func,\\n\\t\\t\\tcenter=False,\\n\\t\\t\\tfloor=0,\\n\\t\\t\\tname=func,\\n\\t\\t\\tuse_numba_cache=engine == \"numba\",\\n\\t\\t\\traw=raw,\\n\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def apply(\\n\\t\\tself,\\n\\t\\tfunc,\\n\\t\\traw: bool = False,\\n\\t\\tengine: str = \"cython\",\\n\\t\\tengine_kwargs: Optional[Dict] = None,\\n\\t\\targs: Optional[Tuple] = None,\\n\\t\\tkwargs: Optional[Dict] = None,\\n\\t):\\n\\t\\tif args is None:\\n\\t\\t\\targs = ()\\n\\t\\tif kwargs is None:\\n\\t\\t\\tkwargs = {}\\n\\t\\tkwargs.pop(\"_level\", None)\\n\\t\\tkwargs.pop(\"floor\", None)\\n\\t\\twindow = self._get_window()\\n\\t\\toffset = calculate_center_offset(window) if self.center else 0\\n\\t\\tif not is_bool(raw):\\n\\t\\t\\traise ValueError(\"raw parameter must be `True` or `False`\")\\n\\t\\tif engine == \"cython\":\\n\\t\\t\\tif engine_kwargs is not None:\\n\\t\\t\\t\\traise ValueError(\"cython engine does not accept engine_kwargs\")\\n\\t\\t\\tapply_func = self._generate_cython_apply_func(\\n\\t\\t\\t\\targs, kwargs, raw, offset, func\\n\\t\\t\\t)\\n\\t\\telif engine == \"numba\":\\n\\t\\t\\tif raw is False:\\n\\t\\t\\t\\traise ValueError(\"raw must be `True` when using the numba engine\")\\n\\t\\t\\tif func in self._numba_func_cache:\\n\\t\\t\\t\\tapply_func = self._numba_func_cache[func]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tapply_func = generate_numba_apply_func(\\n\\t\\t\\t\\t\\targs, kwargs, func, engine_kwargs\\n\\t\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\traise ValueError(\"engine must be either 'numba' or 'cython'\")\\n\\t\\treturn self._apply(\\n\\t\\t\\tapply_func,\\n\\t\\t\\tcenter=False,\\n\\t\\t\\tfloor=0,\\n\\t\\t\\tname=func,\\n\\t\\t\\tuse_numba_cache=engine == \"numba\",\\n\\t\\t\\traw=raw,\\n\\t\\t)"
  },
  {
    "code": "def _hstack(X, sparse_):\\n    if sparse_:\\n        return sparse.hstack(X).tocsr()\\n    else:\\n        X = [f.toarray() if sparse.issparse(f) else f for f in X]\\n        return np.hstack(X)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _hstack(X, sparse_):\\n    if sparse_:\\n        return sparse.hstack(X).tocsr()\\n    else:\\n        X = [f.toarray() if sparse.issparse(f) else f for f in X]\\n        return np.hstack(X)"
  },
  {
    "code": "def pack_fstring(self, n, s):\\n        if n < 0:\\n            raise ValueError, 'fstring size must be nonnegative'\\n        data = s[:n]\\n        n = ((n+3)//4)*4\\n        data = data + (n - len(data)) * '\\0'\\n        self.__buf.write(data)\\n    pack_fopaque = pack_fstring",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Move xdrlib over to the bytes type.",
    "fixed_code": "def pack_fstring(self, n, s):\\n        if n < 0:\\n            raise ValueError, 'fstring size must be nonnegative'\\n        data = s[:n]\\n        n = ((n+3)//4)*4\\n        data = data + (n - len(data)) * b'\\0'\\n        self.__buf.write(data)\\n    pack_fopaque = pack_fstring"
  },
  {
    "code": "def invoke(self, args, from_tty):\\n        import re\\n        start = None\\n        end = None\\n        m = re.match(r'\\s*(\\d+)\\s*', args)\\n        if m:\\n            start = int(m.group(0))\\n            end = start + 10\\n        m = re.match(r'\\s*(\\d+)\\s*,\\s*(\\d+)\\s*', args)\\n        if m:\\n            start, end = map(int, m.groups())\\n        frame = Frame.get_selected_bytecode_frame()\\n        if not frame:\\n            print('Unable to locate gdb frame for python bytecode interpreter')\\n            return\\n        pyop = frame.get_pyop()\\n        if not pyop or pyop.is_optimized_out():\\n            print('Unable to read information on python frame')\\n            return\\n        filename = pyop.filename()\\n        lineno = pyop.current_line_num()\\n        if start is None:\\n            start = lineno - 5\\n            end = lineno + 5\\n        if start<1:\\n            start = 1\\n        try:\\n            f = open(os_fsencode(filename), 'r')\\n        except IOError as err:\\n            sys.stdout.write('Unable to open %s: %s\\n'\\n                             % (filename, err))\\n            return\\n        with f:\\n            all_lines = f.readlines()\\n            for i, line in enumerate(all_lines[start-1:end]):\\n                linestr = str(i+start)\\n                if i + start == lineno:\\n                    linestr = '>' + linestr\\n                sys.stdout.write('%4s    %s' % (linestr, line))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def invoke(self, args, from_tty):\\n        import re\\n        start = None\\n        end = None\\n        m = re.match(r'\\s*(\\d+)\\s*', args)\\n        if m:\\n            start = int(m.group(0))\\n            end = start + 10\\n        m = re.match(r'\\s*(\\d+)\\s*,\\s*(\\d+)\\s*', args)\\n        if m:\\n            start, end = map(int, m.groups())\\n        frame = Frame.get_selected_bytecode_frame()\\n        if not frame:\\n            print('Unable to locate gdb frame for python bytecode interpreter')\\n            return\\n        pyop = frame.get_pyop()\\n        if not pyop or pyop.is_optimized_out():\\n            print('Unable to read information on python frame')\\n            return\\n        filename = pyop.filename()\\n        lineno = pyop.current_line_num()\\n        if start is None:\\n            start = lineno - 5\\n            end = lineno + 5\\n        if start<1:\\n            start = 1\\n        try:\\n            f = open(os_fsencode(filename), 'r')\\n        except IOError as err:\\n            sys.stdout.write('Unable to open %s: %s\\n'\\n                             % (filename, err))\\n            return\\n        with f:\\n            all_lines = f.readlines()\\n            for i, line in enumerate(all_lines[start-1:end]):\\n                linestr = str(i+start)\\n                if i + start == lineno:\\n                    linestr = '>' + linestr\\n                sys.stdout.write('%4s    %s' % (linestr, line))"
  },
  {
    "code": "def modify_unix_group(self, params):\\n        if 'users' in params:\\n            self.modify_users_in_group()\\n            if len(params) == 1:\\n                return\\n        group_modify = netapp_utils.zapi.NaElement('name-mapping-unix-group-modify')\\n        group_details = {'group-name': self.parameters['name']}\\n        for key in params:\\n            if key in self.na_helper.zapi_int_keys:\\n                zapi_key = self.na_helper.zapi_int_keys.get(key)\\n                group_details[zapi_key] = self.na_helper.get_value_for_int(from_zapi=True,\\n                                                                           value=params[key])\\n        group_modify.translate_struct(group_details)\\n        try:\\n            self.server.invoke_successfully(group_modify, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error modifying UNIX group %s: %s' % (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def modify_unix_group(self, params):\\n        if 'users' in params:\\n            self.modify_users_in_group()\\n            if len(params) == 1:\\n                return\\n        group_modify = netapp_utils.zapi.NaElement('name-mapping-unix-group-modify')\\n        group_details = {'group-name': self.parameters['name']}\\n        for key in params:\\n            if key in self.na_helper.zapi_int_keys:\\n                zapi_key = self.na_helper.zapi_int_keys.get(key)\\n                group_details[zapi_key] = self.na_helper.get_value_for_int(from_zapi=True,\\n                                                                           value=params[key])\\n        group_modify.translate_struct(group_details)\\n        try:\\n            self.server.invoke_successfully(group_modify, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error modifying UNIX group %s: %s' % (self.parameters['name'], to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def load_int(self):\\n        data = self.readline()\\n        try:\\n            self.append(int(data))\\n        except ValueError:\\n            self.append(long(data))\\n    dispatch[INT] = load_int",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_int(self):\\n        data = self.readline()\\n        try:\\n            self.append(int(data))\\n        except ValueError:\\n            self.append(long(data))\\n    dispatch[INT] = load_int"
  },
  {
    "code": "def handle(self, **options):\\n        locale = options['locale']\\n        exclude = options['exclude']\\n        self.verbosity = options['verbosity']\\n        if options['fuzzy']:\\n            self.program_options = self.program_options + ['-f']\\n        if find_command(self.program) is None:\\n            raise CommandError(\"Can't find %s. Make sure you have GNU gettext \"\\n                               \"tools 0.15 or newer installed.\" % self.program)\\n        basedirs = [os.path.join('conf', 'locale'), 'locale']\\n        if os.environ.get('DJANGO_SETTINGS_MODULE'):\\n            from django.conf import settings\\n            basedirs.extend(settings.LOCALE_PATHS)\\n        for dirpath, dirnames, filenames in os.walk('.', topdown=True):\\n            for dirname in dirnames:\\n                if dirname == 'locale':\\n                    basedirs.append(os.path.join(dirpath, dirname))\\n        basedirs = set(map(os.path.abspath, filter(os.path.isdir, basedirs)))\\n        if not basedirs:\\n            raise CommandError(\"This script should be run from the Django Git \"\\n                               \"checkout or your project or app tree, or with \"\\n                               \"the settings module specified.\")\\n        all_locales = []\\n        for basedir in basedirs:\\n            locale_dirs = filter(os.path.isdir, glob.glob('%s/*' % basedir))\\n            all_locales.extend(map(os.path.basename, locale_dirs))\\n        locales = locale or all_locales\\n        locales = set(locales).difference(exclude)\\n        self.has_errors = False\\n        for basedir in basedirs:\\n            if locales:\\n                dirs = [os.path.join(basedir, l, 'LC_MESSAGES') for l in locales]\\n            else:\\n                dirs = [basedir]\\n            locations = []\\n            for ldir in dirs:\\n                for dirpath, dirnames, filenames in os.walk(ldir):\\n                    locations.extend((dirpath, f) for f in filenames if f.endswith('.po'))\\n            if locations:\\n                self.compile_messages(locations)\\n        if self.has_errors:\\n            raise CommandError('compilemessages generated one or more errors.')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #29973 -- Added compilemessages --ignore option.",
    "fixed_code": "def handle(self, **options):\\n        locale = options['locale']\\n        exclude = options['exclude']\\n        ignore_patterns = set(options['ignore_patterns'])\\n        self.verbosity = options['verbosity']\\n        if options['fuzzy']:\\n            self.program_options = self.program_options + ['-f']\\n        if find_command(self.program) is None:\\n            raise CommandError(\"Can't find %s. Make sure you have GNU gettext \"\\n                               \"tools 0.15 or newer installed.\" % self.program)\\n        basedirs = [os.path.join('conf', 'locale'), 'locale']\\n        if os.environ.get('DJANGO_SETTINGS_MODULE'):\\n            from django.conf import settings\\n            basedirs.extend(settings.LOCALE_PATHS)\\n        for dirpath, dirnames, filenames in os.walk('.', topdown=True):\\n            for dirname in dirnames:\\n                if is_ignored_path(os.path.normpath(os.path.join(dirpath, dirname)), ignore_patterns):\\n                    dirnames.remove(dirname)\\n                elif dirname == 'locale':\\n                    basedirs.append(os.path.join(dirpath, dirname))\\n        basedirs = set(map(os.path.abspath, filter(os.path.isdir, basedirs)))\\n        if not basedirs:\\n            raise CommandError(\"This script should be run from the Django Git \"\\n                               \"checkout or your project or app tree, or with \"\\n                               \"the settings module specified.\")\\n        all_locales = []\\n        for basedir in basedirs:\\n            locale_dirs = filter(os.path.isdir, glob.glob('%s/*' % basedir))\\n            all_locales.extend(map(os.path.basename, locale_dirs))\\n        locales = locale or all_locales\\n        locales = set(locales).difference(exclude)\\n        self.has_errors = False\\n        for basedir in basedirs:\\n            if locales:\\n                dirs = [os.path.join(basedir, l, 'LC_MESSAGES') for l in locales]\\n            else:\\n                dirs = [basedir]\\n            locations = []\\n            for ldir in dirs:\\n                for dirpath, dirnames, filenames in os.walk(ldir):\\n                    locations.extend((dirpath, f) for f in filenames if f.endswith('.po'))\\n            if locations:\\n                self.compile_messages(locations)\\n        if self.has_errors:\\n            raise CommandError('compilemessages generated one or more errors.')"
  },
  {
    "code": "def _nanmin(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    dtype = values.dtype\\n    if skipna and not issubclass(dtype.type,\\n                                 (np.integer, np.datetime64)):\\n        values = values.copy()\\n        np.putmask(values, mask, np.inf)\\n    if issubclass(dtype.type, np.datetime64):\\n        values = values.view(np.int64)\\n    if (values.dtype == np.object_\\n        and sys.version_info[0] >= 3):  \\n        import __builtin__\\n        if values.ndim > 1:\\n            apply_ax = axis if axis is not None else 0\\n            result = np.apply_along_axis(__builtin__.min, apply_ax, values)\\n        else:\\n            result = __builtin__.min(values)\\n    else:\\n        if ((axis is not None and values.shape[axis] == 0)\\n             or values.size == 0):\\n            result = com.ensure_float(values.sum(axis))\\n            result.fill(np.nan)\\n        else:\\n            result = values.min(axis)\\n    if issubclass(dtype.type, np.datetime64):\\n        if not isinstance(result, np.ndarray):\\n            result = lib.Timestamp(result)\\n        else:\\n            result = result.view(dtype)\\n    return _maybe_null_out(result, axis, mask)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _nanmin(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    dtype = values.dtype\\n    if skipna and not issubclass(dtype.type,\\n                                 (np.integer, np.datetime64)):\\n        values = values.copy()\\n        np.putmask(values, mask, np.inf)\\n    if issubclass(dtype.type, np.datetime64):\\n        values = values.view(np.int64)\\n    if (values.dtype == np.object_\\n        and sys.version_info[0] >= 3):  \\n        import __builtin__\\n        if values.ndim > 1:\\n            apply_ax = axis if axis is not None else 0\\n            result = np.apply_along_axis(__builtin__.min, apply_ax, values)\\n        else:\\n            result = __builtin__.min(values)\\n    else:\\n        if ((axis is not None and values.shape[axis] == 0)\\n             or values.size == 0):\\n            result = com.ensure_float(values.sum(axis))\\n            result.fill(np.nan)\\n        else:\\n            result = values.min(axis)\\n    if issubclass(dtype.type, np.datetime64):\\n        if not isinstance(result, np.ndarray):\\n            result = lib.Timestamp(result)\\n        else:\\n            result = result.view(dtype)\\n    return _maybe_null_out(result, axis, mask)"
  },
  {
    "code": "def can_handle_archive(self):\\n        binaries = (\\n            ('unzip', 'cmd_path'),\\n            ('zipinfo', 'zipinfo_cmd_path'),\\n        )\\n        missing = []\\n        for b in binaries:\\n            try:\\n                setattr(self, b[1], get_bin_path(b[0]))\\n            except ValueError:\\n                missing.append(b[0])\\n        if missing:\\n            return False, \"Unable to find required '{missing}' binary in the path.\".format(missing=\"' or '\".join(missing))\\n        cmd = [self.cmd_path, '-l', self.src]\\n        rc, out, err = self.module.run_command(cmd)\\n        if rc == 0:\\n            return True, None\\n        return False, 'Command \"%s\" could not handle archive.' % self.cmd_path",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def can_handle_archive(self):\\n        binaries = (\\n            ('unzip', 'cmd_path'),\\n            ('zipinfo', 'zipinfo_cmd_path'),\\n        )\\n        missing = []\\n        for b in binaries:\\n            try:\\n                setattr(self, b[1], get_bin_path(b[0]))\\n            except ValueError:\\n                missing.append(b[0])\\n        if missing:\\n            return False, \"Unable to find required '{missing}' binary in the path.\".format(missing=\"' or '\".join(missing))\\n        cmd = [self.cmd_path, '-l', self.src]\\n        rc, out, err = self.module.run_command(cmd)\\n        if rc == 0:\\n            return True, None\\n        return False, 'Command \"%s\" could not handle archive.' % self.cmd_path"
  },
  {
    "code": "def describe(self, percentile_width=50):\\n        numdata = self._get_numeric_data()\\n        if len(numdata.columns) == 0:\\n            return DataFrame(dict((k, v.describe())\\n                                  for k, v in self.iteritems()),\\n                                  columns=self.columns)\\n        lb = .5 * (1. - percentile_width/100.)\\n        ub = 1. - lb",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def describe(self, percentile_width=50):\\n        numdata = self._get_numeric_data()\\n        if len(numdata.columns) == 0:\\n            return DataFrame(dict((k, v.describe())\\n                                  for k, v in self.iteritems()),\\n                                  columns=self.columns)\\n        lb = .5 * (1. - percentile_width/100.)\\n        ub = 1. - lb"
  },
  {
    "code": "def check_class_weight_balanced_linear_classifier(name, Classifier):\\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\\n                  [1.0, 1.0], [1.0, 0.0]])\\n    y = np.array([1, 1, 1, -1, -1])\\n    classifier = Classifier()\\n    if hasattr(classifier, \"n_iter\"):\\n        classifier.set_params(n_iter=1000)\\n    if hasattr(classifier, \"max_iter\"):\\n        classifier.set_params(max_iter=1000)\\n    set_random_state(classifier)\\n    classifier.set_params(class_weight='balanced')\\n    coef_balanced = classifier.fit(X, y).coef_.copy()\\n    n_samples = len(y)\\n    n_classes = float(len(np.unique(y)))\\n    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),\\n                    -1: n_samples / (np.sum(y == -1) * n_classes)}\\n    classifier.set_params(class_weight=class_weight)\\n    coef_manual = classifier.fit(X, y).coef_.copy()\\n    assert_allclose(coef_balanced, coef_manual)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: add test for common test check_class_weight_balanced_linear_classifier (#13313)",
    "fixed_code": "def check_class_weight_balanced_linear_classifier(name, Classifier):\\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\\n                  [1.0, 1.0], [1.0, 0.0]])\\n    y = np.array([1, 1, 1, -1, -1])\\n    classifier = Classifier()\\n    if hasattr(classifier, \"n_iter\"):\\n        classifier.set_params(n_iter=1000)\\n    if hasattr(classifier, \"max_iter\"):\\n        classifier.set_params(max_iter=1000)\\n    set_random_state(classifier)\\n    classifier.set_params(class_weight='balanced')\\n    coef_balanced = classifier.fit(X, y).coef_.copy()\\n    n_samples = len(y)\\n    n_classes = float(len(np.unique(y)))\\n    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),\\n                    -1: n_samples / (np.sum(y == -1) * n_classes)}\\n    classifier.set_params(class_weight=class_weight)\\n    coef_manual = classifier.fit(X, y).coef_.copy()\\n    assert_allclose(coef_balanced, coef_manual,\\n                    err_msg=\"Classifier %s is not computing\"\\n                    \" class_weight=balanced properly.\"\\n                    % name)"
  },
  {
    "code": "def _concat_frames(frames, index, columns=None, axis=0):\\n    if len(frames) == 1:\\n        return frames[0]\\n    if axis == 0:\\n        new_index = _concat_indexes([x.index for x in frames])\\n        if columns is None:\\n            new_columns = frames[0].columns\\n        else:\\n            new_columns = columns\\n    else:\\n        new_columns = _concat_indexes([x.columns for x in frames])\\n        new_index = index\\n    if frames[0]._is_mixed_type:\\n        new_data = {}\\n        for col in new_columns:\\n            new_data[col] = np.concatenate([x[col].values for x in frames])\\n        return DataFrame(new_data, index=new_index, columns=new_columns)\\n    else:\\n        new_values = np.concatenate([x.values for x in frames], axis=axis)\\n        result = DataFrame(new_values, index=new_index, columns=new_columns)\\n        return result.reindex(index=index, columns=columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _concat_frames(frames, index, columns=None, axis=0):\\n    if len(frames) == 1:\\n        return frames[0]\\n    if axis == 0:\\n        new_index = _concat_indexes([x.index for x in frames])\\n        if columns is None:\\n            new_columns = frames[0].columns\\n        else:\\n            new_columns = columns\\n    else:\\n        new_columns = _concat_indexes([x.columns for x in frames])\\n        new_index = index\\n    if frames[0]._is_mixed_type:\\n        new_data = {}\\n        for col in new_columns:\\n            new_data[col] = np.concatenate([x[col].values for x in frames])\\n        return DataFrame(new_data, index=new_index, columns=new_columns)\\n    else:\\n        new_values = np.concatenate([x.values for x in frames], axis=axis)\\n        result = DataFrame(new_values, index=new_index, columns=new_columns)\\n        return result.reindex(index=index, columns=columns)"
  },
  {
    "code": "def g_lasso(X, alpha, cov_init=None, mode='cd', tol=1e-4,\\n            max_iter=100, verbose=False, return_costs=False):\\n    _, n_features = X.shape\\n    mle = empirical_covariance(X)\\n    if alpha == 0:\\n        return mle\\n    if cov_init is None:\\n        covariance_ = mle.copy()\\n        covariance_.flat[::n_features + 1] += alpha\\n    else:\\n        covariance_ = cov_init.copy()\\n        covariance_.flat[::n_features + 1] = mle.flat[::n_features + 1] + alpha\\n    indices = np.arange(n_features)\\n    precision_ = linalg.inv(covariance_)\\n    costs = list()\\n    for i in xrange(max_iter):\\n        for idx in xrange(n_features):\\n            sub_covariance = covariance_[indices != idx].T[indices != idx]\\n            row = mle[idx, indices != idx]\\n            if mode == 'cd':\\n                coefs = -precision_[indices != idx, idx]/precision_[idx, idx]\\n                coefs, _, _ = cd_fast.enet_coordinate_descent_gram(coefs,\\n                                            alpha, 0, sub_covariance,\\n                                            row, row, max_iter, tol)\\n            else:\\n                _, _, coefs = lars_path(sub_covariance, row,\\n                                        Xy=row, Gram=sub_covariance,\\n                                        alpha_min=alpha/(n_features-1),\\n                                        copy_Gram=True,\\n                                        method='lars')\\n                coefs = coefs[:, -1]\\n            precision_[idx, idx] = 1./(covariance_[idx, idx] -\\n                        np.dot(covariance_[indices != idx, idx], coefs))\\n            precision_[indices != idx, idx] = -precision_[idx, idx]*coefs\\n            precision_[idx, indices != idx] = -precision_[idx, idx]*coefs\\n            coefs = np.dot(sub_covariance, coefs)\\n            covariance_[idx, indices != idx] = coefs\\n            covariance_[indices != idx, idx] = coefs\\n        d_gap = _dual_gap(mle, precision_, alpha)\\n        if verbose or return_costs:\\n            cost = _objective(mle, precision_, alpha)\\n            if verbose:\\n                print '[g_lasso] Iteration % 3i, cost %.4f, dual gap %.3e' % (\\n                                                    i, cost, d_gap)\\n            if return_costs:\\n                costs.append((cost, d_gap))\\n        if np.abs(d_gap) < tol:\\n            break\\n    else:\\n        warnings.warn('g_lasso: did not converge after %i iteration:'\\n                        'dual gap: %.3e' % (max_iter, d_gap))\\n    if return_costs:\\n        return covariance_, precision_, costs\\n    return covariance_, precision_",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Glasso: don't penalize the diagonal",
    "fixed_code": "def g_lasso(X, alpha, cov_init=None, mode='cd', tol=1e-4,\\n            max_iter=100, verbose=False, return_costs=False):\\n    _, n_features = X.shape\\n    mle = empirical_covariance(X)\\n    if alpha == 0:\\n        return mle\\n    if cov_init is None:\\n        covariance_ = mle.copy()\\n    else:\\n        covariance_ = cov_init.copy()\\n        covariance_.flat[::n_features + 1] = mle.flat[::n_features + 1]\\n    indices = np.arange(n_features)\\n    precision_ = linalg.inv(covariance_)\\n    costs = list()\\n    for i in xrange(max_iter):\\n        for idx in xrange(n_features):\\n            sub_covariance = covariance_[indices != idx].T[indices != idx]\\n            row = mle[idx, indices != idx]\\n            if mode == 'cd':\\n                coefs = -precision_[indices != idx, idx]/precision_[idx, idx]\\n                coefs, _, _ = cd_fast.enet_coordinate_descent_gram(coefs,\\n                                            alpha, 0, sub_covariance,\\n                                            row, row, max_iter, tol)\\n            else:\\n                _, _, coefs = lars_path(sub_covariance, row,\\n                                        Xy=row, Gram=sub_covariance,\\n                                        alpha_min=alpha/(n_features-1),\\n                                        copy_Gram=True,\\n                                        method='lars')\\n                coefs = coefs[:, -1]\\n            precision_[idx, idx] = 1./(covariance_[idx, idx] -\\n                        np.dot(covariance_[indices != idx, idx], coefs))\\n            precision_[indices != idx, idx] = -precision_[idx, idx]*coefs\\n            precision_[idx, indices != idx] = -precision_[idx, idx]*coefs\\n            coefs = np.dot(sub_covariance, coefs)\\n            covariance_[idx, indices != idx] = coefs\\n            covariance_[indices != idx, idx] = coefs\\n        d_gap = _dual_gap(mle, precision_, alpha)\\n        if verbose or return_costs:\\n            cost = _objective(mle, precision_, alpha)\\n            if verbose:\\n                print '[g_lasso] Iteration % 3i, cost %.4f, dual gap %.3e' % (\\n                                                    i, cost, d_gap)\\n            if return_costs:\\n                costs.append((cost, d_gap))\\n        if np.abs(d_gap) < tol:\\n            break\\n    else:\\n        warnings.warn('g_lasso: did not converge after %i iteration:'\\n                        'dual gap: %.3e' % (max_iter, d_gap))\\n    if return_costs:\\n        return covariance_, precision_, costs\\n    return covariance_, precision_"
  },
  {
    "code": "def set_printoptions(precision=None, column_space=None, max_rows=None,\\n                     max_columns=None, colheader_justify=None,\\n                     max_colwidth=None, notebook_repr_html=None,\\n                     date_dayfirst=None, date_yearfirst=None,\\n                     multi_sparse=None):\\n    if precision is not None:\\n        print_config.precision = precision\\n    if column_space is not None:\\n        print_config.column_space = column_space\\n    if max_rows is not None:\\n        print_config.max_rows = max_rows\\n    if max_colwidth is not None:\\n        print_config.max_colwidth = max_colwidth\\n    if max_columns is not None:\\n        print_config.max_columns = max_columns\\n    if colheader_justify is not None:\\n        print_config.colheader_justify = colheader_justify\\n    if notebook_repr_html is not None:\\n        print_config.notebook_repr_html = notebook_repr_html\\n    if date_dayfirst is not None:\\n        print_config.date_dayfirst = date_dayfirst\\n    if date_yearfirst is not None:\\n        print_config.date_yearfirst = date_yearfirst\\n    if multi_sparse is not None:\\n        print_config.multi_sparse = multi_sparse",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_printoptions(precision=None, column_space=None, max_rows=None,\\n                     max_columns=None, colheader_justify=None,\\n                     max_colwidth=None, notebook_repr_html=None,\\n                     date_dayfirst=None, date_yearfirst=None,\\n                     multi_sparse=None):\\n    if precision is not None:\\n        print_config.precision = precision\\n    if column_space is not None:\\n        print_config.column_space = column_space\\n    if max_rows is not None:\\n        print_config.max_rows = max_rows\\n    if max_colwidth is not None:\\n        print_config.max_colwidth = max_colwidth\\n    if max_columns is not None:\\n        print_config.max_columns = max_columns\\n    if colheader_justify is not None:\\n        print_config.colheader_justify = colheader_justify\\n    if notebook_repr_html is not None:\\n        print_config.notebook_repr_html = notebook_repr_html\\n    if date_dayfirst is not None:\\n        print_config.date_dayfirst = date_dayfirst\\n    if date_yearfirst is not None:\\n        print_config.date_yearfirst = date_yearfirst\\n    if multi_sparse is not None:\\n        print_config.multi_sparse = multi_sparse"
  },
  {
    "code": "def _reduce(self, op, axis=0, skipna=True, numeric_only=None,\\n                filter_type=None, **kwds):\\n        axis = self._get_axis_number(axis)\\n        f = lambda x: op(x, axis=axis, skipna=skipna, **kwds)\\n        labels = self._get_agg_axis(axis)\\n        if numeric_only is None:\\n            try:\\n                values = self.values\\n                result = f(values)\\n            except Exception:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                result = f(data.values)\\n                labels = data._get_agg_axis(axis)\\n        else:\\n            if numeric_only:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                values = data.values\\n                labels = data._get_agg_axis(axis)\\n            else:\\n                values = self.values\\n            result = f(values)\\n        if result.dtype == np.object_:\\n            try:\\n                if filter_type is None or filter_type == 'numeric':\\n                    result = result.astype(np.float64)\\n                elif filter_type == 'bool' and notnull(result).all():\\n                    result = result.astype(np.bool_)\\n            except (ValueError, TypeError):\\n                pass\\n        return Series(result, index=labels)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _reduce(self, op, axis=0, skipna=True, numeric_only=None,\\n                filter_type=None, **kwds):\\n        axis = self._get_axis_number(axis)\\n        f = lambda x: op(x, axis=axis, skipna=skipna, **kwds)\\n        labels = self._get_agg_axis(axis)\\n        if numeric_only is None:\\n            try:\\n                values = self.values\\n                result = f(values)\\n            except Exception:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                result = f(data.values)\\n                labels = data._get_agg_axis(axis)\\n        else:\\n            if numeric_only:\\n                if filter_type is None or filter_type == 'numeric':\\n                    data = self._get_numeric_data()\\n                elif filter_type == 'bool':\\n                    data = self._get_bool_data()\\n                else:\\n                    raise NotImplementedError\\n                values = data.values\\n                labels = data._get_agg_axis(axis)\\n            else:\\n                values = self.values\\n            result = f(values)\\n        if result.dtype == np.object_:\\n            try:\\n                if filter_type is None or filter_type == 'numeric':\\n                    result = result.astype(np.float64)\\n                elif filter_type == 'bool' and notnull(result).all():\\n                    result = result.astype(np.bool_)\\n            except (ValueError, TypeError):\\n                pass\\n        return Series(result, index=labels)"
  },
  {
    "code": "def get_choices(\\n        self, include_blank=True, blank_choice=BLANK_CHOICE_DASH,\\n        limit_choices_to=None, ordering=(),\\n    ):\\n        limit_choices_to = limit_choices_to or self.limit_choices_to\\n        qs = self.related_model._default_manager.complex_filter(limit_choices_to)\\n        if ordering:\\n            qs = qs.order_by(*ordering)\\n        return (blank_choice if include_blank else []) + [\\n            (x.pk, str(x)) for x in qs\\n        ]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_choices(\\n        self, include_blank=True, blank_choice=BLANK_CHOICE_DASH,\\n        limit_choices_to=None, ordering=(),\\n    ):\\n        limit_choices_to = limit_choices_to or self.limit_choices_to\\n        qs = self.related_model._default_manager.complex_filter(limit_choices_to)\\n        if ordering:\\n            qs = qs.order_by(*ordering)\\n        return (blank_choice if include_blank else []) + [\\n            (x.pk, str(x)) for x in qs\\n        ]"
  },
  {
    "code": "def save(self):\\n\\t\\tself.cleaned_data['code'].delete()",
    "label": 1,
    "bug_type": "security",
    "bug_description": "fix: Remove code from database",
    "fixed_code": "def save(self):\\n\\t\\tif self.get_user().login_code:\\n\\t\\t\\tself.get_user().login_code.delete()"
  },
  {
    "code": "def _nanvar(values, axis=None, skipna=True, ddof=1):\\n    mask = isnull(values)\\n    if axis is not None:\\n        count = (values.shape[axis] - mask.sum(axis)).astype(float)\\n    else:\\n        count = float(values.size - mask.sum())\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    X = _ensure_numeric(values.sum(axis))\\n    XX = _ensure_numeric((values ** 2).sum(axis))\\n    return (XX - X ** 2 / count) / (count - ddof)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _nanvar(values, axis=None, skipna=True, ddof=1):\\n    mask = isnull(values)\\n    if axis is not None:\\n        count = (values.shape[axis] - mask.sum(axis)).astype(float)\\n    else:\\n        count = float(values.size - mask.sum())\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n    X = _ensure_numeric(values.sum(axis))\\n    XX = _ensure_numeric((values ** 2).sum(axis))\\n    return (XX - X ** 2 / count) / (count - ddof)"
  },
  {
    "code": "def replace(self, to_replace, value, inplace=False, filter=None,\\n                regex=False, convert=True, mgr=None):\\n        inplace = validate_bool_kwarg(inplace, 'inplace')\\n        original_to_replace = to_replace\\n        try:\\n            values, _, to_replace, _ = self._try_coerce_args(self.values,\\n                                                             to_replace)\\n            mask = missing.mask_missing(values, to_replace)\\n            if filter is not None:\\n                filtered_out = ~self.mgr_locs.isin(filter)\\n                mask[filtered_out.nonzero()[0]] = False\\n            blocks = self.putmask(mask, value, inplace=inplace)\\n            if convert:\\n                blocks = [b.convert(by_item=True, numeric=False,\\n                                    copy=not inplace) for b in blocks]\\n            return blocks\\n        except (TypeError, ValueError):\\n            if is_object_dtype(self):\\n                raise\\n            block = self.astype(object)\\n            return block.replace(to_replace=original_to_replace,\\n                                 value=value,\\n                                 inplace=inplace,\\n                                 filter=filter,\\n                                 regex=regex,\\n                                 convert=convert)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def replace(self, to_replace, value, inplace=False, filter=None,\\n                regex=False, convert=True, mgr=None):\\n        inplace = validate_bool_kwarg(inplace, 'inplace')\\n        original_to_replace = to_replace\\n        try:\\n            values, _, to_replace, _ = self._try_coerce_args(self.values,\\n                                                             to_replace)\\n            mask = missing.mask_missing(values, to_replace)\\n            if filter is not None:\\n                filtered_out = ~self.mgr_locs.isin(filter)\\n                mask[filtered_out.nonzero()[0]] = False\\n            blocks = self.putmask(mask, value, inplace=inplace)\\n            if convert:\\n                blocks = [b.convert(by_item=True, numeric=False,\\n                                    copy=not inplace) for b in blocks]\\n            return blocks\\n        except (TypeError, ValueError):\\n            if is_object_dtype(self):\\n                raise\\n            block = self.astype(object)\\n            return block.replace(to_replace=original_to_replace,\\n                                 value=value,\\n                                 inplace=inplace,\\n                                 filter=filter,\\n                                 regex=regex,\\n                                 convert=convert)"
  },
  {
    "code": "def _ipaddr_info(host, port, family, type, proto):\\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or host is None:\\n        return None\\n    type &= ~_SOCKET_TYPE_MASK\\n    if type == socket.SOCK_STREAM:\\n        proto = socket.IPPROTO_TCP\\n    elif type == socket.SOCK_DGRAM:\\n        proto = socket.IPPROTO_UDP\\n    else:\\n        return None\\n    if port in {None, '', b''}:\\n        port = 0\\n    elif isinstance(port, (bytes, str)):\\n        port = int(port)\\n    if hasattr(socket, 'inet_pton'):\\n        if family == socket.AF_UNSPEC:\\n            afs = [socket.AF_INET, socket.AF_INET6]\\n        else:\\n            afs = [family]\\n        for af in afs:\\n            try:\\n                if af == socket.AF_INET6:\\n                    socket.inet_pton(af, host.partition('%')[0])\\n                else:\\n                    socket.inet_pton(af, host)\\n                return af, type, proto, '', (host, port)\\n            except OSError:\\n                pass\\n        return None\\n    try:\\n        addr = ipaddress.IPv4Address(host)\\n    except ValueError:\\n        try:\\n            addr = ipaddress.IPv6Address(host.partition('%')[0])\\n        except ValueError:\\n            return None\\n    af = socket.AF_INET if addr.version == 4 else socket.AF_INET6\\n    if family not in (socket.AF_UNSPEC, af):\\n        return None\\n    return af, type, proto, '', (host, port)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio: Fix getaddrinfo to accept service names (for port)\\n\\nPatch by A. Jesse Jiryu Davis",
    "fixed_code": "def _ipaddr_info(host, port, family, type, proto):\\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or host is None:\\n        return None\\n    type &= ~_SOCKET_TYPE_MASK\\n    if type == socket.SOCK_STREAM:\\n        proto = socket.IPPROTO_TCP\\n    elif type == socket.SOCK_DGRAM:\\n        proto = socket.IPPROTO_UDP\\n    else:\\n        return None\\n    if port is None:\\n        port = 0\\n    elif isinstance(port, bytes):\\n        if port == b'':\\n            port = 0\\n        else:\\n            try:\\n                port = int(port)\\n            except ValueError:\\n                port = socket.getservbyname(port.decode('ascii'))\\n    elif isinstance(port, str):\\n        if port == '':\\n            port = 0\\n        else:\\n            try:\\n                port = int(port)\\n            except ValueError:\\n                port = socket.getservbyname(port)\\n    if hasattr(socket, 'inet_pton'):\\n        if family == socket.AF_UNSPEC:\\n            afs = [socket.AF_INET, socket.AF_INET6]\\n        else:\\n            afs = [family]\\n        for af in afs:\\n            try:\\n                if af == socket.AF_INET6:\\n                    socket.inet_pton(af, host.partition('%')[0])\\n                else:\\n                    socket.inet_pton(af, host)\\n                return af, type, proto, '', (host, port)\\n            except OSError:\\n                pass\\n        return None\\n    try:\\n        addr = ipaddress.IPv4Address(host)\\n    except ValueError:\\n        try:\\n            addr = ipaddress.IPv6Address(host.partition('%')[0])\\n        except ValueError:\\n            return None\\n    af = socket.AF_INET if addr.version == 4 else socket.AF_INET6\\n    if family not in (socket.AF_UNSPEC, af):\\n        return None\\n    return af, type, proto, '', (host, port)"
  },
  {
    "code": "def __init__(self, name, mode=DEFAULT_MODE, handle=None):\\n        self._name = name\\n        if handle is None:\\n            self._handle = _dlopen(self._name, mode)\\n        else:\\n            self._handle = handle",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #1798: Add ctypes calling convention that allows safe access to errno (and LastError, on Windows).\\n\\nctypes maintains a module-global, but thread-local, variable that\\ncontains an error number; called 'ctypes_errno' for this discussion.\\nThis variable is a private copy of the systems 'errno' value; the copy\\nis swapped with the 'errno' variable on several occasions.\\n\\nForeign functions created with CDLL(..., use_errno=True), when called,\\nswap the values just before the actual function call, and swapped\\nagain immediately afterwards.  The 'use_errno' parameter defaults to\\nFalse, in this case 'ctypes_errno' is not touched.\\n\\nThe values are also swapped immeditately before and after ctypes\\ncallback functions are called, if the callbacks are constructed using\\nthe new optional use_errno parameter set to True: CFUNCTYPE(..., use_errno=TRUE)\\nor WINFUNCTYPE(..., use_errno=True).\\n\\nTwo new ctypes functions are provided to access the 'ctypes_errno'\\nvalue from Python:\\n\\n- ctypes.set_errno(value) sets ctypes_errno to 'value', the previous\\n  ctypes_errno value is returned.\\n\\n- ctypes.get_errno() returns the current ctypes_errno value.\\n\\n---\\n\\nOn Windows, the same scheme is implemented for the error value which\\nis managed by the GetLastError() and SetLastError() windows api calls.\\n\\nThe ctypes functions are 'ctypes.set_last_error(value)' and\\n'ctypes.get_last_error()', the CDLL and WinDLL optional parameter is\\nnamed 'use_last_error', defaults to False.\\n\\n---\\n\\nOn Windows, TlsSetValue and TlsGetValue calls are used to provide\\nthread local storage for the variables; ctypes compiled with __GNUC__\\nuses __thread variables.",
    "fixed_code": "def __init__(self, name, mode=DEFAULT_MODE, handle=None,\\n                 use_errno=False,\\n                 use_last_error=False):\\n        self._name = name\\n        flags = self._func_flags_\\n        if use_errno:\\n            flags |= _FUNCFLAG_USE_ERRNO\\n        if use_last_error:\\n            flags |= _FUNCFLAG_USE_LASTERROR\\n        class _FuncPtr(_CFuncPtr):\\n            _flags_ = flags\\n            _restype_ = self._func_restype_\\n        self._FuncPtr = _FuncPtr\\n        if handle is None:\\n            self._handle = _dlopen(self._name, mode)\\n        else:\\n            self._handle = handle"
  },
  {
    "code": "def _handle_fromlist(module, fromlist, import_):\\n    if hasattr(module, '__path__'):\\n        if '*' in fromlist:\\n            fromlist = list(fromlist)\\n            fromlist.remove('*')\\n            if hasattr(module, '__all__'):\\n                fromlist.extend(module.__all__)\\n        for x in fromlist:\\n            if not hasattr(module, x):\\n                from_name = '{}.{}'.format(module.__name__, x)\\n                try:\\n                    _call_with_frames_removed(import_, from_name)\\n                except ModuleNotFoundError as exc:\\n                    if exc.name == from_name:\\n                        continue\\n                    raise\\n    return module",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #15767: back out 8a0ed9f63c6e, finishing the removal of ModuleNotFoundError.",
    "fixed_code": "def _handle_fromlist(module, fromlist, import_):\\n    if hasattr(module, '__path__'):\\n        if '*' in fromlist:\\n            fromlist = list(fromlist)\\n            fromlist.remove('*')\\n            if hasattr(module, '__all__'):\\n                fromlist.extend(module.__all__)\\n        for x in fromlist:\\n            if not hasattr(module, x):\\n                from_name = '{}.{}'.format(module.__name__, x)\\n                try:\\n                    _call_with_frames_removed(import_, from_name)\\n                except ImportError as exc:\\n                    if getattr(exc, '_not_found', False):\\n                        if exc.name == from_name:\\n                            continue\\n                    raise\\n    return module"
  },
  {
    "code": "def _split_tablename(table_input, default_project_id, var_name=None):\\n    if '.' not in table_input:\\n        raise ValueError(\\n            'Expected deletion_dataset_table name in the format of '\\n            '<dataset>.<table>. Got: {}'.format(table_input))\\n    if not default_project_id:\\n        raise ValueError(\"INTERNAL: No default project is specified\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-461]  Support autodetected schemas in BigQuery run_load (#3880)",
    "fixed_code": "def _split_tablename(table_input, default_project_id, var_name=None):\\n    if '.' not in table_input:\\n        raise ValueError(\\n            'Expected target table name in the format of '\\n            '<dataset>.<table>. Got: {}'.format(table_input))\\n    if not default_project_id:\\n        raise ValueError(\"INTERNAL: No default project is specified\")"
  },
  {
    "code": "def hash_pandas_object(\\n    obj,\\n    index: bool = True,\\n    encoding: str = \"utf8\",\\n    hash_key: str = _default_hash_key,\\n    categorize: bool = True,\\n):\\n    from pandas import Series\\n    if isinstance(obj, ABCMultiIndex):\\n        return Series(hash_tuples(obj, encoding, hash_key), dtype=\"uint64\", copy=False)\\n    elif isinstance(obj, ABCIndexClass):\\n        h = hash_array(obj.values, encoding, hash_key, categorize).astype(\\n            \"uint64\", copy=False\\n        )\\n        h = Series(h, index=obj, dtype=\"uint64\", copy=False)\\n    elif isinstance(obj, ABCSeries):\\n        h = hash_array(obj.values, encoding, hash_key, categorize).astype(\\n            \"uint64\", copy=False\\n        )\\n        if index:\\n            index_iter = (\\n                hash_pandas_object(\\n                    obj.index,\\n                    index=False,\\n                    encoding=encoding,\\n                    hash_key=hash_key,\\n                    categorize=categorize,\\n                ).values\\n                for _ in [None]\\n            )\\n            arrays = itertools.chain([h], index_iter)\\n            h = _combine_hash_arrays(arrays, 2)\\n        h = Series(h, index=obj.index, dtype=\"uint64\", copy=False)\\n    elif isinstance(obj, ABCDataFrame):\\n        hashes = (hash_array(series.values) for _, series in obj.items())\\n        num_items = len(obj.columns)\\n        if index:\\n            index_hash_generator = (\\n                hash_pandas_object(\\n                    obj.index,\\n                    index=False,\\n                    encoding=encoding,\\n                    hash_key=hash_key,\\n                    categorize=categorize,\\n                ).values  \\n                for _ in [None]\\n            )\\n            num_items += 1\\n            _hashes = itertools.chain(hashes, index_hash_generator)\\n            hashes = (x for x in _hashes)\\n        h = _combine_hash_arrays(hashes, num_items)\\n        h = Series(h, index=obj.index, dtype=\"uint64\", copy=False)\\n    else:\\n        raise TypeError(f\"Unexpected type for hashing {type(obj)}\")\\n    return h",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: Fixed hash_key=None for object values (#30900)",
    "fixed_code": "def hash_pandas_object(\\n    obj,\\n    index: bool = True,\\n    encoding: str = \"utf8\",\\n    hash_key: Optional[str] = _default_hash_key,\\n    categorize: bool = True,\\n):\\n    from pandas import Series\\n    if hash_key is None:\\n        hash_key = _default_hash_key\\n    if isinstance(obj, ABCMultiIndex):\\n        return Series(hash_tuples(obj, encoding, hash_key), dtype=\"uint64\", copy=False)\\n    elif isinstance(obj, ABCIndexClass):\\n        h = hash_array(obj.values, encoding, hash_key, categorize).astype(\\n            \"uint64\", copy=False\\n        )\\n        h = Series(h, index=obj, dtype=\"uint64\", copy=False)\\n    elif isinstance(obj, ABCSeries):\\n        h = hash_array(obj.values, encoding, hash_key, categorize).astype(\\n            \"uint64\", copy=False\\n        )\\n        if index:\\n            index_iter = (\\n                hash_pandas_object(\\n                    obj.index,\\n                    index=False,\\n                    encoding=encoding,\\n                    hash_key=hash_key,\\n                    categorize=categorize,\\n                ).values\\n                for _ in [None]\\n            )\\n            arrays = itertools.chain([h], index_iter)\\n            h = _combine_hash_arrays(arrays, 2)\\n        h = Series(h, index=obj.index, dtype=\"uint64\", copy=False)\\n    elif isinstance(obj, ABCDataFrame):\\n        hashes = (hash_array(series.values) for _, series in obj.items())\\n        num_items = len(obj.columns)\\n        if index:\\n            index_hash_generator = (\\n                hash_pandas_object(\\n                    obj.index,\\n                    index=False,\\n                    encoding=encoding,\\n                    hash_key=hash_key,\\n                    categorize=categorize,\\n                ).values  \\n                for _ in [None]\\n            )\\n            num_items += 1\\n            _hashes = itertools.chain(hashes, index_hash_generator)\\n            hashes = (x for x in _hashes)\\n        h = _combine_hash_arrays(hashes, num_items)\\n        h = Series(h, index=obj.index, dtype=\"uint64\", copy=False)\\n    else:\\n        raise TypeError(f\"Unexpected type for hashing {type(obj)}\")\\n    return h"
  },
  {
    "code": "def str_match(arr, pat, case=True, flags=0, na=np.nan, as_indexer=None):\\n    if not case:\\n        flags |= re.IGNORECASE\\n    regex = re.compile(pat, flags=flags)\\n    if (as_indexer is False) and (regex.groups > 0):\\n        raise ValueError(\"as_indexer=False with a pattern with groups is no \"\\n                         \"longer supported. Use '.str.extract(pat)' instead\")\\n    elif as_indexer is not None:\\n        warnings.warn(\"'as_indexer' keyword was specified but is ignored \"\\n                      \"(match now returns a boolean indexer by default), \"\\n                      \"and will be removed in a future version.\",\\n                      FutureWarning, stacklevel=3)\\n    dtype = bool\\n    f = lambda x: bool(regex.match(x))\\n    return _na_map(f, arr, na, dtype=dtype)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN/DEPR: removed deprecated as_indexer arg from str.match() (#22626)",
    "fixed_code": "def str_match(arr, pat, case=True, flags=0, na=np.nan):\\n    if not case:\\n        flags |= re.IGNORECASE\\n    regex = re.compile(pat, flags=flags)\\n    dtype = bool\\n    f = lambda x: bool(regex.match(x))\\n    return _na_map(f, arr, na, dtype=dtype)"
  },
  {
    "code": "def __iter__(self):\\n        if np.issubdtype(self.dtype, np.datetime64):\\n            return (lib.Timestamp(x) for x in self.values)\\n        else:\\n            return iter(self.values)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Series.__iter__ not dealing with category type well (GH7839)",
    "fixed_code": "def __iter__(self):\\n        if  com.is_categorical_dtype(self.dtype):\\n            return iter(self.values)\\n        elif np.issubdtype(self.dtype, np.datetime64):\\n            return (lib.Timestamp(x) for x in self.values)\\n        else:\\n            return iter(self.values)"
  },
  {
    "code": "def __repr__(self):\\n        base, mult = _gfc(self.freq)\\n        formatted = lib.period_ordinal_to_string(self.ordinal, base, mult)\\n        freqstr = _freq_mod._reverse_period_code_map[base]\\n        if mult == 1:\\n            return \"Period('%s', '%s')\" % (formatted, freqstr)\\n        return (\"Period('%s', '%d%s')\" % (formatted, mult, freqstr))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: remove period multipliers, close #1199",
    "fixed_code": "def __repr__(self):\\n        base, mult = _gfc(self.freq)\\n        formatted = lib.period_ordinal_to_string(self.ordinal, base)\\n        freqstr = _freq_mod._reverse_period_code_map[base]\\n        return \"Period('%s', '%s')\" % (formatted, freqstr)"
  },
  {
    "code": "def __iter__(self):\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Reverted 6a64822bf4632707212314a25a843c862bdb3874.\\n\\nThis commit caused every test that does two or more assertContains to\\nfail, because of #6527. It also made HttpResponse non-pickleable.\\n\\nRefs #13222.",
    "fixed_code": "def __iter__(self):\\n        self._iterator = iter(self._container)\\n        return self"
  },
  {
    "code": "def set_global_from_args(self, param_name, task_name, args, is_without_section=False):\\n\\t\\tdest = self.parser_dest(param_name, task_name, glob=True, is_without_section=is_without_section)\\n\\t\\tif dest is not None:\\n\\t\\t\\tvalue = getattr(args, dest, None)\\n\\t\\t\\tif value:\\n\\t\\t\\t\\tself.set_global(self.parse_from_input(param_name, value))\\n\\t\\t\\telse:  \\n\\t\\t\\t\\tself.reset_global()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix bug where command line Tasks skip config\\n\\nBefore this patch, when you run a task from the command line, it'll\\nignore the defaults you've specified in the config. By config I mean the\\n`[MyClass] my_param: myvalue` syntax in `/etc/luigi/client.cfg`.\\n\\nAfter this patch, the config is honored.\\n\\nI also added another small and related fix, the default shown when doing\\n`--help` will now also honor the config and not only the `default=` value.",
    "fixed_code": "def set_global_from_args(self, param_name, task_name, args, is_without_section=False):\\n\\t\\tdest = self.parser_dest(param_name, task_name, glob=True, is_without_section=is_without_section)\\n\\t\\tif dest is not None:\\n\\t\\t\\tvalue = getattr(args, dest, None)\\n\\t\\t\\tif value:\\n\\t\\t\\t\\tself.set_global(self.parse_from_input(param_name, value, task_name=task_name))\\n\\t\\t\\telse:  \\n\\t\\t\\t\\tself.reset_global()"
  },
  {
    "code": "def decode_generalized_number(extended, extpos, bias, errors):\\n    result = 0\\n    w = 1\\n    j = 0\\n    while 1:\\n        try:\\n            char = ord(extended[extpos])\\n        except IndexError:\\n            if errors == \"strict\":\\n                raise UnicodeError(\"incomplete punicode string\")\\n            return extpos + 1, None\\n        extpos += 1\\n        if 0x41 <= char <= 0x5A: \\n            digit = char - 0x41\\n        elif 0x30 <= char <= 0x39:\\n            digit = char - 22 \\n        elif errors == \"strict\":\\n            raise UnicodeError(\"Invalid extended code point '%s'\"\\n                               % extended[extpos-1])\\n        else:\\n            return extpos, None\\n        t = T(j, bias)\\n        result += digit * w\\n        if digit < t:\\n            return extpos, result\\n        w = w * (36 - t)\\n        j += 1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decode_generalized_number(extended, extpos, bias, errors):\\n    result = 0\\n    w = 1\\n    j = 0\\n    while 1:\\n        try:\\n            char = ord(extended[extpos])\\n        except IndexError:\\n            if errors == \"strict\":\\n                raise UnicodeError(\"incomplete punicode string\")\\n            return extpos + 1, None\\n        extpos += 1\\n        if 0x41 <= char <= 0x5A: \\n            digit = char - 0x41\\n        elif 0x30 <= char <= 0x39:\\n            digit = char - 22 \\n        elif errors == \"strict\":\\n            raise UnicodeError(\"Invalid extended code point '%s'\"\\n                               % extended[extpos-1])\\n        else:\\n            return extpos, None\\n        t = T(j, bias)\\n        result += digit * w\\n        if digit < t:\\n            return extpos, result\\n        w = w * (36 - t)\\n        j += 1"
  },
  {
    "code": "def _resolve_name(name, package, level):\\n    if not hasattr(package, 'rindex'):\\n        raise ValueError(\"'package' not set to a string\")\\n    dot = len(package)\\n    for x in xrange(level, 1, -1):\\n        try:\\n            dot = package.rindex('.', 0, dot)\\n        except ValueError:\\n            raise ValueError(\"attempted relative import beyond top-level \"\\n                              \"package\")\\n    return \"%s.%s\" % (package[:dot], name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _resolve_name(name, package, level):\\n    if not hasattr(package, 'rindex'):\\n        raise ValueError(\"'package' not set to a string\")\\n    dot = len(package)\\n    for x in xrange(level, 1, -1):\\n        try:\\n            dot = package.rindex('.', 0, dot)\\n        except ValueError:\\n            raise ValueError(\"attempted relative import beyond top-level \"\\n                              \"package\")\\n    return \"%s.%s\" % (package[:dot], name)"
  },
  {
    "code": "def encode_7or8bit(msg):\\n    orig = msg.get_payload(decode=True)\\n    if orig is None:\\n        msg['Content-Transfer-Encoding'] = '7bit'\\n        return\\n    try:\\n        if isinstance(orig, str):\\n            orig.encode('ascii')\\n        else:\\n            orig.decode('ascii')\\n    except UnicodeError:\\n        charset = msg.get_charset()\\n        output_cset = charset and charset.output_charset\\n        if output_cset and output_cset.lower().startswith('iso-2022-'):\\n            msg['Content-Transfer-Encoding'] = '7bit'\\n        else:\\n            msg['Content-Transfer-Encoding'] = '8bit'\\n    else:\\n        msg['Content-Transfer-Encoding'] = '7bit'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def encode_7or8bit(msg):\\n    orig = msg.get_payload(decode=True)\\n    if orig is None:\\n        msg['Content-Transfer-Encoding'] = '7bit'\\n        return\\n    try:\\n        if isinstance(orig, str):\\n            orig.encode('ascii')\\n        else:\\n            orig.decode('ascii')\\n    except UnicodeError:\\n        charset = msg.get_charset()\\n        output_cset = charset and charset.output_charset\\n        if output_cset and output_cset.lower().startswith('iso-2022-'):\\n            msg['Content-Transfer-Encoding'] = '7bit'\\n        else:\\n            msg['Content-Transfer-Encoding'] = '8bit'\\n    else:\\n        msg['Content-Transfer-Encoding'] = '7bit'"
  },
  {
    "code": "def _possibly_castable(arr):\\n    kind = arr.dtype.kind\\n    if kind == 'M' or kind == 'm':\\n        return arr.dtype in _DATELIKE_DTYPES\\n    return arr.dtype.name not in _POSSIBLY_CAST_DTYPES",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _possibly_castable(arr):\\n    kind = arr.dtype.kind\\n    if kind == 'M' or kind == 'm':\\n        return arr.dtype in _DATELIKE_DTYPES\\n    return arr.dtype.name not in _POSSIBLY_CAST_DTYPES"
  },
  {
    "code": "def format_exc(etype, evalue, etb, context=5, tb_offset=0):\\n    try:\\n        etype = etype.__name__\\n    except AttributeError:\\n        pass\\n    pyver = 'Python ' + sys.version.split()[0] + ': ' + sys.executable\\n    date = time.ctime(time.time())\\n    pid = 'PID: %i' % os.getpid()\\n    head = '%s%s%s\\n%s%s%s' % (etype, ' ' * (75 - len(str(etype)) - len(date)),\\n                           date, pid, ' ' * (75 - len(str(pid)) - len(pyver)),\\n                           pyver)\\n    linecache.checkcache()\\n    try:\\n        records = _fixed_getframes(etb, context, tb_offset)\\n    except:\\n        raise\\n        print '\\nUnfortunately, your original traceback can not be ' + \\\\n              'constructed.\\n'\\n        return ''\\n    try:\\n        etype_str, evalue_str = map(str, (etype, evalue))\\n    except:\\n        etype, evalue = str, sys.exc_info()[:2]\\n        etype_str, evalue_str = map(str, (etype, evalue))\\n    exception = ['%s: %s' % (etype_str, evalue_str)]\\n    if (not PY3) and type(evalue) is types.InstanceType:\\n        try:\\n            names = [w for w in dir(evalue) if isinstance(w, basestring)]\\n        except:\\n            exception.append(\\n                    'Exception reporting error (object with broken dir()):'\\n                    )\\n            etype_str, evalue_str = map(str, sys.exc_info()[:2])\\n            exception.append('%s: %s' % (etype_str, evalue_str))\\n            names = []\\n        for name in names:\\n            value = safe_repr(getattr(evalue, name))\\n            exception.append('\\n%s%s = %s' % (INDENT, name, value))\\n    frames = format_records(records)\\n    return '%s\\n%s\\n%s' % (head, '\\n'.join(frames), ''.join(exception[0]))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_exc(etype, evalue, etb, context=5, tb_offset=0):\\n    try:\\n        etype = etype.__name__\\n    except AttributeError:\\n        pass\\n    pyver = 'Python ' + sys.version.split()[0] + ': ' + sys.executable\\n    date = time.ctime(time.time())\\n    pid = 'PID: %i' % os.getpid()\\n    head = '%s%s%s\\n%s%s%s' % (etype, ' ' * (75 - len(str(etype)) - len(date)),\\n                           date, pid, ' ' * (75 - len(str(pid)) - len(pyver)),\\n                           pyver)\\n    linecache.checkcache()\\n    try:\\n        records = _fixed_getframes(etb, context, tb_offset)\\n    except:\\n        raise\\n        print '\\nUnfortunately, your original traceback can not be ' + \\\\n              'constructed.\\n'\\n        return ''\\n    try:\\n        etype_str, evalue_str = map(str, (etype, evalue))\\n    except:\\n        etype, evalue = str, sys.exc_info()[:2]\\n        etype_str, evalue_str = map(str, (etype, evalue))\\n    exception = ['%s: %s' % (etype_str, evalue_str)]\\n    if (not PY3) and type(evalue) is types.InstanceType:\\n        try:\\n            names = [w for w in dir(evalue) if isinstance(w, basestring)]\\n        except:\\n            exception.append(\\n                    'Exception reporting error (object with broken dir()):'\\n                    )\\n            etype_str, evalue_str = map(str, sys.exc_info()[:2])\\n            exception.append('%s: %s' % (etype_str, evalue_str))\\n            names = []\\n        for name in names:\\n            value = safe_repr(getattr(evalue, name))\\n            exception.append('\\n%s%s = %s' % (INDENT, name, value))\\n    frames = format_records(records)\\n    return '%s\\n%s\\n%s' % (head, '\\n'.join(frames), ''.join(exception[0]))"
  },
  {
    "code": "def _convert_to_charset(self, value, charset, mime_encode=False):\\n        if not isinstance(value, (bytes, six.text_type)):\\n            value = str(value)\\n        try:\\n            if six.PY3:\\n                if isinstance(value, str):\\n                    value.encode(charset)\\n                else:\\n                    value = value.decode(charset)\\n            else:\\n                if isinstance(value, str):\\n                    value.decode(charset)\\n                else:\\n                    value = value.encode(charset)\\n        except UnicodeError as e:\\n            if mime_encode:\\n                value = str(Header(value, 'utf-8', maxlinelen=sys.maxsize).encode())\\n            else:\\n                e.reason += ', HTTP response headers must be in %s format' % charset\\n                raise\\n        if str('\\n') in value or str('\\r') in value:\\n            raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\\n        return value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #21282 -- Made HttpResponse.serialize_headers accept latin-1\\n\\nThanks Rapha\u00ebl Barrois for the report and the initial patch and\\nAymeric Augustin for the review.",
    "fixed_code": "def _convert_to_charset(self, value, charset, mime_encode=False):\\n        if not isinstance(value, (bytes, six.text_type)):\\n            value = str(value)\\n        try:\\n            if six.PY3:\\n                if isinstance(value, str):\\n                    value.encode(charset)\\n                else:\\n                    value = value.decode(charset)\\n            else:\\n                if isinstance(value, str):\\n                    value.decode(charset)\\n                else:\\n                    value = value.encode(charset)\\n        except UnicodeError as e:\\n            if mime_encode:\\n                value = str(Header(value, 'utf-8', maxlinelen=sys.maxsize).encode())\\n            else:\\n                e.reason += ', HTTP response headers must be in %s format' % charset\\n                raise\\n        if str('\\n') in value or str('\\r') in value:\\n            raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\\n        return value"
  },
  {
    "code": "def time_findall(self, dtype):\\n        self.s.str.findall(\"[A-Z]+\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def time_findall(self, dtype):\\n        self.s.str.findall(\"[A-Z]+\")"
  },
  {
    "code": "def __hash__(self):\\n        if self._is_special:\\n            if self._isnan():\\n                raise TypeError('Cannot hash a NaN value.')\\n            return hash(str(self))\\n        if not self:\\n            return 0\\n        if self._isinteger():\\n            op = _WorkRep(self.to_integral_value())\\n            return hash((-1)**op.sign*op.int*pow(10, op.exp, 2**64-1))\\n        return hash((self._sign,\\n                     self._exp+len(self._int),\\n                     self._int.rstrip('0')))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __hash__(self):\\n        if self._is_special:\\n            if self._isnan():\\n                raise TypeError('Cannot hash a NaN value.')\\n            return hash(str(self))\\n        if not self:\\n            return 0\\n        if self._isinteger():\\n            op = _WorkRep(self.to_integral_value())\\n            return hash((-1)**op.sign*op.int*pow(10, op.exp, 2**64-1))\\n        return hash((self._sign,\\n                     self._exp+len(self._int),\\n                     self._int.rstrip('0')))"
  },
  {
    "code": "def reset_index(self, level=None, drop=False, inplace=False):\\n        if inplace:\\n            new_obj  = self\\n        else:\\n            new_obj = self.copy()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reset_index(self, level=None, drop=False, inplace=False):\\n        if inplace:\\n            new_obj  = self\\n        else:\\n            new_obj = self.copy()"
  },
  {
    "code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    return left_ax, None, right_indexer",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\\n    if len(join_keys) > 1:\\n        if not ((isinstance(right_ax, MultiIndex) and\\n                 len(join_keys) == right_ax.nlevels)):\\n            raise AssertionError(\"If more than one join key is given then \"\\n                                 \"'right_ax' must be a MultiIndex and the \"\\n                                 \"number of join keys must be the number of \"\\n                                 \"levels in right_ax\")\\n        left_indexer, right_indexer = \\\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\\n    else:\\n        jkey = join_keys[0]\\n        left_indexer, right_indexer = \\\\n            _get_single_indexer(jkey, right_ax, sort=sort)\\n    if sort or len(left_ax) != len(left_indexer):\\n        join_index = left_ax.take(left_indexer)\\n        return join_index, left_indexer, right_indexer\\n    return left_ax, None, right_indexer"
  },
  {
    "code": "def DataReader(name, data_source=None, start=None, end=None,\\n               retry_count=3, pause=0):\\n    start, end = _sanitize_dates(start, end)\\n    if(data_source == \"yahoo\"):\\n        return get_data_yahoo(symbols=name, start=start, end=end,\\n                              adjust_price=False, chunk=25,\\n                              retry_count=retry_count, pause=pause)\\n    elif(data_source == \"fred\"):\\n        return get_data_fred(name=name, start=start, end=end)\\n    elif(data_source == \"famafrench\"):\\n        return get_data_famafrench(name=name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use google finance as datasource (test only, still pointing to yahoo finance)",
    "fixed_code": "def DataReader(name, data_source=None, start=None, end=None,\\n               retry_count=3, pause=0):\\n    start, end = _sanitize_dates(start, end)\\n    if(data_source == \"yahoo\"):\\n        return get_data_yahoo(symbols=name, start=start, end=end,\\n                              adjust_price=False, chunk=25,\\n                              retry_count=retry_count, pause=pause)\\n    elif(data_source == \"google\"):\\n        return get_data_google(symbols=name, start=start, end=end,\\n                              adjust_price=False, chunk=25,\\n                              retry_count=retry_count, pause=pause)\\n    elif(data_source == \"fred\"):\\n        return get_data_fred(name=name, start=start, end=end)\\n    elif(data_source == \"famafrench\"):\\n        return get_data_famafrench(name=name)"
  },
  {
    "code": "def __init__(self, methodname):\\n\\t\\tself.__name__ = methodname\\n\\t\\tself.__doc__ = self.getdoc()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: np.ma.compress treated inputs in wrong order; closes #2495",
    "fixed_code": "def __init__(self, methodname, reversed=False):\\n\\t\\tself.__name__ = methodname\\n\\t\\tself.__doc__ = self.getdoc()\\n\\t\\tself.reversed = reversed"
  },
  {
    "code": "def _rollback_to_year(self, other):\\n        num_qtrs = 0\\n        norm = Timestamp(other).tz_localize(None)\\n        start = self._offset.rollback(norm)\\n        if start < norm:\\n            qtr_lens = self.get_weeks(norm)\\n            end = liboffsets.shift_day(start, days=7 * sum(qtr_lens))\\n            assert self._offset.onOffset(end), (start, end, qtr_lens)\\n            tdelta = norm - start\\n            for qlen in qtr_lens:\\n                if qlen * 7 <= tdelta.days:\\n                    num_qtrs += 1\\n                    tdelta -= Timedelta(days=qlen * 7)\\n                else:\\n                    break\\n        else:\\n            tdelta = Timedelta(0)\\n        return start, num_qtrs, tdelta\\n    @apply_wraps",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _rollback_to_year(self, other):\\n        num_qtrs = 0\\n        norm = Timestamp(other).tz_localize(None)\\n        start = self._offset.rollback(norm)\\n        if start < norm:\\n            qtr_lens = self.get_weeks(norm)\\n            end = liboffsets.shift_day(start, days=7 * sum(qtr_lens))\\n            assert self._offset.onOffset(end), (start, end, qtr_lens)\\n            tdelta = norm - start\\n            for qlen in qtr_lens:\\n                if qlen * 7 <= tdelta.days:\\n                    num_qtrs += 1\\n                    tdelta -= Timedelta(days=qlen * 7)\\n                else:\\n                    break\\n        else:\\n            tdelta = Timedelta(0)\\n        return start, num_qtrs, tdelta\\n    @apply_wraps"
  },
  {
    "code": "def realpath(path):\\n        path = normpath(path)\\n        if isinstance(path, bytes):\\n            prefix = b'\\\\\\\\?\\\\'\\n            unc_prefix = b'\\\\\\\\?\\\\UNC\\\\'\\n            new_unc_prefix = b'\\\\\\\\'\\n            cwd = os.getcwdb()\\n            if normcase(path) == normcase(os.fsencode(devnull)):\\n                return b'\\\\\\\\.\\\\NUL'\\n        else:\\n            prefix = '\\\\\\\\?\\\\'\\n            unc_prefix = '\\\\\\\\?\\\\UNC\\\\'\\n            new_unc_prefix = '\\\\\\\\'\\n            cwd = os.getcwd()\\n            if normcase(path) == normcase(devnull):\\n                return '\\\\\\\\.\\\\NUL'\\n        had_prefix = path.startswith(prefix)\\n        if not had_prefix and not isabs(path):\\n            path = join(cwd, path)\\n        try:\\n            path = _getfinalpathname(path)\\n            initial_winerror = 0\\n        except OSError as ex:\\n            initial_winerror = ex.winerror\\n            path = _getfinalpathname_nonstrict(path)\\n        if not had_prefix and path.startswith(prefix):\\n            if path.startswith(unc_prefix):\\n                spath = new_unc_prefix + path[len(unc_prefix):]\\n            else:\\n                spath = path[len(prefix):]\\n            try:\\n                if _getfinalpathname(spath) == path:\\n                    path = spath\\n            except OSError as ex:\\n                if ex.winerror == initial_winerror:\\n                    path = spath\\n        return path",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-43757: Make pathlib use os.path.realpath() to resolve symlinks in a path (GH-25264)\\n\\nAlso adds a new \"strict\" argument to realpath() to avoid changing the default behaviour of pathlib while sharing the implementation.",
    "fixed_code": "def realpath(path, *, strict=False):\\n        path = normpath(path)\\n        if isinstance(path, bytes):\\n            prefix = b'\\\\\\\\?\\\\'\\n            unc_prefix = b'\\\\\\\\?\\\\UNC\\\\'\\n            new_unc_prefix = b'\\\\\\\\'\\n            cwd = os.getcwdb()\\n            if normcase(path) == normcase(os.fsencode(devnull)):\\n                return b'\\\\\\\\.\\\\NUL'\\n        else:\\n            prefix = '\\\\\\\\?\\\\'\\n            unc_prefix = '\\\\\\\\?\\\\UNC\\\\'\\n            new_unc_prefix = '\\\\\\\\'\\n            cwd = os.getcwd()\\n            if normcase(path) == normcase(devnull):\\n                return '\\\\\\\\.\\\\NUL'\\n        had_prefix = path.startswith(prefix)\\n        if not had_prefix and not isabs(path):\\n            path = join(cwd, path)\\n        try:\\n            path = _getfinalpathname(path)\\n            initial_winerror = 0\\n        except OSError as ex:\\n            if strict:\\n                raise\\n            initial_winerror = ex.winerror\\n            path = _getfinalpathname_nonstrict(path)\\n        if not had_prefix and path.startswith(prefix):\\n            if path.startswith(unc_prefix):\\n                spath = new_unc_prefix + path[len(unc_prefix):]\\n            else:\\n                spath = path[len(prefix):]\\n            try:\\n                if _getfinalpathname(spath) == path:\\n                    path = spath\\n            except OSError as ex:\\n                if ex.winerror == initial_winerror:\\n                    path = spath\\n        return path"
  },
  {
    "code": "def unmap(self, resolve: Union[None, Dict[str, Any], Tuple[Context, \"Session\"]]) -> \"BaseOperator\":\\n        raise NotImplementedError()\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def unmap(self, resolve: Union[None, Dict[str, Any], Tuple[Context, \"Session\"]]) -> \"BaseOperator\":\\n        raise NotImplementedError()\\n    @property"
  },
  {
    "code": "def url2pathname(url):\\n    import string, urllib.parse\\n    url = url.replace(':', '|')\\n    if not '|' in url:\\n        if url[:4] == '////':\\n            url = url[2:]\\n        components = url.split('/')\\n        return urllib.parse.unquote('\\\\'.join(components))\\n    comp = url.split('|')\\n    if len(comp) != 2 or comp[0][-1] not in string.ascii_letters:\\n        error = 'Bad URL: ' + url\\n        raise IOError(error)\\n    drive = comp[0][-1].upper()\\n    components = comp[1].split('/')\\n    path = drive + ':'\\n    for  comp in components:\\n        if comp:\\n            path = path + '\\\\' + urllib.parse.unquote(comp)\\n    return path",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix Issue11474 - fix url2pathname() handling of '/C|/' on Windows",
    "fixed_code": "def url2pathname(url):\\n    import string, urllib.parse\\n    url = url.replace(':', '|')\\n    if not '|' in url:\\n        if url[:4] == '////':\\n            url = url[2:]\\n        components = url.split('/')\\n        return urllib.parse.unquote('\\\\'.join(components))\\n    comp = url.split('|')\\n    if len(comp) != 2 or comp[0][-1] not in string.ascii_letters:\\n        error = 'Bad URL: ' + url\\n        raise IOError(error)\\n    drive = comp[0][-1].upper()\\n    components = comp[1].split('/')\\n    path = drive + ':'\\n    for comp in components:\\n        if comp:\\n            path = path + '\\\\' + urllib.parse.unquote(comp)\\n    if path.endswith(':') and url.endswith('/'):\\n        path += '\\\\'\\n    return path"
  },
  {
    "code": "def _check_proba(self):\\n        if self.loss not in (\"log\", \"modified_huber\"):\\n            raise AttributeError(\\n                \"probability estimates are not available for loss=%r\" % self.loss\\n            )\\n        return True\\n    @available_if(_check_proba)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEP loss \"log\" in favor of \"log loss\" in SGDClassifier (#23046)",
    "fixed_code": "def _check_proba(self):\\n        if self.loss not in (\"log_loss\", \"log\", \"modified_huber\"):\\n            raise AttributeError(\\n                \"probability estimates are not available for loss=%r\" % self.loss\\n            )\\n        return True\\n    @available_if(_check_proba)"
  },
  {
    "code": "def csv_encode(value, encoding='UTF-8'):\\n    if py3compat.PY3 or not isinstance(value, unicode):\\n        return value\\n    return value.encode(encoding, 'replace')\\nclass UTF8Recoder:",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def csv_encode(value, encoding='UTF-8'):\\n    if py3compat.PY3 or not isinstance(value, unicode):\\n        return value\\n    return value.encode(encoding, 'replace')\\nclass UTF8Recoder:"
  },
  {
    "code": "def _get_varlist(self):\\n        if self.format_version == 117:\\n            b = 33\\n        elif self.format_version == 118:\\n            b = 129\\n        return [self._decode(self.path_or_buf.read(b)) for i in range(self.nvar)]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Add dta 119 reading to StataReader (#28542)\\n\\nAdd requirements for reading 119 format files",
    "fixed_code": "def _get_varlist(self):\\n        if self.format_version == 117:\\n            b = 33\\n        elif self.format_version >= 118:\\n            b = 129\\n        return [self._decode(self.path_or_buf.read(b)) for i in range(self.nvar)]"
  },
  {
    "code": "def quantile_transform(X, axis=0, n_quantiles=1000,\\n                       output_distribution='uniform',\\n                       ignore_implicit_zeros=False,\\n                       subsample=int(1e5),\\n                       random_state=None,\\n                       copy=\"warn\"):\\n    if copy == \"warn\":\\n        warnings.warn(\"The default value of `copy` will change from False to \"\\n                      \"True in 0.23 in order to make it more consistent with \"\\n                      \"the default `copy` values of other functions in \"\\n                      \":mod:`sklearn.preprocessing.data` and prevent \"\\n                      \"unexpected side effects by modifying the value of `X` \"\\n                      \"inplace. To avoid inplace modifications of `X`, it is \"\\n                      \"recommended to explicitly set `copy=True`\",\\n                      FutureWarning)\\n        copy = False\\n    n = QuantileTransformer(n_quantiles=n_quantiles,\\n                            output_distribution=output_distribution,\\n                            subsample=subsample,\\n                            ignore_implicit_zeros=ignore_implicit_zeros,\\n                            random_state=random_state,\\n                            copy=copy)\\n    if axis == 0:\\n        return n.fit_transform(X)\\n    elif axis == 1:\\n        return n.fit_transform(X.T).T\\n    else:\\n        raise ValueError(\"axis should be either equal to 0 or 1. Got\"\\n                         \" axis={}\".format(axis))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quantile_transform(X, axis=0, n_quantiles=1000,\\n                       output_distribution='uniform',\\n                       ignore_implicit_zeros=False,\\n                       subsample=int(1e5),\\n                       random_state=None,\\n                       copy=\"warn\"):\\n    if copy == \"warn\":\\n        warnings.warn(\"The default value of `copy` will change from False to \"\\n                      \"True in 0.23 in order to make it more consistent with \"\\n                      \"the default `copy` values of other functions in \"\\n                      \":mod:`sklearn.preprocessing.data` and prevent \"\\n                      \"unexpected side effects by modifying the value of `X` \"\\n                      \"inplace. To avoid inplace modifications of `X`, it is \"\\n                      \"recommended to explicitly set `copy=True`\",\\n                      FutureWarning)\\n        copy = False\\n    n = QuantileTransformer(n_quantiles=n_quantiles,\\n                            output_distribution=output_distribution,\\n                            subsample=subsample,\\n                            ignore_implicit_zeros=ignore_implicit_zeros,\\n                            random_state=random_state,\\n                            copy=copy)\\n    if axis == 0:\\n        return n.fit_transform(X)\\n    elif axis == 1:\\n        return n.fit_transform(X.T).T\\n    else:\\n        raise ValueError(\"axis should be either equal to 0 or 1. Got\"\\n                         \" axis={}\".format(axis))"
  },
  {
    "code": "def _cursor(self):\\n        settings_dict = self.settings_dict\\n        if self.connection is None:\\n            if settings_dict['NAME'] == '':\\n                from django.core.exceptions import ImproperlyConfigured\\n                raise ImproperlyConfigured(\"You need to specify NAME in your Django settings file.\")\\n            conn_params = {\\n                'database': settings_dict['NAME'],\\n            }\\n            conn_params.update(settings_dict['OPTIONS'])\\n            if 'autocommit' in conn_params:\\n                del conn_params['autocommit']\\n            if settings_dict['USER']:\\n                conn_params['user'] = settings_dict['USER']\\n            if settings_dict['PASSWORD']:\\n                conn_params['password'] = settings_dict['PASSWORD']\\n            if settings_dict['HOST']:\\n                conn_params['host'] = settings_dict['HOST']\\n            if settings_dict['PORT']:\\n                conn_params['port'] = settings_dict['PORT']\\n            self.connection = Database.connect(**conn_params)\\n            self.connection.set_client_encoding('UTF8')\\n            tz = 'UTC' if settings.USE_TZ else settings_dict.get('TIME_ZONE')\\n            if tz:\\n                self.connection.set_isolation_level(\\n                        psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\\n                self.connection.cursor().execute(\"SET TIME ZONE %s\", [tz])\\n            self.connection.set_isolation_level(self.isolation_level)\\n            self._get_pg_version()\\n            connection_created.send(sender=self.__class__, connection=self)\\n        cursor = self.connection.cursor()\\n        cursor.tzinfo_factory = utc_tzinfo_factory if settings.USE_TZ else None\\n        return CursorWrapper(cursor)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _cursor(self):\\n        settings_dict = self.settings_dict\\n        if self.connection is None:\\n            if settings_dict['NAME'] == '':\\n                from django.core.exceptions import ImproperlyConfigured\\n                raise ImproperlyConfigured(\"You need to specify NAME in your Django settings file.\")\\n            conn_params = {\\n                'database': settings_dict['NAME'],\\n            }\\n            conn_params.update(settings_dict['OPTIONS'])\\n            if 'autocommit' in conn_params:\\n                del conn_params['autocommit']\\n            if settings_dict['USER']:\\n                conn_params['user'] = settings_dict['USER']\\n            if settings_dict['PASSWORD']:\\n                conn_params['password'] = settings_dict['PASSWORD']\\n            if settings_dict['HOST']:\\n                conn_params['host'] = settings_dict['HOST']\\n            if settings_dict['PORT']:\\n                conn_params['port'] = settings_dict['PORT']\\n            self.connection = Database.connect(**conn_params)\\n            self.connection.set_client_encoding('UTF8')\\n            tz = 'UTC' if settings.USE_TZ else settings_dict.get('TIME_ZONE')\\n            if tz:\\n                self.connection.set_isolation_level(\\n                        psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\\n                self.connection.cursor().execute(\"SET TIME ZONE %s\", [tz])\\n            self.connection.set_isolation_level(self.isolation_level)\\n            self._get_pg_version()\\n            connection_created.send(sender=self.__class__, connection=self)\\n        cursor = self.connection.cursor()\\n        cursor.tzinfo_factory = utc_tzinfo_factory if settings.USE_TZ else None\\n        return CursorWrapper(cursor)"
  },
  {
    "code": "def subclass_exception(name, parents, module, attached_to=None):\\n    class_dict = {'__module__': module}\\n    if attached_to is not None:",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def subclass_exception(name, parents, module, attached_to=None):\\n    class_dict = {'__module__': module}\\n    if attached_to is not None:"
  },
  {
    "code": "def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):\\n\\t\\treturn utils.format_number(value, max_digits, decimal_places)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):\\n\\t\\treturn utils.format_number(value, max_digits, decimal_places)"
  },
  {
    "code": "def value_counts_arraylike(values, dropna: bool):\\n    values = _ensure_arraylike(values)\\n    original = values\\n    values, _ = _ensure_data(values)\\n    ndtype = values.dtype.name\\n    if needs_i8_conversion(original.dtype):\\n        keys, counts = htable.value_count_int64(values, dropna)\\n        if dropna:\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n    else:\\n        f = getattr(htable, f\"value_count_{ndtype}\")\\n        keys, counts = f(values, dropna)\\n        mask = isna(values)\\n        if not dropna and mask.any() and not isna(keys).any():\\n            keys = np.insert(keys, 0, np.NaN)\\n            counts = np.insert(counts, 0, mask.sum())\\n    keys = _reconstruct_data(keys, original.dtype, original)\\n    return keys, counts",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: making value_counts stable/keeping original ordering (#39009)",
    "fixed_code": "def value_counts_arraylike(values, dropna: bool):\\n    values = _ensure_arraylike(values)\\n    original = values\\n    values, _ = _ensure_data(values)\\n    ndtype = values.dtype.name\\n    if needs_i8_conversion(original.dtype):\\n        keys, counts = htable.value_count_int64(values, dropna)\\n        if dropna:\\n            msk = keys != iNaT\\n            keys, counts = keys[msk], counts[msk]\\n    else:\\n        f = getattr(htable, f\"value_count_{ndtype}\")\\n        keys, counts = f(values, dropna)\\n    keys = _reconstruct_data(keys, original.dtype, original)\\n    return keys, counts"
  },
  {
    "code": "def _build_kwargs(self):\\n        self._become_method = self._play_context.become_method\\n        self._become_user = self._play_context.become_user\\n        self._become_pass = self._play_context.become_pass\\n        self._psrp_host = self.get_option('remote_addr')\\n        self._psrp_user = self.get_option('remote_user')\\n        self._psrp_pass = self._play_context.password\\n        protocol = self.get_option('protocol')\\n        port = self.get_option('port')\\n        if protocol is None and port is None:\\n            protocol = 'https'\\n            port = 5986\\n        elif protocol is None:\\n            protocol = 'https' if int(port) != 5985 else 'http'\\n        elif port is None:\\n            port = 5986 if protocol == 'https' else 5985\\n        self._psrp_protocol = protocol\\n        self._psrp_port = int(port)\\n        self._psrp_path = self.get_option('path')\\n        self._psrp_auth = self.get_option('auth')\\n        cert_validation = self.get_option('cert_validation')\\n        cert_trust_path = self.get_option('ca_cert')\\n        if cert_validation == 'ignore':\\n            self._psrp_cert_validation = False\\n        elif cert_trust_path is not None:\\n            self._psrp_cert_validation = cert_trust_path\\n        else:\\n            self._psrp_cert_validation = True\\n        self._psrp_connection_timeout = self.get_option('connection_timeout')  \\n        self._psrp_read_timeout = self.get_option('read_timeout')  \\n        self._psrp_message_encryption = self.get_option('message_encryption')\\n        self._psrp_proxy = self.get_option('proxy')\\n        self._psrp_ignore_proxy = boolean(self.get_option('ignore_proxy'))\\n        self._psrp_operation_timeout = int(self.get_option('operation_timeout'))\\n        self._psrp_max_envelope_size = int(self.get_option('max_envelope_size'))\\n        self._psrp_configuration_name = self.get_option('configuration_name')\\n        self._psrp_reconnection_retries = int(self.get_option('reconnection_retries'))\\n        self._psrp_reconnection_backoff = float(self.get_option('reconnection_backoff'))\\n        self._psrp_certificate_key_pem = self.get_option('certificate_key_pem')\\n        self._psrp_certificate_pem = self.get_option('certificate_pem')\\n        self._psrp_credssp_auth_mechanism = self.get_option('credssp_auth_mechanism')\\n        self._psrp_credssp_disable_tlsv1_2 = self.get_option('credssp_disable_tlsv1_2')\\n        self._psrp_credssp_minimum_version = self.get_option('credssp_minimum_version')\\n        self._psrp_negotiate_send_cbt = self.get_option('negotiate_send_cbt')\\n        self._psrp_negotiate_delegate = self.get_option('negotiate_delegate')\\n        self._psrp_negotiate_hostname_override = self.get_option('negotiate_hostname_override')\\n        self._psrp_negotiate_service = self.get_option('negotiate_service')\\n        supported_args = []\\n        for auth_kwarg in AUTH_KWARGS.values():\\n            supported_args.extend(auth_kwarg)\\n        extra_args = set([v.replace('ansible_psrp_', '') for v in\\n                          self.get_option('_extras')])\\n        unsupported_args = extra_args.difference(supported_args)\\n        for arg in unsupported_args:\\n            display.warning(\"ansible_psrp_%s is unsupported by the current \"\\n                            \"psrp version installed\" % arg)\\n        self._psrp_conn_kwargs = dict(\\n            server=self._psrp_host, port=self._psrp_port,\\n            username=self._psrp_user, password=self._psrp_pass,\\n            ssl=self._psrp_protocol == 'https', path=self._psrp_path,\\n            auth=self._psrp_auth, cert_validation=self._psrp_cert_validation,\\n            connection_timeout=self._psrp_connection_timeout,\\n            encryption=self._psrp_message_encryption, proxy=self._psrp_proxy,\\n            no_proxy=self._psrp_ignore_proxy,\\n            max_envelope_size=self._psrp_max_envelope_size,\\n            operation_timeout=self._psrp_operation_timeout,\\n            certificate_key_pem=self._psrp_certificate_key_pem,\\n            certificate_pem=self._psrp_certificate_pem,\\n            credssp_auth_mechanism=self._psrp_credssp_auth_mechanism,\\n            credssp_disable_tlsv1_2=self._psrp_credssp_disable_tlsv1_2,\\n            credssp_minimum_version=self._psrp_credssp_minimum_version,\\n            negotiate_send_cbt=self._psrp_negotiate_send_cbt,\\n            negotiate_delegate=self._psrp_negotiate_delegate,\\n            negotiate_hostname_override=self._psrp_negotiate_hostname_override,\\n            negotiate_service=self._psrp_negotiate_service,\\n        )\\n        if hasattr(pypsrp, 'FEATURES') and 'wsman_read_timeout' in pypsrp.FEATURES:\\n            self._psrp_conn_kwargs['read_timeout'] = self._psrp_read_timeout\\n        elif self._psrp_read_timeout is not None:\\n            display.warning(\"ansible_psrp_read_timeout is unsupported by the current psrp version installed, \"\\n                            \"using ansible_psrp_connection_timeout value for read_timeout instead.\")\\n        if hasattr(pypsrp, 'FEATURES') and 'wsman_reconnections' in pypsrp.FEATURES:\\n            self._psrp_conn_kwargs['reconnection_retries'] = self._psrp_reconnection_retries\\n            self._psrp_conn_kwargs['reconnection_backoff'] = self._psrp_reconnection_backoff\\n        else:\\n            if self._psrp_reconnection_retries is not None:\\n                display.warning(\"ansible_psrp_reconnection_retries is unsupported by the current psrp version installed.\")\\n            if self._psrp_reconnection_backoff is not None:\\n                display.warning(\"ansible_psrp_reconnection_backoff is unsupported by the current psrp version installed.\")\\n        for arg in extra_args.intersection(supported_args):\\n            option = self.get_option('_extras')['ansible_psrp_%s' % arg]\\n            self._psrp_conn_kwargs[arg] = option",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "become - stop using play context in more places (#62373)",
    "fixed_code": "def _build_kwargs(self):\\n        self._psrp_host = self.get_option('remote_addr')\\n        self._psrp_user = self.get_option('remote_user')\\n        self._psrp_pass = self._play_context.password\\n        protocol = self.get_option('protocol')\\n        port = self.get_option('port')\\n        if protocol is None and port is None:\\n            protocol = 'https'\\n            port = 5986\\n        elif protocol is None:\\n            protocol = 'https' if int(port) != 5985 else 'http'\\n        elif port is None:\\n            port = 5986 if protocol == 'https' else 5985\\n        self._psrp_protocol = protocol\\n        self._psrp_port = int(port)\\n        self._psrp_path = self.get_option('path')\\n        self._psrp_auth = self.get_option('auth')\\n        cert_validation = self.get_option('cert_validation')\\n        cert_trust_path = self.get_option('ca_cert')\\n        if cert_validation == 'ignore':\\n            self._psrp_cert_validation = False\\n        elif cert_trust_path is not None:\\n            self._psrp_cert_validation = cert_trust_path\\n        else:\\n            self._psrp_cert_validation = True\\n        self._psrp_connection_timeout = self.get_option('connection_timeout')  \\n        self._psrp_read_timeout = self.get_option('read_timeout')  \\n        self._psrp_message_encryption = self.get_option('message_encryption')\\n        self._psrp_proxy = self.get_option('proxy')\\n        self._psrp_ignore_proxy = boolean(self.get_option('ignore_proxy'))\\n        self._psrp_operation_timeout = int(self.get_option('operation_timeout'))\\n        self._psrp_max_envelope_size = int(self.get_option('max_envelope_size'))\\n        self._psrp_configuration_name = self.get_option('configuration_name')\\n        self._psrp_reconnection_retries = int(self.get_option('reconnection_retries'))\\n        self._psrp_reconnection_backoff = float(self.get_option('reconnection_backoff'))\\n        self._psrp_certificate_key_pem = self.get_option('certificate_key_pem')\\n        self._psrp_certificate_pem = self.get_option('certificate_pem')\\n        self._psrp_credssp_auth_mechanism = self.get_option('credssp_auth_mechanism')\\n        self._psrp_credssp_disable_tlsv1_2 = self.get_option('credssp_disable_tlsv1_2')\\n        self._psrp_credssp_minimum_version = self.get_option('credssp_minimum_version')\\n        self._psrp_negotiate_send_cbt = self.get_option('negotiate_send_cbt')\\n        self._psrp_negotiate_delegate = self.get_option('negotiate_delegate')\\n        self._psrp_negotiate_hostname_override = self.get_option('negotiate_hostname_override')\\n        self._psrp_negotiate_service = self.get_option('negotiate_service')\\n        supported_args = []\\n        for auth_kwarg in AUTH_KWARGS.values():\\n            supported_args.extend(auth_kwarg)\\n        extra_args = set([v.replace('ansible_psrp_', '') for v in\\n                          self.get_option('_extras')])\\n        unsupported_args = extra_args.difference(supported_args)\\n        for arg in unsupported_args:\\n            display.warning(\"ansible_psrp_%s is unsupported by the current \"\\n                            \"psrp version installed\" % arg)\\n        self._psrp_conn_kwargs = dict(\\n            server=self._psrp_host, port=self._psrp_port,\\n            username=self._psrp_user, password=self._psrp_pass,\\n            ssl=self._psrp_protocol == 'https', path=self._psrp_path,\\n            auth=self._psrp_auth, cert_validation=self._psrp_cert_validation,\\n            connection_timeout=self._psrp_connection_timeout,\\n            encryption=self._psrp_message_encryption, proxy=self._psrp_proxy,\\n            no_proxy=self._psrp_ignore_proxy,\\n            max_envelope_size=self._psrp_max_envelope_size,\\n            operation_timeout=self._psrp_operation_timeout,\\n            certificate_key_pem=self._psrp_certificate_key_pem,\\n            certificate_pem=self._psrp_certificate_pem,\\n            credssp_auth_mechanism=self._psrp_credssp_auth_mechanism,\\n            credssp_disable_tlsv1_2=self._psrp_credssp_disable_tlsv1_2,\\n            credssp_minimum_version=self._psrp_credssp_minimum_version,\\n            negotiate_send_cbt=self._psrp_negotiate_send_cbt,\\n            negotiate_delegate=self._psrp_negotiate_delegate,\\n            negotiate_hostname_override=self._psrp_negotiate_hostname_override,\\n            negotiate_service=self._psrp_negotiate_service,\\n        )\\n        if hasattr(pypsrp, 'FEATURES') and 'wsman_read_timeout' in pypsrp.FEATURES:\\n            self._psrp_conn_kwargs['read_timeout'] = self._psrp_read_timeout\\n        elif self._psrp_read_timeout is not None:\\n            display.warning(\"ansible_psrp_read_timeout is unsupported by the current psrp version installed, \"\\n                            \"using ansible_psrp_connection_timeout value for read_timeout instead.\")\\n        if hasattr(pypsrp, 'FEATURES') and 'wsman_reconnections' in pypsrp.FEATURES:\\n            self._psrp_conn_kwargs['reconnection_retries'] = self._psrp_reconnection_retries\\n            self._psrp_conn_kwargs['reconnection_backoff'] = self._psrp_reconnection_backoff\\n        else:\\n            if self._psrp_reconnection_retries is not None:\\n                display.warning(\"ansible_psrp_reconnection_retries is unsupported by the current psrp version installed.\")\\n            if self._psrp_reconnection_backoff is not None:\\n                display.warning(\"ansible_psrp_reconnection_backoff is unsupported by the current psrp version installed.\")\\n        for arg in extra_args.intersection(supported_args):\\n            option = self.get_option('_extras')['ansible_psrp_%s' % arg]\\n            self._psrp_conn_kwargs[arg] = option"
  },
  {
    "code": "def _getitem_lowerdim(self, tup):\\n\\t\\tfrom pandas.core.frame import DataFrame\\n\\t\\tax0 = self.obj._get_axis(0)\\n\\t\\tif isinstance(ax0, MultiIndex):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\treturn self._get_label(tup, axis=0)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tpass\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\tif isinstance(tup[0], slice):\\n\\t\\t\\t\\t\\traise IndexingError\\n\\t\\t\\t\\tif tup[0] not in ax0:\\n\\t\\t\\t\\t\\traise\\n\\t\\tfor i, key in enumerate(tup):\\n\\t\\t\\tif _is_label_like(key):\\n\\t\\t\\t\\tsection = self._getitem_axis(key, axis=i)\\n\\t\\t\\t\\tif section.ndim == self.ndim:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + (_NS,) + tup[i+1:]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + tup[i+1:]\\n\\t\\t\\t\\t\\tif (isinstance(section, DataFrame) and i > 0\\n\\t\\t\\t\\t\\t\\tand len(new_key) == 2):\\n\\t\\t\\t\\t\\t\\ta, b = new_key\\n\\t\\t\\t\\t\\t\\tnew_key = b, a\\n\\t\\t\\t\\t\\tif len(new_key) == 1:\\n\\t\\t\\t\\t\\t\\tnew_key, = new_key\\n\\t\\t\\t\\treturn section.ix[new_key]\\n\\t\\traise IndexingError('not applicable')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _getitem_lowerdim(self, tup):\\n\\t\\tfrom pandas.core.frame import DataFrame\\n\\t\\tax0 = self.obj._get_axis(0)\\n\\t\\tif isinstance(ax0, MultiIndex):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\treturn self._get_label(tup, axis=0)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\tpass\\n\\t\\t\\texcept Exception:\\n\\t\\t\\t\\tif isinstance(tup[0], slice):\\n\\t\\t\\t\\t\\traise IndexingError\\n\\t\\t\\t\\tif tup[0] not in ax0:\\n\\t\\t\\t\\t\\traise\\n\\t\\tfor i, key in enumerate(tup):\\n\\t\\t\\tif _is_label_like(key):\\n\\t\\t\\t\\tsection = self._getitem_axis(key, axis=i)\\n\\t\\t\\t\\tif section.ndim == self.ndim:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + (_NS,) + tup[i+1:]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tnew_key = tup[:i] + tup[i+1:]\\n\\t\\t\\t\\t\\tif (isinstance(section, DataFrame) and i > 0\\n\\t\\t\\t\\t\\t\\tand len(new_key) == 2):\\n\\t\\t\\t\\t\\t\\ta, b = new_key\\n\\t\\t\\t\\t\\t\\tnew_key = b, a\\n\\t\\t\\t\\t\\tif len(new_key) == 1:\\n\\t\\t\\t\\t\\t\\tnew_key, = new_key\\n\\t\\t\\t\\treturn section.ix[new_key]\\n\\t\\traise IndexingError('not applicable')"
  },
  {
    "code": "def get_cache(backend, **kwargs):\\n    try:\\n        if '://' in backend:\\n            backend, location, params = parse_backend_uri(backend)\\n            if backend in BACKENDS:\\n                backend = 'django.core.cache.backends.%s' % BACKENDS[backend]\\n            params.update(kwargs)\\n            mod = importlib.import_module(backend)\\n            backend_cls = mod.CacheClass\\n        else:\\n            backend, location, params = parse_backend_conf(backend, **kwargs)\\n            mod_path, cls_name = backend.rsplit('.', 1)\\n            mod = importlib.import_module(mod_path)\\n            backend_cls = getattr(mod, cls_name)\\n    except (AttributeError, ImportError), e:\\n        raise InvalidCacheBackendError(\\n            \"Could not find backend '%s': %s\" % (backend, e))\\n    cache = backend_cls(location, params)\\n    if hasattr(cache, 'close'):\\n        signals.request_finished.connect(cache.close)\\n    return cache\\ncache = get_cache(DEFAULT_CACHE_ALIAS)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_cache(backend, **kwargs):\\n    try:\\n        if '://' in backend:\\n            backend, location, params = parse_backend_uri(backend)\\n            if backend in BACKENDS:\\n                backend = 'django.core.cache.backends.%s' % BACKENDS[backend]\\n            params.update(kwargs)\\n            mod = importlib.import_module(backend)\\n            backend_cls = mod.CacheClass\\n        else:\\n            backend, location, params = parse_backend_conf(backend, **kwargs)\\n            mod_path, cls_name = backend.rsplit('.', 1)\\n            mod = importlib.import_module(mod_path)\\n            backend_cls = getattr(mod, cls_name)\\n    except (AttributeError, ImportError), e:\\n        raise InvalidCacheBackendError(\\n            \"Could not find backend '%s': %s\" % (backend, e))\\n    cache = backend_cls(location, params)\\n    if hasattr(cache, 'close'):\\n        signals.request_finished.connect(cache.close)\\n    return cache\\ncache = get_cache(DEFAULT_CACHE_ALIAS)"
  },
  {
    "code": "def get_features_used(  \\n    node: Node, *, future_imports: Optional[Set[str]] = None\\n) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    if future_imports:\\n        features |= {\\n            FUTURE_FLAG_TO_FEATURE[future_import]\\n            for future_import in future_imports\\n            if future_import in FUTURE_FLAG_TO_FEATURE\\n        }\\n    for n in node.pre_order():\\n        if is_string_token(n):\\n            value_head = n.value[:2]\\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n        elif is_number_token(n):\\n            if \"_\" in n.value:\\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n        elif (\\n            n.type in {syms.return_stmt, syms.yield_expr}\\n            and len(n.children) >= 2\\n            and n.children[1].type == syms.testlist_star_expr\\n            and any(child.type == syms.star_expr for child in n.children[1].children)\\n        ):\\n            features.add(Feature.UNPACKING_ON_FLOW)\\n        elif (\\n            n.type == syms.annassign\\n            and len(n.children) >= 4\\n            and n.children[3].type == syms.testlist_star_expr\\n        ):\\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\\n        elif (\\n            n.type == syms.except_clause\\n            and len(n.children) >= 2\\n            and n.children[1].type == token.STAR\\n        ):\\n            features.add(Feature.EXCEPT_STAR)\\n        elif n.type in {syms.subscriptlist, syms.trailer} and any(\\n            child.type == syms.star_expr for child in n.children\\n        ):\\n            features.add(Feature.VARIADIC_GENERICS)\\n        elif (\\n            n.type == syms.tname_star\\n            and len(n.children) == 3\\n            and n.children[2].type == syms.star_expr\\n        ):\\n            features.add(Feature.VARIADIC_GENERICS)\\n    return features",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use debug f-strings for feature detection (#3215)\\n\\nFixes GH-2907.",
    "fixed_code": "def get_features_used(  \\n    node: Node, *, future_imports: Optional[Set[str]] = None\\n) -> Set[Feature]:\\n    features: Set[Feature] = set()\\n    if future_imports:\\n        features |= {\\n            FUTURE_FLAG_TO_FEATURE[future_import]\\n            for future_import in future_imports\\n            if future_import in FUTURE_FLAG_TO_FEATURE\\n        }\\n    for n in node.pre_order():\\n        if is_string_token(n):\\n            value_head = n.value[:2]\\n            if value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\\n                features.add(Feature.F_STRINGS)\\n                if Feature.DEBUG_F_STRINGS not in features:\\n                    for span_beg, span_end in iter_fexpr_spans(n.value):\\n                        if n.value[span_beg : span_end - 1].rstrip().endswith(\"=\"):\\n                            features.add(Feature.DEBUG_F_STRINGS)\\n                            break\\n        elif is_number_token(n):\\n            if \"_\" in n.value:\\n                features.add(Feature.NUMERIC_UNDERSCORES)\\n        elif n.type == token.SLASH:\\n            if n.parent and n.parent.type in {\\n                syms.typedargslist,\\n                syms.arglist,\\n                syms.varargslist,\\n            }:\\n                features.add(Feature.POS_ONLY_ARGUMENTS)\\n        elif n.type == token.COLONEQUAL:\\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\\n        elif n.type == syms.decorator:\\n            if len(n.children) > 1 and not is_simple_decorator_expression(\\n                n.children[1]\\n            ):\\n                features.add(Feature.RELAXED_DECORATORS)\\n        elif (\\n            n.type in {syms.typedargslist, syms.arglist}\\n            and n.children\\n            and n.children[-1].type == token.COMMA\\n        ):\\n            if n.type == syms.typedargslist:\\n                feature = Feature.TRAILING_COMMA_IN_DEF\\n            else:\\n                feature = Feature.TRAILING_COMMA_IN_CALL\\n            for ch in n.children:\\n                if ch.type in STARS:\\n                    features.add(feature)\\n                if ch.type == syms.argument:\\n                    for argch in ch.children:\\n                        if argch.type in STARS:\\n                            features.add(feature)\\n        elif (\\n            n.type in {syms.return_stmt, syms.yield_expr}\\n            and len(n.children) >= 2\\n            and n.children[1].type == syms.testlist_star_expr\\n            and any(child.type == syms.star_expr for child in n.children[1].children)\\n        ):\\n            features.add(Feature.UNPACKING_ON_FLOW)\\n        elif (\\n            n.type == syms.annassign\\n            and len(n.children) >= 4\\n            and n.children[3].type == syms.testlist_star_expr\\n        ):\\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\\n        elif (\\n            n.type == syms.except_clause\\n            and len(n.children) >= 2\\n            and n.children[1].type == token.STAR\\n        ):\\n            features.add(Feature.EXCEPT_STAR)\\n        elif n.type in {syms.subscriptlist, syms.trailer} and any(\\n            child.type == syms.star_expr for child in n.children\\n        ):\\n            features.add(Feature.VARIADIC_GENERICS)\\n        elif (\\n            n.type == syms.tname_star\\n            and len(n.children) == 3\\n            and n.children[2].type == syms.star_expr\\n        ):\\n            features.add(Feature.VARIADIC_GENERICS)\\n    return features"
  },
  {
    "code": "def get_dataset_tables(self, dataset_id: str, project_id: Optional[str] = None,\\n                           max_results: Optional[int] = None,\\n                           page_token: Optional[str] = None) -> Dict[str, Union[str, int, List]]:\\n        optional_params = {}  \\n        if max_results:\\n            optional_params['maxResults'] = max_results\\n        if page_token:\\n            optional_params['pageToken'] = page_token\\n        dataset_project_id = project_id or self.project_id\\n        return (self.service.tables().list(\\n            projectId=dataset_project_id,\\n            datasetId=dataset_id,\\n            **optional_params).execute(num_retries=self.num_retries))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-6442] BigQuery hook - standardize handling http exceptions (#7028)",
    "fixed_code": "def get_dataset_tables(self, dataset_id: str, project_id: Optional[str] = None,\\n                           max_results: Optional[int] = None,\\n                           page_token: Optional[str] = None) -> Dict[str, Union[str, int, List]]:\\n        optional_params = {}  \\n        if max_results:\\n            optional_params['maxResults'] = max_results\\n        if page_token:\\n            optional_params['pageToken'] = page_token\\n        dataset_project_id = project_id or self.project_id\\n        return (self.service.tables().list(\\n            projectId=dataset_project_id,\\n            datasetId=dataset_id,\\n            **optional_params).execute(num_retries=self.num_retries))\\n    @CloudBaseHook.catch_http_exception"
  },
  {
    "code": "def cv_estimate(n_folds=3):\\n    cv = KFold(n=X_train.shape[0], n_folds=n_folds)\\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\\n    for train, test in cv:\\n        cv_clf.fit(X_train[train], y_train[train])\\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\\n    val_scores /= n_folds\\n    return val_scores",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC Modify documentation/examples for the new model_selection module",
    "fixed_code": "def cv_estimate(n_folds=3):\\n    cv = KFold(n_folds=n_folds)\\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\\n    for train, test in cv.split(X_train, y_train):\\n        cv_clf.fit(X_train[train], y_train[train])\\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\\n    val_scores /= n_folds\\n    return val_scores"
  },
  {
    "code": "def readable(self):\\n        return self._reading and not self.closed",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #15842: the SocketIO.{readable,writable,seekable} methods now raise ValueError when the file-like object is closed. Patch by Alessandro Moura.",
    "fixed_code": "def readable(self):\\n        if self.closed:\\n            raise ValueError(\"I/O operation on closed socket.\")\\n        return self._reading"
  },
  {
    "code": "def _run_command(self, command, data=None):\\n        return self._module.run_command(command, data=data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _run_command(self, command, data=None):\\n        return self._module.run_command(command, data=data)"
  },
  {
    "code": "def nanstd(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\\n    if not (type(a) is np.ndarray):\\n        try:\\n            nanstd = a.nanstd\\n            return nanstd(axis=axis, dtype=dtype, out=out, ddof=ddof)\\n        except AttributeError:\\n            pass\\n    return _nanstd(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\\n                   keepdims=keepdims)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT: Refactor nanfunctions.\\n\\nnanmax, nanmin\\n--------------\\nAdd out and keepdims keywords.\\n\\nnanargmin, nanargmax\\n--------------------\\nA NanWarning is raised if an all NaN slice detected. For all such\\nslices np.iingo(np.intp).min is returned as the index value.\\n\\nnansum\\n------\\nThe keywords dtype, out, and keepdims are added.\\n\\nA FutureWarning is raised, as in the future the mean of an empty\\nslice after NaN replacement will be 0 instead of the current NaN.\\n\\nnanmean, nanvar, nanstd\\n-----------------------\\nFor all, if the input array is of inexact type then the dtype and out\\nparameters must be of inexact type if specified.  That insures that NaNs\\ncan be returned when appropriate.\\n\\nThe nanmean function detects empty slices after NaN replacement and\\nraises a NanWarning. NaN is returned as the value for all such slices.\\n\\nThe nanmean and nanstd functions detect degrees of freedom <= 0 after\\nNaN replacement and raise a NanWarning. NaN is returned as the value for\\nall such slices.",
    "fixed_code": "def nanstd(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\\n    var = nanvar(a, axis, dtype, out, ddof, keepdims)\\n    if isinstance(var, np.ndarray):\\n        std = np.sqrt(var, out=var)\\n    else:\\n        std = var.dtype.type(np.sqrt(var))\\n    return std"
  },
  {
    "code": "def __call__(self,\\n\\t\\t\\t   y_true,\\n\\t\\t\\t   y_pred,\\n\\t\\t\\t   sample_weight=None,\\n\\t\\t\\t   regularization_losses=None):\\n\\ty_true = self._conform_to_outputs(y_pred, y_true)\\n\\tsample_weight = self._conform_to_outputs(y_pred, sample_weight)\\n\\tif not self._built:\\n\\t  self._build(y_pred)\\n\\ty_pred = nest.flatten(y_pred)\\n\\ty_true = nest.flatten(y_true)\\n\\tsample_weight = nest.flatten(sample_weight)\\n\\tloss_values = []  \\n\\tloss_metric_values = []  \\n\\tzip_args = (y_true, y_pred, sample_weight, self._losses, self._loss_weights,\\n\\t\\t\\t\\tself._per_output_metrics)\\n\\tfor y_t, y_p, sw, loss_obj, loss_weight, metric_obj in zip(*zip_args):\\n\\t  if y_t is None or loss_obj is None:  \\n\\t\\tcontinue\\n\\t  y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\\n\\t  sw = apply_mask(y_p, sw)\\n\\t  loss_value = loss_obj(y_t, y_p, sample_weight=sw)\\n\\t  loss_metric_value = loss_value\\n\\t  if loss_obj.reduction == losses_utils.ReductionV2.SUM:\\n\\t\\tloss_metric_value *= ds_context.get_strategy().num_replicas_in_sync\\n\\t  if metric_obj is not None:\\n\\t\\tmetric_obj.update_state(loss_metric_value)\\n\\t  if loss_weight is not None:\\n\\t\\tloss_value *= loss_weight\\n\\t\\tloss_metric_value *= loss_weight\\n\\t  if (loss_obj.reduction == losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE or\\n\\t\\t  loss_obj.reduction == losses_utils.ReductionV2.AUTO):\\n\\t\\tloss_value = losses_utils.scale_loss_for_distribution(loss_value)\\n\\t  loss_values.append(loss_value)\\n\\t  loss_metric_values.append(loss_metric_value)\\n\\tif regularization_losses:\\n\\t  regularization_losses = losses_utils.cast_losses_to_common_dtype(\\n\\t\\t  regularization_losses)\\n\\t  reg_loss = math_ops.add_n(regularization_losses)\\n\\t  loss_metric_values.append(reg_loss)\\n\\t  loss_values.append(losses_utils.scale_loss_for_distribution(reg_loss))\\n\\tif loss_values:\\n\\t  loss_metric_values = losses_utils.cast_losses_to_common_dtype(\\n\\t\\t  loss_metric_values)\\n\\t  total_loss_metric_value = math_ops.add_n(loss_metric_values)\\n\\t  self._loss_metric.update_state(total_loss_metric_value)\\n\\t  loss_values = losses_utils.cast_losses_to_common_dtype(loss_values)\\n\\t  total_loss = math_ops.add_n(loss_values)\\n\\t  return total_loss\\n\\telse:\\n\\t  return array_ops.zeros(shape=())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix last partial batch loss regression in 2.2",
    "fixed_code": "def __call__(self,\\n\\t\\t\\t   y_true,\\n\\t\\t\\t   y_pred,\\n\\t\\t\\t   sample_weight=None,\\n\\t\\t\\t   regularization_losses=None):\\n\\ty_true = self._conform_to_outputs(y_pred, y_true)\\n\\tsample_weight = self._conform_to_outputs(y_pred, sample_weight)\\n\\tif not self._built:\\n\\t  self._build(y_pred)\\n\\ty_pred = nest.flatten(y_pred)\\n\\ty_true = nest.flatten(y_true)\\n\\tsample_weight = nest.flatten(sample_weight)\\n\\tloss_values = []  \\n\\tloss_metric_values = []  \\n\\tbatch_dim = None\\n\\tzip_args = (y_true, y_pred, sample_weight, self._losses, self._loss_weights,\\n\\t\\t\\t\\tself._per_output_metrics)\\n\\tfor y_t, y_p, sw, loss_obj, loss_weight, metric_obj in zip(*zip_args):\\n\\t  if y_t is None or loss_obj is None:  \\n\\t\\tcontinue\\n\\t  y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\\n\\t  sw = apply_mask(y_p, sw)\\n\\t  loss_value = loss_obj(y_t, y_p, sample_weight=sw)\\n\\t  loss_metric_value = loss_value\\n\\t  if loss_obj.reduction == losses_utils.ReductionV2.SUM:\\n\\t\\tloss_metric_value *= ds_context.get_strategy().num_replicas_in_sync\\n\\t  if batch_dim is None:\\n\\t\\tbatch_dim = array_ops.shape(y_t)[0]\\n\\t  if metric_obj is not None:\\n\\t\\tmetric_obj.update_state(loss_metric_value, sample_weight=batch_dim)\\n\\t  if loss_weight is not None:\\n\\t\\tloss_value *= loss_weight\\n\\t\\tloss_metric_value *= loss_weight\\n\\t  if (loss_obj.reduction == losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE or\\n\\t\\t  loss_obj.reduction == losses_utils.ReductionV2.AUTO):\\n\\t\\tloss_value = losses_utils.scale_loss_for_distribution(loss_value)\\n\\t  loss_values.append(loss_value)\\n\\t  loss_metric_values.append(loss_metric_value)\\n\\tif regularization_losses:\\n\\t  regularization_losses = losses_utils.cast_losses_to_common_dtype(\\n\\t\\t  regularization_losses)\\n\\t  reg_loss = math_ops.add_n(regularization_losses)\\n\\t  loss_metric_values.append(reg_loss)\\n\\t  loss_values.append(losses_utils.scale_loss_for_distribution(reg_loss))\\n\\tif loss_values:\\n\\t  loss_metric_values = losses_utils.cast_losses_to_common_dtype(\\n\\t\\t  loss_metric_values)\\n\\t  total_loss_metric_value = math_ops.add_n(loss_metric_values)\\n\\t  self._loss_metric.update_state(\\n\\t\\t  total_loss_metric_value, sample_weight=batch_dim)\\n\\t  loss_values = losses_utils.cast_losses_to_common_dtype(loss_values)\\n\\t  total_loss = math_ops.add_n(loss_values)\\n\\t  return total_loss\\n\\telse:\\n\\t  return array_ops.zeros(shape=())"
  },
  {
    "code": "def __init__(\\n        self, *, collection: str, query: dict, mongo_conn_id: str = \"mongo_default\", **kwargs\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.mongo_conn_id = mongo_conn_id\\n        self.collection = collection\\n        self.query = query",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "#19223 add mongo_db param to MongoSensor (#19276)",
    "fixed_code": "def __init__(\\n        self, *, collection: str, query: dict, mongo_conn_id: str = \"mongo_default\", mongo_db=None, **kwargs\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.mongo_conn_id = mongo_conn_id\\n        self.collection = collection\\n        self.query = query\\n        self.mongo_db = mongo_db"
  },
  {
    "code": "def __getitem__(self, key):\\n        try:\\n            value = self._data[self.encodekey(key)]\\n        except KeyError:\\n            raise KeyError(key)\\n        return self.decodevalue(value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, key):\\n        try:\\n            value = self._data[self.encodekey(key)]\\n        except KeyError:\\n            raise KeyError(key)\\n        return self.decodevalue(value)"
  },
  {
    "code": "def backfill_1d(values, limit=None, mask=None):\\n    if is_float_dtype(values):\\n        _method = _algos.backfill_inplace_float64\\n    elif is_datetime64_dtype(values):\\n        _method = _backfill_1d_datetime\\n    elif values.dtype == np.object_:\\n        _method = _algos.backfill_inplace_object\\n    else: \\n        raise ValueError('Invalid dtype for padding')\\n    if mask is None:\\n        mask = isnull(values)\\n    mask = mask.view(np.uint8)\\n    _method(values, mask, limit=limit)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def backfill_1d(values, limit=None, mask=None):\\n    if is_float_dtype(values):\\n        _method = _algos.backfill_inplace_float64\\n    elif is_datetime64_dtype(values):\\n        _method = _backfill_1d_datetime\\n    elif values.dtype == np.object_:\\n        _method = _algos.backfill_inplace_object\\n    else: \\n        raise ValueError('Invalid dtype for padding')\\n    if mask is None:\\n        mask = isnull(values)\\n    mask = mask.view(np.uint8)\\n    _method(values, mask, limit=limit)"
  },
  {
    "code": "def split_host_pattern(pattern):\\n    if isinstance(pattern, list):\\n        return list(itertools.chain(*map(split_host_pattern, pattern)))\\n    elif not isinstance(pattern, string_types):\\n        pattern = to_text(pattern, errors='surrogate_or_strict')\\n    if u',' in pattern:\\n        patterns = pattern.split(u',')\\n    else:\\n        try:\\n            (base, port) = parse_address(pattern, allow_ranges=True)\\n            patterns = [pattern]\\n        except Exception:\\n            patterns = re.findall(\\n                to_text(r''), pattern, re.X\\n            )\\n    return [p.strip() for p in patterns if p.strip()]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def split_host_pattern(pattern):\\n    if isinstance(pattern, list):\\n        return list(itertools.chain(*map(split_host_pattern, pattern)))\\n    elif not isinstance(pattern, string_types):\\n        pattern = to_text(pattern, errors='surrogate_or_strict')\\n    if u',' in pattern:\\n        patterns = pattern.split(u',')\\n    else:\\n        try:\\n            (base, port) = parse_address(pattern, allow_ranges=True)\\n            patterns = [pattern]\\n        except Exception:\\n            patterns = re.findall(\\n                to_text(r''), pattern, re.X\\n            )\\n    return [p.strip() for p in patterns if p.strip()]"
  },
  {
    "code": "def teardown_module(module):\\n    datasets.mldata.urllib2 = _urllib2_ref\\n    shutil.rmtree(cachedir)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: avoid double HDD copy of mocked datasets + style",
    "fixed_code": "def teardown_module(module):\\n    datasets.mldata.urllib2 = _urllib2_ref\\n    shutil.rmtree(custom_data_home)"
  },
  {
    "code": "def _post_plot_logic_common(self, ax, data):\\n        from matplotlib.ticker import FixedLocator, FixedFormatter",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _post_plot_logic_common(self, ax, data):\\n        from matplotlib.ticker import FixedLocator, FixedFormatter"
  },
  {
    "code": "def get_name(self):\\n        ''\\n        return self.name\\n    @staticmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "play - validate hosts entries (#74147)\\n\\n  - check for empty values\\n  - check for None\\n  - check for values that are not a sequence and are not strings\\n  - add unit tests\\n\\n  By defining this method, it gets called automatically by FieldAttributeBase.validate().",
    "fixed_code": "def get_name(self):\\n        ''\\n        if self.name:\\n            return self.name\\n        if is_sequence(self.hosts):\\n            self.name = ','.join(self.hosts)\\n        else:\\n            self.name = self.hosts or ''\\n        return self.name\\n    @staticmethod"
  },
  {
    "code": "def _build_unsafe_keys(d):\\n\\t\\tunsafe_keys = set()\\n\\t\\tfor key, value in six.iteritems(d):\\n\\t\\t\\tif '@' in value:\\n\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfor unsafe_key in unsafe_keywords:\\n\\t\\t\\t\\t\\tif unsafe_key in key:\\n\\t\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treturn unsafe_keys\\n\\tunsafe_keywords = set(unsafe_keywords or\\n\\t\\t\\t\\t\\t\\t  getattr(settings, 'DJBLETS_PII_UNSAFE_URL_KEYWORDS',\\n\\t\\t\\t\\t\\t\\t\\t\\t  DEFAULT_PII_UNSAFE_URL_KEYWORDS))\\n\\tcur_unsafe_url_keys = _build_unsafe_keys(url_kwargs or {})\\n\\tcur_unsafe_qs_keys = _build_unsafe_keys(query_dict or {})\\n\\tif cur_unsafe_url_keys:\\n\\t\\tnew_url = re.sub(\\n\\t\\t\\t'(%s)' % '|'.join(\\n\\t\\t\\t\\tre.escape(url_kwargs[key])\\n\\t\\t\\t\\tfor key in cur_unsafe_url_keys\\n\\t\\t\\t),\\n\\t\\t\\t'<REDACTED>',\\n\\t\\t\\turl)\\n\\telse:\\n\\t\\tnew_url = url\\n\\tif cur_unsafe_qs_keys:\\n\\t\\tnew_query = query_dict.copy()\\n\\t\\tfor key in cur_unsafe_qs_keys:\\n\\t\\t\\tnew_query[key] = '<REDACTED>'\\n\\telse:\\n\\t\\tnew_query = query_dict\\n\\tif new_query:\\n\\t\\tnew_url = '%s?%s' % (new_url, new_query.urlencode(safe='/<>'))\\n\\treturn new_url",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fix a crash when generating PII-safe URLs with non-strings in kwargs.\\n\\nWhen generating PII-safe URLs, we attempt to analyze the content of any\\nvalues coming into the view from the keyword arguments. If something\\ncomes in that's not a string, this can crash, and that'll happen any\\ntime extra context is provided to a view (like a form class).\\n\\nWe now check the value type prior to checking the content.\\n\\nTesting Done:\\nUnit tests pass.\\n\\nTested with a view that crashed. Verified this fixed it.",
    "fixed_code": "def _build_unsafe_keys(d):\\n\\t\\tunsafe_keys = set()\\n\\t\\tfor key, value in six.iteritems(d):\\n\\t\\t\\tif isinstance(value, six.text_type):\\n\\t\\t\\t\\tif '@' in value:\\n\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tfor unsafe_key in unsafe_keywords:\\n\\t\\t\\t\\t\\t\\tif unsafe_key in key:\\n\\t\\t\\t\\t\\t\\t\\tunsafe_keys.add(key)\\n\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treturn unsafe_keys\\n\\tunsafe_keywords = set(unsafe_keywords or\\n\\t\\t\\t\\t\\t\\t  getattr(settings, 'DJBLETS_PII_UNSAFE_URL_KEYWORDS',\\n\\t\\t\\t\\t\\t\\t\\t\\t  DEFAULT_PII_UNSAFE_URL_KEYWORDS))\\n\\tcur_unsafe_url_keys = _build_unsafe_keys(url_kwargs or {})\\n\\tcur_unsafe_qs_keys = _build_unsafe_keys(query_dict or {})\\n\\tif cur_unsafe_url_keys:\\n\\t\\tnew_url = re.sub(\\n\\t\\t\\t'(%s)' % '|'.join(\\n\\t\\t\\t\\tre.escape(url_kwargs[key])\\n\\t\\t\\t\\tfor key in cur_unsafe_url_keys\\n\\t\\t\\t),\\n\\t\\t\\t'<REDACTED>',\\n\\t\\t\\turl)\\n\\telse:\\n\\t\\tnew_url = url\\n\\tif cur_unsafe_qs_keys:\\n\\t\\tnew_query = query_dict.copy()\\n\\t\\tfor key in cur_unsafe_qs_keys:\\n\\t\\t\\tnew_query[key] = '<REDACTED>'\\n\\telse:\\n\\t\\tnew_query = query_dict\\n\\tif new_query:\\n\\t\\tnew_url = '%s?%s' % (new_url, new_query.urlencode(safe='/<>'))\\n\\treturn new_url"
  },
  {
    "code": "def _get_content(self):\\n        if self.has_header('Content-Encoding'):\\n            return b''.join([str(e) for e in self._container])\\n        return b''.join([smart_bytes(e, self._charset) for e in self._container])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[py3] Ported django.http according to PEP 3333.\\n\\nPerfomed some style cleanup while I was in the area.",
    "fixed_code": "def _get_content(self):\\n        if self.has_header('Content-Encoding'):\\n            return b''.join([bytes(e) for e in self._container])\\n        return b''.join([smart_bytes(e, self._charset) for e in self._container])"
  },
  {
    "code": "def __init__(self,\\n                 sql,\\n                 bigquery_conn_id='google_cloud_default',\\n                 use_legacy_sql=True,\\n                 *args, **kwargs):\\n        super().__init__(sql=sql, *args, **kwargs)\\n        self.bigquery_conn_id = bigquery_conn_id\\n        self.sql = sql\\n        self.use_legacy_sql = use_legacy_sql",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5122] Normalize *_conn_id parameters in Bigquery operators (#5734)",
    "fixed_code": "def __init__(self,\\n                 sql,\\n                 gcp_conn_id='google_cloud_default',\\n                 bigquery_conn_id=None,\\n                 use_legacy_sql=True,\\n                 *args, **kwargs):\\n        super().__init__(sql=sql, *args, **kwargs)\\n        if not bigquery_conn_id:\\n            warnings.warn(\\n                \"The bigquery_conn_id parameter has been deprecated. You should pass \"\\n                \"the gcp_conn_id parameter.\", DeprecationWarning, stacklevel=3)\\n            gcp_conn_id = bigquery_conn_id\\n        self.gcp_conn_id = gcp_conn_id\\n        self.sql = sql\\n        self.use_legacy_sql = use_legacy_sql"
  },
  {
    "code": "def set_dags_paused_state(is_paused):\\n    session = settings.Session()\\n    dms = session.query(DagModel).filter(\\n        DagModel.dag_id.in_(DAG_IDS))\\n    for dm in dms:\\n        logging.info('Setting DAG :: %s is_paused=%s', dm, is_paused)\\n        dm.is_paused = is_paused\\n    session.commit()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_dags_paused_state(is_paused):\\n    session = settings.Session()\\n    dms = session.query(DagModel).filter(\\n        DagModel.dag_id.in_(DAG_IDS))\\n    for dm in dms:\\n        logging.info('Setting DAG :: %s is_paused=%s', dm, is_paused)\\n        dm.is_paused = is_paused\\n    session.commit()"
  },
  {
    "code": "def main(\\n    ctx: click.Context,\\n    code: Optional[str],\\n    line_length: int,\\n    target_version: List[TargetVersion],\\n    check: bool,\\n    diff: bool,\\n    color: bool,\\n    fast: bool,\\n    pyi: bool,\\n    skip_string_normalization: bool,\\n    skip_magic_trailing_comma: bool,\\n    experimental_string_processing: bool,\\n    quiet: bool,\\n    verbose: bool,\\n    include: Pattern,\\n    exclude: Pattern,\\n    extend_exclude: Optional[Pattern],\\n    force_exclude: Optional[Pattern],\\n    stdin_filename: Optional[str],\\n    src: Tuple[str, ...],\\n    config: Optional[str],\\n) -> None:\\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\\n    if target_version:\\n        versions = set(target_version)\\n    else:\\n        versions = set()\\n    mode = Mode(\\n        target_versions=versions,\\n        line_length=line_length,\\n        is_pyi=pyi,\\n        string_normalization=not skip_string_normalization,\\n        magic_trailing_comma=not skip_magic_trailing_comma,\\n        experimental_string_processing=experimental_string_processing,\\n    )\\n    if config and verbose:\\n        out(f\"Using configuration from {config}.\", bold=False, fg=\"blue\")\\n    if code is not None:\\n        print(format_str(code, mode=mode))\\n        ctx.exit(0)\\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\\n    sources = get_sources(\\n        ctx=ctx,\\n        src=src,\\n        quiet=quiet,\\n        verbose=verbose,\\n        include=include,\\n        exclude=exclude,\\n        extend_exclude=extend_exclude,\\n        force_exclude=force_exclude,\\n        report=report,\\n        stdin_filename=stdin_filename,\\n    )\\n    path_empty(\\n        sources,\\n        \"No Python files are present to be formatted. Nothing to do \ud83d\ude34\",\\n        quiet,\\n        verbose,\\n        ctx,\\n    )\\n    if len(sources) == 1:\\n        reformat_one(\\n            src=sources.pop(),\\n            fast=fast,\\n            write_back=write_back,\\n            mode=mode,\\n            report=report,\\n        )\\n    else:\\n        reformat_many(\\n            sources=sources, fast=fast, write_back=write_back, mode=mode, report=report\\n        )\\n    if verbose or not quiet:\\n        out(\"Oh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\" if report.return_code else \"All done! \u2728 \ud83c\udf70 \u2728\")\\n        click.secho(str(report), err=True)\\n    ctx.exit(report.return_code)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not use gitignore if explicitly passing excludes (#2170)\\n\\nCloses #2164.\\n\\nChanges behavior of how .gitignore is handled. With this change, the rules in .gitignore are only used as a fallback if no exclusion rule is explicitly passed on the command line or in pyproject.toml. Previously they were used regardless if explicit exclusion rules were specified, preventing any overriding of .gitignore rules.\\n\\nThose that depend only on .gitignore for their exclusion rules will not be affected. Those that use both .gitignore and exclude will find that exclude will act more like actually specifying exclude and not just another extra-excludes. If the previous behavior was desired, they should move their rules from exclude to extra-excludes.",
    "fixed_code": "def main(\\n    ctx: click.Context,\\n    code: Optional[str],\\n    line_length: int,\\n    target_version: List[TargetVersion],\\n    check: bool,\\n    diff: bool,\\n    color: bool,\\n    fast: bool,\\n    pyi: bool,\\n    skip_string_normalization: bool,\\n    skip_magic_trailing_comma: bool,\\n    experimental_string_processing: bool,\\n    quiet: bool,\\n    verbose: bool,\\n    include: Pattern,\\n    exclude: Optional[Pattern],\\n    extend_exclude: Optional[Pattern],\\n    force_exclude: Optional[Pattern],\\n    stdin_filename: Optional[str],\\n    src: Tuple[str, ...],\\n    config: Optional[str],\\n) -> None:\\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\\n    if target_version:\\n        versions = set(target_version)\\n    else:\\n        versions = set()\\n    mode = Mode(\\n        target_versions=versions,\\n        line_length=line_length,\\n        is_pyi=pyi,\\n        string_normalization=not skip_string_normalization,\\n        magic_trailing_comma=not skip_magic_trailing_comma,\\n        experimental_string_processing=experimental_string_processing,\\n    )\\n    if config and verbose:\\n        out(f\"Using configuration from {config}.\", bold=False, fg=\"blue\")\\n    if code is not None:\\n        print(format_str(code, mode=mode))\\n        ctx.exit(0)\\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\\n    sources = get_sources(\\n        ctx=ctx,\\n        src=src,\\n        quiet=quiet,\\n        verbose=verbose,\\n        include=include,\\n        exclude=exclude,\\n        extend_exclude=extend_exclude,\\n        force_exclude=force_exclude,\\n        report=report,\\n        stdin_filename=stdin_filename,\\n    )\\n    path_empty(\\n        sources,\\n        \"No Python files are present to be formatted. Nothing to do \ud83d\ude34\",\\n        quiet,\\n        verbose,\\n        ctx,\\n    )\\n    if len(sources) == 1:\\n        reformat_one(\\n            src=sources.pop(),\\n            fast=fast,\\n            write_back=write_back,\\n            mode=mode,\\n            report=report,\\n        )\\n    else:\\n        reformat_many(\\n            sources=sources, fast=fast, write_back=write_back, mode=mode, report=report\\n        )\\n    if verbose or not quiet:\\n        out(\"Oh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\" if report.return_code else \"All done! \u2728 \ud83c\udf70 \u2728\")\\n        click.secho(str(report), err=True)\\n    ctx.exit(report.return_code)"
  },
  {
    "code": "def merge(self, other, lsuffix='', rsuffix=''):\\n        l, r = items_overlap_with_suffix(left=self.items, lsuffix=lsuffix,\\n                                         right=other.items, rsuffix=rsuffix)\\n        new_items = _concat_indexes([l, r])\\n        new_blocks = [blk.copy(deep=False) for blk in self.blocks]\\n        offset = self.shape[0]\\n        for blk in other.blocks:\\n            blk = blk.copy(deep=False)\\n            blk.mgr_locs = blk.mgr_locs.add(offset)\\n            new_blocks.append(blk)\\n        new_axes = list(self.axes)\\n        new_axes[0] = new_items\\n        return self.__class__(_consolidate(new_blocks), new_axes)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def merge(self, other, lsuffix='', rsuffix=''):\\n        l, r = items_overlap_with_suffix(left=self.items, lsuffix=lsuffix,\\n                                         right=other.items, rsuffix=rsuffix)\\n        new_items = _concat_indexes([l, r])\\n        new_blocks = [blk.copy(deep=False) for blk in self.blocks]\\n        offset = self.shape[0]\\n        for blk in other.blocks:\\n            blk = blk.copy(deep=False)\\n            blk.mgr_locs = blk.mgr_locs.add(offset)\\n            new_blocks.append(blk)\\n        new_axes = list(self.axes)\\n        new_axes[0] = new_items\\n        return self.__class__(_consolidate(new_blocks), new_axes)"
  },
  {
    "code": "def __eq__(self, other):\\n        if isinstance(other, FrameSummary):\\n            return (self.filename == other.filename and\\n                    self.lineno == other.lineno and\\n                    self.name == other.name and\\n                    self.locals == other.locals)\\n        if isinstance(other, tuple):\\n            return (self.filename, self.lineno, self.name, self.line) == other\\n        return NotImplemented",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __eq__(self, other):\\n        if isinstance(other, FrameSummary):\\n            return (self.filename == other.filename and\\n                    self.lineno == other.lineno and\\n                    self.name == other.name and\\n                    self.locals == other.locals)\\n        if isinstance(other, tuple):\\n            return (self.filename, self.lineno, self.name, self.line) == other\\n        return NotImplemented"
  },
  {
    "code": "def consolidate(self):\\n        cons_data = self._data.consolidate()\\n        if cons_data is self._data:\\n            cons_data = cons_data.copy()\\n        return DataFrame(cons_data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def consolidate(self):\\n        cons_data = self._data.consolidate()\\n        if cons_data is self._data:\\n            cons_data = cons_data.copy()\\n        return DataFrame(cons_data)"
  },
  {
    "code": "def _sample_hiddens(self, v, rng):\\n        p = self._mean_hiddens(v)\\n        p[rng.uniform(size=p.shape) < p] = 1.\\n        return np.floor(p, p)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _sample_hiddens(self, v, rng):\\n        p = self._mean_hiddens(v)\\n        p[rng.uniform(size=p.shape) < p] = 1.\\n        return np.floor(p, p)"
  },
  {
    "code": "def check_declarative_intent_params(want, module):\\n    if module.params['interfaces']:\\n        name = module.params['name']\\n        rc, out, err = exec_command(module, 'show vrf | include {0}'.format(name))\\n        if rc == 0:\\n            data = out.strip().split()\\n            if not data:\\n                return\\n            vrf = data[0]\\n            interface = data[-1]\\n            for w in want:\\n                if w['name'] == vrf:\\n                    for i in w['interfaces']:\\n                        if get_interface_type(i) is not get_interface_type(interface):\\n                            module.fail_json(msg=\"Interface %s not configured on vrf %s\" % (interface, name))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def check_declarative_intent_params(want, module):\\n    if module.params['interfaces']:\\n        name = module.params['name']\\n        rc, out, err = exec_command(module, 'show vrf | include {0}'.format(name))\\n        if rc == 0:\\n            data = out.strip().split()\\n            if not data:\\n                return\\n            vrf = data[0]\\n            interface = data[-1]\\n            for w in want:\\n                if w['name'] == vrf:\\n                    for i in w['interfaces']:\\n                        if get_interface_type(i) is not get_interface_type(interface):\\n                            module.fail_json(msg=\"Interface %s not configured on vrf %s\" % (interface, name))"
  },
  {
    "code": "def login(self, username, password):\\n        if username and password:\\n            payload = {'user': username, 'password': password}\\n            url = '/web_api/login'\\n            response, response_data = self.send_request(url, payload)\\n        else:\\n            raise AnsibleConnectionFailure('Username and password are required for login')\\n        try:\\n            self.connection._auth = {'X-chkp-sid': response_data['sid']}\\n            self.connection._session_uid = response_data['uid']\\n        except KeyError:\\n            raise ConnectionError(\\n                'Server returned response without token info during connection authentication: %s' % response)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Enable logging to any domain in the check point machine (#63976)",
    "fixed_code": "def login(self, username, password):\\n        if username and password:\\n            cp_domain = self.get_option('domain')\\n            if cp_domain:\\n                payload = {'user': username, 'password': password, 'domain': cp_domain}\\n            else:\\n                payload = {'user': username, 'password': password}\\n            url = '/web_api/login'\\n            response, response_data = self.send_request(url, payload)\\n        else:\\n            raise AnsibleConnectionFailure('Username and password are required for login')\\n        try:\\n            self.connection._auth = {'X-chkp-sid': response_data['sid']}\\n            self.connection._session_uid = response_data['uid']\\n        except KeyError:\\n            raise ConnectionError(\\n                'Server returned response without token info during connection authentication: %s' % response)"
  },
  {
    "code": "def _group_reorder(data, label_list, axis=0):\\n    indexer = np.lexsort(label_list[::-1])\\n    sorted_labels = [labels.take(indexer) for labels in label_list]\\n    if isinstance(data, BlockManager):\\n        sorted_axis = data.axes[axis].take(indexer)\\n        sorted_data = data.reindex_axis(sorted_axis, axis=axis)\\n    elif isinstance(data, Series):\\n        sorted_axis = data.index.take(indexer)\\n        sorted_data = data.reindex(sorted_axis)\\n    else:\\n        sorted_data = data.take(indexer)\\n    return sorted_data, sorted_labels",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _group_reorder(data, label_list, axis=0):\\n    indexer = np.lexsort(label_list[::-1])\\n    sorted_labels = [labels.take(indexer) for labels in label_list]\\n    if isinstance(data, BlockManager):\\n        sorted_axis = data.axes[axis].take(indexer)\\n        sorted_data = data.reindex_axis(sorted_axis, axis=axis)\\n    elif isinstance(data, Series):\\n        sorted_axis = data.index.take(indexer)\\n        sorted_data = data.reindex(sorted_axis)\\n    else:\\n        sorted_data = data.take(indexer)\\n    return sorted_data, sorted_labels"
  },
  {
    "code": "def _format(self, object, stream, indent, allowance, context, level):\\n        level = level + 1\\n        objid = _id(object)\\n        if objid in context:\\n            stream.write(_recursion(object))\\n            self._recursive = True\\n            self._readable = False\\n            return\\n        rep = self._repr(object, context, level - 1)\\n        typ = _type(object)\\n        max_width = self._width - 1 - indent - allowance\\n        sepLines = _len(rep) > max_width\\n        write = stream.write\\n        if sepLines:\\n            r = getattr(typ, \"__repr__\", None)\\n            if issubclass(typ, dict):\\n                write('{')\\n                if self._indent_per_level > 1:\\n                    write((self._indent_per_level - 1) * ' ')\\n                length = _len(object)\\n                if length:\\n                    context[objid] = 1\\n                    indent = indent + self._indent_per_level\\n                    if issubclass(typ, _OrderedDict):\\n                        items = list(object.items())\\n                    else:\\n                        items = sorted(object.items(), key=_safe_tuple)\\n                    key, ent = items[0]\\n                    rep = self._repr(key, context, level)\\n                    write(rep)\\n                    write(': ')\\n                    self._format(ent, stream, indent + _len(rep) + 2,\\n                                  allowance + 1, context, level)\\n                    if length > 1:\\n                        for key, ent in items[1:]:\\n                            rep = self._repr(key, context, level)\\n                            write(',\\n%s%s: ' % (' '*indent, rep))\\n                            self._format(ent, stream, indent + _len(rep) + 2,\\n                                          allowance + 1, context, level)\\n                    indent = indent - self._indent_per_level\\n                    del context[objid]\\n                write('}')\\n                return\\n            if ((issubclass(typ, list) and r is list.__repr__) or\\n                (issubclass(typ, tuple) and r is tuple.__repr__) or\\n                (issubclass(typ, set) and r is set.__repr__) or\\n                (issubclass(typ, frozenset) and r is frozenset.__repr__)\\n               ):\\n                length = _len(object)\\n                if issubclass(typ, list):\\n                    write('[')\\n                    endchar = ']'\\n                elif issubclass(typ, tuple):\\n                    write('(')\\n                    endchar = ')'\\n                else:\\n                    if not length:\\n                        write(rep)\\n                        return\\n                    if typ is set:\\n                        write('{')\\n                        endchar = '}'\\n                    else:\\n                        write(typ.__name__)\\n                        write('({')\\n                        endchar = '})'\\n                        indent += len(typ.__name__) + 1\\n                    object = sorted(object, key=_safe_key)\\n                if self._indent_per_level > 1:\\n                    write((self._indent_per_level - 1) * ' ')\\n                if length:\\n                    context[objid] = 1\\n                    self._format_items(object, stream,\\n                                       indent + self._indent_per_level,\\n                                       allowance + 1, context, level)\\n                    del context[objid]\\n                if issubclass(typ, tuple) and length == 1:\\n                    write(',')\\n                write(endchar)\\n                return\\n            if issubclass(typ, str) and len(object) > 0 and r is str.__repr__:",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _format(self, object, stream, indent, allowance, context, level):\\n        level = level + 1\\n        objid = _id(object)\\n        if objid in context:\\n            stream.write(_recursion(object))\\n            self._recursive = True\\n            self._readable = False\\n            return\\n        rep = self._repr(object, context, level - 1)\\n        typ = _type(object)\\n        max_width = self._width - 1 - indent - allowance\\n        sepLines = _len(rep) > max_width\\n        write = stream.write\\n        if sepLines:\\n            r = getattr(typ, \"__repr__\", None)\\n            if issubclass(typ, dict):\\n                write('{')\\n                if self._indent_per_level > 1:\\n                    write((self._indent_per_level - 1) * ' ')\\n                length = _len(object)\\n                if length:\\n                    context[objid] = 1\\n                    indent = indent + self._indent_per_level\\n                    if issubclass(typ, _OrderedDict):\\n                        items = list(object.items())\\n                    else:\\n                        items = sorted(object.items(), key=_safe_tuple)\\n                    key, ent = items[0]\\n                    rep = self._repr(key, context, level)\\n                    write(rep)\\n                    write(': ')\\n                    self._format(ent, stream, indent + _len(rep) + 2,\\n                                  allowance + 1, context, level)\\n                    if length > 1:\\n                        for key, ent in items[1:]:\\n                            rep = self._repr(key, context, level)\\n                            write(',\\n%s%s: ' % (' '*indent, rep))\\n                            self._format(ent, stream, indent + _len(rep) + 2,\\n                                          allowance + 1, context, level)\\n                    indent = indent - self._indent_per_level\\n                    del context[objid]\\n                write('}')\\n                return\\n            if ((issubclass(typ, list) and r is list.__repr__) or\\n                (issubclass(typ, tuple) and r is tuple.__repr__) or\\n                (issubclass(typ, set) and r is set.__repr__) or\\n                (issubclass(typ, frozenset) and r is frozenset.__repr__)\\n               ):\\n                length = _len(object)\\n                if issubclass(typ, list):\\n                    write('[')\\n                    endchar = ']'\\n                elif issubclass(typ, tuple):\\n                    write('(')\\n                    endchar = ')'\\n                else:\\n                    if not length:\\n                        write(rep)\\n                        return\\n                    if typ is set:\\n                        write('{')\\n                        endchar = '}'\\n                    else:\\n                        write(typ.__name__)\\n                        write('({')\\n                        endchar = '})'\\n                        indent += len(typ.__name__) + 1\\n                    object = sorted(object, key=_safe_key)\\n                if self._indent_per_level > 1:\\n                    write((self._indent_per_level - 1) * ' ')\\n                if length:\\n                    context[objid] = 1\\n                    self._format_items(object, stream,\\n                                       indent + self._indent_per_level,\\n                                       allowance + 1, context, level)\\n                    del context[objid]\\n                if issubclass(typ, tuple) and length == 1:\\n                    write(',')\\n                write(endchar)\\n                return\\n            if issubclass(typ, str) and len(object) > 0 and r is str.__repr__:"
  },
  {
    "code": "def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\\n    if not target_versions:\\n        return GRAMMARS\\n    elif all(not version.is_python2() for version in target_versions):\\n        return [\\n            pygram.python_grammar_no_print_statement_no_exec_statement,\\n            pygram.python_grammar_no_print_statement,\\n        ]\\n    else:\\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\\n    if not target_versions:\\n        return GRAMMARS\\n    elif all(not version.is_python2() for version in target_versions):\\n        return [\\n            pygram.python_grammar_no_print_statement_no_exec_statement,\\n            pygram.python_grammar_no_print_statement,\\n        ]\\n    else:\\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]"
  },
  {
    "code": "def executemany(self, query, params=None):\\n        try:\\n            args = [(':arg%d' % i) for i in range(len(params[0]))]\\n        except (IndexError, TypeError):\\n            return None\\n        if query.endswith(';') or query.endswith('/'):\\n            query = query[:-1]\\n        query = convert_unicode(query % tuple(args), self.charset)\\n        formatted = [self._format_params(i) for i in params]\\n        self._guess_input_sizes(formatted)\\n        try:\\n            return self.cursor.executemany(query,\\n                                [self._param_generator(p) for p in formatted])\\n        except Database.IntegrityError, e:\\n            raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n        except Database.DatabaseError, e:\\n            if hasattr(e.args[0], 'code') and e.args[0].code == 1400 and not isinstance(e, IntegrityError):\\n                raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n            raise utils.DatabaseError, utils.DatabaseError(*tuple(e)), sys.exc_info()[2]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #10320 -- Made it possible to use executemany with iterators. Thanks MockSoul for the report.",
    "fixed_code": "def executemany(self, query, params=None):\\n        if params is not None and not isinstance(params, (list, tuple)):\\n            params = list(params)\\n        try:\\n            args = [(':arg%d' % i) for i in range(len(params[0]))]\\n        except (IndexError, TypeError):\\n            return None\\n        if query.endswith(';') or query.endswith('/'):\\n            query = query[:-1]\\n        query = convert_unicode(query % tuple(args), self.charset)\\n        formatted = [self._format_params(i) for i in params]\\n        self._guess_input_sizes(formatted)\\n        try:\\n            return self.cursor.executemany(query,\\n                                [self._param_generator(p) for p in formatted])\\n        except Database.IntegrityError, e:\\n            raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n        except Database.DatabaseError, e:\\n            if hasattr(e.args[0], 'code') and e.args[0].code == 1400 and not isinstance(e, IntegrityError):\\n                raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]\\n            raise utils.DatabaseError, utils.DatabaseError(*tuple(e)), sys.exc_info()[2]"
  },
  {
    "code": "def _chop(self, sdata, slice_obj):\\n        return sdata.iloc[slice_obj]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _chop(self, sdata, slice_obj):\\n        return sdata.iloc[slice_obj]"
  },
  {
    "code": "def _run_image(self):\\n        self.log.info('Starting docker service from image %s', self.image)\\n        self.service = self.cli.create_service(\\n            types.TaskTemplate(\\n                container_spec=types.ContainerSpec(\\n                    image=self.image,\\n                    command=self.get_command(),\\n                    env=self.environment,\\n                    user=self.user,\\n                    tty=self.tty,\\n                ),\\n                restart_policy=types.RestartPolicy(condition='none'),\\n                resources=types.Resources(mem_limit=self.mem_limit)\\n            ),\\n            name='airflow-%s' % get_random_string(),\\n            labels={'name': 'airflow__%s__%s' % (self.dag_id, self.task_id)}\\n        )\\n        self.log.info('Service started: %s', str(self.service))\\n        status = None\\n        while not self.cli.tasks(filters={'service': self.service['ID']}):\\n            continue\\n        while True:\\n            status = self.cli.tasks(\\n                filters={'service': self.service['ID']}\\n            )[0]['Status']['State']\\n            if status in ['failed', 'complete']:\\n                self.log.info('Service status before exiting: %s', status)\\n                break\\n        if self.auto_remove:\\n            self.cli.remove_service(self.service['ID'])\\n        if status == 'failed':\\n            raise AirflowException('Service failed: ' + repr(self.service))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552)",
    "fixed_code": "def _run_image(self):\\n        self.log.info('Starting docker service from image %s', self.image)\\n        self.service = self.cli.create_service(\\n            types.TaskTemplate(\\n                container_spec=types.ContainerSpec(\\n                    image=self.image,\\n                    command=self.get_command(),\\n                    env=self.environment,\\n                    user=self.user,\\n                    tty=self.tty,\\n                ),\\n                restart_policy=types.RestartPolicy(condition='none'),\\n                resources=types.Resources(mem_limit=self.mem_limit)\\n            ),\\n            name='airflow-%s' % get_random_string(),\\n            labels={'name': 'airflow__%s__%s' % (self.dag_id, self.task_id)}\\n        )\\n        self.log.info('Service started: %s', str(self.service))\\n        while not self.cli.tasks(filters={'service': self.service['ID']}):\\n            continue\\n        if self.enable_logging:\\n            self._stream_logs_to_output()\\n        while True:\\n            if self._has_service_terminated():\\n                self.log.info('Service status before exiting: %s', self._service_status())\\n                break\\n        if self.auto_remove:\\n            self.cli.remove_service(self.service['ID'])\\n        if self._service_status() == 'failed':\\n            raise AirflowException('Service failed: ' + repr(self.service))"
  },
  {
    "code": "class EmailBackend(ConsoleEmailBackend):\\n\\tdef __init__(self, *args, **kwargs):\\n\\t\\tself._fname = None\\n\\t\\tif 'file_path' in kwargs:\\n\\t\\t\\tself.file_path = kwargs.pop('file_path')\\n\\t\\telse:\\n\\t\\t\\tself.file_path = getattr(settings, 'EMAIL_FILE_PATH', None)\\n\\t\\tif not isinstance(self.file_path, six.string_types):\\n\\t\\t\\traise ImproperlyConfigured('Path for saving emails is invalid: %r' % self.file_path)\\n\\t\\tself.file_path = os.path.abspath(self.file_path)\\n\\t\\tif os.path.exists(self.file_path) and not os.path.isdir(self.file_path):\\n\\t\\t\\traise ImproperlyConfigured('Path for saving email messages exists, but is not a directory: %s' % self.file_path)\\n\\t\\telif not os.path.exists(self.file_path):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tos.makedirs(self.file_path)\\n\\t\\t\\texcept OSError as err:\\n\\t\\t\\t\\traise ImproperlyConfigured('Could not create directory for saving email messages: %s (%s)' % (self.file_path, err))\\n\\t\\tif not os.access(self.file_path, os.W_OK):\\n\\t\\t\\traise ImproperlyConfigured('Could not write to directory: %s' % self.file_path)\\n\\t\\tkwargs['stream'] = None\\n\\t\\tsuper(EmailBackend, self).__init__(*args, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "class EmailBackend(ConsoleEmailBackend):\\n\\tdef __init__(self, *args, **kwargs):\\n\\t\\tself._fname = None\\n\\t\\tif 'file_path' in kwargs:\\n\\t\\t\\tself.file_path = kwargs.pop('file_path')\\n\\t\\telse:\\n\\t\\t\\tself.file_path = getattr(settings, 'EMAIL_FILE_PATH', None)\\n\\t\\tif not isinstance(self.file_path, six.string_types):\\n\\t\\t\\traise ImproperlyConfigured('Path for saving emails is invalid: %r' % self.file_path)\\n\\t\\tself.file_path = os.path.abspath(self.file_path)\\n\\t\\tif os.path.exists(self.file_path) and not os.path.isdir(self.file_path):\\n\\t\\t\\traise ImproperlyConfigured('Path for saving email messages exists, but is not a directory: %s' % self.file_path)\\n\\t\\telif not os.path.exists(self.file_path):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tos.makedirs(self.file_path)\\n\\t\\t\\texcept OSError as err:\\n\\t\\t\\t\\traise ImproperlyConfigured('Could not create directory for saving email messages: %s (%s)' % (self.file_path, err))\\n\\t\\tif not os.access(self.file_path, os.W_OK):\\n\\t\\t\\traise ImproperlyConfigured('Could not write to directory: %s' % self.file_path)\\n\\t\\tkwargs['stream'] = None\\n\\t\\tsuper(EmailBackend, self).__init__(*args, **kwargs)"
  },
  {
    "code": "def _resolve_output_field(self):\\n        sources_iter = (\\n            source for source in self.get_source_fields() if source is not None\\n        )\\n        for output_field in sources_iter:\\n            for source in sources_iter:\\n                if not isinstance(output_field, source.__class__):\\n                    raise FieldError(\\n                        \"Expression contains mixed types: %s, %s. You must \"\\n                        \"set output_field.\"\\n                        % (\\n                            output_field.__class__.__name__,\\n                            source.__class__.__name__,\\n                        )\\n                    )\\n            return output_field\\n    @staticmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #33397 -- Corrected resolving output_field for DateField/DateTimeField/TimeField/DurationFields.\\n\\nThis includes refactoring of CombinedExpression._resolve_output_field()\\nso it no longer uses the behavior inherited from Expression of guessing\\nsame output type if argument types match, and instead we explicitly\\ndefine the output type of all supported operations.\\n\\nThis also makes nonsensical operations involving dates\\n(e.g. date + date) raise a FieldError, and adds support for\\nautomatically inferring output_field for cases such as:",
    "fixed_code": "def _resolve_output_field(self):\\n        sources_iter = (\\n            source for source in self.get_source_fields() if source is not None\\n        )\\n        for output_field in sources_iter:\\n            for source in sources_iter:\\n                if not isinstance(output_field, source.__class__):\\n                    raise FieldError(\\n                        \"Expression contains mixed types: %s, %s. You must \"\\n                        \"set output_field.\"\\n                        % (\\n                            output_field.__class__.__name__,\\n                            source.__class__.__name__,\\n                        )\\n                    )\\n            return output_field\\n    @staticmethod"
  },
  {
    "code": "def format_file_in_place(\\n    src: Path,\\n    fast: bool,\\n    mode: Mode,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    if src.suffix == \".pyi\":\\n        mode = replace(mode, is_pyi=True)\\n    then = datetime.utcfromtimestamp(src.stat().st_mtime)\\n    with open(src, \"rb\") as buf:\\n        src_contents, encoding, newline = decode_bytes(buf.read())\\n    try:\\n        dst_contents = format_file_contents(src_contents, fast=fast, mode=mode)\\n    except NothingChanged:\\n        return False\\n    if write_back == WriteBack.YES:\\n        with open(src, \"w\", encoding=encoding, newline=newline) as f:\\n            f.write(dst_contents)\\n    elif write_back == WriteBack.DIFF:\\n        now = datetime.utcnow()\\n        src_name = f\"{src}\\t{then} +0000\"\\n        dst_name = f\"{src}\\t{now} +0000\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        with lock or nullcontext():\\n            f = io.TextIOWrapper(\\n                sys.stdout.buffer,\\n                encoding=encoding,\\n                newline=newline,\\n                write_through=True,\\n            )\\n            f.write(diff_contents)\\n            f.detach()\\n    return True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add option for printing a colored diff (#1266)",
    "fixed_code": "def format_file_in_place(\\n    src: Path,\\n    fast: bool,\\n    mode: Mode,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    if src.suffix == \".pyi\":\\n        mode = replace(mode, is_pyi=True)\\n    then = datetime.utcfromtimestamp(src.stat().st_mtime)\\n    with open(src, \"rb\") as buf:\\n        src_contents, encoding, newline = decode_bytes(buf.read())\\n    try:\\n        dst_contents = format_file_contents(src_contents, fast=fast, mode=mode)\\n    except NothingChanged:\\n        return False\\n    if write_back == WriteBack.YES:\\n        with open(src, \"w\", encoding=encoding, newline=newline) as f:\\n            f.write(dst_contents)\\n    elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\\n        now = datetime.utcnow()\\n        src_name = f\"{src}\\t{then} +0000\"\\n        dst_name = f\"{src}\\t{now} +0000\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if write_back == write_back.COLOR_DIFF:\\n            diff_contents = color_diff(diff_contents)\\n        with lock or nullcontext():\\n            f = io.TextIOWrapper(\\n                sys.stdout.buffer,\\n                encoding=encoding,\\n                newline=newline,\\n                write_through=True,\\n            )\\n            f = wrap_stream_for_windows(f)\\n            f.write(diff_contents)\\n            f.detach()\\n    return True"
  },
  {
    "code": "def load_binstring(self):\\n        len, = unpack('<i', self.read(4))\\n        if len < 0:\\n            raise UnpicklingError(\"BINSTRING pickle has negative byte count\")\\n        data = self.read(len)\\n        value = str(data, self.encoding, self.errors)\\n        self.append(value)\\n    dispatch[BINSTRING[0]] = load_binstring",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #6784: Strings from Python 2 can now be unpickled as bytes objects.\\n\\nInitial patch by Merlijn van Deen.\\n\\nI've added a few unrelated docstring fixes in the patch while I was at\\nit, which makes the documentation for pickle a bit more consistent.",
    "fixed_code": "def load_binstring(self):\\n        len, = unpack('<i', self.read(4))\\n        if len < 0:\\n            raise UnpicklingError(\"BINSTRING pickle has negative byte count\")\\n        data = self.read(len)\\n        self.append(self._decode_string(data))\\n    dispatch[BINSTRING[0]] = load_binstring"
  },
  {
    "code": "def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\\n    arr, mask = _replace_nan(a, 0)\\n    if mask is None:\\n        return np.var(arr, axis=axis, dtype=dtype, out=out, ddof=ddof,\\n                      keepdims=keepdims)\\n    if dtype is not None:\\n        dtype = np.dtype(dtype)\\n    if dtype is not None and not issubclass(dtype.type, np.inexact):\\n        raise TypeError(\"If a is inexact, then dtype must be inexact\")\\n    if out is not None and not issubclass(out.dtype.type, np.inexact):\\n        raise TypeError(\"If a is inexact, then out must be inexact\")\\n    with warnings.catch_warnings():\\n        warnings.simplefilter('ignore')\\n        cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=True)\\n        avg = np.sum(arr, axis=axis, dtype=dtype, keepdims=True)\\n        avg = _divide_by_count(avg, cnt)\\n        np.subtract(arr, avg, out=arr, casting='unsafe')\\n        arr = _copyto(arr, 0, mask)\\n        if issubclass(arr.dtype.type, np.complexfloating):\\n            sqr = np.multiply(arr, arr.conj(), out=arr).real\\n        else:\\n            sqr = np.multiply(arr, arr, out=arr)\\n        var = np.sum(sqr, axis=axis, dtype=dtype, out=out, keepdims=keepdims)\\n        if var.ndim < cnt.ndim:\\n            cnt = cnt.squeeze(axis)\\n        dof = cnt - ddof\\n        var = _divide_by_count(var, dof)\\n    isbad = (dof <= 0)\\n    if np.any(isbad):\\n        warnings.warn(\"Degrees of freedom <= 0 for slice.\", RuntimeWarning)\\n        var = _copyto(var, np.nan, isbad)\\n    return var",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):\\n    arr, mask = _replace_nan(a, 0)\\n    if mask is None:\\n        return np.var(arr, axis=axis, dtype=dtype, out=out, ddof=ddof,\\n                      keepdims=keepdims)\\n    if dtype is not None:\\n        dtype = np.dtype(dtype)\\n    if dtype is not None and not issubclass(dtype.type, np.inexact):\\n        raise TypeError(\"If a is inexact, then dtype must be inexact\")\\n    if out is not None and not issubclass(out.dtype.type, np.inexact):\\n        raise TypeError(\"If a is inexact, then out must be inexact\")\\n    with warnings.catch_warnings():\\n        warnings.simplefilter('ignore')\\n        cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=True)\\n        avg = np.sum(arr, axis=axis, dtype=dtype, keepdims=True)\\n        avg = _divide_by_count(avg, cnt)\\n        np.subtract(arr, avg, out=arr, casting='unsafe')\\n        arr = _copyto(arr, 0, mask)\\n        if issubclass(arr.dtype.type, np.complexfloating):\\n            sqr = np.multiply(arr, arr.conj(), out=arr).real\\n        else:\\n            sqr = np.multiply(arr, arr, out=arr)\\n        var = np.sum(sqr, axis=axis, dtype=dtype, out=out, keepdims=keepdims)\\n        if var.ndim < cnt.ndim:\\n            cnt = cnt.squeeze(axis)\\n        dof = cnt - ddof\\n        var = _divide_by_count(var, dof)\\n    isbad = (dof <= 0)\\n    if np.any(isbad):\\n        warnings.warn(\"Degrees of freedom <= 0 for slice.\", RuntimeWarning)\\n        var = _copyto(var, np.nan, isbad)\\n    return var"
  },
  {
    "code": "def flush(self):\\n\\t\\tself.clear()\\n\\t\\tself.delete(self.session_key)\\n\\t\\tself._session_key = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def flush(self):\\n\\t\\tself.clear()\\n\\t\\tself.delete(self.session_key)\\n\\t\\tself._session_key = None"
  },
  {
    "code": "def take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan, mask_info=None,\\n            allow_fill=True):\\n    if is_extension_array_dtype(arr):\\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\\n    elif is_datetimetz(arr):\\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\\n    elif is_interval_dtype(arr):\\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\\n    if is_sparse(arr):\\n        arr = arr.get_values()\\n    if indexer is None:\\n        indexer = np.arange(arr.shape[axis], dtype=np.int64)\\n        dtype, fill_value = arr.dtype, arr.dtype.type()\\n    else:\\n        indexer = _ensure_int64(indexer, copy=False)\\n        if not allow_fill:\\n            dtype, fill_value = arr.dtype, arr.dtype.type()\\n            mask_info = None, False\\n        else:\\n            dtype, fill_value = maybe_promote(arr.dtype, fill_value)\\n            if dtype != arr.dtype and (out is None or out.dtype != dtype):\\n                if mask_info is not None:\\n                    mask, needs_masking = mask_info\\n                else:\\n                    mask = indexer == -1\\n                    needs_masking = mask.any()\\n                    mask_info = mask, needs_masking\\n                if needs_masking:\\n                    if out is not None and out.dtype != dtype:\\n                        raise TypeError('Incompatible type for fill_value')\\n                else:\\n                    dtype, fill_value = arr.dtype, arr.dtype.type()\\n    flip_order = False\\n    if arr.ndim == 2:\\n        if arr.flags.f_contiguous:\\n            flip_order = True\\n    if flip_order:\\n        arr = arr.T\\n        axis = arr.ndim - axis - 1\\n        if out is not None:\\n            out = out.T\\n    if out is None:\\n        out_shape = list(arr.shape)\\n        out_shape[axis] = len(indexer)\\n        out_shape = tuple(out_shape)\\n        if arr.flags.f_contiguous and axis == arr.ndim - 1:\\n            out = np.empty(out_shape, dtype=dtype, order='F')\\n        else:\\n            out = np.empty(out_shape, dtype=dtype)\\n    func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\\n                                 mask_info=mask_info)\\n    func(arr, indexer, out, fill_value)\\n    if flip_order:\\n        out = out.T\\n    return out",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DEPR: Series ndarray properties (strides, data, base, itemsize, flags) (#20721)",
    "fixed_code": "def take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan, mask_info=None,\\n            allow_fill=True):\\n    if is_extension_array_dtype(arr):\\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\\n    elif is_datetimetz(arr):\\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\\n    elif is_interval_dtype(arr):\\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\\n    if is_sparse(arr):\\n        arr = arr.get_values()\\n    arr = np.asarray(arr)\\n    if indexer is None:\\n        indexer = np.arange(arr.shape[axis], dtype=np.int64)\\n        dtype, fill_value = arr.dtype, arr.dtype.type()\\n    else:\\n        indexer = _ensure_int64(indexer, copy=False)\\n        if not allow_fill:\\n            dtype, fill_value = arr.dtype, arr.dtype.type()\\n            mask_info = None, False\\n        else:\\n            dtype, fill_value = maybe_promote(arr.dtype, fill_value)\\n            if dtype != arr.dtype and (out is None or out.dtype != dtype):\\n                if mask_info is not None:\\n                    mask, needs_masking = mask_info\\n                else:\\n                    mask = indexer == -1\\n                    needs_masking = mask.any()\\n                    mask_info = mask, needs_masking\\n                if needs_masking:\\n                    if out is not None and out.dtype != dtype:\\n                        raise TypeError('Incompatible type for fill_value')\\n                else:\\n                    dtype, fill_value = arr.dtype, arr.dtype.type()\\n    flip_order = False\\n    if arr.ndim == 2:\\n        if arr.flags.f_contiguous:\\n            flip_order = True\\n    if flip_order:\\n        arr = arr.T\\n        axis = arr.ndim - axis - 1\\n        if out is not None:\\n            out = out.T\\n    if out is None:\\n        out_shape = list(arr.shape)\\n        out_shape[axis] = len(indexer)\\n        out_shape = tuple(out_shape)\\n        if arr.flags.f_contiguous and axis == arr.ndim - 1:\\n            out = np.empty(out_shape, dtype=dtype, order='F')\\n        else:\\n            out = np.empty(out_shape, dtype=dtype)\\n    func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\\n                                 mask_info=mask_info)\\n    func(arr, indexer, out, fill_value)\\n    if flip_order:\\n        out = out.T\\n    return out"
  },
  {
    "code": "def _key_to_file(self, key):\\n        return os.path.join(self._dir, urllib.quote_plus(key))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #4845 -- Fixed some problems with Unicode usage and caching. Thanks, Jeremy Dunck.",
    "fixed_code": "def _key_to_file(self, key):\\n        return os.path.join(self._dir, urlquote_plus(key))"
  },
  {
    "code": "def get_args(tp):\\n    if isinstance(tp, _AnnotatedAlias):\\n        return (tp.__origin__,) + tp.__metadata__\\n    if isinstance(tp, (_GenericAlias, GenericAlias)):\\n        res = tp.__args__\\n        if (tp.__origin__ is collections.abc.Callable\\n                and not (len(res) == 2 and _is_param_expr(res[0]))):\\n            res = (list(res[:-1]), res[-1])\\n        return res\\n    if isinstance(tp, types.UnionType):\\n        return tp.__args__\\n    return ()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "gh-91621: Fix typing.get_type_hints for collections.abc.Callable (#91656)\\n\\nThis mirrors logic in typing.get_args. The trickiness comes from how we\\nflatten args in collections.abc.Callable, see",
    "fixed_code": "def get_args(tp):\\n    if isinstance(tp, _AnnotatedAlias):\\n        return (tp.__origin__,) + tp.__metadata__\\n    if isinstance(tp, (_GenericAlias, GenericAlias)):\\n        res = tp.__args__\\n        if _should_unflatten_callable_args(tp, res):\\n            res = (list(res[:-1]), res[-1])\\n        return res\\n    if isinstance(tp, types.UnionType):\\n        return tp.__args__\\n    return ()"
  },
  {
    "code": "def get_param_values(cls, params, args, kwargs):\\n\\t\\tresult = {}\\n\\t\\tparams_dict = dict(params)\\n\\t\\ttask_name = cls.task_family\\n\\t\\texc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\\n\\t\\tpositional_params = [(n, p) for n, p in params if p.significant]\\n\\t\\tfor i, arg in enumerate(args):\\n\\t\\t\\tif i >= len(positional_params):\\n\\t\\t\\t\\traise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))\\n\\t\\t\\tparam_name, param_obj = positional_params[i]\\n\\t\\t\\tresult[param_name] = arg\\n\\t\\tfor param_name, arg in six.iteritems(kwargs):\\n\\t\\t\\tif param_name in result:\\n\\t\\t\\t\\traise parameter.DuplicateParameterException('%s: parameter %s was already set as a positional parameter' % (exc_desc, param_name))\\n\\t\\t\\tif param_name not in params_dict:\\n\\t\\t\\t\\traise parameter.UnknownParameterException('%s: unknown parameter %s' % (exc_desc, param_name))\\n\\t\\t\\tresult[param_name] = arg\\n\\t\\tfor param_name, param_obj in params:\\n\\t\\t\\tif param_name not in result:\\n\\t\\t\\t\\tif not param_obj.has_task_value(task_name, param_name):\\n\\t\\t\\t\\t\\traise parameter.MissingParameterException(\"%s: requires the '%s' parameter to be set\" % (exc_desc, param_name))\\n\\t\\t\\t\\tresult[param_name] = param_obj.task_value(task_name, param_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Positional params: revert behavior to a78338c^\\n\\nAt the moment, we think the behavioral change introduced in a78338c was\\na mistake. There were no test cases that covered the behavior of passing\\nglobal parameters positionally, so this commits adds that.\\n\\nThe major issue was partially solved in f5c3316, but I think that was\\nkind of an arbitrary fix that still breaks backward compatibility.\\nPlease see the discussion in spotify/luigi#738.  Hopefully, nobody\\nstarted to rely on this intermediete behavior that would have existed\\nfor about 2 weeks.\\n\\nEventually, we'll have to change the condition\\n\\n\\t\\tpositional_params = [(n, p) for n, p in params if not p.is_global]\\n\\nto just\\n\\n\\t\\tpositional_params = [(n, p) for n, p in params]\\n\\njust like we did in a78338c. Though that should be done after global\\nparameters have been deprecated for half a year or something at least.\\nPeople need time to change their cron lines from `--pool development` to\\n`--BaseHadoopJobTask-pool development`.  Then we can change it to\\n\\n\\t\\tpositional_params = [(n, p) for n, p in params]\\n\\nAND remove is_global (you can't just do one of them like we did in\\na78338c)",
    "fixed_code": "def get_param_values(cls, params, args, kwargs):\\n\\t\\tresult = {}\\n\\t\\tparams_dict = dict(params)\\n\\t\\ttask_name = cls.task_family\\n\\t\\texc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\\n\\t\\tpositional_params = [(n, p) for n, p in params if not p.is_global]\\n\\t\\tfor i, arg in enumerate(args):\\n\\t\\t\\tif i >= len(positional_params):\\n\\t\\t\\t\\traise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))\\n\\t\\t\\tparam_name, param_obj = positional_params[i]\\n\\t\\t\\tresult[param_name] = arg\\n\\t\\tfor param_name, arg in six.iteritems(kwargs):\\n\\t\\t\\tif param_name in result:\\n\\t\\t\\t\\traise parameter.DuplicateParameterException('%s: parameter %s was already set as a positional parameter' % (exc_desc, param_name))\\n\\t\\t\\tif param_name not in params_dict:\\n\\t\\t\\t\\traise parameter.UnknownParameterException('%s: unknown parameter %s' % (exc_desc, param_name))\\n\\t\\t\\tresult[param_name] = arg\\n\\t\\tfor param_name, param_obj in params:\\n\\t\\t\\tif param_name not in result:\\n\\t\\t\\t\\tif not param_obj.has_task_value(task_name, param_name):\\n\\t\\t\\t\\t\\traise parameter.MissingParameterException(\"%s: requires the '%s' parameter to be set\" % (exc_desc, param_name))\\n\\t\\t\\t\\tresult[param_name] = param_obj.task_value(task_name, param_name)"
  },
  {
    "code": "def handle_agnos_update(wait_helper: WaitTimeHelper) -> None:\\n  from system.hardware.tici.agnos import flash_agnos_update, get_target_slot_number\\n  cur_version = HARDWARE.get_os_version()\\n  updated_version = run([\"bash\", \"-c\", r\"unset AGNOS_VERSION && source launch_env.sh && \\\\n                          echo -n $AGNOS_VERSION\"], OVERLAY_MERGED).strip()\\n  cloudlog.info(f\"AGNOS version check: {cur_version} vs {updated_version}\")\\n  if cur_version == updated_version:\\n    return\\n  set_consistent_flag(False)\\n  cloudlog.info(f\"Beginning background installation for AGNOS {updated_version}\")\\n  set_offroad_alert(\"Offroad_NeosUpdate\", True)\\n  manifest_path = os.path.join(OVERLAY_MERGED, \"system/hardware/tici/agnos.json\")\\n  target_slot_number = get_target_slot_number()\\n  flash_agnos_update(manifest_path, target_slot_number, cloudlog)\\n  set_offroad_alert(\"Offroad_NeosUpdate\", False)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def handle_agnos_update(wait_helper: WaitTimeHelper) -> None:\\n  from system.hardware.tici.agnos import flash_agnos_update, get_target_slot_number\\n  cur_version = HARDWARE.get_os_version()\\n  updated_version = run([\"bash\", \"-c\", r\"unset AGNOS_VERSION && source launch_env.sh && \\\\n                          echo -n $AGNOS_VERSION\"], OVERLAY_MERGED).strip()\\n  cloudlog.info(f\"AGNOS version check: {cur_version} vs {updated_version}\")\\n  if cur_version == updated_version:\\n    return\\n  set_consistent_flag(False)\\n  cloudlog.info(f\"Beginning background installation for AGNOS {updated_version}\")\\n  set_offroad_alert(\"Offroad_NeosUpdate\", True)\\n  manifest_path = os.path.join(OVERLAY_MERGED, \"system/hardware/tici/agnos.json\")\\n  target_slot_number = get_target_slot_number()\\n  flash_agnos_update(manifest_path, target_slot_number, cloudlog)\\n  set_offroad_alert(\"Offroad_NeosUpdate\", False)"
  },
  {
    "code": "def _try_convert_dates(parser, colspec, data_dict, columns):\\n    colspec = _get_col_names(colspec, columns)\\n    new_name = '_'.join(colspec)\\n    to_parse = [data_dict[c] for c in colspec if c in data_dict]\\n    try:\\n        new_col = parser(*to_parse)\\n    except DateConversionError:\\n        new_col = parser(_concat_date_cols(to_parse))\\n    return new_name, new_col, colspec",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _try_convert_dates(parser, colspec, data_dict, columns):\\n    colspec = _get_col_names(colspec, columns)\\n    new_name = '_'.join(colspec)\\n    to_parse = [data_dict[c] for c in colspec if c in data_dict]\\n    try:\\n        new_col = parser(*to_parse)\\n    except DateConversionError:\\n        new_col = parser(_concat_date_cols(to_parse))\\n    return new_name, new_col, colspec"
  },
  {
    "code": "def kneighbors(self, X, return_distance=True, **params):\\n        self._set_params(**params)\\n        X = np.atleast_2d(X)\\n        if self.ball_tree is None:\\n            if self.algorithm == 'brute_inplace' and not return_distance:\\n                return knn_brute(self._fit_X, X, self.n_neighbors)\\n            else:\\n                dist = euclidean_distances(X, self._fit_X, squared=True)\\n                neigh_ind = dist.argsort(axis=1)[:, :self.n_neighbors]\\n            if not return_distance:\\n                return neigh_ind\\n            else:\\n                return dist.T[neigh_ind], neigh_ind\\n        else:\\n            return self.ball_tree.query(X, self.n_neighbors,\\n                                        return_distance=return_distance)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def kneighbors(self, X, return_distance=True, **params):\\n        self._set_params(**params)\\n        X = np.atleast_2d(X)\\n        if self.ball_tree is None:\\n            if self.algorithm == 'brute_inplace' and not return_distance:\\n                return knn_brute(self._fit_X, X, self.n_neighbors)\\n            else:\\n                dist = euclidean_distances(X, self._fit_X, squared=True)\\n                neigh_ind = dist.argsort(axis=1)[:, :self.n_neighbors]\\n            if not return_distance:\\n                return neigh_ind\\n            else:\\n                return dist.T[neigh_ind], neigh_ind\\n        else:\\n            return self.ball_tree.query(X, self.n_neighbors,\\n                                        return_distance=return_distance)"
  },
  {
    "code": "def max(self, axis=0):\\n        if axis == 0:\\n            med = [np.max(self[col].valid()) for col in self.columns]\\n            return Series(med, index=self.columns)\\n        elif axis == 1:\\n            med = [np.max(self.getXS(k).valid()) for k in self.index]\\n            return Series(med, index=self.index)\\n        else:\\n            raise Exception('Must have 0<= axis <= 1')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "reindex _backfill bugfix (switched to buffer interface), min / max performance enhancements",
    "fixed_code": "def max(self, axis=0):\\n        return self.apply(Series.max, axis=axis)"
  },
  {
    "code": "def __init__(self, s, tags, numoflines, squeezer):\\n        self.s = s\\n        self.tags = tags\\n        self.numoflines = numoflines\\n        self.squeezer = squeezer\\n        self.editwin = editwin = squeezer.editwin\\n        self.text = text = editwin.text\\n        self.base_text = editwin.per.bottom\\n        line_plurality = \"lines\" if numoflines != 1 else \"line\"\\n        button_text = f\"Squeezed text ({numoflines} {line_plurality}).\"\\n        tk.Button.__init__(self, text, text=button_text,\\n                           background=\"\\n        button_tooltip_text = (\\n            \"Double-click to expand, right-click for more options.\"\\n        )\\n        Hovertip(self, button_tooltip_text, hover_delay=80)\\n        self.bind(\"<Double-Button-1>\", self.expand)\\n        if macosx.isAquaTk():\\n            self.bind(\"<Button-2>\", self.context_menu_event)\\n        else:\\n            self.bind(\"<Button-3>\", self.context_menu_event)\\n        self.selection_handle(\\n            lambda offset, length: s[int(offset):int(offset) + int(length)])\\n        self.is_dangerous = None\\n        self.after_idle(self.set_is_dangerous)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-35196: Optimize Squeezer's write() interception (GH-10454)\\n\\nThe new functionality of Squeezer.reload() is also tested, along with some general\\nre-working of the tests in test_squeezer.py.",
    "fixed_code": "def __init__(self, s, tags, numoflines, squeezer):\\n        self.s = s\\n        self.tags = tags\\n        self.numoflines = numoflines\\n        self.squeezer = squeezer\\n        self.editwin = editwin = squeezer.editwin\\n        self.text = text = editwin.text\\n        self.base_text = editwin.per.bottom\\n        line_plurality = \"lines\" if numoflines != 1 else \"line\"\\n        button_text = f\"Squeezed text ({numoflines} {line_plurality}).\"\\n        tk.Button.__init__(self, text, text=button_text,\\n                           background=\"\\n        button_tooltip_text = (\\n            \"Double-click to expand, right-click for more options.\"\\n        )\\n        Hovertip(self, button_tooltip_text, hover_delay=80)\\n        self.bind(\"<Double-Button-1>\", self.expand)\\n        if macosx.isAquaTk():\\n            self.bind(\"<Button-2>\", self.context_menu_event)\\n        else:\\n            self.bind(\"<Button-3>\", self.context_menu_event)\\n        self.selection_handle(  \\n            lambda offset, length: s[int(offset):int(offset) + int(length)])\\n        self.is_dangerous = None\\n        self.after_idle(self.set_is_dangerous)"
  },
  {
    "code": "def __getitem__(self, key):\\n        return self.getGroup(self.primary.indices[key])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: tests pass except for multi-column. renamed self.klass to DataFrame in test_frame.py",
    "fixed_code": "def __getitem__(self, key):\\n        if key not in self.obj:\\n            raise KeyError('column %s not found' % key)\\n        return DataFrameGroupBy(self.obj, groupings=self.groupings,\\n                                column=key)"
  },
  {
    "code": "def __rsub__(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        return other.__sub__(self, context=context)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __rsub__(self, other, context=None):\\n        other = _convert_other(other)\\n        if other is NotImplemented:\\n            return other\\n        return other.__sub__(self, context=context)"
  },
  {
    "code": "def maybe_downcast_to_dtype(\\n    result: ArrayLike, dtype: Union[str, np.dtype]\\n) -> ArrayLike:\\n    if isinstance(result, ABCDataFrame):\\n        return result\\n    do_round = False\\n    if isinstance(dtype, str):\\n        if dtype == \"infer\":\\n            inferred_type = lib.infer_dtype(ensure_object(result), skipna=False)\\n            if inferred_type == \"boolean\":\\n                dtype = \"bool\"\\n            elif inferred_type == \"integer\":\\n                dtype = \"int64\"\\n            elif inferred_type == \"datetime64\":\\n                dtype = \"datetime64[ns]\"\\n            elif inferred_type == \"timedelta64\":\\n                dtype = \"timedelta64[ns]\"\\n            elif inferred_type == \"floating\":\\n                dtype = \"int64\"\\n                if issubclass(result.dtype.type, np.number):\\n                    do_round = True\\n            else:\\n                dtype = \"object\"\\n        dtype = np.dtype(dtype)\\n    elif dtype.type is Period:\\n        from pandas.core.arrays import PeriodArray\\n        with suppress(TypeError):\\n            return PeriodArray(result, freq=dtype.freq)\\n    converted = maybe_downcast_numeric(result, dtype, do_round)\\n    if converted is not result:\\n        return converted\\n    if dtype.kind in [\"M\", \"m\"] and result.dtype.kind in [\"i\", \"f\"]:\\n        if isinstance(dtype, DatetimeTZDtype):\\n            i8values = result.astype(\"i8\", copy=False)\\n            cls = dtype.construct_array_type()\\n            result = cls._simple_new(i8values, dtype=dtype)\\n        else:\\n            result = result.astype(dtype)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def maybe_downcast_to_dtype(\\n    result: ArrayLike, dtype: Union[str, np.dtype]\\n) -> ArrayLike:\\n    if isinstance(result, ABCDataFrame):\\n        return result\\n    do_round = False\\n    if isinstance(dtype, str):\\n        if dtype == \"infer\":\\n            inferred_type = lib.infer_dtype(ensure_object(result), skipna=False)\\n            if inferred_type == \"boolean\":\\n                dtype = \"bool\"\\n            elif inferred_type == \"integer\":\\n                dtype = \"int64\"\\n            elif inferred_type == \"datetime64\":\\n                dtype = \"datetime64[ns]\"\\n            elif inferred_type == \"timedelta64\":\\n                dtype = \"timedelta64[ns]\"\\n            elif inferred_type == \"floating\":\\n                dtype = \"int64\"\\n                if issubclass(result.dtype.type, np.number):\\n                    do_round = True\\n            else:\\n                dtype = \"object\"\\n        dtype = np.dtype(dtype)\\n    elif dtype.type is Period:\\n        from pandas.core.arrays import PeriodArray\\n        with suppress(TypeError):\\n            return PeriodArray(result, freq=dtype.freq)\\n    converted = maybe_downcast_numeric(result, dtype, do_round)\\n    if converted is not result:\\n        return converted\\n    if dtype.kind in [\"M\", \"m\"] and result.dtype.kind in [\"i\", \"f\"]:\\n        if isinstance(dtype, DatetimeTZDtype):\\n            i8values = result.astype(\"i8\", copy=False)\\n            cls = dtype.construct_array_type()\\n            result = cls._simple_new(i8values, dtype=dtype)\\n        else:\\n            result = result.astype(dtype)\\n    return result"
  },
  {
    "code": "class ContextBase(common_context.RequestContext):\\n\\tdef __init__(self, user_id, tenant_id, is_admin=None, read_deleted=\"no\",\\n\\t\\t\\t\\t roles=None, timestamp=None, **kwargs):\\n\\t\\tif kwargs:\\n\\t\\t\\tLOG.warn(_('Arguments dropped when creating '\\n\\t\\t\\t\\t\\t   'context: %s'), kwargs)\\n\\t\\tsuper(ContextBase, self).__init__(user=user_id, tenant=tenant_id,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  is_admin=is_admin)\\n\\t\\tself.read_deleted = read_deleted\\n\\t\\tif not timestamp:\\n\\t\\t\\ttimestamp = datetime.utcnow()\\n\\t\\tself.timestamp = timestamp\\n\\t\\tself._session = None\\n\\t\\tself.roles = roles or []\\n\\t\\tif self.is_admin is None:\\n\\t\\t\\tself.is_admin = policy.check_is_admin(self)\\n\\t\\telif self.is_admin:\\n\\t\\t\\tadmin_roles = policy.get_admin_roles()\\n\\t\\t\\tif admin_roles:\\n\\t\\t\\t\\tself.roles = list(set(self.roles) | set(admin_roles))\\n\\t@property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Allow for skipping admin roles loading on context creation\\n\\nBug 1216866\\n\\nThere are cases in which an admin context is created only to grab a db\\nsession and ensure no tenant filters are applied in _model_query. In\\nthese cases evaluating the policy engine for grabbing admin roles is not\\nnecessary, and can cause unexpected and serious issues if the context is\\ngrabbed before all the extensions are loaded.",
    "fixed_code": "class ContextBase(common_context.RequestContext):\\n\\tdef __init__(self, user_id, tenant_id, is_admin=None, read_deleted=\"no\",\\n\\t\\t\\t\\t roles=None, timestamp=None, load_admin_roles=True, **kwargs):\\n\\t\\tif kwargs:\\n\\t\\t\\tLOG.warn(_('Arguments dropped when creating '\\n\\t\\t\\t\\t\\t   'context: %s'), kwargs)\\n\\t\\tsuper(ContextBase, self).__init__(user=user_id, tenant=tenant_id,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  is_admin=is_admin)\\n\\t\\tself.read_deleted = read_deleted\\n\\t\\tif not timestamp:\\n\\t\\t\\ttimestamp = datetime.utcnow()\\n\\t\\tself.timestamp = timestamp\\n\\t\\tself._session = None\\n\\t\\tself.roles = roles or []\\n\\t\\tif self.is_admin is None:\\n\\t\\t\\tself.is_admin = policy.check_is_admin(self)\\n\\t\\telif self.is_admin and load_admin_roles:\\n\\t\\t\\tadmin_roles = policy.get_admin_roles()\\n\\t\\t\\tif admin_roles:\\n\\t\\t\\t\\tself.roles = list(set(self.roles) | set(admin_roles))\\n\\t@property"
  },
  {
    "code": "def _unstack_multiple(data, clocs, fill_value=None):\\n\\tif len(clocs) == 0:\\n\\t\\treturn data\\n\\tindex = data.index\\n\\tif clocs in index.names:\\n\\t\\tclocs = [clocs]\\n\\tclocs = [index._get_level_number(i) for i in clocs]\\n\\trlocs = [i for i in range(index.nlevels) if i not in clocs]\\n\\tclevels = [index.levels[i] for i in clocs]\\n\\tccodes = [index.codes[i] for i in clocs]\\n\\tcnames = [index.names[i] for i in clocs]\\n\\trlevels = [index.levels[i] for i in rlocs]\\n\\trcodes = [index.codes[i] for i in rlocs]\\n\\trnames = [index.names[i] for i in rlocs]\\n\\tshape = [len(x) for x in clevels]\\n\\tgroup_index = get_group_index(ccodes, shape, sort=False, xnull=False)\\n\\tcomp_ids, obs_ids = compress_group_index(group_index, sort=False)\\n\\trecons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\\n\\tif not rlocs:\\n\\t\\tdummy_index = Index(obs_ids, name=\"__placeholder__\")\\n\\telse:\\n\\t\\tdummy_index = MultiIndex(\\n\\t\\t\\tlevels=rlevels + [obs_ids],\\n\\t\\t\\tcodes=rcodes + [comp_ids],\\n\\t\\t\\tnames=rnames + [\"__placeholder__\"],\\n\\t\\t\\tverify_integrity=False,\\n\\t\\t)\\n\\tif isinstance(data, Series):\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tnew_levels = clevels\\n\\t\\tnew_names = cnames\\n\\t\\tnew_codes = recons_codes\\n\\telse:\\n\\t\\tif isinstance(data.columns, MultiIndex):\\n\\t\\t\\tresult = data\\n\\t\\t\\tfor i in range(len(clocs)):\\n\\t\\t\\t\\tval = clocs[i]\\n\\t\\t\\t\\tresult = result.unstack(val, fill_value=fill_value)\\n\\t\\t\\t\\tclocs = [v if v < val else v - 1 for v in clocs]\\n\\t\\t\\treturn result\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tif isinstance(unstacked, Series):\\n\\t\\t\\tunstcols = unstacked.index\\n\\t\\telse:\\n\\t\\t\\tunstcols = unstacked.columns\\n\\t\\tassert isinstance(unstcols, MultiIndex)  \\n\\t\\tnew_levels = [unstcols.levels[0]] + clevels\\n\\t\\tnew_names = [data.columns.name] + cnames\\n\\t\\tnew_codes = [unstcols.codes[0]]\\n\\t\\tfor rec in recons_codes:\\n\\t\\t\\tnew_codes.append(rec.take(unstcols.codes[-1]))\\n\\tnew_columns = MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)\\n\\tif isinstance(unstacked, Series):\\n\\t\\tunstacked.index = new_columns\\n\\telse:\\n\\t\\tunstacked.columns = new_columns\\n\\treturn unstacked",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _unstack_multiple(data, clocs, fill_value=None):\\n\\tif len(clocs) == 0:\\n\\t\\treturn data\\n\\tindex = data.index\\n\\tif clocs in index.names:\\n\\t\\tclocs = [clocs]\\n\\tclocs = [index._get_level_number(i) for i in clocs]\\n\\trlocs = [i for i in range(index.nlevels) if i not in clocs]\\n\\tclevels = [index.levels[i] for i in clocs]\\n\\tccodes = [index.codes[i] for i in clocs]\\n\\tcnames = [index.names[i] for i in clocs]\\n\\trlevels = [index.levels[i] for i in rlocs]\\n\\trcodes = [index.codes[i] for i in rlocs]\\n\\trnames = [index.names[i] for i in rlocs]\\n\\tshape = [len(x) for x in clevels]\\n\\tgroup_index = get_group_index(ccodes, shape, sort=False, xnull=False)\\n\\tcomp_ids, obs_ids = compress_group_index(group_index, sort=False)\\n\\trecons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\\n\\tif not rlocs:\\n\\t\\tdummy_index = Index(obs_ids, name=\"__placeholder__\")\\n\\telse:\\n\\t\\tdummy_index = MultiIndex(\\n\\t\\t\\tlevels=rlevels + [obs_ids],\\n\\t\\t\\tcodes=rcodes + [comp_ids],\\n\\t\\t\\tnames=rnames + [\"__placeholder__\"],\\n\\t\\t\\tverify_integrity=False,\\n\\t\\t)\\n\\tif isinstance(data, Series):\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tnew_levels = clevels\\n\\t\\tnew_names = cnames\\n\\t\\tnew_codes = recons_codes\\n\\telse:\\n\\t\\tif isinstance(data.columns, MultiIndex):\\n\\t\\t\\tresult = data\\n\\t\\t\\tfor i in range(len(clocs)):\\n\\t\\t\\t\\tval = clocs[i]\\n\\t\\t\\t\\tresult = result.unstack(val, fill_value=fill_value)\\n\\t\\t\\t\\tclocs = [v if v < val else v - 1 for v in clocs]\\n\\t\\t\\treturn result\\n\\t\\tdummy = data.copy()\\n\\t\\tdummy.index = dummy_index\\n\\t\\tunstacked = dummy.unstack(\"__placeholder__\", fill_value=fill_value)\\n\\t\\tif isinstance(unstacked, Series):\\n\\t\\t\\tunstcols = unstacked.index\\n\\t\\telse:\\n\\t\\t\\tunstcols = unstacked.columns\\n\\t\\tassert isinstance(unstcols, MultiIndex)  \\n\\t\\tnew_levels = [unstcols.levels[0]] + clevels\\n\\t\\tnew_names = [data.columns.name] + cnames\\n\\t\\tnew_codes = [unstcols.codes[0]]\\n\\t\\tfor rec in recons_codes:\\n\\t\\t\\tnew_codes.append(rec.take(unstcols.codes[-1]))\\n\\tnew_columns = MultiIndex(\\n\\t\\tlevels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\\n\\t)\\n\\tif isinstance(unstacked, Series):\\n\\t\\tunstacked.index = new_columns\\n\\telse:\\n\\t\\tunstacked.columns = new_columns\\n\\treturn unstacked"
  },
  {
    "code": "def leaf_nodes(self, app=None):\\n        leaves = set()\\n        for node in self.nodes:\\n            if not any(key[0] == node[0] for key in self.dependents.get(node, set())) and (not app or app == node[0]):\\n                leaves.add(node)\\n        return leaves",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #22861: Internal migrations done first so __first__ works\\n\\nThanks to Chris Beaven.",
    "fixed_code": "def leaf_nodes(self, app=None):\\n        leaves = set()\\n        for node in self.nodes:\\n            if not any(key[0] == node[0] for key in self.dependents.get(node, set())) and (not app or app == node[0]):\\n                leaves.add(node)\\n        return sorted(leaves)"
  },
  {
    "code": "def __init__(self, theta=1.0, thetaL=1e-5, thetaU=np.inf):\\n        if not np.iterable(theta):\\n            theta = np.array([theta])\\n        self.params = np.asarray(theta, dtype=np.float)\\n        self.bounds = (np.asarray(thetaL, dtype=np.float),\\n                       np.asarray(thetaU, dtype=np.float))\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, theta=1.0, thetaL=1e-5, thetaU=np.inf):\\n        if not np.iterable(theta):\\n            theta = np.array([theta])\\n        self.params = np.asarray(theta, dtype=np.float)\\n        self.bounds = (np.asarray(thetaL, dtype=np.float),\\n                       np.asarray(thetaU, dtype=np.float))\\n    @property"
  },
  {
    "code": "def _get_cythonized_result(\\n\\t\\tself,\\n\\t\\thow,\\n\\t\\tgrouper,\\n\\t\\taggregate=False,\\n\\t\\tcython_dtype=None,\\n\\t\\tneeds_values=False,\\n\\t\\tneeds_mask=False,\\n\\t\\tneeds_ngroups=False,\\n\\t\\tresult_is_index=False,\\n\\t\\tpre_processing=None,\\n\\t\\tpost_processing=None,\\n\\t\\t**kwargs\\n\\t):\\n\\t\\tif result_is_index and aggregate:\\n\\t\\t\\traise ValueError(\"'result_is_index' and 'aggregate' cannot both be True!\")\\n\\t\\tif post_processing:\\n\\t\\t\\tif not callable(pre_processing):\\n\\t\\t\\t\\traise ValueError(\"'post_processing' must be a callable!\")\\n\\t\\tif pre_processing:\\n\\t\\t\\tif not callable(pre_processing):\\n\\t\\t\\t\\traise ValueError(\"'pre_processing' must be a callable!\")\\n\\t\\t\\tif not needs_values:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\"Cannot use 'pre_processing' without specifying 'needs_values'!\"\\n\\t\\t\\t\\t)\\n\\t\\tlabels, _, ngroups = grouper.group_info\\n\\t\\toutput = collections.OrderedDict()\\n\\t\\tbase_func = getattr(libgroupby, how)\\n\\t\\tfor name, obj in self._iterate_slices():\\n\\t\\t\\tvalues = obj._data._values\\n\\t\\t\\tif aggregate:\\n\\t\\t\\t\\tresult_sz = ngroups\\n\\t\\t\\telse:\\n\\t\\t\\t\\tresult_sz = len(values)\\n\\t\\t\\tif not cython_dtype:\\n\\t\\t\\t\\tcython_dtype = values.dtype\\n\\t\\t\\tresult = np.zeros(result_sz, dtype=cython_dtype)\\n\\t\\t\\tfunc = partial(base_func, result, labels)\\n\\t\\t\\tinferences = None\\n\\t\\t\\tif needs_values:\\n\\t\\t\\t\\tvals = values\\n\\t\\t\\t\\tif pre_processing:\\n\\t\\t\\t\\t\\tvals, inferences = pre_processing(vals)\\n\\t\\t\\t\\tfunc = partial(func, vals)\\n\\t\\t\\tif needs_mask:\\n\\t\\t\\t\\tmask = isna(values).view(np.uint8)\\n\\t\\t\\t\\tfunc = partial(func, mask)\\n\\t\\t\\tif needs_ngroups:\\n\\t\\t\\t\\tfunc = partial(func, ngroups)\\n\\t\\t\\tfunc(**kwargs)  \\n\\t\\t\\tif result_is_index:\\n\\t\\t\\t\\tresult = algorithms.take_nd(values, result)\\n\\t\\t\\tif post_processing:\\n\\t\\t\\t\\tresult = post_processing(result, inferences)\\n\\t\\t\\toutput[name] = result\\n\\t\\tif aggregate:\\n\\t\\t\\treturn self._wrap_aggregated_output(output)\\n\\t\\telse:\\n\\t\\t\\treturn self._wrap_transformed_output(output)\\n\\t@Substitution(name=\"groupby\")\\n\\t@Appender(_common_see_also)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_cythonized_result(\\n\\t\\tself,\\n\\t\\thow,\\n\\t\\tgrouper,\\n\\t\\taggregate=False,\\n\\t\\tcython_dtype=None,\\n\\t\\tneeds_values=False,\\n\\t\\tneeds_mask=False,\\n\\t\\tneeds_ngroups=False,\\n\\t\\tresult_is_index=False,\\n\\t\\tpre_processing=None,\\n\\t\\tpost_processing=None,\\n\\t\\t**kwargs\\n\\t):\\n\\t\\tif result_is_index and aggregate:\\n\\t\\t\\traise ValueError(\"'result_is_index' and 'aggregate' cannot both be True!\")\\n\\t\\tif post_processing:\\n\\t\\t\\tif not callable(pre_processing):\\n\\t\\t\\t\\traise ValueError(\"'post_processing' must be a callable!\")\\n\\t\\tif pre_processing:\\n\\t\\t\\tif not callable(pre_processing):\\n\\t\\t\\t\\traise ValueError(\"'pre_processing' must be a callable!\")\\n\\t\\t\\tif not needs_values:\\n\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\"Cannot use 'pre_processing' without specifying 'needs_values'!\"\\n\\t\\t\\t\\t)\\n\\t\\tlabels, _, ngroups = grouper.group_info\\n\\t\\toutput = collections.OrderedDict()\\n\\t\\tbase_func = getattr(libgroupby, how)\\n\\t\\tfor name, obj in self._iterate_slices():\\n\\t\\t\\tvalues = obj._data._values\\n\\t\\t\\tif aggregate:\\n\\t\\t\\t\\tresult_sz = ngroups\\n\\t\\t\\telse:\\n\\t\\t\\t\\tresult_sz = len(values)\\n\\t\\t\\tif not cython_dtype:\\n\\t\\t\\t\\tcython_dtype = values.dtype\\n\\t\\t\\tresult = np.zeros(result_sz, dtype=cython_dtype)\\n\\t\\t\\tfunc = partial(base_func, result, labels)\\n\\t\\t\\tinferences = None\\n\\t\\t\\tif needs_values:\\n\\t\\t\\t\\tvals = values\\n\\t\\t\\t\\tif pre_processing:\\n\\t\\t\\t\\t\\tvals, inferences = pre_processing(vals)\\n\\t\\t\\t\\tfunc = partial(func, vals)\\n\\t\\t\\tif needs_mask:\\n\\t\\t\\t\\tmask = isna(values).view(np.uint8)\\n\\t\\t\\t\\tfunc = partial(func, mask)\\n\\t\\t\\tif needs_ngroups:\\n\\t\\t\\t\\tfunc = partial(func, ngroups)\\n\\t\\t\\tfunc(**kwargs)  \\n\\t\\t\\tif result_is_index:\\n\\t\\t\\t\\tresult = algorithms.take_nd(values, result)\\n\\t\\t\\tif post_processing:\\n\\t\\t\\t\\tresult = post_processing(result, inferences)\\n\\t\\t\\toutput[name] = result\\n\\t\\tif aggregate:\\n\\t\\t\\treturn self._wrap_aggregated_output(output)\\n\\t\\telse:\\n\\t\\t\\treturn self._wrap_transformed_output(output)\\n\\t@Substitution(name=\"groupby\")\\n\\t@Appender(_common_see_also)"
  },
  {
    "code": "def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\\n            *, cafile=None, capath=None, cadefault=False, context=None):\\n    global _opener\\n    if cafile or capath or cadefault:\\n        if context is not None:\\n            raise ValueError(\\n                \"You can't pass both context and any of cafile, capath, and \"\\n                \"cadefault\"\\n            )\\n        if not _have_ssl:\\n            raise ValueError('SSL support not available')\\n        context = ssl._create_stdlib_context(cert_reqs=ssl.CERT_REQUIRED,\\n                                             cafile=cafile,\\n                                             capath=capath)\\n        https_handler = HTTPSHandler(context=context, check_hostname=True)\\n        opener = build_opener(https_handler)\\n    elif context:\\n        https_handler = HTTPSHandler(context=context)\\n        opener = build_opener(https_handler)\\n    elif _opener is None:\\n        _opener = opener = build_opener()\\n    else:\\n        opener = _opener\\n    return opener.open(url, data, timeout)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\\n            *, cafile=None, capath=None, cadefault=False, context=None):\\n    global _opener\\n    if cafile or capath or cadefault:\\n        if context is not None:\\n            raise ValueError(\\n                \"You can't pass both context and any of cafile, capath, and \"\\n                \"cadefault\"\\n            )\\n        if not _have_ssl:\\n            raise ValueError('SSL support not available')\\n        context = ssl._create_stdlib_context(cert_reqs=ssl.CERT_REQUIRED,\\n                                             cafile=cafile,\\n                                             capath=capath)\\n        https_handler = HTTPSHandler(context=context, check_hostname=True)\\n        opener = build_opener(https_handler)\\n    elif context:\\n        https_handler = HTTPSHandler(context=context)\\n        opener = build_opener(https_handler)\\n    elif _opener is None:\\n        _opener = opener = build_opener()\\n    else:\\n        opener = _opener\\n    return opener.open(url, data, timeout)"
  },
  {
    "code": "def asof(self, label):\\n        if label not in self.indexMap:\\n            loc = self.searchsorted(label, side='left')\\n            if loc > 0:\\n                return self[loc-1]\\n            else:\\n                return np.nan\\n        return label",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: fixed index unit test",
    "fixed_code": "def asof(self, label):\\n        if label not in self.indexMap:\\n            loc = self.searchsorted(label, side='left')\\n            if loc > 0:\\n                return self[loc-1]\\n            else:\\n                return np.nan\\n        return label"
  },
  {
    "code": "def cluster_image_get_iter(self):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get-iter')\\n        query = netapp_utils.zapi.NaElement('query')\\n        cluster_image_info = netapp_utils.zapi.NaElement('cluster-image-info')\\n        query.add_child_elem(cluster_image_info)\\n        cluster_image_get.add_child_elem(query)\\n        return cluster_image_get\\n    def cluster_image_get(self):\\n        cluster_image_get_iter = self.cluster_image_get_iter()\\n        try:\\n            result = self.server.invoke_successfully(cluster_image_get_iter, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details: %s: %s'\\n                                      % (self.parameters['package_version'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        if result.get_child_by_name('num-records') and \\\\n                int(result.get_child_content('num-records')) > 0:\\n            return True\\n        return None\\n    def cluster_image_get_for_node(self, node_name):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get')\\n        cluster_image_get.add_new_child('node-id', node_name)\\n        try:\\n            self.server.invoke_successfully(cluster_image_get, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details for %s: %s'\\n                                      % (node_name, to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cluster_image_get_iter(self):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get-iter')\\n        query = netapp_utils.zapi.NaElement('query')\\n        cluster_image_info = netapp_utils.zapi.NaElement('cluster-image-info')\\n        query.add_child_elem(cluster_image_info)\\n        cluster_image_get.add_child_elem(query)\\n        return cluster_image_get\\n    def cluster_image_get(self):\\n        cluster_image_get_iter = self.cluster_image_get_iter()\\n        try:\\n            result = self.server.invoke_successfully(cluster_image_get_iter, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details: %s: %s'\\n                                      % (self.parameters['package_version'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        if result.get_child_by_name('num-records') and \\\\n                int(result.get_child_content('num-records')) > 0:\\n            return True\\n        return None\\n    def cluster_image_get_for_node(self, node_name):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get')\\n        cluster_image_get.add_new_child('node-id', node_name)\\n        try:\\n            self.server.invoke_successfully(cluster_image_get, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details for %s: %s'\\n                                      % (node_name, to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def delete_cookie(self, key, path='/', domain=None):\\n\\t\\tself.set_cookie(key, max_age=0, path=path, domain=domain,\\n\\t\\t\\t\\t\\t\\texpires='Thu, 01 Jan 1970 00:00:00 GMT')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def delete_cookie(self, key, path='/', domain=None):\\n\\t\\tself.set_cookie(key, max_age=0, path=path, domain=domain,\\n\\t\\t\\t\\t\\t\\texpires='Thu, 01 Jan 1970 00:00:00 GMT')"
  },
  {
    "code": "def test_urlfield_10(self):\\n\\t\\tf = URLField(verify_exists=True)\\n\\t\\turl = u'http://t\\xfcr.djangoproject.com/'\\n\\t\\tself.assertEqual(url, f.clean(url))",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Altered the behavior of URLField to avoid a potential DOS vector, and to avoid potential leakage of local filesystem data. A security announcement will be made shortly.",
    "fixed_code": "def test_urlfield_10(self):\\n\\t\\tf = URLField(verify_exists=True)\\n\\t\\turl = u'http://\\u03b5\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac.idn.icann.org/\\u0391\\u03c1\\u03c7\\u03b9\\u03ba\\u03ae_\\u03c3\\u03b5\\u03bb\\u03af\\u03b4\\u03b1'\\n\\t\\tself.assertEqual(url, f.clean(url))"
  },
  {
    "code": "def _get_field(self, field_name: str):\\n        backcompat_prefix = \"extra__grpc__\"\\n        if field_name.startswith('extra_'):\\n            raise ValueError(\\n                f\"Got prefixed name {field_name}; please remove the '{backcompat_prefix}' prefix \"\\n                \"when using this method.\"\\n            )\\n        if field_name in self.extras:\\n            return self.extras[field_name]\\n        prefixed_name = f\"{backcompat_prefix}{field_name}\"\\n        if prefixed_name in self.extras:\\n            return self.extras[prefixed_name]\\n        raise KeyError(f\"Param {field_name} not found in extra dict\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_field(self, field_name: str):\\n        backcompat_prefix = \"extra__grpc__\"\\n        if field_name.startswith('extra_'):\\n            raise ValueError(\\n                f\"Got prefixed name {field_name}; please remove the '{backcompat_prefix}' prefix \"\\n                \"when using this method.\"\\n            )\\n        if field_name in self.extras:\\n            return self.extras[field_name]\\n        prefixed_name = f\"{backcompat_prefix}{field_name}\"\\n        if prefixed_name in self.extras:\\n            return self.extras[prefixed_name]\\n        raise KeyError(f\"Param {field_name} not found in extra dict\")"
  },
  {
    "code": "def abspath(path):\\n        if not isabs(path):\\n            path = join(os.getcwd(), path)\\n        return normpath(path)\\nelse:  \\n    def abspath(path):\\n        if path: \\n            try:\\n                path = _getfullpathname(path)\\n            except WindowsError:\\n                pass \\n        else:\\n            path = os.getcwd()\\n        return normpath(path)\\nrealpath = abspath\\nsupports_unicode_filenames = (hasattr(sys, \"getwindowsversion\") and\\n                              sys.getwindowsversion()[3] >= 2)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def abspath(path):\\n        if not isabs(path):\\n            path = join(os.getcwd(), path)\\n        return normpath(path)\\nelse:  \\n    def abspath(path):\\n        if path: \\n            try:\\n                path = _getfullpathname(path)\\n            except WindowsError:\\n                pass \\n        else:\\n            path = os.getcwd()\\n        return normpath(path)\\nrealpath = abspath\\nsupports_unicode_filenames = (hasattr(sys, \"getwindowsversion\") and\\n                              sys.getwindowsversion()[3] >= 2)"
  },
  {
    "code": "def _get_defaults(func):\\n    try:\\n        code = func.__code__\\n    except AttributeError:\\n        return {}\\n    pos_count = code.co_argcount\\n    arg_names = code.co_varnames\\n    arg_names = arg_names[:pos_count]\\n    defaults = func.__defaults__ or ()\\n    kwdefaults = func.__kwdefaults__\\n    res = dict(kwdefaults) if kwdefaults else {}\\n    pos_offset = pos_count - len(defaults)\\n    for name, value in zip(arg_names[pos_offset:], defaults):\\n        assert name not in res\\n        res[name] = value\\n    return res",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_defaults(func):\\n    try:\\n        code = func.__code__\\n    except AttributeError:\\n        return {}\\n    pos_count = code.co_argcount\\n    arg_names = code.co_varnames\\n    arg_names = arg_names[:pos_count]\\n    defaults = func.__defaults__ or ()\\n    kwdefaults = func.__kwdefaults__\\n    res = dict(kwdefaults) if kwdefaults else {}\\n    pos_offset = pos_count - len(defaults)\\n    for name, value in zip(arg_names[pos_offset:], defaults):\\n        assert name not in res\\n        res[name] = value\\n    return res"
  },
  {
    "code": "def __unicode__(self):\\n        width, height = get_terminal_size()\\n        max_rows = (height if get_option(\"display.max_rows\") == 0\\n                    else get_option(\"display.max_rows\"))\\n        if len(self.labels) > (max_rows or 1000):\\n            result = self._tidy_repr(min(30, max_rows) - 4)\\n        elif len(self.labels) > 0:\\n            result = self._get_repr(length=len(self) > 50,\\n                                    name=True)\\n        else:\\n            result = u'Categorical([], %s' % self._get_repr(name=True,\\n                                                            length=False,\\n                                                            footer=True,\\n                                                            )\\n        return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __unicode__(self):\\n        width, height = get_terminal_size()\\n        max_rows = (height if get_option(\"display.max_rows\") == 0\\n                    else get_option(\"display.max_rows\"))\\n        if len(self.labels) > (max_rows or 1000):\\n            result = self._tidy_repr(min(30, max_rows) - 4)\\n        elif len(self.labels) > 0:\\n            result = self._get_repr(length=len(self) > 50,\\n                                    name=True)\\n        else:\\n            result = u'Categorical([], %s' % self._get_repr(name=True,\\n                                                            length=False,\\n                                                            footer=True,\\n                                                            )\\n        return result"
  },
  {
    "code": "def __init__(self, charset='utf-8', min_n=1, max_n=1,\\n                 preprocessor=DEFAULT_PREPROCESSOR,\\n                 stop_words=\"english\",\\n                 token_pattern=DEFAULT_TOKEN_PATTERN,\\n                 charset_error='strict'):\\n        self.charset = charset\\n        self.stop_words = _check_stop_list(stop_words)\\n        self.min_n = min_n\\n        self.max_n = max_n\\n        self.preprocessor = preprocessor\\n        self.token_pattern = token_pattern\\n        self.charset_error = charset_error",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Flatten the feature extraction API",
    "fixed_code": "def __init__(self, input='content', charset='utf-8',\\n                 charset_error='strict', strip_accents='ascii',\\n                 strip_tags=False, lowercase=True, tokenize='word',\\n                 stop_words=None, token_pattern=ur\"\\b\\w\\w+\\b\",\\n                 min_n=1, max_n=1, max_df=1.0, max_features=None,\\n                 fixed_vocabulary=None, dtype=long):\\n        self.input = input\\n        self.charset = charset\\n        self.charset_error = charset_error\\n        self.strip_accents = strip_accents\\n        self.strip_tags = strip_tags\\n        self.lowercase = lowercase\\n        self.min_n = min_n\\n        self.max_n = max_n\\n        self.tokenize = tokenize\\n        self.token_pattern = token_pattern\\n        self.stop_words = stop_words\\n        self.max_df = max_df\\n        self.max_features = max_features\\n        if (fixed_vocabulary is not None\\n            and not hasattr(fixed_vocabulary, 'get')):\\n            fixed_vocabulary = dict(\\n                (t, i) for i, t in enumerate(fixed_vocabulary))\\n        self.fixed_vocabulary = fixed_vocabulary\\n        self.dtype = dtype"
  },
  {
    "code": "def process_response(self, request, response, spider):\\n\\t\\tif (\\n\\t\\t\\trequest.meta.get('dont_redirect', False)\\n\\t\\t\\tor response.status in getattr(spider, 'handle_httpstatus_list', [])\\n\\t\\t\\tor response.status in request.meta.get('handle_httpstatus_list', [])\\n\\t\\t\\tor request.meta.get('handle_httpstatus_all', False)\\n\\t\\t):\\n\\t\\t\\treturn response\\n\\t\\tallowed_status = (301, 302, 303, 307, 308)\\n\\t\\tif 'Location' not in response.headers or response.status not in allowed_status:\\n\\t\\t\\treturn response\\n\\t\\tlocation = safe_url_string(response.headers['Location'])\\n\\t\\tif response.headers['Location'].startswith(b'//'):\\n\\t\\t\\trequest_scheme = urlparse(request.url).scheme\\n\\t\\t\\tlocation = request_scheme + '://' + location.lstrip('/')\\n\\t\\tredirected_url = urljoin(request.url, location)\\n\\t\\tif response.status in (301, 307, 308) or request.method == 'HEAD':\\n\\t\\t\\tredirected = _build_redirect_request(request, url=redirected_url)\\n\\t\\t\\treturn self._redirect(redirected, request, spider, response.status)\\n\\t\\tredirected = self._redirect_request_using_get(request, redirected_url)\\n\\t\\treturn self._redirect(redirected, request, spider, response.status)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_response(self, request, response, spider):\\n\\t\\tif (\\n\\t\\t\\trequest.meta.get('dont_redirect', False)\\n\\t\\t\\tor response.status in getattr(spider, 'handle_httpstatus_list', [])\\n\\t\\t\\tor response.status in request.meta.get('handle_httpstatus_list', [])\\n\\t\\t\\tor request.meta.get('handle_httpstatus_all', False)\\n\\t\\t):\\n\\t\\t\\treturn response\\n\\t\\tallowed_status = (301, 302, 303, 307, 308)\\n\\t\\tif 'Location' not in response.headers or response.status not in allowed_status:\\n\\t\\t\\treturn response\\n\\t\\tlocation = safe_url_string(response.headers['Location'])\\n\\t\\tif response.headers['Location'].startswith(b'//'):\\n\\t\\t\\trequest_scheme = urlparse(request.url).scheme\\n\\t\\t\\tlocation = request_scheme + '://' + location.lstrip('/')\\n\\t\\tredirected_url = urljoin(request.url, location)\\n\\t\\tif response.status in (301, 307, 308) or request.method == 'HEAD':\\n\\t\\t\\tredirected = _build_redirect_request(request, url=redirected_url)\\n\\t\\t\\treturn self._redirect(redirected, request, spider, response.status)\\n\\t\\tredirected = self._redirect_request_using_get(request, redirected_url)\\n\\t\\treturn self._redirect(redirected, request, spider, response.status)"
  },
  {
    "code": "def __add__(self, other):\\n        if isinstance(other, UserList):\\n            return self.__class__(self.data + other.data)\\n        elif isinstance(other, type(self.data)):\\n            return self.__class__(self.data + other)\\n        return self.__class__(self.data + list(other))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Forward port r68792 and r68789 putting Counter in __all__ and adding Counter buildouts.",
    "fixed_code": "def __add__(self, other):\\n        abbb') + Counter('bcc')\\n        Counter({'b': 4, 'c': 2, 'a': 1})\\n        '''\\n        if not isinstance(other, Counter):\\n            return NotImplemented\\n        result = Counter()\\n        for elem in set(self) | set(other):\\n            newcount = self[elem] + other[elem]\\n            if newcount > 0:\\n                result[elem] = newcount\\n        return result"
  },
  {
    "code": "def execute_show_command(command, module, text=False):\\n    if text is False:\\n        command += ' | json'\\n    cmds = [command]\\n    return run_commands(module, cmds)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix structured output not supported in nxos_pim_interface (#28125)",
    "fixed_code": "def execute_show_command(command, module, text=False):\\n    if text:\\n        cmds = [{\\n            'command': command,\\n            'output': 'text'\\n        }]\\n    else:\\n        cmds = [{\\n            'command': command,\\n            'output': 'json'\\n        }]\\n    return run_commands(module, cmds)"
  },
  {
    "code": "def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,\\n                          scoring=None, fit_intercept=False,\\n                          max_iter=100, tol=1e-4, class_weight=None,\\n                          verbose=0, solver='lbfgs', penalty='l2',\\n                          dual=False, intercept_scaling=1.,\\n                          multi_class='auto', random_state=None,\\n                          max_squared_sum=None, sample_weight=None,\\n                          l1_ratio=None):\\n    X_train = X[train]\\n    X_test = X[test]\\n    y_train = y[train]\\n    y_test = y[test]\\n    if sample_weight is not None:\\n        sample_weight = check_array(sample_weight, ensure_2d=False)\\n        check_consistent_length(y, sample_weight)\\n        sample_weight = sample_weight[train]\\n    coefs, Cs, n_iter = _logistic_regression_path(\\n        X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,\\n        fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,\\n        class_weight=class_weight, pos_class=pos_class,\\n        multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,\\n        penalty=penalty, intercept_scaling=intercept_scaling,\\n        random_state=random_state, check_input=False,\\n        max_squared_sum=max_squared_sum, sample_weight=sample_weight)\\n    log_reg = LogisticRegression(solver=solver, multi_class=multi_class)\\n    if multi_class == 'ovr':\\n        log_reg.classes_ = np.array([-1, 1])\\n    elif multi_class == 'multinomial':\\n        log_reg.classes_ = np.unique(y_train)\\n    else:\\n        raise ValueError(\"multi_class should be either multinomial or ovr, \"\\n                         \"got %d\" % multi_class)\\n    if pos_class is not None:\\n        mask = (y_test == pos_class)\\n        y_test = np.ones(y_test.shape, dtype=np.float64)\\n        y_test[~mask] = -1.\\n    scores = list()\\n    if isinstance(scoring, str):\\n        scoring = get_scorer(scoring)\\n    for w in coefs:\\n        if multi_class == 'ovr':\\n            w = w[np.newaxis, :]\\n        if fit_intercept:\\n            log_reg.coef_ = w[:, :-1]\\n            log_reg.intercept_ = w[:, -1]\\n        else:\\n            log_reg.coef_ = w\\n            log_reg.intercept_ = 0.\\n        if scoring is None:\\n            scores.append(log_reg.score(X_test, y_test))\\n        else:\\n            scores.append(scoring(log_reg, X_test, y_test))\\n    return coefs, Cs, np.array(scores), n_iter",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,\\n                          scoring=None, fit_intercept=False,\\n                          max_iter=100, tol=1e-4, class_weight=None,\\n                          verbose=0, solver='lbfgs', penalty='l2',\\n                          dual=False, intercept_scaling=1.,\\n                          multi_class='auto', random_state=None,\\n                          max_squared_sum=None, sample_weight=None,\\n                          l1_ratio=None):\\n    X_train = X[train]\\n    X_test = X[test]\\n    y_train = y[train]\\n    y_test = y[test]\\n    if sample_weight is not None:\\n        sample_weight = check_array(sample_weight, ensure_2d=False)\\n        check_consistent_length(y, sample_weight)\\n        sample_weight = sample_weight[train]\\n    coefs, Cs, n_iter = _logistic_regression_path(\\n        X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,\\n        fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,\\n        class_weight=class_weight, pos_class=pos_class,\\n        multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,\\n        penalty=penalty, intercept_scaling=intercept_scaling,\\n        random_state=random_state, check_input=False,\\n        max_squared_sum=max_squared_sum, sample_weight=sample_weight)\\n    log_reg = LogisticRegression(solver=solver, multi_class=multi_class)\\n    if multi_class == 'ovr':\\n        log_reg.classes_ = np.array([-1, 1])\\n    elif multi_class == 'multinomial':\\n        log_reg.classes_ = np.unique(y_train)\\n    else:\\n        raise ValueError(\"multi_class should be either multinomial or ovr, \"\\n                         \"got %d\" % multi_class)\\n    if pos_class is not None:\\n        mask = (y_test == pos_class)\\n        y_test = np.ones(y_test.shape, dtype=np.float64)\\n        y_test[~mask] = -1.\\n    scores = list()\\n    if isinstance(scoring, str):\\n        scoring = get_scorer(scoring)\\n    for w in coefs:\\n        if multi_class == 'ovr':\\n            w = w[np.newaxis, :]\\n        if fit_intercept:\\n            log_reg.coef_ = w[:, :-1]\\n            log_reg.intercept_ = w[:, -1]\\n        else:\\n            log_reg.coef_ = w\\n            log_reg.intercept_ = 0.\\n        if scoring is None:\\n            scores.append(log_reg.score(X_test, y_test))\\n        else:\\n            scores.append(scoring(log_reg, X_test, y_test))\\n    return coefs, Cs, np.array(scores), n_iter"
  },
  {
    "code": "def _ensure_data(values, dtype=None):\\n    if not isinstance(values, ABCMultiIndex):\\n        values = extract_array(values, extract_numpy=True)\\n    if is_object_dtype(dtype):\\n        return ensure_object(np.asarray(values)), \"object\"\\n    elif is_object_dtype(values) and dtype is None:\\n        return ensure_object(np.asarray(values)), \"object\"\\n    try:\\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\\n            return np.asarray(values).astype(\"uint64\"), \"bool\"\\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\\n            return ensure_int64(values), \"int64\"\\n        elif is_unsigned_integer_dtype(values) or is_unsigned_integer_dtype(dtype):\\n            return ensure_uint64(values), \"uint64\"\\n        elif is_float_dtype(values) or is_float_dtype(dtype):\\n            return ensure_float64(values), \"float64\"\\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\\n            with catch_warnings():\\n                simplefilter(\"ignore\", np.ComplexWarning)\\n                values = ensure_float64(values)\\n            return values, \"float64\"\\n    except (TypeError, ValueError, OverflowError):\\n        return ensure_object(values), \"object\"\\n    vals_dtype = getattr(values, \"dtype\", None)\\n    if needs_i8_conversion(vals_dtype) or needs_i8_conversion(dtype):\\n        if is_period_dtype(vals_dtype) or is_period_dtype(dtype):\\n            from pandas import PeriodIndex\\n            values = PeriodIndex(values)\\n            dtype = values.dtype\\n        elif is_timedelta64_dtype(vals_dtype) or is_timedelta64_dtype(dtype):\\n            from pandas import TimedeltaIndex\\n            values = TimedeltaIndex(values)\\n            dtype = values.dtype\\n        else:\\n            if values.ndim > 1 and is_datetime64_ns_dtype(vals_dtype):\\n                asi8 = values.view(\"i8\")\\n                dtype = values.dtype\\n                return asi8, dtype\\n            from pandas import DatetimeIndex\\n            values = DatetimeIndex(values)\\n            dtype = values.dtype\\n        return values.asi8, dtype\\n    elif is_categorical_dtype(vals_dtype) and (\\n        is_categorical_dtype(dtype) or dtype is None\\n    ):\\n        values = values.codes\\n        dtype = \"category\"\\n        values = ensure_int64(values)\\n        return values, dtype\\n    values = np.asarray(values, dtype=np.object)\\n    return ensure_object(values), \"object\"",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ensure_data(values, dtype=None):\\n    if not isinstance(values, ABCMultiIndex):\\n        values = extract_array(values, extract_numpy=True)\\n    if is_object_dtype(dtype):\\n        return ensure_object(np.asarray(values)), \"object\"\\n    elif is_object_dtype(values) and dtype is None:\\n        return ensure_object(np.asarray(values)), \"object\"\\n    try:\\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\\n            return np.asarray(values).astype(\"uint64\"), \"bool\"\\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\\n            return ensure_int64(values), \"int64\"\\n        elif is_unsigned_integer_dtype(values) or is_unsigned_integer_dtype(dtype):\\n            return ensure_uint64(values), \"uint64\"\\n        elif is_float_dtype(values) or is_float_dtype(dtype):\\n            return ensure_float64(values), \"float64\"\\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\\n            with catch_warnings():\\n                simplefilter(\"ignore\", np.ComplexWarning)\\n                values = ensure_float64(values)\\n            return values, \"float64\"\\n    except (TypeError, ValueError, OverflowError):\\n        return ensure_object(values), \"object\"\\n    vals_dtype = getattr(values, \"dtype\", None)\\n    if needs_i8_conversion(vals_dtype) or needs_i8_conversion(dtype):\\n        if is_period_dtype(vals_dtype) or is_period_dtype(dtype):\\n            from pandas import PeriodIndex\\n            values = PeriodIndex(values)\\n            dtype = values.dtype\\n        elif is_timedelta64_dtype(vals_dtype) or is_timedelta64_dtype(dtype):\\n            from pandas import TimedeltaIndex\\n            values = TimedeltaIndex(values)\\n            dtype = values.dtype\\n        else:\\n            if values.ndim > 1 and is_datetime64_ns_dtype(vals_dtype):\\n                asi8 = values.view(\"i8\")\\n                dtype = values.dtype\\n                return asi8, dtype\\n            from pandas import DatetimeIndex\\n            values = DatetimeIndex(values)\\n            dtype = values.dtype\\n        return values.asi8, dtype\\n    elif is_categorical_dtype(vals_dtype) and (\\n        is_categorical_dtype(dtype) or dtype is None\\n    ):\\n        values = values.codes\\n        dtype = \"category\"\\n        values = ensure_int64(values)\\n        return values, dtype\\n    values = np.asarray(values, dtype=np.object)\\n    return ensure_object(values), \"object\""
  },
  {
    "code": "def getouterframes(frame, context=1):\\n    framelist = []\\n    while frame:\\n        traceback_info = getframeinfo(frame, context)\\n        frameinfo = (frame,) + traceback_info\\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\\n        frame = frame.f_back\\n    return framelist",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getouterframes(frame, context=1):\\n    framelist = []\\n    while frame:\\n        traceback_info = getframeinfo(frame, context)\\n        frameinfo = (frame,) + traceback_info\\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\\n        frame = frame.f_back\\n    return framelist"
  },
  {
    "code": "def uname():\\n    global _uname_cache\\n    if _uname_cache is not None:\\n        return _uname_cache\\n    try:\\n        system, node, release, version, machine = infos = os.uname()\\n    except AttributeError:\\n        system = sys.platform\\n        node = _node()\\n        release = version = machine = ''\\n        infos = ()\\n    if not any(infos):\\n        if system == 'win32':\\n            release, version, csd, ptype = win32_ver()\\n            machine = machine or _get_machine_win32()\\n        if not (release and version):\\n            system, release, version = _syscmd_ver(system)\\n            if system == 'Microsoft Windows':\\n                system = 'Windows'\\n            elif system == 'Microsoft' and release == 'Windows':\\n                system = 'Windows'\\n                if '6.0' == version[:3]:\\n                    release = 'Vista'\\n                else:\\n                    release = ''\\n        if system in ('win32', 'win16'):\\n            if not version:\\n                if system == 'win32':\\n                    version = '32bit'\\n                else:\\n                    version = '16bit'\\n            system = 'Windows'\\n        elif system[:4] == 'java':\\n            release, vendor, vminfo, osinfo = java_ver()\\n            system = 'Java'\\n            version = ', '.join(vminfo)\\n            if not version:\\n                version = vendor\\n    if system == 'OpenVMS':\\n        if not release or release == '0':\\n            release = version\\n            version = ''\\n    if system == 'Microsoft' and release == 'Windows':\\n        system = 'Windows'\\n        release = 'Vista'\\n    vals = system, node, release, version, machine\\n    _uname_cache = uname_result(*map(_unknown_as_blank, vals))\\n    return _uname_cache",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def uname():\\n    global _uname_cache\\n    if _uname_cache is not None:\\n        return _uname_cache\\n    try:\\n        system, node, release, version, machine = infos = os.uname()\\n    except AttributeError:\\n        system = sys.platform\\n        node = _node()\\n        release = version = machine = ''\\n        infos = ()\\n    if not any(infos):\\n        if system == 'win32':\\n            release, version, csd, ptype = win32_ver()\\n            machine = machine or _get_machine_win32()\\n        if not (release and version):\\n            system, release, version = _syscmd_ver(system)\\n            if system == 'Microsoft Windows':\\n                system = 'Windows'\\n            elif system == 'Microsoft' and release == 'Windows':\\n                system = 'Windows'\\n                if '6.0' == version[:3]:\\n                    release = 'Vista'\\n                else:\\n                    release = ''\\n        if system in ('win32', 'win16'):\\n            if not version:\\n                if system == 'win32':\\n                    version = '32bit'\\n                else:\\n                    version = '16bit'\\n            system = 'Windows'\\n        elif system[:4] == 'java':\\n            release, vendor, vminfo, osinfo = java_ver()\\n            system = 'Java'\\n            version = ', '.join(vminfo)\\n            if not version:\\n                version = vendor\\n    if system == 'OpenVMS':\\n        if not release or release == '0':\\n            release = version\\n            version = ''\\n    if system == 'Microsoft' and release == 'Windows':\\n        system = 'Windows'\\n        release = 'Vista'\\n    vals = system, node, release, version, machine\\n    _uname_cache = uname_result(*map(_unknown_as_blank, vals))\\n    return _uname_cache"
  },
  {
    "code": "def _combine_match_index(self, other, func, level=None):\\n\\t\\tif level is not None:\\n\\t\\t\\traise NotImplementedError(\"'level' argument is not supported\")\\n\\t\\tthis, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\\n\\t\\tnew_data = {}\\n\\t\\tfor col in this.columns:\\n\\t\\t\\tnew_data[col] = func(this[col], other)\\n\\t\\tfill_value = self._get_op_result_fill_value(other, func)\\n\\t\\treturn self._constructor(\\n\\t\\t\\tnew_data,\\n\\t\\t\\tindex=this.index,\\n\\t\\t\\tcolumns=self.columns,",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _combine_match_index(self, other, func, level=None):\\n\\t\\tif level is not None:\\n\\t\\t\\traise NotImplementedError(\"'level' argument is not supported\")\\n\\t\\tthis, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\\n\\t\\tnew_data = {}\\n\\t\\tfor col in this.columns:\\n\\t\\t\\tnew_data[col] = func(this[col], other)\\n\\t\\tfill_value = self._get_op_result_fill_value(other, func)\\n\\t\\treturn self._constructor(\\n\\t\\t\\tnew_data,\\n\\t\\t\\tindex=this.index,\\n\\t\\t\\tcolumns=self.columns,"
  },
  {
    "code": "def _fit_transform(self, X):\\n        self.nbrs_.fit(X)\\n        self.training_data_ = self.nbrs_._fit_X\\n        self.kernel_pca_ = KernelPCA(n_components=self.out_dim,\\n                                     kernel=\"precomputed\",\\n                                     eigen_solver=self.eigen_solver,\\n                                     tol=self.tol, max_iter=self.max_iter)\\n        kng = kneighbors_graph(self.nbrs_, self.n_neighbors,\\n                               mode='distance')\\n        self.dist_matrix_ = graph_shortest_path(kng,\\n                                                method=self.path_method,\\n                                                directed=False)\\n        G = self.dist_matrix_ ** 2\\n        G *= -0.5\\n        self.embedding_ = self.kernel_pca_.fit_transform(G)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH rename out_dim to  n_components in manifold module",
    "fixed_code": "def _fit_transform(self, X):\\n        self.nbrs_.fit(X)\\n        self.training_data_ = self.nbrs_._fit_X\\n        self.kernel_pca_ = KernelPCA(n_components=self.n_components,\\n                                     kernel=\"precomputed\",\\n                                     eigen_solver=self.eigen_solver,\\n                                     tol=self.tol, max_iter=self.max_iter)\\n        kng = kneighbors_graph(self.nbrs_, self.n_neighbors,\\n                               mode='distance')\\n        self.dist_matrix_ = graph_shortest_path(kng,\\n                                                method=self.path_method,\\n                                                directed=False)\\n        G = self.dist_matrix_ ** 2\\n        G *= -0.5\\n        self.embedding_ = self.kernel_pca_.fit_transform(G)"
  },
  {
    "code": "def get_loc(self, key):\\n        try:\\n            return self._engine.get_loc(key)\\n        except KeyError:\\n            try:\\n                return self._get_string_slice(key)\\n            except (TypeError, KeyError, ValueError):\\n                pass\\n            if isinstance(key, time):\\n                return self.indexer_at_time(key)\\n            try:\\n                if isinstance(key, basestring):\\n                    stamp = Timestamp(key, tz=self.tz)\\n                else:\\n                    stamp = Timestamp(key)\\n                return self._engine.get_loc(stamp)\\n            except (KeyError, ValueError):\\n                raise KeyError(key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_loc(self, key):\\n        try:\\n            return self._engine.get_loc(key)\\n        except KeyError:\\n            try:\\n                return self._get_string_slice(key)\\n            except (TypeError, KeyError, ValueError):\\n                pass\\n            if isinstance(key, time):\\n                return self.indexer_at_time(key)\\n            try:\\n                if isinstance(key, basestring):\\n                    stamp = Timestamp(key, tz=self.tz)\\n                else:\\n                    stamp = Timestamp(key)\\n                return self._engine.get_loc(stamp)\\n            except (KeyError, ValueError):\\n                raise KeyError(key)"
  },
  {
    "code": "def _getitem_single(self, key):\\n        values = self._data.get(key)\\n        return Series(values, index=self.index)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: added a cache of DataFrame columns to improve perf with many calls to __getitem__",
    "fixed_code": "def _getitem_single(self, key):\\n        res = self._series_cache.get(key)\\n        if res is not None:\\n            return res\\n        values = self._data.get(key)\\n        res = Series(values, index=self.index)\\n        self._series_cache[key] = res\\n        return res"
  },
  {
    "code": "def add(self, key, value, timeout=0):\\n        if isinstance(value, unicode):\\n            value = value.encode('utf-8')\\n        return self._cache.add(smart_str(key), value, timeout or self.default_timeout)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #12399 -- Added handling for memcache timeouts longer than 30 days. Thanks to houdinihound for the report, and gciotta for the patch.",
    "fixed_code": "def add(self, key, value, timeout=0):\\n        if isinstance(value, unicode):\\n            value = value.encode('utf-8')\\n        return self._cache.add(smart_str(key), value, self._get_memcache_timeout(timeout))"
  },
  {
    "code": "def _get_default_tempdir():\\n    namer = _RandomNameSequence()\\n    dirlist = _candidate_tempdir_list()\\n    for dir in dirlist:\\n        if dir != _os.curdir:\\n            dir = _os.path.normcase(_os.path.abspath(dir))\\n        for seq in range(100):\\n            name = next(namer)\\n            filename = _os.path.join(dir, name)\\n            try:\\n                fd = _os.open(filename, _bin_openflags, 0o600)\\n                fp = _io.open(fd, 'wb')\\n                fp.write(b'blat')\\n                fp.close()\\n                _os.unlink(filename)\\n                del fp, fd\\n                return dir\\n            except FileExistsError:\\n                pass\\n            except OSError:\\n                break   \\n    raise FileNotFoundError(\"No usable temporary directory found in %s\" % dirlist)\\n_name_sequence = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #17169: Restore errno in tempfile exceptions.",
    "fixed_code": "def _get_default_tempdir():\\n    namer = _RandomNameSequence()\\n    dirlist = _candidate_tempdir_list()\\n    for dir in dirlist:\\n        if dir != _os.curdir:\\n            dir = _os.path.normcase(_os.path.abspath(dir))\\n        for seq in range(100):\\n            name = next(namer)\\n            filename = _os.path.join(dir, name)\\n            try:\\n                fd = _os.open(filename, _bin_openflags, 0o600)\\n                fp = _io.open(fd, 'wb')\\n                fp.write(b'blat')\\n                fp.close()\\n                _os.unlink(filename)\\n                del fp, fd\\n                return dir\\n            except FileExistsError:\\n                pass\\n            except OSError:\\n                break   \\n    raise FileNotFoundError(_errno.ENOENT,\\n                            \"No usable temporary directory found in %s\" %\\n                            dirlist)\\n_name_sequence = None"
  },
  {
    "code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               comment=None,\\n               parse_dates=False,\\n               keep_date_col=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None,\\n               squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               comment=None,\\n               parse_dates=False,\\n               keep_date_col=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None,\\n               squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)"
  },
  {
    "code": "def _replace_nan(a, val):\\n    is_new = not isinstance(a, np.ndarray)\\n    if is_new:\\n        a = np.array(a)\\n    if not issubclass(a.dtype.type, np.inexact):\\n        return a, None\\n    if not is_new:\\n        a = np.array(a, subok=True)\\n    mask = np.isnan(a)\\n    np.copyto(a, val, where=mask)\\n    return a, mask",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _replace_nan(a, val):\\n    is_new = not isinstance(a, np.ndarray)\\n    if is_new:\\n        a = np.array(a)\\n    if not issubclass(a.dtype.type, np.inexact):\\n        return a, None\\n    if not is_new:\\n        a = np.array(a, subok=True)\\n    mask = np.isnan(a)\\n    np.copyto(a, val, where=mask)\\n    return a, mask"
  },
  {
    "code": "def is_dataclass(obj):\\n    return hasattr(obj, _MARKER)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-32953: Dataclasses: frozen should not be inherited for non-dataclass derived classes (#6147)\\n\\nIf a non-dataclass derives from a frozen dataclass, allow attributes to be set.\\nRequire either all of the dataclasses in a class hierarchy to be frozen, or all non-frozen.\\nStore `@dataclass` parameters on the class object under `__dataclass_params__`. This is needed to detect frozen base classes.",
    "fixed_code": "def is_dataclass(obj):\\n    return hasattr(obj, _FIELDS)"
  },
  {
    "code": "def decrypt(\\n        self,\\n        key_name: str,\\n        ciphertext: str,\\n        authenticated_data: Optional[bytes] = None,\\n        retry: Optional[Retry] = None,\\n        timeout: Optional[float] = None,\\n        metadata: Optional[Sequence[Tuple[str, str]]] = None,\\n    ) -> bytes:\\n        response = self.get_conn().decrypt(\\n            name=key_name,\\n            ciphertext=_b64decode(ciphertext),\\n            additional_authenticated_data=authenticated_data,\\n            retry=retry,\\n            timeout=timeout,\\n            metadata=metadata,\\n        )\\n        plaintext = response.plaintext\\n        return plaintext",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def decrypt(\\n        self,\\n        key_name: str,\\n        ciphertext: str,\\n        authenticated_data: Optional[bytes] = None,\\n        retry: Optional[Retry] = None,\\n        timeout: Optional[float] = None,\\n        metadata: Optional[Sequence[Tuple[str, str]]] = None,\\n    ) -> bytes:\\n        response = self.get_conn().decrypt(\\n            name=key_name,\\n            ciphertext=_b64decode(ciphertext),\\n            additional_authenticated_data=authenticated_data,\\n            retry=retry,\\n            timeout=timeout,\\n            metadata=metadata,\\n        )\\n        plaintext = response.plaintext\\n        return plaintext"
  },
  {
    "code": "def __init__(self, url=''):\\n        self.entries = []\\n        self.sitemaps = []\\n        self.default_entry = None\\n        self.disallow_all = False\\n        self.allow_all = False\\n        self.set_url(url)\\n        self.last_checked = 0",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, url=''):\\n        self.entries = []\\n        self.sitemaps = []\\n        self.default_entry = None\\n        self.disallow_all = False\\n        self.allow_all = False\\n        self.set_url(url)\\n        self.last_checked = 0"
  },
  {
    "code": "def get_reindexed_values(self, empty_dtype, upcasted_na):\\n        if upcasted_na is None:\\n            fill_value = self.block.fill_value\\n            values = self.block.get_values()\\n        else:\\n            fill_value = upcasted_na\\n            if self.is_null:\\n                if getattr(self.block, 'is_object', False):\\n                    values = self.block.values.ravel(order='K')\\n                    if len(values) and values[0] is None:\\n                        fill_value = None\\n                if getattr(self.block, 'is_datetimetz', False):\\n                    pass\\n                elif getattr(self.block, 'is_categorical', False):\\n                    pass\\n                elif getattr(self.block, 'is_sparse', False):\\n                    pass\\n                else:\\n                    missing_arr = np.empty(self.shape, dtype=empty_dtype)\\n                    missing_arr.fill(fill_value)\\n                    return missing_arr\\n            if not self.indexers:\\n                if not self.block._can_consolidate:\\n                    return self.block.values\\n            if self.block.is_bool:\\n                values = self.block.astype(np.object_).values\\n            elif self.block.is_categorical:\\n                values = self.block.values\\n            else:\\n                values = self.block.get_values()\\n        if not self.indexers:\\n            values = values.view()\\n        else:\\n            for ax, indexer in self.indexers.items():\\n                values = algos.take_nd(values, indexer, axis=ax,\\n                                       fill_value=fill_value)\\n        return values",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_reindexed_values(self, empty_dtype, upcasted_na):\\n        if upcasted_na is None:\\n            fill_value = self.block.fill_value\\n            values = self.block.get_values()\\n        else:\\n            fill_value = upcasted_na\\n            if self.is_null:\\n                if getattr(self.block, 'is_object', False):\\n                    values = self.block.values.ravel(order='K')\\n                    if len(values) and values[0] is None:\\n                        fill_value = None\\n                if getattr(self.block, 'is_datetimetz', False):\\n                    pass\\n                elif getattr(self.block, 'is_categorical', False):\\n                    pass\\n                elif getattr(self.block, 'is_sparse', False):\\n                    pass\\n                else:\\n                    missing_arr = np.empty(self.shape, dtype=empty_dtype)\\n                    missing_arr.fill(fill_value)\\n                    return missing_arr\\n            if not self.indexers:\\n                if not self.block._can_consolidate:\\n                    return self.block.values\\n            if self.block.is_bool:\\n                values = self.block.astype(np.object_).values\\n            elif self.block.is_categorical:\\n                values = self.block.values\\n            else:\\n                values = self.block.get_values()\\n        if not self.indexers:\\n            values = values.view()\\n        else:\\n            for ax, indexer in self.indexers.items():\\n                values = algos.take_nd(values, indexer, axis=ax,\\n                                       fill_value=fill_value)\\n        return values"
  },
  {
    "code": "def send_email(\\n    to: Union[List[str], str],\\n    subject: str,\\n    html_content: str,\\n    files: Optional[List] = None,\\n    cc: Optional[Union[List[str], str]] = None,\\n    bcc: Optional[Union[List[str], str]] = None,\\n    mime_subtype: str = 'mixed',\\n    mime_charset: str = 'utf-8',\\n    conn_id: str = 'aws_default',\\n    from_email: Optional[str] = None,\\n    **kwargs,\\n) -> None:\\n    if from_email is None:\\n        raise RuntimeError(\"The `from_email' configuration has to be set for the SES emailer.\")\\n    hook = SesHook(aws_conn_id=conn_id)\\n    hook.send_email(\\n        mail_from=from_email,\\n        to=to,\\n        subject=subject,\\n        html_content=html_content,\\n        files=files,\\n        cc=cc,\\n        bcc=bcc,\\n        mime_subtype=mime_subtype,\\n        mime_charset=mime_charset,\\n    )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Pass custom headers through in SES email backend (#22667)",
    "fixed_code": "def send_email(\\n    to: Union[List[str], str],\\n    subject: str,\\n    html_content: str,\\n    files: Optional[List] = None,\\n    cc: Optional[Union[List[str], str]] = None,\\n    bcc: Optional[Union[List[str], str]] = None,\\n    mime_subtype: str = 'mixed',\\n    mime_charset: str = 'utf-8',\\n    conn_id: str = 'aws_default',\\n    from_email: Optional[str] = None,\\n    custom_headers: Optional[Dict[str, Any]] = None,\\n    **kwargs,\\n) -> None:\\n    if from_email is None:\\n        raise RuntimeError(\"The `from_email' configuration has to be set for the SES emailer.\")\\n    hook = SesHook(aws_conn_id=conn_id)\\n    hook.send_email(\\n        mail_from=from_email,\\n        to=to,\\n        subject=subject,\\n        html_content=html_content,\\n        files=files,\\n        cc=cc,\\n        bcc=bcc,\\n        mime_subtype=mime_subtype,\\n        mime_charset=mime_charset,\\n        custom_headers=custom_headers,\\n    )"
  },
  {
    "code": "def _set_names(self, values, level=None):\\n        if len(values) != 1:\\n            raise ValueError('Length of new names must be 1, got %d' %\\n                             len(values))\\n        self.name = values[0]\\n    names = property(fset=_set_names, fget=_get_names)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ERR: disallow non-hashables in Index/MultiIndex construction & rename (#20548)",
    "fixed_code": "def _set_names(self, values, level=None):\\n        if not is_list_like(values):\\n            raise ValueError('Names must be a list-like')\\n        if len(values) != 1:\\n            raise ValueError('Length of new names must be 1, got %d' %\\n                             len(values))\\n        for name in values:\\n            if not is_hashable(name):\\n                raise TypeError('{}.name must be a hashable type'\\n                                .format(self.__class__.__name__))\\n        self.name = values[0]\\n    names = property(fset=_set_names, fget=_get_names)"
  },
  {
    "code": "def generalized_exponential(theta, d):\\n    theta = np.asanyarray(theta, dtype=np.float)\\n    d = np.asanyarray(d, dtype=np.float)\\n    if d.ndim > 1:\\n        n_features = d.shape[1]\\n    else:\\n        n_features = 1\\n    lth = theta.size\\n    if n_features > 1 and lth == 2:\\n        theta = np.hstack([np.repeat(theta[0], n_features), theta[1]])\\n    elif lth != n_features + 1:\\n        raise Exception(\"Length of theta must be 2 or %s\" % (n_features + 1))\\n    else:\\n        theta = theta.reshape(1, lth)\\n    td = theta[:, 0:-1].reshape(1, n_features) * np.abs(d) ** theta[:, -1]\\n    r = np.exp(- np.sum(td, 1))\\n    return r",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generalized_exponential(theta, d):\\n    theta = np.asanyarray(theta, dtype=np.float)\\n    d = np.asanyarray(d, dtype=np.float)\\n    if d.ndim > 1:\\n        n_features = d.shape[1]\\n    else:\\n        n_features = 1\\n    lth = theta.size\\n    if n_features > 1 and lth == 2:\\n        theta = np.hstack([np.repeat(theta[0], n_features), theta[1]])\\n    elif lth != n_features + 1:\\n        raise Exception(\"Length of theta must be 2 or %s\" % (n_features + 1))\\n    else:\\n        theta = theta.reshape(1, lth)\\n    td = theta[:, 0:-1].reshape(1, n_features) * np.abs(d) ** theta[:, -1]\\n    r = np.exp(- np.sum(td, 1))\\n    return r"
  },
  {
    "code": "def __repr__(self):\\n        return f'<FileHash mode: {self.mode} value: {self.value}>'",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-44893: Implement EntryPoint as simple class with attributes. (GH-30150)",
    "fixed_code": "def __repr__(self):\\n        return (\\n            f'EntryPoint(name={self.name!r}, value={self.value!r}, '\\n            f'group={self.group!r})'\\n        )"
  },
  {
    "code": "def __init__(self, kernel=None, jitter=0.0, optimizer=\"fmin_l_bfgs_b\",\\n                 n_restarts_optimizer=1, max_iter=100, warm_start=False,\\n                 copy_X_train=False, random_state=None):\\n        self.kernel = kernel\\n        self.jitter = jitter\\n        self.optimizer = optimizer\\n        self.n_restarts_optimizer = n_restarts_optimizer\\n        self.max_iter = max_iter\\n        self.warm_start = warm_start\\n        self.copy_X_train = copy_X_train\\n        self.random_state = random_state\\n        self.rng = check_random_state(self.random_state)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF Several small changes based on @eickenberg's suggestions",
    "fixed_code": "def __init__(self, kernel=None, optimizer=\"fmin_l_bfgs_b\",\\n                 n_restarts_optimizer=0, max_iter=100, warm_start=False,\\n                 copy_X_train=False, random_state=None):\\n        self.kernel = kernel\\n        self.optimizer = optimizer\\n        self.n_restarts_optimizer = n_restarts_optimizer\\n        self.max_iter = max_iter\\n        self.warm_start = warm_start\\n        self.copy_X_train = copy_X_train\\n        self.random_state = random_state"
  },
  {
    "code": "def test_k_means_plus_plus_init_2_jobs():\\n\\tif _get_mac_os_version() >= '10.7':\\n\\t\\traise SkipTest('Multi-process bug in Mac OS X Lion (see issue \\n\\tk_means = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_jobs=2,\\n\\t\\t\\t\\t\\t random_state=42).fit(X)\\n\\t_check_fitted_model(k_means)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_k_means_plus_plus_init_2_jobs():\\n\\tif _get_mac_os_version() >= '10.7':\\n\\t\\traise SkipTest('Multi-process bug in Mac OS X Lion (see issue \\n\\tk_means = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_jobs=2,\\n\\t\\t\\t\\t\\t random_state=42).fit(X)\\n\\t_check_fitted_model(k_means)"
  },
  {
    "code": "def handle(self, app_or_project, name, target=None, **options):\\n        self.app_or_project = app_or_project\\n        self.paths_to_remove = []\\n        self.verbosity = options['verbosity']\\n        self.validate_name(name, app_or_project)\\n        if target is None:\\n            top_dir = path.join(os.getcwd(), name)\\n            try:\\n                os.makedirs(top_dir)\\n            except OSError as e:\\n                if e.errno == errno.EEXIST:\\n                    message = \"'%s' already exists\" % top_dir\\n                else:\\n                    message = e\\n                raise CommandError(message)\\n        else:\\n            top_dir = os.path.abspath(path.expanduser(target))\\n            if not os.path.exists(top_dir):\\n                raise CommandError(\"Destination directory '%s' does not \"\\n                                   \"exist, please create it first.\" % top_dir)\\n        extensions = tuple(handle_extensions(options['extensions']))\\n        extra_files = []\\n        for file in options['files']:\\n            extra_files.extend(map(lambda x: x.strip(), file.split(',')))\\n        if self.verbosity >= 2:\\n            self.stdout.write(\"Rendering %s template files with \"\\n                              \"extensions: %s\\n\" %\\n                              (app_or_project, ', '.join(extensions)))\\n            self.stdout.write(\"Rendering %s template files with \"\\n                              \"filenames: %s\\n\" %\\n                              (app_or_project, ', '.join(extra_files)))\\n        base_name = '%s_name' % app_or_project\\n        base_subdir = '%s_template' % app_or_project\\n        base_directory = '%s_directory' % app_or_project\\n        context = Context(dict(options, **{\\n            base_name: name,\\n            base_directory: top_dir,\\n            'docs_version': get_docs_version(),\\n            'django_version': django.__version__,\\n        }), autoescape=False)\\n        from django.conf import settings\\n        if not settings.configured:\\n            settings.configure()\\n        template_dir = self.handle_template(options['template'],\\n                                            base_subdir)\\n        prefix_length = len(template_dir) + 1\\n        for root, dirs, files in os.walk(template_dir):\\n            path_rest = root[prefix_length:]\\n            relative_dir = path_rest.replace(base_name, name)\\n            if relative_dir:\\n                target_dir = path.join(top_dir, relative_dir)\\n                if not path.exists(target_dir):\\n                    os.mkdir(target_dir)\\n            for dirname in dirs[:]:\\n                if dirname.startswith('.') or dirname == '__pycache__':\\n                    dirs.remove(dirname)\\n            for filename in files:\\n                if filename.endswith(('.pyo', '.pyc', '.py.class')):\\n                    continue\\n                old_path = path.join(root, filename)\\n                new_path = path.join(top_dir, relative_dir,\\n                                     filename.replace(base_name, name))\\n                if path.exists(new_path):\\n                    raise CommandError(\"%s already exists, overlaying a \"\\n                                       \"project or app into an existing \"\\n                                       \"directory won't replace conflicting \"\\n                                       \"files\" % new_path)\\n                with open(old_path, 'rb') as template_file:\\n                    content = template_file.read()\\n                if filename.endswith(extensions) or filename in extra_files:\\n                    content = content.decode('utf-8')\\n                    template = Template(content)\\n                    content = template.render(context)\\n                    content = content.encode('utf-8')\\n                with open(new_path, 'wb') as new_file:\\n                    new_file.write(content)\\n                if self.verbosity >= 2:\\n                    self.stdout.write(\"Creating %s\\n\" % new_path)\\n                try:\\n                    shutil.copymode(old_path, new_path)\\n                    self.make_writeable(new_path)\\n                except OSError:\\n                    self.stderr.write(\\n                        \"Notice: Couldn't set permission bits on %s. You're \"\\n                        \"probably using an uncommon filesystem setup. No \"\\n                        \"problem.\" % new_path, self.style.NOTICE)\\n        if self.paths_to_remove:\\n            if self.verbosity >= 2:\\n                self.stdout.write(\"Cleaning up temporary files.\\n\")\\n            for path_to_remove in self.paths_to_remove:\\n                if path.isfile(path_to_remove):\\n                    os.remove(path_to_remove)\\n                else:\\n                    shutil.rmtree(path_to_remove)\\n    def handle_template(self, template, subdir):\\n        if template is None:\\n            return path.join(django.__path__[0], 'conf', subdir)\\n        else:\\n            if template.startswith('file://'):\\n                template = template[7:]\\n            expanded_template = path.expanduser(template)\\n            expanded_template = path.normpath(expanded_template)\\n            if path.isdir(expanded_template):\\n                return expanded_template\\n            if self.is_url(template):\\n                absolute_path = self.download(template)\\n            else:\\n                absolute_path = path.abspath(expanded_template)\\n            if path.exists(absolute_path):\\n                return self.extract(absolute_path)\\n        raise CommandError(\"couldn't handle %s template %s.\" %\\n                           (self.app_or_project, template))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def handle(self, app_or_project, name, target=None, **options):\\n        self.app_or_project = app_or_project\\n        self.paths_to_remove = []\\n        self.verbosity = options['verbosity']\\n        self.validate_name(name, app_or_project)\\n        if target is None:\\n            top_dir = path.join(os.getcwd(), name)\\n            try:\\n                os.makedirs(top_dir)\\n            except OSError as e:\\n                if e.errno == errno.EEXIST:\\n                    message = \"'%s' already exists\" % top_dir\\n                else:\\n                    message = e\\n                raise CommandError(message)\\n        else:\\n            top_dir = os.path.abspath(path.expanduser(target))\\n            if not os.path.exists(top_dir):\\n                raise CommandError(\"Destination directory '%s' does not \"\\n                                   \"exist, please create it first.\" % top_dir)\\n        extensions = tuple(handle_extensions(options['extensions']))\\n        extra_files = []\\n        for file in options['files']:\\n            extra_files.extend(map(lambda x: x.strip(), file.split(',')))\\n        if self.verbosity >= 2:\\n            self.stdout.write(\"Rendering %s template files with \"\\n                              \"extensions: %s\\n\" %\\n                              (app_or_project, ', '.join(extensions)))\\n            self.stdout.write(\"Rendering %s template files with \"\\n                              \"filenames: %s\\n\" %\\n                              (app_or_project, ', '.join(extra_files)))\\n        base_name = '%s_name' % app_or_project\\n        base_subdir = '%s_template' % app_or_project\\n        base_directory = '%s_directory' % app_or_project\\n        context = Context(dict(options, **{\\n            base_name: name,\\n            base_directory: top_dir,\\n            'docs_version': get_docs_version(),\\n            'django_version': django.__version__,\\n        }), autoescape=False)\\n        from django.conf import settings\\n        if not settings.configured:\\n            settings.configure()\\n        template_dir = self.handle_template(options['template'],\\n                                            base_subdir)\\n        prefix_length = len(template_dir) + 1\\n        for root, dirs, files in os.walk(template_dir):\\n            path_rest = root[prefix_length:]\\n            relative_dir = path_rest.replace(base_name, name)\\n            if relative_dir:\\n                target_dir = path.join(top_dir, relative_dir)\\n                if not path.exists(target_dir):\\n                    os.mkdir(target_dir)\\n            for dirname in dirs[:]:\\n                if dirname.startswith('.') or dirname == '__pycache__':\\n                    dirs.remove(dirname)\\n            for filename in files:\\n                if filename.endswith(('.pyo', '.pyc', '.py.class')):\\n                    continue\\n                old_path = path.join(root, filename)\\n                new_path = path.join(top_dir, relative_dir,\\n                                     filename.replace(base_name, name))\\n                if path.exists(new_path):\\n                    raise CommandError(\"%s already exists, overlaying a \"\\n                                       \"project or app into an existing \"\\n                                       \"directory won't replace conflicting \"\\n                                       \"files\" % new_path)\\n                with open(old_path, 'rb') as template_file:\\n                    content = template_file.read()\\n                if filename.endswith(extensions) or filename in extra_files:\\n                    content = content.decode('utf-8')\\n                    template = Template(content)\\n                    content = template.render(context)\\n                    content = content.encode('utf-8')\\n                with open(new_path, 'wb') as new_file:\\n                    new_file.write(content)\\n                if self.verbosity >= 2:\\n                    self.stdout.write(\"Creating %s\\n\" % new_path)\\n                try:\\n                    shutil.copymode(old_path, new_path)\\n                    self.make_writeable(new_path)\\n                except OSError:\\n                    self.stderr.write(\\n                        \"Notice: Couldn't set permission bits on %s. You're \"\\n                        \"probably using an uncommon filesystem setup. No \"\\n                        \"problem.\" % new_path, self.style.NOTICE)\\n        if self.paths_to_remove:\\n            if self.verbosity >= 2:\\n                self.stdout.write(\"Cleaning up temporary files.\\n\")\\n            for path_to_remove in self.paths_to_remove:\\n                if path.isfile(path_to_remove):\\n                    os.remove(path_to_remove)\\n                else:\\n                    shutil.rmtree(path_to_remove)\\n    def handle_template(self, template, subdir):\\n        if template is None:\\n            return path.join(django.__path__[0], 'conf', subdir)\\n        else:\\n            if template.startswith('file://'):\\n                template = template[7:]\\n            expanded_template = path.expanduser(template)\\n            expanded_template = path.normpath(expanded_template)\\n            if path.isdir(expanded_template):\\n                return expanded_template\\n            if self.is_url(template):\\n                absolute_path = self.download(template)\\n            else:\\n                absolute_path = path.abspath(expanded_template)\\n            if path.exists(absolute_path):\\n                return self.extract(absolute_path)\\n        raise CommandError(\"couldn't handle %s template %s.\" %\\n                           (self.app_or_project, template))"
  },
  {
    "code": "def callable_virtualenv():\\n        from time import sleep\\n        from colorama import Back, Fore, Style\\n        print(Fore.RED + 'some red text')\\n        print(Back.GREEN + 'and with a green background')\\n        print(Style.DIM + 'and in dim text')\\n        print(Style.RESET_ALL)\\n        for _ in range(10):\\n            print(Style.DIM + 'Please wait...', flush=True)\\n            sleep(10)\\n        print('Finished')\\n    virtualenv_task = PythonVirtualenvOperator(\\n        task_id=\"virtualenv_python\",\\n        python_callable=callable_virtualenv,\\n        requirements=[\"colorama==0.4.0\"],\\n        system_site_packages=False,\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def callable_virtualenv():\\n        from time import sleep\\n        from colorama import Back, Fore, Style\\n        print(Fore.RED + 'some red text')\\n        print(Back.GREEN + 'and with a green background')\\n        print(Style.DIM + 'and in dim text')\\n        print(Style.RESET_ALL)\\n        for _ in range(10):\\n            print(Style.DIM + 'Please wait...', flush=True)\\n            sleep(10)\\n        print('Finished')\\n    virtualenv_task = PythonVirtualenvOperator(\\n        task_id=\"virtualenv_python\",\\n        python_callable=callable_virtualenv,\\n        requirements=[\"colorama==0.4.0\"],\\n        system_site_packages=False,\\n    )"
  },
  {
    "code": "def remove(client, nat_gateway_id, wait=False, wait_timeout=0,\\n           release_eip=False, check_mode=False):\\n    params = {\\n        'NatGatewayId': nat_gateway_id\\n    }\\n    success = False\\n    changed = False\\n    err_msg = \"\"\\n    results = list()\\n    states = ['pending', 'available' ]\\n    try:\\n        exist, _, gw = (\\n            get_nat_gateways(\\n                client, nat_gateway_id=nat_gateway_id,\\n                states=states, check_mode=check_mode\\n            )\\n        )\\n        if exist and len(gw) == 1:\\n            results = gw[0]\\n            if not check_mode:\\n                client.delete_nat_gateway(**params)\\n            allocation_id = (\\n                results['nat_gateway_addresses'][0]['allocation_id']\\n            )\\n            changed = True\\n            success = True\\n            err_msg = (\\n                'NAT gateway {0} is in a deleting state. Delete was successfull'\\n                .format(nat_gateway_id)\\n            )\\n            if wait:\\n                status_achieved, err_msg, results = (\\n                    wait_for_status(\\n                        client, wait_timeout, nat_gateway_id, 'deleted',\\n                        check_mode=check_mode\\n                    )\\n                )\\n                if status_achieved:\\n                    err_msg = (\\n                        'NAT gateway {0} was deleted successfully'\\n                        .format(nat_gateway_id)\\n                    )\\n    except botocore.exceptions.ClientError as e:\\n        err_msg = str(e)\\n    if release_eip:\\n        eip_released, eip_err = (\\n            release_address(client, allocation_id, check_mode)\\n        )\\n        if not eip_released:\\n            err_msg = (\\n                \"{0}: Failed to release EIP {1}: {2}\"\\n                .format(err_msg, allocation_id, eip_err)\\n            )\\n            success = False\\n    return success, changed, err_msg, results",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def remove(client, nat_gateway_id, wait=False, wait_timeout=0,\\n           release_eip=False, check_mode=False):\\n    params = {\\n        'NatGatewayId': nat_gateway_id\\n    }\\n    success = False\\n    changed = False\\n    err_msg = \"\"\\n    results = list()\\n    states = ['pending', 'available' ]\\n    try:\\n        exist, _, gw = (\\n            get_nat_gateways(\\n                client, nat_gateway_id=nat_gateway_id,\\n                states=states, check_mode=check_mode\\n            )\\n        )\\n        if exist and len(gw) == 1:\\n            results = gw[0]\\n            if not check_mode:\\n                client.delete_nat_gateway(**params)\\n            allocation_id = (\\n                results['nat_gateway_addresses'][0]['allocation_id']\\n            )\\n            changed = True\\n            success = True\\n            err_msg = (\\n                'NAT gateway {0} is in a deleting state. Delete was successfull'\\n                .format(nat_gateway_id)\\n            )\\n            if wait:\\n                status_achieved, err_msg, results = (\\n                    wait_for_status(\\n                        client, wait_timeout, nat_gateway_id, 'deleted',\\n                        check_mode=check_mode\\n                    )\\n                )\\n                if status_achieved:\\n                    err_msg = (\\n                        'NAT gateway {0} was deleted successfully'\\n                        .format(nat_gateway_id)\\n                    )\\n    except botocore.exceptions.ClientError as e:\\n        err_msg = str(e)\\n    if release_eip:\\n        eip_released, eip_err = (\\n            release_address(client, allocation_id, check_mode)\\n        )\\n        if not eip_released:\\n            err_msg = (\\n                \"{0}: Failed to release EIP {1}: {2}\"\\n                .format(err_msg, allocation_id, eip_err)\\n            )\\n            success = False\\n    return success, changed, err_msg, results"
  },
  {
    "code": "def __getattribute__(self, attr):\\n        if attr in ['setfield', 'getfield', 'dtype']:\\n            return nt.void.__getattribute__(self, attr)\\n        try:\\n            return nt.void.__getattribute__(self, attr)\\n        except AttributeError:\\n            pass\\n        fielddict = nt.void.__getattribute__(self, 'dtype').fields\\n        res = fielddict.get(attr, None)\\n        if res:\\n            obj = self.getfield(*res[:2])\\n            try:\\n                dt = obj.dtype\\n            except AttributeError:\\n                return obj\\n            if dt.fields:\\n                return obj.view((record, obj.dtype.descr))\\n            return obj\\n        else:\\n            raise AttributeError(\"'record' object has no \"\\n                    \"attribute '%s'\" % attr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getattribute__(self, attr):\\n        if attr in ['setfield', 'getfield', 'dtype']:\\n            return nt.void.__getattribute__(self, attr)\\n        try:\\n            return nt.void.__getattribute__(self, attr)\\n        except AttributeError:\\n            pass\\n        fielddict = nt.void.__getattribute__(self, 'dtype').fields\\n        res = fielddict.get(attr, None)\\n        if res:\\n            obj = self.getfield(*res[:2])\\n            try:\\n                dt = obj.dtype\\n            except AttributeError:\\n                return obj\\n            if dt.fields:\\n                return obj.view((record, obj.dtype.descr))\\n            return obj\\n        else:\\n            raise AttributeError(\"'record' object has no \"\\n                    \"attribute '%s'\" % attr)"
  },
  {
    "code": "def _python_apply_general(self, arg):\\n        result_keys = []\\n        result_values = []\\n        not_indexed_same = False\\n        for key, group in self:\\n            group.name = key\\n            res = arg(group)\\n            if not _is_indexed_like(res, group):\\n                not_indexed_same = True\\n            result_keys.append(key)\\n            result_values.append(res)\\n        return self._wrap_applied_output(result_keys, result_values,\\n                                         not_indexed_same=not_indexed_same)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: multi-key groupby now works with multiple functions. starting to add *args, **kwargs. address GH #133",
    "fixed_code": "def _python_apply_general(self, func, *args, **kwargs):\\n        result_keys = []\\n        result_values = []\\n        not_indexed_same = False\\n        for key, group in self:\\n            group.name = key\\n            res = func(group, *args, **kwargs)\\n            if not _is_indexed_like(res, group):\\n                not_indexed_same = True\\n            result_keys.append(key)\\n            result_values.append(res)\\n        return self._wrap_applied_output(result_keys, result_values,\\n                                         not_indexed_same=not_indexed_same)"
  },
  {
    "code": "def header_items(self):\\n\\t\\treturn [\\n\\t\\t\\t(to_native_str(k, errors='replace'),\\n\\t\\t\\t [to_native_str(x, errors='replace') for x in v])\\n\\t\\t\\tfor k, v in self.request.headers.items()\\n\\t\\t]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def header_items(self):\\n\\t\\treturn [\\n\\t\\t\\t(to_native_str(k, errors='replace'),\\n\\t\\t\\t [to_native_str(x, errors='replace') for x in v])\\n\\t\\t\\tfor k, v in self.request.headers.items()\\n\\t\\t]"
  },
  {
    "code": "def _dtype_to_stata_type(dtype, column):\\n    if dtype.type == np.string_:\\n        return chr(dtype.itemsize)\\n    elif dtype.type == np.object_:  \\n        itemsize = max_len_string_array(column.values)\\n        return chr(max(itemsize, 1))\\n    elif dtype == np.float64:\\n        return chr(255)\\n    elif dtype == np.float32:\\n        return chr(254)\\n    elif dtype == np.int32:\\n        return chr(253)\\n    elif dtype == np.int16:\\n        return chr(252)\\n    elif dtype == np.int8:\\n        return chr(251)\\n    else:  \\n        raise ValueError(\"Data type %s not currently understood. \"\\n                         \"Please report an error to the developers.\" % dtype)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Ensure object on stata",
    "fixed_code": "def _dtype_to_stata_type(dtype, column):\\n    if dtype.type == np.string_:\\n        return chr(dtype.itemsize)\\n    elif dtype.type == np.object_:  \\n        itemsize = max_len_string_array(com._ensure_object(column.values))\\n        return chr(max(itemsize, 1))\\n    elif dtype == np.float64:\\n        return chr(255)\\n    elif dtype == np.float32:\\n        return chr(254)\\n    elif dtype == np.int32:\\n        return chr(253)\\n    elif dtype == np.int16:\\n        return chr(252)\\n    elif dtype == np.int8:\\n        return chr(251)\\n    else:  \\n        raise ValueError(\"Data type %s not currently understood. \"\\n                         \"Please report an error to the developers.\" % dtype)"
  },
  {
    "code": "def _should_reindex_frame_op(\\n\\tleft: \"DataFrame\", right, axis, default_axis: int, fill_value, level\\n) -> bool:\\n\\tassert isinstance(left, ABCDataFrame)\\n\\tif not isinstance(right, ABCDataFrame):\\n\\t\\treturn False\\n\\tif fill_value is None and level is None and axis is default_axis:\\n\\t\\tcols = left.columns.intersection(right.columns)\\n\\t\\tif not (cols.equals(left.columns) and cols.equals(right.columns)):\\n\\t\\t\\treturn True\\n\\treturn False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: arithmetic with reindex pow (#32734)",
    "fixed_code": "def _should_reindex_frame_op(\\n\\tleft: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\\n) -> bool:\\n\\tassert isinstance(left, ABCDataFrame)\\n\\tif op is operator.pow or op is rpow:\\n\\t\\treturn False\\n\\tif not isinstance(right, ABCDataFrame):\\n\\t\\treturn False\\n\\tif fill_value is None and level is None and axis is default_axis:\\n\\t\\tcols = left.columns.intersection(right.columns)\\n\\t\\tif not (cols.equals(left.columns) and cols.equals(right.columns)):\\n\\t\\t\\treturn True\\n\\treturn False"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        websphere_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        weblogic_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        type=dict(required=False, type=\"str\",\\n                  choices=[\"static-nat\", \"load-balance\", \"server-load-balance\", \"dns-translation\", \"fqdn\"]),\\n        ssl_server_session_state_type=dict(required=False, type=\"str\", choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        ssl_server_session_state_timeout=dict(required=False, type=\"int\"),\\n        ssl_server_session_state_max=dict(required=False, type=\"int\"),\\n        ssl_server_min_version=dict(required=False, type=\"str\",\\n                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        ssl_server_max_version=dict(required=False, type=\"str\",\\n                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        ssl_server_algorithm=dict(required=False, type=\"str\", choices=[\"high\", \"low\", \"medium\", \"custom\", \"client\"]),\\n        ssl_send_empty_frags=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_pfs=dict(required=False, type=\"str\", choices=[\"require\", \"deny\", \"allow\"]),\\n        ssl_mode=dict(required=False, type=\"str\", choices=[\"half\", \"full\"]),\\n        ssl_min_version=dict(required=False, type=\"str\", choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        ssl_max_version=dict(required=False, type=\"str\", choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        ssl_http_match_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_http_location_conversion=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hsts_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hsts_age=dict(required=False, type=\"int\"),\\n        ssl_hsts=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hpkp_report_uri=dict(required=False, type=\"str\"),\\n        ssl_hpkp_primary=dict(required=False, type=\"str\"),\\n        ssl_hpkp_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hpkp_backup=dict(required=False, type=\"str\"),\\n        ssl_hpkp_age=dict(required=False, type=\"int\"),\\n        ssl_hpkp=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"report-only\"]),\\n        ssl_dh_bits=dict(required=False, type=\"str\", choices=[\"768\", \"1024\", \"1536\", \"2048\", \"3072\", \"4096\"]),\\n        ssl_client_session_state_type=dict(required=False, type=\"str\", choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        ssl_client_session_state_timeout=dict(required=False, type=\"int\"),\\n        ssl_client_session_state_max=dict(required=False, type=\"int\"),\\n        ssl_client_renegotiation=dict(required=False, type=\"str\", choices=[\"deny\", \"allow\", \"secure\"]),\\n        ssl_client_fallback=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_certificate=dict(required=False, type=\"str\"),\\n        ssl_algorithm=dict(required=False, type=\"str\", choices=[\"high\", \"medium\", \"low\", \"custom\"]),\\n        srcintf_filter=dict(required=False, type=\"str\"),\\n        src_filter=dict(required=False, type=\"str\"),\\n        service=dict(required=False, type=\"str\"),\\n        server_type=dict(required=False, type=\"str\",\\n                         choices=[\"http\", \"https\", \"ssl\", \"tcp\", \"udp\", \"ip\", \"imaps\", \"pop3s\", \"smtps\"]),\\n        protocol=dict(required=False, type=\"str\", choices=[\"tcp\", \"udp\", \"sctp\", \"icmp\"]),\\n        portmapping_type=dict(required=False, type=\"str\", choices=[\"1-to-1\", \"m-to-n\"]),\\n        portforward=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        persistence=dict(required=False, type=\"str\", choices=[\"none\", \"http-cookie\", \"ssl-session-id\"]),\\n        outlook_web_access=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        nat_source_vip=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        name=dict(required=False, type=\"str\"),\\n        monitor=dict(required=False, type=\"str\"),\\n        max_embryonic_connections=dict(required=False, type=\"int\"),\\n        mappedport=dict(required=False, type=\"str\"),\\n        mappedip=dict(required=False, type=\"str\"),\\n        mapped_addr=dict(required=False, type=\"str\"),\\n        ldb_method=dict(required=False, type=\"str\",\\n                        choices=[\"static\", \"round-robin\", \"weighted\", \"least-session\", \"least-rtt\", \"first-alive\",\\n                                 \"http-host\"]),\\n        https_cookie_secure=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_multiplex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_ip_header_name=dict(required=False, type=\"str\"),\\n        http_ip_header=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_cookie_share=dict(required=False, type=\"str\", choices=[\"disable\", \"same-ip\"]),\\n        http_cookie_path=dict(required=False, type=\"str\"),\\n        http_cookie_generation=dict(required=False, type=\"int\"),\\n        http_cookie_domain_from_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_cookie_domain=dict(required=False, type=\"str\"),\\n        http_cookie_age=dict(required=False, type=\"int\"),\\n        gratuitous_arp_interval=dict(required=False, type=\"int\"),\\n        extport=dict(required=False, type=\"str\"),\\n        extip=dict(required=False, type=\"str\"),\\n        extintf=dict(required=False, type=\"str\"),\\n        extaddr=dict(required=False, type=\"str\"),\\n        dns_mapping_ttl=dict(required=False, type=\"int\"),\\n        comment=dict(required=False, type=\"str\"),\\n        color=dict(required=False, type=\"int\"),\\n        arp_reply=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping=dict(required=False, type=\"list\"),\\n        dynamic_mapping_arp_reply=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_color=dict(required=False, type=\"int\"),\\n        dynamic_mapping_comment=dict(required=False, type=\"str\"),\\n        dynamic_mapping_dns_mapping_ttl=dict(required=False, type=\"int\"),\\n        dynamic_mapping_extaddr=dict(required=False, type=\"str\"),\\n        dynamic_mapping_extintf=dict(required=False, type=\"str\"),\\n        dynamic_mapping_extip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_extport=dict(required=False, type=\"str\"),\\n        dynamic_mapping_gratuitous_arp_interval=dict(required=False, type=\"int\"),\\n        dynamic_mapping_http_cookie_age=dict(required=False, type=\"int\"),\\n        dynamic_mapping_http_cookie_domain=dict(required=False, type=\"str\"),\\n        dynamic_mapping_http_cookie_domain_from_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_http_cookie_generation=dict(required=False, type=\"int\"),\\n        dynamic_mapping_http_cookie_path=dict(required=False, type=\"str\"),\\n        dynamic_mapping_http_cookie_share=dict(required=False, type=\"str\", choices=[\"disable\", \"same-ip\"]),\\n        dynamic_mapping_http_ip_header=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_http_ip_header_name=dict(required=False, type=\"str\"),\\n        dynamic_mapping_http_multiplex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_https_cookie_secure=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ldb_method=dict(required=False, type=\"str\", choices=[\"static\",\\n                                                                             \"round-robin\",\\n                                                                             \"weighted\",\\n                                                                             \"least-session\",\\n                                                                             \"least-rtt\",\\n                                                                             \"first-alive\",\\n                                                                             \"http-host\"]),\\n        dynamic_mapping_mapped_addr=dict(required=False, type=\"str\"),\\n        dynamic_mapping_mappedip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_mappedport=dict(required=False, type=\"str\"),\\n        dynamic_mapping_max_embryonic_connections=dict(required=False, type=\"int\"),\\n        dynamic_mapping_monitor=dict(required=False, type=\"str\"),\\n        dynamic_mapping_nat_source_vip=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_outlook_web_access=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_persistence=dict(required=False, type=\"str\", choices=[\"none\", \"http-cookie\", \"ssl-session-id\"]),\\n        dynamic_mapping_portforward=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_portmapping_type=dict(required=False, type=\"str\", choices=[\"1-to-1\", \"m-to-n\"]),\\n        dynamic_mapping_protocol=dict(required=False, type=\"str\", choices=[\"tcp\", \"udp\", \"sctp\", \"icmp\"]),\\n        dynamic_mapping_server_type=dict(required=False, type=\"str\",\\n                                         choices=[\"http\", \"https\", \"ssl\", \"tcp\", \"udp\", \"ip\", \"imaps\", \"pop3s\",\\n                                                  \"smtps\"]),\\n        dynamic_mapping_service=dict(required=False, type=\"str\"),\\n        dynamic_mapping_src_filter=dict(required=False, type=\"str\"),\\n        dynamic_mapping_srcintf_filter=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_algorithm=dict(required=False, type=\"str\", choices=[\"high\", \"medium\", \"low\", \"custom\"]),\\n        dynamic_mapping_ssl_certificate=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_client_fallback=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_client_renegotiation=dict(required=False, type=\"str\", choices=[\"deny\", \"allow\", \"secure\"]),\\n        dynamic_mapping_ssl_client_session_state_max=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_client_session_state_timeout=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_client_session_state_type=dict(required=False, type=\"str\",\\n                                                           choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        dynamic_mapping_ssl_dh_bits=dict(required=False, type=\"str\",\\n                                         choices=[\"768\", \"1024\", \"1536\", \"2048\", \"3072\", \"4096\"]),\\n        dynamic_mapping_ssl_hpkp=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"report-only\"]),\\n        dynamic_mapping_ssl_hpkp_age=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_hpkp_backup=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_hpkp_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_hpkp_primary=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_hpkp_report_uri=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_hsts=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_hsts_age=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_hsts_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_http_location_conversion=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_http_match_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_max_version=dict(required=False, type=\"str\",\\n                                             choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        dynamic_mapping_ssl_min_version=dict(required=False, type=\"str\",\\n                                             choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        dynamic_mapping_ssl_mode=dict(required=False, type=\"str\", choices=[\"half\", \"full\"]),\\n        dynamic_mapping_ssl_pfs=dict(required=False, type=\"str\", choices=[\"require\", \"deny\", \"allow\"]),\\n        dynamic_mapping_ssl_send_empty_frags=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_server_algorithm=dict(required=False, type=\"str\",\\n                                                  choices=[\"high\", \"low\", \"medium\", \"custom\", \"client\"]),\\n        dynamic_mapping_ssl_server_max_version=dict(required=False, type=\"str\",\\n                                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        dynamic_mapping_ssl_server_min_version=dict(required=False, type=\"str\",\\n                                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        dynamic_mapping_ssl_server_session_state_max=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_server_session_state_timeout=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_server_session_state_type=dict(required=False, type=\"str\",\\n                                                           choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        dynamic_mapping_type=dict(required=False, type=\"str\",\\n                                  choices=[\"static-nat\", \"load-balance\", \"server-load-balance\", \"dns-translation\",\\n                                           \"fqdn\"]),\\n        dynamic_mapping_weblogic_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_websphere_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_realservers_client_ip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_healthcheck=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"vip\"]),\\n        dynamic_mapping_realservers_holddown_interval=dict(required=False, type=\"int\"),\\n        dynamic_mapping_realservers_http_host=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_ip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_max_connections=dict(required=False, type=\"int\"),\\n        dynamic_mapping_realservers_monitor=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_port=dict(required=False, type=\"int\"),\\n        dynamic_mapping_realservers_seq=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_status=dict(required=False, type=\"str\", choices=[\"active\", \"standby\", \"disable\"]),\\n        dynamic_mapping_realservers_weight=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_cipher_suites_cipher=dict(required=False,\\n                                                      type=\"str\",\\n                                                      choices=[\"TLS-RSA-WITH-RC4-128-MD5\",\\n                                                               \"TLS-RSA-WITH-RC4-128-SHA\",\\n                                                               \"TLS-RSA-WITH-DES-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-SEED-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-DHE-RSA-WITH-DES-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-SEED-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-RC4-128-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-SEED-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-DHE-DSS-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-DES-CBC-SHA\"]),\\n        dynamic_mapping_ssl_cipher_suites_versions=dict(required=False, type=\"str\",\\n                                                        choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        realservers=dict(required=False, type=\"list\"),\\n        realservers_client_ip=dict(required=False, type=\"str\"),\\n        realservers_healthcheck=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"vip\"]),\\n        realservers_holddown_interval=dict(required=False, type=\"int\"),\\n        realservers_http_host=dict(required=False, type=\"str\"),\\n        realservers_ip=dict(required=False, type=\"str\"),\\n        realservers_max_connections=dict(required=False, type=\"int\"),\\n        realservers_monitor=dict(required=False, type=\"str\"),\\n        realservers_port=dict(required=False, type=\"int\"),\\n        realservers_seq=dict(required=False, type=\"str\"),\\n        realservers_status=dict(required=False, type=\"str\", choices=[\"active\", \"standby\", \"disable\"]),\\n        realservers_weight=dict(required=False, type=\"int\"),\\n        ssl_cipher_suites=dict(required=False, type=\"list\"),\\n        ssl_cipher_suites_cipher=dict(required=False,\\n                                      type=\"str\",\\n                                      choices=[\"TLS-RSA-WITH-RC4-128-MD5\",\\n                                               \"TLS-RSA-WITH-RC4-128-SHA\",\\n                                               \"TLS-RSA-WITH-DES-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-AES-256-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-SEED-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-DHE-RSA-WITH-DES-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-SEED-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-RC4-128-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-RSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-RSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-SEED-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-DHE-DSS-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-DES-CBC-SHA\"]),\\n        ssl_cipher_suites_versions=dict(required=False, type=\"str\",\\n                                        choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        ssl_server_cipher_suites=dict(required=False, type=\"list\"),\\n        ssl_server_cipher_suites_cipher=dict(required=False,\\n                                             type=\"str\",\\n                                             choices=[\"TLS-RSA-WITH-RC4-128-MD5\",\\n                                                      \"TLS-RSA-WITH-RC4-128-SHA\",\\n                                                      \"TLS-RSA-WITH-DES-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-AES-128-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-AES-256-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-SEED-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                      \"TLS-DHE-RSA-WITH-DES-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-SEED-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                      \"TLS-ECDHE-RSA-WITH-RC4-128-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                      \"TLS-ECDHE-ECDSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                      \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA\",",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(type=\"str\", default=\"root\"),\\n        mode=dict(choices=[\"add\", \"set\", \"delete\", \"update\"], type=\"str\", default=\"add\"),\\n        websphere_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        weblogic_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        type=dict(required=False, type=\"str\",\\n                  choices=[\"static-nat\", \"load-balance\", \"server-load-balance\", \"dns-translation\", \"fqdn\"]),\\n        ssl_server_session_state_type=dict(required=False, type=\"str\", choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        ssl_server_session_state_timeout=dict(required=False, type=\"int\"),\\n        ssl_server_session_state_max=dict(required=False, type=\"int\"),\\n        ssl_server_min_version=dict(required=False, type=\"str\",\\n                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        ssl_server_max_version=dict(required=False, type=\"str\",\\n                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        ssl_server_algorithm=dict(required=False, type=\"str\", choices=[\"high\", \"low\", \"medium\", \"custom\", \"client\"]),\\n        ssl_send_empty_frags=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_pfs=dict(required=False, type=\"str\", choices=[\"require\", \"deny\", \"allow\"]),\\n        ssl_mode=dict(required=False, type=\"str\", choices=[\"half\", \"full\"]),\\n        ssl_min_version=dict(required=False, type=\"str\", choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        ssl_max_version=dict(required=False, type=\"str\", choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        ssl_http_match_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_http_location_conversion=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hsts_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hsts_age=dict(required=False, type=\"int\"),\\n        ssl_hsts=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hpkp_report_uri=dict(required=False, type=\"str\"),\\n        ssl_hpkp_primary=dict(required=False, type=\"str\"),\\n        ssl_hpkp_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_hpkp_backup=dict(required=False, type=\"str\"),\\n        ssl_hpkp_age=dict(required=False, type=\"int\"),\\n        ssl_hpkp=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"report-only\"]),\\n        ssl_dh_bits=dict(required=False, type=\"str\", choices=[\"768\", \"1024\", \"1536\", \"2048\", \"3072\", \"4096\"]),\\n        ssl_client_session_state_type=dict(required=False, type=\"str\", choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        ssl_client_session_state_timeout=dict(required=False, type=\"int\"),\\n        ssl_client_session_state_max=dict(required=False, type=\"int\"),\\n        ssl_client_renegotiation=dict(required=False, type=\"str\", choices=[\"deny\", \"allow\", \"secure\"]),\\n        ssl_client_fallback=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        ssl_certificate=dict(required=False, type=\"str\"),\\n        ssl_algorithm=dict(required=False, type=\"str\", choices=[\"high\", \"medium\", \"low\", \"custom\"]),\\n        srcintf_filter=dict(required=False, type=\"str\"),\\n        src_filter=dict(required=False, type=\"str\"),\\n        service=dict(required=False, type=\"str\"),\\n        server_type=dict(required=False, type=\"str\",\\n                         choices=[\"http\", \"https\", \"ssl\", \"tcp\", \"udp\", \"ip\", \"imaps\", \"pop3s\", \"smtps\"]),\\n        protocol=dict(required=False, type=\"str\", choices=[\"tcp\", \"udp\", \"sctp\", \"icmp\"]),\\n        portmapping_type=dict(required=False, type=\"str\", choices=[\"1-to-1\", \"m-to-n\"]),\\n        portforward=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        persistence=dict(required=False, type=\"str\", choices=[\"none\", \"http-cookie\", \"ssl-session-id\"]),\\n        outlook_web_access=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        nat_source_vip=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        name=dict(required=False, type=\"str\"),\\n        monitor=dict(required=False, type=\"str\"),\\n        max_embryonic_connections=dict(required=False, type=\"int\"),\\n        mappedport=dict(required=False, type=\"str\"),\\n        mappedip=dict(required=False, type=\"str\"),\\n        mapped_addr=dict(required=False, type=\"str\"),\\n        ldb_method=dict(required=False, type=\"str\",\\n                        choices=[\"static\", \"round-robin\", \"weighted\", \"least-session\", \"least-rtt\", \"first-alive\",\\n                                 \"http-host\"]),\\n        https_cookie_secure=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_multiplex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_ip_header_name=dict(required=False, type=\"str\"),\\n        http_ip_header=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_cookie_share=dict(required=False, type=\"str\", choices=[\"disable\", \"same-ip\"]),\\n        http_cookie_path=dict(required=False, type=\"str\"),\\n        http_cookie_generation=dict(required=False, type=\"int\"),\\n        http_cookie_domain_from_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        http_cookie_domain=dict(required=False, type=\"str\"),\\n        http_cookie_age=dict(required=False, type=\"int\"),\\n        gratuitous_arp_interval=dict(required=False, type=\"int\"),\\n        extport=dict(required=False, type=\"str\"),\\n        extip=dict(required=False, type=\"str\"),\\n        extintf=dict(required=False, type=\"str\"),\\n        extaddr=dict(required=False, type=\"str\"),\\n        dns_mapping_ttl=dict(required=False, type=\"int\"),\\n        comment=dict(required=False, type=\"str\"),\\n        color=dict(required=False, type=\"int\"),\\n        arp_reply=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping=dict(required=False, type=\"list\"),\\n        dynamic_mapping_arp_reply=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_color=dict(required=False, type=\"int\"),\\n        dynamic_mapping_comment=dict(required=False, type=\"str\"),\\n        dynamic_mapping_dns_mapping_ttl=dict(required=False, type=\"int\"),\\n        dynamic_mapping_extaddr=dict(required=False, type=\"str\"),\\n        dynamic_mapping_extintf=dict(required=False, type=\"str\"),\\n        dynamic_mapping_extip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_extport=dict(required=False, type=\"str\"),\\n        dynamic_mapping_gratuitous_arp_interval=dict(required=False, type=\"int\"),\\n        dynamic_mapping_http_cookie_age=dict(required=False, type=\"int\"),\\n        dynamic_mapping_http_cookie_domain=dict(required=False, type=\"str\"),\\n        dynamic_mapping_http_cookie_domain_from_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_http_cookie_generation=dict(required=False, type=\"int\"),\\n        dynamic_mapping_http_cookie_path=dict(required=False, type=\"str\"),\\n        dynamic_mapping_http_cookie_share=dict(required=False, type=\"str\", choices=[\"disable\", \"same-ip\"]),\\n        dynamic_mapping_http_ip_header=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_http_ip_header_name=dict(required=False, type=\"str\"),\\n        dynamic_mapping_http_multiplex=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_https_cookie_secure=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ldb_method=dict(required=False, type=\"str\", choices=[\"static\",\\n                                                                             \"round-robin\",\\n                                                                             \"weighted\",\\n                                                                             \"least-session\",\\n                                                                             \"least-rtt\",\\n                                                                             \"first-alive\",\\n                                                                             \"http-host\"]),\\n        dynamic_mapping_mapped_addr=dict(required=False, type=\"str\"),\\n        dynamic_mapping_mappedip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_mappedport=dict(required=False, type=\"str\"),\\n        dynamic_mapping_max_embryonic_connections=dict(required=False, type=\"int\"),\\n        dynamic_mapping_monitor=dict(required=False, type=\"str\"),\\n        dynamic_mapping_nat_source_vip=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_outlook_web_access=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_persistence=dict(required=False, type=\"str\", choices=[\"none\", \"http-cookie\", \"ssl-session-id\"]),\\n        dynamic_mapping_portforward=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_portmapping_type=dict(required=False, type=\"str\", choices=[\"1-to-1\", \"m-to-n\"]),\\n        dynamic_mapping_protocol=dict(required=False, type=\"str\", choices=[\"tcp\", \"udp\", \"sctp\", \"icmp\"]),\\n        dynamic_mapping_server_type=dict(required=False, type=\"str\",\\n                                         choices=[\"http\", \"https\", \"ssl\", \"tcp\", \"udp\", \"ip\", \"imaps\", \"pop3s\",\\n                                                  \"smtps\"]),\\n        dynamic_mapping_service=dict(required=False, type=\"str\"),\\n        dynamic_mapping_src_filter=dict(required=False, type=\"str\"),\\n        dynamic_mapping_srcintf_filter=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_algorithm=dict(required=False, type=\"str\", choices=[\"high\", \"medium\", \"low\", \"custom\"]),\\n        dynamic_mapping_ssl_certificate=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_client_fallback=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_client_renegotiation=dict(required=False, type=\"str\", choices=[\"deny\", \"allow\", \"secure\"]),\\n        dynamic_mapping_ssl_client_session_state_max=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_client_session_state_timeout=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_client_session_state_type=dict(required=False, type=\"str\",\\n                                                           choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        dynamic_mapping_ssl_dh_bits=dict(required=False, type=\"str\",\\n                                         choices=[\"768\", \"1024\", \"1536\", \"2048\", \"3072\", \"4096\"]),\\n        dynamic_mapping_ssl_hpkp=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"report-only\"]),\\n        dynamic_mapping_ssl_hpkp_age=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_hpkp_backup=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_hpkp_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_hpkp_primary=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_hpkp_report_uri=dict(required=False, type=\"str\"),\\n        dynamic_mapping_ssl_hsts=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_hsts_age=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_hsts_include_subdomains=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_http_location_conversion=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_http_match_host=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_max_version=dict(required=False, type=\"str\",\\n                                             choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        dynamic_mapping_ssl_min_version=dict(required=False, type=\"str\",\\n                                             choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        dynamic_mapping_ssl_mode=dict(required=False, type=\"str\", choices=[\"half\", \"full\"]),\\n        dynamic_mapping_ssl_pfs=dict(required=False, type=\"str\", choices=[\"require\", \"deny\", \"allow\"]),\\n        dynamic_mapping_ssl_send_empty_frags=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_ssl_server_algorithm=dict(required=False, type=\"str\",\\n                                                  choices=[\"high\", \"low\", \"medium\", \"custom\", \"client\"]),\\n        dynamic_mapping_ssl_server_max_version=dict(required=False, type=\"str\",\\n                                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        dynamic_mapping_ssl_server_min_version=dict(required=False, type=\"str\",\\n                                                    choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\", \"client\"]),\\n        dynamic_mapping_ssl_server_session_state_max=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_server_session_state_timeout=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_server_session_state_type=dict(required=False, type=\"str\",\\n                                                           choices=[\"disable\", \"time\", \"count\", \"both\"]),\\n        dynamic_mapping_type=dict(required=False, type=\"str\",\\n                                  choices=[\"static-nat\", \"load-balance\", \"server-load-balance\", \"dns-translation\",\\n                                           \"fqdn\"]),\\n        dynamic_mapping_weblogic_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_websphere_server=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\"]),\\n        dynamic_mapping_realservers_client_ip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_healthcheck=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"vip\"]),\\n        dynamic_mapping_realservers_holddown_interval=dict(required=False, type=\"int\"),\\n        dynamic_mapping_realservers_http_host=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_ip=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_max_connections=dict(required=False, type=\"int\"),\\n        dynamic_mapping_realservers_monitor=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_port=dict(required=False, type=\"int\"),\\n        dynamic_mapping_realservers_seq=dict(required=False, type=\"str\"),\\n        dynamic_mapping_realservers_status=dict(required=False, type=\"str\", choices=[\"active\", \"standby\", \"disable\"]),\\n        dynamic_mapping_realservers_weight=dict(required=False, type=\"int\"),\\n        dynamic_mapping_ssl_cipher_suites_cipher=dict(required=False,\\n                                                      type=\"str\",\\n                                                      choices=[\"TLS-RSA-WITH-RC4-128-MD5\",\\n                                                               \"TLS-RSA-WITH-RC4-128-SHA\",\\n                                                               \"TLS-RSA-WITH-DES-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-SEED-CBC-SHA\",\\n                                                               \"TLS-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-DHE-RSA-WITH-DES-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-SEED-CBC-SHA\",\\n                                                               \"TLS-DHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-RC4-128-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-ECDHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-DHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                               \"TLS-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-SEED-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-DHE-DSS-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                               \"TLS-DHE-DSS-WITH-3DES-EDE-CBC-SHA\",\\n                                                               \"TLS-DHE-DSS-WITH-DES-CBC-SHA\"]),\\n        dynamic_mapping_ssl_cipher_suites_versions=dict(required=False, type=\"str\",\\n                                                        choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        realservers=dict(required=False, type=\"list\"),\\n        realservers_client_ip=dict(required=False, type=\"str\"),\\n        realservers_healthcheck=dict(required=False, type=\"str\", choices=[\"disable\", \"enable\", \"vip\"]),\\n        realservers_holddown_interval=dict(required=False, type=\"int\"),\\n        realservers_http_host=dict(required=False, type=\"str\"),\\n        realservers_ip=dict(required=False, type=\"str\"),\\n        realservers_max_connections=dict(required=False, type=\"int\"),\\n        realservers_monitor=dict(required=False, type=\"str\"),\\n        realservers_port=dict(required=False, type=\"int\"),\\n        realservers_seq=dict(required=False, type=\"str\"),\\n        realservers_status=dict(required=False, type=\"str\", choices=[\"active\", \"standby\", \"disable\"]),\\n        realservers_weight=dict(required=False, type=\"int\"),\\n        ssl_cipher_suites=dict(required=False, type=\"list\"),\\n        ssl_cipher_suites_cipher=dict(required=False,\\n                                      type=\"str\",\\n                                      choices=[\"TLS-RSA-WITH-RC4-128-MD5\",\\n                                               \"TLS-RSA-WITH-RC4-128-SHA\",\\n                                               \"TLS-RSA-WITH-DES-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-AES-256-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-SEED-CBC-SHA\",\\n                                               \"TLS-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-DHE-RSA-WITH-DES-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-SEED-CBC-SHA\",\\n                                               \"TLS-DHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-RC4-128-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-ECDHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-DHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-AES-256-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-RSA-WITH-AES-128-GCM-SHA256\",\\n                                               \"TLS-RSA-WITH-AES-256-GCM-SHA384\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-SEED-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-DHE-DSS-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-128-CBC-SHA256\",\\n                                               \"TLS-ECDHE-ECDSA-WITH-ARIA-256-CBC-SHA384\",\\n                                               \"TLS-DHE-DSS-WITH-3DES-EDE-CBC-SHA\",\\n                                               \"TLS-DHE-DSS-WITH-DES-CBC-SHA\"]),\\n        ssl_cipher_suites_versions=dict(required=False, type=\"str\",\\n                                        choices=[\"ssl-3.0\", \"tls-1.0\", \"tls-1.1\", \"tls-1.2\"]),\\n        ssl_server_cipher_suites=dict(required=False, type=\"list\"),\\n        ssl_server_cipher_suites_cipher=dict(required=False,\\n                                             type=\"str\",\\n                                             choices=[\"TLS-RSA-WITH-RC4-128-MD5\",\\n                                                      \"TLS-RSA-WITH-RC4-128-SHA\",\\n                                                      \"TLS-RSA-WITH-DES-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-AES-128-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-AES-256-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-SEED-CBC-SHA\",\\n                                                      \"TLS-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                      \"TLS-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                      \"TLS-DHE-RSA-WITH-DES-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-128-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-256-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-128-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-CAMELLIA-256-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-SEED-CBC-SHA\",\\n                                                      \"TLS-DHE-RSA-WITH-ARIA-128-CBC-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-ARIA-256-CBC-SHA384\",\\n                                                      \"TLS-ECDHE-RSA-WITH-RC4-128-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-3DES-EDE-CBC-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-AES-128-CBC-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-AES-256-CBC-SHA\",\\n                                                      \"TLS-ECDHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                      \"TLS-ECDHE-ECDSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-CHACHA20-POLY1305-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-128-GCM-SHA256\",\\n                                                      \"TLS-DHE-RSA-WITH-AES-256-GCM-SHA384\",\\n                                                      \"TLS-DHE-DSS-WITH-AES-128-CBC-SHA\","
  },
  {
    "code": "def format_meter(n, total, elapsed, ncols=None, prefix='', ascii=False,\\n\\t\\t\\t\\t\\t unit='it', unit_scale=False, rate=None, bar_format=None,\\n\\t\\t\\t\\t\\t postfix=None, unit_divisor=1000, **extra_kwargs):\\n\\t\\tif total and n > total:\\n\\t\\t\\ttotal = None\\n\\t\\tif unit_scale and unit_scale not in (True, 1):\\n\\t\\t\\tif total:\\n\\t\\t\\t\\ttotal *= unit_scale\\n\\t\\t\\tn *= unit_scale\\n\\t\\t\\tif rate:\\n\\t\\t\\t\\trate *= unit_scale  \\n\\t\\t\\tunit_scale = False\\n\\t\\tformat_interval = tqdm.format_interval\\n\\t\\telapsed_str = format_interval(elapsed)\\n\\t\\tif rate is None and elapsed:\\n\\t\\t\\trate = n / elapsed\\n\\t\\tinv_rate = 1 / rate if rate else None\\n\\t\\tformat_sizeof = tqdm.format_sizeof\\n\\t\\trate_noinv_fmt = ((format_sizeof(rate) if unit_scale else\\n\\t\\t\\t\\t\\t\\t   '{0:5.2f}'.format(rate))\\n\\t\\t\\t\\t\\t\\t  if rate else '?') + unit + '/s'\\n\\t\\trate_inv_fmt = ((format_sizeof(inv_rate) if unit_scale else\\n\\t\\t\\t\\t\\t\\t '{0:5.2f}'.format(inv_rate))\\n\\t\\t\\t\\t\\t\\tif inv_rate else '?') + 's/' + unit\\n\\t\\trate_fmt = rate_inv_fmt if inv_rate and inv_rate > 1 else rate_noinv_fmt\\n\\t\\tif unit_scale:\\n\\t\\t\\tn_fmt = format_sizeof(n, divisor=unit_divisor)\\n\\t\\t\\ttotal_fmt = format_sizeof(total, divisor=unit_divisor) \\\\n\\t\\t\\t\\tif total else None\\n\\t\\telse:\\n\\t\\t\\tn_fmt = str(n)\\n\\t\\t\\ttotal_fmt = str(total)\\n\\t\\ttry:\\n\\t\\t\\tpostfix = ', ' + postfix if postfix else ''\\n\\t\\texcept TypeError:\\n\\t\\t\\tpass\\n\\t\\tif total:\\n\\t\\t\\tfrac = n / total\\n\\t\\t\\tpercentage = frac * 100\\n\\t\\t\\tremaining = (total - n) / rate if rate else 0\\n\\t\\t\\tremaining_str = format_interval(remaining) if rate else '?'\\n\\t\\t\\tif prefix:\\n\\t\\t\\t\\tbool_prefix_colon_already = (prefix[-2:] == \": \")\\n\\t\\t\\t\\tl_bar = prefix if bool_prefix_colon_already else prefix + \": \"\\n\\t\\t\\telse:\\n\\t\\t\\t\\tl_bar = ''\\n\\t\\t\\tl_bar += '{0:3.0f}%|'.format(percentage)\\n\\t\\t\\tr_bar = '| {0}/{1} [{2}<{3}, {4}{5}]'.format(\\n\\t\\t\\t\\tn_fmt, total_fmt, elapsed_str, remaining_str, rate_fmt, postfix)\\n\\t\\t\\tif ncols == 0:\\n\\t\\t\\t\\treturn l_bar[:-1] + r_bar[1:]\\n\\t\\t\\tif bar_format:\\n\\t\\t\\t\\tformat_dict = dict(\\n\\t\\t\\t\\t\\tn=n, n_fmt=n_fmt, total=total, total_fmt=total_fmt,\\n\\t\\t\\t\\t\\tpercentage=percentage,\\n\\t\\t\\t\\t\\trate=inv_rate if inv_rate and inv_rate > 1 else rate,\\n\\t\\t\\t\\t\\trate_fmt=rate_fmt, rate_noinv=rate,\\n\\t\\t\\t\\t\\trate_noinv_fmt=rate_noinv_fmt, rate_inv=inv_rate,\\n\\t\\t\\t\\t\\trate_inv_fmt=rate_inv_fmt,\\n\\t\\t\\t\\t\\telapsed=elapsed_str, elapsed_s=elapsed,\\n\\t\\t\\t\\t\\tremaining=remaining_str, remaining_s=remaining,\\n\\t\\t\\t\\t\\tl_bar=l_bar, r_bar=r_bar,\\n\\t\\t\\t\\t\\tdesc=prefix or '', postfix=postfix, unit=unit,\\n\\t\\t\\t\\t\\t**extra_kwargs)\\n\\t\\t\\t\\tif not prefix:\\n\\t\\t\\t\\t\\tbar_format = bar_format.replace(\"{desc}: \", '')\\n\\t\\t\\t\\tif '{bar}' in bar_format:\\n\\t\\t\\t\\t\\tl_bar_user, r_bar_user = bar_format.split('{bar}')\\n\\t\\t\\t\\t\\tl_bar = l_bar_user.format(**format_dict)\\n\\t\\t\\t\\t\\tr_bar = r_bar_user.format(**format_dict)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\treturn bar_format.format(**format_dict)\\n\\t\\t\\tif ncols:\\n\\t\\t\\t\\tN_BARS = max(1, ncols - len(RE_ANSI.sub('', l_bar + r_bar)))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tN_BARS = 10\\n\\t\\t\\tif ascii:\\n\\t\\t\\t\\tbar_length, frac_bar_length = divmod(\\n\\t\\t\\t\\t\\tint(frac * N_BARS * 10), 10)\\n\\t\\t\\t\\tbar = '\\n\\t\\t\\t\\tfrac_bar = chr(48 + frac_bar_length) if frac_bar_length \\\\n\\t\\t\\t\\t\\telse ' '\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbar_length, frac_bar_length = divmod(int(frac * N_BARS * 8), 8)\\n\\t\\t\\t\\tbar = _unich(0x2588) * bar_length\\n\\t\\t\\t\\tfrac_bar = _unich(0x2590 - frac_bar_length) \\\\n\\t\\t\\t\\t\\tif frac_bar_length else ' '\\n\\t\\t\\tif bar_length < N_BARS:\\n\\t\\t\\t\\tfull_bar = bar + frac_bar + \\\\n\\t\\t\\t\\t\\t' ' * max(N_BARS - bar_length - 1, 0)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfull_bar = bar + \\\\n\\t\\t\\t\\t\\t' ' * max(N_BARS - bar_length, 0)\\n\\t\\t\\treturn l_bar + full_bar + r_bar\\n\\t\\telse:\\n\\t\\t\\treturn ((prefix + \": \") if prefix else '') + \\\\n\\t\\t\\t\\t'{0}{1} [{2}, {3}{4}]'.format(\\n\\t\\t\\t\\t\\tn_fmt, unit, elapsed_str, rate_fmt, postfix)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_meter(n, total, elapsed, ncols=None, prefix='', ascii=False,\\n\\t\\t\\t\\t\\t unit='it', unit_scale=False, rate=None, bar_format=None,\\n\\t\\t\\t\\t\\t postfix=None, unit_divisor=1000, **extra_kwargs):\\n\\t\\tif total and n > total:\\n\\t\\t\\ttotal = None\\n\\t\\tif unit_scale and unit_scale not in (True, 1):\\n\\t\\t\\tif total:\\n\\t\\t\\t\\ttotal *= unit_scale\\n\\t\\t\\tn *= unit_scale\\n\\t\\t\\tif rate:\\n\\t\\t\\t\\trate *= unit_scale  \\n\\t\\t\\tunit_scale = False\\n\\t\\tformat_interval = tqdm.format_interval\\n\\t\\telapsed_str = format_interval(elapsed)\\n\\t\\tif rate is None and elapsed:\\n\\t\\t\\trate = n / elapsed\\n\\t\\tinv_rate = 1 / rate if rate else None\\n\\t\\tformat_sizeof = tqdm.format_sizeof\\n\\t\\trate_noinv_fmt = ((format_sizeof(rate) if unit_scale else\\n\\t\\t\\t\\t\\t\\t   '{0:5.2f}'.format(rate))\\n\\t\\t\\t\\t\\t\\t  if rate else '?') + unit + '/s'\\n\\t\\trate_inv_fmt = ((format_sizeof(inv_rate) if unit_scale else\\n\\t\\t\\t\\t\\t\\t '{0:5.2f}'.format(inv_rate))\\n\\t\\t\\t\\t\\t\\tif inv_rate else '?') + 's/' + unit\\n\\t\\trate_fmt = rate_inv_fmt if inv_rate and inv_rate > 1 else rate_noinv_fmt\\n\\t\\tif unit_scale:\\n\\t\\t\\tn_fmt = format_sizeof(n, divisor=unit_divisor)\\n\\t\\t\\ttotal_fmt = format_sizeof(total, divisor=unit_divisor) \\\\n\\t\\t\\t\\tif total else None\\n\\t\\telse:\\n\\t\\t\\tn_fmt = str(n)\\n\\t\\t\\ttotal_fmt = str(total)\\n\\t\\ttry:\\n\\t\\t\\tpostfix = ', ' + postfix if postfix else ''\\n\\t\\texcept TypeError:\\n\\t\\t\\tpass\\n\\t\\tif total:\\n\\t\\t\\tfrac = n / total\\n\\t\\t\\tpercentage = frac * 100\\n\\t\\t\\tremaining = (total - n) / rate if rate else 0\\n\\t\\t\\tremaining_str = format_interval(remaining) if rate else '?'\\n\\t\\t\\tif prefix:\\n\\t\\t\\t\\tbool_prefix_colon_already = (prefix[-2:] == \": \")\\n\\t\\t\\t\\tl_bar = prefix if bool_prefix_colon_already else prefix + \": \"\\n\\t\\t\\telse:\\n\\t\\t\\t\\tl_bar = ''\\n\\t\\t\\tl_bar += '{0:3.0f}%|'.format(percentage)\\n\\t\\t\\tr_bar = '| {0}/{1} [{2}<{3}, {4}{5}]'.format(\\n\\t\\t\\t\\tn_fmt, total_fmt, elapsed_str, remaining_str, rate_fmt, postfix)\\n\\t\\t\\tif ncols == 0:\\n\\t\\t\\t\\treturn l_bar[:-1] + r_bar[1:]\\n\\t\\t\\tif bar_format:\\n\\t\\t\\t\\tformat_dict = dict(\\n\\t\\t\\t\\t\\tn=n, n_fmt=n_fmt, total=total, total_fmt=total_fmt,\\n\\t\\t\\t\\t\\tpercentage=percentage,\\n\\t\\t\\t\\t\\trate=inv_rate if inv_rate and inv_rate > 1 else rate,\\n\\t\\t\\t\\t\\trate_fmt=rate_fmt, rate_noinv=rate,\\n\\t\\t\\t\\t\\trate_noinv_fmt=rate_noinv_fmt, rate_inv=inv_rate,\\n\\t\\t\\t\\t\\trate_inv_fmt=rate_inv_fmt,\\n\\t\\t\\t\\t\\telapsed=elapsed_str, elapsed_s=elapsed,\\n\\t\\t\\t\\t\\tremaining=remaining_str, remaining_s=remaining,\\n\\t\\t\\t\\t\\tl_bar=l_bar, r_bar=r_bar,\\n\\t\\t\\t\\t\\tdesc=prefix or '', postfix=postfix, unit=unit,\\n\\t\\t\\t\\t\\t**extra_kwargs)\\n\\t\\t\\t\\tif not prefix:\\n\\t\\t\\t\\t\\tbar_format = bar_format.replace(\"{desc}: \", '')\\n\\t\\t\\t\\tif '{bar}' in bar_format:\\n\\t\\t\\t\\t\\tl_bar_user, r_bar_user = bar_format.split('{bar}')\\n\\t\\t\\t\\t\\tl_bar = l_bar_user.format(**format_dict)\\n\\t\\t\\t\\t\\tr_bar = r_bar_user.format(**format_dict)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\treturn bar_format.format(**format_dict)\\n\\t\\t\\tif ncols:\\n\\t\\t\\t\\tN_BARS = max(1, ncols - len(RE_ANSI.sub('', l_bar + r_bar)))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tN_BARS = 10\\n\\t\\t\\tif ascii:\\n\\t\\t\\t\\tbar_length, frac_bar_length = divmod(\\n\\t\\t\\t\\t\\tint(frac * N_BARS * 10), 10)\\n\\t\\t\\t\\tbar = '\\n\\t\\t\\t\\tfrac_bar = chr(48 + frac_bar_length) if frac_bar_length \\\\n\\t\\t\\t\\t\\telse ' '\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbar_length, frac_bar_length = divmod(int(frac * N_BARS * 8), 8)\\n\\t\\t\\t\\tbar = _unich(0x2588) * bar_length\\n\\t\\t\\t\\tfrac_bar = _unich(0x2590 - frac_bar_length) \\\\n\\t\\t\\t\\t\\tif frac_bar_length else ' '\\n\\t\\t\\tif bar_length < N_BARS:\\n\\t\\t\\t\\tfull_bar = bar + frac_bar + \\\\n\\t\\t\\t\\t\\t' ' * max(N_BARS - bar_length - 1, 0)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfull_bar = bar + \\\\n\\t\\t\\t\\t\\t' ' * max(N_BARS - bar_length, 0)\\n\\t\\t\\treturn l_bar + full_bar + r_bar\\n\\t\\telse:\\n\\t\\t\\treturn ((prefix + \": \") if prefix else '') + \\\\n\\t\\t\\t\\t'{0}{1} [{2}, {3}{4}]'.format(\\n\\t\\t\\t\\t\\tn_fmt, unit, elapsed_str, rate_fmt, postfix)"
  },
  {
    "code": "def set_index(self, col_or_cols, drop=True, inplace=False,\\n                  verify_integrity=True):\\n        cols = col_or_cols\\n        if not isinstance(col_or_cols, (list, tuple)):\\n            cols = [col_or_cols]\\n        if inplace:\\n            frame = self\\n        else:\\n            frame = self.copy()\\n        arrays = []\\n        for col in cols:\\n            level = frame[col]\\n            if drop:\\n                del frame[col]\\n            arrays.append(level)\\n        index = MultiIndex.from_arrays(arrays, names=cols)\\n        if verify_integrity and not index.is_unique:\\n            duplicates = index.get_duplicates()\\n            raise Exception('Index has duplicate keys: %s' % duplicates)\\n        index._cleanup()\\n        frame.index = index\\n        return frame",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: can pass external arrays in lieu of column names to DataFrame.set_index, close #402",
    "fixed_code": "def set_index(self, keys, drop=True, inplace=False,\\n                  verify_integrity=False):\\n        if not isinstance(keys, (list, tuple)):\\n            keys = [keys]\\n        if inplace:\\n            frame = self\\n        else:\\n            frame = self.copy()\\n        arrays = []\\n        for col in keys:\\n            if isinstance(col, (list, Series, np.ndarray)):\\n                level = col\\n            else:\\n                level = frame[col]\\n                if drop:\\n                    del frame[col]\\n            arrays.append(level)\\n        index = MultiIndex.from_arrays(arrays, names=keys)\\n        if verify_integrity and not index.is_unique:\\n            duplicates = index.get_duplicates()\\n            raise Exception('Index has duplicate keys: %s' % duplicates)\\n        index._cleanup()\\n        frame.index = index\\n        return frame"
  },
  {
    "code": "def sum(self, axis: int = 0, min_count: int = 0, *args, **kwargs) -> Scalar:\\n        nv.validate_sum(args, kwargs)\\n        valid_vals = self._valid_sp_values\\n        sp_sum = valid_vals.sum()\\n        if self._null_fill_value:\\n            if check_below_min_count(valid_vals.shape, None, min_count):\\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\\n            return sp_sum\\n        else:\\n            nsparse = self.sp_index.ngaps\\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\\n            return sp_sum + self.fill_value * nsparse",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sum(self, axis: int = 0, min_count: int = 0, *args, **kwargs) -> Scalar:\\n        nv.validate_sum(args, kwargs)\\n        valid_vals = self._valid_sp_values\\n        sp_sum = valid_vals.sum()\\n        if self._null_fill_value:\\n            if check_below_min_count(valid_vals.shape, None, min_count):\\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\\n            return sp_sum\\n        else:\\n            nsparse = self.sp_index.ngaps\\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\\n            return sp_sum + self.fill_value * nsparse"
  },
  {
    "code": "def static_file(path):\\n\\treturn send_file(\"../plugins/\" + path)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "fix path traversal vulnerability (#156)\\n\\n\\nThe `os.path.join` call is unsafe for use with untrusted input. When the `os.path.join` call encounters an absolute path, it ignores all the parameters it has encountered till that point and starts working with the new absolute path.  Please see the example below.\\n```\\n>>> import os.path\\n>>> static = \"path/to/mySafeStaticDir\"\\n>>> malicious = \"/../../../../../etc/passwd\"\\n>>> os.path.join(t,malicious)\\n'/../../../../../etc/passwd'\\n```\\nSince the \"malicious\" parameter represents an absolute path, the result of `os.path.join` ignores the static directory completely. Hence, untrusted input is passed via the `os.path.join` call to `flask.send_file` can lead to path traversal attacks.\\n\\nIn this case, the problems occurs due to the following code :\\n\\nHere, the `path` parameter is attacker controlled. This parameter passes through the unsafe `os.path.join` call making the effective directory and filename passed to the `send_file` call attacker controlled. This leads to a path traversal attack.\\n\\nThe bug can be verified using a proof of concept similar to the one shown below.\\n\\n```\\n```\\n\\nThis can be fixed by preventing flow of untrusted data to the vulnerable `send_file` function. In case the application logic necessiates this behaviour, one can either use the `flask.safe_join` to join untrusted paths or replace `flask.send_file` calls with `flask.send_from_directory` calls.\\n\\n\\n\\n---------",
    "fixed_code": "def static_file(path):\\n\\tpath = safe_join(\"../plugins/\", path)\\n\\tif path is None:\\n\\t\\tabort(404)\\n\\telse:\\n\\t\\treturn send_file(path)"
  },
  {
    "code": "def get_redirect_url(self, *args, **kwargs):\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n        args = self.request.META.get('QUERY_STRING', '')\\n        if args and self.query_string:\\n            url = \"%s?%s\" % (url, args)\\n        return url",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_redirect_url(self, *args, **kwargs):\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n        args = self.request.META.get('QUERY_STRING', '')\\n        if args and self.query_string:\\n            url = \"%s?%s\" % (url, args)\\n        return url"
  },
  {
    "code": "def _get_nobs(self):\\n        if self.format_version >= 118:\\n            return struct.unpack(self.byteorder + \"Q\", self.path_or_buf.read(8))[0]\\n        else:\\n            return struct.unpack(self.byteorder + \"I\", self.path_or_buf.read(4))[0]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_nobs(self):\\n        if self.format_version >= 118:\\n            return struct.unpack(self.byteorder + \"Q\", self.path_or_buf.read(8))[0]\\n        else:\\n            return struct.unpack(self.byteorder + \"I\", self.path_or_buf.read(4))[0]"
  },
  {
    "code": "def ftp_open(self, req):\\n        import ftplib\\n        import mimetypes\\n        host = req.host\\n        if not host:\\n            raise URLError('ftp error: no host given')\\n        host, port = splitport(host)\\n        if port is None:\\n            port = ftplib.FTP_PORT\\n        else:\\n            port = int(port)\\n        user, host = splituser(host)\\n        if user:\\n            user, passwd = splitpasswd(user)\\n        else:\\n            passwd = None\\n        host = unquote(host)\\n        user = user or ''\\n        passwd = passwd or ''\\n        try:\\n            host = socket.gethostbyname(host)\\n        except socket.error as msg:\\n            raise URLError(msg)\\n        path, attrs = splitattr(req.selector)\\n        dirs = path.split('/')\\n        dirs = list(map(unquote, dirs))\\n        dirs, file = dirs[:-1], dirs[-1]\\n        if dirs and not dirs[0]:\\n            dirs = dirs[1:]\\n        try:\\n            fw = self.connect_ftp(user, passwd, host, port, dirs, req.timeout)\\n            type = file and 'I' or 'D'\\n            for attr in attrs:\\n                attr, value = splitvalue(attr)\\n                if attr.lower() == 'type' and \\\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\\n                    type = value.upper()\\n            fp, retrlen = fw.retrfile(file, type)\\n            headers = \"\"\\n            mtype = mimetypes.guess_type(req.full_url)[0]\\n            if mtype:\\n                headers += \"Content-type: %s\\n\" % mtype\\n            if retrlen is not None and retrlen >= 0:\\n                headers += \"Content-length: %d\\n\" % retrlen\\n            headers = email.message_from_string(headers)\\n            return addinfourl(fp, headers, req.full_url)\\n        except ftplib.all_errors as msg:\\n            exc = URLError('ftp error: %s' % msg)\\n            raise exc.with_traceback(sys.exc_info()[2])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def ftp_open(self, req):\\n        import ftplib\\n        import mimetypes\\n        host = req.host\\n        if not host:\\n            raise URLError('ftp error: no host given')\\n        host, port = splitport(host)\\n        if port is None:\\n            port = ftplib.FTP_PORT\\n        else:\\n            port = int(port)\\n        user, host = splituser(host)\\n        if user:\\n            user, passwd = splitpasswd(user)\\n        else:\\n            passwd = None\\n        host = unquote(host)\\n        user = user or ''\\n        passwd = passwd or ''\\n        try:\\n            host = socket.gethostbyname(host)\\n        except socket.error as msg:\\n            raise URLError(msg)\\n        path, attrs = splitattr(req.selector)\\n        dirs = path.split('/')\\n        dirs = list(map(unquote, dirs))\\n        dirs, file = dirs[:-1], dirs[-1]\\n        if dirs and not dirs[0]:\\n            dirs = dirs[1:]\\n        try:\\n            fw = self.connect_ftp(user, passwd, host, port, dirs, req.timeout)\\n            type = file and 'I' or 'D'\\n            for attr in attrs:\\n                attr, value = splitvalue(attr)\\n                if attr.lower() == 'type' and \\\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\\n                    type = value.upper()\\n            fp, retrlen = fw.retrfile(file, type)\\n            headers = \"\"\\n            mtype = mimetypes.guess_type(req.full_url)[0]\\n            if mtype:\\n                headers += \"Content-type: %s\\n\" % mtype\\n            if retrlen is not None and retrlen >= 0:\\n                headers += \"Content-length: %d\\n\" % retrlen\\n            headers = email.message_from_string(headers)\\n            return addinfourl(fp, headers, req.full_url)\\n        except ftplib.all_errors as msg:\\n            exc = URLError('ftp error: %s' % msg)\\n            raise exc.with_traceback(sys.exc_info()[2])"
  },
  {
    "code": "def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\\n\\t\\t\\t   label='', **kwargs):\\n\\t\\tself._process_unit_info(xdata=x, ydata=[ymin, ymax], kwargs=kwargs)\\n\\t\\tx = self.convert_xunits(x)\\n\\t\\tymin = self.convert_yunits(ymin)\\n\\t\\tymax = self.convert_yunits(ymax)\\n\\t\\tif not np.iterable(x):\\n\\t\\t\\tx = [x]\\n\\t\\tif not np.iterable(ymin):\\n\\t\\t\\tymin = [ymin]\\n\\t\\tif not np.iterable(ymax):\\n\\t\\t\\tymax = [ymax]\\n\\t\\tx, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)\\n\\t\\tx = np.ravel(x)\\n\\t\\tymin = np.resize(ymin, x.shape)\\n\\t\\tymax = np.resize(ymax, x.shape)\\n\\t\\tverts = [((thisx, thisymin), (thisx, thisymax))\\n\\t\\t\\t\\t for thisx, thisymin, thisymax in zip(x, ymin, ymax)]\\n\\t\\tlines = mcoll.LineCollection(verts, colors=colors,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t linestyles=linestyles, label=label)\\n\\t\\tself.add_collection(lines, autolim=False)\\n\\t\\tlines.update(kwargs)\\n\\t\\tif len(x) > 0:\\n\\t\\t\\tminx = x.min()\\n\\t\\t\\tmaxx = x.max()\\n\\t\\t\\tminy = min(ymin.min(), ymax.min())\\n\\t\\t\\tmaxy = max(ymin.max(), ymax.max())\\n\\t\\t\\tcorners = (minx, miny), (maxx, maxy)\\n\\t\\t\\tself.update_datalim(corners)\\n\\t\\t\\tself._request_autoscale_view()\\n\\t\\treturn lines\\n\\t@_preprocess_data(replace_names=[\"positions\", \"lineoffsets\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"linelengths\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"colors\", \"linestyles\"])\\n\\t@docstring.dedent_interpd",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed incorrect colour in ErrorBar when Nan value is presented (#16724)\\n\\n\\nCombine array masks rather than deleting masked points to maintain\\nconsistency across the project.\\n\\nAdd appropriate test cases for validating color correctness for hlines\\nand vlines.\\n\\nFixes issue #13799.",
    "fixed_code": "def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\\n\\t\\t\\t   label='', **kwargs):\\n\\t\\tself._process_unit_info(xdata=x, ydata=[ymin, ymax], kwargs=kwargs)\\n\\t\\tx = self.convert_xunits(x)\\n\\t\\tymin = self.convert_yunits(ymin)\\n\\t\\tymax = self.convert_yunits(ymax)\\n\\t\\tif not np.iterable(x):\\n\\t\\t\\tx = [x]\\n\\t\\tif not np.iterable(ymin):\\n\\t\\t\\tymin = [ymin]\\n\\t\\tif not np.iterable(ymax):\\n\\t\\t\\tymax = [ymax]\\n\\t\\tx, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\\n\\t\\tx = np.ravel(x)\\n\\t\\tymin = np.ravel(ymin)\\n\\t\\tymax = np.ravel(ymax)\\n\\t\\tmasked_verts = np.ma.empty((len(x), 2, 2))\\n\\t\\tmasked_verts[:, 0, 0] = x\\n\\t\\tmasked_verts[:, 0, 1] = ymin\\n\\t\\tmasked_verts[:, 1, 0] = x\\n\\t\\tmasked_verts[:, 1, 1] = ymax\\n\\t\\tlines = mcoll.LineCollection(masked_verts, colors=colors,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t linestyles=linestyles, label=label)\\n\\t\\tself.add_collection(lines, autolim=False)\\n\\t\\tlines.update(kwargs)\\n\\t\\tif len(x) > 0:\\n\\t\\t\\tminx = x.min()\\n\\t\\t\\tmaxx = x.max()\\n\\t\\t\\tminy = min(ymin.min(), ymax.min())\\n\\t\\t\\tmaxy = max(ymin.max(), ymax.max())\\n\\t\\t\\tcorners = (minx, miny), (maxx, maxy)\\n\\t\\t\\tself.update_datalim(corners)\\n\\t\\t\\tself._request_autoscale_view()\\n\\t\\treturn lines\\n\\t@_preprocess_data(replace_names=[\"positions\", \"lineoffsets\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"linelengths\", \"linewidths\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t \"colors\", \"linestyles\"])\\n\\t@docstring.dedent_interpd"
  },
  {
    "code": "def nested_to_record(ds, prefix=\"\", sep=\".\", level=0):\\n    singleton = False\\n    if isinstance(ds, dict):\\n        ds = [ds]\\n        singleton = True\\n    new_ds = []\\n    for d in ds:\\n        new_d = copy.deepcopy(d)\\n        for k, v in d.items():\\n            if not isinstance(k, compat.string_types):\\n                k = str(k)\\n            if level == 0:\\n                newkey = k\\n            else:\\n                newkey = prefix + sep + k\\n            if not isinstance(v, dict):\\n                if level != 0:  \\n                    v = new_d.pop(k)\\n                    new_d[newkey] = v\\n                elif v is None:  \\n                    new_d.pop(k)\\n                continue\\n            else:\\n                v = new_d.pop(k)\\n                new_d.update(nested_to_record(v, newkey, sep, level + 1))\\n        new_ds.append(new_d)\\n    if singleton:\\n        return new_ds[0]\\n    return new_ds",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def nested_to_record(ds, prefix=\"\", sep=\".\", level=0):\\n    singleton = False\\n    if isinstance(ds, dict):\\n        ds = [ds]\\n        singleton = True\\n    new_ds = []\\n    for d in ds:\\n        new_d = copy.deepcopy(d)\\n        for k, v in d.items():\\n            if not isinstance(k, compat.string_types):\\n                k = str(k)\\n            if level == 0:\\n                newkey = k\\n            else:\\n                newkey = prefix + sep + k\\n            if not isinstance(v, dict):\\n                if level != 0:  \\n                    v = new_d.pop(k)\\n                    new_d[newkey] = v\\n                elif v is None:  \\n                    new_d.pop(k)\\n                continue\\n            else:\\n                v = new_d.pop(k)\\n                new_d.update(nested_to_record(v, newkey, sep, level + 1))\\n        new_ds.append(new_d)\\n    if singleton:\\n        return new_ds[0]\\n    return new_ds"
  },
  {
    "code": "def _is_s3_url(url):\\n    try:\\n        return parse_url(url).scheme == 's3'\\n    except:\\n        return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Recognize 's3n' and 's3a' as an S3 address",
    "fixed_code": "def _is_s3_url(url):\\n    try:\\n        return parse_url(url).scheme in ['s3', 's3n', 's3a']\\n    except:\\n        return False"
  },
  {
    "code": "def __array__(self):\\n        return self._deprecated().__array__()\\n    __gt__ = _make_deprecated_binop('__gt__')\\n    __ge__ = _make_deprecated_binop('__ge__')\\n    __lt__ = _make_deprecated_binop('__lt__')\\n    __le__ = _make_deprecated_binop('__le__')\\n    __eq__ = _make_deprecated_binop('__eq__')\\n    __ne__ = _make_deprecated_binop('__ne__')\\n    __add__ = __radd__ = _make_deprecated_binop('__add__')\\n    __sub__ = __rsub__ = _make_deprecated_binop('__sub__')\\n    __mul__ = __rmul__ = _make_deprecated_binop('__mul__')\\n    __floordiv__ = __rfloordiv__ = _make_deprecated_binop('__floordiv__')\\n    __truediv__ = __rtruediv__ = _make_deprecated_binop('__truediv__')\\n    if not compat.PY3:\\n        __div__ = __rdiv__ = _make_deprecated_binop('__div__')\\n    __neg__ = _make_deprecated_unary(lambda x: -x)\\n    __pos__ = _make_deprecated_unary(lambda x: x)\\n    __abs__ = _make_deprecated_unary(lambda x: np.abs(x))\\n    __inv__ = _make_deprecated_unary(lambda x: -x)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC: resample warnings\\n\\ncloses #13618\\ncloses #13520\\n\\nAuthor: Chris <cbartak@gmail.com>\\n\\nCloses #13675 from chris-b1/resample-warning and squashes the following commits:\\n\\n2185c1f [Chris] whatsnew note\\nc58c70c [Chris] DOC: resample warnings",
    "fixed_code": "def __array__(self):\\n        return self._deprecated('__array__').__array__()\\n    __gt__ = _make_deprecated_binop('__gt__')\\n    __ge__ = _make_deprecated_binop('__ge__')\\n    __lt__ = _make_deprecated_binop('__lt__')\\n    __le__ = _make_deprecated_binop('__le__')\\n    __eq__ = _make_deprecated_binop('__eq__')\\n    __ne__ = _make_deprecated_binop('__ne__')\\n    __add__ = __radd__ = _make_deprecated_binop('__add__')\\n    __sub__ = __rsub__ = _make_deprecated_binop('__sub__')\\n    __mul__ = __rmul__ = _make_deprecated_binop('__mul__')\\n    __floordiv__ = __rfloordiv__ = _make_deprecated_binop('__floordiv__')\\n    __truediv__ = __rtruediv__ = _make_deprecated_binop('__truediv__')\\n    if not compat.PY3:\\n        __div__ = __rdiv__ = _make_deprecated_binop('__div__')\\n    __neg__ = _make_deprecated_unary(lambda x: -x, '__neg__')\\n    __pos__ = _make_deprecated_unary(lambda x: x, '__pos__')\\n    __abs__ = _make_deprecated_unary(lambda x: np.abs(x), '__abs__')\\n    __inv__ = _make_deprecated_unary(lambda x: -x, '__inv__')"
  },
  {
    "code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.reshape.reshape import pivot\\n        return pivot(self, index=index, columns=columns, values=values)\\n    _shared_docs['pivot_table'] = \\n    @Substitution('')\\n    @Appender(_shared_docs['pivot_table'])\\n    def pivot_table(self, values=None, index=None, columns=None,\\n                    aggfunc='mean', fill_value=None, margins=False,\\n                    dropna=True, margins_name='All'):\\n        from pandas.core.reshape.pivot import pivot_table\\n        return pivot_table(self, values=values, index=index, columns=columns,\\n                           aggfunc=aggfunc, fill_value=fill_value,\\n                           margins=margins, dropna=dropna,\\n                           margins_name=margins_name)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API/CLN: Have toplevel pd.pivot mirror pivot instead of pivot_simple (#22209)",
    "fixed_code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.reshape.pivot import pivot\\n        return pivot(self, index=index, columns=columns, values=values)\\n    _shared_docs['pivot_table'] = \\n    @Substitution('')\\n    @Appender(_shared_docs['pivot_table'])\\n    def pivot_table(self, values=None, index=None, columns=None,\\n                    aggfunc='mean', fill_value=None, margins=False,\\n                    dropna=True, margins_name='All'):\\n        from pandas.core.reshape.pivot import pivot_table\\n        return pivot_table(self, values=values, index=index, columns=columns,\\n                           aggfunc=aggfunc, fill_value=fill_value,\\n                           margins=margins, dropna=dropna,\\n                           margins_name=margins_name)"
  },
  {
    "code": "def predict(self, T):\\n        T = sparse.csr_matrix(T)\\n        T.data = np.asanyarray(T.data, dtype=np.float64, order='C')\\n        kernel_type = self._kernel_types.index(self.kernel)\\n        return _libsvm.csr_predict_from_model_wrap(T.data,\\n                      T.indices, T.indptr, self.support_.data,\\n                      self.support_.indices, self.support_.indptr,\\n                      self.dual_coef_.data, self.intercept_,\\n                      self._svm_types.index(self.impl),\\n                      kernel_type, self.degree,\\n                      self.gamma, self.coef0, self.eps, self.C,\\n                      self.weight_label, self.weight,\\n                      self.nu, self.cache_size, self.p,\\n                      self.shrinking, self.probability,\\n                      self.n_support, self.label_, self.probA_,\\n                      self.probB_)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def predict(self, T):\\n        T = sparse.csr_matrix(T)\\n        T.data = np.asanyarray(T.data, dtype=np.float64, order='C')\\n        kernel_type = self._kernel_types.index(self.kernel)\\n        return _libsvm.csr_predict_from_model_wrap(T.data,\\n                      T.indices, T.indptr, self.support_.data,\\n                      self.support_.indices, self.support_.indptr,\\n                      self.dual_coef_.data, self.intercept_,\\n                      self._svm_types.index(self.impl),\\n                      kernel_type, self.degree,\\n                      self.gamma, self.coef0, self.eps, self.C,\\n                      self.weight_label, self.weight,\\n                      self.nu, self.cache_size, self.p,\\n                      self.shrinking, self.probability,\\n                      self.n_support, self.label_, self.probA_,\\n                      self.probB_)"
  },
  {
    "code": "def to_offset(freq):\\n    if freq is None:\\n        return None\\n    if isinstance(freq, DateOffset):\\n        return freq\\n    if isinstance(freq, tuple):\\n        name = freq[0]\\n        stride = freq[1]\\n        if isinstance(stride, str):\\n            name, stride = stride, name\\n        name, _ = libfreqs._base_and_stride(name)\\n        delta = get_offset(name) * stride\\n    elif isinstance(freq, timedelta):\\n        delta = None\\n        freq = Timedelta(freq)\\n        try:\\n            for name in freq.components._fields:\\n                offset = _name_to_offset_map[name]\\n                stride = getattr(freq.components, name)\\n                if stride != 0:\\n                    offset = stride * offset\\n                    if delta is None:\\n                        delta = offset\\n                    else:\\n                        delta = delta + offset\\n        except ValueError:\\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(freq))\\n    else:\\n        delta = None\\n        stride_sign = None\\n        try:\\n            splitted = re.split(libfreqs.opattern, freq)\\n            if splitted[-1] != \"\" and not splitted[-1].isspace():\\n                raise ValueError(\"last element must be blank\")\\n            for sep, stride, name in zip(\\n                splitted[0::4], splitted[1::4], splitted[2::4]\\n            ):\\n                if sep != \"\" and not sep.isspace():\\n                    raise ValueError(\"separator must be spaces\")\\n                prefix = libfreqs._lite_rule_alias.get(name) or name\\n                if stride_sign is None:\\n                    stride_sign = -1 if stride.startswith(\"-\") else 1\\n                if not stride:\\n                    stride = 1\\n                if prefix in Resolution._reso_str_bump_map.keys():\\n                    stride, name = Resolution.get_stride_from_decimal(\\n                        float(stride), prefix\\n                    )\\n                stride = int(stride)\\n                offset = get_offset(name)\\n                offset = offset * int(np.fabs(stride) * stride_sign)\\n                if delta is None:\\n                    delta = offset\\n                else:\\n                    delta = delta + offset\\n        except (ValueError, TypeError):\\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(freq))\\n    if delta is None:\\n        raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(freq))\\n    return delta",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_offset(freq):\\n    if freq is None:\\n        return None\\n    if isinstance(freq, DateOffset):\\n        return freq\\n    if isinstance(freq, tuple):\\n        name = freq[0]\\n        stride = freq[1]\\n        if isinstance(stride, str):\\n            name, stride = stride, name\\n        name, _ = libfreqs._base_and_stride(name)\\n        delta = get_offset(name) * stride\\n    elif isinstance(freq, timedelta):\\n        delta = None\\n        freq = Timedelta(freq)\\n        try:\\n            for name in freq.components._fields:\\n                offset = _name_to_offset_map[name]\\n                stride = getattr(freq.components, name)\\n                if stride != 0:\\n                    offset = stride * offset\\n                    if delta is None:\\n                        delta = offset\\n                    else:\\n                        delta = delta + offset\\n        except ValueError:\\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(freq))\\n    else:\\n        delta = None\\n        stride_sign = None\\n        try:\\n            splitted = re.split(libfreqs.opattern, freq)\\n            if splitted[-1] != \"\" and not splitted[-1].isspace():\\n                raise ValueError(\"last element must be blank\")\\n            for sep, stride, name in zip(\\n                splitted[0::4], splitted[1::4], splitted[2::4]\\n            ):\\n                if sep != \"\" and not sep.isspace():\\n                    raise ValueError(\"separator must be spaces\")\\n                prefix = libfreqs._lite_rule_alias.get(name) or name\\n                if stride_sign is None:\\n                    stride_sign = -1 if stride.startswith(\"-\") else 1\\n                if not stride:\\n                    stride = 1\\n                if prefix in Resolution._reso_str_bump_map.keys():\\n                    stride, name = Resolution.get_stride_from_decimal(\\n                        float(stride), prefix\\n                    )\\n                stride = int(stride)\\n                offset = get_offset(name)\\n                offset = offset * int(np.fabs(stride) * stride_sign)\\n                if delta is None:\\n                    delta = offset\\n                else:\\n                    delta = delta + offset\\n        except (ValueError, TypeError):\\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(freq))\\n    if delta is None:\\n        raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(freq))\\n    return delta"
  },
  {
    "code": "def quit(self):\\n        resp = self.shortcmd('QUIT')\\n        self.file.close()\\n        self.sock.close()\\n        del self.file, self.sock\\n        return resp",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #9360: Cleanup and improvements to the nntplib module.  The API now conforms to the philosophy of bytes and unicode separation in Python 3. A test suite has also been added.",
    "fixed_code": "def quit(self):\\n        try:\\n            resp = self._shortcmd('QUIT')\\n        finally:\\n            self._close()\\n        return resp"
  },
  {
    "code": "def _get_delegate(self):\\n        return ImpImporter(self.filename).find_module('__init__')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #19708: Update pkgutil to use the new importer APIs.",
    "fixed_code": "def _get_delegate(self):\\n        finder = ImpImporter(self.filename)\\n        spec = _get_spec(finder, '__init__')\\n        return spec.loader"
  },
  {
    "code": "def set_break(self, filename, lineno, temporary=False, cond=None,\\n                  funcname=None):\\n        filename = self.canonic(filename)\\n        import linecache \\n        line = linecache.getline(filename, lineno)\\n        if not line:\\n            return 'Line %s:%d does not exist' % (filename, lineno)\\n        self._add_to_breaks(filename, lineno)\\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\\n        return None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_break(self, filename, lineno, temporary=False, cond=None,\\n                  funcname=None):\\n        filename = self.canonic(filename)\\n        import linecache \\n        line = linecache.getline(filename, lineno)\\n        if not line:\\n            return 'Line %s:%d does not exist' % (filename, lineno)\\n        self._add_to_breaks(filename, lineno)\\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\\n        return None"
  },
  {
    "code": "def __init__(\\n        self,\\n        connections_prefix: str = '/airflow/connections',\\n        profile_name: Optional[str] = None,\\n        **kwargs\\n    ):\\n        self.connections_prefix = connections_prefix.rstrip(\"/\")\\n        self.profile_name = profile_name\\n        super().__init__(**kwargs)\\n    @cached_property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        connections_prefix: str = '/airflow/connections',\\n        profile_name: Optional[str] = None,\\n        **kwargs\\n    ):\\n        self.connections_prefix = connections_prefix.rstrip(\"/\")\\n        self.profile_name = profile_name\\n        super().__init__(**kwargs)\\n    @cached_property"
  },
  {
    "code": "def encode_base64(msg):\\n    orig = msg.get_payload(decode=True)\\n    encdata = str(_bencode(orig), 'ascii')\\n    msg.set_payload(encdata)\\n    msg['Content-Transfer-Encoding'] = 'base64'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def encode_base64(msg):\\n    orig = msg.get_payload(decode=True)\\n    encdata = str(_bencode(orig), 'ascii')\\n    msg.set_payload(encdata)\\n    msg['Content-Transfer-Encoding'] = 'base64'"
  },
  {
    "code": "def _intersection(self, other: Index, sort=False):\\n        if self.is_monotonic and other.is_monotonic:\\n            try:\\n                result = self._inner_indexer(other)[0]\\n            except TypeError:\\n                pass\\n            else:\\n                res = algos.unique1d(result)\\n                return ensure_wrapped_if_datetimelike(res)\\n        res_values = self._intersection_via_get_indexer(other, sort=sort)\\n        res_values = _maybe_try_sort(res_values, sort)\\n        return res_values\\n    def _intersection_via_get_indexer(self, other: Index, sort) -> ArrayLike:\\n        left_unique = self.drop_duplicates()\\n        right_unique = other.drop_duplicates()\\n        indexer = left_unique.get_indexer_for(right_unique)\\n        mask = indexer != -1\\n        taker = indexer.take(mask.nonzero()[0])\\n        if sort is False:\\n            taker = np.sort(taker)\\n        result = left_unique.take(taker)._values\\n        return result\\n    @final",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: MultiIndex.difference incorrectly raising TypeError when indexes contain non-sortable entries (#41915)",
    "fixed_code": "def _intersection(self, other: Index, sort=False):\\n        if self.is_monotonic and other.is_monotonic:\\n            try:\\n                result = self._inner_indexer(other)[0]\\n            except TypeError:\\n                pass\\n            else:\\n                res = algos.unique1d(result)\\n                return ensure_wrapped_if_datetimelike(res)\\n        res_values = self._intersection_via_get_indexer(other, sort=sort)\\n        res_values = _maybe_try_sort(res_values, sort)\\n        return res_values"
  },
  {
    "code": "def __new__(cls, name, bases, attrs):\\n        fields = [(field_name, attrs.pop(field_name)) for field_name, obj in attrs.items() if isinstance(obj, Field)]\\n        fields.sort(lambda x, y: cmp(x[1].creation_counter, y[1].creation_counter))\\n        attrs['base_fields'] = SortedDictFromList(fields)\\n        return type.__new__(cls, name, bases, attrs)\\nclass BaseForm(StrAndUnicode):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Implemented subclassing Forms in newforms",
    "fixed_code": "def __new__(cls, name, bases, attrs):\\n        fields = [(field_name, attrs.pop(field_name)) for field_name, obj in attrs.items() if isinstance(obj, Field)]\\n        fields.sort(lambda x, y: cmp(x[1].creation_counter, y[1].creation_counter))\\n        for base in bases[::-1]:\\n            if hasattr(base, 'base_fields'):\\n                fields = base.base_fields.items() + fields\\n        attrs['base_fields'] = SortedDictFromList(fields)\\n        return type.__new__(cls, name, bases, attrs)\\nclass BaseForm(StrAndUnicode):"
  },
  {
    "code": "def __repr__(self):\\n        return \"_Feature(\" + `self.getOptionalRelease()` + \", \" + \\\\n                             `self.getMandatoryRelease()` + \")\"\\nnested_scopes = _Feature((2, 1, 0, \"beta\",  1),\\n                         (2, 2, 0, \"alpha\", 0),\\n                         CO_NESTED)\\ngenerators = _Feature((2, 2, 0, \"alpha\", 1),\\n                      (2, 3, 0, \"final\", 0),\\n                      CO_GENERATOR_ALLOWED)\\ndivision = _Feature((2, 2, 0, \"alpha\", 2),\\n                    (3, 0, 0, \"alpha\", 0),\\n                    CO_FUTURE_DIVISION)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n        return \"_Feature(\" + `self.getOptionalRelease()` + \", \" + \\\\n                             `self.getMandatoryRelease()` + \")\"\\nnested_scopes = _Feature((2, 1, 0, \"beta\",  1),\\n                         (2, 2, 0, \"alpha\", 0),\\n                         CO_NESTED)\\ngenerators = _Feature((2, 2, 0, \"alpha\", 1),\\n                      (2, 3, 0, \"final\", 0),\\n                      CO_GENERATOR_ALLOWED)\\ndivision = _Feature((2, 2, 0, \"alpha\", 2),\\n                    (3, 0, 0, \"alpha\", 0),\\n                    CO_FUTURE_DIVISION)"
  },
  {
    "code": "def _create_instances_here(self, ctxt, instance_uuids, instance_properties,\\n\\t\\t\\tinstance_type, image, security_groups, block_device_mapping):\\n\\t\\tinstance_values = copy.copy(instance_properties)\\n\\t\\tinstance_values['metadata'] = utils.instance_meta(instance_values)\\n\\t\\tsys_metadata = utils.instance_sys_meta(instance_values)\\n\\t\\tsys_metadata = flavors.save_flavor_info(sys_metadata, instance_type)\\n\\t\\tinstance_values['system_metadata'] = sys_metadata\\n\\t\\tinstance_values.pop('name')\\n\\t\\tif 'security_groups' in instance_values:\\n\\t\\t\\tinstance_values = security_group.make_secgroup_list(\\n\\t\\t\\t\\tinstance_values['security_groups'])\\n\\t\\tnum_instances = len(instance_uuids)\\n\\t\\tfor i, instance_uuid in enumerate(instance_uuids):\\n\\t\\t\\tinstance = instance_obj.Instance()\\n\\t\\t\\tinstance.update(instance_values)\\n\\t\\t\\tinstance.uuid = instance_uuid\\n\\t\\t\\tinstance = self.compute_api.create_db_entry_for_new_instance(\\n\\t\\t\\t\\t\\tctxt,\\n\\t\\t\\t\\t\\tinstance_type,\\n\\t\\t\\t\\t\\timage,\\n\\t\\t\\t\\t\\tinstance,\\n\\t\\t\\t\\t\\tsecurity_groups,\\n\\t\\t\\t\\t\\tblock_device_mapping,\\n\\t\\t\\t\\t\\tnum_instances, i)\\n\\t\\t\\tself.msg_runner.instance_update_at_top(ctxt, instance)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix various cells issues due to object changes\\n\\n1) Child cell scheduling fails.  The change to use Instance.create() in\\nchild cells broke things.  The instance properties was being overwritten\\ndue to a typo.  I also found a number of other things that need to be\\npopped out before passing to create_db_entry_for_new_instance().\\n\\n2) Notification to the API cell was failing.  We cannot pass an object to\\ninstance_update_at_top().\\n\\n3) When cells is enabled, deleting an instance hits the '_local_delete'\\npart of code in the API because we don't see the compute node.  There's\\na trap in cells_api.py to forward the delete on to child cells, but we\\nneed to make sure not to try to deallocate network in the API cell.\\n\\nFixes bug 1216113",
    "fixed_code": "def _create_instances_here(self, ctxt, instance_uuids, instance_properties,\\n\\t\\t\\tinstance_type, image, security_groups, block_device_mapping):\\n\\t\\tinstance_values = copy.copy(instance_properties)\\n\\t\\tinstance_values['metadata'] = utils.instance_meta(instance_values)\\n\\t\\tsys_metadata = utils.instance_sys_meta(instance_values)\\n\\t\\tsys_metadata = flavors.save_flavor_info(sys_metadata, instance_type)\\n\\t\\tinstance_values['system_metadata'] = sys_metadata\\n\\t\\tinstance_values.pop('id')\\n\\t\\tinstance_values.pop('name')\\n\\t\\tinstance_values.pop('info_cache')\\n\\t\\tinstance_values.pop('security_groups')\\n\\t\\tnum_instances = len(instance_uuids)\\n\\t\\tfor i, instance_uuid in enumerate(instance_uuids):\\n\\t\\t\\tinstance = instance_obj.Instance()\\n\\t\\t\\tinstance.update(instance_values)\\n\\t\\t\\tinstance.uuid = instance_uuid\\n\\t\\t\\tinstance = self.compute_api.create_db_entry_for_new_instance(\\n\\t\\t\\t\\t\\tctxt,\\n\\t\\t\\t\\t\\tinstance_type,\\n\\t\\t\\t\\t\\timage,\\n\\t\\t\\t\\t\\tinstance,\\n\\t\\t\\t\\t\\tsecurity_groups,\\n\\t\\t\\t\\t\\tblock_device_mapping,\\n\\t\\t\\t\\t\\tnum_instances, i)\\n\\t\\t\\tinstance = obj_base.obj_to_primitive(instance)\\n\\t\\t\\tself.msg_runner.instance_update_at_top(ctxt, instance)"
  },
  {
    "code": "def _set_stopinfo(self, stopframe, returnframe, stoplineno=-1):\\n        self.stopframe = stopframe\\n        self.returnframe = returnframe\\n        self.quitting = 0\\n        self.stoplineno = stoplineno",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 83429,83436 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/release27-maint\\n\\n................\\n  r83429 | georg.brandl | 2010-08-01 21:14:56 +0200 (So, 01 Aug 2010) | 37 lines\\n\\n  Merged revisions 83352,83356-83358,83362,83366,83368-83369 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n  ........\\n    r83352 | georg.brandl | 2010-07-31 20:11:07 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #9440: Remove borderline test case that fails based on unpredictable conditions such as compiler flags.\\n  ........\\n    r83356 | georg.brandl | 2010-07-31 21:29:15 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    Remove trailing whitespace.\\n  ........\\n    r83357 | georg.brandl | 2010-07-31 21:59:55 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #5778: document that sys.version can contain a newline.\\n  ........\\n    r83358 | georg.brandl | 2010-07-31 22:05:31 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #9442: do not document a specific format for sys.version; rather refer to version_info and the platform module.\\n  ........\\n    r83362 | georg.brandl | 2010-07-31 23:12:15 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #8910: add a file explaining why Lib/test/data is there.\\n  ........\\n    r83366 | georg.brandl | 2010-07-31 23:26:40 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    There always is a False and True now.\\n  ........\\n    r83368 | georg.brandl | 2010-07-31 23:40:15 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n  ........\\n    r83369 | georg.brandl | 2010-07-31 23:41:42 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    Fix \"Berkeley\" name.\\n  ........\\n................\\n  r83436 | georg.brandl | 2010-08-01 21:33:15 +0200 (So, 01 Aug 2010) | 42 lines\\n\\n  Merged revisions 83259,83261,83264-83265,83268-83269,83271-83272,83281 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n  ........\\n    r83259 | georg.brandl | 2010-07-30 09:03:39 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    Clarification.\\n  ........\\n    r83261 | georg.brandl | 2010-07-30 09:21:26 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #9230: allow Pdb.checkline() to be called without a current frame, for setting breakpoints before starting debugging.\\n  ........\\n    r83264 | georg.brandl | 2010-07-30 10:45:26 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    Document the \"jump\" command in pdb.__doc__, and add a version tag for \"until X\".\\n  ........\\n    r83265 | georg.brandl | 2010-07-30 10:54:49 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #8015: fix crash when entering an empty line for breakpoint commands.  Also restore environment properly when an exception occurs during the definition of commands.\\n  ........\\n    r83268 | georg.brandl | 2010-07-30 11:23:23 +0200 (Fr, 30 Jul 2010) | 2 lines\\n\\n    Issue #8048: Prevent doctests from failing when sys.displayhook has\\n    been reassigned.\\n  ........\\n    r83269 | georg.brandl | 2010-07-30 11:43:00 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #6719: In pdb, do not stop somewhere in the encodings machinery if the source file to be debugged is in a non-builtin encoding.\\n  ........\\n    r83271 | georg.brandl | 2010-07-30 11:59:28 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #5727: Restore the ability to use readline when calling into pdb in doctests.\\n  ........\\n    r83272 | georg.brandl | 2010-07-30 12:29:19 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #5294: Fix the behavior of pdb \"continue\" command when called in the top-level debugged frame.\\n  ........\\n    r83281 | georg.brandl | 2010-07-30 15:36:43 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    Add myself for pdb.\\n  ........\\n................",
    "fixed_code": "def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\\n        self.stopframe = stopframe\\n        self.returnframe = returnframe\\n        self.quitting = 0\\n        self.stoplineno = stoplineno"
  },
  {
    "code": "def execute(self, context: 'Context'):\\n        self.hook = PostgresHook(postgres_conn_id=self.postgres_conn_id, schema=self.database)\\n        if self.runtime_parameters:\\n            final_sql = []\\n            sql_param = {}\\n            for param in self.runtime_parameters:\\n                set_param_sql = f\"SET {{}} TO %({param})s;\"\\n                dynamic_sql = SQL(set_param_sql).format(Identifier(f\"{param}\"))\\n                final_sql.append(dynamic_sql)\\n            for param, val in self.runtime_parameters.items():\\n                sql_param.update({f\"{param}\": f\"{val}\"})\\n            if self.parameters:\\n                sql_param.update(self.parameters)\\n            if isinstance(self.sql, str):\\n                final_sql.append(SQL(self.sql))\\n            else:\\n                final_sql.extend(list(map(SQL, self.sql)))\\n            self.hook.run(final_sql, self.autocommit, parameters=sql_param)\\n        else:\\n            self.hook.run(self.sql, self.autocommit, parameters=self.parameters)\\n        for output in self.hook.conn.notices:\\n            self.log.info(output)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(self, context: 'Context'):\\n        self.hook = PostgresHook(postgres_conn_id=self.postgres_conn_id, schema=self.database)\\n        if self.runtime_parameters:\\n            final_sql = []\\n            sql_param = {}\\n            for param in self.runtime_parameters:\\n                set_param_sql = f\"SET {{}} TO %({param})s;\"\\n                dynamic_sql = SQL(set_param_sql).format(Identifier(f\"{param}\"))\\n                final_sql.append(dynamic_sql)\\n            for param, val in self.runtime_parameters.items():\\n                sql_param.update({f\"{param}\": f\"{val}\"})\\n            if self.parameters:\\n                sql_param.update(self.parameters)\\n            if isinstance(self.sql, str):\\n                final_sql.append(SQL(self.sql))\\n            else:\\n                final_sql.extend(list(map(SQL, self.sql)))\\n            self.hook.run(final_sql, self.autocommit, parameters=sql_param)\\n        else:\\n            self.hook.run(self.sql, self.autocommit, parameters=self.parameters)\\n        for output in self.hook.conn.notices:\\n            self.log.info(output)"
  },
  {
    "code": "def main():\\n    argument_spec = cs_argument_spec()\\n    argument_spec.update(dict(\\n        name=dict(required=True),\\n        affinty_type=dict(removed_in_version='2.9'),\\n        affinity_type=dict(),\\n        description=dict(),\\n        state=dict(choices=['present', 'absent'], default='present'),\\n        domain=dict(),\\n        account=dict(),\\n        project=dict(),\\n        poll_async=dict(type='bool', default=True),\\n    ))\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_together=cs_required_together(),\\n        mutually_exclusive=(\\n            ['affinity_type', 'affinty_type'],\\n        ),\\n        supports_check_mode=True\\n    )\\n    acs_ag = AnsibleCloudStackAffinityGroup(module)\\n    state = module.params.get('state')\\n    if state in ['absent']:\\n        affinity_group = acs_ag.remove_affinity_group()\\n    else:\\n        affinity_group = acs_ag.create_affinity_group()\\n    result = acs_ag.get_result(affinity_group)\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "cloudstack: exoscale: fix boilerplate (#63532)",
    "fixed_code": "def main():\\n    argument_spec = cs_argument_spec()\\n    argument_spec.update(dict(\\n        name=dict(required=True),\\n        affinity_type=dict(),\\n        description=dict(),\\n        state=dict(choices=['present', 'absent'], default='present'),\\n        domain=dict(),\\n        account=dict(),\\n        project=dict(),\\n        poll_async=dict(type='bool', default=True),\\n    ))\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_together=cs_required_together(),\\n        supports_check_mode=True\\n    )\\n    acs_ag = AnsibleCloudStackAffinityGroup(module)\\n    state = module.params.get('state')\\n    if state in ['absent']:\\n        affinity_group = acs_ag.remove_affinity_group()\\n    else:\\n        affinity_group = acs_ag.create_affinity_group()\\n    result = acs_ag.get_result(affinity_group)\\n    module.exit_json(**result)"
  },
  {
    "code": "def __contains__(self, key):\\n        if isna(key):\\n            return self.hasnans\\n        return contains(self, key, container=self._engine)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix CategoricalIndex.__contains__ with non-hashable, closes #21729 (#27284)",
    "fixed_code": "def __contains__(self, key):\\n        if is_scalar(key) and isna(key):\\n            return self.hasnans\\n        return contains(self, key, container=self._engine)"
  },
  {
    "code": "def replace(self, to_replace=None, value=None, method='pad', axis=0,\\n                inplace=False, limit=None, regex=False, infer_types=False):\\n        if not isinstance(regex, bool) and to_replace is not None:\\n            raise AssertionError(\"'to_replace' must be 'None' if 'regex' is \"\\n                                 \"not a bool\")\\n        self._consolidate_inplace()\\n        axis = self._get_axis_number(axis)\\n        method = com._clean_fill_method(method)\\n        if value is None:\\n            if not isinstance(to_replace, (dict, Series)):\\n                if not isinstance(regex, (dict, Series)):\\n                    raise TypeError('If \"to_replace\" and \"value\" are both None'\\n                                    ' then regex must be a mapping')\\n                to_replace = regex\\n                regex = True\\n            items = to_replace.items()\\n            keys, values = itertools.izip(*items)\\n            are_mappings = [isinstance(v, (dict, Series)) for v in values]\\n            if any(are_mappings):\\n                if not all(are_mappings):\\n                    raise TypeError(\"If a nested mapping is passed, all values\"\\n                                    \" of the top level mapping must be \"\\n                                    \"mappings\")\\n                to_rep_dict = {}\\n                value_dict = {}\\n                for k, v in items:\\n                    to_rep_dict[k] = v.keys()\\n                    value_dict[k] = v.values()\\n                to_replace, value = to_rep_dict, value_dict\\n            else:\\n                to_replace, value = keys, values\\n            return self.replace(to_replace, value, method=method, axis=axis,\\n                                inplace=inplace, limit=limit, regex=regex,\\n                                infer_types=infer_types)\\n        else:\\n            if not len(self.columns):\\n                return self\\n            new_data = self._data\\n            if isinstance(to_replace, (dict, Series)):\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for c, src in to_replace.iteritems():\\n                        if c in value and c in self:\\n                            new_data = new_data.replace(src, value[c],\\n                                                        filter=[ c ],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data\\n                    for k, src in to_replace.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(src, value,\\n                                                        filter = [ k ],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                else:\\n                    raise TypeError('Fill value must be scalar, dict, or '\\n                                    'Series')\\n            elif isinstance(to_replace, (list, np.ndarray)):\\n                if isinstance(value, (list, np.ndarray)):\\n                    if len(to_replace) != len(value):\\n                        raise ValueError('Replacement lists must match '\\n                                         'in length. Expecting %d got %d ' %\\n                                         (len(to_replace), len(value)))\\n                    new_data = self._data.replace_list(to_replace, value,\\n                                                       inplace=inplace,\\n                                                       regex=regex)\\n                else:  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n            elif to_replace is None:\\n                if not (_re_compilable(regex) or\\n                        isinstance(regex, (list, dict, np.ndarray, Series))):\\n                    raise TypeError(\"'regex' must be a string or a compiled \"\\n                                    \"regular expression or a list or dict of \"\\n                                    \"strings or regular expressions, you \"\\n                                    \"passed a {0}\".format(type(regex)))\\n                return self.replace(regex, value, method=method, axis=axis,\\n                                    inplace=inplace, limit=limit, regex=True,\\n                                    infer_types=infer_types)\\n            else:\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for k, v in value.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(to_replace, v,\\n                                                        filter=[ k ],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n                else:\\n                    raise TypeError('Invalid \"to_replace\" type: '\\n                                    '{0}'.format(type(to_replace)))  \\n        if infer_types:\\n            new_data = new_data.convert()\\n        if inplace:\\n            self._data = new_data\\n        else:\\n            return self._constructor(new_data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: deprecate unused DataFrame.replace arguments",
    "fixed_code": "def replace(self, to_replace=None, value=None, inplace=False, limit=None,\\n                regex=False, infer_types=False, method=None, axis=None):\\n        if not isinstance(regex, bool) and to_replace is not None:\\n            raise AssertionError(\"'to_replace' must be 'None' if 'regex' is \"\\n                                 \"not a bool\")\\n        if method is not None:\\n            from warnings import warn\\n            warn('the \"method\" argument is deprecated and will be removed in'\\n                 'v0.12; this argument has no effect')\\n        if axis is not None:\\n            from warnings import warn\\n            warn('the \"axis\" argument is deprecated and will be removed in'\\n                 'v0.12; this argument has no effect')\\n        self._consolidate_inplace()\\n        if value is None:\\n            if not isinstance(to_replace, (dict, Series)):\\n                if not isinstance(regex, (dict, Series)):\\n                    raise TypeError('If \"to_replace\" and \"value\" are both None'\\n                                    ' then regex must be a mapping')\\n                to_replace = regex\\n                regex = True\\n            items = to_replace.items()\\n            keys, values = itertools.izip(*items)\\n            are_mappings = [isinstance(v, (dict, Series)) for v in values]\\n            if any(are_mappings):\\n                if not all(are_mappings):\\n                    raise TypeError(\"If a nested mapping is passed, all values\"\\n                                    \" of the top level mapping must be \"\\n                                    \"mappings\")\\n                to_rep_dict = {}\\n                value_dict = {}\\n                for k, v in items:\\n                    to_rep_dict[k] = v.keys()\\n                    value_dict[k] = v.values()\\n                to_replace, value = to_rep_dict, value_dict\\n            else:\\n                to_replace, value = keys, values\\n            return self.replace(to_replace, value, inplace=inplace,\\n                                limit=limit, regex=regex,\\n                                infer_types=infer_types)\\n        else:\\n            if not len(self.columns):\\n                return self\\n            new_data = self._data\\n            if isinstance(to_replace, (dict, Series)):\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for c, src in to_replace.iteritems():\\n                        if c in value and c in self:\\n                            new_data = new_data.replace(src, value[c],\\n                                                        filter=[c],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data\\n                    for k, src in to_replace.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(src, value,\\n                                                        filter=[k],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                else:\\n                    raise TypeError('Fill value must be scalar, dict, or '\\n                                    'Series')\\n            elif isinstance(to_replace, (list, np.ndarray)):\\n                if isinstance(value, (list, np.ndarray)):\\n                    if len(to_replace) != len(value):\\n                        raise ValueError('Replacement lists must match '\\n                                         'in length. Expecting %d got %d ' %\\n                                         (len(to_replace), len(value)))\\n                    new_data = self._data.replace_list(to_replace, value,\\n                                                       inplace=inplace,\\n                                                       regex=regex)\\n                else:  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n            elif to_replace is None:\\n                if not (_re_compilable(regex) or\\n                        isinstance(regex, (list, dict, np.ndarray, Series))):\\n                    raise TypeError(\"'regex' must be a string or a compiled \"\\n                                    \"regular expression or a list or dict of \"\\n                                    \"strings or regular expressions, you \"\\n                                    \"passed a {0}\".format(type(regex)))\\n                return self.replace(regex, value, inplace=inplace, limit=limit,\\n                                    regex=True, infer_types=infer_types)\\n            else:\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for k, v in value.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(to_replace, v,\\n                                                        filter=[k],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n                else:\\n                    raise TypeError('Invalid \"to_replace\" type: '\\n                                    '{0}'.format(type(to_replace)))  \\n        if infer_types:\\n            new_data = new_data.convert()\\n        if inplace:\\n            self._data = new_data\\n        else:\\n            return self._constructor(new_data)"
  },
  {
    "code": "def mode(self):\\n        return self._file.mode\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #10355: SpooledTemporaryFile properties and xreadline method now work for unrolled files.",
    "fixed_code": "def mode(self):\\n        try:\\n            return self._file.mode\\n        except AttributeError:\\n            return self._TemporaryFileArgs[0]\\n    @property"
  },
  {
    "code": "class HttpResponseRedirectBase(HttpResponse):\\n\\tallowed_schemes = ['http', 'https', 'ftp']\\n\\tdef __init__(self, redirect_to, *args, **kwargs):\\n\\t\\tparsed = urlparse(force_text(redirect_to))\\n\\t\\tif parsed.scheme and parsed.scheme not in self.allowed_schemes:\\n\\t\\t\\traise DisallowedRedirect(\"Unsafe redirect to URL with protocol '%s'\" % parsed.scheme)\\n\\t\\tsuper(HttpResponseRedirectBase, self).__init__(*args, **kwargs)\\n\\t\\tself['Location'] = iri_to_uri(redirect_to)\\n\\turl = property(lambda self: self['Location'])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #27606 -- Fixed HttpResponseRedirect.__repr__() crash when DisallowedRedirect is raised.",
    "fixed_code": "class HttpResponseRedirectBase(HttpResponse):\\n\\tallowed_schemes = ['http', 'https', 'ftp']\\n\\tdef __init__(self, redirect_to, *args, **kwargs):\\n\\t\\tsuper(HttpResponseRedirectBase, self).__init__(*args, **kwargs)\\n\\t\\tself['Location'] = iri_to_uri(redirect_to)\\n\\t\\tparsed = urlparse(force_text(redirect_to))\\n\\t\\tif parsed.scheme and parsed.scheme not in self.allowed_schemes:\\n\\t\\t\\traise DisallowedRedirect(\"Unsafe redirect to URL with protocol '%s'\" % parsed.scheme)\\n\\turl = property(lambda self: self['Location'])"
  },
  {
    "code": "def strip_entities(value):\\n    \"Returns the given HTML with all entities (&something;) stripped\"\\n    return re.sub(r'&(?:\\w+|\\nstrip_entities = allow_lazy(strip_entities, unicode)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #4310 -- Fixed a regular expression bug in `strip_entities` function and added tests for several `django.utils.html` functions.  Based on patch from Brian Harring.",
    "fixed_code": "def strip_entities(value):\\n    \"Returns the given HTML with all entities (&something;) stripped\"\\n    return re.sub(r'&(?:\\w+|\\nstrip_entities = allow_lazy(strip_entities, unicode)"
  },
  {
    "code": "def add_command_to_vrf(name, cmd, commands):\\n    if 'vrf definition %s' % name not in commands:\\n        commands.extend([\\n            'vrf definition %s' % name,\\n            'address-family ipv4', 'exit',\\n            'address-family ipv6', 'exit',\\n        ])\\n    commands.append(cmd)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "removed unnecessary ipv* address-family statements from vrf if the ip version is not used (#53866)",
    "fixed_code": "def add_command_to_vrf(name, cmd, commands):\\n    if 'vrf definition %s' % name not in commands:\\n        commands.extend(['vrf definition %s' % name])\\n    commands.append(cmd)"
  },
  {
    "code": "def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\\n        resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        resolved.set_source_expressions([\\n            Coalesce(\\n                expression\\n                if isinstance(expression.output_field, (CharField, TextField))\\n                else Cast(expression, TextField()),\\n                Value('')\\n            ) for expression in resolved.get_source_expressions()\\n        ])\\n        if self.config:\\n            if not hasattr(self.config, 'resolve_expression'):\\n                resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n            else:\\n                resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        return resolved",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\\n        resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        resolved.set_source_expressions([\\n            Coalesce(\\n                expression\\n                if isinstance(expression.output_field, (CharField, TextField))\\n                else Cast(expression, TextField()),\\n                Value('')\\n            ) for expression in resolved.get_source_expressions()\\n        ])\\n        if self.config:\\n            if not hasattr(self.config, 'resolve_expression'):\\n                resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n            else:\\n                resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        return resolved"
  },
  {
    "code": "def handle_error(self, request, client_address):\\n        if is_broken_pipe_error():\\n            logger.info(\"- Broken pipe from %s\\n\", client_address)\\n        else:\\n            super().handle_error(request, client_address)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #25684 -- Removed double newline from request/response output of runserver.\\n\\nFollow up to 0bc5cd628042bf0a44df60a93085a4f991a84dfb.",
    "fixed_code": "def handle_error(self, request, client_address):\\n        if is_broken_pipe_error():\\n            logger.info(\"- Broken pipe from %s\", client_address)\\n        else:\\n            super().handle_error(request, client_address)"
  },
  {
    "code": "def connect(self):\\n        self.sock = self._create_connection(\\n            (self.host,self.port), self.timeout, self.source_address)\\n        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\\n        if self._tunnel_host:\\n            self._tunnel()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-37363: Add audit events to the `http.client` module (GH-21321)\\n\\nAdd audit events to the `http.client` module",
    "fixed_code": "def connect(self):\\n        sys.audit(\"http.client.connect\", self, self.host, self.port)\\n        self.sock = self._create_connection(\\n            (self.host,self.port), self.timeout, self.source_address)\\n        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\\n        if self._tunnel_host:\\n            self._tunnel()"
  },
  {
    "code": "def filter(self, items=None, like=None, regex=None):\\n        import re\\n        if items is not None:\\n            columns = [r for r in items if r in self]\\n        elif like:\\n            columns = [c for c in self.cols() if like in c]\\n        elif regex:\\n            matcher = re.compile(regex)\\n            columns = [c for c in self.cols() if matcher.match(c)]\\n        else:\\n            raise Exception('items was None!')\\n        return self.reindex(columns=columns)\\n    def filterItems(self, items):\\n        return self.filter(items=items)\\n    def filterLike(self, arg):\\n        return self.filter(like=arg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "test coverage. deleted deprecated filter* methods. working on docs for 0.2 release",
    "fixed_code": "def filter(self, items=None, like=None, regex=None):\\n        import re\\n        if items is not None:\\n            columns = [r for r in items if r in self]\\n        elif like:\\n            columns = [c for c in self.cols() if like in c]\\n        elif regex:\\n            matcher = re.compile(regex)\\n            columns = [c for c in self.cols() if matcher.match(c)]\\n        else:\\n            raise Exception('items was None!')\\n        return self.reindex(columns=columns)"
  },
  {
    "code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            return self._get_models_cache[cache_key]\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        return model_list\\n    def get_model(self, app_label, model_name, only_installed=True):\\n        if not self.master:\\n            only_installed = False\\n        self.populate_models()\\n        if only_installed:\\n            app_config = self.app_configs.get(app_label)\\n            if app_config is None:\\n                return None\\n        return self.all_models[app_label].get(model_name.lower())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_models(self, app_mod=None,\\n                   include_auto_created=False, include_deferred=False,\\n                   only_installed=True, include_swapped=False):\\n        if not self.master:\\n            only_installed = False\\n        cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\\n        model_list = None\\n        try:\\n            return self._get_models_cache[cache_key]\\n        except KeyError:\\n            pass\\n        self.populate_models()\\n        if app_mod:\\n            app_label = app_mod.__name__.split('.')[-2]\\n            if only_installed:\\n                try:\\n                    model_dicts = [self.app_configs[app_label].models]\\n                except KeyError:\\n                    model_dicts = []\\n            else:\\n                model_dicts = [self.all_models[app_label]]\\n        else:\\n            if only_installed:\\n                model_dicts = [app_config.models for app_config in self.app_configs.values()]\\n            else:\\n                model_dicts = self.all_models.values()\\n        model_list = []\\n        for model_dict in model_dicts:\\n            model_list.extend(\\n                model for model in model_dict.values()\\n                if ((not model._deferred or include_deferred) and\\n                    (not model._meta.auto_created or include_auto_created) and\\n                    (not model._meta.swapped or include_swapped))\\n            )\\n        self._get_models_cache[cache_key] = model_list\\n        return model_list\\n    def get_model(self, app_label, model_name, only_installed=True):\\n        if not self.master:\\n            only_installed = False\\n        self.populate_models()\\n        if only_installed:\\n            app_config = self.app_configs.get(app_label)\\n            if app_config is None:\\n                return None\\n        return self.all_models[app_label].get(model_name.lower())"
  },
  {
    "code": "def url_patterns(self):\\n        patterns = getattr(self.urlconf_module, \"urlpatterns\", self.urlconf_module)\\n        try:\\n            iter(patterns)\\n        except TypeError:\\n            raise ImproperlyConfigured(\"The included urlconf %s doesn't have any patterns in it\" % self.urlconf_name)\\n        return patterns",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #21435 -- Improved error message when urlconf is empty.\\n\\nThe new error message now hints that the most likely issue\\nis a circular import.\\n\\nThanks to trac user elena for the report and to\\nbpeschier for the original patch.",
    "fixed_code": "def url_patterns(self):\\n        patterns = getattr(self.urlconf_module, \"urlpatterns\", self.urlconf_module)\\n        try:\\n            iter(patterns)\\n        except TypeError:\\n            msg = (\\n                \"The included urlconf '{name}' does not appear to have any \"\\n                \"patterns in it. If you see valid patterns in the file then \"\\n                \"the issue is probably caused by a circular import.\"\\n            )\\n            raise ImproperlyConfigured(msg.format(name=self.urlconf_name))\\n        return patterns"
  },
  {
    "code": "def admin_view(self, view):\\n        \"\"\"\\n        Decorator to create an \"admin view attached to this ``AdminSite``. This\\n        wraps the view and provides permission checking by calling\\n        ``self.has_permission``.\\n        You'll want to use this from within ``AdminSite.get_urls()``:\\n            class MyAdminSite(AdminSite):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #11416 -- Restored use of the never_cache decorator on admin views. Thanks to Ramiro Morales and Michael Newmann for their work on the patch.",
    "fixed_code": "def admin_view(self, view, cacheable=False):\\n        \"\"\"\\n        Decorator to create an admin view attached to this ``AdminSite``. This\\n        wraps the view and provides permission checking by calling\\n        ``self.has_permission``.\\n        You'll want to use this from within ``AdminSite.get_urls()``:\\n            class MyAdminSite(AdminSite):"
  },
  {
    "code": "def take_2d_multi(arr, row_idx, col_idx, fill_value=np.nan):\\n    dtype_str = arr.dtype.name\\n    take_f = _get_take2d_function(dtype_str, axis='multi')\\n    row_idx = _ensure_int64(row_idx)\\n    col_idx = _ensure_int64(col_idx)\\n    out_shape = len(row_idx), len(col_idx)\\n    if dtype_str in ('int32', 'int64', 'bool'):\\n        row_mask = row_idx == -1\\n        col_mask=  col_idx == -1\\n        needs_masking = row_mask.any() or col_mask.any()\\n        if needs_masking:\\n            return take_2d_multi(_maybe_upcast(arr), row_idx, col_idx,\\n                                 fill_value=fill_value)\\n        else:\\n            out = np.empty(out_shape, dtype=arr.dtype)\\n            take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n            return out\\n    elif dtype_str in ('float64', 'object', 'datetime64[us]'):\\n        out = np.empty(out_shape, dtype=arr.dtype)\\n        take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n        return out\\n    else:\\n        return take_2d(take_2d(arr, row_idx, axis=0, fill_value=fill_value),\\n                       col_idx, axis=1, fill_value=fill_value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def take_2d_multi(arr, row_idx, col_idx, fill_value=np.nan):\\n    dtype_str = arr.dtype.name\\n    take_f = _get_take2d_function(dtype_str, axis='multi')\\n    row_idx = _ensure_int64(row_idx)\\n    col_idx = _ensure_int64(col_idx)\\n    out_shape = len(row_idx), len(col_idx)\\n    if dtype_str in ('int32', 'int64', 'bool'):\\n        row_mask = row_idx == -1\\n        col_mask=  col_idx == -1\\n        needs_masking = row_mask.any() or col_mask.any()\\n        if needs_masking:\\n            return take_2d_multi(_maybe_upcast(arr), row_idx, col_idx,\\n                                 fill_value=fill_value)\\n        else:\\n            out = np.empty(out_shape, dtype=arr.dtype)\\n            take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n            return out\\n    elif dtype_str in ('float64', 'object', 'datetime64[us]'):\\n        out = np.empty(out_shape, dtype=arr.dtype)\\n        take_f(arr, row_idx, col_idx, out=out, fill_value=fill_value)\\n        return out\\n    else:\\n        return take_2d(take_2d(arr, row_idx, axis=0, fill_value=fill_value),\\n                       col_idx, axis=1, fill_value=fill_value)"
  },
  {
    "code": "def _repr_fn(fields):\\n    fn = _create_fn('__repr__',\\n                    ('self',),\\n                    ['return self.__class__.__qualname__ + f\"(' +\\n                     ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\\n                                for f in fields]) +\\n                     ')\"'])\\n    return _recursive_repr(fn)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34776: Fix dataclasses to support __future__ \"annotations\" mode (#9518)",
    "fixed_code": "def _repr_fn(fields, globals):\\n    fn = _create_fn('__repr__',\\n                    ('self',),\\n                    ['return self.__class__.__qualname__ + f\"(' +\\n                     ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\\n                                for f in fields]) +\\n                     ')\"'],\\n                     globals=globals)\\n    return _recursive_repr(fn)"
  },
  {
    "code": "def run():\\n    if configuration.conf.get('kerberos', 'keytab') is None:\\n        log.debug(\"Keytab renewer not starting, no keytab configured\")\\n        sys.exit(0)\\n    while True:\\n        renew_from_kt()\\n        time.sleep(configuration.conf.getint('kerberos', 'reinit_frequency'))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-987] pass kerberos cli args keytab and principal to kerberos.run() (#4238)",
    "fixed_code": "def run(principal, keytab):\\n    if not keytab:\\n        log.debug(\"Keytab renewer not starting, no keytab configured\")\\n        sys.exit(0)\\n    while True:\\n        renew_from_kt(principal, keytab)\\n        time.sleep(configuration.conf.getint('kerberos', 'reinit_frequency'))"
  },
  {
    "code": "def import_hook(self, name, caller=None, fromlist=None):\\n        self.msg(3, \"import_hook\", name, caller, fromlist)\\n        parent = self.determine_parent(caller)\\n        q, tail = self.find_head_package(parent, name)\\n        m = self.load_tail(q, tail)\\n        if not fromlist:\\n            return q\\n        if m.__path__:\\n            self.ensure_fromlist(m, fromlist)\\n        return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Modulefinder now handles absolute and relative imports, including tests.\\n\\nWill backport to release25-maint.",
    "fixed_code": "def import_hook(self, name, caller=None, fromlist=None, level=-1):\\n        self.msg(3, \"import_hook\", name, caller, fromlist, level)\\n        parent = self.determine_parent(caller, level=level)\\n        q, tail = self.find_head_package(parent, name)\\n        m = self.load_tail(q, tail)\\n        if not fromlist:\\n            return q\\n        if m.__path__:\\n            self.ensure_fromlist(m, fromlist)\\n        return None"
  },
  {
    "code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               comment=None,\\n               parse_dates=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: option to keep component date columns. #1252",
    "fixed_code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               comment=None,\\n               parse_dates=False,\\n               keep_date_col=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)"
  },
  {
    "code": "def __add__(self, other):\\n        combined = Media()\\n        combined._css_lists = self._css_lists[:]\\n        combined._js_lists = self._js_lists[:]\\n        for item in other._css_lists:\\n            if item and item not in self._css_lists:\\n                combined._css_lists.append(item)\\n        for item in other._js_lists:\\n            if item and item not in self._js_lists:\\n                combined._js_lists.append(item)\\n        return combined",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __add__(self, other):\\n        combined = Media()\\n        combined._css_lists = self._css_lists[:]\\n        combined._js_lists = self._js_lists[:]\\n        for item in other._css_lists:\\n            if item and item not in self._css_lists:\\n                combined._css_lists.append(item)\\n        for item in other._js_lists:\\n            if item and item not in self._js_lists:\\n                combined._js_lists.append(item)\\n        return combined"
  },
  {
    "code": "def load_rules_from_file(self):\\n        self.fname = self.path + \"/\" + self.name\\n        stringline = ''\\n        try:\\n            for line in open(self.fname, 'r'):\\n                stringline += line.rstrip()\\n                stringline += '\\n'\\n            self.load_rules_from_string(stringline)\\n        except IOError:\\n            e = get_exception()\\n            self.ansible.fail_json(msg='Unable to open/read PAM module \\\\n                                   file %s with error %s.  And line %s' %\\n                                   (self.fname, str(e), stringline))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixes #35629 (#35668)",
    "fixed_code": "def load_rules_from_file(self):\\n        self.fname = os.path.join(self.path, self.name)\\n        stringline = ''\\n        try:\\n            for line in open(self.fname, 'r'):\\n                stringline += line.rstrip().lstrip()\\n                stringline += '\\n'\\n            self.load_rules_from_string(stringline.replace(\"\\\\\\n\", \"\"))\\n        except IOError:\\n            e = get_exception()\\n            self.ansible.fail_json(msg='Unable to open/read PAM module \\\\n                                   file %s with error %s.  And line %s' %\\n                                   (self.fname, str(e), stringline))"
  },
  {
    "code": "def _normalize(op1, op2, prec = 0):\\n    if op1.exp < op2.exp:\\n        tmp = op2\\n        other = op1\\n    else:\\n        tmp = op1\\n        other = op2\\n    tmp_len = len(str(tmp.int))\\n    other_len = len(str(other.int))\\n    exp = tmp.exp + min(-1, tmp_len - prec - 2)\\n    if other_len + other.exp - 1 < exp:\\n        other.int = 1\\n        other.exp = exp\\n    tmp.int *= 10 ** (tmp.exp - other.exp)\\n    tmp.exp = other.exp\\n    return op1, op2",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _normalize(op1, op2, prec = 0):\\n    if op1.exp < op2.exp:\\n        tmp = op2\\n        other = op1\\n    else:\\n        tmp = op1\\n        other = op2\\n    tmp_len = len(str(tmp.int))\\n    other_len = len(str(other.int))\\n    exp = tmp.exp + min(-1, tmp_len - prec - 2)\\n    if other_len + other.exp - 1 < exp:\\n        other.int = 1\\n        other.exp = exp\\n    tmp.int *= 10 ** (tmp.exp - other.exp)\\n    tmp.exp = other.exp\\n    return op1, op2"
  },
  {
    "code": "def execute(sql, con, retry=True, cur=None, params=None):\\n    try:\\n        if cur is None:\\n            cur = con.cursor()\\n        if params is None:\\n            cur.execute(sql)\\n        else:\\n            cur.execute(sql, params)\\n        return cur\\n    except Exception:\\n        try:\\n            con.rollback()\\n        except Exception:  \\n            pass\\n        print('Error on sql %s' % sql)\\n        raise",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH #4163 Use SQLAlchemy for DB abstraction\\n\\nTST Import sqlalchemy on Travis.\\n\\nDOC add docstrings to read sql\\n\\nENH read_sql connects via Connection, Engine, file path, or :memory: string\\n\\nCLN Separate legacy code into new file, and fallback so that all old tests pass.\\n\\nTST to use sqlachemy syntax in tests\\n\\nCLN sql into classes, legacy passes\\n\\nFIX few engine vs con calls\\n\\nCLN pep8 cleanup\\n\\nadd postgres support for pandas.io.sql.get_schema\\n\\nWIP: cleaup of sql io module - imported correct SQLALCHEMY type, delete redundant PandasSQLWithCon\\n\\nTODO: renamed _engine_read_table, need to think of a better name.\\nTODO: clean up get_conneciton function\\n\\nENH: cleanup of SQL io\\n\\nTODO: check that legacy mode works\\nTODO: run tests\\n\\ncorrectly enabled coerce_float option\\n\\nCleanup and bug-fixing mainly on legacy mode sql.\\nIMPORTANT - changed legacy to require connection rather than cursor. This is still not yet finalized.\\nTODO: tests and doc\\n\\nAdded Test coverage for basic functionality using in-memory SQLite database\\n\\nSimplified API by automatically distinguishing between engine and connection. Added warnings",
    "fixed_code": "def execute(sql, con, cur=None, params=[], engine=None, flavor='sqlite'):\\n    pandas_sql = pandasSQL_builder(con=con, flavor=flavor)\\n    args = _convert_params(sql, params)\\n    return pandas_sql.execute(*args)"
  },
  {
    "code": "def partial_fit(self, X, y, classes=None, sample_weight=None):\\n        X = atleast2d_or_csr(X, dtype=np.float64)\\n        _, n_features = X.shape\\n        if _check_partial_fit_first_call(self, classes):\\n            n_effective_classes = len(classes) if len(classes) > 1 else 2\\n            self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\\n            self.feature_count_ = np.zeros((n_effective_classes, n_features),\\n                                           dtype=np.float64)\\n        Y = label_binarize(y, classes=self.classes_)\\n        if Y.shape[1] == 1:\\n            Y = np.concatenate((1 - Y, Y), axis=1)\\n        n_samples, n_classes = Y.shape\\n        if X.shape[0] != Y.shape[0]:\\n            msg = \"X.shape[0]=%d and y.shape[0]=%d are incompatible.\"\\n            raise ValueError(msg % (X.shape[0], y.shape[0]))\\n        Y = Y.astype(np.float64)\\n        if sample_weight is not None:\\n            Y *= array2d(sample_weight).T\\n        self._count(X, Y)\\n        self._update_feature_log_prob()\\n        self._update_class_log_prior()\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add partial_fit to GaussianNB\\n\\nUses Chan/Golub/LeVeque update for model parameters. Does not implement\\nsample weighting.",
    "fixed_code": "def partial_fit(self, X, y, classes=None):\\n        X, y = check_arrays(X, y, sparse_format='dense')\\n        y = column_or_1d(y, warn=True)\\n        epsilon = 1e-9\\n        if _check_partial_fit_first_call(self, classes):\\n            n_features = X.shape[1]\\n            n_classes = len(self.classes_)\\n            self.theta_ = np.zeros((n_classes, n_features))\\n            self.sigma_ = np.zeros((n_classes, n_features))\\n            self.class_prior_ = np.zeros(n_classes)\\n            self.class_count_ = np.zeros(n_classes)\\n        else:\\n            self.sigma_[:, :] -= epsilon\\n        class2idx = dict((cls, idx) for idx, cls in enumerate(self.classes_))\\n        for y_i in np.unique(y):\\n            i = class2idx[y_i]\\n            X_i = X[y == y_i, :]\\n            N_i = X_i.shape[0]\\n            new_theta, new_sigma = self._update_mean_variance(\\n                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],\\n                X_i)\\n            self.theta_[i, :] = new_theta\\n            self.sigma_[i, :] = new_sigma\\n            self.class_count_[i] += N_i\\n        self.sigma_[:, :] += epsilon\\n        self.class_prior_[:] = self.class_count_ / np.sum(self.class_count_)\\n        return self"
  },
  {
    "code": "def make_comment(content: str) -> str:\\n    content = content.rstrip()\\n    if not content:\\n        return \"\\n    if content[0] == \"\\n        content = content[1:]\\n    NON_BREAKING_SPACE = \"\u00a0\"\\n    if (\\n        content\\n        and content[0] == NON_BREAKING_SPACE\\n        and not content.lstrip().startswith(\"type:\")\\n    ):\\n        content = \" \" + content[1:]  \\n    if content and content[0] not in \" !:\\n        content = \" \" + content\\n    return \"",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def make_comment(content: str) -> str:\\n    content = content.rstrip()\\n    if not content:\\n        return \"\\n    if content[0] == \"\\n        content = content[1:]\\n    NON_BREAKING_SPACE = \"\u00a0\"\\n    if (\\n        content\\n        and content[0] == NON_BREAKING_SPACE\\n        and not content.lstrip().startswith(\"type:\")\\n    ):\\n        content = \" \" + content[1:]  \\n    if content and content[0] not in \" !:\\n        content = \" \" + content\\n    return \""
  },
  {
    "code": "def get_net_port(self, port):\\n        net_port_get = netapp_utils.zapi.NaElement('net-port-get-iter')\\n        attributes = {\\n            'query': {\\n                'net-port-info': {\\n                    'node': self.parameters['node'],\\n                    'port': port\\n                }\\n            }\\n        }\\n        net_port_get.translate_struct(attributes)\\n        try:\\n            result = self.server.invoke_successfully(net_port_get, True)\\n            if result.get_child_by_name('num-records') and int(result.get_child_content('num-records')) >= 1:\\n                port_info = result['attributes-list']['net-port-info']\\n                port_details = dict()\\n            else:\\n                return None\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error getting net ports for %s: %s' % (self.parameters['node'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        for item_key, zapi_key in self.na_helper.zapi_string_keys.items():\\n            port_details[item_key] = port_info.get_child_content(zapi_key)\\n        return port_details",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_net_port(self, port):\\n        net_port_get = netapp_utils.zapi.NaElement('net-port-get-iter')\\n        attributes = {\\n            'query': {\\n                'net-port-info': {\\n                    'node': self.parameters['node'],\\n                    'port': port\\n                }\\n            }\\n        }\\n        net_port_get.translate_struct(attributes)\\n        try:\\n            result = self.server.invoke_successfully(net_port_get, True)\\n            if result.get_child_by_name('num-records') and int(result.get_child_content('num-records')) >= 1:\\n                port_info = result['attributes-list']['net-port-info']\\n                port_details = dict()\\n            else:\\n                return None\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error getting net ports for %s: %s' % (self.parameters['node'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        for item_key, zapi_key in self.na_helper.zapi_string_keys.items():\\n            port_details[item_key] = port_info.get_child_content(zapi_key)\\n        return port_details"
  },
  {
    "code": "def rolling_count(arg, window, freq=None, center=False, how=None):\\n\\targ = _conv_timerule(arg, freq, how)\\n\\twindow = min(window, len(arg))\\n\\treturn_hook, values = _process_data_structure(arg, kill_inf=False)\\n\\tconverted = np.isfinite(values).astype(float)\\n\\tresult = rolling_sum(converted, window, min_periods=1,\\n\\t\\t\\t\\t\\t\\t center=center)  \\n\\tresult[np.isnan(result)] = 0\\n\\treturn return_hook(result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def rolling_count(arg, window, freq=None, center=False, how=None):\\n\\targ = _conv_timerule(arg, freq, how)\\n\\twindow = min(window, len(arg))\\n\\treturn_hook, values = _process_data_structure(arg, kill_inf=False)\\n\\tconverted = np.isfinite(values).astype(float)\\n\\tresult = rolling_sum(converted, window, min_periods=1,\\n\\t\\t\\t\\t\\t\\t center=center)  \\n\\tresult[np.isnan(result)] = 0\\n\\treturn return_hook(result)"
  },
  {
    "code": "def _high_bit(value):\\n    return value.bit_length() - 1 if value > 0 else -1",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "issue23591: fix flag decomposition and repr",
    "fixed_code": "def _high_bit(value):\\n    return value.bit_length() - 1"
  },
  {
    "code": "def map_obj_to_commands(want, have, module):\\n    commands = list()\\n    if module.params['state'] == 'absent':\\n        if have:\\n            templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s del-br\"\\n                                   \" %(bridge)s\")\\n            command = templatized_command % module.params\\n            commands.append(command)\\n    else:\\n        if have:\\n            if want['fail_mode'] != have['fail_mode']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" set-fail-mode %(bridge)s\"\\n                                       \" %(fail_mode)s\")\\n                command = templatized_command % module.params\\n                commands.append(command)\\n            if want['external_ids'] != have['external_ids']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" br-set-external-id %(bridge)s\")\\n                command = templatized_command % module.params\\n                if want['external_ids']:\\n                    for k, v in iteritems(want['external_ids']):\\n                        if (k not in have['external_ids']\\n                                or want['external_ids'][k] != have['external_ids'][k]):\\n                            command += \" \" + k + \" \" + v\\n                            commands.append(command)\\n            if want['vlan'] and want['vlan'] != have['vlan']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" set port %(bridge)s tag=%(vlan)s\")\\n                command = templatized_command % module.params\\n                commands.append(command)\\n        else:\\n            templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s add-br\"\\n                                   \" %(bridge)s\")\\n            command = templatized_command % module.params\\n            if want['parent']:\\n                templatized_command = \"%(parent)s %(vlan)s\"\\n                command += \" \" + templatized_command % module.params\\n            if want['set']:\\n                templatized_command = \" -- set %(set)s\"\\n                command += templatized_command % module.params\\n            commands.append(command)\\n            if want['fail_mode']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" set-fail-mode %(bridge)s\"\\n                                       \" %(fail_mode)s\")\\n                command = templatized_command % module.params\\n                commands.append(command)\\n            if want['external_ids']:\\n                for k, v in iteritems(want['external_ids']):\\n                    templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                           \" br-set-external-id %(bridge)s\")\\n                    command = templatized_command % module.params\\n                    command += \" \" + k + \" \" + v\\n                    commands.append(command)\\n    return commands",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def map_obj_to_commands(want, have, module):\\n    commands = list()\\n    if module.params['state'] == 'absent':\\n        if have:\\n            templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s del-br\"\\n                                   \" %(bridge)s\")\\n            command = templatized_command % module.params\\n            commands.append(command)\\n    else:\\n        if have:\\n            if want['fail_mode'] != have['fail_mode']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" set-fail-mode %(bridge)s\"\\n                                       \" %(fail_mode)s\")\\n                command = templatized_command % module.params\\n                commands.append(command)\\n            if want['external_ids'] != have['external_ids']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" br-set-external-id %(bridge)s\")\\n                command = templatized_command % module.params\\n                if want['external_ids']:\\n                    for k, v in iteritems(want['external_ids']):\\n                        if (k not in have['external_ids']\\n                                or want['external_ids'][k] != have['external_ids'][k]):\\n                            command += \" \" + k + \" \" + v\\n                            commands.append(command)\\n            if want['vlan'] and want['vlan'] != have['vlan']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" set port %(bridge)s tag=%(vlan)s\")\\n                command = templatized_command % module.params\\n                commands.append(command)\\n        else:\\n            templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s add-br\"\\n                                   \" %(bridge)s\")\\n            command = templatized_command % module.params\\n            if want['parent']:\\n                templatized_command = \"%(parent)s %(vlan)s\"\\n                command += \" \" + templatized_command % module.params\\n            if want['set']:\\n                templatized_command = \" -- set %(set)s\"\\n                command += templatized_command % module.params\\n            commands.append(command)\\n            if want['fail_mode']:\\n                templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                       \" set-fail-mode %(bridge)s\"\\n                                       \" %(fail_mode)s\")\\n                command = templatized_command % module.params\\n                commands.append(command)\\n            if want['external_ids']:\\n                for k, v in iteritems(want['external_ids']):\\n                    templatized_command = (\"%(ovs-vsctl)s -t %(timeout)s\"\\n                                           \" br-set-external-id %(bridge)s\")\\n                    command = templatized_command % module.params\\n                    command += \" \" + k + \" \" + v\\n                    commands.append(command)\\n    return commands"
  },
  {
    "code": "def is_updated_after(self, bucket_name, object_name, ts):\\n        client = self.get_conn()\\n        bucket = client.bucket(bucket_name)\\n        blob = bucket.get_blob(blob_name=object_name)\\n        if blob is None:\\n            raise ValueError(\"Object ({}) not found in Bucket ({})\".format(\\n                object_name, bucket_name))\\n        blob_update_time = blob.updated\\n        if blob_update_time is not None:\\n            import dateutil.tz\\n            if not ts.tzinfo:\\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\\n            if blob_update_time > ts:\\n                return True\\n        return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add ability to specify a maximum modified time for objects in GCSToGCSOperator (#7791)",
    "fixed_code": "def is_updated_after(self, bucket_name, object_name, ts):\\n        blob_update_time = self.get_blob_update_time(bucket_name, object_name)\\n        if blob_update_time is not None:\\n            import dateutil.tz\\n            if not ts.tzinfo:\\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\\n            if blob_update_time > ts:\\n                return True\\n        return False"
  },
  {
    "code": "def _get_data_from_filepath(self, filepath_or_buffer):\\n\\t\\tdata = filepath_or_buffer\\n\\t\\texists = False\\n\\t\\tif isinstance(data, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\texists = os.path.exists(filepath_or_buffer)\\n\\t\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\t\\tpass\\n\\t\\tif exists or self.compression is not None:\\n\\t\\t\\tdata, _ = get_handle(\\n\\t\\t\\t\\tfilepath_or_buffer,\\n\\t\\t\\t\\t\"r\",\\n\\t\\t\\t\\tencoding=self.encoding,\\n\\t\\t\\t\\tcompression=self.compression,\\n\\t\\t\\t)\\n\\t\\t\\tself.should_close = True\\n\\t\\t\\tself.open_stream = data\\n\\t\\tif isinstance(data, BytesIO):\\n\\t\\t\\tdata = data.getvalue().decode()\\n\\t\\treturn data",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_data_from_filepath(self, filepath_or_buffer):\\n\\t\\tdata = filepath_or_buffer\\n\\t\\texists = False\\n\\t\\tif isinstance(data, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\texists = os.path.exists(filepath_or_buffer)\\n\\t\\t\\texcept (TypeError, ValueError):\\n\\t\\t\\t\\tpass\\n\\t\\tif exists or self.compression is not None:\\n\\t\\t\\tdata, _ = get_handle(\\n\\t\\t\\t\\tfilepath_or_buffer,\\n\\t\\t\\t\\t\"r\",\\n\\t\\t\\t\\tencoding=self.encoding,\\n\\t\\t\\t\\tcompression=self.compression,\\n\\t\\t\\t)\\n\\t\\t\\tself.should_close = True\\n\\t\\t\\tself.open_stream = data\\n\\t\\tif isinstance(data, BytesIO):\\n\\t\\t\\tdata = data.getvalue().decode()\\n\\t\\treturn data"
  },
  {
    "code": "def insert(self, loc: int, item) -> Index:\\n        if len(self) and (is_integer(item) or is_float(item)):\\n            rng = self._range\\n            if loc == 0 and item == self[0] - self.step:\\n                new_rng = range(rng.start - rng.step, rng.stop, rng.step)\\n                return type(self)._simple_new(new_rng, name=self.name)\\n            elif loc == len(self) and item == self[-1] + self.step:\\n                new_rng = range(rng.start, rng.stop + rng.step, rng.step)\\n                return type(self)._simple_new(new_rng, name=self.name)\\n            elif len(self) == 2 and item == self[0] + self.step / 2:\\n                step = int(self.step / 2)\\n                new_rng = range(self.start, self.stop, step)\\n                return type(self)._simple_new(new_rng, name=self.name)\\n        return super().insert(loc, item)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def insert(self, loc: int, item) -> Index:\\n        if len(self) and (is_integer(item) or is_float(item)):\\n            rng = self._range\\n            if loc == 0 and item == self[0] - self.step:\\n                new_rng = range(rng.start - rng.step, rng.stop, rng.step)\\n                return type(self)._simple_new(new_rng, name=self.name)\\n            elif loc == len(self) and item == self[-1] + self.step:\\n                new_rng = range(rng.start, rng.stop + rng.step, rng.step)\\n                return type(self)._simple_new(new_rng, name=self.name)\\n            elif len(self) == 2 and item == self[0] + self.step / 2:\\n                step = int(self.step / 2)\\n                new_rng = range(self.start, self.stop, step)\\n                return type(self)._simple_new(new_rng, name=self.name)\\n        return super().insert(loc, item)"
  },
  {
    "code": "def __iter__(self):\\n        return iter(self.data.keys())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __iter__(self):\\n        return iter(self.data.keys())"
  },
  {
    "code": "def main():\\n    fields = {\\n        \"host\": {\"required\": False, \"type\": \"str\"},\\n        \"username\": {\"required\": False, \"type\": \"str\"},\\n        \"password\": {\"required\": False, \"type\": \"str\", \"default\": \"\", \"no_log\": True},\\n        \"vdom\": {\"required\": False, \"type\": \"str\", \"default\": \"root\"},\\n        \"https\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"ssl_verify\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"state\": {\"required\": False, \"type\": \"str\",\\n                  \"choices\": [\"present\", \"absent\"]},\\n        \"vpn_ipsec_phase2_interface\": {\\n            \"required\": False, \"type\": \"dict\", \"default\": None,\\n            \"options\": {\\n                \"state\": {\"required\": False, \"type\": \"str\",\\n                          \"choices\": [\"present\", \"absent\"]},\\n                \"add_route\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"phase1\", \"enable\", \"disable\"]},\\n                \"auto_discovery_forwarder\": {\"required\": False, \"type\": \"str\",\\n                                             \"choices\": [\"phase1\", \"enable\", \"disable\"]},\\n                \"auto_discovery_sender\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"phase1\", \"enable\", \"disable\"]},\\n                \"auto_negotiate\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"comments\": {\"required\": False, \"type\": \"str\"},\\n                \"dhcp_ipsec\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"dhgrp\": {\"required\": False, \"type\": \"str\",\\n                          \"choices\": [\"1\", \"2\", \"5\",\\n                                      \"14\", \"15\", \"16\",\\n                                      \"17\", \"18\", \"19\",\\n                                      \"20\", \"21\", \"27\",\\n                                      \"28\", \"29\", \"30\",\\n                                      \"31\"]},\\n                \"dst_addr_type\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"subnet\", \"range\", \"ip\",\\n                                              \"name\", \"subnet6\", \"range6\",\\n                                              \"ip6\", \"name6\"]},\\n                \"dst_end_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_end_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_name\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_name6\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_port\": {\"required\": False, \"type\": \"int\"},\\n                \"dst_start_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_start_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_subnet\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_subnet6\": {\"required\": False, \"type\": \"str\"},\\n                \"encapsulation\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"tunnel-mode\", \"transport-mode\"]},\\n                \"keepalive\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"enable\", \"disable\"]},\\n                \"keylife_type\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"seconds\", \"kbs\", \"both\"]},\\n                \"keylifekbs\": {\"required\": False, \"type\": \"int\"},\\n                \"keylifeseconds\": {\"required\": False, \"type\": \"int\"},\\n                \"l2tp\": {\"required\": False, \"type\": \"str\",\\n                         \"choices\": [\"enable\", \"disable\"]},\\n                \"name\": {\"required\": True, \"type\": \"str\"},\\n                \"pfs\": {\"required\": False, \"type\": \"str\",\\n                        \"choices\": [\"enable\", \"disable\"]},\\n                \"phase1name\": {\"required\": False, \"type\": \"str\"},\\n                \"proposal\": {\"required\": False, \"type\": \"str\",\\n                             \"choices\": [\"null-md5\", \"null-sha1\", \"null-sha256\",\\n                                         \"null-sha384\", \"null-sha512\", \"des-null\",\\n                                         \"des-md5\", \"des-sha1\", \"des-sha256\",\\n                                         \"des-sha384\", \"des-sha512\"]},\\n                \"protocol\": {\"required\": False, \"type\": \"int\"},\\n                \"replay\": {\"required\": False, \"type\": \"str\",\\n                           \"choices\": [\"enable\", \"disable\"]},\\n                \"route_overlap\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"use-old\", \"use-new\", \"allow\"]},\\n                \"single_source\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"src_addr_type\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"subnet\", \"range\", \"ip\",\\n                                              \"name\", \"subnet6\", \"range6\",\\n                                              \"ip6\", \"name6\"]},\\n                \"src_end_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"src_end_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"src_name\": {\"required\": False, \"type\": \"str\"},\\n                \"src_name6\": {\"required\": False, \"type\": \"str\"},\\n                \"src_port\": {\"required\": False, \"type\": \"int\"},\\n                \"src_start_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"src_start_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"src_subnet\": {\"required\": False, \"type\": \"str\"},\\n                \"src_subnet6\": {\"required\": False, \"type\": \"str\"}\\n            }\\n        }\\n    }\\n    module = AnsibleModule(argument_spec=fields,\\n                           supports_check_mode=False)\\n    legacy_mode = 'host' in module.params and module.params['host'] is not None and \\\\n                  'username' in module.params and module.params['username'] is not None and \\\\n                  'password' in module.params and module.params['password'] is not None\\n    if not legacy_mode:\\n        if module._socket_path:\\n            connection = Connection(module._socket_path)\\n            fos = FortiOSHandler(connection)\\n            is_error, has_changed, result = fortios_vpn_ipsec(module.params, fos)\\n        else:\\n            module.fail_json(**FAIL_SOCKET_MSG)\\n    else:\\n        try:\\n            from fortiosapi import FortiOSAPI\\n        except ImportError:\\n            module.fail_json(msg=\"fortiosapi module is required\")\\n        fos = FortiOSAPI()\\n        login(module.params, fos)\\n        is_error, has_changed, result = fortios_vpn_ipsec(module.params, fos)\\n        fos.logout()\\n    if not is_error:\\n        module.exit_json(changed=has_changed, meta=result)\\n    else:\\n        module.fail_json(msg=\"Error in repo\", meta=result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fortinet's new module for fortios_vpn_ipsec_phase2_interface (#64767)",
    "fixed_code": "def main():\\n    fields = {\\n        \"host\": {\"required\": False, \"type\": \"str\"},\\n        \"username\": {\"required\": False, \"type\": \"str\"},\\n        \"password\": {\"required\": False, \"type\": \"str\", \"default\": \"\", \"no_log\": True},\\n        \"vdom\": {\"required\": False, \"type\": \"str\", \"default\": \"root\"},\\n        \"https\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"ssl_verify\": {\"required\": False, \"type\": \"bool\", \"default\": True},\\n        \"state\": {\"required\": False, \"type\": \"str\",\\n                  \"choices\": [\"present\", \"absent\"]},\\n        \"vpn_ipsec_phase2_interface\": {\\n            \"required\": False, \"type\": \"dict\", \"default\": None,\\n            \"options\": {\\n                \"state\": {\"required\": False, \"type\": \"str\",\\n                          \"choices\": [\"present\", \"absent\"]},\\n                \"add_route\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"phase1\", \"enable\", \"disable\"]},\\n                \"auto_discovery_forwarder\": {\"required\": False, \"type\": \"str\",\\n                                             \"choices\": [\"phase1\", \"enable\", \"disable\"]},\\n                \"auto_discovery_sender\": {\"required\": False, \"type\": \"str\",\\n                                          \"choices\": [\"phase1\", \"enable\", \"disable\"]},\\n                \"auto_negotiate\": {\"required\": False, \"type\": \"str\",\\n                                   \"choices\": [\"enable\", \"disable\"]},\\n                \"comments\": {\"required\": False, \"type\": \"str\"},\\n                \"dhcp_ipsec\": {\"required\": False, \"type\": \"str\",\\n                               \"choices\": [\"enable\", \"disable\"]},\\n                \"dhgrp\": {\"required\": False, \"type\": \"str\",\\n                          \"choices\": [\"1\", \"2\", \"5\",\\n                                      \"14\", \"15\", \"16\",\\n                                      \"17\", \"18\", \"19\",\\n                                      \"20\", \"21\", \"27\",\\n                                      \"28\", \"29\", \"30\",\\n                                      \"31\"]},\\n                \"dst_addr_type\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"subnet\", \"range\", \"ip\",\\n                                              \"name\", \"subnet6\", \"range6\",\\n                                              \"ip6\", \"name6\"]},\\n                \"dst_end_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_end_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_name\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_name6\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_port\": {\"required\": False, \"type\": \"int\"},\\n                \"dst_start_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_start_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_subnet\": {\"required\": False, \"type\": \"str\"},\\n                \"dst_subnet6\": {\"required\": False, \"type\": \"str\"},\\n                \"encapsulation\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"tunnel-mode\", \"transport-mode\"]},\\n                \"keepalive\": {\"required\": False, \"type\": \"str\",\\n                              \"choices\": [\"enable\", \"disable\"]},\\n                \"keylife_type\": {\"required\": False, \"type\": \"str\",\\n                                 \"choices\": [\"seconds\", \"kbs\", \"both\"]},\\n                \"keylifekbs\": {\"required\": False, \"type\": \"int\"},\\n                \"keylifeseconds\": {\"required\": False, \"type\": \"int\"},\\n                \"l2tp\": {\"required\": False, \"type\": \"str\",\\n                         \"choices\": [\"enable\", \"disable\"]},\\n                \"name\": {\"required\": True, \"type\": \"str\"},\\n                \"pfs\": {\"required\": False, \"type\": \"str\",\\n                        \"choices\": [\"enable\", \"disable\"]},\\n                \"phase1name\": {\"required\": False, \"type\": \"str\"},\\n                \"proposal\": {\"required\": False, \"type\": \"list\",\\n                             \"choices\": [\"null-md5\", \"null-sha1\", \"null-sha256\",\\n                                         \"null-sha384\", \"null-sha512\", \"des-null\",\\n                                         \"des-md5\", \"des-sha1\", \"des-sha256\",\\n                                         \"des-sha384\", \"des-sha512\", \"3des-null\",\\n                                         \"3des-md5\", \"3des-sha1\", \"3des-sha256\",\\n                                         \"3des-sha384\", \"3des-sha512\", \"aes128-null\",\\n                                         \"aes128-md5\", \"aes128-sha1\", \"aes128-sha256\",\\n                                         \"aes128-sha384\", \"aes128-sha512\", \"aes128gcm\",\\n                                         \"aes192-null\", \"aes192-md5\", \"aes192-sha1\",\\n                                         \"aes192-sha256\", \"aes192-sha384\", \"aes192-sha512\",\\n                                         \"aes256-null\", \"aes256-md5\", \"aes256-sha1\",\\n                                         \"aes256-sha256\", \"aes256-sha384\", \"aes256-sha512\",\\n                                         \"aes256gcm\", \"chacha20poly1305\", \"aria128-null\",\\n                                         \"aria128-md5\", \"aria128-sha1\", \"aria128-sha256\",\\n                                         \"aria128-sha384\", \"aria128-sha512\", \"aria192-null\",\\n                                         \"aria192-md5\", \"aria192-sha1\", \"aria192-sha256\",\\n                                         \"aria192-sha384\", \"aria192-sha512\", \"aria256-null\",\\n                                         \"aria256-md5\", \"aria256-sha1\", \"aria256-sha256\",\\n                                         \"aria256-sha384\", \"aria256-sha512\", \"seed-null\",\\n                                         \"seed-md5\", \"seed-sha1\", \"seed-sha256\",\\n                                         \"seed-sha384\", \"seed-sha512\"]},\\n                \"protocol\": {\"required\": False, \"type\": \"int\"},\\n                \"replay\": {\"required\": False, \"type\": \"str\",\\n                           \"choices\": [\"enable\", \"disable\"]},\\n                \"route_overlap\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"use-old\", \"use-new\", \"allow\"]},\\n                \"single_source\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"enable\", \"disable\"]},\\n                \"src_addr_type\": {\"required\": False, \"type\": \"str\",\\n                                  \"choices\": [\"subnet\", \"range\", \"ip\",\\n                                              \"name\", \"subnet6\", \"range6\",\\n                                              \"ip6\", \"name6\"]},\\n                \"src_end_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"src_end_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"src_name\": {\"required\": False, \"type\": \"str\"},\\n                \"src_name6\": {\"required\": False, \"type\": \"str\"},\\n                \"src_port\": {\"required\": False, \"type\": \"int\"},\\n                \"src_start_ip\": {\"required\": False, \"type\": \"str\"},\\n                \"src_start_ip6\": {\"required\": False, \"type\": \"str\"},\\n                \"src_subnet\": {\"required\": False, \"type\": \"str\"},\\n                \"src_subnet6\": {\"required\": False, \"type\": \"str\"}\\n            }\\n        }\\n    }\\n    module = AnsibleModule(argument_spec=fields,\\n                           supports_check_mode=False)\\n    legacy_mode = 'host' in module.params and module.params['host'] is not None and \\\\n                  'username' in module.params and module.params['username'] is not None and \\\\n                  'password' in module.params and module.params['password'] is not None\\n    if not legacy_mode:\\n        if module._socket_path:\\n            connection = Connection(module._socket_path)\\n            fos = FortiOSHandler(connection)\\n            is_error, has_changed, result = fortios_vpn_ipsec(module.params, fos)\\n        else:\\n            module.fail_json(**FAIL_SOCKET_MSG)\\n    else:\\n        try:\\n            from fortiosapi import FortiOSAPI\\n        except ImportError:\\n            module.fail_json(msg=\"fortiosapi module is required\")\\n        fos = FortiOSAPI()\\n        login(module.params, fos)\\n        is_error, has_changed, result = fortios_vpn_ipsec(module.params, fos)\\n        fos.logout()\\n    if not is_error:\\n        module.exit_json(changed=has_changed, meta=result)\\n    else:\\n        module.fail_json(msg=\"Error in repo\", meta=result)"
  },
  {
    "code": "def _create_provider_info_schema_validator():\\n    schema = json.loads(importlib_resources.read_text('airflow', 'provider_info.schema.json'))\\n    cls = jsonschema.validators.validator_for(schema)\\n    validator = cls(schema)\\n    return validator",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Move away from legacy importlib.resources API (#19091)",
    "fixed_code": "def _create_provider_info_schema_validator():\\n    with resource_files(\"airflow\").joinpath(\"provider_info.schema.json\").open(\"rb\") as f:\\n        schema = json.load(f)\\n    cls = jsonschema.validators.validator_for(schema)\\n    validator = cls(schema)\\n    return validator"
  },
  {
    "code": "def _setmode(self, mode=None):\\n        if mode is None:\\n            return self._mode\\n        if mode not in [\"standard\", \"logo\", \"world\"]:\\n            return\\n        self._mode = mode\\n        if mode in [\"standard\", \"world\"]:\\n            self._angleOffset = 0\\n            self._angleOrient = 1\\n        else: \\n            self._angleOffset = self._fullcircle/4.\\n            self._angleOrient = -1",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _setmode(self, mode=None):\\n        if mode is None:\\n            return self._mode\\n        if mode not in [\"standard\", \"logo\", \"world\"]:\\n            return\\n        self._mode = mode\\n        if mode in [\"standard\", \"world\"]:\\n            self._angleOffset = 0\\n            self._angleOrient = 1\\n        else: \\n            self._angleOffset = self._fullcircle/4.\\n            self._angleOrient = -1"
  },
  {
    "code": "def _reformat_schedule(self):\\n        if SCHEDULE not in self.body:\\n            return\\n        self._reformat_date(SCHEDULE_START_DATE)\\n        self._reformat_date(SCHEDULE_END_DATE)\\n        self._reformat_time(START_TIME_OF_DAY)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5104] Set default schedule for GCP Transfer operators (#5726)\\n\\nThe GCS Transfer Service REST API requires that a schedule be set, even for\\none-time immediate runs. This adds code to\\n`S3ToGoogleCloudStorageTransferOperator` and\\n`GoogleCloudStorageToGoogleCloudStorageTransferOperator` to set a default\\none-time immediate run schedule when no `schedule` argument is passed.",
    "fixed_code": "def _reformat_schedule(self):\\n        if SCHEDULE not in self.body:\\n            if self.default_schedule:\\n                self.body[SCHEDULE] = {\\n                    SCHEDULE_START_DATE: date.today(),\\n                    SCHEDULE_END_DATE: date.today()\\n                }\\n            else:\\n                return\\n        self._reformat_date(SCHEDULE_START_DATE)\\n        self._reformat_date(SCHEDULE_END_DATE)\\n        self._reformat_time(START_TIME_OF_DAY)"
  },
  {
    "code": "def bind(__bind_self, *args, **kwargs):\\n        s signature.  Raises `TypeError`\\n        if the passed arguments can not be bound.\\n        ''Get a BoundArguments object, that partially maps the\\n        passed `args` and `kwargs` to the function's signature.\\n        Raises `TypeError` if the passed arguments can not be bound.\\n        '''\\n        return __bind_self._bind(args, kwargs, partial=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def bind(__bind_self, *args, **kwargs):\\n        s signature.  Raises `TypeError`\\n        if the passed arguments can not be bound.\\n        ''Get a BoundArguments object, that partially maps the\\n        passed `args` and `kwargs` to the function's signature.\\n        Raises `TypeError` if the passed arguments can not be bound.\\n        '''\\n        return __bind_self._bind(args, kwargs, partial=True)"
  },
  {
    "code": "def xs(self, key, axis=1, copy=True):\\n        assert(axis >= 1)\\n        i = self.axes[axis].get_loc(key)\\n        slicer = [slice(None, None) for _ in range(self.ndim)]\\n        slicer[axis] = i\\n        slicer = tuple(slicer)\\n        new_axes = list(self.axes)\\n        new_axes.pop(axis)\\n        new_blocks = []\\n        if len(self.blocks) > 1:\\n            if not copy:\\n                raise Exception('cannot get view of mixed-type or '\\n                                'non-consolidated DataFrame')\\n            for blk in self.blocks:\\n                newb = make_block(blk.values[slicer], blk.items, blk.ref_items)\\n                new_blocks.append(newb)\\n        elif len(self.blocks) == 1:\\n            vals = self.blocks[0].values[slicer]\\n            if copy:\\n                vals = vals.copy()\\n            new_blocks = [make_block(vals, self.items, self.items)]\\n        return BlockManager(new_blocks, new_axes)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def xs(self, key, axis=1, copy=True):\\n        assert(axis >= 1)\\n        i = self.axes[axis].get_loc(key)\\n        slicer = [slice(None, None) for _ in range(self.ndim)]\\n        slicer[axis] = i\\n        slicer = tuple(slicer)\\n        new_axes = list(self.axes)\\n        new_axes.pop(axis)\\n        new_blocks = []\\n        if len(self.blocks) > 1:\\n            if not copy:\\n                raise Exception('cannot get view of mixed-type or '\\n                                'non-consolidated DataFrame')\\n            for blk in self.blocks:\\n                newb = make_block(blk.values[slicer], blk.items, blk.ref_items)\\n                new_blocks.append(newb)\\n        elif len(self.blocks) == 1:\\n            vals = self.blocks[0].values[slicer]\\n            if copy:\\n                vals = vals.copy()\\n            new_blocks = [make_block(vals, self.items, self.items)]\\n        return BlockManager(new_blocks, new_axes)"
  },
  {
    "code": "def confusion_matrix(y_true, y_pred, labels=None):\\n\\tif labels is None:\\n\\t\\tlabels = unique_labels(y_true, y_pred)\\n\\telse:\\n\\t\\tlabels = np.asarray(labels, dtype=np.int)\\n\\tn_labels = labels.size\\n\\tlabel_to_ind = dict((y, x) for x, y in enumerate(labels))\\n\\ty_pred = np.array([label_to_ind[x] for x in y_pred])\\n\\ty_true = np.array([label_to_ind[x] for x in y_true])\\n\\ty_pred = y_pred[y_pred < n_labels]\\n\\ty_true = y_true[y_true < n_labels]\\n\\tCM = np.asarray(coo_matrix((np.ones(y_true.shape[0]),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t(y_true, y_pred)),\\n\\t\\t\\t\\t\\t\\t\\t   shape=(n_labels, n_labels),\\n\\t\\t\\t\\t\\t\\t\\t   dtype=np.int).todense())\\n\\treturn CM",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX issue #1239 when confusion matrix y_true/y_labels has unexpected labels",
    "fixed_code": "def confusion_matrix(y_true, y_pred, labels=None):\\n\\tif labels is None:\\n\\t\\tlabels = unique_labels(y_true, y_pred)\\n\\telse:\\n\\t\\tlabels = np.asarray(labels, dtype=np.int)\\n\\tn_labels = labels.size\\n\\tlabel_to_ind = dict((y, x) for x, y in enumerate(labels))\\n\\ty_pred = np.array([label_to_ind.get(x,n_labels+1) for x in y_pred])\\n\\ty_true = np.array([label_to_ind.get(x,n_labels+1) for x in y_true])\\n\\tind = np.logical_and(y_pred < n_labels, y_true < n_labels)\\n\\ty_pred = y_pred[ind]\\n\\ty_true = y_true[ind]\\n\\tCM = np.asarray(coo_matrix((np.ones(y_true.shape[0]),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t(y_true, y_pred)),\\n\\t\\t\\t\\t\\t\\t\\t   shape=(n_labels, n_labels),\\n\\t\\t\\t\\t\\t\\t\\t   dtype=np.int).todense())\\n\\treturn CM"
  },
  {
    "code": "def update_device_interface(fmg, paramgram):\\n    access_list = list()\\n    allow_access_list = paramgram[\"interface_allow_access\"].replace(' ', '')\\n    access_list = allow_access_list.split(',')\\n    datagram = {\\n        \"allowaccess\": access_list,\\n        \"ip\": paramgram[\"interface_ip\"]\\n    }\\n    url = \"/pm/config/device/{device_name}/global/system/interface\" \\\\n          \"/{interface}\".format(device_name=paramgram[\"device_unique_name\"], interface=paramgram[\"interface\"])\\n    response = fmg.update(url, datagram)\\n    return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_device_config (#52765)",
    "fixed_code": "def update_device_interface(fmgr, paramgram):\\n    access_list = list()\\n    allow_access_list = paramgram[\"interface_allow_access\"].replace(' ', '')\\n    access_list = allow_access_list.split(',')\\n    datagram = {\\n        \"allowaccess\": access_list,\\n        \"ip\": paramgram[\"interface_ip\"]\\n    }\\n    url = \"/pm/config/device/{device_name}/global/system/interface\" \\\\n          \"/{interface}\".format(device_name=paramgram[\"device_unique_name\"], interface=paramgram[\"interface\"])\\n    response = fmgr.process_request(url, datagram, FMGRMethods.UPDATE)\\n    return response"
  },
  {
    "code": "def _take_nd_object(\\n    arr: np.ndarray,\\n    indexer: np.ndarray,  \\n    out: np.ndarray,\\n    axis: int,\\n    fill_value,\\n    mask_info,\\n):\\n    if mask_info is not None:\\n        mask, needs_masking = mask_info\\n    else:\\n        mask = indexer == -1\\n        needs_masking = mask.any()\\n    if arr.dtype != out.dtype:\\n        arr = arr.astype(out.dtype)\\n    if arr.shape[axis] > 0:\\n        arr.take(ensure_platform_int(indexer), axis=axis, out=out)\\n    if needs_masking:\\n        outindexer = [slice(None)] * arr.ndim\\n        outindexer[axis] = mask\\n        out[tuple(outindexer)] = fill_value",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TYP: intp in libalgos (#40623)",
    "fixed_code": "def _take_nd_object(\\n    arr: np.ndarray,\\n    indexer: np.ndarray,  \\n    out: np.ndarray,\\n    axis: int,\\n    fill_value,\\n    mask_info,\\n):\\n    if mask_info is not None:\\n        mask, needs_masking = mask_info\\n    else:\\n        mask = indexer == -1\\n        needs_masking = mask.any()\\n    if arr.dtype != out.dtype:\\n        arr = arr.astype(out.dtype)\\n    if arr.shape[axis] > 0:\\n        arr.take(indexer, axis=axis, out=out)\\n    if needs_masking:\\n        outindexer = [slice(None)] * arr.ndim\\n        outindexer[axis] = mask\\n        out[tuple(outindexer)] = fill_value"
  },
  {
    "code": "def itertuples(self, index=True):\\n        arrays = []\\n        if index:\\n            arrays.append(self.index)\\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\\n        return zip(*arrays)\\n    if compat.PY3:  \\n        items = iteritems",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: itertuples() returns namedtuples",
    "fixed_code": "def itertuples(self, index=True, name=\"Pandas\"):\\n        arrays = []\\n        fields = []\\n        if index:\\n            arrays.append(self.index)\\n            fields.append(\"Index\")\\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\\n        if len(self.columns) + index < 256:\\n            try:\\n                itertuple = collections.namedtuple(\\n                    name, fields+list(self.columns), rename=True)\\n                return (itertuple(*row) for row in zip(*arrays))\\n            except:\\n                pass\\n        return zip(*arrays)\\n    if compat.PY3:  \\n        items = iteritems"
  },
  {
    "code": "def get_host(self):\\n\\t\\thost = self._get_raw_host()\\n\\t\\tallowed_hosts = settings.ALLOWED_HOSTS\\n\\t\\tif settings.DEBUG and not allowed_hosts:\\n\\t\\t\\tallowed_hosts = ['localhost', '127.0.0.1', '[::1]']\\n\\t\\tdomain, port = split_domain_port(host)\\n\\t\\tif domain and validate_host(domain, allowed_hosts):\\n\\t\\t\\treturn host\\n\\t\\telse:\\n\\t\\t\\tmsg = \"Invalid HTTP_HOST header: %r.\" % host\\n\\t\\t\\tif domain:\\n\\t\\t\\t\\tmsg += \" You may need to add %r to ALLOWED_HOSTS.\" % domain\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmsg += \" The domain name provided is not valid according to RFC 1034/1035.\"\\n\\t\\t\\traise DisallowedHost(msg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #31010 -- Allowed subdomains of localhost in the Host header by default when DEBUG=True.",
    "fixed_code": "def get_host(self):\\n\\t\\thost = self._get_raw_host()\\n\\t\\tallowed_hosts = settings.ALLOWED_HOSTS\\n\\t\\tif settings.DEBUG and not allowed_hosts:\\n\\t\\t\\tallowed_hosts = ['.localhost', '127.0.0.1', '[::1]']\\n\\t\\tdomain, port = split_domain_port(host)\\n\\t\\tif domain and validate_host(domain, allowed_hosts):\\n\\t\\t\\treturn host\\n\\t\\telse:\\n\\t\\t\\tmsg = \"Invalid HTTP_HOST header: %r.\" % host\\n\\t\\t\\tif domain:\\n\\t\\t\\t\\tmsg += \" You may need to add %r to ALLOWED_HOSTS.\" % domain\\n\\t\\t\\telse:\\n\\t\\t\\t\\tmsg += \" The domain name provided is not valid according to RFC 1034/1035.\"\\n\\t\\t\\traise DisallowedHost(msg)"
  },
  {
    "code": "def _to_wide_mixed(self, mask):\\n        _, N, K = self.wide_shape\\n        data = {}\\n        for i, item in enumerate(self.items):\\n            item_vals = self[item].values\\n            values = np.empty((N, K), dtype=item_vals.dtype)\\n            values.ravel()[mask] = item_vals\\n            data[item] = DataFrame(values, index=self.major_axis,\\n                                   columns=self.minor_axis)\\n        return WidePanel.from_dict(data)\\n    toWide = deprecate('toWide', to_wide)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _to_wide_mixed(self, mask):\\n        _, N, K = self.wide_shape\\n        data = {}\\n        for i, item in enumerate(self.items):\\n            item_vals = self[item].values\\n            values = np.empty((N, K), dtype=item_vals.dtype)\\n            values.ravel()[mask] = item_vals\\n            data[item] = DataFrame(values, index=self.major_axis,\\n                                   columns=self.minor_axis)\\n        return WidePanel.from_dict(data)\\n    toWide = deprecate('toWide', to_wide)"
  },
  {
    "code": "def to_string(self, buf=None, columns=None, col_space=None, header=True,\\n                  index=True, na_rep='NaN', formatters=None, float_format=None,\\n                  sparsify=None, index_names=True, justify=None,\\n                  max_rows=None, min_rows=None, max_cols=None,\\n                  show_dimensions=False, decimal='.', line_width=None):\\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\\n                                           col_space=col_space, na_rep=na_rep,\\n                                           formatters=formatters,\\n                                           float_format=float_format,\\n                                           sparsify=sparsify, justify=justify,\\n                                           index_names=index_names,\\n                                           header=header, index=index,\\n                                           min_rows=min_rows,\\n                                           max_rows=max_rows,\\n                                           max_cols=max_cols,\\n                                           show_dimensions=show_dimensions,\\n                                           decimal=decimal,\\n                                           line_width=line_width)\\n        formatter.to_string()\\n        if buf is None:\\n            result = formatter.buf.getvalue()\\n            return result\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_string(self, buf=None, columns=None, col_space=None, header=True,\\n                  index=True, na_rep='NaN', formatters=None, float_format=None,\\n                  sparsify=None, index_names=True, justify=None,\\n                  max_rows=None, min_rows=None, max_cols=None,\\n                  show_dimensions=False, decimal='.', line_width=None):\\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\\n                                           col_space=col_space, na_rep=na_rep,\\n                                           formatters=formatters,\\n                                           float_format=float_format,\\n                                           sparsify=sparsify, justify=justify,\\n                                           index_names=index_names,\\n                                           header=header, index=index,\\n                                           min_rows=min_rows,\\n                                           max_rows=max_rows,\\n                                           max_cols=max_cols,\\n                                           show_dimensions=show_dimensions,\\n                                           decimal=decimal,\\n                                           line_width=line_width)\\n        formatter.to_string()\\n        if buf is None:\\n            result = formatter.buf.getvalue()\\n            return result\\n    @property"
  },
  {
    "code": "def _process_worker(call_queue, result_queue):\\n    while True:\\n        call_item = call_queue.get(block=True)\\n        if call_item is None:\\n            result_queue.put(os.getpid())\\n            return\\n        try:\\n            r = call_item.fn(*call_item.args, **call_item.kwargs)\\n        except BaseException as e:\\n            exc = _ExceptionWithTraceback(e, e.__traceback__)\\n            result_queue.put(_ResultItem(call_item.work_id, exception=exc))\\n        else:\\n            result_queue.put(_ResultItem(call_item.work_id,\\n                                         result=r))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _process_worker(call_queue, result_queue):\\n    while True:\\n        call_item = call_queue.get(block=True)\\n        if call_item is None:\\n            result_queue.put(os.getpid())\\n            return\\n        try:\\n            r = call_item.fn(*call_item.args, **call_item.kwargs)\\n        except BaseException as e:\\n            exc = _ExceptionWithTraceback(e, e.__traceback__)\\n            result_queue.put(_ResultItem(call_item.work_id, exception=exc))\\n        else:\\n            result_queue.put(_ResultItem(call_item.work_id,\\n                                         result=r))"
  },
  {
    "code": "def build_ssm_path(self, conn_id: str):\\n        param_name = (CONN_ENV_PREFIX + conn_id).upper()\\n        param_path = self.prefix + \"/\" + param_name\\n        return param_path",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5705] Make AwsSsmSecretsBackend consistent with VaultBackend (#7753)",
    "fixed_code": "def build_ssm_path(self, conn_id: str):\\n        param_path = self.connections_prefix + \"/\" + conn_id\\n        return param_path"
  },
  {
    "code": "def nansum(a, axis=None):\\n    \"\"\"\\n    Return the sum of array elements over a given axis treating\\n    Not a Numbers (NaNs) as zero.\\n    Parameters\\n    ----------\\n    a : array_like\\n        Array containing numbers whose sum is desired. If `a` is not an\\n        array, a conversion is attempted.\\n    axis : int, optional\\n        Axis along which the sum is computed. The default is to compute\\n        the sum of the flattened array.\\n    Returns\\n    -------\\n    y : ndarray\\n        An array with the same shape as a, with the specified axis removed.\\n        If a is a 0-d array, or if axis is None, a scalar is returned with\\n        the same dtype as `a`.\\n    See Also\\n    --------\\n    numpy.sum : Sum across array including Not a Numbers.\\n    isnan : Shows which elements are Not a Number (NaN).\\n    isfinite: Shows which elements are not: Not a Number, positive and\\n             negative infinity\\n    Notes\\n    -----\\n    Numpy uses the IEEE Standard for Binary Floating-Point for Arithmetic\\n    (IEEE 754). This means that Not a Number is not equivalent to infinity.\\n    If positive or negative infinity are present the result is positive or\\n    negative infinity. But if both positive and negative infinity are present,\\n    the result is Not A Number (NaN).\\n    Arithmetic is modular when using integer types (all elements of `a` must\\n    be finite i.e. no elements that are NaNs, positive infinity and negative\\n    infinity because NaNs are floating point types), and no error is raised\\n    on overflow.",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT: Refactor nanfunctions.\\n\\nnanmax, nanmin\\n--------------\\nAdd out and keepdims keywords.\\n\\nnanargmin, nanargmax\\n--------------------\\nA NanWarning is raised if an all NaN slice detected. For all such\\nslices np.iingo(np.intp).min is returned as the index value.\\n\\nnansum\\n------\\nThe keywords dtype, out, and keepdims are added.\\n\\nA FutureWarning is raised, as in the future the mean of an empty\\nslice after NaN replacement will be 0 instead of the current NaN.\\n\\nnanmean, nanvar, nanstd\\n-----------------------\\nFor all, if the input array is of inexact type then the dtype and out\\nparameters must be of inexact type if specified.  That insures that NaNs\\ncan be returned when appropriate.\\n\\nThe nanmean function detects empty slices after NaN replacement and\\nraises a NanWarning. NaN is returned as the value for all such slices.\\n\\nThe nanmean and nanstd functions detect degrees of freedom <= 0 after\\nNaN replacement and raise a NanWarning. NaN is returned as the value for\\nall such slices.",
    "fixed_code": "def nansum(a, axis=None, dtype=None, out=None, keepdims=0):\\n    \"\"\"\\n    Return the sum of array elements over a given axis treating\\n    Not a Numbers (NaNs) as zero.\\n    FutureWarning: In Numpy versions <= 1.8 Nan is returned for slices that\\n    are all NaN or empty. In later versions zero will be returned."
  },
  {
    "code": "def _classify_pyc(data, name, exc_details):\\n    magic = data[:4]\\n    if magic != MAGIC_NUMBER:\\n        message = f'bad magic number in {name!r}: {magic!r}'\\n        _bootstrap._verbose_message('{}', message)\\n        raise ImportError(message, **exc_details)\\n    if len(data) < 16:\\n        message = f'reached EOF while reading pyc header of {name!r}'\\n        _bootstrap._verbose_message('{}', message)\\n        raise EOFError(message)\\n    flags = _unpack_uint32(data[4:8])\\n    if flags & ~0b11:\\n        message = f'invalid flags {flags!r} in {name!r}'\\n        raise ImportError(message, **exc_details)\\n    return flags",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _classify_pyc(data, name, exc_details):\\n    magic = data[:4]\\n    if magic != MAGIC_NUMBER:\\n        message = f'bad magic number in {name!r}: {magic!r}'\\n        _bootstrap._verbose_message('{}', message)\\n        raise ImportError(message, **exc_details)\\n    if len(data) < 16:\\n        message = f'reached EOF while reading pyc header of {name!r}'\\n        _bootstrap._verbose_message('{}', message)\\n        raise EOFError(message)\\n    flags = _unpack_uint32(data[4:8])\\n    if flags & ~0b11:\\n        message = f'invalid flags {flags!r} in {name!r}'\\n        raise ImportError(message, **exc_details)\\n    return flags"
  },
  {
    "code": "def parse_makefile(fn, g=None):\\n    from distutils.text_file import TextFile\\n    fp = TextFile(fn, strip_comments=1, skip_blanks=1, join_lines=1)\\n    if g is None:\\n        g = {}\\n    done = {}\\n    notdone = {}\\n    while True:\\n        line = fp.readline()\\n        if line is None: \\n            break\\n        m = _variable_rx.match(line)\\n        if m:\\n            n, v = m.group(1, 2)\\n            v = v.strip()\\n            tmpv = v.replace('$$', '')\\n            if \"$\" in tmpv:\\n                notdone[n] = v\\n            else:\\n                try:\\n                    v = int(v)\\n                except ValueError:\\n                    done[n] = v.replace('$$', '$')\\n                else:\\n                    done[n] = v\\n    while notdone:\\n        for name in list(notdone):\\n            value = notdone[name]\\n            m = _findvar1_rx.search(value) or _findvar2_rx.search(value)\\n            if m:\\n                n = m.group(1)\\n                found = True\\n                if n in done:\\n                    item = str(done[n])\\n                elif n in notdone:\\n                    found = False\\n                elif n in os.environ:\\n                    item = os.environ[n]\\n                else:\\n                    done[n] = item = \"\"\\n                if found:\\n                    after = value[m.end():]\\n                    value = value[:m.start()] + item + after\\n                    if \"$\" in after:\\n                        notdone[name] = value\\n                    else:\\n                        try: value = int(value)\\n                        except ValueError:\\n                            done[name] = value.strip()\\n                        else:\\n                            done[name] = value\\n                        del notdone[name]\\n            else:\\n                del notdone[name]\\n    fp.close()\\n    g.update(done)\\n    return g",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Ensure that the Makefile variable expansion in distutils.sysconfig matches that in the toplevel sysconfig module.\\n\\nWithout this patch universal builds on OSX are\\nbroken.\\n\\nAls add a test that checks that the two version\\nof get_config_vars agree on important values.",
    "fixed_code": "def parse_makefile(fn, g=None):\\n    from distutils.text_file import TextFile\\n    fp = TextFile(fn, strip_comments=1, skip_blanks=1, join_lines=1)\\n    if g is None:\\n        g = {}\\n    done = {}\\n    notdone = {}\\n    while True:\\n        line = fp.readline()\\n        if line is None: \\n            break\\n        m = _variable_rx.match(line)\\n        if m:\\n            n, v = m.group(1, 2)\\n            v = v.strip()\\n            tmpv = v.replace('$$', '')\\n            if \"$\" in tmpv:\\n                notdone[n] = v\\n            else:\\n                try:\\n                    v = int(v)\\n                except ValueError:\\n                    done[n] = v.replace('$$', '$')\\n                else:\\n                    done[n] = v\\n    renamed_variables = ('CFLAGS', 'LDFLAGS', 'CPPFLAGS')\\n    while notdone:\\n        for name in list(notdone):\\n            value = notdone[name]\\n            m = _findvar1_rx.search(value) or _findvar2_rx.search(value)\\n            if m:\\n                n = m.group(1)\\n                found = True\\n                if n in done:\\n                    item = str(done[n])\\n                elif n in notdone:\\n                    found = False\\n                elif n in os.environ:\\n                    item = os.environ[n]\\n                elif n in renamed_variables:\\n                    if name.startswith('PY_') and name[3:] in renamed_variables:\\n                        item = \"\"\\n                    elif 'PY_' + n in notdone:\\n                        found = False\\n                    else:\\n                        item = str(done['PY_' + n])\\n                else:\\n                    done[n] = item = \"\"\\n                if found:\\n                    after = value[m.end():]\\n                    value = value[:m.start()] + item + after\\n                    if \"$\" in after:\\n                        notdone[name] = value\\n                    else:\\n                        try: value = int(value)\\n                        except ValueError:\\n                            done[name] = value.strip()\\n                        else:\\n                            done[name] = value\\n                        del notdone[name]\\n                        if name.startswith('PY_') \\\\n                            and name[3:] in renamed_variables:\\n                            name = name[3:]\\n                            if name not in done:\\n                                done[name] = value\\n            else:\\n                del notdone[name]\\n    fp.close()\\n    g.update(done)\\n    return g"
  },
  {
    "code": "def init_appbuilder_links(app):\\n    appbuilder = app.appbuilder\\n    appbuilder.add_link(\"Documentation\", href=get_docs_url(), category=\"Docs\")\\n    appbuilder.add_link(\"Airflow Website\", href='https://airflow.apache.org', category=\"Docs\")\\n    appbuilder.add_link(\"GitHub Repo\", href='https://github.com/apache/airflow', category=\"Docs\")\\n    appbuilder.add_link(\\n        \"REST API Reference (Swagger UI)\", href='/api/v1./api/v1_swagger_ui_index', category=\"Docs\"\\n    )\\n    appbuilder.add_link(\"REST API Reference (Redoc)\", href=\"RedocView.redoc\", category='Docs')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Allow viewers to see all docs links (#14197)\\n\\nCurrently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and above) to see all of the links.",
    "fixed_code": "def init_appbuilder_links(app):\\n    appbuilder = app.appbuilder\\n    appbuilder.add_link(name=\"docs_links\", label=\"Documentation\", href=get_docs_url(), category=\"Docs\")\\n    appbuilder.add_link(\\n        name=\"docs_links\", label=\"Airflow Website\", href='https://airflow.apache.org', category=\"Docs\"\\n    )\\n    appbuilder.add_link(\\n        name=\"docs_links\", label=\"GitHub Repo\", href='https://github.com/apache/airflow', category=\"Docs\"\\n    )\\n    appbuilder.add_link(\\n        name=\"docs_links\",\\n        label=\"REST API Reference (Swagger UI)\",\\n        href='/api/v1./api/v1_swagger_ui_index',\\n        category=\"Docs\",\\n    )\\n    appbuilder.add_link(\\n        name=\"docs_links\", label=\"REST API Reference (Redoc)\", href=\"RedocView.redoc\", category='Docs'\\n    )"
  },
  {
    "code": "def set_continue(self):\\n        self._set_stopinfo(self.botframe, None)\\n        if not self.breaks:\\n            sys.settrace(None)\\n            frame = sys._getframe().f_back\\n            while frame and frame is not self.botframe:\\n                del frame.f_trace\\n                frame = frame.f_back",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 83429,83436 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/release27-maint\\n\\n................\\n  r83429 | georg.brandl | 2010-08-01 21:14:56 +0200 (So, 01 Aug 2010) | 37 lines\\n\\n  Merged revisions 83352,83356-83358,83362,83366,83368-83369 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n  ........\\n    r83352 | georg.brandl | 2010-07-31 20:11:07 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #9440: Remove borderline test case that fails based on unpredictable conditions such as compiler flags.\\n  ........\\n    r83356 | georg.brandl | 2010-07-31 21:29:15 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    Remove trailing whitespace.\\n  ........\\n    r83357 | georg.brandl | 2010-07-31 21:59:55 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #5778: document that sys.version can contain a newline.\\n  ........\\n    r83358 | georg.brandl | 2010-07-31 22:05:31 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #9442: do not document a specific format for sys.version; rather refer to version_info and the platform module.\\n  ........\\n    r83362 | georg.brandl | 2010-07-31 23:12:15 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    #8910: add a file explaining why Lib/test/data is there.\\n  ........\\n    r83366 | georg.brandl | 2010-07-31 23:26:40 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    There always is a False and True now.\\n  ........\\n    r83368 | georg.brandl | 2010-07-31 23:40:15 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n  ........\\n    r83369 | georg.brandl | 2010-07-31 23:41:42 +0200 (Sa, 31 Jul 2010) | 1 line\\n\\n    Fix \"Berkeley\" name.\\n  ........\\n................\\n  r83436 | georg.brandl | 2010-08-01 21:33:15 +0200 (So, 01 Aug 2010) | 42 lines\\n\\n  Merged revisions 83259,83261,83264-83265,83268-83269,83271-83272,83281 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n  ........\\n    r83259 | georg.brandl | 2010-07-30 09:03:39 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    Clarification.\\n  ........\\n    r83261 | georg.brandl | 2010-07-30 09:21:26 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #9230: allow Pdb.checkline() to be called without a current frame, for setting breakpoints before starting debugging.\\n  ........\\n    r83264 | georg.brandl | 2010-07-30 10:45:26 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    Document the \"jump\" command in pdb.__doc__, and add a version tag for \"until X\".\\n  ........\\n    r83265 | georg.brandl | 2010-07-30 10:54:49 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #8015: fix crash when entering an empty line for breakpoint commands.  Also restore environment properly when an exception occurs during the definition of commands.\\n  ........\\n    r83268 | georg.brandl | 2010-07-30 11:23:23 +0200 (Fr, 30 Jul 2010) | 2 lines\\n\\n    Issue #8048: Prevent doctests from failing when sys.displayhook has\\n    been reassigned.\\n  ........\\n    r83269 | georg.brandl | 2010-07-30 11:43:00 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #6719: In pdb, do not stop somewhere in the encodings machinery if the source file to be debugged is in a non-builtin encoding.\\n  ........\\n    r83271 | georg.brandl | 2010-07-30 11:59:28 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #5727: Restore the ability to use readline when calling into pdb in doctests.\\n  ........\\n    r83272 | georg.brandl | 2010-07-30 12:29:19 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    #5294: Fix the behavior of pdb \"continue\" command when called in the top-level debugged frame.\\n  ........\\n    r83281 | georg.brandl | 2010-07-30 15:36:43 +0200 (Fr, 30 Jul 2010) | 1 line\\n\\n    Add myself for pdb.\\n  ........\\n................",
    "fixed_code": "def set_continue(self):\\n        self._set_stopinfo(self.botframe, None, -1)\\n        if not self.breaks:\\n            sys.settrace(None)\\n            frame = sys._getframe().f_back\\n            while frame and frame is not self.botframe:\\n                del frame.f_trace\\n                frame = frame.f_back"
  },
  {
    "code": "def apply(self):\\n        ''\\n        self.asup_log_for_cserver(\"na_ontap_svm\")\\n        current = self.get_vserver()\\n        cd_action, rename = None, None\\n        if self.parameters.get('from_name'):\\n            rename = self.na_helper.is_rename_action(self.get_vserver(self.parameters['from_name']), current)\\n        else:\\n            cd_action = self.na_helper.get_cd_action(current, self.parameters)\\n        modify = self.na_helper.get_modified_attributes(current, self.parameters)\\n        for attribute in modify:\\n            if attribute in ['root_volume', 'root_volume_aggregate', 'root_volume_security_style', 'subtype', 'ipspace']:\\n                self.module.fail_json(msg='Error modifying SVM %s: can not modify %s.' % (self.parameters['name'], attribute))\\n            if attribute == 'language':\\n                if self.parameters['language'].lower() == 'c.utf-8':\\n                    self.parameters['language'] = 'c.utf_8'\\n        if self.na_helper.changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if rename:\\n                    self.rename_vserver()\\n                if cd_action == 'create':\\n                    self.create_vserver()\\n                elif cd_action == 'delete':\\n                    self.delete_vserver()\\n                elif modify:\\n                    self.modify_vserver(modify)\\n        self.module.exit_json(changed=self.na_helper.changed)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def apply(self):\\n        ''\\n        self.asup_log_for_cserver(\"na_ontap_svm\")\\n        current = self.get_vserver()\\n        cd_action, rename = None, None\\n        if self.parameters.get('from_name'):\\n            rename = self.na_helper.is_rename_action(self.get_vserver(self.parameters['from_name']), current)\\n        else:\\n            cd_action = self.na_helper.get_cd_action(current, self.parameters)\\n        modify = self.na_helper.get_modified_attributes(current, self.parameters)\\n        for attribute in modify:\\n            if attribute in ['root_volume', 'root_volume_aggregate', 'root_volume_security_style', 'subtype', 'ipspace']:\\n                self.module.fail_json(msg='Error modifying SVM %s: can not modify %s.' % (self.parameters['name'], attribute))\\n            if attribute == 'language':\\n                if self.parameters['language'].lower() == 'c.utf-8':\\n                    self.parameters['language'] = 'c.utf_8'\\n        if self.na_helper.changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if rename:\\n                    self.rename_vserver()\\n                if cd_action == 'create':\\n                    self.create_vserver()\\n                elif cd_action == 'delete':\\n                    self.delete_vserver()\\n                elif modify:\\n                    self.modify_vserver(modify)\\n        self.module.exit_json(changed=self.na_helper.changed)"
  },
  {
    "code": "def _get_next_week(self, date):\\n\\t\\treturn date + datetime.timedelta(days=7 - self._get_weekday(date))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #28209 -- Made date-based generic views return a 404 rather than crash when given an out of range date.",
    "fixed_code": "def _get_next_week(self, date):\\n\\t\\ttry:\\n\\t\\t\\treturn date + datetime.timedelta(days=7 - self._get_weekday(date))\\n\\t\\texcept OverflowError:\\n\\t\\t\\traise Http404(_(\"Date out of range\"))"
  },
  {
    "code": "def _type_convert(arg, module=None, *, allow_special_forms=False):\\n    if arg is None:\\n        return type(None)\\n    if isinstance(arg, str):\\n        return ForwardRef(arg, module=module, is_class=allow_special_forms)\\n    return arg",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _type_convert(arg, module=None, *, allow_special_forms=False):\\n    if arg is None:\\n        return type(None)\\n    if isinstance(arg, str):\\n        return ForwardRef(arg, module=module, is_class=allow_special_forms)\\n    return arg"
  },
  {
    "code": "def run_and_check(self, session, prepped_request, extra_options):\\n        extra_options = extra_options or {}\\n        response = session.send(\\n            prepped_request,\\n            stream=extra_options.get(\"stream\", False),\\n            verify=extra_options.get(\"verify\", False),\\n            proxies=extra_options.get(\"proxies\", {}),\\n            cert=extra_options.get(\"cert\"),\\n            timeout=extra_options.get(\"timeout\"),\\n            allow_redirects=extra_options.get(\"allow_redirects\", True))\\n        try:\\n            response.raise_for_status()\\n        except requests.exceptions.HTTPError:\\n            self.log.error(\"HTTP error: %s\", response.reason)\\n            if self.method not in ('GET', 'HEAD'):\\n                self.log.error(response.text)\\n            raise AirflowException(str(response.status_code)+\":\"+response.reason)\\n        return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2110][AIRFLOW-2122] Enhance Http Hook\\n\\n- Use a header in passed in the \"extra\" argument and\\n  add tenacity retry\\n- Fix the tests with proper mocking\\n\\nCloses #3071 from albertocalderari/master",
    "fixed_code": "def run_and_check(self, session, prepped_request, extra_options):\\n        extra_options = extra_options or {}\\n        try:\\n            response = session.send(\\n                prepped_request,\\n                stream=extra_options.get(\"stream\", False),\\n                verify=extra_options.get(\"verify\", False),\\n                proxies=extra_options.get(\"proxies\", {}),\\n                cert=extra_options.get(\"cert\"),\\n                timeout=extra_options.get(\"timeout\"),\\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\\n            if extra_options.get('check_response', True):\\n                self.check_response(response)\\n            return response\\n        except requests.exceptions.ConnectionError as ex:\\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\\n            raise ex"
  },
  {
    "code": "def interact(self):\\n        if sys.platform == \"win32\":\\n            self.mt_interact()\\n            return\\n        while 1:\\n            rfd, wfd, xfd = select.select([self, sys.stdin], [], [])\\n            if self in rfd:\\n                try:\\n                    text = self.read_eager()\\n                except EOFError:\\n                    print('*** Connection closed by remote host ***')\\n                    break\\n                if text:\\n                    sys.stdout.write(text.decode('ascii'))\\n                    sys.stdout.flush()\\n            if sys.stdin in rfd:\\n                line = sys.stdin.readline().encode('ascii')\\n                if not line:\\n                    break\\n                self.write(line)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #19170: telnetlib: use selectors.",
    "fixed_code": "def interact(self):\\n        if sys.platform == \"win32\":\\n            self.mt_interact()\\n            return\\n        with _TelnetSelector() as selector:\\n            selector.register(self, selectors.EVENT_READ)\\n            selector.register(sys.stdin, selectors.EVENT_READ)\\n            while True:\\n                for key, events in selector.select():\\n                    if key.fileobj is self:\\n                        try:\\n                            text = self.read_eager()\\n                        except EOFError:\\n                            print('*** Connection closed by remote host ***')\\n                            return\\n                        if text:\\n                            sys.stdout.write(text.decode('ascii'))\\n                            sys.stdout.flush()\\n                    elif key.fileobj is sys.stdin:\\n                        line = sys.stdin.readline().encode('ascii')\\n                        if not line:\\n                            return\\n                        self.write(line)"
  },
  {
    "code": "def check_class_weight_classifiers(name, Classifier):\\n    if name == \"NuSVC\":\\n        raise SkipTest\\n    if name.endswith(\"NB\"):\\n        raise SkipTest\\n    for n_centers in [2, 3]:\\n        X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\\n                                                            random_state=0)\\n        n_centers = len(np.unique(y_train))\\n        if n_centers == 2:\\n            class_weight = {0: 1000, 1: 0.0001}\\n        else:\\n            class_weight = {0: 1000, 1: 0.0001, 2: 0.0001}\\n        with warnings.catch_warnings(record=True):\\n            classifier = Classifier(class_weight=class_weight)\\n        if hasattr(classifier, \"n_iter\"):\\n            classifier.set_params(n_iter=100)\\n        if hasattr(classifier, \"min_weight_fraction_leaf\"):\\n            classifier.set_params(min_weight_fraction_leaf=0.01)\\n        set_random_state(classifier)\\n        classifier.fit(X_train, y_train)\\n        y_pred = classifier.predict(X_test)\\n        assert_greater(np.mean(y_pred == 0), 0.89)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_class_weight_classifiers(name, Classifier):\\n    if name == \"NuSVC\":\\n        raise SkipTest\\n    if name.endswith(\"NB\"):\\n        raise SkipTest\\n    for n_centers in [2, 3]:\\n        X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\\n                                                            random_state=0)\\n        n_centers = len(np.unique(y_train))\\n        if n_centers == 2:\\n            class_weight = {0: 1000, 1: 0.0001}\\n        else:\\n            class_weight = {0: 1000, 1: 0.0001, 2: 0.0001}\\n        with ignore_warnings(category=DeprecationWarning):\\n            classifier = Classifier(class_weight=class_weight)\\n        if hasattr(classifier, \"n_iter\"):\\n            classifier.set_params(n_iter=100)\\n        if hasattr(classifier, \"min_weight_fraction_leaf\"):\\n            classifier.set_params(min_weight_fraction_leaf=0.01)\\n        set_random_state(classifier)\\n        classifier.fit(X_train, y_train)\\n        y_pred = classifier.predict(X_test)\\n        assert_greater(np.mean(y_pred == 0), 0.89)"
  },
  {
    "code": "def _intersection(self, other, sort=False):\\n        if not isinstance(other, RangeIndex):\\n            if is_dtype_equal(other.dtype, self.dtype):\\n                result = super()._intersection(other, sort=sort)\\n                return self._wrap_setop_result(other, result)\\n            return super().intersection(other, sort=sort)\\n        if not len(self) or not len(other):\\n            return self._simple_new(_empty_range)\\n        first = self._range[::-1] if self.step < 0 else self._range\\n        second = other._range[::-1] if other.step < 0 else other._range\\n        int_low = max(first.start, second.start)\\n        int_high = min(first.stop, second.stop)\\n        if int_high <= int_low:\\n            return self._simple_new(_empty_range)\\n        gcd, s, t = self._extended_gcd(first.step, second.step)\\n        if (first.start - second.start) % gcd:\\n            return self._simple_new(_empty_range)\\n        tmp_start = first.start + (second.start - first.start) * first.step // gcd * s\\n        new_step = first.step * second.step // gcd\\n        new_range = range(tmp_start, int_high, new_step)\\n        new_index = self._simple_new(new_range)\\n        new_start = new_index._min_fitting_element(int_low)\\n        new_range = range(new_start, new_index.stop, new_index.step)\\n        new_index = self._simple_new(new_range)\\n        if (self.step < 0 and other.step < 0) is not (new_index.step < 0):\\n            new_index = new_index[::-1]\\n        if sort is None:\\n            new_index = new_index.sort_values()\\n        return self._wrap_setop_result(other, new_index)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: name attr in RangeIndex.intersection (#38197)",
    "fixed_code": "def _intersection(self, other, sort=False):\\n        if not isinstance(other, RangeIndex):\\n            return super()._intersection(other, sort=sort)\\n        if not len(self) or not len(other):\\n            return self._simple_new(_empty_range)\\n        first = self._range[::-1] if self.step < 0 else self._range\\n        second = other._range[::-1] if other.step < 0 else other._range\\n        int_low = max(first.start, second.start)\\n        int_high = min(first.stop, second.stop)\\n        if int_high <= int_low:\\n            return self._simple_new(_empty_range)\\n        gcd, s, t = self._extended_gcd(first.step, second.step)\\n        if (first.start - second.start) % gcd:\\n            return self._simple_new(_empty_range)\\n        tmp_start = first.start + (second.start - first.start) * first.step // gcd * s\\n        new_step = first.step * second.step // gcd\\n        new_range = range(tmp_start, int_high, new_step)\\n        new_index = self._simple_new(new_range)\\n        new_start = new_index._min_fitting_element(int_low)\\n        new_range = range(new_start, new_index.stop, new_index.step)\\n        new_index = self._simple_new(new_range)\\n        if (self.step < 0 and other.step < 0) is not (new_index.step < 0):\\n            new_index = new_index[::-1]\\n        if sort is None:\\n            new_index = new_index.sort_values()\\n        return new_index"
  },
  {
    "code": "def format_file_in_place(\\n    src: Path,\\n    fast: bool,\\n    mode: Mode,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    if src.suffix == \".pyi\":\\n        mode = replace(mode, is_pyi=True)\\n    then = datetime.utcfromtimestamp(src.stat().st_mtime)\\n    with open(src, \"rb\") as buf:\\n        src_contents, encoding, newline = decode_bytes(buf.read())\\n    try:\\n        dst_contents = format_file_contents(src_contents, fast=fast, mode=mode)\\n    except NothingChanged:\\n        return False\\n    if write_back == WriteBack.YES:\\n        with open(src, \"w\", encoding=encoding, newline=newline) as f:\\n            f.write(dst_contents)\\n    elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\\n        now = datetime.utcnow()\\n        src_name = f\"{src}\\t{then} +0000\"\\n        dst_name = f\"{src}\\t{now} +0000\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if write_back == write_back.COLOR_DIFF:\\n            diff_contents = color_diff(diff_contents)\\n        with lock or nullcontext():\\n            f = io.TextIOWrapper(\\n                sys.stdout.buffer,\\n                encoding=encoding,\\n                newline=newline,\\n                write_through=True,\\n            )\\n            f = wrap_stream_for_windows(f)\\n            f.write(diff_contents)\\n            f.detach()\\n    return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_file_in_place(\\n    src: Path,\\n    fast: bool,\\n    mode: Mode,\\n    write_back: WriteBack = WriteBack.NO,\\n    lock: Any = None,  \\n) -> bool:\\n    if src.suffix == \".pyi\":\\n        mode = replace(mode, is_pyi=True)\\n    then = datetime.utcfromtimestamp(src.stat().st_mtime)\\n    with open(src, \"rb\") as buf:\\n        src_contents, encoding, newline = decode_bytes(buf.read())\\n    try:\\n        dst_contents = format_file_contents(src_contents, fast=fast, mode=mode)\\n    except NothingChanged:\\n        return False\\n    if write_back == WriteBack.YES:\\n        with open(src, \"w\", encoding=encoding, newline=newline) as f:\\n            f.write(dst_contents)\\n    elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\\n        now = datetime.utcnow()\\n        src_name = f\"{src}\\t{then} +0000\"\\n        dst_name = f\"{src}\\t{now} +0000\"\\n        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\\n        if write_back == write_back.COLOR_DIFF:\\n            diff_contents = color_diff(diff_contents)\\n        with lock or nullcontext():\\n            f = io.TextIOWrapper(\\n                sys.stdout.buffer,\\n                encoding=encoding,\\n                newline=newline,\\n                write_through=True,\\n            )\\n            f = wrap_stream_for_windows(f)\\n            f.write(diff_contents)\\n            f.detach()\\n    return True"
  },
  {
    "code": "def setitem(self, indexer, value):\\n        transpose = self.ndim == 2\\n        if value is None:\\n            if self.is_numeric:\\n                value = np.nan\\n        values = self.values\\n        if self._can_hold_element(value):\\n            value = self._try_coerce_args(value)\\n        else:\\n            find_dtype = False\\n            if hasattr(value, \"dtype\"):\\n                dtype = value.dtype\\n                find_dtype = True\\n            elif lib.is_scalar(value) and not isna(value):\\n                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)\\n                find_dtype = True\\n            if find_dtype:\\n                dtype = find_common_type([values.dtype, dtype])\\n                if not is_dtype_equal(self.dtype, dtype):\\n                    b = self.astype(dtype)\\n                    return b.setitem(indexer, value)\\n        arr_value = np.array(value)\\n        if not self._can_hold_element(value):\\n            dtype, _ = maybe_promote(arr_value.dtype)\\n            values = values.astype(dtype)\\n        if transpose:\\n            values = values.T\\n        check_setitem_lengths(indexer, value, values)\\n        if is_empty_indexer(indexer, arr_value):\\n            pass\\n        elif is_scalar_indexer(indexer, arr_value):\\n            values[indexer] = value\\n        elif (\\n            len(arr_value.shape)\\n            and arr_value.shape[0] == values.shape[0]\\n            and arr_value.size == values.size\\n        ):\\n            values[indexer] = value\\n            try:\\n                values = values.astype(arr_value.dtype)\\n            except ValueError:\\n                pass\\n        else:\\n            values[indexer] = value\\n        if transpose:\\n            values = values.T\\n        block = self.make_block(values)\\n        return block",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: remove Block._try_coerce_arg (#29139)",
    "fixed_code": "def setitem(self, indexer, value):\\n        transpose = self.ndim == 2\\n        if value is None:\\n            if self.is_numeric:\\n                value = np.nan\\n        values = self.values\\n        if self._can_hold_element(value):\\n            if lib.is_scalar(value):\\n                value = convert_scalar(values, value)\\n        else:\\n            find_dtype = False\\n            if hasattr(value, \"dtype\"):\\n                dtype = value.dtype\\n                find_dtype = True\\n            elif lib.is_scalar(value) and not isna(value):\\n                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)\\n                find_dtype = True\\n            if find_dtype:\\n                dtype = find_common_type([values.dtype, dtype])\\n                if not is_dtype_equal(self.dtype, dtype):\\n                    b = self.astype(dtype)\\n                    return b.setitem(indexer, value)\\n        if is_extension_array_dtype(getattr(value, \"dtype\", None)):\\n            arr_value = value\\n        else:\\n            arr_value = np.array(value)\\n        if not self._can_hold_element(value):\\n            dtype, _ = maybe_promote(arr_value.dtype)\\n            values = values.astype(dtype)\\n        if transpose:\\n            values = values.T\\n        check_setitem_lengths(indexer, value, values)\\n        if is_empty_indexer(indexer, arr_value):\\n            pass\\n        elif is_scalar_indexer(indexer, arr_value):\\n            values[indexer] = value\\n        elif (\\n            len(arr_value.shape)\\n            and arr_value.shape[0] == values.shape[0]\\n            and arr_value.size == values.size\\n        ):\\n            values[indexer] = value\\n            try:\\n                values = values.astype(arr_value.dtype)\\n            except ValueError:\\n                pass\\n        else:\\n            values[indexer] = value\\n        if transpose:\\n            values = values.T\\n        block = self.make_block(values)\\n        return block"
  },
  {
    "code": "def execute(sql, con, cur=None, params=None):\\n    if cur is None:\\n        pandas_sql = pandasSQL_builder(con)\\n    else:\\n        pandas_sql = pandasSQL_builder(cur, is_cursor=True)\\n    args = _convert_params(sql, params)\\n    return pandas_sql.execute(*args)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def execute(sql, con, cur=None, params=None):\\n    if cur is None:\\n        pandas_sql = pandasSQL_builder(con)\\n    else:\\n        pandas_sql = pandasSQL_builder(cur, is_cursor=True)\\n    args = _convert_params(sql, params)\\n    return pandas_sql.execute(*args)"
  },
  {
    "code": "def parse_multipart(fp, pdict, encoding=\"utf-8\", errors=\"replace\"):\\n\\tboundary = pdict['boundary'].decode('ascii')\\n\\tctype = \"multipart/form-data; boundary={}\".format(boundary)\\n\\theaders = Message()\\n\\theaders.set_type(ctype)\\n\\ttry:\\n\\t\\theaders['Content-Length'] = pdict['CONTENT-LENGTH']\\n\\texcept KeyError:\\n\\t\\tpass\\n\\tfs = FieldStorage(fp, headers=headers, encoding=encoding, errors=errors,\\n\\t\\tenviron={'REQUEST_METHOD': 'POST'})\\n\\treturn {k: fs.getlist(k) for k in fs}",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-42967: only use '&' as a query string separator (#24297)\\n\\nbpo-42967: [security] Address a web cache-poisoning issue reported in urllib.parse.parse_qsl().\\n\\nurllib.parse will only us \"&\" as query string separator by default instead of both \";\" and \"&\" as allowed in earlier versions. An optional argument seperator with default value \"&\" is added to specify the separator.",
    "fixed_code": "def parse_multipart(fp, pdict, encoding=\"utf-8\", errors=\"replace\", separator='&'):\\n\\tboundary = pdict['boundary'].decode('ascii')\\n\\tctype = \"multipart/form-data; boundary={}\".format(boundary)\\n\\theaders = Message()\\n\\theaders.set_type(ctype)\\n\\ttry:\\n\\t\\theaders['Content-Length'] = pdict['CONTENT-LENGTH']\\n\\texcept KeyError:\\n\\t\\tpass\\n\\tfs = FieldStorage(fp, headers=headers, encoding=encoding, errors=errors,\\n\\t\\tenviron={'REQUEST_METHOD': 'POST'}, separator=separator)\\n\\treturn {k: fs.getlist(k) for k in fs}"
  },
  {
    "code": "class ServerThread(threading.Thread):\\n\\t\\tdef __init__(self, urlhandler, port):\\n\\t\\t\\tself.urlhandler = urlhandler\\n\\t\\t\\tself.port = int(port)\\n\\t\\t\\tthreading.Thread.__init__(self)\\n\\t\\t\\tself.serving = False\\n\\t\\t\\tself.error = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-31128: Allow pydoc to bind to arbitrary hostnames (#3011)\\n\\nNew -n flag allow overriding localhost with custom value,\\nfor example to run from containers.",
    "fixed_code": "class ServerThread(threading.Thread):\\n\\t\\tdef __init__(self, urlhandler, host, port):\\n\\t\\t\\tself.urlhandler = urlhandler\\n\\t\\t\\tself.host = host\\n\\t\\t\\tself.port = int(port)\\n\\t\\t\\tthreading.Thread.__init__(self)\\n\\t\\t\\tself.serving = False\\n\\t\\t\\tself.error = None"
  },
  {
    "code": "def _safe_import_hook(self, name, caller, fromlist):\\n        if name in self.badmodules:\\n            self._add_badmodule(name, caller)\\n            return\\n        try:\\n            self.import_hook(name, caller)\\n        except ImportError, msg:\\n            self.msg(2, \"ImportError:\", str(msg))\\n            self._add_badmodule(name, caller)\\n        else:\\n            if fromlist:\\n                for sub in fromlist:\\n                    if sub in self.badmodules:\\n                        self._add_badmodule(sub, caller)\\n                        continue\\n                    try:\\n                        self.import_hook(name, caller, [sub])\\n                    except ImportError, msg:\\n                        self.msg(2, \"ImportError:\", str(msg))\\n                        fullname = name + \".\" + sub\\n                        self._add_badmodule(fullname, caller)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Modulefinder now handles absolute and relative imports, including tests.\\n\\nWill backport to release25-maint.",
    "fixed_code": "def _safe_import_hook(self, name, caller, fromlist, level=-1):\\n        if name in self.badmodules:\\n            self._add_badmodule(name, caller)\\n            return\\n        try:\\n            self.import_hook(name, caller, level=level)\\n        except ImportError, msg:\\n            self.msg(2, \"ImportError:\", str(msg))\\n            self._add_badmodule(name, caller)\\n        else:\\n            if fromlist:\\n                for sub in fromlist:\\n                    if sub in self.badmodules:\\n                        self._add_badmodule(sub, caller)\\n                        continue\\n                    try:\\n                        self.import_hook(name, caller, [sub], level=level)\\n                    except ImportError, msg:\\n                        self.msg(2, \"ImportError:\", str(msg))\\n                        fullname = name + \".\" + sub\\n                        self._add_badmodule(fullname, caller)"
  },
  {
    "code": "def __init__(\\n        self,\\n        job_name,\\n        job_definition,\\n        job_queue,\\n        overrides,\\n        array_properties=None,\\n        parameters=None,\\n        max_retries=MAX_RETRIES,\\n        status_retries=STATUS_RETRIES,\\n        aws_conn_id=None,\\n        region_name=None,\\n        **kwargs,\\n    ):\\n        super().__init__(**kwargs)\\n        self.job_name = job_name\\n        self.aws_conn_id = aws_conn_id\\n        self.region_name = region_name\\n        self.job_definition = job_definition\\n        self.job_queue = job_queue\\n        self.overrides = overrides\\n        self.array_properties = array_properties or {}\\n        self.parameters = parameters\\n        self.max_retries = max_retries\\n        self.status_retries = status_retries\\n        self.jobId = None  \\n        self.jobName = None  \\n        self.hook = self.get_hook()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(\\n        self,\\n        job_name,\\n        job_definition,\\n        job_queue,\\n        overrides,\\n        array_properties=None,\\n        parameters=None,\\n        max_retries=MAX_RETRIES,\\n        status_retries=STATUS_RETRIES,\\n        aws_conn_id=None,\\n        region_name=None,\\n        **kwargs,\\n    ):\\n        super().__init__(**kwargs)\\n        self.job_name = job_name\\n        self.aws_conn_id = aws_conn_id\\n        self.region_name = region_name\\n        self.job_definition = job_definition\\n        self.job_queue = job_queue\\n        self.overrides = overrides\\n        self.array_properties = array_properties or {}\\n        self.parameters = parameters\\n        self.max_retries = max_retries\\n        self.status_retries = status_retries\\n        self.jobId = None  \\n        self.jobName = None  \\n        self.hook = self.get_hook()"
  },
  {
    "code": "def get_finder(freq):\\n    if isinstance(freq, basestring):\\n        freq = frequencies.get_freq(freq)\\n    fgroup = frequencies.get_freq_group(freq)\\n    if fgroup == FreqGroup.FR_ANN:\\n        return _annual_finder\\n    elif fgroup == FreqGroup.FR_QTR:\\n        return _quarterly_finder\\n    elif freq ==FreqGroup.FR_MTH:\\n        return _monthly_finder\\n    elif ((freq >= FreqGroup.FR_BUS) or fgroup == FreqGroup.FR_WK):\\n        return _daily_finder\\n    else: \\n        errmsg = \"Unsupported frequency: %s\" % (freq)\\n        raise NotImplementedError(errmsg)\\nclass TimeSeries_DateLocator(Locator):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_finder(freq):\\n    if isinstance(freq, basestring):\\n        freq = frequencies.get_freq(freq)\\n    fgroup = frequencies.get_freq_group(freq)\\n    if fgroup == FreqGroup.FR_ANN:\\n        return _annual_finder\\n    elif fgroup == FreqGroup.FR_QTR:\\n        return _quarterly_finder\\n    elif freq ==FreqGroup.FR_MTH:\\n        return _monthly_finder\\n    elif ((freq >= FreqGroup.FR_BUS) or fgroup == FreqGroup.FR_WK):\\n        return _daily_finder\\n    else: \\n        errmsg = \"Unsupported frequency: %s\" % (freq)\\n        raise NotImplementedError(errmsg)\\nclass TimeSeries_DateLocator(Locator):"
  },
  {
    "code": "def __init__(self, image=None, image_pull_policy=None, request_memory=None,\\n                 request_cpu=None, limit_memory=None, limit_cpu=None,\\n                 gcp_service_account_key=None):\\n        self.image = image\\n        self.image_pull_policy = image_pull_policy\\n        self.request_memory = request_memory\\n        self.request_cpu = request_cpu\\n        self.limit_memory = limit_memory\\n        self.limit_cpu = limit_cpu\\n        self.gcp_service_account_key = gcp_service_account_key",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2662][AIRFLOW-2397] Add k8s node_selectors and affinity\\n\\nAdd the ability to set the node selection and the affinity\\nfor the k8s executor\\n\\nCloses #3535 from Cplo/affinity",
    "fixed_code": "def __init__(self, image=None, image_pull_policy=None, request_memory=None,\\n                 request_cpu=None, limit_memory=None, limit_cpu=None,\\n                 gcp_service_account_key=None, node_selectors=None, affinity=None):\\n        self.image = image\\n        self.image_pull_policy = image_pull_policy\\n        self.request_memory = request_memory\\n        self.request_cpu = request_cpu\\n        self.limit_memory = limit_memory\\n        self.limit_cpu = limit_cpu\\n        self.gcp_service_account_key = gcp_service_account_key\\n        self.node_selectors = node_selectors\\n        self.affinity = affinity"
  },
  {
    "code": "def min(self, axis=0):\\n        values = self.values.copy()\\n        np.putmask(values, -np.isfinite(values), np.inf)\\n        return Series(values.min(axis), index=self._get_agg_axis(axis))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: skipna in DataFrame stat functions",
    "fixed_code": "def min(self, axis=0, skipna=True):\\n        values = self.values.copy()\\n        if skipna:\\n            np.putmask(values, -np.isfinite(values), np.inf)\\n        return Series(values.min(axis), index=self._get_agg_axis(axis))"
  },
  {
    "code": "def normpath(path):\\n    path = path.replace(\"/\", \"\\\\\")\\n    prefix, path = splitdrive(path)\\n    while path[:1] == \"\\\\\":\\n        prefix = prefix + \"\\\\\"\\n        path = path[1:]\\n    comps = path.split(\"\\\\\")\\n    i = 0\\n    while i < len(comps):\\n        if comps[i] == '.':\\n            del comps[i]\\n        elif comps[i] == '..' and i > 0 and comps[i-1] not in ('', '..'):\\n            del comps[i-1:i+1]\\n            i = i - 1\\n        elif comps[i] == '' and i > 0 and comps[i-1] != '':\\n            del comps[i]\\n        else:\\n            i = i + 1\\n    if not prefix and not comps:\\n        comps.append('.')\\n    return prefix + \"\\\\\".join(comps)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "SF bug #456621:  normpath on Win32 not collapsing c:\\\\.. I actually rewrote normpath quite a bit:  it had no test cases, and as soon as I starting writing some I found several cases that didn't make sense.",
    "fixed_code": "def normpath(path):\\n    path = path.replace(\"/\", \"\\\\\")\\n    prefix, path = splitdrive(path)\\n    while path[:1] == \"\\\\\":\\n        prefix = prefix + \"\\\\\"\\n        path = path[1:]\\n    comps = path.split(\"\\\\\")\\n    i = 0\\n    while i < len(comps):\\n        if comps[i] in ('.', ''):\\n            del comps[i]\\n        elif comps[i] == '..':\\n            if i > 0 and comps[i-1] != '..':\\n                del comps[i-1:i+1]\\n                i -= 1\\n            elif i == 0 and prefix.endswith(\"\\\\\"):\\n                del comps[i]\\n            else:\\n                i += 1\\n        else:\\n            i += 1\\n    if not prefix and not comps:\\n        comps.append('.')\\n    return prefix + \"\\\\\".join(comps)"
  },
  {
    "code": "def _transpose(self):\\n        return _Xt_operator(self.X, self.X_mean, self.sqrt_sw)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[MRG] RidgeCV minor refactor to improve readability (#13832)\\n\\nThank you very much for this work @thomasjpfan and @jeromedockes.",
    "fixed_code": "def _transpose(self):\\n        return _XT_CenterStackOp(self.X, self.X_mean, self.sqrt_sw)"
  },
  {
    "code": "def ndarray_to_mgr(\\n    values, index, columns, dtype: DtypeObj | None, copy: bool, typ: str\\n) -> Manager:\\n    if isinstance(values, ABCSeries):\\n        if columns is None:\\n            if values.name is not None:\\n                columns = Index([values.name])\\n        if index is None:\\n            index = values.index\\n        else:\\n            values = values.reindex(index)\\n        if not len(values) and columns is not None and len(columns):\\n            values = np.empty((0, 1), dtype=object)\\n    vdtype = getattr(values, \"dtype\", None)\\n    if is_1d_only_ea_dtype(vdtype) or isinstance(dtype, ExtensionDtype):\\n        if isinstance(values, np.ndarray) and values.ndim > 1:\\n            values = [values[:, n] for n in range(values.shape[1])]\\n        else:\\n            values = [values]\\n        if columns is None:\\n            columns = Index(range(len(values)))\\n        else:\\n            columns = ensure_index(columns)\\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype, typ=typ)\\n    elif is_extension_array_dtype(vdtype) and not is_1d_only_ea_dtype(vdtype):\\n        values = extract_array(values, extract_numpy=True)\\n        if copy:\\n            values = values.copy()\\n        if values.ndim == 1:\\n            values = values.reshape(-1, 1)\\n    else:\\n        values = _prep_ndarray(values, copy=copy)\\n    if dtype is not None and not is_dtype_equal(values.dtype, dtype):\\n        shape = values.shape\\n        flat = values.ravel()\\n        rcf = not (is_integer_dtype(dtype) and values.dtype.kind == \"f\")\\n        values = sanitize_array(\\n            flat, None, dtype=dtype, copy=copy, raise_cast_failure=rcf\\n        )\\n        values = values.reshape(shape)\\n    index, columns = _get_axes(\\n        values.shape[0], values.shape[1], index=index, columns=columns\\n    )\\n    _check_values_indices_shape_match(values, index, columns)\\n    if typ == \"array\":\\n        if issubclass(values.dtype.type, str):\\n            values = np.array(values, dtype=object)\\n        if dtype is None and is_object_dtype(values.dtype):\\n            arrays = [\\n                ensure_wrapped_if_datetimelike(\\n                    maybe_infer_to_datetimelike(values[:, i].copy())\\n                )\\n                for i in range(values.shape[1])\\n            ]\\n        else:\\n            if is_datetime_or_timedelta_dtype(values.dtype):\\n                values = ensure_wrapped_if_datetimelike(values)\\n            arrays = [values[:, i].copy() for i in range(values.shape[1])]\\n        return ArrayManager(arrays, [index, columns], verify_integrity=False)\\n    values = values.T\\n    if dtype is None and is_object_dtype(values.dtype):\\n        if values.ndim == 2 and values.shape[0] != 1:\\n            dtlike_vals = [maybe_infer_to_datetimelike(row) for row in values]\\n            dvals_list = [ensure_block_shape(dval, 2) for dval in dtlike_vals]\\n            block_values = [\\n                new_block(dvals_list[n], placement=n, ndim=2)\\n                for n in range(len(dvals_list))\\n            ]\\n        else:\\n            datelike_vals = maybe_infer_to_datetimelike(values)\\n            nb = new_block(datelike_vals, placement=slice(len(columns)), ndim=2)\\n            block_values = [nb]\\n    else:\\n        nb = new_block(values, placement=slice(len(columns)), ndim=2)\\n        block_values = [nb]\\n    if len(columns) == 0:\\n        block_values = []\\n    return create_block_manager_from_blocks(block_values, [columns, index])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: 2D ndarray of dtype 'object' is always copied upon construction (#39272)",
    "fixed_code": "def ndarray_to_mgr(\\n    values, index, columns, dtype: DtypeObj | None, copy: bool, typ: str\\n) -> Manager:\\n    if isinstance(values, ABCSeries):\\n        if columns is None:\\n            if values.name is not None:\\n                columns = Index([values.name])\\n        if index is None:\\n            index = values.index\\n        else:\\n            values = values.reindex(index)\\n        if not len(values) and columns is not None and len(columns):\\n            values = np.empty((0, 1), dtype=object)\\n    vdtype = getattr(values, \"dtype\", None)\\n    if is_1d_only_ea_dtype(vdtype) or isinstance(dtype, ExtensionDtype):\\n        if isinstance(values, np.ndarray) and values.ndim > 1:\\n            values = [values[:, n] for n in range(values.shape[1])]\\n        else:\\n            values = [values]\\n        if columns is None:\\n            columns = Index(range(len(values)))\\n        else:\\n            columns = ensure_index(columns)\\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype, typ=typ)\\n    elif is_extension_array_dtype(vdtype) and not is_1d_only_ea_dtype(vdtype):\\n        values = extract_array(values, extract_numpy=True)\\n        if copy:\\n            values = values.copy()\\n        if values.ndim == 1:\\n            values = values.reshape(-1, 1)\\n    else:\\n        values = _prep_ndarray(values, copy=copy)\\n    if dtype is not None and not is_dtype_equal(values.dtype, dtype):\\n        shape = values.shape\\n        flat = values.ravel()\\n        rcf = not (is_integer_dtype(dtype) and values.dtype.kind == \"f\")\\n        values = sanitize_array(\\n            flat, None, dtype=dtype, copy=copy, raise_cast_failure=rcf\\n        )\\n        values = values.reshape(shape)\\n    index, columns = _get_axes(\\n        values.shape[0], values.shape[1], index=index, columns=columns\\n    )\\n    _check_values_indices_shape_match(values, index, columns)\\n    if typ == \"array\":\\n        if issubclass(values.dtype.type, str):\\n            values = np.array(values, dtype=object)\\n        if dtype is None and is_object_dtype(values.dtype):\\n            arrays = [\\n                ensure_wrapped_if_datetimelike(\\n                    maybe_infer_to_datetimelike(values[:, i].copy())\\n                )\\n                for i in range(values.shape[1])\\n            ]\\n        else:\\n            if is_datetime_or_timedelta_dtype(values.dtype):\\n                values = ensure_wrapped_if_datetimelike(values)\\n            arrays = [values[:, i].copy() for i in range(values.shape[1])]\\n        return ArrayManager(arrays, [index, columns], verify_integrity=False)\\n    values = values.T\\n    if dtype is None and is_object_dtype(values.dtype):\\n        obj_columns = list(values)\\n        maybe_datetime = [maybe_infer_to_datetimelike(x) for x in obj_columns]\\n        if any(x is not y for x, y in zip(obj_columns, maybe_datetime)):\\n            dvals_list = [ensure_block_shape(dval, 2) for dval in maybe_datetime]\\n            block_values = [\\n                new_block(dvals_list[n], placement=n, ndim=2)\\n                for n in range(len(dvals_list))\\n            ]\\n        else:\\n            nb = new_block(values, placement=slice(len(columns)), ndim=2)\\n            block_values = [nb]\\n    else:\\n        nb = new_block(values, placement=slice(len(columns)), ndim=2)\\n        block_values = [nb]\\n    if len(columns) == 0:\\n        block_values = []\\n    return create_block_manager_from_blocks(block_values, [columns, index])"
  },
  {
    "code": "def construct_ansible_facts(response, params):\\n    facts = dict()\\n    if response:\\n        response_body = response['items'] if 'items' in response else response\\n        if params.get('register_as'):\\n            facts[params['register_as']] = response_body\\n        elif type(response_body) is dict and response_body.get('name') and response_body.get('type'):\\n            object_name = re.sub(INVALID_IDENTIFIER_SYMBOLS, '_', response_body['name'].lower())\\n            fact_name = '%s_%s' % (response_body['type'], object_name)\\n            facts[fact_name] = response_body\\n    return facts",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def construct_ansible_facts(response, params):\\n    facts = dict()\\n    if response:\\n        response_body = response['items'] if 'items' in response else response\\n        if params.get('register_as'):\\n            facts[params['register_as']] = response_body\\n        elif type(response_body) is dict and response_body.get('name') and response_body.get('type'):\\n            object_name = re.sub(INVALID_IDENTIFIER_SYMBOLS, '_', response_body['name'].lower())\\n            fact_name = '%s_%s' % (response_body['type'], object_name)\\n            facts[fact_name] = response_body\\n    return facts"
  },
  {
    "code": "def toprettyxml(self, indent=\"\\t\", newl=\"\\n\", encoding=None):\\n        if encoding is None:\\n            writer = io.StringIO()\\n        else:\\n            writer = io.TextIOWrapper(io.BytesIO(),\\n                                      encoding=encoding,\\n                                      errors=\"xmlcharrefreplace\",\\n                                      newline='\\n')\\n        if self.nodeType == Node.DOCUMENT_NODE:\\n            self.writexml(writer, \"\", indent, newl, encoding)\\n        else:\\n            self.writexml(writer, \"\", indent, newl)\\n        if encoding is None:\\n            return writer.getvalue()\\n        else:\\n            return writer.detach().getvalue()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def toprettyxml(self, indent=\"\\t\", newl=\"\\n\", encoding=None):\\n        if encoding is None:\\n            writer = io.StringIO()\\n        else:\\n            writer = io.TextIOWrapper(io.BytesIO(),\\n                                      encoding=encoding,\\n                                      errors=\"xmlcharrefreplace\",\\n                                      newline='\\n')\\n        if self.nodeType == Node.DOCUMENT_NODE:\\n            self.writexml(writer, \"\", indent, newl, encoding)\\n        else:\\n            self.writexml(writer, \"\", indent, newl)\\n        if encoding is None:\\n            return writer.getvalue()\\n        else:\\n            return writer.detach().getvalue()"
  },
  {
    "code": "def process_view(self, request, callback, callback_args, callback_kwargs):\\n        if request.method == 'POST':\\n            try:\\n                session_id = request.COOKIES[settings.SESSION_COOKIE_NAME]\\n            except KeyError:\\n                return None\\n            csrf_token = _make_token(session_id)\\n            try:\\n                request_csrf_token = request.POST['csrfmiddlewaretoken']\\n            except KeyError:\\n                return HttpResponseForbidden(_ERROR_MSG)\\n            if request_csrf_token != csrf_token:\\n                return HttpResponseForbidden(_ERROR_MSG)\\n        return None\\nclass CsrfResponseMiddleware(object):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "New CsrfMiddleware features: automatic exceptions for known AJAX and decorator for manual exceptions",
    "fixed_code": "def process_view(self, request, callback, callback_args, callback_kwargs):\\n        if request.method == 'POST':\\n            if getattr(callback, 'csrf_exempt', False):\\n                return None\\n            if request.is_ajax():\\n                return None\\n            try:\\n                session_id = request.COOKIES[settings.SESSION_COOKIE_NAME]\\n            except KeyError:\\n                return None\\n            csrf_token = _make_token(session_id)\\n            try:\\n                request_csrf_token = request.POST['csrfmiddlewaretoken']\\n            except KeyError:\\n                return HttpResponseForbidden(_ERROR_MSG)\\n            if request_csrf_token != csrf_token:\\n                return HttpResponseForbidden(_ERROR_MSG)\\n        return None\\nclass CsrfResponseMiddleware(object):"
  },
  {
    "code": "def intersection(self, other, sort=False):\\n        self._validate_sort_keyword(sort)\\n        self._assert_can_do_setop(other)\\n        other, _ = self._convert_can_do_setop(other)\\n        if self.equals(other):\\n            return self._get_reconciled_name_object(other)\\n        return self._intersection(other, sort=sort)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def intersection(self, other, sort=False):\\n        self._validate_sort_keyword(sort)\\n        self._assert_can_do_setop(other)\\n        other, _ = self._convert_can_do_setop(other)\\n        if self.equals(other):\\n            return self._get_reconciled_name_object(other)\\n        return self._intersection(other, sort=sort)"
  },
  {
    "code": "def _generate_cache_key(request, method, headerlist, key_prefix):\\n    ctx = hashlib.md5()\\n    for header in headerlist:\\n        value = request.META.get(header, None)\\n        if value is not None:\\n            ctx.update(force_bytes(value))\\n    url = hashlib.md5(force_bytes(iri_to_uri(request.build_absolute_uri())))\\n    cache_key = 'views.decorators.cache.cache_page.%s.%s.%s.%s' % (\\n        key_prefix, method, url.hexdigest(), ctx.hexdigest())\\n    return _i18n_cache_key_suffix(request, cache_key)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _generate_cache_key(request, method, headerlist, key_prefix):\\n    ctx = hashlib.md5()\\n    for header in headerlist:\\n        value = request.META.get(header, None)\\n        if value is not None:\\n            ctx.update(force_bytes(value))\\n    url = hashlib.md5(force_bytes(iri_to_uri(request.build_absolute_uri())))\\n    cache_key = 'views.decorators.cache.cache_page.%s.%s.%s.%s' % (\\n        key_prefix, method, url.hexdigest(), ctx.hexdigest())\\n    return _i18n_cache_key_suffix(request, cache_key)"
  },
  {
    "code": "def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\\n\\t\\t   force_grouping=False):\\n\\tuse_grouping = settings.USE_L10N and settings.USE_THOUSAND_SEPARATOR\\n\\tuse_grouping = use_grouping or force_grouping\\n\\tuse_grouping = use_grouping and grouping != 0\\n\\tif isinstance(number, int) and not use_grouping and not decimal_pos:\\n\\t\\treturn mark_safe(six.text_type(number))\\n\\tsign = ''\\n\\tif isinstance(number, Decimal):\\n\\t\\t_, digits, exponent = number.as_tuple()\\n\\t\\tif abs(exponent) + len(digits) > 200:\\n\\t\\t\\tnumber = '{:e}'.format(number)\\n\\t\\t\\tcoefficient, exponent = number.split('e')\\n\\t\\t\\tcoefficient = format(\\n\\t\\t\\t\\tcoefficient, decimal_sep, decimal_pos, grouping,\\n\\t\\t\\t\\tthousand_sep, force_grouping,\\n\\t\\t\\t)\\n\\t\\t\\treturn '{}e{}'.format(coefficient, exponent)\\n\\t\\telse:\\n\\t\\t\\tstr_number = '{:f}'.format(number)\\n\\telse:\\n\\t\\tstr_number = six.text_type(number)\\n\\tif str_number[0] == '-':\\n\\t\\tsign = '-'\\n\\t\\tstr_number = str_number[1:]\\n\\tif '.' in str_number:\\n\\t\\tint_part, dec_part = str_number.split('.')\\n\\t\\tif decimal_pos is not None:\\n\\t\\t\\tdec_part = dec_part[:decimal_pos]\\n\\telse:\\n\\t\\tint_part, dec_part = str_number, ''\\n\\tif decimal_pos is not None:\\n\\t\\tdec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\\n\\tif dec_part:\\n\\t\\tdec_part = decimal_sep + dec_part\\n\\tif use_grouping:\\n\\t\\ttry:\\n\\t\\t\\tintervals = list(grouping)\\n\\t\\texcept TypeError:\\n\\t\\t\\tintervals = [grouping, 0]\\n\\t\\tactive_interval = intervals.pop(0)\\n\\t\\tint_part_gd = ''\\n\\t\\tcnt = 0\\n\\t\\tfor digit in int_part[::-1]:\\n\\t\\t\\tif cnt and cnt == active_interval:\\n\\t\\t\\t\\tif intervals:\\n\\t\\t\\t\\t\\tactive_interval = intervals.pop(0) or active_interval\\n\\t\\t\\t\\tint_part_gd += thousand_sep[::-1]\\n\\t\\t\\t\\tcnt = 0\\n\\t\\t\\tint_part_gd += digit\\n\\t\\t\\tcnt += 1\\n\\t\\tint_part = int_part_gd[::-1]\\n\\treturn sign + int_part + dec_part",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\\n\\t\\t   force_grouping=False):\\n\\tuse_grouping = settings.USE_L10N and settings.USE_THOUSAND_SEPARATOR\\n\\tuse_grouping = use_grouping or force_grouping\\n\\tuse_grouping = use_grouping and grouping != 0\\n\\tif isinstance(number, int) and not use_grouping and not decimal_pos:\\n\\t\\treturn mark_safe(six.text_type(number))\\n\\tsign = ''\\n\\tif isinstance(number, Decimal):\\n\\t\\t_, digits, exponent = number.as_tuple()\\n\\t\\tif abs(exponent) + len(digits) > 200:\\n\\t\\t\\tnumber = '{:e}'.format(number)\\n\\t\\t\\tcoefficient, exponent = number.split('e')\\n\\t\\t\\tcoefficient = format(\\n\\t\\t\\t\\tcoefficient, decimal_sep, decimal_pos, grouping,\\n\\t\\t\\t\\tthousand_sep, force_grouping,\\n\\t\\t\\t)\\n\\t\\t\\treturn '{}e{}'.format(coefficient, exponent)\\n\\t\\telse:\\n\\t\\t\\tstr_number = '{:f}'.format(number)\\n\\telse:\\n\\t\\tstr_number = six.text_type(number)\\n\\tif str_number[0] == '-':\\n\\t\\tsign = '-'\\n\\t\\tstr_number = str_number[1:]\\n\\tif '.' in str_number:\\n\\t\\tint_part, dec_part = str_number.split('.')\\n\\t\\tif decimal_pos is not None:\\n\\t\\t\\tdec_part = dec_part[:decimal_pos]\\n\\telse:\\n\\t\\tint_part, dec_part = str_number, ''\\n\\tif decimal_pos is not None:\\n\\t\\tdec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\\n\\tif dec_part:\\n\\t\\tdec_part = decimal_sep + dec_part\\n\\tif use_grouping:\\n\\t\\ttry:\\n\\t\\t\\tintervals = list(grouping)\\n\\t\\texcept TypeError:\\n\\t\\t\\tintervals = [grouping, 0]\\n\\t\\tactive_interval = intervals.pop(0)\\n\\t\\tint_part_gd = ''\\n\\t\\tcnt = 0\\n\\t\\tfor digit in int_part[::-1]:\\n\\t\\t\\tif cnt and cnt == active_interval:\\n\\t\\t\\t\\tif intervals:\\n\\t\\t\\t\\t\\tactive_interval = intervals.pop(0) or active_interval\\n\\t\\t\\t\\tint_part_gd += thousand_sep[::-1]\\n\\t\\t\\t\\tcnt = 0\\n\\t\\t\\tint_part_gd += digit\\n\\t\\t\\tcnt += 1\\n\\t\\tint_part = int_part_gd[::-1]\\n\\treturn sign + int_part + dec_part"
  },
  {
    "code": "def sort(self, *args, **kwargs):\\n        raise TypeError('Cannot sort an %r object' % self.__class__.__name__)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sort(self, *args, **kwargs):\\n        raise TypeError('Cannot sort an %r object' % self.__class__.__name__)"
  },
  {
    "code": "def add_prefix(self, prefix):\\n        f = partial('{prefix}{}'.format, prefix=prefix)\\n        return self.rename_axis(f, axis=0)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def add_prefix(self, prefix):\\n        f = partial('{prefix}{}'.format, prefix=prefix)\\n        return self.rename_axis(f, axis=0)"
  },
  {
    "code": "def load_entrypoint_plugins():\\n    global import_errors  \\n    global plugins  \\n    log.debug(\"Loading plugins from entrypoints\")\\n    for entry_point, dist in entry_points_with_dist('airflow.plugins'):\\n        log.debug('Importing entry_point plugin %s', entry_point.name)\\n        try:\\n            plugin_class = entry_point.load()\\n            if not is_valid_plugin(plugin_class):\\n                continue\\n            plugin_instance = plugin_class()\\n            if callable(getattr(plugin_instance, 'on_load', None)):\\n                plugin_instance.on_load()\\n                plugin_instance.source = EntryPointSource(entry_point, dist)\\n                plugins.append(plugin_instance)\\n        except Exception as e:  \\n            log.exception(\"Failed to import plugin %s\", entry_point.name)\\n            import_errors[entry_point.module] = str(e)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_entrypoint_plugins():\\n    global import_errors  \\n    global plugins  \\n    log.debug(\"Loading plugins from entrypoints\")\\n    for entry_point, dist in entry_points_with_dist('airflow.plugins'):\\n        log.debug('Importing entry_point plugin %s', entry_point.name)\\n        try:\\n            plugin_class = entry_point.load()\\n            if not is_valid_plugin(plugin_class):\\n                continue\\n            plugin_instance = plugin_class()\\n            if callable(getattr(plugin_instance, 'on_load', None)):\\n                plugin_instance.on_load()\\n                plugin_instance.source = EntryPointSource(entry_point, dist)\\n                plugins.append(plugin_instance)\\n        except Exception as e:  \\n            log.exception(\"Failed to import plugin %s\", entry_point.name)\\n            import_errors[entry_point.module] = str(e)"
  },
  {
    "code": "def get_xenserver_version(module):\\n    xapi_session = XAPI.connect(module)\\n    host_ref = xapi_session.xenapi.session.get_this_host(xapi_session._session)\\n    return xapi_session.xenapi.host.get_software_version(host_ref)['product_version_text_short'].split('.')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix a bug that caused exception on XenServer 7.1 with Cummulative Update (#52303)\\n\\n- xenserver module_util: fixed a bug in gather_vm_params function where\\n   an exception was generated if XenServer product_version_text_short\\n   parameter contained non numeric characters, e.g. \"7.1 CU1\" on\\n   XenServer version 7.1 with Cummulative Update 1. Code was changed\\n   to use product_version parameter instead which is all numeric.\\n - xenserver module_util: get_xenserver_version function is changed\\n   to return a list of integers for major, minor and update version\\n   instead of list of strings.\\n - xenserver module_util: unit tests are updated according to changes.\\n - xenserver module_util: removed unused imports.",
    "fixed_code": "def get_xenserver_version(module):\\n    xapi_session = XAPI.connect(module)\\n    host_ref = xapi_session.xenapi.session.get_this_host(xapi_session._session)\\n    try:\\n        xenserver_version = [int(version_number) for version_number in xapi_session.xenapi.host.get_software_version(host_ref)['product_version'].split('.')]\\n    except ValueError:\\n        xenserver_version = [0, 0, 0]\\n    return xenserver_version"
  },
  {
    "code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.panel import _make_long_index, LongPanel\\n        index_vals = self[index]\\n        column_vals = self[columns]\\n        long_index = _make_long_index(index_vals, column_vals)\\n        if values is None:\\n            items = self.columns - [index, columns]\\n            mat = self.reindex(columns=items).values\\n        else:\\n            items = [values]\\n            mat = np.atleast_2d(self[values].values).T\\n        lp = LongPanel(mat, items, long_index)\\n        lp = lp.sort()\\n        wp = lp.to_wide()\\n        if values is not None:\\n            return wp[values]\\n        else:\\n            return wp",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pivot(self, index=None, columns=None, values=None):\\n        from pandas.core.panel import _make_long_index, LongPanel\\n        index_vals = self[index]\\n        column_vals = self[columns]\\n        long_index = _make_long_index(index_vals, column_vals)\\n        if values is None:\\n            items = self.columns - [index, columns]\\n            mat = self.reindex(columns=items).values\\n        else:\\n            items = [values]\\n            mat = np.atleast_2d(self[values].values).T\\n        lp = LongPanel(mat, items, long_index)\\n        lp = lp.sort()\\n        wp = lp.to_wide()\\n        if values is not None:\\n            return wp[values]\\n        else:\\n            return wp"
  },
  {
    "code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None, mtime=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.name = filename\\n            self.min_readsize = 100\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        self.mtime = mtime\\n        if self.mode == WRITE:\\n            self._write_gzip_header()\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, filename=None, mode=None,\\n                 compresslevel=9, fileobj=None, mtime=None):\\n        if mode and 'b' not in mode:\\n            mode += 'b'\\n        if fileobj is None:\\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\\n        if filename is None:\\n            if hasattr(fileobj, 'name'): filename = fileobj.name\\n            else: filename = ''\\n        if mode is None:\\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\\n            else: mode = 'rb'\\n        if mode[0:1] == 'r':\\n            self.mode = READ\\n            self._new_member = True\\n            self.extrabuf = \"\"\\n            self.extrasize = 0\\n            self.name = filename\\n            self.min_readsize = 100\\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\\n            self.mode = WRITE\\n            self._init_write(filename)\\n            self.compress = zlib.compressobj(compresslevel,\\n                                             zlib.DEFLATED,\\n                                             -zlib.MAX_WBITS,\\n                                             zlib.DEF_MEM_LEVEL,\\n                                             0)\\n        else:\\n            raise IOError, \"Mode \" + mode + \" not supported\"\\n        self.fileobj = fileobj\\n        self.offset = 0\\n        self.mtime = mtime\\n        if self.mode == WRITE:\\n            self._write_gzip_header()\\n    @property"
  },
  {
    "code": "def _fit(self, X):\\n        if issparse(X):\\n            raise TypeError(\\n                \"PCA does not support sparse input. See \"\\n                \"TruncatedSVD for a possible alternative.\"\\n            )\\n        X = self._validate_data(\\n            X, dtype=[np.float64, np.float32], ensure_2d=True, copy=self.copy\\n        )\\n        if self.n_components is None:\\n            if self.svd_solver != \"arpack\":\\n                n_components = min(X.shape)\\n            else:\\n                n_components = min(X.shape) - 1\\n        else:\\n            n_components = self.n_components\\n        self._fit_svd_solver = self.svd_solver\\n        if self._fit_svd_solver == \"auto\":\\n            if max(X.shape) <= 500 or n_components == \"mle\":\\n                self._fit_svd_solver = \"full\"\\n            elif 1 <= n_components < 0.8 * min(X.shape):\\n                self._fit_svd_solver = \"randomized\"\\n            else:\\n                self._fit_svd_solver = \"full\"\\n        if self._fit_svd_solver == \"full\":\\n            return self._fit_full(X, n_components)\\n        elif self._fit_svd_solver in [\"arpack\", \"randomized\"]:\\n            return self._fit_truncated(X, n_components, self._fit_svd_solver)\\n        else:\\n            raise ValueError(\\n                \"Unrecognized svd_solver='{0}'\".format(self._fit_svd_solver)\\n            )\\n    def _fit_full(self, X, n_components):\\n        n_samples, n_features = X.shape\\n        if n_components == \"mle\":\\n            if n_samples < n_features:\\n                raise ValueError(\\n                    \"n_components='mle' is only supported if n_samples >= n_features\"\\n                )\\n        elif not 0 <= n_components <= min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be between 0 and \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='full'\" % (n_components, min(n_samples, n_features))\\n            )\\n        elif n_components >= 1:\\n            if not isinstance(n_components, numbers.Integral):\\n                raise ValueError(\\n                    \"n_components=%r must be of type int \"\\n                    \"when greater than or equal to 1, \"\\n                    \"was of type=%r\" % (n_components, type(n_components))\\n                )\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        U, S, Vt = linalg.svd(X, full_matrices=False)\\n        U, Vt = svd_flip(U, Vt)\\n        components_ = Vt\\n        explained_variance_ = (S**2) / (n_samples - 1)\\n        total_var = explained_variance_.sum()\\n        explained_variance_ratio_ = explained_variance_ / total_var\\n        singular_values_ = S.copy()  \\n        if n_components == \"mle\":\\n            n_components = _infer_dimension(explained_variance_, n_samples)\\n        elif 0 < n_components < 1.0:\\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\\n            n_components = np.searchsorted(ratio_cumsum, n_components, side=\"right\") + 1\\n        if n_components < min(n_features, n_samples):\\n            self.noise_variance_ = explained_variance_[n_components:].mean()\\n        else:\\n            self.noise_variance_ = 0.0\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = components_[:n_components]\\n        self.n_components_ = n_components\\n        self.explained_variance_ = explained_variance_[:n_components]\\n        self.explained_variance_ratio_ = explained_variance_ratio_[:n_components]\\n        self.singular_values_ = singular_values_[:n_components]\\n        return U, S, Vt\\n    def _fit_truncated(self, X, n_components, svd_solver):\\n        n_samples, n_features = X.shape\\n        if isinstance(n_components, str):\\n            raise ValueError(\\n                \"n_components=%r cannot be a string with svd_solver='%s'\"\\n                % (n_components, svd_solver)\\n            )\\n        elif not 1 <= n_components <= min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be between 1 and \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='%s'\"\\n                % (n_components, min(n_samples, n_features), svd_solver)\\n            )\\n        elif not isinstance(n_components, numbers.Integral):\\n            raise ValueError(\\n                \"n_components=%r must be of type int \"\\n                \"when greater than or equal to 1, was of type=%r\"\\n                % (n_components, type(n_components))\\n            )\\n        elif svd_solver == \"arpack\" and n_components == min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be strictly less than \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='%s'\"\\n                % (n_components, min(n_samples, n_features), svd_solver)\\n            )\\n        random_state = check_random_state(self.random_state)\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        if svd_solver == \"arpack\":\\n            v0 = _init_arpack_v0(min(X.shape), random_state)\\n            U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0)\\n            S = S[::-1]\\n            U, Vt = svd_flip(U[:, ::-1], Vt[::-1])\\n        elif svd_solver == \"randomized\":\\n            U, S, Vt = randomized_svd(\\n                X,\\n                n_components=n_components,\\n                n_oversamples=self.n_oversamples,\\n                n_iter=self.iterated_power,\\n                power_iteration_normalizer=self.power_iteration_normalizer,\\n                flip_sign=True,\\n                random_state=random_state,\\n            )\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = Vt\\n        self.n_components_ = n_components\\n        self.explained_variance_ = (S**2) / (n_samples - 1)\\n        N = X.shape[0] - 1\\n        np.square(X, out=X)\\n        np.sum(X, axis=0, out=X[0])\\n        total_var = (X[0] / N).sum()\\n        self.explained_variance_ratio_ = self.explained_variance_ / total_var\\n        self.singular_values_ = S.copy()  \\n        if self.n_components_ < min(n_features, n_samples):\\n            self.noise_variance_ = total_var - self.explained_variance_.sum()\\n            self.noise_variance_ /= min(n_features, n_samples) - n_components\\n        else:\\n            self.noise_variance_ = 0.0\\n        return U, S, Vt",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT Use _validate_params in PCA  (#23485)",
    "fixed_code": "def _fit(self, X):\\n        if issparse(X):\\n            raise TypeError(\\n                \"PCA does not support sparse input. See \"\\n                \"TruncatedSVD for a possible alternative.\"\\n            )\\n        X = self._validate_data(\\n            X, dtype=[np.float64, np.float32], ensure_2d=True, copy=self.copy\\n        )\\n        if self.n_components is None:\\n            if self.svd_solver != \"arpack\":\\n                n_components = min(X.shape)\\n            else:\\n                n_components = min(X.shape) - 1\\n        else:\\n            n_components = self.n_components\\n        self._fit_svd_solver = self.svd_solver\\n        if self._fit_svd_solver == \"auto\":\\n            if max(X.shape) <= 500 or n_components == \"mle\":\\n                self._fit_svd_solver = \"full\"\\n            elif 1 <= n_components < 0.8 * min(X.shape):\\n                self._fit_svd_solver = \"randomized\"\\n            else:\\n                self._fit_svd_solver = \"full\"\\n        if self._fit_svd_solver == \"full\":\\n            return self._fit_full(X, n_components)\\n        elif self._fit_svd_solver in [\"arpack\", \"randomized\"]:\\n            return self._fit_truncated(X, n_components, self._fit_svd_solver)\\n    def _fit_full(self, X, n_components):\\n        n_samples, n_features = X.shape\\n        if n_components == \"mle\":\\n            if n_samples < n_features:\\n                raise ValueError(\\n                    \"n_components='mle' is only supported if n_samples >= n_features\"\\n                )\\n        elif not 0 <= n_components <= min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be between 0 and \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='full'\" % (n_components, min(n_samples, n_features))\\n            )\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        U, S, Vt = linalg.svd(X, full_matrices=False)\\n        U, Vt = svd_flip(U, Vt)\\n        components_ = Vt\\n        explained_variance_ = (S**2) / (n_samples - 1)\\n        total_var = explained_variance_.sum()\\n        explained_variance_ratio_ = explained_variance_ / total_var\\n        singular_values_ = S.copy()  \\n        if n_components == \"mle\":\\n            n_components = _infer_dimension(explained_variance_, n_samples)\\n        elif 0 < n_components < 1.0:\\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\\n            n_components = np.searchsorted(ratio_cumsum, n_components, side=\"right\") + 1\\n        if n_components < min(n_features, n_samples):\\n            self.noise_variance_ = explained_variance_[n_components:].mean()\\n        else:\\n            self.noise_variance_ = 0.0\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = components_[:n_components]\\n        self.n_components_ = n_components\\n        self.explained_variance_ = explained_variance_[:n_components]\\n        self.explained_variance_ratio_ = explained_variance_ratio_[:n_components]\\n        self.singular_values_ = singular_values_[:n_components]\\n        return U, S, Vt\\n    def _fit_truncated(self, X, n_components, svd_solver):\\n        n_samples, n_features = X.shape\\n        if isinstance(n_components, str):\\n            raise ValueError(\\n                \"n_components=%r cannot be a string with svd_solver='%s'\"\\n                % (n_components, svd_solver)\\n            )\\n        elif not 1 <= n_components <= min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be between 1 and \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='%s'\"\\n                % (n_components, min(n_samples, n_features), svd_solver)\\n            )\\n        elif svd_solver == \"arpack\" and n_components == min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be strictly less than \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='%s'\"\\n                % (n_components, min(n_samples, n_features), svd_solver)\\n            )\\n        random_state = check_random_state(self.random_state)\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        if svd_solver == \"arpack\":\\n            v0 = _init_arpack_v0(min(X.shape), random_state)\\n            U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0)\\n            S = S[::-1]\\n            U, Vt = svd_flip(U[:, ::-1], Vt[::-1])\\n        elif svd_solver == \"randomized\":\\n            U, S, Vt = randomized_svd(\\n                X,\\n                n_components=n_components,\\n                n_oversamples=self.n_oversamples,\\n                n_iter=self.iterated_power,\\n                power_iteration_normalizer=self.power_iteration_normalizer,\\n                flip_sign=True,\\n                random_state=random_state,\\n            )\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = Vt\\n        self.n_components_ = n_components\\n        self.explained_variance_ = (S**2) / (n_samples - 1)\\n        N = X.shape[0] - 1\\n        np.square(X, out=X)\\n        np.sum(X, axis=0, out=X[0])\\n        total_var = (X[0] / N).sum()\\n        self.explained_variance_ratio_ = self.explained_variance_ / total_var\\n        self.singular_values_ = S.copy()  \\n        if self.n_components_ < min(n_features, n_samples):\\n            self.noise_variance_ = total_var - self.explained_variance_.sum()\\n            self.noise_variance_ /= min(n_features, n_samples) - n_components\\n        else:\\n            self.noise_variance_ = 0.0\\n        return U, S, Vt"
  },
  {
    "code": "def _setup_master(self):\\n\\t\\tRouter.max_message_size = self.config['max_message_size']\\n\\t\\tif self.config['profiling']:\\n\\t\\t\\tenable_profiling()\\n\\t\\tself.broker = Broker(activate_compat=False)\\n\\t\\tself.router = Router(self.broker)\\n\\t\\tself.router.debug = self.config.get('debug', False)\\n\\t\\tself.router.undirectional = self.config['unidirectional']\\n\\t\\tself.router.add_handler(\\n\\t\\t\\tfn=self._on_shutdown_msg,\\n\\t\\t\\thandle=SHUTDOWN,\\n\\t\\t\\tpolicy=has_parent_authority,\\n\\t\\t)\\n\\t\\tself.master = Context(self.router, 0, 'master')\\n\\t\\tparent_id = self.config['parent_ids'][0]\\n\\t\\tif parent_id == 0:\\n\\t\\t\\tself.parent = self.master\\n\\t\\telse:\\n\\t\\t\\tself.parent = Context(self.router, parent_id, 'parent')\\n\\t\\tin_fd = self.config.get('in_fd', 100)\\n\\t\\tin_fp = os.fdopen(os.dup(in_fd), 'rb', 0)\\n\\t\\tos.close(in_fd)\\n\\t\\tout_fp = os.fdopen(os.dup(self.config.get('out_fd', 1)), 'wb', 0)\\n\\t\\tself.stream = MitogenProtocol.build_stream(self.router, parent_id)\\n\\t\\tself.stream.accept(in_fp, out_fp)\\n\\t\\tself.stream.name = 'parent'\\n\\t\\tself.stream.receive_side.keep_alive = False\\n\\t\\tlisten(self.stream, 'disconnect', self._on_parent_disconnect)\\n\\t\\tlisten(self.broker, 'exit', self._on_broker_exit)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "[security] core: undirectional routing wasn't respected in some cases\\n\\nWhen creating a context using Router.method(via=somechild),\\nunidirectional mode was set on the new child correctly, however if the\\nchild were to call Router.method(), due to a typing mistake the new\\nchild would start without it.\\n\\nThis doesn't impact the Ansible extension, as only forked tasks are\\nstarted directly by children, and they are not responsible for routing\\nmessages.\\n\\nAdd test so it can't happen again.",
    "fixed_code": "def _setup_master(self):\\n\\t\\tRouter.max_message_size = self.config['max_message_size']\\n\\t\\tif self.config['profiling']:\\n\\t\\t\\tenable_profiling()\\n\\t\\tself.broker = Broker(activate_compat=False)\\n\\t\\tself.router = Router(self.broker)\\n\\t\\tself.router.debug = self.config.get('debug', False)\\n\\t\\tself.router.unidirectional = self.config['unidirectional']\\n\\t\\tself.router.add_handler(\\n\\t\\t\\tfn=self._on_shutdown_msg,\\n\\t\\t\\thandle=SHUTDOWN,\\n\\t\\t\\tpolicy=has_parent_authority,\\n\\t\\t)\\n\\t\\tself.master = Context(self.router, 0, 'master')\\n\\t\\tparent_id = self.config['parent_ids'][0]\\n\\t\\tif parent_id == 0:\\n\\t\\t\\tself.parent = self.master\\n\\t\\telse:\\n\\t\\t\\tself.parent = Context(self.router, parent_id, 'parent')\\n\\t\\tin_fd = self.config.get('in_fd', 100)\\n\\t\\tin_fp = os.fdopen(os.dup(in_fd), 'rb', 0)\\n\\t\\tos.close(in_fd)\\n\\t\\tout_fp = os.fdopen(os.dup(self.config.get('out_fd', 1)), 'wb', 0)\\n\\t\\tself.stream = MitogenProtocol.build_stream(self.router, parent_id)\\n\\t\\tself.stream.accept(in_fp, out_fp)\\n\\t\\tself.stream.name = 'parent'\\n\\t\\tself.stream.receive_side.keep_alive = False\\n\\t\\tlisten(self.stream, 'disconnect', self._on_parent_disconnect)\\n\\t\\tlisten(self.broker, 'exit', self._on_broker_exit)"
  },
  {
    "code": "def display_for_value(value, empty_value_display, boolean=False):\\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\\n    if boolean:\\n        return _boolean_icon(value)\\n    elif value is None:\\n        return empty_value_display\\n    elif isinstance(value, datetime.datetime):\\n        return formats.localize(timezone.template_localtime(value))\\n    elif isinstance(value, (datetime.date, datetime.time)):\\n        return formats.localize(value)\\n    elif isinstance(value, six.integer_types + (decimal.Decimal, float)):\\n        return formats.number_format(value)\\n    else:\\n        return smart_text(value)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26582 -- Added prettier admin display for list values.",
    "fixed_code": "def display_for_value(value, empty_value_display, boolean=False):\\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\\n    if boolean:\\n        return _boolean_icon(value)\\n    elif value is None:\\n        return empty_value_display\\n    elif isinstance(value, datetime.datetime):\\n        return formats.localize(timezone.template_localtime(value))\\n    elif isinstance(value, (datetime.date, datetime.time)):\\n        return formats.localize(value)\\n    elif isinstance(value, six.integer_types + (decimal.Decimal, float)):\\n        return formats.number_format(value)\\n    elif isinstance(value, (list, tuple)):\\n        return ', '.join(force_text(v) for v in value)\\n    else:\\n        return smart_text(value)"
  },
  {
    "code": "def set_atom(self, block, existing_col, min_itemsize, nan_rep, **kwargs):\\n        self.values = list(block.items)\\n        if block.dtype.name == 'object':\\n            self.set_atom_object(block, existing_col, min_itemsize, nan_rep)\\n        else:\\n            self.set_atom_data(block)\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: correctly interpret data column dtypes and raise NotImplementedError (in cases of unicode/datetime64/date)",
    "fixed_code": "def set_atom(self, block, existing_col, min_itemsize, nan_rep, **kwargs):\\n        self.values   = list(block.items)\\n        dtype         = block.dtype.name\\n        if dtype == 'object':\\n            inferred_type = lib.infer_dtype(block.values.flatten())\\n            if inferred_type == 'unicode':\\n                raise NotImplementedError(\"unicode is not implemented as a table column\")\\n            elif inferred_type == 'date':\\n                raise NotImplementedError(\"date is not implemented as a table column\")\\n            self.set_atom_object(block, existing_col, min_itemsize, nan_rep)\\n        elif dtype == 'datetime64[ns]':\\n            raise NotImplementedError(\"datetime64[ns] is not implemented as a table column\")\\n        else:\\n            self.set_atom_data(block)\\n        return self"
  },
  {
    "code": "def __init__(self):\\n        overrides = FORMFIELD_FOR_DBFIELD_DEFAULTS.copy()\\n        overrides.update(self.formfield_overrides)\\n        self.formfield_overrides = overrides",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19303 -- Fixed ModelAdmin.formfield_overrides on fields with choices",
    "fixed_code": "def __init__(self):\\n        self._orig_formfield_overrides = self.formfield_overrides\\n        overrides = FORMFIELD_FOR_DBFIELD_DEFAULTS.copy()\\n        overrides.update(self.formfield_overrides)\\n        self.formfield_overrides = overrides"
  },
  {
    "code": "def _maybe_upcast_indexer(result, indexer, other, dtype=None):\\n    original_dtype = result.dtype",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: explicity change nan -> NaT when assigning to datelike dtypes",
    "fixed_code": "def _maybe_upcast_indexer(result, indexer, other, dtype=None):\\n    other = _maybe_cast_scalar(result.dtype, other)\\n    original_dtype = result.dtype"
  },
  {
    "code": "def fetch_url(module, url, data=None, headers=None, method=None,\\n              use_proxy=True, force=False, last_mod_time=None, timeout=10):\\n    if not HAS_URLPARSE:\\n        module.fail_json(msg='urlparse is not installed')\\n    old_tempdir = tempfile.tempdir\\n    tempfile.tempdir = module.tmpdir\\n    validate_certs = module.params.get('validate_certs', True)\\n    username = module.params.get('url_username', '')\\n    password = module.params.get('url_password', '')\\n    http_agent = module.params.get('http_agent', 'ansible-httpget')\\n    force_basic_auth = module.params.get('force_basic_auth', '')\\n    follow_redirects = module.params.get('follow_redirects', 'urllib2')\\n    client_cert = module.params.get('client_cert')\\n    client_key = module.params.get('client_key')\\n    cookies = cookiejar.LWPCookieJar()\\n    r = None\\n    info = dict(url=url)\\n    try:\\n        r = open_url(url, data=data, headers=headers, method=method,\\n                     use_proxy=use_proxy, force=force, last_mod_time=last_mod_time, timeout=timeout,\\n                     validate_certs=validate_certs, url_username=username,\\n                     url_password=password, http_agent=http_agent, force_basic_auth=force_basic_auth,\\n                     follow_redirects=follow_redirects, client_cert=client_cert,\\n                     client_key=client_key, cookies=cookies)\\n        info.update(dict((k.lower(), v) for k, v in r.info().items()))\\n        if PY3:\\n            temp_headers = {}\\n            for name, value in r.headers.items():\\n                name = name.lower()\\n                if name in temp_headers:\\n                    temp_headers[name] = ', '.join((temp_headers[name], value))\\n                else:\\n                    temp_headers[name] = value\\n            info.update(temp_headers)\\n        cookie_list = []\\n        cookie_dict = dict()\\n        for cookie in cookies:\\n            cookie_dict[cookie.name] = cookie.value\\n            cookie_list.append((cookie.name, cookie.value))\\n        info['cookies_string'] = '; '.join('%s=%s' % c for c in cookie_list)\\n        info['cookies'] = cookie_dict\\n        info.update(dict(msg=\"OK (%s bytes)\" % r.headers.get('Content-Length', 'unknown'), url=r.geturl(), status=r.code))\\n    except NoSSLError as e:\\n        distribution = get_distribution()\\n        if distribution is not None and distribution.lower() == 'redhat':\\n            module.fail_json(msg='%s. You can also install python-ssl from EPEL' % to_native(e))\\n        else:\\n            module.fail_json(msg='%s' % to_native(e))\\n    except (ConnectionError, ValueError) as e:\\n        module.fail_json(msg=to_native(e))\\n    except urllib_error.HTTPError as e:\\n        try:\\n            body = e.read()\\n        except AttributeError:\\n            body = ''\\n        try:\\n            info.update(dict(**e.info()))\\n        except:\\n            pass\\n        info.update({'msg': to_native(e), 'body': body, 'status': e.code})\\n    except urllib_error.URLError as e:\\n        code = int(getattr(e, 'code', -1))\\n        info.update(dict(msg=\"Request failed: %s\" % to_native(e), status=code))\\n    except socket.error as e:\\n        info.update(dict(msg=\"Connection failure: %s\" % to_native(e), status=-1))\\n    except httplib.BadStatusLine as e:\\n        info.update(dict(msg=\"Connection failure: connection was closed before a valid response was received: %s\" % to_native(e.line), status=-1))\\n    except Exception as e:\\n        info.update(dict(msg=\"An unknown error occurred: %s\" % to_native(e), status=-1),\\n                    exception=traceback.format_exc())\\n    finally:\\n        tempfile.tempdir = old_tempdir\\n    return r, info",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def fetch_url(module, url, data=None, headers=None, method=None,\\n              use_proxy=True, force=False, last_mod_time=None, timeout=10):\\n    if not HAS_URLPARSE:\\n        module.fail_json(msg='urlparse is not installed')\\n    old_tempdir = tempfile.tempdir\\n    tempfile.tempdir = module.tmpdir\\n    validate_certs = module.params.get('validate_certs', True)\\n    username = module.params.get('url_username', '')\\n    password = module.params.get('url_password', '')\\n    http_agent = module.params.get('http_agent', 'ansible-httpget')\\n    force_basic_auth = module.params.get('force_basic_auth', '')\\n    follow_redirects = module.params.get('follow_redirects', 'urllib2')\\n    client_cert = module.params.get('client_cert')\\n    client_key = module.params.get('client_key')\\n    cookies = cookiejar.LWPCookieJar()\\n    r = None\\n    info = dict(url=url)\\n    try:\\n        r = open_url(url, data=data, headers=headers, method=method,\\n                     use_proxy=use_proxy, force=force, last_mod_time=last_mod_time, timeout=timeout,\\n                     validate_certs=validate_certs, url_username=username,\\n                     url_password=password, http_agent=http_agent, force_basic_auth=force_basic_auth,\\n                     follow_redirects=follow_redirects, client_cert=client_cert,\\n                     client_key=client_key, cookies=cookies)\\n        info.update(dict((k.lower(), v) for k, v in r.info().items()))\\n        if PY3:\\n            temp_headers = {}\\n            for name, value in r.headers.items():\\n                name = name.lower()\\n                if name in temp_headers:\\n                    temp_headers[name] = ', '.join((temp_headers[name], value))\\n                else:\\n                    temp_headers[name] = value\\n            info.update(temp_headers)\\n        cookie_list = []\\n        cookie_dict = dict()\\n        for cookie in cookies:\\n            cookie_dict[cookie.name] = cookie.value\\n            cookie_list.append((cookie.name, cookie.value))\\n        info['cookies_string'] = '; '.join('%s=%s' % c for c in cookie_list)\\n        info['cookies'] = cookie_dict\\n        info.update(dict(msg=\"OK (%s bytes)\" % r.headers.get('Content-Length', 'unknown'), url=r.geturl(), status=r.code))\\n    except NoSSLError as e:\\n        distribution = get_distribution()\\n        if distribution is not None and distribution.lower() == 'redhat':\\n            module.fail_json(msg='%s. You can also install python-ssl from EPEL' % to_native(e))\\n        else:\\n            module.fail_json(msg='%s' % to_native(e))\\n    except (ConnectionError, ValueError) as e:\\n        module.fail_json(msg=to_native(e))\\n    except urllib_error.HTTPError as e:\\n        try:\\n            body = e.read()\\n        except AttributeError:\\n            body = ''\\n        try:\\n            info.update(dict(**e.info()))\\n        except:\\n            pass\\n        info.update({'msg': to_native(e), 'body': body, 'status': e.code})\\n    except urllib_error.URLError as e:\\n        code = int(getattr(e, 'code', -1))\\n        info.update(dict(msg=\"Request failed: %s\" % to_native(e), status=code))\\n    except socket.error as e:\\n        info.update(dict(msg=\"Connection failure: %s\" % to_native(e), status=-1))\\n    except httplib.BadStatusLine as e:\\n        info.update(dict(msg=\"Connection failure: connection was closed before a valid response was received: %s\" % to_native(e.line), status=-1))\\n    except Exception as e:\\n        info.update(dict(msg=\"An unknown error occurred: %s\" % to_native(e), status=-1),\\n                    exception=traceback.format_exc())\\n    finally:\\n        tempfile.tempdir = old_tempdir\\n    return r, info"
  },
  {
    "code": "def remove_level(self) -> None:\\n        self.print(\"p->level--;\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Revert \"bpo-46110: Add a recursion check to avoid stack overflow in the PEG parser (GH-30177)\" (GH-30363)",
    "fixed_code": "def remove_level(self) -> None:\\n        self.print(\"D(p->level--);\")"
  },
  {
    "code": "def _read_object(self, ref):\\n\\t\\tresult = self._objects[ref]\\n\\t\\tif result is not _undefined:\\n\\t\\t\\treturn result\\n\\t\\toffset = self._object_offsets[ref]\\n\\t\\tself._fp.seek(offset)\\n\\t\\ttoken = self._fp.read(1)[0]\\n\\t\\ttokenH, tokenL = token & 0xF0, token & 0x0F\\n\\t\\tif token == 0x00:\\n\\t\\t\\tresult = None\\n\\t\\telif token == 0x08:\\n\\t\\t\\tresult = False\\n\\t\\telif token == 0x09:\\n\\t\\t\\tresult = True\\n\\t\\telif token == 0x0f:\\n\\t\\t\\tresult = b''\\n\\t\\telif tokenH == 0x10:  \\n\\t\\t\\tresult = int.from_bytes(self._fp.read(1 << tokenL),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t'big', signed=tokenL >= 3)\\n\\t\\telif token == 0x22: \\n\\t\\t\\tresult = struct.unpack('>f', self._fp.read(4))[0]\\n\\t\\telif token == 0x23: \\n\\t\\t\\tresult = struct.unpack('>d', self._fp.read(8))[0]\\n\\t\\telif token == 0x33:  \\n\\t\\t\\tf = struct.unpack('>d', self._fp.read(8))[0]\\n\\t\\t\\tresult = (datetime.datetime(2001, 1, 1) +\\n\\t\\t\\t\\t\\t  datetime.timedelta(seconds=f))\\n\\t\\telif tokenH == 0x40:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tresult = self._fp.read(s)\\n\\t\\t\\tif len(result) != s:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\telif tokenH == 0x50:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tdata = self._fp.read(s)\\n\\t\\t\\tif len(data) != s:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\t\\tresult = data.decode('ascii')\\n\\t\\telif tokenH == 0x60:  \\n\\t\\t\\ts = self._get_size(tokenL) * 2\\n\\t\\t\\tdata = self._fp.read(s)\\n\\t\\t\\tif len(data) != s:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\t\\tresult = data.decode('utf-16be')\\n\\t\\telif tokenH == 0x80:  \\n\\t\\t\\tresult = UID(int.from_bytes(self._fp.read(1 + tokenL), 'big'))\\n\\t\\telif tokenH == 0xA0:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tobj_refs = self._read_refs(s)\\n\\t\\t\\tresult = []\\n\\t\\t\\tself._objects[ref] = result\\n\\t\\t\\tresult.extend(self._read_object(x) for x in obj_refs)\\n\\t\\telif tokenH == 0xD0:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tkey_refs = self._read_refs(s)\\n\\t\\t\\tobj_refs = self._read_refs(s)\\n\\t\\t\\tresult = self._dict_type()\\n\\t\\t\\tself._objects[ref] = result\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tfor k, o in zip(key_refs, obj_refs):\\n\\t\\t\\t\\t\\tresult[self._read_object(k)] = self._read_object(o)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\telse:\\n\\t\\t\\traise InvalidFileException()\\n\\t\\tself._objects[ref] = result\\n\\t\\treturn result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read_object(self, ref):\\n\\t\\tresult = self._objects[ref]\\n\\t\\tif result is not _undefined:\\n\\t\\t\\treturn result\\n\\t\\toffset = self._object_offsets[ref]\\n\\t\\tself._fp.seek(offset)\\n\\t\\ttoken = self._fp.read(1)[0]\\n\\t\\ttokenH, tokenL = token & 0xF0, token & 0x0F\\n\\t\\tif token == 0x00:\\n\\t\\t\\tresult = None\\n\\t\\telif token == 0x08:\\n\\t\\t\\tresult = False\\n\\t\\telif token == 0x09:\\n\\t\\t\\tresult = True\\n\\t\\telif token == 0x0f:\\n\\t\\t\\tresult = b''\\n\\t\\telif tokenH == 0x10:  \\n\\t\\t\\tresult = int.from_bytes(self._fp.read(1 << tokenL),\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t'big', signed=tokenL >= 3)\\n\\t\\telif token == 0x22: \\n\\t\\t\\tresult = struct.unpack('>f', self._fp.read(4))[0]\\n\\t\\telif token == 0x23: \\n\\t\\t\\tresult = struct.unpack('>d', self._fp.read(8))[0]\\n\\t\\telif token == 0x33:  \\n\\t\\t\\tf = struct.unpack('>d', self._fp.read(8))[0]\\n\\t\\t\\tresult = (datetime.datetime(2001, 1, 1) +\\n\\t\\t\\t\\t\\t  datetime.timedelta(seconds=f))\\n\\t\\telif tokenH == 0x40:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tresult = self._fp.read(s)\\n\\t\\t\\tif len(result) != s:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\telif tokenH == 0x50:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tdata = self._fp.read(s)\\n\\t\\t\\tif len(data) != s:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\t\\tresult = data.decode('ascii')\\n\\t\\telif tokenH == 0x60:  \\n\\t\\t\\ts = self._get_size(tokenL) * 2\\n\\t\\t\\tdata = self._fp.read(s)\\n\\t\\t\\tif len(data) != s:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\t\\tresult = data.decode('utf-16be')\\n\\t\\telif tokenH == 0x80:  \\n\\t\\t\\tresult = UID(int.from_bytes(self._fp.read(1 + tokenL), 'big'))\\n\\t\\telif tokenH == 0xA0:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tobj_refs = self._read_refs(s)\\n\\t\\t\\tresult = []\\n\\t\\t\\tself._objects[ref] = result\\n\\t\\t\\tresult.extend(self._read_object(x) for x in obj_refs)\\n\\t\\telif tokenH == 0xD0:  \\n\\t\\t\\ts = self._get_size(tokenL)\\n\\t\\t\\tkey_refs = self._read_refs(s)\\n\\t\\t\\tobj_refs = self._read_refs(s)\\n\\t\\t\\tresult = self._dict_type()\\n\\t\\t\\tself._objects[ref] = result\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tfor k, o in zip(key_refs, obj_refs):\\n\\t\\t\\t\\t\\tresult[self._read_object(k)] = self._read_object(o)\\n\\t\\t\\texcept TypeError:\\n\\t\\t\\t\\traise InvalidFileException()\\n\\t\\telse:\\n\\t\\t\\traise InvalidFileException()\\n\\t\\tself._objects[ref] = result\\n\\t\\treturn result"
  },
  {
    "code": "def bind_partial(__bind_self, *args, **kwargs):\\n        s signature.\\n        Raises `TypeError` if the passed arguments can not be bound.\\n        '''\\n        return __bind_self._bind(args, kwargs, partial=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def bind_partial(__bind_self, *args, **kwargs):\\n        s signature.\\n        Raises `TypeError` if the passed arguments can not be bound.\\n        '''\\n        return __bind_self._bind(args, kwargs, partial=True)"
  },
  {
    "code": "def get_block_type(values, dtype: Optional[Dtype] = None):\\n    dtype = cast(np.dtype, pandas_dtype(dtype) if dtype else values.dtype)\\n    vtype = dtype.type\\n    kind = dtype.kind\\n    cls: Type[Block]\\n    if is_sparse(dtype):\\n        cls = ExtensionBlock\\n    elif isinstance(dtype, CategoricalDtype):\\n        cls = ExtensionBlock\\n    elif vtype is Timestamp:\\n        cls = DatetimeTZBlock\\n    elif vtype is Interval or vtype is Period:\\n        cls = ObjectValuesExtensionBlock\\n    elif isinstance(dtype, ExtensionDtype):\\n        cls = ExtensionBlock\\n    elif kind == \"M\":\\n        cls = DatetimeBlock\\n    elif kind == \"m\":\\n        cls = TimeDeltaBlock\\n    elif kind == \"f\":\\n        cls = FloatBlock\\n    elif kind in [\"c\", \"i\", \"u\", \"b\"]:\\n        cls = NumericBlock\\n    else:\\n        cls = ObjectBlock\\n    return cls",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: remove FloatBlock, share _can_hold_na (#40526)",
    "fixed_code": "def get_block_type(values, dtype: Optional[Dtype] = None):\\n    dtype = cast(np.dtype, pandas_dtype(dtype) if dtype else values.dtype)\\n    vtype = dtype.type\\n    kind = dtype.kind\\n    cls: Type[Block]\\n    if is_sparse(dtype):\\n        cls = ExtensionBlock\\n    elif isinstance(dtype, CategoricalDtype):\\n        cls = ExtensionBlock\\n    elif vtype is Timestamp:\\n        cls = DatetimeTZBlock\\n    elif vtype is Interval or vtype is Period:\\n        cls = ObjectValuesExtensionBlock\\n    elif isinstance(dtype, ExtensionDtype):\\n        cls = ExtensionBlock\\n    elif kind == \"M\":\\n        cls = DatetimeBlock\\n    elif kind == \"m\":\\n        cls = TimeDeltaBlock\\n    elif kind in [\"f\", \"c\", \"i\", \"u\", \"b\"]:\\n        cls = NumericBlock\\n    else:\\n        cls = ObjectBlock\\n    return cls"
  },
  {
    "code": "def get_sequences(self):\\n        results = {}\\n        f = open(os.path.join(self._path, '.mh_sequences'), 'r', newline='')\\n        try:\\n            all_keys = set(self.keys())\\n            for line in f:\\n                try:\\n                    name, contents = line.split(':')\\n                    keys = set()\\n                    for spec in contents.split():\\n                        if spec.isdigit():\\n                            keys.add(int(spec))\\n                        else:\\n                            start, stop = (int(x) for x in spec.split('-'))\\n                            keys.update(range(start, stop + 1))\\n                    results[name] = [key for key in sorted(keys) \\\\n                                         if key in all_keys]\\n                    if len(results[name]) == 0:\\n                        del results[name]\\n                except ValueError:\\n                    raise FormatError('Invalid sequence specification: %s' %\\n                                      line.rstrip())\\n        finally:\\n            f.close()\\n        return results",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_sequences(self):\\n        results = {}\\n        f = open(os.path.join(self._path, '.mh_sequences'), 'r', newline='')\\n        try:\\n            all_keys = set(self.keys())\\n            for line in f:\\n                try:\\n                    name, contents = line.split(':')\\n                    keys = set()\\n                    for spec in contents.split():\\n                        if spec.isdigit():\\n                            keys.add(int(spec))\\n                        else:\\n                            start, stop = (int(x) for x in spec.split('-'))\\n                            keys.update(range(start, stop + 1))\\n                    results[name] = [key for key in sorted(keys) \\\\n                                         if key in all_keys]\\n                    if len(results[name]) == 0:\\n                        del results[name]\\n                except ValueError:\\n                    raise FormatError('Invalid sequence specification: %s' %\\n                                      line.rstrip())\\n        finally:\\n            f.close()\\n        return results"
  },
  {
    "code": "def __new__(cls, filename, lineno, function, code_context, index, *, positions=None):\\n        instance = super().__new__(cls, filename, lineno, function, code_context, index)\\n        instance.positions = positions\\n        return instance",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __new__(cls, filename, lineno, function, code_context, index, *, positions=None):\\n        instance = super().__new__(cls, filename, lineno, function, code_context, index)\\n        instance.positions = positions\\n        return instance"
  },
  {
    "code": "def __setitem__(self, key, value):\\n            self.putenv(key, str(value))\\n            self.data[self.keymap(key)] = str(value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __setitem__(self, key, value):\\n            self.putenv(key, str(value))\\n            self.data[self.keymap(key)] = str(value)"
  },
  {
    "code": "def _read_frame(self, group, where=None):\\n        from pandas.core.internals import BlockManager, make_block\\n        index = _read_index(group, 'index')\\n        frame_columns = _read_index(group, 'columns')\\n        blocks = []\\n        for i in range(group._v_attrs.nblocks):\\n            blk_columns = _read_index(group, 'block%d_columns' % i)\\n            values = _read_array(group, 'block%d_values' % i)\\n            blk = make_block(values, blk_columns, frame_columns)\\n            blocks.append(blk)\\n        mgr = BlockManager(blocks, index=index, columns=frame_columns)\\n        return DataFrame(mgr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read_frame(self, group, where=None):\\n        from pandas.core.internals import BlockManager, make_block\\n        index = _read_index(group, 'index')\\n        frame_columns = _read_index(group, 'columns')\\n        blocks = []\\n        for i in range(group._v_attrs.nblocks):\\n            blk_columns = _read_index(group, 'block%d_columns' % i)\\n            values = _read_array(group, 'block%d_values' % i)\\n            blk = make_block(values, blk_columns, frame_columns)\\n            blocks.append(blk)\\n        mgr = BlockManager(blocks, index=index, columns=frame_columns)\\n        return DataFrame(mgr)"
  },
  {
    "code": "def _get_obj_absolute_path(obj_path):\\n\\treturn safe_join(DATAROOT, obj_path)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_obj_absolute_path(obj_path):\\n\\treturn safe_join(DATAROOT, obj_path)"
  },
  {
    "code": "def get_header(self, name, default=None):\\n        return to_native_str(self.request.headers.get(name, default))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Do not break cookie parsing on non-utf8 headers",
    "fixed_code": "def get_header(self, name, default=None):\\n        return to_native_str(self.request.headers.get(name, default),\\n                             errors='replace')"
  },
  {
    "code": "def strip_prefix(s):\\n\\t\\t\\treturn s[1:] if s.startswith(\"u'\") else s\\n\\t\\twith connection.cursor() as cursor:\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield (\\n\\t\\t\\t\\t\"\\n\\t\\t\\t\\t\"Django to create, modify, and delete the table\"\\n\\t\\t\\t)\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"from __future__ import unicode_literals\"\\n\\t\\t\\tyield ''\\n\\t\\t\\tyield 'from %s import models' % self.db_module\\n\\t\\t\\tknown_models = []\\n\\t\\t\\tfor table_name in connection.introspection.table_names(cursor):\\n\\t\\t\\t\\tif table_name_filter is not None and callable(table_name_filter):\\n\\t\\t\\t\\t\\tif not table_name_filter(table_name):\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\trelations = connection.introspection.get_relations(cursor, table_name)\\n\\t\\t\\t\\t\\texcept NotImplementedError:\\n\\t\\t\\t\\t\\t\\trelations = {}\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tindexes = connection.introspection.get_indexes(cursor, table_name)\\n\\t\\t\\t\\t\\texcept NotImplementedError:\\n\\t\\t\\t\\t\\t\\tindexes = {}\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tconstraints = connection.introspection.get_constraints(cursor, table_name)\\n\\t\\t\\t\\t\\texcept NotImplementedError:\\n\\t\\t\\t\\t\\t\\tconstraints = {}\\n\\t\\t\\t\\t\\ttable_description = connection.introspection.get_table_description(cursor, table_name)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tyield \"\\n\\t\\t\\t\\t\\tyield \"\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tyield ''\\n\\t\\t\\t\\tyield ''\\n\\t\\t\\t\\tyield 'class %s(models.Model):' % table2model(table_name)\\n\\t\\t\\t\\tknown_models.append(table2model(table_name))\\n\\t\\t\\t\\tused_column_names = []  \\n\\t\\t\\t\\tcolumn_to_field_name = {}  \\n\\t\\t\\t\\tfor row in table_description:\\n\\t\\t\\t\\t\\tcomment_notes = []  \\n\\t\\t\\t\\t\\textra_params = OrderedDict()  \\n\\t\\t\\t\\t\\tcolumn_name = row[0]\\n\\t\\t\\t\\t\\tis_relation = column_name in relations\\n\\t\\t\\t\\t\\tatt_name, params, notes = self.normalize_col_name(\\n\\t\\t\\t\\t\\t\\tcolumn_name, used_column_names, is_relation)\\n\\t\\t\\t\\t\\textra_params.update(params)\\n\\t\\t\\t\\t\\tcomment_notes.extend(notes)\\n\\t\\t\\t\\t\\tused_column_names.append(att_name)\\n\\t\\t\\t\\t\\tcolumn_to_field_name[column_name] = att_name\\n\\t\\t\\t\\t\\tif column_name in indexes:\\n\\t\\t\\t\\t\\t\\tif indexes[column_name]['primary_key']:\\n\\t\\t\\t\\t\\t\\t\\textra_params['primary_key'] = True\\n\\t\\t\\t\\t\\t\\telif indexes[column_name]['unique']:\\n\\t\\t\\t\\t\\t\\t\\textra_params['unique'] = True\\n\\t\\t\\t\\t\\tif is_relation:\\n\\t\\t\\t\\t\\t\\trel_to = (\\n\\t\\t\\t\\t\\t\\t\\t\"self\" if relations[column_name][1] == table_name\\n\\t\\t\\t\\t\\t\\t\\telse table2model(relations[column_name][1])\\n\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\tif rel_to in known_models:\\n\\t\\t\\t\\t\\t\\t\\tfield_type = 'ForeignKey(%s' % rel_to\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tfield_type = \"ForeignKey('%s'\" % rel_to\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tfield_type, field_params, field_notes = self.get_field_type(connection, table_name, row)\\n\\t\\t\\t\\t\\t\\textra_params.update(field_params)\\n\\t\\t\\t\\t\\t\\tcomment_notes.extend(field_notes)\\n\\t\\t\\t\\t\\t\\tfield_type += '('\\n\\t\\t\\t\\t\\tif att_name == 'id' and extra_params == {'primary_key': True}:\\n\\t\\t\\t\\t\\t\\tif field_type == 'AutoField(':\\n\\t\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\t\\telif field_type == 'IntegerField(' and not connection.features.can_introspect_autofield:\\n\\t\\t\\t\\t\\t\\t\\tcomment_notes.append('AutoField?')\\n\\t\\t\\t\\t\\tif row[6]:  \\n\\t\\t\\t\\t\\t\\tif field_type == 'BooleanField(':\\n\\t\\t\\t\\t\\t\\t\\tfield_type = 'NullBooleanField('\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\textra_params['blank'] = True\\n\\t\\t\\t\\t\\t\\t\\textra_params['null'] = True\\n\\t\\t\\t\\t\\tfield_desc = '%s = %s%s' % (\\n\\t\\t\\t\\t\\t\\tatt_name,\\n\\t\\t\\t\\t\\t\\t'' if '.' in field_type else 'models.',\\n\\t\\t\\t\\t\\t\\tfield_type,\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tif field_type.startswith('ForeignKey('):\\n\\t\\t\\t\\t\\t\\tfield_desc += ', models.DO_NOTHING'\\n\\t\\t\\t\\t\\tif extra_params:\\n\\t\\t\\t\\t\\t\\tif not field_desc.endswith('('):\\n\\t\\t\\t\\t\\t\\t\\tfield_desc += ', '\\n\\t\\t\\t\\t\\t\\tfield_desc += ', '.join(\\n\\t\\t\\t\\t\\t\\t\\t'%s=%s' % (k, strip_prefix(repr(v)))\\n\\t\\t\\t\\t\\t\\t\\tfor k, v in extra_params.items())\\n\\t\\t\\t\\t\\tfield_desc += ')'\\n\\t\\t\\t\\t\\tif comment_notes:\\n\\t\\t\\t\\t\\t\\tfield_desc += '  \\n\\t\\t\\t\\t\\tyield '\\t%s' % field_desc\\n\\t\\t\\t\\tfor meta_line in self.get_meta(table_name, constraints, column_to_field_name):\\n\\t\\t\\t\\t\\tyield meta_line",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def strip_prefix(s):\\n\\t\\t\\treturn s[1:] if s.startswith(\"u'\") else s\\n\\t\\twith connection.cursor() as cursor:\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield (\\n\\t\\t\\t\\t\"\\n\\t\\t\\t\\t\"Django to create, modify, and delete the table\"\\n\\t\\t\\t)\\n\\t\\t\\tyield \"\\n\\t\\t\\tyield \"from __future__ import unicode_literals\"\\n\\t\\t\\tyield ''\\n\\t\\t\\tyield 'from %s import models' % self.db_module\\n\\t\\t\\tknown_models = []\\n\\t\\t\\tfor table_name in connection.introspection.table_names(cursor):\\n\\t\\t\\t\\tif table_name_filter is not None and callable(table_name_filter):\\n\\t\\t\\t\\t\\tif not table_name_filter(table_name):\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\trelations = connection.introspection.get_relations(cursor, table_name)\\n\\t\\t\\t\\t\\texcept NotImplementedError:\\n\\t\\t\\t\\t\\t\\trelations = {}\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tindexes = connection.introspection.get_indexes(cursor, table_name)\\n\\t\\t\\t\\t\\texcept NotImplementedError:\\n\\t\\t\\t\\t\\t\\tindexes = {}\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tconstraints = connection.introspection.get_constraints(cursor, table_name)\\n\\t\\t\\t\\t\\texcept NotImplementedError:\\n\\t\\t\\t\\t\\t\\tconstraints = {}\\n\\t\\t\\t\\t\\ttable_description = connection.introspection.get_table_description(cursor, table_name)\\n\\t\\t\\t\\texcept Exception as e:\\n\\t\\t\\t\\t\\tyield \"\\n\\t\\t\\t\\t\\tyield \"\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tyield ''\\n\\t\\t\\t\\tyield ''\\n\\t\\t\\t\\tyield 'class %s(models.Model):' % table2model(table_name)\\n\\t\\t\\t\\tknown_models.append(table2model(table_name))\\n\\t\\t\\t\\tused_column_names = []  \\n\\t\\t\\t\\tcolumn_to_field_name = {}  \\n\\t\\t\\t\\tfor row in table_description:\\n\\t\\t\\t\\t\\tcomment_notes = []  \\n\\t\\t\\t\\t\\textra_params = OrderedDict()  \\n\\t\\t\\t\\t\\tcolumn_name = row[0]\\n\\t\\t\\t\\t\\tis_relation = column_name in relations\\n\\t\\t\\t\\t\\tatt_name, params, notes = self.normalize_col_name(\\n\\t\\t\\t\\t\\t\\tcolumn_name, used_column_names, is_relation)\\n\\t\\t\\t\\t\\textra_params.update(params)\\n\\t\\t\\t\\t\\tcomment_notes.extend(notes)\\n\\t\\t\\t\\t\\tused_column_names.append(att_name)\\n\\t\\t\\t\\t\\tcolumn_to_field_name[column_name] = att_name\\n\\t\\t\\t\\t\\tif column_name in indexes:\\n\\t\\t\\t\\t\\t\\tif indexes[column_name]['primary_key']:\\n\\t\\t\\t\\t\\t\\t\\textra_params['primary_key'] = True\\n\\t\\t\\t\\t\\t\\telif indexes[column_name]['unique']:\\n\\t\\t\\t\\t\\t\\t\\textra_params['unique'] = True\\n\\t\\t\\t\\t\\tif is_relation:\\n\\t\\t\\t\\t\\t\\trel_to = (\\n\\t\\t\\t\\t\\t\\t\\t\"self\" if relations[column_name][1] == table_name\\n\\t\\t\\t\\t\\t\\t\\telse table2model(relations[column_name][1])\\n\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\tif rel_to in known_models:\\n\\t\\t\\t\\t\\t\\t\\tfield_type = 'ForeignKey(%s' % rel_to\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tfield_type = \"ForeignKey('%s'\" % rel_to\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tfield_type, field_params, field_notes = self.get_field_type(connection, table_name, row)\\n\\t\\t\\t\\t\\t\\textra_params.update(field_params)\\n\\t\\t\\t\\t\\t\\tcomment_notes.extend(field_notes)\\n\\t\\t\\t\\t\\t\\tfield_type += '('\\n\\t\\t\\t\\t\\tif att_name == 'id' and extra_params == {'primary_key': True}:\\n\\t\\t\\t\\t\\t\\tif field_type == 'AutoField(':\\n\\t\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\t\\t\\telif field_type == 'IntegerField(' and not connection.features.can_introspect_autofield:\\n\\t\\t\\t\\t\\t\\t\\tcomment_notes.append('AutoField?')\\n\\t\\t\\t\\t\\tif row[6]:  \\n\\t\\t\\t\\t\\t\\tif field_type == 'BooleanField(':\\n\\t\\t\\t\\t\\t\\t\\tfield_type = 'NullBooleanField('\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\textra_params['blank'] = True\\n\\t\\t\\t\\t\\t\\t\\textra_params['null'] = True\\n\\t\\t\\t\\t\\tfield_desc = '%s = %s%s' % (\\n\\t\\t\\t\\t\\t\\tatt_name,\\n\\t\\t\\t\\t\\t\\t'' if '.' in field_type else 'models.',\\n\\t\\t\\t\\t\\t\\tfield_type,\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tif field_type.startswith('ForeignKey('):\\n\\t\\t\\t\\t\\t\\tfield_desc += ', models.DO_NOTHING'\\n\\t\\t\\t\\t\\tif extra_params:\\n\\t\\t\\t\\t\\t\\tif not field_desc.endswith('('):\\n\\t\\t\\t\\t\\t\\t\\tfield_desc += ', '\\n\\t\\t\\t\\t\\t\\tfield_desc += ', '.join(\\n\\t\\t\\t\\t\\t\\t\\t'%s=%s' % (k, strip_prefix(repr(v)))\\n\\t\\t\\t\\t\\t\\t\\tfor k, v in extra_params.items())\\n\\t\\t\\t\\t\\tfield_desc += ')'\\n\\t\\t\\t\\t\\tif comment_notes:\\n\\t\\t\\t\\t\\t\\tfield_desc += '  \\n\\t\\t\\t\\t\\tyield '\\t%s' % field_desc\\n\\t\\t\\t\\tfor meta_line in self.get_meta(table_name, constraints, column_to_field_name):\\n\\t\\t\\t\\t\\tyield meta_line"
  },
  {
    "code": "def __init__(self, *args, **kwargs) -> None:\\n        super().__init__(*args, **kwargs)\\n        self.schema: Optional[str] = kwargs.pop(\"schema\", None)\\n        self.connection: Optional[Connection] = kwargs.pop(\"connection\", None)\\n        self.conn: connection = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add schema as DbApiHook instance attribute (#16521)",
    "fixed_code": "def __init__(self, *args, **kwargs) -> None:\\n        super().__init__(*args, **kwargs)\\n        self.connection: Optional[Connection] = kwargs.pop(\"connection\", None)\\n        self.conn: connection = None"
  },
  {
    "code": "def read_fwf(filepath_or_buffer,\\n             colspecs=None,\\n             widths=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             delimiter=None,\\n             verbose=False,\\n             encoding=None):\\n    kwds = locals()\\n    colspecs = kwds.get('colspecs', None)\\n    widths = kwds.pop('widths', None)\\n    if bool(colspecs is None) == bool(widths is None):\\n        raise ValueError(\"You must specify only one of 'widths' and \"\\n                         \"'colspecs'\")\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append( (col, col+w) )\\n            col += w\\n        kwds['colspecs'] = colspecs\\n    kwds['thousands'] = thousands\\n    return _read(FixedWidthFieldParser, filepath_or_buffer, kwds)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_fwf(filepath_or_buffer,\\n             colspecs=None,\\n             widths=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             delimiter=None,\\n             verbose=False,\\n             encoding=None):\\n    kwds = locals()\\n    colspecs = kwds.get('colspecs', None)\\n    widths = kwds.pop('widths', None)\\n    if bool(colspecs is None) == bool(widths is None):\\n        raise ValueError(\"You must specify only one of 'widths' and \"\\n                         \"'colspecs'\")\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append( (col, col+w) )\\n            col += w\\n        kwds['colspecs'] = colspecs\\n    kwds['thousands'] = thousands\\n    return _read(FixedWidthFieldParser, filepath_or_buffer, kwds)"
  },
  {
    "code": "def is_embedded(self):\\n        self._check_web_services_version()\\n        if self.is_embedded_mode is None:\\n            about_url = self.url + self.DEFAULT_REST_API_ABOUT_PATH\\n            try:\\n                rc, data = request(about_url, timeout=self.DEFAULT_TIMEOUT, headers=self.DEFAULT_HEADERS, **self.creds)\\n                self.is_embedded_mode = not data[\"runningAsProxy\"]\\n            except Exception as error:\\n                self.module.fail_json(msg=\"Failed to retrieve the webservices about information! Array Id [%s]. Error [%s].\"\\n                                          % (self.ssid, to_native(error)))\\n        return self.is_embedded_mode",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_embedded(self):\\n        self._check_web_services_version()\\n        if self.is_embedded_mode is None:\\n            about_url = self.url + self.DEFAULT_REST_API_ABOUT_PATH\\n            try:\\n                rc, data = request(about_url, timeout=self.DEFAULT_TIMEOUT, headers=self.DEFAULT_HEADERS, **self.creds)\\n                self.is_embedded_mode = not data[\"runningAsProxy\"]\\n            except Exception as error:\\n                self.module.fail_json(msg=\"Failed to retrieve the webservices about information! Array Id [%s]. Error [%s].\"\\n                                          % (self.ssid, to_native(error)))\\n        return self.is_embedded_mode"
  },
  {
    "code": "def execute(self, file_path, **kwargs):\\n\\t\\ttarget_format = CONF.conversion_plugin_options.output_format\\n\\t\\tsrc_path = file_path.split('file://')[-1]\\n\\t\\tdest_path = \"%(path)s.%(target)s\" % {'path': src_path,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t 'target': target_format}\\n\\t\\tself.dest_path = dest_path\\n\\t\\ttry:\\n\\t\\t\\tstdout, stderr = putils.trycmd(\"qemu-img\", \"info\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \"--output=json\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   src_path,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   prlimit=utils.QEMU_IMG_PROC_LIMITS,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   log_errors=putils.LOG_ALL_ERRORS,)\\n\\t\\texcept OSError as exc:\\n\\t\\t\\twith excutils.save_and_reraise_exception():\\n\\t\\t\\t\\texc_message = encodeutils.exception_to_unicode(exc)\\n\\t\\t\\t\\tmsg = (\"Failed to do introspection as part of image \"\\n\\t\\t\\t\\t\\t   \"conversion for %(iid)s: %(err)s\")\\n\\t\\t\\t\\tLOG.error(msg, {'iid': self.image_id, 'err': exc_message})\\n\\t\\tif stderr:\\n\\t\\t\\traise RuntimeError(stderr)\\n\\t\\tmetadata = json.loads(stdout)\\n\\t\\tsource_format = metadata.get('format')\\n\\t\\tvirtual_size = metadata.get('virtual-size', 0)\\n\\t\\timage = self.image_repo.get(self.image_id)\\n\\t\\timage.virtual_size = virtual_size\\n\\t\\tif source_format == target_format:\\n\\t\\t\\tLOG.debug(\"Source is already in target format, \"\\n\\t\\t\\t\\t\\t  \"not doing conversion for %s\", self.image_id)\\n\\t\\t\\tself.image_repo.save(image)\\n\\t\\t\\treturn file_path\\n\\t\\ttry:\\n\\t\\t\\tstdout, stderr = putils.trycmd('qemu-img', 'convert',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   '-f', source_format,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   '-O', target_format,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   src_path, dest_path,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   log_errors=putils.LOG_ALL_ERRORS)\\n\\t\\texcept OSError as exc:\\n\\t\\t\\twith excutils.save_and_reraise_exception():\\n\\t\\t\\t\\texc_message = encodeutils.exception_to_unicode(exc)\\n\\t\\t\\t\\tmsg = \"Failed to do image conversion for %(iid)s: %(err)s\"\\n\\t\\t\\t\\tLOG.error(msg, {'iid': self.image_id, 'err': exc_message})\\n\\t\\tif stderr:\\n\\t\\t\\traise RuntimeError(stderr)\\n\\t\\timage.disk_format = target_format\\n\\t\\timage.container_format = 'bare'\\n\\t\\tself.image_repo.save(image)\\n\\t\\tos.remove(src_path)\\n\\t\\treturn \"file://%s\" % dest_path",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Image conversion fails\\n\\nIt is not possible to convert glance images as it fails with error,\\nNoSuchOptError: no such option 'conversion_plugin_options' in group\\n[DEFAULT] as there is no 'conversion_plugin_options' option or group\\nin glance-image-import.conf file.\\n\\nUsed correct option group 'image_conversion' to fetch the image\\n'output_format' option.\\n\\nCloses-Bug: #1805765",
    "fixed_code": "def execute(self, file_path, **kwargs):\\n\\t\\ttarget_format = CONF.image_conversion.output_format\\n\\t\\tsrc_path = file_path.split('file://')[-1]\\n\\t\\tdest_path = \"%(path)s.%(target)s\" % {'path': src_path,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t 'target': target_format}\\n\\t\\tself.dest_path = dest_path\\n\\t\\ttry:\\n\\t\\t\\tstdout, stderr = putils.trycmd(\"qemu-img\", \"info\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \"--output=json\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   src_path,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   prlimit=utils.QEMU_IMG_PROC_LIMITS,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   log_errors=putils.LOG_ALL_ERRORS,)\\n\\t\\texcept OSError as exc:\\n\\t\\t\\twith excutils.save_and_reraise_exception():\\n\\t\\t\\t\\texc_message = encodeutils.exception_to_unicode(exc)\\n\\t\\t\\t\\tmsg = (\"Failed to do introspection as part of image \"\\n\\t\\t\\t\\t\\t   \"conversion for %(iid)s: %(err)s\")\\n\\t\\t\\t\\tLOG.error(msg, {'iid': self.image_id, 'err': exc_message})\\n\\t\\tif stderr:\\n\\t\\t\\traise RuntimeError(stderr)\\n\\t\\tmetadata = json.loads(stdout)\\n\\t\\tsource_format = metadata.get('format')\\n\\t\\tvirtual_size = metadata.get('virtual-size', 0)\\n\\t\\timage = self.image_repo.get(self.image_id)\\n\\t\\timage.virtual_size = virtual_size\\n\\t\\tif source_format == target_format:\\n\\t\\t\\tLOG.debug(\"Source is already in target format, \"\\n\\t\\t\\t\\t\\t  \"not doing conversion for %s\", self.image_id)\\n\\t\\t\\tself.image_repo.save(image)\\n\\t\\t\\treturn file_path\\n\\t\\ttry:\\n\\t\\t\\tstdout, stderr = putils.trycmd('qemu-img', 'convert',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   '-f', source_format,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   '-O', target_format,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   src_path, dest_path,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   log_errors=putils.LOG_ALL_ERRORS)\\n\\t\\texcept OSError as exc:\\n\\t\\t\\twith excutils.save_and_reraise_exception():\\n\\t\\t\\t\\texc_message = encodeutils.exception_to_unicode(exc)\\n\\t\\t\\t\\tmsg = \"Failed to do image conversion for %(iid)s: %(err)s\"\\n\\t\\t\\t\\tLOG.error(msg, {'iid': self.image_id, 'err': exc_message})\\n\\t\\tif stderr:\\n\\t\\t\\traise RuntimeError(stderr)\\n\\t\\timage.disk_format = target_format\\n\\t\\timage.container_format = 'bare'\\n\\t\\tself.image_repo.save(image)\\n\\t\\tos.remove(src_path)\\n\\t\\treturn \"file://%s\" % dest_path"
  },
  {
    "code": "def Array(typecode_or_type, size_or_initializer, **kwds):\\n    ''\\n    lock = kwds.pop('lock', None)\\n    if kwds:\\n        raise ValueError('unrecognized keyword argument(s): %s' % list(kwds.keys()))\\n    obj = RawArray(typecode_or_type, size_or_initializer)\\n    if lock is False:\\n        return obj\\n    if lock in (True, None):\\n        lock = RLock()\\n    if not hasattr(lock, 'acquire'):\\n        raise AttributeError(\"'%r' has no method 'acquire'\" % lock)\\n    return synchronized(obj, lock)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def Array(typecode_or_type, size_or_initializer, **kwds):\\n    ''\\n    lock = kwds.pop('lock', None)\\n    if kwds:\\n        raise ValueError('unrecognized keyword argument(s): %s' % list(kwds.keys()))\\n    obj = RawArray(typecode_or_type, size_or_initializer)\\n    if lock is False:\\n        return obj\\n    if lock in (True, None):\\n        lock = RLock()\\n    if not hasattr(lock, 'acquire'):\\n        raise AttributeError(\"'%r' has no method 'acquire'\" % lock)\\n    return synchronized(obj, lock)"
  },
  {
    "code": "def parse_from_args(self, param_name, task_name, args, params):\\n\\t\\tdest = self.parser_dest(param_name, task_name, glob=False)\\n\\t\\tif dest is not None:\\n\\t\\t\\tvalue = getattr(args, dest, None)\\n\\t\\t\\tparams[param_name] = self.parse_from_input(param_name, value, task_name=task_name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_from_args(self, param_name, task_name, args, params):\\n\\t\\tdest = self.parser_dest(param_name, task_name, glob=False)\\n\\t\\tif dest is not None:\\n\\t\\t\\tvalue = getattr(args, dest, None)\\n\\t\\t\\tparams[param_name] = self.parse_from_input(param_name, value, task_name=task_name)"
  },
  {
    "code": "def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\\n        if len(args) == 0 and len(kwargs) == 0:\\n            target = self._sort_levels_monotonic(raise_if_incomparable=True)\\n            return lexsort_indexer(target._get_codes_for_sorting())\\n        return self._values.argsort(*args, **kwargs)\\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\\n        if len(args) == 0 and len(kwargs) == 0:\\n            target = self._sort_levels_monotonic(raise_if_incomparable=True)\\n            return lexsort_indexer(target._get_codes_for_sorting())\\n        return self._values.argsort(*args, **kwargs)\\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)"
  },
  {
    "code": "def _check_max_length_attribute(self, **kwargs):\\n        try:\\n            max_length = int(self.max_length)\\n            if max_length <= 0:\\n                raise ValueError()\\n        except TypeError:\\n            return [\\n                checks.Error(\\n                    \"CharFields must define a 'max_length' attribute.\",\\n                    hint=None,\\n                    obj=self,\\n                    id='fields.E120',\\n                )\\n            ]\\n        except ValueError:\\n            return [\\n                checks.Error(\\n                    \"'max_length' must be a positive integer.\",\\n                    hint=None,\\n                    obj=self,\\n                    id='fields.E121',\\n                )\\n            ]\\n        else:\\n            return []",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #24818 -- Prevented models.CharField from accepting a string as max_length",
    "fixed_code": "def _check_max_length_attribute(self, **kwargs):\\n        if self.max_length is None:\\n            return [\\n                checks.Error(\\n                    \"CharFields must define a 'max_length' attribute.\",\\n                    hint=None,\\n                    obj=self,\\n                    id='fields.E120',\\n                )\\n            ]\\n        elif not isinstance(self.max_length, six.integer_types) or self.max_length <= 0:\\n            return [\\n                checks.Error(\\n                    \"'max_length' must be a positive integer.\",\\n                    hint=None,\\n                    obj=self,\\n                    id='fields.E121',\\n                )\\n            ]\\n        else:\\n            return []"
  },
  {
    "code": "def _subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,\\n              subplot_kw=None, ax=None, **fig_kw):\\n    import matplotlib.pyplot as plt\\n    if subplot_kw is None:\\n        subplot_kw = {}\\n    if ax is None:\\n        fig = plt.figure(**fig_kw)\\n    else:\\n        fig = ax.get_figure()\\n    nplots = nrows*ncols\\n    axarr = np.empty(nplots, dtype=object)\\n    ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)\\n    if sharex:\\n        subplot_kw['sharex'] = ax0\\n    if sharey:\\n        subplot_kw['sharey'] = ax0\\n    axarr[0] = ax0\\n    for i in range(1, nplots):\\n        axarr[i] = fig.add_subplot(nrows, ncols, i+1, **subplot_kw)\\n    if squeeze:\\n        if nplots==1:\\n            return fig, axarr[0]\\n        else:\\n            return fig, axarr.reshape(nrows, ncols).squeeze()\\n    else:\\n        return fig, axarr.reshape(nrows, ncols)\\nif __name__ == '__main__':\\n    import matplotlib.pyplot as plt\\n    import pandas.tools.plotting as plots\\n    import pandas.core.frame as fr\\n    reload(plots)\\n    reload(fr)\\n    from pandas.core.frame import DataFrame\\n    data = DataFrame([[3, 6, -5], [4, 8, 2], [4, 9, -6],\\n                      [4, 9, -3], [2, 5, -1]],\\n                     columns=['A', 'B', 'C'])\\n    data.plot(kind='barh', stacked=True)\\n    plt.show()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: turn off redundant tick labels if sharex and sharey GH #1124",
    "fixed_code": "def _subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,\\n              subplot_kw=None, ax=None, **fig_kw):\\n    import matplotlib.pyplot as plt\\n    if subplot_kw is None:\\n        subplot_kw = {}\\n    if ax is None:\\n        fig = plt.figure(**fig_kw)\\n    else:\\n        fig = ax.get_figure()\\n    nplots = nrows*ncols\\n    axarr = np.empty(nplots, dtype=object)\\n    ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)\\n    if sharex:\\n        subplot_kw['sharex'] = ax0\\n    if sharey:\\n        subplot_kw['sharey'] = ax0\\n    axarr[0] = ax0\\n    for i in range(1, nplots):\\n        axarr[i] = fig.add_subplot(nrows, ncols, i+1, **subplot_kw)\\n    if nplots > 1:\\n        if sharex and nrows > 1:\\n            for i, ax in enumerate(axarr):\\n                if np.ceil(float(i + 1) / ncols) < nrows: \\n                    [label.set_visible(False) for label in ax.get_xticklabels()]\\n        if sharey and ncols > 1:\\n            for i, ax in enumerate(axarr):\\n                if (i % ncols) != 0: \\n                    [label.set_visible(False) for label in ax.get_yticklabels()]\\n    if squeeze:\\n        if nplots==1:\\n            axes = axarr[0]\\n        else:\\n            axes = axarr.reshape(nrows, ncols).squeeze()\\n    else:\\n        axes = axarr.reshape(nrows, ncols)\\n    return fig, axes\\nif __name__ == '__main__':\\n    import matplotlib.pyplot as plt\\n    import pandas.tools.plotting as plots\\n    import pandas.core.frame as fr\\n    reload(plots)\\n    reload(fr)\\n    from pandas.core.frame import DataFrame\\n    data = DataFrame([[3, 6, -5], [4, 8, 2], [4, 9, -6],\\n                      [4, 9, -3], [2, 5, -1]],\\n                     columns=['A', 'B', 'C'])\\n    data.plot(kind='barh', stacked=True)\\n    plt.show()"
  },
  {
    "code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options",
    "label": 1,
    "bug_type": "security",
    "bug_description": "[tf.data] Solve the stack overflow problem of getting all options on Python side.",
    "fixed_code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tself._options = input_dataset.options()\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)"
  },
  {
    "code": "def is_in_obj(gpr) -> bool:\\n\\t\\tif not hasattr(gpr, \"name\"):\\n\\t\\t\\treturn False\\n\\t\\ttry:\\n\\t\\t\\treturn gpr is obj[gpr.name]\\n\\t\\texcept (KeyError, IndexError, ValueError):\\n\\t\\t\\treturn False\\n\\tfor i, (gpr, level) in enumerate(zip(keys, levels)):\\n\\t\\tif is_in_obj(gpr):  \\n\\t\\t\\tin_axis, name = True, gpr.name\\n\\t\\t\\texclusions.append(name)\\n\\t\\telif is_in_axis(gpr):  \\n\\t\\t\\tif gpr in obj:\\n\\t\\t\\t\\tif validate:\\n\\t\\t\\t\\t\\tobj._check_label_or_level_ambiguity(gpr, axis=axis)\\n\\t\\t\\t\\tin_axis, name, gpr = True, gpr, obj[gpr]\\n\\t\\t\\t\\texclusions.append(name)\\n\\t\\t\\telif obj._is_level_reference(gpr, axis=axis):\\n\\t\\t\\t\\tin_axis, name, level, gpr = False, None, gpr, None\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(gpr)\\n\\t\\telif isinstance(gpr, Grouper) and gpr.key is not None:\\n\\t\\t\\texclusions.append(gpr.key)\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\telse:\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\tif is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\tf\"Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) \"\\n\\t\\t\\t\\t\"must be same length\"\\n\\t\\t\\t)\\n\\t\\tping = (\\n\\t\\t\\tGrouping(\\n\\t\\t\\t\\tgroup_axis,\\n\\t\\t\\t\\tgpr,\\n\\t\\t\\t\\tobj=obj,\\n\\t\\t\\t\\tname=name,\\n\\t\\t\\t\\tlevel=level,\\n\\t\\t\\t\\tsort=sort,\\n\\t\\t\\t\\tobserved=observed,\\n\\t\\t\\t\\tin_axis=in_axis,\\n\\t\\t\\t\\tdropna=dropna,\\n\\t\\t\\t)\\n\\t\\t\\tif not isinstance(gpr, Grouping)\\n\\t\\t\\telse gpr\\n\\t\\t)\\n\\t\\tgroupings.append(ping)\\n\\tif len(groupings) == 0 and len(obj):\\n\\t\\traise ValueError(\"No group keys passed!\")\\n\\telif len(groupings) == 0:\\n\\t\\tgroupings.append(Grouping(Index([], dtype=\"int\"), np.array([], dtype=np.intp)))\\n\\tgrouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)\\n\\treturn grouper, exclusions, obj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def is_in_obj(gpr) -> bool:\\n\\t\\tif not hasattr(gpr, \"name\"):\\n\\t\\t\\treturn False\\n\\t\\ttry:\\n\\t\\t\\treturn gpr is obj[gpr.name]\\n\\t\\texcept (KeyError, IndexError, ValueError):\\n\\t\\t\\treturn False\\n\\tfor i, (gpr, level) in enumerate(zip(keys, levels)):\\n\\t\\tif is_in_obj(gpr):  \\n\\t\\t\\tin_axis, name = True, gpr.name\\n\\t\\t\\texclusions.append(name)\\n\\t\\telif is_in_axis(gpr):  \\n\\t\\t\\tif gpr in obj:\\n\\t\\t\\t\\tif validate:\\n\\t\\t\\t\\t\\tobj._check_label_or_level_ambiguity(gpr, axis=axis)\\n\\t\\t\\t\\tin_axis, name, gpr = True, gpr, obj[gpr]\\n\\t\\t\\t\\texclusions.append(name)\\n\\t\\t\\telif obj._is_level_reference(gpr, axis=axis):\\n\\t\\t\\t\\tin_axis, name, level, gpr = False, None, gpr, None\\n\\t\\t\\telse:\\n\\t\\t\\t\\traise KeyError(gpr)\\n\\t\\telif isinstance(gpr, Grouper) and gpr.key is not None:\\n\\t\\t\\texclusions.append(gpr.key)\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\telse:\\n\\t\\t\\tin_axis, name = False, None\\n\\t\\tif is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\tf\"Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) \"\\n\\t\\t\\t\\t\"must be same length\"\\n\\t\\t\\t)\\n\\t\\tping = (\\n\\t\\t\\tGrouping(\\n\\t\\t\\t\\tgroup_axis,\\n\\t\\t\\t\\tgpr,\\n\\t\\t\\t\\tobj=obj,\\n\\t\\t\\t\\tname=name,\\n\\t\\t\\t\\tlevel=level,\\n\\t\\t\\t\\tsort=sort,\\n\\t\\t\\t\\tobserved=observed,\\n\\t\\t\\t\\tin_axis=in_axis,\\n\\t\\t\\t\\tdropna=dropna,\\n\\t\\t\\t)\\n\\t\\t\\tif not isinstance(gpr, Grouping)\\n\\t\\t\\telse gpr\\n\\t\\t)\\n\\t\\tgroupings.append(ping)\\n\\tif len(groupings) == 0 and len(obj):\\n\\t\\traise ValueError(\"No group keys passed!\")\\n\\telif len(groupings) == 0:\\n\\t\\tgroupings.append(Grouping(Index([], dtype=\"int\"), np.array([], dtype=np.intp)))\\n\\tgrouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)\\n\\treturn grouper, exclusions, obj"
  },
  {
    "code": "def _rotate(self, components, n_components=None, tol=1e-6):\\n        \"Rotate the factor analysis solution.\"\\n        return _ortho_rotation(components.T, method=self.rotation, tol=tol)[\\n            : self.n_components\\n        ]\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _rotate(self, components, n_components=None, tol=1e-6):\\n        \"Rotate the factor analysis solution.\"\\n        return _ortho_rotation(components.T, method=self.rotation, tol=tol)[\\n            : self.n_components\\n        ]\\n    @property"
  },
  {
    "code": "def apply(self):\\n        changed = False\\n        vserver_details = self.get_vserver()\\n        rename_vserver = False\\n        modify_protocols = False\\n        modify_aggr_list = False\\n        modify_snapshot_policy = False\\n        modify_language = False\\n        if vserver_details is not None:\\n            if self.state == 'absent':\\n                changed = True\\n            elif self.state == 'present':\\n                if self.allowed_protocols is not None:\\n                    self.allowed_protocols.sort()\\n                    vserver_details['allowed_protocols'].sort()\\n                    if self.allowed_protocols != vserver_details['allowed_protocols']:\\n                        modify_protocols = True\\n                        changed = True\\n                if self.aggr_list is not None:\\n                    self.aggr_list.sort()\\n                    vserver_details['aggr_list'].sort()\\n                    if self.aggr_list != vserver_details['aggr_list']:\\n                        modify_aggr_list = True\\n                        changed = True\\n                if self.snapshot_policy is not None:\\n                    if self.snapshot_policy != vserver_details['snapshot_policy']:\\n                        modify_snapshot_policy = True\\n                        changed = True\\n                if self.language is not None:\\n                    if self.language != vserver_details['language']:\\n                        modify_language = True\\n                        changed = True\\n                if self.root_volume is not None and self.root_volume != vserver_details['root_volume']:\\n                    self.module.fail_json(msg='Error modifying SVM %s: %s' % (self.name, 'cannot change root volume'))\\n                if self.root_volume_aggregate is not None and self.root_volume_aggregate != vserver_details['root_volume_aggregate']:\\n                    self.module.fail_json(msg='Error modifying SVM %s: %s' % (self.name, 'cannot change root volume aggregate'))\\n                if self.root_volume_security_style is not None and self.root_volume_security_style != vserver_details['root_volume_security_style']:\\n                    self.module.fail_json(msg='Error modifying SVM %s: %s' % (self.name, 'cannot change root volume security style'))\\n                if self.subtype is not None and self.subtype != vserver_details['subtype']:\\n                    self.module.fail_json(msg='Error modifying SVM %s: %s' % (self.name, 'cannot change subtype'))\\n                if self.ipspace is not None and self.ipspace != vserver_details['ipspace']:\\n                    self.module.fail_json(msg='Error modifying SVM %s: %s' % (self.name, 'cannot change ipspace'))\\n        else:\\n            if self.state == 'present':\\n                changed = True\\n        if changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if self.state == 'present':\\n                    if vserver_details is None:\\n                        if self.from_name is not None and self.get_vserver(self.from_name):\\n                            self.rename_vserver()\\n                        else:\\n                            self.create_vserver()\\n                    else:\\n                        if modify_protocols or modify_aggr_list:\\n                            self.modify_vserver(\\n                                modify_protocols, modify_aggr_list, modify_language, modify_snapshot_policy)\\n                elif self.state == 'absent':\\n                    self.delete_vserver()\\n        self.module.exit_json(changed=changed)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Bug Fix and rewrite (#49508)",
    "fixed_code": "def apply(self):\\n        ''\\n        self.asup_log_for_cserver(\"na_ontap_svm\")\\n        current = self.get_vserver()\\n        cd_action, rename = None, None\\n        if self.parameters.get('from_name'):\\n            rename = self.na_helper.is_rename_action(self.get_vserver(self.parameters['from_name']), current)\\n        else:\\n            cd_action = self.na_helper.get_cd_action(current, self.parameters)\\n        modify = self.na_helper.get_modified_attributes(current, self.parameters)\\n        for attribute in modify:\\n            if attribute in ['root_volume', 'root_volume_aggregate', 'root_volume_security_style', 'subtype', 'ipspace']:\\n                self.module.fail_json(msg='Error modifying SVM %s: can not modify %s.' % (self.parameters['name'], attribute))\\n            if attribute == 'language':\\n                if self.parameters['language'].lower() == 'c.utf-8':\\n                    self.parameters['language'] = 'c.utf_8'\\n        if self.na_helper.changed:\\n            if self.module.check_mode:\\n                pass\\n            else:\\n                if rename:\\n                    self.rename_vserver()\\n                if cd_action == 'create':\\n                    self.create_vserver()\\n                elif cd_action == 'delete':\\n                    self.delete_vserver()\\n                elif modify:\\n                    self.modify_vserver(modify)\\n        self.module.exit_json(changed=self.na_helper.changed)"
  },
  {
    "code": "def _get_combined_index(\\n    indexes: list[Index],\\n    intersect: bool = False,\\n    sort: bool = False,\\n    copy: bool = False,\\n) -> Index:\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = union_indexes(indexes, sort=False)\\n        index = ensure_index(index)\\n    if sort:\\n        index = safe_sort_index(index)\\n    if copy:\\n        index = index.copy()\\n    return index",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_combined_index(\\n    indexes: list[Index],\\n    intersect: bool = False,\\n    sort: bool = False,\\n    copy: bool = False,\\n) -> Index:\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = union_indexes(indexes, sort=False)\\n        index = ensure_index(index)\\n    if sort:\\n        index = safe_sort_index(index)\\n    if copy:\\n        index = index.copy()\\n    return index"
  },
  {
    "code": "def read_feather(path):\\n    feather = _try_import()\\n    path = _stringify_path(path)\\n    return feather.read_dataframe(path)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add ntrheads option to feather-format IO (#16476)\\n\\nENH: add nthreads option to feather-format IO",
    "fixed_code": "def read_feather(path, nthreads=1):\\n    feather = _try_import()\\n    path = _stringify_path(path)\\n    if feather.__version__ < LooseVersion('0.4.0'):\\n        return feather.read_dataframe(path)\\n    return feather.read_dataframe(path, nthreads=nthreads)"
  },
  {
    "code": "def setcontext(context, _local=local):\\n    if context in (DefaultContext, BasicContext, ExtendedContext):\\n        context = context.copy()\\n        context.clear_flags()\\n    _local.__decimal_context__ = context\\ndel threading, local",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setcontext(context, _local=local):\\n    if context in (DefaultContext, BasicContext, ExtendedContext):\\n        context = context.copy()\\n        context.clear_flags()\\n    _local.__decimal_context__ = context\\ndel threading, local"
  },
  {
    "code": "def _queue_dagruns(self, dataset: DatasetModel, session: Session) -> None:\\n        if session.bind.dialect.name == \"postgresql\":\\n            return self._postgres_queue_dagruns(dataset, session)\\n        return self._slow_path_queue_dagruns(dataset, session)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _queue_dagruns(self, dataset: DatasetModel, session: Session) -> None:\\n        if session.bind.dialect.name == \"postgresql\":\\n            return self._postgres_queue_dagruns(dataset, session)\\n        return self._slow_path_queue_dagruns(dataset, session)"
  },
  {
    "code": "def generate_tokens(readline):\\n    lnum = parenlev = continued = 0\\n    contstr, needcont = '', 0\\n    contline = None\\n    indents = [0]\\n    stashed = None\\n    async_def = False\\n    async_def_indent = 0\\n    async_def_nl = False\\n    while 1:                                   \\n        try:\\n            line = readline()\\n        except StopIteration:\\n            line = ''\\n        lnum = lnum + 1\\n        pos, max = 0, len(line)\\n        if contstr:                            \\n            if not line:\\n                raise TokenError(\"EOF in multi-line string\", strstart)\\n            endmatch = endprog.match(line)\\n            if endmatch:\\n                pos = end = endmatch.end(0)\\n                yield (STRING, contstr + line[:end],\\n                       strstart, (lnum, end), contline + line)\\n                contstr, needcont = '', 0\\n                contline = None\\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\\n                yield (ERRORTOKEN, contstr + line,\\n                           strstart, (lnum, len(line)), contline)\\n                contstr = ''\\n                contline = None\\n                continue\\n            else:\\n                contstr = contstr + line\\n                contline = contline + line\\n                continue\\n        elif parenlev == 0 and not continued:  \\n            if not line: break\\n            column = 0\\n            while pos < max:                   \\n                if line[pos] == ' ': column = column + 1\\n                elif line[pos] == '\\t': column = (column//tabsize + 1)*tabsize\\n                elif line[pos] == '\\f': column = 0\\n                else: break\\n                pos = pos + 1\\n            if pos == max: break\\n            if stashed:\\n                yield stashed\\n                stashed = None\\n            if line[pos] in '\\n':           \\n                if line[pos] == '\\n                    comment_token = line[pos:].rstrip('\\r\\n')\\n                    nl_pos = pos + len(comment_token)\\n                    yield (COMMENT, comment_token,\\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\\n                    yield (NL, line[nl_pos:],\\n                           (lnum, nl_pos), (lnum, len(line)), line)\\n                else:\\n                    yield ((NL, COMMENT)[line[pos] == '\\n                           (lnum, pos), (lnum, len(line)), line)\\n                continue\\n            if column > indents[-1]:           \\n                indents.append(column)\\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\\n            while column < indents[-1]:\\n                if column not in indents:\\n                    raise IndentationError(\\n                        \"unindent does not match any outer indentation level\",\\n                        (\"<tokenize>\", lnum, pos, line))\\n                indents = indents[:-1]\\n                if async_def and async_def_indent >= indents[-1]:\\n                    async_def = False\\n                    async_def_nl = False\\n                    async_def_indent = 0\\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\\n                async_def = False\\n                async_def_nl = False\\n                async_def_indent = 0\\n        else:                                  \\n            if not line:\\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\\n            continued = 0\\n        while pos < max:\\n            pseudomatch = pseudoprog.match(line, pos)\\n            if pseudomatch:                                \\n                start, end = pseudomatch.span(1)\\n                spos, epos, pos = (lnum, start), (lnum, end), end\\n                token, initial = line[start:end], line[start]\\n                if initial in string.digits or \\\\n                   (initial == '.' and token != '.'):      \\n                    yield (NUMBER, token, spos, epos, line)\\n                elif initial in '\\r\\n':\\n                    newline = NEWLINE\\n                    if parenlev > 0:\\n                        newline = NL\\n                    elif async_def:\\n                        async_def_nl = True\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (newline, token, spos, epos, line)\\n                elif initial == '\\n                    assert not token.endswith(\"\\n\")\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (COMMENT, token, spos, epos, line)\\n                elif token in triple_quoted:\\n                    endprog = endprogs[token]\\n                    endmatch = endprog.match(line, pos)\\n                    if endmatch:                           \\n                        pos = endmatch.end(0)\\n                        token = line[start:pos]\\n                        if stashed:\\n                            yield stashed\\n                            stashed = None\\n                        yield (STRING, token, spos, (lnum, pos), line)\\n                    else:\\n                        strstart = (lnum, start)           \\n                        contstr = line[start:]\\n                        contline = line\\n                        break\\n                elif initial in single_quoted or \\\\n                    token[:2] in single_quoted or \\\\n                    token[:3] in single_quoted:\\n                    if token[-1] == '\\n':                  \\n                        strstart = (lnum, start)\\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\\n                                   endprogs[token[2]])\\n                        contstr, needcont = line[start:], 1\\n                        contline = line\\n                        break\\n                    else:                                  \\n                        if stashed:\\n                            yield stashed\\n                            stashed = None\\n                        yield (STRING, token, spos, epos, line)\\n                elif initial.isidentifier():               \\n                    if token in ('async', 'await'):\\n                        if async_def:\\n                            yield (ASYNC if token == 'async' else AWAIT,\\n                                   token, spos, epos, line)\\n                            continue\\n                    tok = (NAME, token, spos, epos, line)\\n                    if token == 'async' and not stashed:\\n                        stashed = tok\\n                        continue\\n                    if token in ('def', 'for'):\\n                        if (stashed\\n                                and stashed[0] == NAME\\n                                and stashed[1] == 'async'):\\n                            if token == 'def':\\n                                async_def = True\\n                                async_def_indent = indents[-1]\\n                            yield (ASYNC, stashed[1],\\n                                   stashed[2], stashed[3],\\n                                   stashed[4])\\n                            stashed = None\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield tok\\n                elif initial == '\\\\':                      \\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (NL, token, spos, (lnum, pos), line)\\n                    continued = 1\\n                else:\\n                    if initial in '([{': parenlev = parenlev + 1\\n                    elif initial in ')]}': parenlev = parenlev - 1\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (OP, token, spos, epos, line)\\n            else:\\n                yield (ERRORTOKEN, line[pos],\\n                           (lnum, pos), (lnum, pos+1), line)\\n                pos = pos + 1\\n    if stashed:\\n        yield stashed\\n        stashed = None\\n    for indent in indents[1:]:                 \\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\\nif __name__ == '__main__':                     \\n    import sys\\n    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)\\n    else: tokenize(sys.stdin.readline)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def generate_tokens(readline):\\n    lnum = parenlev = continued = 0\\n    contstr, needcont = '', 0\\n    contline = None\\n    indents = [0]\\n    stashed = None\\n    async_def = False\\n    async_def_indent = 0\\n    async_def_nl = False\\n    while 1:                                   \\n        try:\\n            line = readline()\\n        except StopIteration:\\n            line = ''\\n        lnum = lnum + 1\\n        pos, max = 0, len(line)\\n        if contstr:                            \\n            if not line:\\n                raise TokenError(\"EOF in multi-line string\", strstart)\\n            endmatch = endprog.match(line)\\n            if endmatch:\\n                pos = end = endmatch.end(0)\\n                yield (STRING, contstr + line[:end],\\n                       strstart, (lnum, end), contline + line)\\n                contstr, needcont = '', 0\\n                contline = None\\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\\n                yield (ERRORTOKEN, contstr + line,\\n                           strstart, (lnum, len(line)), contline)\\n                contstr = ''\\n                contline = None\\n                continue\\n            else:\\n                contstr = contstr + line\\n                contline = contline + line\\n                continue\\n        elif parenlev == 0 and not continued:  \\n            if not line: break\\n            column = 0\\n            while pos < max:                   \\n                if line[pos] == ' ': column = column + 1\\n                elif line[pos] == '\\t': column = (column//tabsize + 1)*tabsize\\n                elif line[pos] == '\\f': column = 0\\n                else: break\\n                pos = pos + 1\\n            if pos == max: break\\n            if stashed:\\n                yield stashed\\n                stashed = None\\n            if line[pos] in '\\n':           \\n                if line[pos] == '\\n                    comment_token = line[pos:].rstrip('\\r\\n')\\n                    nl_pos = pos + len(comment_token)\\n                    yield (COMMENT, comment_token,\\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\\n                    yield (NL, line[nl_pos:],\\n                           (lnum, nl_pos), (lnum, len(line)), line)\\n                else:\\n                    yield ((NL, COMMENT)[line[pos] == '\\n                           (lnum, pos), (lnum, len(line)), line)\\n                continue\\n            if column > indents[-1]:           \\n                indents.append(column)\\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\\n            while column < indents[-1]:\\n                if column not in indents:\\n                    raise IndentationError(\\n                        \"unindent does not match any outer indentation level\",\\n                        (\"<tokenize>\", lnum, pos, line))\\n                indents = indents[:-1]\\n                if async_def and async_def_indent >= indents[-1]:\\n                    async_def = False\\n                    async_def_nl = False\\n                    async_def_indent = 0\\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\\n                async_def = False\\n                async_def_nl = False\\n                async_def_indent = 0\\n        else:                                  \\n            if not line:\\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\\n            continued = 0\\n        while pos < max:\\n            pseudomatch = pseudoprog.match(line, pos)\\n            if pseudomatch:                                \\n                start, end = pseudomatch.span(1)\\n                spos, epos, pos = (lnum, start), (lnum, end), end\\n                token, initial = line[start:end], line[start]\\n                if initial in string.digits or \\\\n                   (initial == '.' and token != '.'):      \\n                    yield (NUMBER, token, spos, epos, line)\\n                elif initial in '\\r\\n':\\n                    newline = NEWLINE\\n                    if parenlev > 0:\\n                        newline = NL\\n                    elif async_def:\\n                        async_def_nl = True\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (newline, token, spos, epos, line)\\n                elif initial == '\\n                    assert not token.endswith(\"\\n\")\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (COMMENT, token, spos, epos, line)\\n                elif token in triple_quoted:\\n                    endprog = endprogs[token]\\n                    endmatch = endprog.match(line, pos)\\n                    if endmatch:                           \\n                        pos = endmatch.end(0)\\n                        token = line[start:pos]\\n                        if stashed:\\n                            yield stashed\\n                            stashed = None\\n                        yield (STRING, token, spos, (lnum, pos), line)\\n                    else:\\n                        strstart = (lnum, start)           \\n                        contstr = line[start:]\\n                        contline = line\\n                        break\\n                elif initial in single_quoted or \\\\n                    token[:2] in single_quoted or \\\\n                    token[:3] in single_quoted:\\n                    if token[-1] == '\\n':                  \\n                        strstart = (lnum, start)\\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\\n                                   endprogs[token[2]])\\n                        contstr, needcont = line[start:], 1\\n                        contline = line\\n                        break\\n                    else:                                  \\n                        if stashed:\\n                            yield stashed\\n                            stashed = None\\n                        yield (STRING, token, spos, epos, line)\\n                elif initial.isidentifier():               \\n                    if token in ('async', 'await'):\\n                        if async_def:\\n                            yield (ASYNC if token == 'async' else AWAIT,\\n                                   token, spos, epos, line)\\n                            continue\\n                    tok = (NAME, token, spos, epos, line)\\n                    if token == 'async' and not stashed:\\n                        stashed = tok\\n                        continue\\n                    if token in ('def', 'for'):\\n                        if (stashed\\n                                and stashed[0] == NAME\\n                                and stashed[1] == 'async'):\\n                            if token == 'def':\\n                                async_def = True\\n                                async_def_indent = indents[-1]\\n                            yield (ASYNC, stashed[1],\\n                                   stashed[2], stashed[3],\\n                                   stashed[4])\\n                            stashed = None\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield tok\\n                elif initial == '\\\\':                      \\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (NL, token, spos, (lnum, pos), line)\\n                    continued = 1\\n                else:\\n                    if initial in '([{': parenlev = parenlev + 1\\n                    elif initial in ')]}': parenlev = parenlev - 1\\n                    if stashed:\\n                        yield stashed\\n                        stashed = None\\n                    yield (OP, token, spos, epos, line)\\n            else:\\n                yield (ERRORTOKEN, line[pos],\\n                           (lnum, pos), (lnum, pos+1), line)\\n                pos = pos + 1\\n    if stashed:\\n        yield stashed\\n        stashed = None\\n    for indent in indents[1:]:                 \\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\\nif __name__ == '__main__':                     \\n    import sys\\n    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)\\n    else: tokenize(sys.stdin.readline)"
  },
  {
    "code": "def sequence_to_dt64ns(\\n    data,\\n    dtype=None,\\n    copy=False,\\n    tz=None,\\n    dayfirst=False,\\n    yearfirst=False,\\n    ambiguous=\"raise\",\\n):\\n    inferred_freq = None\\n    dtype = _validate_dt64_dtype(dtype)\\n    tz = timezones.maybe_get_tz(tz)\\n    if not hasattr(data, \"dtype\"):\\n        if np.ndim(data) == 0:\\n            data = list(data)\\n        data = np.asarray(data)\\n        copy = False\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    if isinstance(data, ABCPandasArray):\\n        data = data.to_numpy()\\n    if hasattr(data, \"freq\"):\\n        inferred_freq = data.freq\\n    tz = validate_tz_from_dtype(dtype, tz)\\n    if isinstance(data, ABCIndexClass):\\n        if data.nlevels > 1:\\n            raise TypeError(\"Cannot create a DatetimeArray from a MultiIndex.\")\\n        data = data._data\\n    data, copy = maybe_convert_dtype(data, copy)\\n    data_dtype = getattr(data, \"dtype\", None)\\n    if (\\n        is_object_dtype(data_dtype)\\n        or is_string_dtype(data_dtype)\\n        or is_sparse(data_dtype)\\n    ):\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\\n            data = data.astype(np.int64)\\n        else:\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data, dayfirst=dayfirst, yearfirst=yearfirst\\n            )\\n            if tz and inferred_tz:\\n                data = tzconversion.tz_convert_from_utc(data.view(\"i8\"), tz)\\n                data = data.view(DT64NS_DTYPE)\\n            elif inferred_tz:\\n                tz = inferred_tz\\n        data_dtype = data.dtype\\n    if is_datetime64tz_dtype(data_dtype):\\n        tz = _maybe_infer_tz(tz, data.tz)\\n        result = data._data\\n    elif is_datetime64_dtype(data_dtype):\\n        data = getattr(data, \"_data\", data)\\n        if data.dtype != DT64NS_DTYPE:\\n            data = conversion.ensure_datetime64ns(data)\\n        if tz is not None:\\n            tz = timezones.maybe_get_tz(tz)\\n            data = tzconversion.tz_localize_to_utc(\\n                data.view(\"i8\"), tz, ambiguous=ambiguous\\n            )\\n            data = data.view(DT64NS_DTYPE)\\n        assert data.dtype == DT64NS_DTYPE, data.dtype\\n        result = data\\n    else:\\n        if tz:\\n            tz = timezones.maybe_get_tz(tz)\\n        if data.dtype != INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        result = data.view(DT64NS_DTYPE)\\n    if copy:\\n        result = result.copy()\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \"M8[ns]\", result.dtype\\n    validate_tz_from_dtype(dtype, tz)\\n    return result, tz, inferred_freq",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sequence_to_dt64ns(\\n    data,\\n    dtype=None,\\n    copy=False,\\n    tz=None,\\n    dayfirst=False,\\n    yearfirst=False,\\n    ambiguous=\"raise\",\\n):\\n    inferred_freq = None\\n    dtype = _validate_dt64_dtype(dtype)\\n    tz = timezones.maybe_get_tz(tz)\\n    if not hasattr(data, \"dtype\"):\\n        if np.ndim(data) == 0:\\n            data = list(data)\\n        data = np.asarray(data)\\n        copy = False\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    if isinstance(data, ABCPandasArray):\\n        data = data.to_numpy()\\n    if hasattr(data, \"freq\"):\\n        inferred_freq = data.freq\\n    tz = validate_tz_from_dtype(dtype, tz)\\n    if isinstance(data, ABCIndexClass):\\n        if data.nlevels > 1:\\n            raise TypeError(\"Cannot create a DatetimeArray from a MultiIndex.\")\\n        data = data._data\\n    data, copy = maybe_convert_dtype(data, copy)\\n    data_dtype = getattr(data, \"dtype\", None)\\n    if (\\n        is_object_dtype(data_dtype)\\n        or is_string_dtype(data_dtype)\\n        or is_sparse(data_dtype)\\n    ):\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\\n            data = data.astype(np.int64)\\n        else:\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data, dayfirst=dayfirst, yearfirst=yearfirst\\n            )\\n            if tz and inferred_tz:\\n                data = tzconversion.tz_convert_from_utc(data.view(\"i8\"), tz)\\n                data = data.view(DT64NS_DTYPE)\\n            elif inferred_tz:\\n                tz = inferred_tz\\n        data_dtype = data.dtype\\n    if is_datetime64tz_dtype(data_dtype):\\n        tz = _maybe_infer_tz(tz, data.tz)\\n        result = data._data\\n    elif is_datetime64_dtype(data_dtype):\\n        data = getattr(data, \"_data\", data)\\n        if data.dtype != DT64NS_DTYPE:\\n            data = conversion.ensure_datetime64ns(data)\\n        if tz is not None:\\n            tz = timezones.maybe_get_tz(tz)\\n            data = tzconversion.tz_localize_to_utc(\\n                data.view(\"i8\"), tz, ambiguous=ambiguous\\n            )\\n            data = data.view(DT64NS_DTYPE)\\n        assert data.dtype == DT64NS_DTYPE, data.dtype\\n        result = data\\n    else:\\n        if tz:\\n            tz = timezones.maybe_get_tz(tz)\\n        if data.dtype != INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        result = data.view(DT64NS_DTYPE)\\n    if copy:\\n        result = result.copy()\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \"M8[ns]\", result.dtype\\n    validate_tz_from_dtype(dtype, tz)\\n    return result, tz, inferred_freq"
  },
  {
    "code": "def _GetGrad(grads, t, unconnected_gradients):\\n  op = t.op\\n  op_grads = grads.get(op)\\n  if not op_grads:\\n\\tif unconnected_gradients == UnconnectedGradients.ZERO:\\n\\t  t_dtype = default_gradient.get_zeros_dtype(t)\\n\\t  if t.dtype == dtypes.resource:\\n\\t\\treturn array_ops.zeros(\\n\\t\\t\\tresource_variable_ops.variable_shape(t), dtype=t_dtype)\\n\\t  else:\\n\\t\\treturn array_ops.zeros_like(t, dtype=t_dtype)\\n\\telif unconnected_gradients == UnconnectedGradients.NONE:\\n\\t  return None\\n\\telse:\\n\\t  raise ValueError(\\n\\t\\t  \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\\n  t_grad = op_grads[t.value_index]\\n  assert not isinstance(\\n\\t  t_grad, list), (\"gradients list should have been aggregated by now.\")\\n  return t_grad",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _GetGrad(grads, t, unconnected_gradients):\\n  op = t.op\\n  op_grads = grads.get(op)\\n  if not op_grads:\\n\\tif unconnected_gradients == UnconnectedGradients.ZERO:\\n\\t  t_dtype = default_gradient.get_zeros_dtype(t)\\n\\t  if t.dtype == dtypes.resource:\\n\\t\\treturn array_ops.zeros(\\n\\t\\t\\tresource_variable_ops.variable_shape(t), dtype=t_dtype)\\n\\t  else:\\n\\t\\treturn array_ops.zeros_like(t, dtype=t_dtype)\\n\\telif unconnected_gradients == UnconnectedGradients.NONE:\\n\\t  return None\\n\\telse:\\n\\t  raise ValueError(\\n\\t\\t  \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\\n  t_grad = op_grads[t.value_index]\\n  assert not isinstance(\\n\\t  t_grad, list), (\"gradients list should have been aggregated by now.\")\\n  return t_grad"
  },
  {
    "code": "def adapt_decimalfield_value(self, value, max_digits, decimal_places):\\n\\t\\treturn utils.format_number(value, max_digits, decimal_places)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #26219 -- Fixed crash when filtering by Decimal in RawQuery.",
    "fixed_code": "def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):\\n\\t\\treturn utils.format_number(value, max_digits, decimal_places)"
  },
  {
    "code": "def fetchone(self):\\n        row = Database.Cursor.fetchone(self)\\n        if row is None:\\n            return row\\n        return self._rowfactory(row)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #5543: callproc() and friends now work with Oracle and our FormatStylePlaceholderCursor.",
    "fixed_code": "def fetchone(self):\\n        row = self.cursor.fetchone()\\n        if row is None:\\n            return row\\n        return self._rowfactory(row)"
  },
  {
    "code": "def __init__(self, timefunc=time.time, delayfunc=time.sleep):\\n        self._queue = []\\n        self.timefunc = timefunc\\n        self.delayfunc = delayfunc",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, timefunc=time.time, delayfunc=time.sleep):\\n        self._queue = []\\n        self.timefunc = timefunc\\n        self.delayfunc = delayfunc"
  },
  {
    "code": "def fit(self, X, y):\\n\\t\\tif self.base_estimator is not None:\\n\\t\\t\\tbase_estimator = clone(self.base_estimator)\\n\\t\\telse:\\n\\t\\t\\tbase_estimator = LinearRegression()\\n\\t\\tif self.min_samples is None:\\n\\t\\t\\tmin_samples = X.shape[1] + 1\\n\\t\\telif 0 < self.min_samples < 1:\\n\\t\\t\\tmin_samples = np.ceil(self.min_samples * X.shape[0])\\n\\t\\telif self.min_samples >= 1:\\n\\t\\t\\tif self.min_samples % 1 != 0:\\n\\t\\t\\t\\traise ValueError(\"Absolute number of samples must be an \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"integer value.\")\\n\\t\\t\\tmin_samples = self.min_samples\\n\\t\\telse:\\n\\t\\t\\traise ValueError(\"Value for `min_samples` must be scalar and \"\\n\\t\\t\\t\\t\\t\\t\\t \"positive.\")\\n\\t\\tif min_samples > X.shape[0]:\\n\\t\\t\\traise ValueError(\"`min_samples` may not be larger than number \"\\n\\t\\t\\t\\t\\t\\t\\t \"of samples ``X.shape[0]``.\")\\n\\t\\tif self.stop_probability < 0 or self.stop_probability > 1:\\n\\t\\t\\traise ValueError(\"`stop_probability` must be in range [0, 1].\")\\n\\t\\tif self.residual_threshold is None:\\n\\t\\t\\tresidual_threshold = np.median(np.abs(y - np.median(y)))\\n\\t\\telse:\\n\\t\\t\\tresidual_threshold = self.residual_threshold\\n\\t\\tif self.residual_metric is None:\\n\\t\\t\\tresidual_metric = lambda dy: np.sum(np.abs(dy), axis=1)\\n\\t\\telse:\\n\\t\\t\\tresidual_metric = self.residual_metric\\n\\t\\trandom_state = check_random_state(self.random_state)\\n\\t\\ttry:  \\n\\t\\t\\tbase_estimator.set_params(random_state=random_state)\\n\\t\\texcept ValueError:\\n\\t\\t\\tpass\\n\\t\\tn_inliers_best = 0\\n\\t\\tscore_best = np.inf\\n\\t\\tinlier_mask_best = None\\n\\t\\tX_inlier_best = None\\n\\t\\ty_inlier_best = None\\n\\t\\tn_samples = X.shape[0]\\n\\t\\tsample_idxs = np.arange(n_samples)\\n\\t\\tX = atleast2d_or_csr(X)\\n\\t\\ty = np.asarray(y)\\n\\t\\tif y.ndim == 1:\\n\\t\\t\\ty = y[:, None]\\n\\t\\tfor self.n_trials_ in range(1, self.max_trials + 1):\\n\\t\\t\\tsubset_idxs = sample_without_replacement(n_samples, min_samples,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t random_state=random_state)\\n\\t\\t\\tX_subset = X[subset_idxs]\\n\\t\\t\\ty_subset = y[subset_idxs]\\n\\t\\t\\tif (self.is_data_valid is not None\\n\\t\\t\\t\\t\\tand not self.is_data_valid(X_subset, y_subset)):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tbase_estimator.fit(X_subset, y_subset)\\n\\t\\t\\tif (self.is_model_valid is not None and not\\n\\t\\t\\t\\t\\tself.is_model_valid(base_estimator, X_subset, y_subset)):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\ty_pred = base_estimator.predict(X)\\n\\t\\t\\tif y_pred.ndim == 1:\\n\\t\\t\\t\\ty_pred = y_pred[:, None]\\n\\t\\t\\tresiduals_subset = residual_metric(y_pred - y)\\n\\t\\t\\tinlier_mask_subset = residuals_subset < residual_threshold\\n\\t\\t\\tn_inliers_subset = np.sum(inlier_mask_subset)\\n\\t\\t\\tif n_inliers_subset < n_inliers_best:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tinlier_idxs_subset = sample_idxs[inlier_mask_subset]\\n\\t\\t\\tX_inlier_subset = X[inlier_idxs_subset]\\n\\t\\t\\ty_inlier_subset = y[inlier_idxs_subset]\\n\\t\\t\\tscore_subset = base_estimator.score(X_inlier_subset,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ty_inlier_subset)\\n\\t\\t\\tif (n_inliers_subset == n_inliers_best\\n\\t\\t\\t\\t\\tand score_subset < score_best):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tn_inliers_best = n_inliers_subset\\n\\t\\t\\tscore_best = score_subset\\n\\t\\t\\tinlier_mask_best = inlier_mask_subset\\n\\t\\t\\tX_inlier_best = X_inlier_subset\\n\\t\\t\\ty_inlier_best = y_inlier_subset\\n\\t\\t\\tif (n_inliers_best >= self.stop_n_inliers\\n\\t\\t\\t\\t\\tor score_best >= self.stop_score\\n\\t\\t\\t\\t\\tor self.n_trials_\\n\\t\\t\\t\\t\\t   >= _dynamic_max_trials(n_inliers_best, n_samples,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  min_samples,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  self.stop_probability)):\\n\\t\\t\\t\\tbreak\\n\\t\\tif inlier_mask_best is None:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"RANSAC could not find valid consensus set, because\"\\n\\t\\t\\t\\t\" either the `residual_threshold` rejected all the samples or\"\\n\\t\\t\\t\\t\" `is_data_valid` and `is_model_valid` returned False for all\"\\n\\t\\t\\t\\t\" `max_trials` randomly \"\"chosen sub-samples. Consider \"\\n\\t\\t\\t\\t\"relaxing the \"\"constraints.\")\\n\\t\\tbase_estimator.fit(X_inlier_best, y_inlier_best)\\n\\t\\tself.estimator_ = base_estimator\\n\\t\\tself.inlier_mask_ = inlier_mask_best\\n\\t\\treturn self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX #3372: unstable input check test for RANSACRegressor",
    "fixed_code": "def fit(self, X, y):\\n\\t\\tif self.base_estimator is not None:\\n\\t\\t\\tbase_estimator = clone(self.base_estimator)\\n\\t\\telse:\\n\\t\\t\\tbase_estimator = LinearRegression()\\n\\t\\tif self.min_samples is None:\\n\\t\\t\\tmin_samples = X.shape[1] + 1\\n\\t\\telif 0 < self.min_samples < 1:\\n\\t\\t\\tmin_samples = np.ceil(self.min_samples * X.shape[0])\\n\\t\\telif self.min_samples >= 1:\\n\\t\\t\\tif self.min_samples % 1 != 0:\\n\\t\\t\\t\\traise ValueError(\"Absolute number of samples must be an \"\\n\\t\\t\\t\\t\\t\\t\\t\\t \"integer value.\")\\n\\t\\t\\tmin_samples = self.min_samples\\n\\t\\telse:\\n\\t\\t\\traise ValueError(\"Value for `min_samples` must be scalar and \"\\n\\t\\t\\t\\t\\t\\t\\t \"positive.\")\\n\\t\\tif min_samples > X.shape[0]:\\n\\t\\t\\traise ValueError(\"`min_samples` may not be larger than number \"\\n\\t\\t\\t\\t\\t\\t\\t \"of samples ``X.shape[0]``.\")\\n\\t\\tif self.stop_probability < 0 or self.stop_probability > 1:\\n\\t\\t\\traise ValueError(\"`stop_probability` must be in range [0, 1].\")\\n\\t\\tif self.residual_threshold is None:\\n\\t\\t\\tresidual_threshold = np.median(np.abs(y - np.median(y)))\\n\\t\\telse:\\n\\t\\t\\tresidual_threshold = self.residual_threshold\\n\\t\\tif self.residual_metric is None:\\n\\t\\t\\tresidual_metric = lambda dy: np.sum(np.abs(dy), axis=1)\\n\\t\\telse:\\n\\t\\t\\tresidual_metric = self.residual_metric\\n\\t\\trandom_state = check_random_state(self.random_state)\\n\\t\\ttry:  \\n\\t\\t\\tbase_estimator.set_params(random_state=random_state)\\n\\t\\texcept ValueError:\\n\\t\\t\\tpass\\n\\t\\tn_inliers_best = 0\\n\\t\\tscore_best = np.inf\\n\\t\\tinlier_mask_best = None\\n\\t\\tX_inlier_best = None\\n\\t\\ty_inlier_best = None\\n\\t\\tn_samples = X.shape[0]\\n\\t\\tsample_idxs = np.arange(n_samples)\\n\\t\\tX = atleast2d_or_csr(X)\\n\\t\\ty = np.asarray(y)\\n\\t\\tif y.ndim == 1:\\n\\t\\t\\ty = y[:, None]\\n\\t\\tn_samples, _ = X.shape\\n\\t\\tif n_samples != y.shape[0]:\\n\\t\\t\\traise ValueError(\"Number of samples of X (%d) and y (%d) \"\\n\\t\\t\\t\\t\\t\\t\\t \"do not match.\" % (n_samples, y.shape[0]))\\n\\t\\tfor self.n_trials_ in range(1, self.max_trials + 1):\\n\\t\\t\\tsubset_idxs = sample_without_replacement(n_samples, min_samples,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t random_state=random_state)\\n\\t\\t\\tX_subset = X[subset_idxs]\\n\\t\\t\\ty_subset = y[subset_idxs]\\n\\t\\t\\tif (self.is_data_valid is not None\\n\\t\\t\\t\\t\\tand not self.is_data_valid(X_subset, y_subset)):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tbase_estimator.fit(X_subset, y_subset)\\n\\t\\t\\tif (self.is_model_valid is not None and not\\n\\t\\t\\t\\t\\tself.is_model_valid(base_estimator, X_subset, y_subset)):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\ty_pred = base_estimator.predict(X)\\n\\t\\t\\tif y_pred.ndim == 1:\\n\\t\\t\\t\\ty_pred = y_pred[:, None]\\n\\t\\t\\tresiduals_subset = residual_metric(y_pred - y)\\n\\t\\t\\tinlier_mask_subset = residuals_subset < residual_threshold\\n\\t\\t\\tn_inliers_subset = np.sum(inlier_mask_subset)\\n\\t\\t\\tif n_inliers_subset < n_inliers_best:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tinlier_idxs_subset = sample_idxs[inlier_mask_subset]\\n\\t\\t\\tX_inlier_subset = X[inlier_idxs_subset]\\n\\t\\t\\ty_inlier_subset = y[inlier_idxs_subset]\\n\\t\\t\\tscore_subset = base_estimator.score(X_inlier_subset,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ty_inlier_subset)\\n\\t\\t\\tif (n_inliers_subset == n_inliers_best\\n\\t\\t\\t\\t\\tand score_subset < score_best):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tn_inliers_best = n_inliers_subset\\n\\t\\t\\tscore_best = score_subset\\n\\t\\t\\tinlier_mask_best = inlier_mask_subset\\n\\t\\t\\tX_inlier_best = X_inlier_subset\\n\\t\\t\\ty_inlier_best = y_inlier_subset\\n\\t\\t\\tif (n_inliers_best >= self.stop_n_inliers\\n\\t\\t\\t\\t\\tor score_best >= self.stop_score\\n\\t\\t\\t\\t\\tor self.n_trials_\\n\\t\\t\\t\\t\\t   >= _dynamic_max_trials(n_inliers_best, n_samples,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  min_samples,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  self.stop_probability)):\\n\\t\\t\\t\\tbreak\\n\\t\\tif inlier_mask_best is None:\\n\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\"RANSAC could not find valid consensus set, because\"\\n\\t\\t\\t\\t\" either the `residual_threshold` rejected all the samples or\"\\n\\t\\t\\t\\t\" `is_data_valid` and `is_model_valid` returned False for all\"\\n\\t\\t\\t\\t\" `max_trials` randomly \"\"chosen sub-samples. Consider \"\\n\\t\\t\\t\\t\"relaxing the \"\"constraints.\")\\n\\t\\tbase_estimator.fit(X_inlier_best, y_inlier_best)\\n\\t\\tself.estimator_ = base_estimator\\n\\t\\tself.inlier_mask_ = inlier_mask_best\\n\\t\\treturn self"
  },
  {
    "code": "def model_to_estimator(keras_model=None,\\n\\t\\t\\t\\t\\t   keras_model_path=None,\\n\\t\\t\\t\\t\\t   custom_objects=None,\\n\\t\\t\\t\\t\\t   model_dir=None,\\n\\t\\t\\t\\t\\t   config=None):\\n  if (not keras_model) and (not keras_model_path):\\n\\traise ValueError(\\n\\t\\t'Either keras_model or keras_model_path needs to be provided.')\\n  if keras_model and keras_model_path:\\n\\traise ValueError(\\n\\t\\t'Please specity either keras_model or keras_model_path but not both.')\\n  if not keras_model:\\n\\tif keras_model_path.startswith(\\n\\t\\t'gs://') or 'storage.googleapis.com' in keras_model_path:\\n\\t  raise ValueError(\\n\\t\\t  '%s is not a local path. Please copy the model locally first.' %\\n\\t\\t  keras_model_path)\\n\\tlogging.info('Loading models from %s', keras_model_path)\\n\\tkeras_model = models.load_model(keras_model_path)\\n  else:\\n\\tlogging.info('Using the Keras model from memory.')\\n\\tkeras_model = keras_model\\n  if not hasattr(keras_model, 'optimizer'):\\n\\traise ValueError(\\n\\t\\t'Given keras model has not been compiled yet. Please compile first '\\n\\t\\t'before creating the estimator.')\\n  keras_model_fn = _create_keras_model_fn(keras_model, custom_objects)\\n  est = estimator_lib.Estimator(\\n\\t  keras_model_fn, model_dir=model_dir, config=config)\\n  with session.Session(config=est._session_config) as sess:\\n\\tK.set_session(sess)\\n  keras_weights = keras_model.get_weights()\\n  _save_first_checkpoint(keras_model, est, custom_objects, keras_weights)\\n  return est",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def model_to_estimator(keras_model=None,\\n\\t\\t\\t\\t\\t   keras_model_path=None,\\n\\t\\t\\t\\t\\t   custom_objects=None,\\n\\t\\t\\t\\t\\t   model_dir=None,\\n\\t\\t\\t\\t\\t   config=None):\\n  if (not keras_model) and (not keras_model_path):\\n\\traise ValueError(\\n\\t\\t'Either keras_model or keras_model_path needs to be provided.')\\n  if keras_model and keras_model_path:\\n\\traise ValueError(\\n\\t\\t'Please specity either keras_model or keras_model_path but not both.')\\n  if not keras_model:\\n\\tif keras_model_path.startswith(\\n\\t\\t'gs://') or 'storage.googleapis.com' in keras_model_path:\\n\\t  raise ValueError(\\n\\t\\t  '%s is not a local path. Please copy the model locally first.' %\\n\\t\\t  keras_model_path)\\n\\tlogging.info('Loading models from %s', keras_model_path)\\n\\tkeras_model = models.load_model(keras_model_path)\\n  else:\\n\\tlogging.info('Using the Keras model from memory.')\\n\\tkeras_model = keras_model\\n  if not hasattr(keras_model, 'optimizer'):\\n\\traise ValueError(\\n\\t\\t'Given keras model has not been compiled yet. Please compile first '\\n\\t\\t'before creating the estimator.')\\n  keras_model_fn = _create_keras_model_fn(keras_model, custom_objects)\\n  est = estimator_lib.Estimator(\\n\\t  keras_model_fn, model_dir=model_dir, config=config)\\n  with session.Session(config=est._session_config) as sess:\\n\\tK.set_session(sess)\\n  keras_weights = keras_model.get_weights()\\n  _save_first_checkpoint(keras_model, est, custom_objects, keras_weights)\\n  return est"
  },
  {
    "code": "def normalize(self, context=None):\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n        dup = self._fix(context)\\n        if dup._isinfinity():\\n            return dup\\n        if not dup:\\n            return Decimal( (dup._sign, (0,), 0) )\\n        end = len(dup._int)\\n        exp = dup._exp\\n        while dup._int[end-1] == 0:\\n            exp += 1\\n            end -= 1\\n        return Decimal( (dup._sign, dup._int[:end], exp) )",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def normalize(self, context=None):\\n        if context is None:\\n            context = getcontext()\\n        if self._is_special:\\n            ans = self._check_nans(context=context)\\n            if ans:\\n                return ans\\n        dup = self._fix(context)\\n        if dup._isinfinity():\\n            return dup\\n        if not dup:\\n            return _dec_from_triple(dup._sign, '0', 0)\\n        exp_max = [context.Emax, context.Etop()][context._clamp]\\n        end = len(dup._int)\\n        exp = dup._exp\\n        while dup._int[end-1] == '0' and exp < exp_max:\\n            exp += 1\\n            end -= 1\\n        return _dec_from_triple(dup._sign, dup._int[:end], exp)"
  },
  {
    "code": "def time_translate(self):\\n        self.s.str.translate({\"A\": \"\\x01\\x01\"})",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[ArrowStringArray] Use `utf8_is_*` functions from Apache Arrow if available (#41041)",
    "fixed_code": "def time_translate(self, dtype):\\n        self.s.str.translate({\"A\": \"\\x01\\x01\"})"
  },
  {
    "code": "def invalidate_caches(cls):\\n        for finder in sys.path_importer_cache.values():\\n            if hasattr(finder, 'invalidate_caches'):\\n                finder.invalidate_caches()\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-33169: Remove values of `None` from sys.path_importer_cache when invalidating caches (GH-6402)\\n\\nAn entry of None in sys.path_importer_cache represents a negative/missing finder for a path, so clearing it out makes sense.",
    "fixed_code": "def invalidate_caches(cls):\\n        for name, finder in list(sys.path_importer_cache.items()):\\n            if finder is None:\\n                del sys.path_importer_cache[name]\\n            elif hasattr(finder, 'invalidate_caches'):\\n                finder.invalidate_caches()\\n    @classmethod"
  },
  {
    "code": "def process_router(self, ri):\\n\\t\\tri.iptables_manager.defer_apply_on()\\n\\t\\tex_gw_port = self._get_ex_gw_port(ri)\\n\\t\\tinternal_ports = ri.router.get(l3_constants.INTERFACE_KEY, [])\\n\\t\\texisting_port_ids = set([p['id'] for p in ri.internal_ports])\\n\\t\\tcurrent_port_ids = set([p['id'] for p in internal_ports\\n\\t\\t\\t\\t\\t\\t\\t\\tif p['admin_state_up']])\\n\\t\\tnew_ports = [p for p in internal_ports if\\n\\t\\t\\t\\t\\t p['id'] in current_port_ids and\\n\\t\\t\\t\\t\\t p['id'] not in existing_port_ids]\\n\\t\\told_ports = [p for p in ri.internal_ports if\\n\\t\\t\\t\\t\\t p['id'] not in current_port_ids]\\n\\t\\tfor p in new_ports:\\n\\t\\t\\tself._set_subnet_info(p)\\n\\t\\t\\tri.internal_ports.append(p)\\n\\t\\t\\tself.internal_network_added(ri, p['network_id'], p['id'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tp['ip_cidr'], p['mac_address'])\\n\\t\\tfor p in old_ports:\\n\\t\\t\\tri.internal_ports.remove(p)\\n\\t\\t\\tself.internal_network_removed(ri, p['id'], p['ip_cidr'])\\n\\t\\tinternal_cidrs = [p['ip_cidr'] for p in ri.internal_ports\\n\\t\\t\\t\\t\\t\\t  if netaddr.IPNetwork(p['ip_cidr']).version == 4]\\n\\t\\tex_gw_port_id = (ex_gw_port and ex_gw_port['id'] or\\n\\t\\t\\t\\t\\t\\t ri.ex_gw_port and ri.ex_gw_port['id'])\\n\\t\\tinterface_name = None\\n\\t\\tif ex_gw_port_id:\\n\\t\\t\\tinterface_name = self.get_external_device_name(ex_gw_port_id)\\n\\t\\tif ex_gw_port and not ri.ex_gw_port:\\n\\t\\t\\tself._set_subnet_info(ex_gw_port)\\n\\t\\t\\tself.external_gateway_added(ri, ex_gw_port,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tinterface_name, internal_cidrs)\\n\\t\\telif not ex_gw_port and ri.ex_gw_port:\\n\\t\\t\\tself.external_gateway_removed(ri, ri.ex_gw_port,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  interface_name, internal_cidrs)\\n\\t\\tri.perform_snat_action(self._handle_router_snat_rules,\\n\\t\\t\\t\\t\\t\\t\\t   internal_cidrs, interface_name)\\n\\t\\tif ex_gw_port:\\n\\t\\t\\tself.process_router_floating_ips(ri, ex_gw_port)\\n\\t\\tri.ex_gw_port = ex_gw_port\\n\\t\\tri.enable_snat = ri.router.get('enable_snat')\\n\\t\\tself.routes_updated(ri)\\n\\t\\tri.iptables_manager.defer_apply_off()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def process_router(self, ri):\\n\\t\\tri.iptables_manager.defer_apply_on()\\n\\t\\tex_gw_port = self._get_ex_gw_port(ri)\\n\\t\\tinternal_ports = ri.router.get(l3_constants.INTERFACE_KEY, [])\\n\\t\\texisting_port_ids = set([p['id'] for p in ri.internal_ports])\\n\\t\\tcurrent_port_ids = set([p['id'] for p in internal_ports\\n\\t\\t\\t\\t\\t\\t\\t\\tif p['admin_state_up']])\\n\\t\\tnew_ports = [p for p in internal_ports if\\n\\t\\t\\t\\t\\t p['id'] in current_port_ids and\\n\\t\\t\\t\\t\\t p['id'] not in existing_port_ids]\\n\\t\\told_ports = [p for p in ri.internal_ports if\\n\\t\\t\\t\\t\\t p['id'] not in current_port_ids]\\n\\t\\tfor p in new_ports:\\n\\t\\t\\tself._set_subnet_info(p)\\n\\t\\t\\tri.internal_ports.append(p)\\n\\t\\t\\tself.internal_network_added(ri, p['network_id'], p['id'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tp['ip_cidr'], p['mac_address'])\\n\\t\\tfor p in old_ports:\\n\\t\\t\\tri.internal_ports.remove(p)\\n\\t\\t\\tself.internal_network_removed(ri, p['id'], p['ip_cidr'])\\n\\t\\tinternal_cidrs = [p['ip_cidr'] for p in ri.internal_ports\\n\\t\\t\\t\\t\\t\\t  if netaddr.IPNetwork(p['ip_cidr']).version == 4]\\n\\t\\tex_gw_port_id = (ex_gw_port and ex_gw_port['id'] or\\n\\t\\t\\t\\t\\t\\t ri.ex_gw_port and ri.ex_gw_port['id'])\\n\\t\\tinterface_name = None\\n\\t\\tif ex_gw_port_id:\\n\\t\\t\\tinterface_name = self.get_external_device_name(ex_gw_port_id)\\n\\t\\tif ex_gw_port and not ri.ex_gw_port:\\n\\t\\t\\tself._set_subnet_info(ex_gw_port)\\n\\t\\t\\tself.external_gateway_added(ri, ex_gw_port,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tinterface_name, internal_cidrs)\\n\\t\\telif not ex_gw_port and ri.ex_gw_port:\\n\\t\\t\\tself.external_gateway_removed(ri, ri.ex_gw_port,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  interface_name, internal_cidrs)\\n\\t\\tri.perform_snat_action(self._handle_router_snat_rules,\\n\\t\\t\\t\\t\\t\\t\\t   internal_cidrs, interface_name)\\n\\t\\tif ex_gw_port:\\n\\t\\t\\tself.process_router_floating_ips(ri, ex_gw_port)\\n\\t\\tri.ex_gw_port = ex_gw_port\\n\\t\\tri.enable_snat = ri.router.get('enable_snat')\\n\\t\\tself.routes_updated(ri)\\n\\t\\tri.iptables_manager.defer_apply_off()"
  },
  {
    "code": "def _make_concat_multiindex(indexes, keys, levels=None, names=None):\\n    single_level = levels is None or len(levels) == 1\\n    if single_level:\\n        zipped = [keys]\\n        if names is None:\\n            names = [None]\\n    else:\\n        zipped = zip(*keys)\\n        if names is None:\\n            names = [None] * len(zipped)\\n    if levels is None:\\n        if single_level:\\n            levels = [_ensure_index(keys)]\\n        else:\\n            levels = [Factor(zp).levels for zp in zipped]\\n    else:\\n        levels = [_ensure_index(x) for x in levels]\\n    if not _all_indexes_same(indexes):\\n        label_list = []\\n        for hlevel, level in zip(zipped, levels):\\n            to_concat = []\\n            for key, index in zip(hlevel, indexes):\\n                i = level.get_loc(key)\\n                to_concat.append(np.repeat(i, len(index)))\\n            label_list.append(np.concatenate(to_concat))\\n        concat_index = _concat_indexes(indexes)\\n        if isinstance(concat_index, MultiIndex):\\n            levels.extend(concat_index.levels)\\n            label_list.extend(concat_index.labels)\\n        else:\\n            factor = Factor(concat_index)\\n            levels.append(factor.levels)\\n            label_list.append(factor.labels)\\n        names = names + _get_consensus_names(indexes)\\n        return MultiIndex(levels=levels, labels=label_list, names=names)\\n    new_index = indexes[0]\\n    n = len(new_index)\\n    kpieces = len(indexes)\\n    new_names = list(names)\\n    new_levels = list(levels)\\n    new_labels = []\\n    for hlevel, level in zip(zipped, levels):\\n        mapped = level.get_indexer(hlevel)\\n        new_labels.append(np.repeat(mapped, n))\\n    if isinstance(new_index, MultiIndex):\\n        new_levels.extend(new_index.levels)\\n        new_labels.extend([np.tile(lab, kpieces) for lab in new_index.labels])\\n        new_names.extend(new_index.names)\\n    else:\\n        new_levels.append(new_index)\\n        new_names.append(new_index.name)\\n        new_labels.append(np.tile(np.arange(n), kpieces))\\n    return MultiIndex(levels=new_levels, labels=new_labels, names=new_names)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: convert tuples in concat to MultiIndex",
    "fixed_code": "def _make_concat_multiindex(indexes, keys, levels=None, names=None):\\n    if ((levels is None and isinstance(keys[0], tuple)) or\\n        (levels is not None and len(levels) > 1)):\\n        zipped = zip(*keys)\\n        if names is None:\\n            names = [None] * len(zipped)\\n        if levels is None:\\n            levels = [Factor(zp).levels for zp in zipped]\\n        else:\\n            levels = [_ensure_index(x) for x in levels]\\n    else:\\n        zipped = [keys]\\n        if names is None:\\n            names = [None]\\n        if levels is None:\\n            levels = [_ensure_index(keys)]\\n        else:\\n            levels = [_ensure_index(x) for x in levels]\\n    if not _all_indexes_same(indexes):\\n        label_list = []\\n        for hlevel, level in zip(zipped, levels):\\n            to_concat = []\\n            for key, index in zip(hlevel, indexes):\\n                i = level.get_loc(key)\\n                to_concat.append(np.repeat(i, len(index)))\\n            label_list.append(np.concatenate(to_concat))\\n        concat_index = _concat_indexes(indexes)\\n        if isinstance(concat_index, MultiIndex):\\n            levels.extend(concat_index.levels)\\n            label_list.extend(concat_index.labels)\\n        else:\\n            factor = Factor(concat_index)\\n            levels.append(factor.levels)\\n            label_list.append(factor.labels)\\n        names = names + _get_consensus_names(indexes)\\n        return MultiIndex(levels=levels, labels=label_list, names=names)\\n    new_index = indexes[0]\\n    n = len(new_index)\\n    kpieces = len(indexes)\\n    new_names = list(names)\\n    new_levels = list(levels)\\n    new_labels = []\\n    for hlevel, level in zip(zipped, levels):\\n        mapped = level.get_indexer(hlevel)\\n        new_labels.append(np.repeat(mapped, n))\\n    if isinstance(new_index, MultiIndex):\\n        new_levels.extend(new_index.levels)\\n        new_labels.extend([np.tile(lab, kpieces) for lab in new_index.labels])\\n        new_names.extend(new_index.names)\\n    else:\\n        new_levels.append(new_index)\\n        new_names.append(new_index.name)\\n        new_labels.append(np.tile(np.arange(n), kpieces))\\n    return MultiIndex(levels=new_levels, labels=new_labels, names=new_names)"
  },
  {
    "code": "def get_car(logcan, sendcan):\\n  candidate, fingerprints, vin, car_fw, source, exact_match = fingerprint(logcan, sendcan)\\n  if candidate is None:\\n    cloudlog.warning(\"car doesn't match any fingerprints: %r\", fingerprints)\\n    candidate = \"mock\"\\n  experimental_long = Params().get_bool(\"ExperimentalLongitudinalEnabled\")\\n  CarInterface, CarController, CarState = interfaces[candidate]\\n  CP = CarInterface.get_params(candidate, fingerprints, car_fw, experimental_long)\\n  CP.carVin = vin\\n  CP.carFw = car_fw\\n  CP.fingerprintSource = source\\n  CP.fuzzyFingerprint = not exact_match\\n  return CarInterface(CP, CarController, CarState), CP",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CAN-FD HKG: query FW versions from camera (#26063)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfix\\n\\n\\n\\n\\nalways ignore",
    "fixed_code": "def get_car(logcan, sendcan, num_pandas=1):\\n  candidate, fingerprints, vin, car_fw, source, exact_match = fingerprint(logcan, sendcan, num_pandas)\\n  if candidate is None:\\n    cloudlog.warning(\"car doesn't match any fingerprints: %r\", fingerprints)\\n    candidate = \"mock\"\\n  experimental_long = Params().get_bool(\"ExperimentalLongitudinalEnabled\")\\n  CarInterface, CarController, CarState = interfaces[candidate]\\n  CP = CarInterface.get_params(candidate, fingerprints, car_fw, experimental_long)\\n  CP.carVin = vin\\n  CP.carFw = car_fw\\n  CP.fingerprintSource = source\\n  CP.fuzzyFingerprint = not exact_match\\n  return CarInterface(CP, CarController, CarState), CP"
  },
  {
    "code": "class SymLogNorm(Normalize):\\n\\tdef __init__(self, linthresh, linscale=1.0,\\n\\t\\t\\t\\t vmin=None, vmax=None, clip=False):\\n\\t\\tNormalize.__init__(self, vmin, vmax, clip)\\n\\t\\tself.linthresh = float(linthresh)\\n\\t\\tself._linscale_adj = (linscale / (1.0 - np.e ** -1))\\n\\t\\tif vmin is not None and vmax is not None:\\n\\t\\t\\tself._transform_vmin_vmax()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: add base kwarg to symlognorm",
    "fixed_code": "class SymLogNorm(Normalize):\\n\\tdef __init__(self, linthresh, linscale=1.0, vmin=None, vmax=None,\\n\\t\\t\\t\\t clip=False, base=None):\\n\\t\\tNormalize.__init__(self, vmin, vmax, clip)\\n\\t\\tif base is None:\\n\\t\\t\\tself._base = np.e\\n\\t\\t\\tcbook.warn_deprecated(\"3.3\", message=\"default base will change \"\\n\\t\\t\\t\\t\"from np.e to 10.  To suppress this warning specify the base \"\\n\\t\\t\\t\\t\"kwarg.\")\\n\\t\\telse:\\n\\t\\t\\tself._base = base\\n\\t\\tself._log_base = np.log(self._base)\\n\\t\\tself.linthresh = float(linthresh)\\n\\t\\tself._linscale_adj = (linscale / (1.0 - self._base ** -1))\\n\\t\\tif vmin is not None and vmax is not None:\\n\\t\\t\\tself._transform_vmin_vmax()"
  },
  {
    "code": "def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\\n        if is_period_dtype(self.dtype):\\n            if is_period_dtype(result.dtype):\\n                return self.freq\\n            return None\\n        elif self.freq is None:\\n            return None\\n        elif lib.is_scalar(other) and isna(other):\\n            return None\\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n            new_freq = None\\n            if isinstance(self.freq, Tick):\\n                new_freq = self.freq\\n            return new_freq\\n        elif isinstance(other, DateOffset):\\n            return None  \\n        elif isinstance(other, (datetime, np.datetime64)):\\n            return self.freq\\n        elif is_timedelta64_dtype(other):\\n            return None  \\n        elif is_object_dtype(other):\\n            return None  \\n        elif is_datetime64_any_dtype(other):\\n            return None  \\n        else:\\n            raise NotImplementedError\\n    __add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\\n    __sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\\n    __radd__ = make_wrapped_arith_op(\"__radd__\")\\n    __rsub__ = make_wrapped_arith_op(\"__rsub__\")\\n    __pow__ = make_wrapped_arith_op(\"__pow__\")\\n    __rpow__ = make_wrapped_arith_op(\"__rpow__\")\\n    __mul__ = make_wrapped_arith_op(\"__mul__\")\\n    __rmul__ = make_wrapped_arith_op(\"__rmul__\")\\n    __floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\\n    __rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\\n    __mod__ = make_wrapped_arith_op(\"__mod__\")\\n    __rmod__ = make_wrapped_arith_op(\"__rmod__\")\\n    __divmod__ = make_wrapped_arith_op(\"__divmod__\")\\n    __rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\\n    __truediv__ = make_wrapped_arith_op(\"__truediv__\")\\n    __rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\\n        if is_period_dtype(self.dtype):\\n            if is_period_dtype(result.dtype):\\n                return self.freq\\n            return None\\n        elif self.freq is None:\\n            return None\\n        elif lib.is_scalar(other) and isna(other):\\n            return None\\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n            new_freq = None\\n            if isinstance(self.freq, Tick):\\n                new_freq = self.freq\\n            return new_freq\\n        elif isinstance(other, DateOffset):\\n            return None  \\n        elif isinstance(other, (datetime, np.datetime64)):\\n            return self.freq\\n        elif is_timedelta64_dtype(other):\\n            return None  \\n        elif is_object_dtype(other):\\n            return None  \\n        elif is_datetime64_any_dtype(other):\\n            return None  \\n        else:\\n            raise NotImplementedError\\n    __add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\\n    __sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\\n    __radd__ = make_wrapped_arith_op(\"__radd__\")\\n    __rsub__ = make_wrapped_arith_op(\"__rsub__\")\\n    __pow__ = make_wrapped_arith_op(\"__pow__\")\\n    __rpow__ = make_wrapped_arith_op(\"__rpow__\")\\n    __mul__ = make_wrapped_arith_op(\"__mul__\")\\n    __rmul__ = make_wrapped_arith_op(\"__rmul__\")\\n    __floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\\n    __rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\\n    __mod__ = make_wrapped_arith_op(\"__mod__\")\\n    __rmod__ = make_wrapped_arith_op(\"__rmod__\")\\n    __divmod__ = make_wrapped_arith_op(\"__divmod__\")\\n    __rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\\n    __truediv__ = make_wrapped_arith_op(\"__truediv__\")\\n    __rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")"
  },
  {
    "code": "def getfloat(self, section, key):\\n        return float(self.get(section, key))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-3742] Fix handling of \"fallback\" for AirflowConfigParsxer.getint/boolean (#4674)\\n\\nWe added (and used) fallback as an argument on `getboolean` but didn't\\nadd it to the method, or add tests covering those \"casting\" accessors,\\nso they broke.\\n\\nThis fixes those methods, and adds tests covering them",
    "fixed_code": "def getfloat(self, section, key, **kwargs):\\n        return float(self.get(section, key, **kwargs))"
  },
  {
    "code": "def linebreaks_filter(value, autoescape=True):\\n\\tautoescape = autoescape and not isinstance(value, SafeData)\\n\\treturn mark_safe(linebreaks(value, autoescape))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def linebreaks_filter(value, autoescape=True):\\n\\tautoescape = autoescape and not isinstance(value, SafeData)\\n\\treturn mark_safe(linebreaks(value, autoescape))"
  },
  {
    "code": "def __repr__(self):\\n        token_name = self.token_type.name.capitalize()\\n        return ('<%s token: \"%s...\">' %\\n                (token_name, self.contents[:20].replace('\\n', '')))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __repr__(self):\\n        token_name = self.token_type.name.capitalize()\\n        return ('<%s token: \"%s...\">' %\\n                (token_name, self.contents[:20].replace('\\n', '')))"
  },
  {
    "code": "def isneginf(x, y=None):\\n    if y is None:\\n        y = empty(x.shape, dtype=nx.bool_)\\n    umath.logical_and(isinf(x), signbit(x), y)\\n    return y",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def isneginf(x, y=None):\\n    if y is None:\\n        y = empty(x.shape, dtype=nx.bool_)\\n    umath.logical_and(isinf(x), signbit(x), y)\\n    return y"
  },
  {
    "code": "def sparse_categorical_accuracy(y_true, y_pred):\\n\\treturn K.cast(K.equal(K.flatten(y_true),\\n\\t\\t\\t\\t\\t\\t  K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\\n\\t\\t\\t\\t  K.floatx())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def sparse_categorical_accuracy(y_true, y_pred):\\n\\treturn K.cast(K.equal(K.flatten(y_true),\\n\\t\\t\\t\\t\\t\\t  K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\\n\\t\\t\\t\\t  K.floatx())"
  },
  {
    "code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               parse_dates=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: parsing numbers with commas in read_csv/table/clipboard/fwf #796 VB: read_csv with/without thousands separator parsing",
    "fixed_code": "def read_table(filepath_or_buffer,\\n               sep='\\t',\\n               header=0,\\n               index_col=None,\\n               names=None,\\n               skiprows=None,\\n               na_values=None,\\n               thousands=None,\\n               parse_dates=False,\\n               dayfirst=False,\\n               date_parser=None,\\n               nrows=None,\\n               iterator=False,\\n               chunksize=None,\\n               skip_footer=0,\\n               converters=None,\\n               verbose=False,\\n               delimiter=None,\\n               encoding=None):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    kwds['encoding'] = None\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_fwf_doc)"
  },
  {
    "code": "def search(\\n        self, client_ids: List[str], query: str, page_size: int = 10000, **kwargs\\n    ) -> List[GoogleAdsRow]:\\n        service = self._get_service\\n        iterators = (\\n            service.search(client_id, query=query, page_size=page_size, **kwargs) for client_id in client_ids\\n        )\\n        self.log.info(\"Fetched Google Ads Iterators\")\\n        return self._extract_rows(iterators)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Google Ads Hook: Support newer versions of the google-ads library (#17160)",
    "fixed_code": "def search(\\n        self, client_ids: List[str], query: str, page_size: int = 10000, **kwargs\\n    ) -> List[GoogleAdsRow]:\\n        data_proto_plus = self._search(client_ids, query, page_size, **kwargs)\\n        data_native_pb = [row._pb for row in data_proto_plus]\\n        return data_native_pb\\n    def search_proto_plus(\\n        self, client_ids: List[str], query: str, page_size: int = 10000, **kwargs\\n    ) -> List[GoogleAdsRow]:\\n        return self._search(client_ids, query, page_size, **kwargs)"
  },
  {
    "code": "def check_estimators_overwrite_params(name, Estimator):\\n    X, y = make_blobs(random_state=0, n_samples=9)\\n    y = multioutput_estimator_convert_y_2d(name, y)\\n    X -= X.min()\\n    with warnings.catch_warnings(record=True):\\n        estimator = Estimator()\\n    set_testing_parameters(estimator)\\n    set_random_state(estimator)\\n    params = estimator.get_params()\\n    original_params = deepcopy(params)\\n    estimator.fit(X, y)\\n    new_params = estimator.get_params()\\n    for param_name, original_value in original_params.items():\\n        new_value = new_params[param_name]\\n        assert_equal(hash(new_value), hash(original_value),\\n                     \"Estimator %s should not change or mutate \"\\n                     \" the parameter %s from %s to %s during fit.\"\\n                     % (name, param_name, original_value, new_value))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "clean up deprecation warning stuff in common tests\\n\\nminor fixes in preprocessing tests",
    "fixed_code": "def check_estimators_overwrite_params(name, Estimator):\\n    X, y = make_blobs(random_state=0, n_samples=9)\\n    y = multioutput_estimator_convert_y_2d(name, y)\\n    X -= X.min()\\n    estimator = Estimator()\\n    set_testing_parameters(estimator)\\n    set_random_state(estimator)\\n    params = estimator.get_params()\\n    original_params = deepcopy(params)\\n    estimator.fit(X, y)\\n    new_params = estimator.get_params()\\n    for param_name, original_value in original_params.items():\\n        new_value = new_params[param_name]\\n        assert_equal(hash(new_value), hash(original_value),\\n                     \"Estimator %s should not change or mutate \"\\n                     \" the parameter %s from %s to %s during fit.\"\\n                     % (name, param_name, original_value, new_value))"
  },
  {
    "code": "def cancel(self):\\n        self._cancelled = True\\n        self._callback = None\\n        self._args = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "asyncio, Tulip issue 206: In debug mode, keep the callback in the representation of Handle and TimerHandle after cancel().",
    "fixed_code": "def cancel(self):\\n        self._cancelled = True\\n        if self._loop.get_debug():\\n            self._repr = repr(self)\\n        self._callback = None\\n        self._args = None"
  },
  {
    "code": "def send_head(self):\\n\\t\\tpath = self.translate_path(self.path)\\n\\t\\tif os.path.isdir(path):\\n\\t\\t\\tparts = urlparse.urlsplit(self.path)\\n\\t\\t\\tif not parts.path.endswith('/'):\\n\\t\\t\\t\\tif self.path.startswith('//'):\\n\\t\\t\\t\\t\\tself.send_error(400,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"URI must not start with //\")\\n\\t\\t\\t\\t\\treturn None\\n\\t\\treturn super(NovaProxyRequestHandler, self).send_head()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def send_head(self):\\n\\t\\tpath = self.translate_path(self.path)\\n\\t\\tif os.path.isdir(path):\\n\\t\\t\\tparts = urlparse.urlsplit(self.path)\\n\\t\\t\\tif not parts.path.endswith('/'):\\n\\t\\t\\t\\tif self.path.startswith('//'):\\n\\t\\t\\t\\t\\tself.send_error(400,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"URI must not start with //\")\\n\\t\\t\\t\\t\\treturn None\\n\\t\\treturn super(NovaProxyRequestHandler, self).send_head()"
  },
  {
    "code": "def _get_column_indices(X, key):\\n    n_columns = X.shape[1]\\n    if _check_key_type(key, int):\\n        if isinstance(key, int):\\n            return [key]\\n        elif isinstance(key, slice):\\n            return list(range(n_columns)[key])\\n        else:\\n            return list(key)\\n    elif _check_key_type(key, str):\\n        try:\\n            all_columns = list(X.columns)\\n        except AttributeError:\\n            raise ValueError(\"Specifying the columns using strings is only \"\\n                             \"supported for pandas DataFrames\")\\n        if isinstance(key, str):\\n            columns = [key]\\n        elif isinstance(key, slice):\\n            start, stop = key.start, key.stop\\n            if start is not None:\\n                start = all_columns.index(start)\\n            if stop is not None:\\n                stop = all_columns.index(stop) + 1\\n            else:\\n                stop = n_columns + 1\\n            return list(range(n_columns)[slice(start, stop)])\\n        else:\\n            columns = list(key)\\n        return [all_columns.index(col) for col in columns]\\n    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):\\n        return list(np.arange(n_columns)[key])\\n    else:\\n        raise ValueError(\"No valid specification of the columns. Only a \"\\n                         \"scalar, list or slice of all integers or all \"\\n                         \"strings, or boolean mask is allowed\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX Convert the negative indices to positive ones in ColumnTransformer._get_column_indices (#13017)",
    "fixed_code": "def _get_column_indices(X, key):\\n    n_columns = X.shape[1]\\n    if (_check_key_type(key, int)\\n            or hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_)):\\n        idx = np.arange(n_columns)[key]\\n        return np.atleast_1d(idx).tolist()\\n    elif _check_key_type(key, str):\\n        try:\\n            all_columns = list(X.columns)\\n        except AttributeError:\\n            raise ValueError(\"Specifying the columns using strings is only \"\\n                             \"supported for pandas DataFrames\")\\n        if isinstance(key, str):\\n            columns = [key]\\n        elif isinstance(key, slice):\\n            start, stop = key.start, key.stop\\n            if start is not None:\\n                start = all_columns.index(start)\\n            if stop is not None:\\n                stop = all_columns.index(stop) + 1\\n            else:\\n                stop = n_columns + 1\\n            return list(range(n_columns)[slice(start, stop)])\\n        else:\\n            columns = list(key)\\n        return [all_columns.index(col) for col in columns]\\n    else:\\n        raise ValueError(\"No valid specification of the columns. Only a \"\\n                         \"scalar, list or slice of all integers or all \"\\n                         \"strings, or boolean mask is allowed\")"
  },
  {
    "code": "def holidays(self, start=None, end=None, return_name=False):\\n        if self.rules is None:\\n            raise Exception(\\n                f\"Holiday Calendar {self.name} does not have any rules specified\"\\n            )\\n        if start is None:\\n            start = AbstractHolidayCalendar.start_date\\n        if end is None:\\n            end = AbstractHolidayCalendar.end_date\\n        start = Timestamp(start)\\n        end = Timestamp(end)\\n        if self._cache is None or start < self._cache[0] or end > self._cache[1]:\\n            holidays = [rule.dates(start, end, return_name=True) for rule in self.rules]\\n            if holidays:\\n                holidays = concat(holidays)\\n            else:\\n                holidays = Series(index=DatetimeIndex([]), dtype=object)\\n            self._cache = (start, end, holidays.sort_index())\\n        holidays = self._cache[2]\\n        holidays = holidays[start:end]\\n        if return_name:\\n            return holidays\\n        else:\\n            return holidays.index\\n    @staticmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def holidays(self, start=None, end=None, return_name=False):\\n        if self.rules is None:\\n            raise Exception(\\n                f\"Holiday Calendar {self.name} does not have any rules specified\"\\n            )\\n        if start is None:\\n            start = AbstractHolidayCalendar.start_date\\n        if end is None:\\n            end = AbstractHolidayCalendar.end_date\\n        start = Timestamp(start)\\n        end = Timestamp(end)\\n        if self._cache is None or start < self._cache[0] or end > self._cache[1]:\\n            holidays = [rule.dates(start, end, return_name=True) for rule in self.rules]\\n            if holidays:\\n                holidays = concat(holidays)\\n            else:\\n                holidays = Series(index=DatetimeIndex([]), dtype=object)\\n            self._cache = (start, end, holidays.sort_index())\\n        holidays = self._cache[2]\\n        holidays = holidays[start:end]\\n        if return_name:\\n            return holidays\\n        else:\\n            return holidays.index\\n    @staticmethod"
  },
  {
    "code": "def connections_add(args):\\n    missing_args = list()\\n    invalid_args = list()\\n    if args.conn_uri:\\n        for arg in alternative_conn_specs:\\n            if getattr(args, arg) is not None:\\n                invalid_args.append(arg)\\n    elif not args.conn_type:\\n        missing_args.append('conn_uri or conn_type')\\n    if missing_args:\\n        msg = ('The following args are required to add a connection:' +\\n               ' {missing!r}'.format(missing=missing_args))\\n        raise SystemExit(msg)\\n    if invalid_args:\\n        msg = ('The following args are not compatible with the ' +\\n               '--add flag and --conn_uri flag: {invalid!r}')\\n        msg = msg.format(invalid=invalid_args)\\n        raise SystemExit(msg)\\n    if args.conn_uri:\\n        new_conn = Connection(conn_id=args.conn_id, uri=args.conn_uri)\\n    else:\\n        new_conn = Connection(conn_id=args.conn_id,\\n                              conn_type=args.conn_type,\\n                              host=args.conn_host,\\n                              login=args.conn_login,\\n                              password=args.conn_password,\\n                              schema=args.conn_schema,\\n                              port=args.conn_port)\\n    if args.conn_extra is not None:\\n        new_conn.set_extra(args.conn_extra)\\n    with db.create_session() as session:\\n        if not (session.query(Connection)\\n                .filter(Connection.conn_id == new_conn.conn_id).first()):\\n            session.add(new_conn)\\n            msg = '\\n\\tSuccessfully added `conn_id`={conn_id} : {uri}\\n'\\n            msg = msg.format(conn_id=new_conn.conn_id,\\n                             uri=args.conn_uri or\\n                             urlunparse((args.conn_type,\\n                                         '{login}:{password}@{host}:{port}'\\n                                             .format(login=args.conn_login or '',\\n                                                     password=args.conn_password or '',\\n                                                     host=args.conn_host or '',\\n                                                     port=args.conn_port or ''),\\n                                         args.conn_schema or '', '', '', '')))\\n            print(msg)\\n        else:\\n            msg = '\\n\\tA connection with `conn_id`={conn_id} already exists\\n'\\n            msg = msg.format(conn_id=new_conn.conn_id)\\n            print(msg)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294)",
    "fixed_code": "def connections_add(args):\\n    missing_args = list()\\n    invalid_args = list()\\n    if args.conn_uri:\\n        for arg in alternative_conn_specs:\\n            if getattr(args, arg) is not None:\\n                invalid_args.append(arg)\\n    elif not args.conn_type:\\n        missing_args.append('conn_uri or conn_type')\\n    if missing_args:\\n        msg = ('The following args are required to add a connection:' +\\n               ' {missing!r}'.format(missing=missing_args))\\n        raise SystemExit(msg)\\n    if invalid_args:\\n        msg = ('The following args are not compatible with the ' +\\n               '--add flag and --conn_uri flag: {invalid!r}')\\n        msg = msg.format(invalid=invalid_args)\\n        raise SystemExit(msg)\\n    if args.conn_uri:\\n        new_conn = Connection(conn_id=args.conn_id, uri=args.conn_uri)\\n    else:\\n        new_conn = Connection(conn_id=args.conn_id,\\n                              conn_type=args.conn_type,\\n                              host=args.conn_host,\\n                              login=args.conn_login,\\n                              password=args.conn_password,\\n                              schema=args.conn_schema,\\n                              port=args.conn_port)\\n    if args.conn_extra is not None:\\n        new_conn.set_extra(args.conn_extra)\\n    with db.create_session() as session:\\n        if not (session.query(Connection)\\n                .filter(Connection.conn_id == new_conn.conn_id).first()):\\n            session.add(new_conn)\\n            msg = '\\n\\tSuccessfully added `conn_id`={conn_id} : {uri}\\n'\\n            msg = msg.format(conn_id=new_conn.conn_id,\\n                             uri=args.conn_uri or\\n                             urlunparse((args.conn_type,\\n                                         '{login}:{password}@{host}:{port}'\\n                                             .format(login=args.conn_login or '',\\n                                                     password=args.conn_password or '',\\n                                                     host=args.conn_host or '',\\n                                                     port=args.conn_port or ''),\\n                                         args.conn_schema or '', '', '', '')))\\n            print(msg)\\n        else:\\n            msg = '\\n\\tA connection with `conn_id`={conn_id} already exists\\n'\\n            msg = msg.format(conn_id=new_conn.conn_id)\\n            print(msg)"
  },
  {
    "code": "def add_columns(self) -> list[str]:\\n\\t\\tif self._add_columns is type(self)._add_columns and has_request_context():\\n\\t\\t\\tself._add_columns = [\\n\\t\\t\\t\\t*self._add_columns,\\n\\t\\t\\t\\t*(k for k, _, _ in self._iter_extra_field_names_and_sensitivity()),\\n\\t\\t\\t]\\n\\t\\treturn self._add_columns\\n\\t@property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def add_columns(self) -> list[str]:\\n\\t\\tif self._add_columns is type(self)._add_columns and has_request_context():\\n\\t\\t\\tself._add_columns = [\\n\\t\\t\\t\\t*self._add_columns,\\n\\t\\t\\t\\t*(k for k, _, _ in self._iter_extra_field_names_and_sensitivity()),\\n\\t\\t\\t]\\n\\t\\treturn self._add_columns\\n\\t@property"
  },
  {
    "code": "def __init__(self, methodname, reversed=False):\\n\\t\\tself.__name__ = methodname\\n\\t\\tself.__doc__ = self.getdoc()\\n\\t\\tself.reversed = reversed",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, methodname, reversed=False):\\n\\t\\tself.__name__ = methodname\\n\\t\\tself.__doc__ = self.getdoc()\\n\\t\\tself.reversed = reversed"
  },
  {
    "code": "def _convert_arff_data_dataframe(\\n    arff: ArffContainerType, columns: List, features_dict: Dict[str, Any]\\n) -> Tuple:\\n    pd = check_pandas_support('fetch_openml with as_frame=True')\\n    attributes = OrderedDict(arff['attributes'])\\n    arff_columns = list(attributes)\\n    if not isinstance(arff['data'], Generator):\\n        raise ValueError(\\n            \"arff['data'] must be a generator when converting to pd.DataFrame.\"\\n        )\\n    first_row = next(arff['data'])\\n    first_df = pd.DataFrame([first_row], columns=arff_columns)\\n    row_bytes = first_df.memory_usage(deep=True).sum()\\n    chunksize = get_chunk_n_rows(row_bytes)\\n    columns_to_keep = [col for col in arff_columns if col in columns]\\n    dfs = []\\n    dfs.append(first_df[columns_to_keep])\\n    for data in _chunk_generator(arff['data'], chunksize):\\n        dfs.append(pd.DataFrame(data, columns=arff_columns)[columns_to_keep])\\n    df = pd.concat(dfs, ignore_index=True)\\n    for column in columns_to_keep:\\n        dtype = _feature_to_dtype(features_dict[column])\\n        if dtype == 'category':\\n            cats_without_missing = [cat for cat in attributes[column]\\n                                    if cat is not None and\\n                                    not is_scalar_nan(cat)]\\n            dtype = pd.api.types.CategoricalDtype(cats_without_missing)\\n        df[column] = df[column].astype(dtype, copy=False)\\n    return (df, )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_arff_data_dataframe(\\n    arff: ArffContainerType, columns: List, features_dict: Dict[str, Any]\\n) -> Tuple:\\n    pd = check_pandas_support('fetch_openml with as_frame=True')\\n    attributes = OrderedDict(arff['attributes'])\\n    arff_columns = list(attributes)\\n    if not isinstance(arff['data'], Generator):\\n        raise ValueError(\\n            \"arff['data'] must be a generator when converting to pd.DataFrame.\"\\n        )\\n    first_row = next(arff['data'])\\n    first_df = pd.DataFrame([first_row], columns=arff_columns)\\n    row_bytes = first_df.memory_usage(deep=True).sum()\\n    chunksize = get_chunk_n_rows(row_bytes)\\n    columns_to_keep = [col for col in arff_columns if col in columns]\\n    dfs = []\\n    dfs.append(first_df[columns_to_keep])\\n    for data in _chunk_generator(arff['data'], chunksize):\\n        dfs.append(pd.DataFrame(data, columns=arff_columns)[columns_to_keep])\\n    df = pd.concat(dfs, ignore_index=True)\\n    for column in columns_to_keep:\\n        dtype = _feature_to_dtype(features_dict[column])\\n        if dtype == 'category':\\n            cats_without_missing = [cat for cat in attributes[column]\\n                                    if cat is not None and\\n                                    not is_scalar_nan(cat)]\\n            dtype = pd.api.types.CategoricalDtype(cats_without_missing)\\n        df[column] = df[column].astype(dtype, copy=False)\\n    return (df, )"
  },
  {
    "code": "def update(self, iterable=None, **kwds):\\n        which')\\n        >>> c.update('witch')           \\n        >>> d = Counter('watch')\\n        >>> c.update(d)                 \\n        >>> c['h']                      \\n        4\\n        '''\\n        if iterable is not None:\\n            if isinstance(iterable, Mapping):\\n                for elem, count in iterable.items():\\n                    self[elem] += count\\n            else:\\n                for elem in iterable:\\n                    self[elem] += 1\\n        if kwds:\\n            self.update(kwds)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update(self, iterable=None, **kwds):\\n        which')\\n        >>> c.update('witch')           \\n        >>> d = Counter('watch')\\n        >>> c.update(d)                 \\n        >>> c['h']                      \\n        4\\n        '''\\n        if iterable is not None:\\n            if isinstance(iterable, Mapping):\\n                for elem, count in iterable.items():\\n                    self[elem] += count\\n            else:\\n                for elem in iterable:\\n                    self[elem] += 1\\n        if kwds:\\n            self.update(kwds)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        http=dict(aliases=['enable_http'], type='bool'),\\n        http_port=dict(type='int'),\\n        https=dict(aliases=['enable_https'], type='bool'),\\n        https_port=dict(type='int'),\\n        local_http=dict(aliases=['enable_local_http'], type='bool'),\\n        local_http_port=dict(type='int'),\\n        socket=dict(aliases=['enable_socket'], type='bool'),\\n        vrf=dict(default='default'),\\n        config=dict(),\\n        state=dict(default='started', choices=['stopped', 'started']),\\n    )\\n    argument_spec.update(eos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        http=dict(aliases=['enable_http'], type='bool'),\\n        http_port=dict(type='int'),\\n        https=dict(aliases=['enable_https'], type='bool'),\\n        https_port=dict(type='int'),\\n        local_http=dict(aliases=['enable_local_http'], type='bool'),\\n        local_http_port=dict(type='int'),\\n        socket=dict(aliases=['enable_socket'], type='bool'),\\n        vrf=dict(default='default'),\\n        config=dict(),\\n        state=dict(default='started', choices=['stopped', 'started']),\\n    )\\n    argument_spec.update(eos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)"
  },
  {
    "code": "def packkey(ae, key, value):\\n    if hasattr(key, 'which'):\\n        keystr = key.which\\n    elif hasattr(key, 'want'):\\n        keystr = key.want\\n    else:\\n        keystr = key\\n    ae.AEPutParamDesc(keystr, pack(value))\\ndef pack(x, forcetype = None):\\n    if forcetype:\\n        if isinstance(x, bytes):\\n            return AE.AECreateDesc(forcetype, x)\\n        else:\\n            return pack(x).AECoerceDesc(forcetype)\\n    if x == None:\\n        return AE.AECreateDesc(b'null', '')\\n    if isinstance(x, AEDescType):\\n        return x\\n    if isinstance(x, FSSType):\\n        return AE.AECreateDesc(b'fss ', x.data)\\n    if isinstance(x, FSRefType):\\n        return AE.AECreateDesc(b'fsrf', x.data)\\n    if isinstance(x, AliasType):\\n        return AE.AECreateDesc(b'alis', x.data)\\n    if isinstance(x, int):\\n        return AE.AECreateDesc(b'long', struct.pack('l', x))\\n    if isinstance(x, float):\\n        return AE.AECreateDesc(b'doub', struct.pack('d', x))\\n    if isinstance(x, (bytes, str8)):\\n        return AE.AECreateDesc(b'TEXT', x)\\n    if isinstance(x, str):\\n        data = x.encode('utf16')\\n        if data[:2] == '\\xfe\\xff':\\n            data = data[2:]\\n        return AE.AECreateDesc(b'utxt', data)\\n    if isinstance(x, list):\\n        lst = AE.AECreateList('', 0)\\n        for item in x:\\n            lst.AEPutDesc(0, pack(item))\\n        return lst\\n    if isinstance(x, dict):\\n        record = AE.AECreateList('', 1)\\n        for key, value in x.items():\\n            packkey(record, key, value)\\n        return record\\n    if isinstance(x, type) and issubclass(x, ObjectSpecifier):\\n        return AE.AECreateDesc(b'type', x.want)\\n    if hasattr(x, '__aepack__'):\\n        return x.__aepack__()\\n    if hasattr(x, 'which'):\\n        return AE.AECreateDesc(b'TEXT', x.which)\\n    if hasattr(x, 'want'):\\n        return AE.AECreateDesc(b'TEXT', x.want)\\n    return AE.AECreateDesc(b'TEXT', repr(x))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def packkey(ae, key, value):\\n    if hasattr(key, 'which'):\\n        keystr = key.which\\n    elif hasattr(key, 'want'):\\n        keystr = key.want\\n    else:\\n        keystr = key\\n    ae.AEPutParamDesc(keystr, pack(value))\\ndef pack(x, forcetype = None):\\n    if forcetype:\\n        if isinstance(x, bytes):\\n            return AE.AECreateDesc(forcetype, x)\\n        else:\\n            return pack(x).AECoerceDesc(forcetype)\\n    if x == None:\\n        return AE.AECreateDesc(b'null', '')\\n    if isinstance(x, AEDescType):\\n        return x\\n    if isinstance(x, FSSType):\\n        return AE.AECreateDesc(b'fss ', x.data)\\n    if isinstance(x, FSRefType):\\n        return AE.AECreateDesc(b'fsrf', x.data)\\n    if isinstance(x, AliasType):\\n        return AE.AECreateDesc(b'alis', x.data)\\n    if isinstance(x, int):\\n        return AE.AECreateDesc(b'long', struct.pack('l', x))\\n    if isinstance(x, float):\\n        return AE.AECreateDesc(b'doub', struct.pack('d', x))\\n    if isinstance(x, (bytes, str8)):\\n        return AE.AECreateDesc(b'TEXT', x)\\n    if isinstance(x, str):\\n        data = x.encode('utf16')\\n        if data[:2] == '\\xfe\\xff':\\n            data = data[2:]\\n        return AE.AECreateDesc(b'utxt', data)\\n    if isinstance(x, list):\\n        lst = AE.AECreateList('', 0)\\n        for item in x:\\n            lst.AEPutDesc(0, pack(item))\\n        return lst\\n    if isinstance(x, dict):\\n        record = AE.AECreateList('', 1)\\n        for key, value in x.items():\\n            packkey(record, key, value)\\n        return record\\n    if isinstance(x, type) and issubclass(x, ObjectSpecifier):\\n        return AE.AECreateDesc(b'type', x.want)\\n    if hasattr(x, '__aepack__'):\\n        return x.__aepack__()\\n    if hasattr(x, 'which'):\\n        return AE.AECreateDesc(b'TEXT', x.which)\\n    if hasattr(x, 'want'):\\n        return AE.AECreateDesc(b'TEXT', x.want)\\n    return AE.AECreateDesc(b'TEXT', repr(x))"
  },
  {
    "code": "def parse(fp=None, environ=os.environ, keep_blank_values=0,\\n\\t\\t  strict_parsing=0, separator='&'):\\n\\tif fp is None:\\n\\t\\tfp = sys.stdin\\n\\tif hasattr(fp,'encoding'):\\n\\t\\tencoding = fp.encoding\\n\\telse:\\n\\t\\tencoding = 'latin-1'\\n\\tif isinstance(fp, TextIOWrapper):\\n\\t\\tfp = fp.buffer\\n\\tif not 'REQUEST_METHOD' in environ:\\n\\t\\tenviron['REQUEST_METHOD'] = 'GET'\\t   \\n\\tif environ['REQUEST_METHOD'] == 'POST':\\n\\t\\tctype, pdict = parse_header(environ['CONTENT_TYPE'])\\n\\t\\tif ctype == 'multipart/form-data':\\n\\t\\t\\treturn parse_multipart(fp, pdict, separator=separator)\\n\\t\\telif ctype == 'application/x-www-form-urlencoded':\\n\\t\\t\\tclength = int(environ['CONTENT_LENGTH'])\\n\\t\\t\\tif maxlen and clength > maxlen:\\n\\t\\t\\t\\traise ValueError('Maximum content length exceeded')\\n\\t\\t\\tqs = fp.read(clength).decode(encoding)\\n\\t\\telse:\\n\\t\\t\\tqs = ''\\t\\t\\t\\t\\t \\n\\t\\tif 'QUERY_STRING' in environ:\\n\\t\\t\\tif qs: qs = qs + '&'\\n\\t\\t\\tqs = qs + environ['QUERY_STRING']\\n\\t\\telif sys.argv[1:]:\\n\\t\\t\\tif qs: qs = qs + '&'\\n\\t\\t\\tqs = qs + sys.argv[1]\\n\\t\\tenviron['QUERY_STRING'] = qs\\t\\n\\telif 'QUERY_STRING' in environ:\\n\\t\\tqs = environ['QUERY_STRING']\\n\\telse:\\n\\t\\tif sys.argv[1:]:\\n\\t\\t\\tqs = sys.argv[1]\\n\\t\\telse:\\n\\t\\t\\tqs = \"\"\\n\\t\\tenviron['QUERY_STRING'] = qs\\t\\n\\treturn urllib.parse.parse_qs(qs, keep_blank_values, strict_parsing,\\n\\t\\t\\t\\t\\t\\t\\t\\t encoding=encoding, separator=separator)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse(fp=None, environ=os.environ, keep_blank_values=0,\\n\\t\\t  strict_parsing=0, separator='&'):\\n\\tif fp is None:\\n\\t\\tfp = sys.stdin\\n\\tif hasattr(fp,'encoding'):\\n\\t\\tencoding = fp.encoding\\n\\telse:\\n\\t\\tencoding = 'latin-1'\\n\\tif isinstance(fp, TextIOWrapper):\\n\\t\\tfp = fp.buffer\\n\\tif not 'REQUEST_METHOD' in environ:\\n\\t\\tenviron['REQUEST_METHOD'] = 'GET'\\t   \\n\\tif environ['REQUEST_METHOD'] == 'POST':\\n\\t\\tctype, pdict = parse_header(environ['CONTENT_TYPE'])\\n\\t\\tif ctype == 'multipart/form-data':\\n\\t\\t\\treturn parse_multipart(fp, pdict, separator=separator)\\n\\t\\telif ctype == 'application/x-www-form-urlencoded':\\n\\t\\t\\tclength = int(environ['CONTENT_LENGTH'])\\n\\t\\t\\tif maxlen and clength > maxlen:\\n\\t\\t\\t\\traise ValueError('Maximum content length exceeded')\\n\\t\\t\\tqs = fp.read(clength).decode(encoding)\\n\\t\\telse:\\n\\t\\t\\tqs = ''\\t\\t\\t\\t\\t \\n\\t\\tif 'QUERY_STRING' in environ:\\n\\t\\t\\tif qs: qs = qs + '&'\\n\\t\\t\\tqs = qs + environ['QUERY_STRING']\\n\\t\\telif sys.argv[1:]:\\n\\t\\t\\tif qs: qs = qs + '&'\\n\\t\\t\\tqs = qs + sys.argv[1]\\n\\t\\tenviron['QUERY_STRING'] = qs\\t\\n\\telif 'QUERY_STRING' in environ:\\n\\t\\tqs = environ['QUERY_STRING']\\n\\telse:\\n\\t\\tif sys.argv[1:]:\\n\\t\\t\\tqs = sys.argv[1]\\n\\t\\telse:\\n\\t\\t\\tqs = \"\"\\n\\t\\tenviron['QUERY_STRING'] = qs\\t\\n\\treturn urllib.parse.parse_qs(qs, keep_blank_values, strict_parsing,\\n\\t\\t\\t\\t\\t\\t\\t\\t encoding=encoding, separator=separator)"
  },
  {
    "code": "def do_list(self, arg):\\n        self.lastcmd = 'list'\\n        last = None\\n        if arg and arg != '.':\\n            try:\\n                if ',' in arg:\\n                    first, last = arg.split(',')\\n                    first = int(first.strip())\\n                    last = int(last.strip())\\n                    if last < first:\\n                        last = first + last\\n                else:\\n                    first = int(arg.strip())\\n                    first = max(1, first - 5)\\n            except ValueError:\\n                self.error('Error in argument: %r' % arg)\\n                return\\n        elif self.lineno is None or arg == '.':\\n            first = max(1, self.curframe.f_lineno - 5)\\n        else:\\n            first = self.lineno + 1\\n        if last is None:\\n            last = first + 10\\n        filename = self.curframe.f_code.co_filename\\n        breaklist = self.get_file_breaks(filename)\\n        try:\\n            lines = linecache.getlines(filename, self.curframe.f_globals)\\n            self._print_lines(lines[first-1:last], first, breaklist,\\n                              self.curframe.f_lineno, -1)\\n            self.lineno = min(last, len(lines))\\n            if len(lines) < last:\\n                self.message('[EOF]')\\n        except KeyboardInterrupt:\\n            pass\\n    do_l = do_list",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Show the traceback line numbers as well as the current line numbers if an exception is being debugged.  Courtesy of pdb++ by Antonio Cuni.  Also document -> and >> markers for \"list\".",
    "fixed_code": "def do_list(self, arg):\\n        self.lastcmd = 'list'\\n        last = None\\n        if arg and arg != '.':\\n            try:\\n                if ',' in arg:\\n                    first, last = arg.split(',')\\n                    first = int(first.strip())\\n                    last = int(last.strip())\\n                    if last < first:\\n                        last = first + last\\n                else:\\n                    first = int(arg.strip())\\n                    first = max(1, first - 5)\\n            except ValueError:\\n                self.error('Error in argument: %r' % arg)\\n                return\\n        elif self.lineno is None or arg == '.':\\n            first = max(1, self.curframe.f_lineno - 5)\\n        else:\\n            first = self.lineno + 1\\n        if last is None:\\n            last = first + 10\\n        filename = self.curframe.f_code.co_filename\\n        breaklist = self.get_file_breaks(filename)\\n        try:\\n            lines = linecache.getlines(filename, self.curframe.f_globals)\\n            self._print_lines(lines[first-1:last], first, breaklist,\\n                              self.curframe)\\n            self.lineno = min(last, len(lines))\\n            if len(lines) < last:\\n                self.message('[EOF]')\\n        except KeyboardInterrupt:\\n            pass\\n    do_l = do_list"
  },
  {
    "code": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,\\n                     sample_weight=None, return_mean=False, check_input=True):\\n    if isinstance(sample_weight, numbers.Number):\\n        sample_weight = None\\n    if sample_weight is not None:\\n        sample_weight = np.asarray(sample_weight)\\n    if check_input:\\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\\n                        dtype=FLOAT_DTYPES)\\n    elif copy:\\n        if sp.issparse(X):\\n            X = X.copy()\\n        else:\\n            X = X.copy(order='K')\\n    y = np.asarray(y, dtype=X.dtype)\\n    if fit_intercept:\\n        if sp.issparse(X):\\n            X_offset, X_var = mean_variance_axis(\\n                X, axis=0, weights=sample_weight\\n            )\\n            if not return_mean:\\n                X_offset[:] = X.dtype.type(0)\\n        else:\\n            if normalize:\\n                X_offset, X_var, _ = _incremental_mean_and_var(\\n                    X, last_mean=0., last_variance=0., last_sample_count=0.,\\n                    sample_weight=sample_weight\\n                )\\n            else:\\n                X_offset = np.average(X, axis=0, weights=sample_weight)\\n            X_offset = X_offset.astype(X.dtype, copy=False)\\n            X -= X_offset\\n        if normalize:\\n            X_var = X_var.astype(X.dtype, copy=False)\\n            constant_mask = X_var < 10 * np.finfo(X.dtype).eps\\n            X_var *= X.shape[0]\\n            X_scale = np.sqrt(X_var, out=X_var)\\n            X_scale[constant_mask] = 1.\\n            if sp.issparse(X):\\n                inplace_column_scale(X, 1. / X_scale)\\n            else:\\n                X /= X_scale\\n        else:\\n            X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        y_offset = np.average(y, axis=0, weights=sample_weight)\\n        y = y - y_offset\\n    else:\\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        if y.ndim == 1:\\n            y_offset = X.dtype.type(0)\\n        else:\\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\\n    return X, y, X_offset, y_offset, X_scale",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX detect near constant feature in StandardScaler and linear models (#19788)",
    "fixed_code": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True,\\n                     sample_weight=None, return_mean=False, check_input=True):\\n    if isinstance(sample_weight, numbers.Number):\\n        sample_weight = None\\n    if sample_weight is not None:\\n        sample_weight = np.asarray(sample_weight)\\n    if check_input:\\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\\n                        dtype=FLOAT_DTYPES)\\n    elif copy:\\n        if sp.issparse(X):\\n            X = X.copy()\\n        else:\\n            X = X.copy(order='K')\\n    y = np.asarray(y, dtype=X.dtype)\\n    if fit_intercept:\\n        if sp.issparse(X):\\n            X_offset, X_var = mean_variance_axis(\\n                X, axis=0, weights=sample_weight\\n            )\\n            if not return_mean:\\n                X_offset[:] = X.dtype.type(0)\\n        else:\\n            if normalize:\\n                X_offset, X_var, _ = _incremental_mean_and_var(\\n                    X, last_mean=0., last_variance=0., last_sample_count=0.,\\n                    sample_weight=sample_weight\\n                )\\n            else:\\n                X_offset = np.average(X, axis=0, weights=sample_weight)\\n            X_offset = X_offset.astype(X.dtype, copy=False)\\n            X -= X_offset\\n        if normalize:\\n            X_var = X_var.astype(X.dtype, copy=False)\\n            constant_mask = _is_constant_feature(X_var, X_offset, X.shape[0])\\n            X_var *= X.shape[0]\\n            X_scale = np.sqrt(X_var, out=X_var)\\n            X_scale[constant_mask] = 1.\\n            if sp.issparse(X):\\n                inplace_column_scale(X, 1. / X_scale)\\n            else:\\n                X /= X_scale\\n        else:\\n            X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        y_offset = np.average(y, axis=0, weights=sample_weight)\\n        y = y - y_offset\\n    else:\\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\\n        if y.ndim == 1:\\n            y_offset = X.dtype.type(0)\\n        else:\\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\\n    return X, y, X_offset, y_offset, X_scale"
  },
  {
    "code": "def _get_join_indexers(left_keys, right_keys, sort=False, how='inner'):\\n    from functools import partial\\n    assert len(left_keys) == len(right_keys), \\\\n            'left_key and right_keys must be the same length'\\n    fkeys = partial(_factorize_keys, sort=sort)\\n    llab, rlab, shape = map(list, zip( * map(fkeys, left_keys, right_keys)))\\n    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\\n    lkey, rkey, count = fkeys(lkey, rkey)\\n    kwargs = {'sort':sort} if how == 'left' else {}\\n    join_func = _join_functions[how]\\n    return join_func(lkey, rkey, count, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_join_indexers(left_keys, right_keys, sort=False, how='inner'):\\n    from functools import partial\\n    assert len(left_keys) == len(right_keys), \\\\n            'left_key and right_keys must be the same length'\\n    fkeys = partial(_factorize_keys, sort=sort)\\n    llab, rlab, shape = map(list, zip( * map(fkeys, left_keys, right_keys)))\\n    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\\n    lkey, rkey, count = fkeys(lkey, rkey)\\n    kwargs = {'sort':sort} if how == 'left' else {}\\n    join_func = _join_functions[how]\\n    return join_func(lkey, rkey, count, **kwargs)"
  },
  {
    "code": "def read_array(fp, allow_pickle=True, pickle_kwargs=None):\\n\\tversion = read_magic(fp)\\n\\t_check_version(version)\\n\\tshape, fortran_order, dtype = _read_array_header(fp, version)\\n\\tif len(shape) == 0:\\n\\t\\tcount = 1\\n\\telse:\\n\\t\\tcount = numpy.multiply.reduce(shape, dtype=numpy.int64)\\n\\tif dtype.hasobject:\\n\\t\\tif not allow_pickle:\\n\\t\\t\\traise ValueError(\"Object arrays cannot be loaded when \"\\n\\t\\t\\t\\t\\t\\t\\t \"allow_pickle=False\")\\n\\t\\tif pickle_kwargs is None:\\n\\t\\t\\tpickle_kwargs = {}\\n\\t\\ttry:\\n\\t\\t\\tarray = pickle.load(fp, **pickle_kwargs)\\n\\t\\texcept UnicodeError as err:\\n\\t\\t\\tif sys.version_info[0] >= 3:\\n\\t\\t\\t\\traise UnicodeError(\"Unpickling a python object failed: %r\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"You may need to pass the encoding= option \"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"to numpy.load\" % (err,))\\n\\t\\t\\traise\\n\\telse:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray = numpy.fromfile(fp, dtype=dtype, count=count)\\n\\t\\telse:\\n\\t\\t\\tmax_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dtype.itemsize)\\n\\t\\t\\tarray = numpy.empty(count, dtype=dtype)\\n\\t\\t\\tfor i in range(0, count, max_read_count):\\n\\t\\t\\t\\tread_count = min(max_read_count, count - i)\\n\\t\\t\\t\\tread_size = int(read_count * dtype.itemsize)\\n\\t\\t\\t\\tdata = _read_bytes(fp, read_size, \"array data\")\\n\\t\\t\\t\\tarray[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t count=read_count)\\n\\t\\tif fortran_order:\\n\\t\\t\\tarray.shape = shape[::-1]\\n\\t\\t\\tarray = array.transpose()\\n\\t\\telse:\\n\\t\\t\\tarray.shape = shape\\n\\treturn array",
    "label": 1,
    "bug_type": "corruption - memory",
    "bug_description": "BUG: Fixes to reading and writing of empty arrays, and in particular arrays with empty dtypes. See #6430",
    "fixed_code": "def read_array(fp, allow_pickle=True, pickle_kwargs=None):\\n\\tversion = read_magic(fp)\\n\\t_check_version(version)\\n\\tshape, fortran_order, dtype = _read_array_header(fp, version)\\n\\tif len(shape) == 0:\\n\\t\\tcount = 1\\n\\telse:\\n\\t\\tcount = numpy.multiply.reduce(shape, dtype=numpy.int64)\\n\\tif dtype.hasobject:\\n\\t\\tif not allow_pickle:\\n\\t\\t\\traise ValueError(\"Object arrays cannot be loaded when \"\\n\\t\\t\\t\\t\\t\\t\\t \"allow_pickle=False\")\\n\\t\\tif pickle_kwargs is None:\\n\\t\\t\\tpickle_kwargs = {}\\n\\t\\ttry:\\n\\t\\t\\tarray = pickle.load(fp, **pickle_kwargs)\\n\\t\\texcept UnicodeError as err:\\n\\t\\t\\tif sys.version_info[0] >= 3:\\n\\t\\t\\t\\traise UnicodeError(\"Unpickling a python object failed: %r\\n\"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"You may need to pass the encoding= option \"\\n\\t\\t\\t\\t\\t\\t\\t\\t   \"to numpy.load\" % (err,))\\n\\t\\t\\traise\\n\\telse:\\n\\t\\tif isfileobj(fp):\\n\\t\\t\\tarray = numpy.fromfile(fp, dtype=dtype, count=count)\\n\\t\\telse:\\n\\t\\t\\tarray = numpy.ndarray(count, dtype=dtype)\\n\\t\\t\\tif dtype.itemsize > 0:\\n\\t\\t\\t\\tmax_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dtype.itemsize)\\n\\t\\t\\t\\tfor i in range(0, count, max_read_count):\\n\\t\\t\\t\\t\\tread_count = min(max_read_count, count - i)\\n\\t\\t\\t\\t\\tread_size = int(read_count * dtype.itemsize)\\n\\t\\t\\t\\t\\tdata = _read_bytes(fp, read_size, \"array data\")\\n\\t\\t\\t\\t\\tarray[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t count=read_count)\\n\\t\\tif fortran_order:\\n\\t\\t\\tarray.shape = shape[::-1]\\n\\t\\t\\tarray = array.transpose()\\n\\t\\telse:\\n\\t\\t\\tarray.shape = shape\\n\\treturn array"
  },
  {
    "code": "def _average_binary_score(binary_metric, y_true, y_score, average,\\n                          sample_weight=None):\\n    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\\n    if average not in average_options:\\n        raise ValueError('average has to be one of {0}'\\n                         ''.format(average_options))\\n    y_type = type_of_target(y_true)\\n    if y_type not in (\"binary\", \"multilabel-indicator\"):\\n        raise ValueError(\"{0} format is not supported\".format(y_type))\\n    if y_type == \"binary\":\\n        return binary_metric(y_true, y_score, sample_weight=sample_weight)\\n    check_consistent_length(y_true, y_score)\\n    y_true = check_array(y_true)\\n    y_score = check_array(y_score)\\n    not_average_axis = 1\\n    score_weight = sample_weight\\n    average_weight = None\\n    if average == \"micro\":\\n        if score_weight is not None:\\n            score_weight = np.repeat(score_weight, y_true.shape[1])\\n        y_true = y_true.ravel()\\n        y_score = y_score.ravel()\\n    elif average == 'weighted':\\n        if score_weight is not None:\\n            average_weight = np.sum(np.multiply(\\n                y_true, np.reshape(score_weight, (-1, 1))), axis=0)\\n        else:\\n            average_weight = np.sum(y_true, axis=0)\\n        if average_weight.sum() == 0:\\n            return 0\\n    elif average == 'samples':\\n        average_weight = score_weight\\n        score_weight = None\\n        not_average_axis = 0\\n    if y_true.ndim == 1:\\n        y_true = y_true.reshape((-1, 1))\\n    if y_score.ndim == 1:\\n        y_score = y_score.reshape((-1, 1))\\n    n_classes = y_score.shape[not_average_axis]\\n    score = np.zeros((n_classes,))\\n    for c in range(n_classes):\\n        y_true_c = y_true.take([c], axis=not_average_axis).ravel()\\n        y_score_c = y_score.take([c], axis=not_average_axis).ravel()\\n        score[c] = binary_metric(y_true_c, y_score_c,\\n                                 sample_weight=score_weight)\\n    if average is not None:\\n        return np.average(score, weights=average_weight)\\n    else:\\n        return score",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST + FIX ensure that it fails if sample_weight has not proper length",
    "fixed_code": "def _average_binary_score(binary_metric, y_true, y_score, average,\\n                          sample_weight=None):\\n    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\\n    if average not in average_options:\\n        raise ValueError('average has to be one of {0}'\\n                         ''.format(average_options))\\n    y_type = type_of_target(y_true)\\n    if y_type not in (\"binary\", \"multilabel-indicator\"):\\n        raise ValueError(\"{0} format is not supported\".format(y_type))\\n    if y_type == \"binary\":\\n        return binary_metric(y_true, y_score, sample_weight=sample_weight)\\n    check_consistent_length(y_true, y_score, sample_weight)\\n    y_true = check_array(y_true)\\n    y_score = check_array(y_score)\\n    not_average_axis = 1\\n    score_weight = sample_weight\\n    average_weight = None\\n    if average == \"micro\":\\n        if score_weight is not None:\\n            score_weight = np.repeat(score_weight, y_true.shape[1])\\n        y_true = y_true.ravel()\\n        y_score = y_score.ravel()\\n    elif average == 'weighted':\\n        if score_weight is not None:\\n            average_weight = np.sum(np.multiply(\\n                y_true, np.reshape(score_weight, (-1, 1))), axis=0)\\n        else:\\n            average_weight = np.sum(y_true, axis=0)\\n        if average_weight.sum() == 0:\\n            return 0\\n    elif average == 'samples':\\n        average_weight = score_weight\\n        score_weight = None\\n        not_average_axis = 0\\n    if y_true.ndim == 1:\\n        y_true = y_true.reshape((-1, 1))\\n    if y_score.ndim == 1:\\n        y_score = y_score.reshape((-1, 1))\\n    n_classes = y_score.shape[not_average_axis]\\n    score = np.zeros((n_classes,))\\n    for c in range(n_classes):\\n        y_true_c = y_true.take([c], axis=not_average_axis).ravel()\\n        y_score_c = y_score.take([c], axis=not_average_axis).ravel()\\n        score[c] = binary_metric(y_true_c, y_score_c,\\n                                 sample_weight=score_weight)\\n    if average is not None:\\n        return np.average(score, weights=average_weight)\\n    else:\\n        return score"
  },
  {
    "code": "def setdefault(self, key, value):\\n            if key not in self:\\n                self[key] = str(value)\\n            return self[key]\\n    try:\\n        _putenv = putenv\\n    except NameError:\\n        _putenv = lambda key, value: None\\n    else:\\n        __all__.append(\"putenv\")\\n    try:\\n        _unsetenv = unsetenv\\n    except NameError:\\n        _unsetenv = lambda key: _putenv(key, \"\")\\n    else:\\n        __all__.append(\"unsetenv\")\\n    if name in ('os2', 'nt'): \\n        _keymap = lambda key: str(key.upper())\\n    else:  \\n        _keymap = lambda key: str(key)\\n    environ = _Environ(environ, _keymap, _putenv, _unsetenv)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setdefault(self, key, value):\\n            if key not in self:\\n                self[key] = str(value)\\n            return self[key]\\n    try:\\n        _putenv = putenv\\n    except NameError:\\n        _putenv = lambda key, value: None\\n    else:\\n        __all__.append(\"putenv\")\\n    try:\\n        _unsetenv = unsetenv\\n    except NameError:\\n        _unsetenv = lambda key: _putenv(key, \"\")\\n    else:\\n        __all__.append(\"unsetenv\")\\n    if name in ('os2', 'nt'): \\n        _keymap = lambda key: str(key.upper())\\n    else:  \\n        _keymap = lambda key: str(key)\\n    environ = _Environ(environ, _keymap, _putenv, _unsetenv)"
  },
  {
    "code": "def task_decorator_factory(\\n    python_callable: Optional[Callable] = None,\\n    *,\\n    multiple_outputs: Optional[bool] = None,\\n    decorated_operator_class: Type[BaseOperator],\\n    **kwargs,\\n) -> TaskDecorator:\\n    if multiple_outputs is None:\\n        multiple_outputs = cast(bool, attr.NOTHING)\\n    if python_callable:\\n        decorator = _TaskDecorator(\\n            function=python_callable,\\n            multiple_outputs=multiple_outputs,\\n            operator_class=decorated_operator_class,\\n            kwargs=kwargs,\\n        )\\n        return cast(TaskDecorator, decorator)\\n    elif python_callable is not None:\\n        raise TypeError('No args allowed while using @task, use kwargs instead')\\n    decorator_factory = functools.partial(\\n        _TaskDecorator,\\n        multiple_outputs=multiple_outputs,\\n        operator_class=decorated_operator_class,\\n        kwargs=kwargs,\\n    )\\n    return cast(TaskDecorator, decorator_factory)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def task_decorator_factory(\\n    python_callable: Optional[Callable] = None,\\n    *,\\n    multiple_outputs: Optional[bool] = None,\\n    decorated_operator_class: Type[BaseOperator],\\n    **kwargs,\\n) -> TaskDecorator:\\n    if multiple_outputs is None:\\n        multiple_outputs = cast(bool, attr.NOTHING)\\n    if python_callable:\\n        decorator = _TaskDecorator(\\n            function=python_callable,\\n            multiple_outputs=multiple_outputs,\\n            operator_class=decorated_operator_class,\\n            kwargs=kwargs,\\n        )\\n        return cast(TaskDecorator, decorator)\\n    elif python_callable is not None:\\n        raise TypeError('No args allowed while using @task, use kwargs instead')\\n    decorator_factory = functools.partial(\\n        _TaskDecorator,\\n        multiple_outputs=multiple_outputs,\\n        operator_class=decorated_operator_class,\\n        kwargs=kwargs,\\n    )\\n    return cast(TaskDecorator, decorator_factory)"
  },
  {
    "code": "def cluster_image_get_iter(self):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get-iter')\\n        query = netapp_utils.zapi.NaElement('query')\\n        cluster_image_info = netapp_utils.zapi.NaElement('cluster-image-info')\\n        query.add_child_elem(cluster_image_info)\\n        cluster_image_get.add_child_elem(query)\\n        return cluster_image_get\\n    def cluster_image_get(self):\\n        cluster_image_get_iter = self.cluster_image_get_iter()\\n        try:\\n            result = self.server.invoke_successfully(cluster_image_get_iter, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details for %s: %s'\\n                                      % (self.parameters['node'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        if result.get_child_by_name('num-records') and \\\\n                int(result.get_child_content('num-records')) > 0:\\n            return True\\n        return None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Allow to to upgrade more than 1 node at a time.  (#50780)",
    "fixed_code": "def cluster_image_get_iter(self):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get-iter')\\n        query = netapp_utils.zapi.NaElement('query')\\n        cluster_image_info = netapp_utils.zapi.NaElement('cluster-image-info')\\n        query.add_child_elem(cluster_image_info)\\n        cluster_image_get.add_child_elem(query)\\n        return cluster_image_get\\n    def cluster_image_get(self):\\n        cluster_image_get_iter = self.cluster_image_get_iter()\\n        try:\\n            result = self.server.invoke_successfully(cluster_image_get_iter, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details: %s: %s'\\n                                      % (self.parameters['package_version'], to_native(error)),\\n                                  exception=traceback.format_exc())\\n        if result.get_child_by_name('num-records') and \\\\n                int(result.get_child_content('num-records')) > 0:\\n            return True\\n        return None\\n    def cluster_image_get_for_node(self, node_name):\\n        cluster_image_get = netapp_utils.zapi.NaElement('cluster-image-get')\\n        cluster_image_get.add_new_child('node-id', node_name)\\n        try:\\n            self.server.invoke_successfully(cluster_image_get, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg='Error fetching cluster image details for %s: %s'\\n                                      % (node_name, to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def update_expected_environment_variables(env: Dict[str, str]) -> None:\\n    set_value_to_default_if_not_set(env, 'AIRFLOW_CONSTRAINTS_MODE', \"constraints-source-providers\")\\n    set_value_to_default_if_not_set(env, 'AIRFLOW_CONSTRAINTS_REFERENCE', \"constraints-source-providers\")\\n    set_value_to_default_if_not_set(env, 'AIRFLOW_EXTRAS', \"\")\\n    set_value_to_default_if_not_set(env, 'ANSWER', \"\")\\n    set_value_to_default_if_not_set(env, 'BREEZE', \"true\")\\n    set_value_to_default_if_not_set(env, 'BREEZE_INIT_COMMAND', \"\")\\n    set_value_to_default_if_not_set(env, 'CI', \"false\")\\n    set_value_to_default_if_not_set(env, 'CI_BUILD_ID', \"0\")\\n    set_value_to_default_if_not_set(env, 'CI_EVENT_TYPE', \"pull_request\")\\n    set_value_to_default_if_not_set(env, 'CI_JOB_ID', \"0\")\\n    set_value_to_default_if_not_set(env, 'CI_TARGET_BRANCH', AIRFLOW_BRANCH)\\n    set_value_to_default_if_not_set(env, 'CI_TARGET_REPO', \"apache/airflow\")\\n    set_value_to_default_if_not_set(env, 'COMMIT_SHA', commit_sha())\\n    set_value_to_default_if_not_set(env, 'DB_RESET', \"false\")\\n    set_value_to_default_if_not_set(env, 'DEBIAN_VERSION', \"bullseye\")\\n    set_value_to_default_if_not_set(env, 'DEFAULT_BRANCH', AIRFLOW_BRANCH)\\n    set_value_to_default_if_not_set(env, 'ENABLED_SYSTEMS', \"\")\\n    set_value_to_default_if_not_set(env, 'ENABLE_TEST_COVERAGE', \"false\")\\n    set_value_to_default_if_not_set(env, 'GITHUB_REGISTRY_PULL_IMAGE_TAG', \"latest\")\\n    set_value_to_default_if_not_set(env, 'HOST_GROUP_ID', get_host_group_id())\\n    set_value_to_default_if_not_set(env, 'HOST_OS', get_host_os())\\n    set_value_to_default_if_not_set(env, 'HOST_USER_ID', get_host_user_id())\\n    set_value_to_default_if_not_set(env, 'INIT_SCRIPT_FILE', \"init.sh\")\\n    set_value_to_default_if_not_set(env, 'INSTALL_PACKAGES_FROM_CONTEXT', \"false\")\\n    set_value_to_default_if_not_set(env, 'INSTALL_PROVIDERS_FROM_SOURCES', \"true\")\\n    set_value_to_default_if_not_set(env, 'LIST_OF_INTEGRATION_TESTS_TO_RUN', \"\")\\n    set_value_to_default_if_not_set(env, 'LOAD_DEFAULT_CONNECTIONS', \"false\")\\n    set_value_to_default_if_not_set(env, 'LOAD_EXAMPLES', \"false\")\\n    set_value_to_default_if_not_set(env, 'MSSQL_DATA_VOLUME', str(MSSQL_DATA_VOLUME))\\n    set_value_to_default_if_not_set(env, 'PACKAGE_FORMAT', ALLOWED_PACKAGE_FORMATS[0])\\n    set_value_to_default_if_not_set(env, 'PRINT_INFO_FROM_SCRIPTS', \"true\")\\n    set_value_to_default_if_not_set(env, 'PYTHONDONTWRITEBYTECODE', \"true\")\\n    set_value_to_default_if_not_set(env, 'RUN_SYSTEM_TESTS', \"false\")\\n    set_value_to_default_if_not_set(env, 'RUN_TESTS', \"false\")\\n    set_value_to_default_if_not_set(env, 'SKIP_ENVIRONMENT_INITIALIZATION', \"false\")\\n    set_value_to_default_if_not_set(env, 'SKIP_SSH_SETUP', \"false\")\\n    set_value_to_default_if_not_set(env, 'TEST_TYPE', \"\")\\n    set_value_to_default_if_not_set(env, 'TEST_TIMEOUT', \"60\")\\n    set_value_to_default_if_not_set(env, 'UPGRADE_TO_NEWER_DEPENDENCIES', \"false\")\\n    set_value_to_default_if_not_set(env, 'USE_PACKAGES_FROM_DIST', \"false\")\\n    set_value_to_default_if_not_set(env, 'VERBOSE', \"false\")\\n    set_value_to_default_if_not_set(env, 'VERBOSE_COMMANDS', \"false\")\\n    set_value_to_default_if_not_set(env, 'VERSION_SUFFIX_FOR_PYPI', \"\")\\n    set_value_to_default_if_not_set(env, 'WHEEL_VERSION', \"0.36.2\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def update_expected_environment_variables(env: Dict[str, str]) -> None:\\n    set_value_to_default_if_not_set(env, 'AIRFLOW_CONSTRAINTS_MODE', \"constraints-source-providers\")\\n    set_value_to_default_if_not_set(env, 'AIRFLOW_CONSTRAINTS_REFERENCE', \"constraints-source-providers\")\\n    set_value_to_default_if_not_set(env, 'AIRFLOW_EXTRAS', \"\")\\n    set_value_to_default_if_not_set(env, 'ANSWER', \"\")\\n    set_value_to_default_if_not_set(env, 'BREEZE', \"true\")\\n    set_value_to_default_if_not_set(env, 'BREEZE_INIT_COMMAND', \"\")\\n    set_value_to_default_if_not_set(env, 'CI', \"false\")\\n    set_value_to_default_if_not_set(env, 'CI_BUILD_ID', \"0\")\\n    set_value_to_default_if_not_set(env, 'CI_EVENT_TYPE', \"pull_request\")\\n    set_value_to_default_if_not_set(env, 'CI_JOB_ID', \"0\")\\n    set_value_to_default_if_not_set(env, 'CI_TARGET_BRANCH', AIRFLOW_BRANCH)\\n    set_value_to_default_if_not_set(env, 'CI_TARGET_REPO', \"apache/airflow\")\\n    set_value_to_default_if_not_set(env, 'COMMIT_SHA', commit_sha())\\n    set_value_to_default_if_not_set(env, 'DB_RESET', \"false\")\\n    set_value_to_default_if_not_set(env, 'DEBIAN_VERSION', \"bullseye\")\\n    set_value_to_default_if_not_set(env, 'DEFAULT_BRANCH', AIRFLOW_BRANCH)\\n    set_value_to_default_if_not_set(env, 'ENABLED_SYSTEMS', \"\")\\n    set_value_to_default_if_not_set(env, 'ENABLE_TEST_COVERAGE', \"false\")\\n    set_value_to_default_if_not_set(env, 'GITHUB_REGISTRY_PULL_IMAGE_TAG', \"latest\")\\n    set_value_to_default_if_not_set(env, 'HOST_GROUP_ID', get_host_group_id())\\n    set_value_to_default_if_not_set(env, 'HOST_OS', get_host_os())\\n    set_value_to_default_if_not_set(env, 'HOST_USER_ID', get_host_user_id())\\n    set_value_to_default_if_not_set(env, 'INIT_SCRIPT_FILE', \"init.sh\")\\n    set_value_to_default_if_not_set(env, 'INSTALL_PACKAGES_FROM_CONTEXT', \"false\")\\n    set_value_to_default_if_not_set(env, 'INSTALL_PROVIDERS_FROM_SOURCES', \"true\")\\n    set_value_to_default_if_not_set(env, 'LIST_OF_INTEGRATION_TESTS_TO_RUN', \"\")\\n    set_value_to_default_if_not_set(env, 'LOAD_DEFAULT_CONNECTIONS', \"false\")\\n    set_value_to_default_if_not_set(env, 'LOAD_EXAMPLES', \"false\")\\n    set_value_to_default_if_not_set(env, 'MSSQL_DATA_VOLUME', str(MSSQL_DATA_VOLUME))\\n    set_value_to_default_if_not_set(env, 'PACKAGE_FORMAT', ALLOWED_PACKAGE_FORMATS[0])\\n    set_value_to_default_if_not_set(env, 'PRINT_INFO_FROM_SCRIPTS', \"true\")\\n    set_value_to_default_if_not_set(env, 'PYTHONDONTWRITEBYTECODE', \"true\")\\n    set_value_to_default_if_not_set(env, 'RUN_SYSTEM_TESTS', \"false\")\\n    set_value_to_default_if_not_set(env, 'RUN_TESTS', \"false\")\\n    set_value_to_default_if_not_set(env, 'SKIP_ENVIRONMENT_INITIALIZATION', \"false\")\\n    set_value_to_default_if_not_set(env, 'SKIP_SSH_SETUP', \"false\")\\n    set_value_to_default_if_not_set(env, 'TEST_TYPE', \"\")\\n    set_value_to_default_if_not_set(env, 'TEST_TIMEOUT', \"60\")\\n    set_value_to_default_if_not_set(env, 'UPGRADE_TO_NEWER_DEPENDENCIES', \"false\")\\n    set_value_to_default_if_not_set(env, 'USE_PACKAGES_FROM_DIST', \"false\")\\n    set_value_to_default_if_not_set(env, 'VERBOSE', \"false\")\\n    set_value_to_default_if_not_set(env, 'VERBOSE_COMMANDS', \"false\")\\n    set_value_to_default_if_not_set(env, 'VERSION_SUFFIX_FOR_PYPI', \"\")\\n    set_value_to_default_if_not_set(env, 'WHEEL_VERSION', \"0.36.2\")"
  },
  {
    "code": "def gis_operators(self):\\n        operators = {\\n            'bbcontains': SpatialOperator(func='MBRContains'),  \\n            'bboverlaps': SpatialOperator(func='MBROverlaps'),  \\n            'contained': SpatialOperator(func='MBRWithin'),  \\n            'contains': SpatialOperator(func='ST_Contains'),\\n            'crosses': SpatialOperator(func='ST_Crosses'),\\n            'disjoint': SpatialOperator(func='ST_Disjoint'),\\n            'equals': SpatialOperator(func='ST_Equals'),\\n            'exact': SpatialOperator(func='ST_Equals'),\\n            'intersects': SpatialOperator(func='ST_Intersects'),\\n            'overlaps': SpatialOperator(func='ST_Overlaps'),\\n            'same_as': SpatialOperator(func='ST_Equals'),\\n            'touches': SpatialOperator(func='ST_Touches'),\\n            'within': SpatialOperator(func='ST_Within'),\\n        }\\n        if self.connection.mysql_is_mariadb:\\n            operators['relate'] = SpatialOperator(func='ST_Relate')\\n        return operators\\n    disallowed_aggregates = (\\n        aggregates.Collect, aggregates.Extent, aggregates.Extent3D,\\n        aggregates.MakeLine, aggregates.Union,\\n    )\\n    @cached_property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def gis_operators(self):\\n        operators = {\\n            'bbcontains': SpatialOperator(func='MBRContains'),  \\n            'bboverlaps': SpatialOperator(func='MBROverlaps'),  \\n            'contained': SpatialOperator(func='MBRWithin'),  \\n            'contains': SpatialOperator(func='ST_Contains'),\\n            'crosses': SpatialOperator(func='ST_Crosses'),\\n            'disjoint': SpatialOperator(func='ST_Disjoint'),\\n            'equals': SpatialOperator(func='ST_Equals'),\\n            'exact': SpatialOperator(func='ST_Equals'),\\n            'intersects': SpatialOperator(func='ST_Intersects'),\\n            'overlaps': SpatialOperator(func='ST_Overlaps'),\\n            'same_as': SpatialOperator(func='ST_Equals'),\\n            'touches': SpatialOperator(func='ST_Touches'),\\n            'within': SpatialOperator(func='ST_Within'),\\n        }\\n        if self.connection.mysql_is_mariadb:\\n            operators['relate'] = SpatialOperator(func='ST_Relate')\\n        return operators\\n    disallowed_aggregates = (\\n        aggregates.Collect, aggregates.Extent, aggregates.Extent3D,\\n        aggregates.MakeLine, aggregates.Union,\\n    )\\n    @cached_property"
  },
  {
    "code": "def median(self, engine=None, engine_kwargs=None, **kwargs):\\n        if maybe_use_numba(engine):\\n            if self.method == \"table\":\\n                func = generate_manual_numpy_nan_agg_with_axis(np.nanmedian)\\n            else:\\n                func = np.nanmedian\\n            return self.apply(\\n                func,\\n                raw=True,\\n                engine=engine,\\n                engine_kwargs=engine_kwargs,\\n            )\\n        window_func = window_aggregations.roll_median_c\\n        return self._apply(window_func, name=\"median\", **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def median(self, engine=None, engine_kwargs=None, **kwargs):\\n        if maybe_use_numba(engine):\\n            if self.method == \"table\":\\n                func = generate_manual_numpy_nan_agg_with_axis(np.nanmedian)\\n            else:\\n                func = np.nanmedian\\n            return self.apply(\\n                func,\\n                raw=True,\\n                engine=engine,\\n                engine_kwargs=engine_kwargs,\\n            )\\n        window_func = window_aggregations.roll_median_c\\n        return self._apply(window_func, name=\"median\", **kwargs)"
  },
  {
    "code": "def sql_schema(self):\\n        return str(self.table.compile())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/TST: fix get_schema + add tests\\n\\n- adapt signature of get_schema to what it was before (for backwards compatibility), possibility to call function without connection object\\n- fix get_schema for sqlalchemy mode\\n- add tests",
    "fixed_code": "def sql_schema(self):\\n        from sqlalchemy.schema import CreateTable\\n        return str(CreateTable(self.table))"
  },
  {
    "code": "def post_dag_run(dag_id, session):\\n    if not session.query(DagModel).filter(DagModel.dag_id == dag_id).first():\\n        raise NotFound(title=\"DAG not found\", detail=f\"DAG with dag_id: '{dag_id}' not found\")\\n    try:\\n        post_body = dagrun_schema.load(request.json, session=session)\\n    except ValidationError as err:\\n        raise BadRequest(detail=str(err))\\n    dagrun_instance = (\\n        session.query(DagRun)\\n        .filter(\\n            DagRun.dag_id == dag_id,\\n            or_(DagRun.run_id == post_body[\"run_id\"], DagRun.execution_date == post_body[\"execution_date\"]),\\n        )\\n        .first()\\n    )\\n    if not dagrun_instance:\\n        dag_run = DagRun(dag_id=dag_id, run_type=DagRunType.MANUAL, **post_body)\\n        session.add(dag_run)\\n        session.commit()\\n        return dagrun_schema.dump(dag_run)\\n    if dagrun_instance.execution_date == post_body[\"execution_date\"]:\\n        raise AlreadyExists(\\n            detail=f\"DAGRun with DAG ID: '{dag_id}' and \"\\n            f\"DAGRun ExecutionDate: '{post_body['execution_date']}' already exists\"\\n        )\\n    raise AlreadyExists(\\n        detail=f\"DAGRun with DAG ID: '{dag_id}' and DAGRun ID: '{post_body['run_id']}' already exists\"\\n    )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def post_dag_run(dag_id, session):\\n    if not session.query(DagModel).filter(DagModel.dag_id == dag_id).first():\\n        raise NotFound(title=\"DAG not found\", detail=f\"DAG with dag_id: '{dag_id}' not found\")\\n    try:\\n        post_body = dagrun_schema.load(request.json, session=session)\\n    except ValidationError as err:\\n        raise BadRequest(detail=str(err))\\n    dagrun_instance = (\\n        session.query(DagRun)\\n        .filter(\\n            DagRun.dag_id == dag_id,\\n            or_(DagRun.run_id == post_body[\"run_id\"], DagRun.execution_date == post_body[\"execution_date\"]),\\n        )\\n        .first()\\n    )\\n    if not dagrun_instance:\\n        dag_run = DagRun(dag_id=dag_id, run_type=DagRunType.MANUAL, **post_body)\\n        session.add(dag_run)\\n        session.commit()\\n        return dagrun_schema.dump(dag_run)\\n    if dagrun_instance.execution_date == post_body[\"execution_date\"]:\\n        raise AlreadyExists(\\n            detail=f\"DAGRun with DAG ID: '{dag_id}' and \"\\n            f\"DAGRun ExecutionDate: '{post_body['execution_date']}' already exists\"\\n        )\\n    raise AlreadyExists(\\n        detail=f\"DAGRun with DAG ID: '{dag_id}' and DAGRun ID: '{post_body['run_id']}' already exists\"\\n    )"
  },
  {
    "code": "def show_versions(as_json=False):\\n    sys_info = get_sys_info()\\n    deps = [\\n        (\"pandas\", lambda mod: mod.__version__),\\n        (\"pytest\", lambda mod: mod.__version__),\\n        (\"pip\", lambda mod: mod.__version__),\\n        (\"setuptools\", lambda mod: mod.__version__),\\n        (\"Cython\", lambda mod: mod.__version__),\\n        (\"numpy\", lambda mod: mod.version.version),\\n        (\"scipy\", lambda mod: mod.version.version),\\n        (\"pyarrow\", lambda mod: mod.__version__),\\n        (\"xarray\", lambda mod: mod.__version__),\\n        (\"IPython\", lambda mod: mod.__version__),\\n        (\"sphinx\", lambda mod: mod.__version__),\\n        (\"patsy\", lambda mod: mod.__version__),\\n        (\"dateutil\", lambda mod: mod.__version__),\\n        (\"pytz\", lambda mod: mod.VERSION),\\n        (\"blosc\", lambda mod: mod.__version__),\\n        (\"bottleneck\", lambda mod: mod.__version__),\\n        (\"tables\", lambda mod: mod.__version__),\\n        (\"numexpr\", lambda mod: mod.__version__),\\n        (\"feather\", lambda mod: mod.__version__),\\n        (\"matplotlib\", lambda mod: mod.__version__),\\n        (\"openpyxl\", lambda mod: mod.__version__),\\n        (\"xlrd\", lambda mod: mod.__VERSION__),\\n        (\"xlwt\", lambda mod: mod.__VERSION__),\\n        (\"xlsxwriter\", lambda mod: mod.__version__),\\n        (\"lxml.etree\", lambda mod: mod.__version__),\\n        (\"bs4\", lambda mod: mod.__version__),\\n        (\"html5lib\", lambda mod: mod.__version__),\\n        (\"sqlalchemy\", lambda mod: mod.__version__),\\n        (\"pymysql\", lambda mod: mod.__version__),\\n        (\"psycopg2\", lambda mod: mod.__version__),\\n        (\"jinja2\", lambda mod: mod.__version__),\\n        (\"s3fs\", lambda mod: mod.__version__),\\n        (\"fastparquet\", lambda mod: mod.__version__),\\n        (\"pandas_gbq\", lambda mod: mod.__version__),\\n        (\"pandas_datareader\", lambda mod: mod.__version__),\\n        (\"gcsfs\", lambda mod: mod.__version__),\\n    ]\\n    deps_blob = list()\\n    for (modname, ver_f) in deps:\\n        try:\\n            if modname in sys.modules:\\n                mod = sys.modules[modname]\\n            else:\\n                mod = importlib.import_module(modname)\\n            ver = ver_f(mod)\\n            deps_blob.append((modname, ver))\\n        except ImportError:\\n            deps_blob.append((modname, None))\\n    if (as_json):\\n        try:\\n            import json\\n        except ImportError:\\n            import simplejson as json\\n        j = dict(system=dict(sys_info), dependencies=dict(deps_blob))\\n        if as_json is True:\\n            print(j)\\n        else:\\n            with codecs.open(as_json, \"wb\", encoding='utf8') as f:\\n                json.dump(j, f, indent=2)\\n    else:\\n        print(\"\\nINSTALLED VERSIONS\")\\n        print(\"------------------\")\\n        for k, stat in sys_info:\\n            print(\"{k}: {stat}\".format(k=k, stat=stat))\\n        print(\"\")\\n        for k, stat in deps_blob:\\n            print(\"{k}: {stat}\".format(k=k, stat=stat))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "CLN: Deduplicate show_versions (#26816)",
    "fixed_code": "def show_versions(as_json=False):\\n    sys_info = get_sys_info()\\n    deps = [\\n        'pandas',\\n        'numpy',\\n        'pytz',\\n        'dateutil',\\n        'pip',\\n        'setuptools',\\n        'Cython',\\n        'pytest',\\n        'hypothesis',\\n        \"sphinx\",\\n        \"blosc\",\\n        \"feather\",\\n        \"xlsxwriter\",\\n        \"lxml.etree\",\\n        \"html5lib\",\\n        \"pymysql\",\\n        \"psycopg2\",\\n        \"jinja2\",\\n        \"IPython\",\\n        \"pandas_datareader\",\\n    ]\\n    deps.extend(list(VERSIONS))\\n    deps_blob = []\\n    for modname in deps:\\n        mod = import_optional_dependency(modname,\\n                                         raise_on_missing=False,\\n                                         on_version=\"ignore\")\\n        if mod:\\n            ver = _get_version(mod)\\n        else:\\n            ver = None\\n        deps_blob.append((modname, ver))\\n    if as_json:\\n        try:\\n            import json\\n        except ImportError:\\n            import simplejson as json\\n        j = dict(system=dict(sys_info), dependencies=dict(deps_blob))\\n        if as_json is True:\\n            print(j)\\n        else:\\n            with codecs.open(as_json, \"wb\", encoding='utf8') as f:\\n                json.dump(j, f, indent=2)\\n    else:\\n        maxlen = max(len(x) for x in deps)\\n        tpl = '{{k:<{maxlen}}}: {{stat}}'.format(maxlen=maxlen)\\n        print(\"\\nINSTALLED VERSIONS\")\\n        print(\"------------------\")\\n        for k, stat in sys_info:\\n            print(tpl.format(k=k, stat=stat))\\n        print(\"\")\\n        for k, stat in deps_blob:\\n            print(tpl.format(k=k, stat=stat))"
  },
  {
    "code": "def __mul__(self, other):\\n\\t\\tif isinstance(other, NUMERIC_TYPES):\\n\\t\\t\\treturn self.__class__(default_unit=self._default_unit,\\n\\t\\t\\t\\t**{self.STANDARD_UNIT: (self.standard * other)})\\n\\t\\telse:\\n\\t\\t\\traise TypeError('%(class)s must be multiplied with number' % {\"class\": pretty_name(self)})",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __mul__(self, other):\\n\\t\\tif isinstance(other, NUMERIC_TYPES):\\n\\t\\t\\treturn self.__class__(default_unit=self._default_unit,\\n\\t\\t\\t\\t**{self.STANDARD_UNIT: (self.standard * other)})\\n\\t\\telse:\\n\\t\\t\\traise TypeError('%(class)s must be multiplied with number' % {\"class\": pretty_name(self)})"
  },
  {
    "code": "def _validate_join_method(method):\\n    if method not in ['left', 'right', 'inner', 'outer']:\\n        raise Exception('do not recognize join method %s' % method)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Make core/index exceptions more descriptive\\n\\n  produce copies that are not the same object. (uses `assert_almost_equal`\\n  under the hood).\\n  after iterable check)",
    "fixed_code": "def _validate_join_method(method):\\n    if method not in ['left', 'right', 'inner', 'outer']:\\n        raise ValueError('do not recognize join method %s' % method)"
  },
  {
    "code": "def format_sizeof(num, suffix=''):\\n\\tfor unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\\n\\t\\tif abs(num) < 999.95:\\n\\t\\t\\tif abs(num) < 99.95:\\n\\t\\t\\t\\tif abs(num) < 9.995:\\n\\t\\t\\t\\t\\treturn '{0:1.2f}'.format(num) + unit + suffix\\n\\t\\t\\t\\treturn '{0:2.1f}'.format(num) + unit + suffix\\n\\t\\t\\treturn '{0:3.0f}'.format(num) + unit + suffix\\n\\t\\tnum /= 1000.0\\n\\treturn '{0:3.1f}Y'.format(num) + suffix",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def format_sizeof(num, suffix=''):\\n\\tfor unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\\n\\t\\tif abs(num) < 999.95:\\n\\t\\t\\tif abs(num) < 99.95:\\n\\t\\t\\t\\tif abs(num) < 9.995:\\n\\t\\t\\t\\t\\treturn '{0:1.2f}'.format(num) + unit + suffix\\n\\t\\t\\t\\treturn '{0:2.1f}'.format(num) + unit + suffix\\n\\t\\t\\treturn '{0:3.0f}'.format(num) + unit + suffix\\n\\t\\tnum /= 1000.0\\n\\treturn '{0:3.1f}Y'.format(num) + suffix"
  },
  {
    "code": "def _read_group(self, group, where=None):\\n        kind = group._v_attrs.pandas_type\\n        kind = _LEGACY_MAP.get(kind, kind)\\n        handler = self._get_handler(op='read', kind=kind)\\n        v = handler(group, where)\\n        if v is not None:\\n            meta = getattr(group._v_attrs,'meta',None)\\n            if meta is not None:\\n                v.meta = meta\\n        return v",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: better detection of legacy_frame tables (and ability to force certain typ's)",
    "fixed_code": "def _read_group(self, group, where=None, **kwargs):\\n        kind = group._v_attrs.pandas_type\\n        kind = _LEGACY_MAP.get(kind, kind)\\n        handler = self._get_handler(op='read', kind=kind)\\n        v = handler(group, where, **kwargs)\\n        if v is not None:\\n            meta = getattr(group._v_attrs,'meta',None)\\n            if meta is not None:\\n                v.meta = meta\\n        return v"
  },
  {
    "code": "def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\\n    if not target_versions:\\n        return GRAMMARS\\n    elif all(not version.is_python2() for version in target_versions):\\n        return [\\n            pygram.python_grammar_no_print_statement_no_exec_statement,\\n            pygram.python_grammar_no_print_statement,\\n        ]\\n    else:\\n        return [pygram.python_grammar]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix print() function on Python 2 (#754)\\n\\nFixes #752",
    "fixed_code": "def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\\n    if not target_versions:\\n        return GRAMMARS\\n    elif all(not version.is_python2() for version in target_versions):\\n        return [\\n            pygram.python_grammar_no_print_statement_no_exec_statement,\\n            pygram.python_grammar_no_print_statement,\\n        ]\\n    else:\\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]"
  },
  {
    "code": "def lsplit(self, string):\\n        compiled = self.get_compiled()\\n        t = compiled.split(string) \\n        if len(t) < 3: return\\n        lhs = t[0].strip()\\n        pattern_match = t[1].strip()\\n        rhs = (''.join(t[2:])).strip()\\n        assert abs(self).match(pattern_match),`pattern_match`\\n        return lhs, pattern_match, rhs",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def lsplit(self, string):\\n        compiled = self.get_compiled()\\n        t = compiled.split(string) \\n        if len(t) < 3: return\\n        lhs = t[0].strip()\\n        pattern_match = t[1].strip()\\n        rhs = (''.join(t[2:])).strip()\\n        assert abs(self).match(pattern_match),`pattern_match`\\n        return lhs, pattern_match, rhs"
  },
  {
    "code": "def database_backwards(self, app_label, schema_editor, from_state, to_state):\\n        if not router.allow_migrate(schema_editor.connection.alias, app_label):\\n            return\\n        if self.extension_exists(schema_editor, self.name):\\n            schema_editor.execute(\\n                'DROP EXTENSION %s' % schema_editor.quote_name(self.name)\\n            )\\n        get_hstore_oids.cache_clear()\\n        get_citext_oids.cache_clear()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def database_backwards(self, app_label, schema_editor, from_state, to_state):\\n        if not router.allow_migrate(schema_editor.connection.alias, app_label):\\n            return\\n        if self.extension_exists(schema_editor, self.name):\\n            schema_editor.execute(\\n                'DROP EXTENSION %s' % schema_editor.quote_name(self.name)\\n            )\\n        get_hstore_oids.cache_clear()\\n        get_citext_oids.cache_clear()"
  },
  {
    "code": "def process_response(self, request, response):\\n        if response.status_code != 200 or len(response.content) < 200:\\n            return response\\n        patch_vary_headers(response, ('Accept-Encoding',))\\n        if response.has_header('Content-Encoding'):\\n            return response\\n        if \"msie\" in request.META.get('HTTP_USER_AGENT', '').lower():\\n            ctype = response.get('Content-Type', '').lower()\\n            if not ctype.startswith(\"text/\") or \"javascript\" in ctype:\\n                return response\\n        ae = request.META.get('HTTP_ACCEPT_ENCODING', '')\\n        if not re_accepts_gzip.search(ae):\\n            return response\\n        response.content = compress_string(response.content)\\n        response['Content-Encoding'] = 'gzip'\\n        response['Content-Length'] = str(len(response.content))\\n        return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #10762, #17514 -- Prevented the GZip middleware from returning a response longer than the original content, allowed compression of non-200 responses, and added tests (there were none). Thanks cannona for the initial patch.",
    "fixed_code": "def process_response(self, request, response):\\n        if len(response.content) < 200:\\n            return response\\n        patch_vary_headers(response, ('Accept-Encoding',))\\n        if response.has_header('Content-Encoding'):\\n            return response\\n        if \"msie\" in request.META.get('HTTP_USER_AGENT', '').lower():\\n            ctype = response.get('Content-Type', '').lower()\\n            if not ctype.startswith(\"text/\") or \"javascript\" in ctype:\\n                return response\\n        ae = request.META.get('HTTP_ACCEPT_ENCODING', '')\\n        if not re_accepts_gzip.search(ae):\\n            return response\\n        compressed_content = compress_string(response.content)\\n        if len(compressed_content) >= len(response.content):\\n            return response\\n        response.content = compressed_content\\n        response['Content-Encoding'] = 'gzip'\\n        response['Content-Length'] = str(len(response.content))\\n        return response"
  },
  {
    "code": "def read_settings(self):\\n        ''\\n        if six.PY3:\\n            config = configparser.ConfigParser()\\n        else:\\n            config = configparser.SafeConfigParser()\\n        ec2_default_ini_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'ec2.ini')\\n        ec2_ini_path = os.path.expanduser(os.path.expandvars(os.environ.get('EC2_INI_PATH', ec2_default_ini_path)))\\n        config.read(ec2_ini_path)\\n        self.eucalyptus_host = None\\n        self.eucalyptus = False\\n        if config.has_option('ec2', 'eucalyptus'):\\n            self.eucalyptus = config.getboolean('ec2', 'eucalyptus')\\n        if self.eucalyptus and config.has_option('ec2', 'eucalyptus_host'):\\n            self.eucalyptus_host = config.get('ec2', 'eucalyptus_host')\\n        self.regions = []\\n        configRegions = config.get('ec2', 'regions')\\n        configRegions_exclude = config.get('ec2', 'regions_exclude')\\n        if (configRegions == 'all'):\\n            if self.eucalyptus_host:\\n                self.regions.append(boto.connect_euca(host=self.eucalyptus_host).region.name)\\n            else:\\n                for regionInfo in ec2.regions():\\n                    if regionInfo.name not in configRegions_exclude:\\n                        self.regions.append(regionInfo.name)\\n        else:\\n            self.regions = configRegions.split(\",\")\\n        self.destination_variable = config.get('ec2', 'destination_variable')\\n        self.vpc_destination_variable = config.get('ec2', 'vpc_destination_variable')\\n        self.route53_enabled = config.getboolean('ec2', 'route53')\\n        self.route53_excluded_zones = []\\n        if config.has_option('ec2', 'route53_excluded_zones'):\\n            self.route53_excluded_zones.extend(\\n                config.get('ec2', 'route53_excluded_zones', '').split(','))\\n        self.rds_enabled = True\\n        if config.has_option('ec2', 'rds'):\\n            self.rds_enabled = config.getboolean('ec2', 'rds')\\n        self.elasticache_enabled = True\\n        if config.has_option('ec2', 'elasticache'):\\n            self.elasticache_enabled = config.getboolean('ec2', 'elasticache')\\n        if config.has_option('ec2', 'all_instances'):\\n            self.all_instances = config.getboolean('ec2', 'all_instances')\\n        else:\\n            self.all_instances = False\\n        ec2_valid_instance_states = [\\n            'pending',\\n            'running',\\n            'shutting-down',\\n            'terminated',\\n            'stopping',\\n            'stopped'\\n        ]\\n        self.ec2_instance_states = []\\n        if self.all_instances:\\n            self.ec2_instance_states = ec2_valid_instance_states\\n        elif config.has_option('ec2', 'instance_states'):\\n          for instance_state in config.get('ec2', 'instance_states').split(','):\\n            instance_state = instance_state.strip()\\n            if instance_state not in ec2_valid_instance_states:\\n              continue\\n            self.ec2_instance_states.append(instance_state)\\n        else:\\n          self.ec2_instance_states = ['running']\\n        if config.has_option('ec2', 'all_rds_instances') and self.rds_enabled:\\n            self.all_rds_instances = config.getboolean('ec2', 'all_rds_instances')\\n        else:\\n            self.all_rds_instances = False\\n        if config.has_option('ec2', 'all_elasticache_replication_groups') and self.elasticache_enabled:\\n            self.all_elasticache_replication_groups = config.getboolean('ec2', 'all_elasticache_replication_groups')\\n        else:\\n            self.all_elasticache_replication_groups = False\\n        if config.has_option('ec2', 'all_elasticache_clusters') and self.elasticache_enabled:\\n            self.all_elasticache_clusters = config.getboolean('ec2', 'all_elasticache_clusters')\\n        else:\\n            self.all_elasticache_clusters = False\\n        if config.has_option('ec2', 'all_elasticache_nodes') and self.elasticache_enabled:\\n            self.all_elasticache_nodes = config.getboolean('ec2', 'all_elasticache_nodes')\\n        else:\\n            self.all_elasticache_nodes = False\\n        cache_dir = os.path.expanduser(config.get('ec2', 'cache_path'))\\n        if not os.path.exists(cache_dir):\\n            os.makedirs(cache_dir)\\n        self.cache_path_cache = cache_dir + \"/ansible-ec2.cache\"\\n        self.cache_path_index = cache_dir + \"/ansible-ec2.index\"\\n        self.cache_max_age = config.getint('ec2', 'cache_max_age')\\n        if config.has_option('ec2', 'nested_groups'):\\n            self.nested_groups = config.getboolean('ec2', 'nested_groups')\\n        else:\\n            self.nested_groups = False\\n        group_by_options = [\\n            'group_by_instance_id',\\n            'group_by_region',\\n            'group_by_availability_zone',\\n            'group_by_ami_id',\\n            'group_by_instance_type',\\n            'group_by_key_pair',\\n            'group_by_vpc_id',\\n            'group_by_security_group',\\n            'group_by_tag_keys',\\n            'group_by_tag_none',\\n            'group_by_route53_names',\\n            'group_by_rds_engine',\\n            'group_by_rds_parameter_group',\\n            'group_by_elasticache_engine',\\n            'group_by_elasticache_cluster',\\n            'group_by_elasticache_parameter_group',\\n            'group_by_elasticache_replication_group',\\n        ]\\n        for option in group_by_options:\\n            if config.has_option('ec2', option):\\n                setattr(self, option, config.getboolean('ec2', option))\\n            else:\\n                setattr(self, option, True)\\n        try:\\n            pattern_include = config.get('ec2', 'pattern_include')\\n            if pattern_include and len(pattern_include) > 0:\\n                self.pattern_include = re.compile(pattern_include)\\n            else:\\n                self.pattern_include = None\\n        except configparser.NoOptionError as e:\\n            self.pattern_include = None\\n        try:\\n            pattern_exclude = config.get('ec2', 'pattern_exclude');\\n            if pattern_exclude and len(pattern_exclude) > 0:\\n                self.pattern_exclude = re.compile(pattern_exclude)\\n            else:\\n                self.pattern_exclude = None\\n        except configparser.NoOptionError as e:\\n            self.pattern_exclude = None\\n        self.ec2_instance_filters = defaultdict(list)\\n        if config.has_option('ec2', 'instance_filters'):\\n            for instance_filter in config.get('ec2', 'instance_filters', '').split(','):\\n                instance_filter = instance_filter.strip()\\n                if not instance_filter or '=' not in instance_filter:\\n                    continue\\n                filter_key, filter_value = [x.strip() for x in instance_filter.split('=', 1)]\\n                if not filter_key:\\n                    continue\\n                self.ec2_instance_filters[filter_key].append(filter_value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def read_settings(self):\\n        ''\\n        if six.PY3:\\n            config = configparser.ConfigParser()\\n        else:\\n            config = configparser.SafeConfigParser()\\n        ec2_default_ini_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'ec2.ini')\\n        ec2_ini_path = os.path.expanduser(os.path.expandvars(os.environ.get('EC2_INI_PATH', ec2_default_ini_path)))\\n        config.read(ec2_ini_path)\\n        self.eucalyptus_host = None\\n        self.eucalyptus = False\\n        if config.has_option('ec2', 'eucalyptus'):\\n            self.eucalyptus = config.getboolean('ec2', 'eucalyptus')\\n        if self.eucalyptus and config.has_option('ec2', 'eucalyptus_host'):\\n            self.eucalyptus_host = config.get('ec2', 'eucalyptus_host')\\n        self.regions = []\\n        configRegions = config.get('ec2', 'regions')\\n        configRegions_exclude = config.get('ec2', 'regions_exclude')\\n        if (configRegions == 'all'):\\n            if self.eucalyptus_host:\\n                self.regions.append(boto.connect_euca(host=self.eucalyptus_host).region.name)\\n            else:\\n                for regionInfo in ec2.regions():\\n                    if regionInfo.name not in configRegions_exclude:\\n                        self.regions.append(regionInfo.name)\\n        else:\\n            self.regions = configRegions.split(\",\")\\n        self.destination_variable = config.get('ec2', 'destination_variable')\\n        self.vpc_destination_variable = config.get('ec2', 'vpc_destination_variable')\\n        self.route53_enabled = config.getboolean('ec2', 'route53')\\n        self.route53_excluded_zones = []\\n        if config.has_option('ec2', 'route53_excluded_zones'):\\n            self.route53_excluded_zones.extend(\\n                config.get('ec2', 'route53_excluded_zones', '').split(','))\\n        self.rds_enabled = True\\n        if config.has_option('ec2', 'rds'):\\n            self.rds_enabled = config.getboolean('ec2', 'rds')\\n        self.elasticache_enabled = True\\n        if config.has_option('ec2', 'elasticache'):\\n            self.elasticache_enabled = config.getboolean('ec2', 'elasticache')\\n        if config.has_option('ec2', 'all_instances'):\\n            self.all_instances = config.getboolean('ec2', 'all_instances')\\n        else:\\n            self.all_instances = False\\n        ec2_valid_instance_states = [\\n            'pending',\\n            'running',\\n            'shutting-down',\\n            'terminated',\\n            'stopping',\\n            'stopped'\\n        ]\\n        self.ec2_instance_states = []\\n        if self.all_instances:\\n            self.ec2_instance_states = ec2_valid_instance_states\\n        elif config.has_option('ec2', 'instance_states'):\\n          for instance_state in config.get('ec2', 'instance_states').split(','):\\n            instance_state = instance_state.strip()\\n            if instance_state not in ec2_valid_instance_states:\\n              continue\\n            self.ec2_instance_states.append(instance_state)\\n        else:\\n          self.ec2_instance_states = ['running']\\n        if config.has_option('ec2', 'all_rds_instances') and self.rds_enabled:\\n            self.all_rds_instances = config.getboolean('ec2', 'all_rds_instances')\\n        else:\\n            self.all_rds_instances = False\\n        if config.has_option('ec2', 'all_elasticache_replication_groups') and self.elasticache_enabled:\\n            self.all_elasticache_replication_groups = config.getboolean('ec2', 'all_elasticache_replication_groups')\\n        else:\\n            self.all_elasticache_replication_groups = False\\n        if config.has_option('ec2', 'all_elasticache_clusters') and self.elasticache_enabled:\\n            self.all_elasticache_clusters = config.getboolean('ec2', 'all_elasticache_clusters')\\n        else:\\n            self.all_elasticache_clusters = False\\n        if config.has_option('ec2', 'all_elasticache_nodes') and self.elasticache_enabled:\\n            self.all_elasticache_nodes = config.getboolean('ec2', 'all_elasticache_nodes')\\n        else:\\n            self.all_elasticache_nodes = False\\n        cache_dir = os.path.expanduser(config.get('ec2', 'cache_path'))\\n        if not os.path.exists(cache_dir):\\n            os.makedirs(cache_dir)\\n        self.cache_path_cache = cache_dir + \"/ansible-ec2.cache\"\\n        self.cache_path_index = cache_dir + \"/ansible-ec2.index\"\\n        self.cache_max_age = config.getint('ec2', 'cache_max_age')\\n        if config.has_option('ec2', 'nested_groups'):\\n            self.nested_groups = config.getboolean('ec2', 'nested_groups')\\n        else:\\n            self.nested_groups = False\\n        group_by_options = [\\n            'group_by_instance_id',\\n            'group_by_region',\\n            'group_by_availability_zone',\\n            'group_by_ami_id',\\n            'group_by_instance_type',\\n            'group_by_key_pair',\\n            'group_by_vpc_id',\\n            'group_by_security_group',\\n            'group_by_tag_keys',\\n            'group_by_tag_none',\\n            'group_by_route53_names',\\n            'group_by_rds_engine',\\n            'group_by_rds_parameter_group',\\n            'group_by_elasticache_engine',\\n            'group_by_elasticache_cluster',\\n            'group_by_elasticache_parameter_group',\\n            'group_by_elasticache_replication_group',\\n        ]\\n        for option in group_by_options:\\n            if config.has_option('ec2', option):\\n                setattr(self, option, config.getboolean('ec2', option))\\n            else:\\n                setattr(self, option, True)\\n        try:\\n            pattern_include = config.get('ec2', 'pattern_include')\\n            if pattern_include and len(pattern_include) > 0:\\n                self.pattern_include = re.compile(pattern_include)\\n            else:\\n                self.pattern_include = None\\n        except configparser.NoOptionError as e:\\n            self.pattern_include = None\\n        try:\\n            pattern_exclude = config.get('ec2', 'pattern_exclude');\\n            if pattern_exclude and len(pattern_exclude) > 0:\\n                self.pattern_exclude = re.compile(pattern_exclude)\\n            else:\\n                self.pattern_exclude = None\\n        except configparser.NoOptionError as e:\\n            self.pattern_exclude = None\\n        self.ec2_instance_filters = defaultdict(list)\\n        if config.has_option('ec2', 'instance_filters'):\\n            for instance_filter in config.get('ec2', 'instance_filters', '').split(','):\\n                instance_filter = instance_filter.strip()\\n                if not instance_filter or '=' not in instance_filter:\\n                    continue\\n                filter_key, filter_value = [x.strip() for x in instance_filter.split('=', 1)]\\n                if not filter_key:\\n                    continue\\n                self.ec2_instance_filters[filter_key].append(filter_value)"
  },
  {
    "code": "def __getitem__(self, index):\\n        if not isinstance(index, (slice,) + six.integer_types):\\n            raise TypeError\\n        return list(self.object_list)[index]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __getitem__(self, index):\\n        if not isinstance(index, (slice,) + six.integer_types):\\n            raise TypeError\\n        return list(self.object_list)[index]"
  },
  {
    "code": "def reindex_columns(self, new_columns):\\n        indexer, mask = self.columns.get_indexer(columns)\\n        new_values = self.values.take(indexer, axis=1)\\n        notmask = -mask\\n        if len(mask) > 0 and notmask.any():\\n            if issubclass(mat.dtype.type, np.int_):\\n                mat = mat.astype(float)\\n            elif issubclass(mat.dtype.type, np.bool_):\\n                mat = mat.astype(object)\\n            common.null_out_axis(mat, notmask, 1)\\n        return Block(new_values, columns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "whack-a-mole with the test suite. we're getting there",
    "fixed_code": "def reindex_columns(self, new_columns):\\n        indexer, mask = self.columns.get_indexer(new_columns)\\n        new_values = self.values.take(indexer, axis=1)\\n        notmask = -mask\\n        if len(mask) > 0 and notmask.any():\\n            if issubclass(new_values.dtype.type, np.int_):\\n                new_values = new_values.astype(float)\\n            elif issubclass(new_values.dtype.type, np.bool_):\\n                new_values = new_values.astype(object)\\n            common.null_out_axis(new_values, notmask, 1)\\n        return Block(new_values, new_columns)"
  },
  {
    "code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_dates=False, date_parser=None, na_values=None,\\n                    thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    def _parse_xls(self, sheetname, header=0, skiprows=None, index_col=None,\\n                   parse_dates=False, date_parser=None, na_values=None,\\n                   thousands=None, chunksize=None):\\n        from datetime import MINYEAR, time, datetime\\n        from xlrd import xldate_as_tuple, XL_CELL_DATE\\n        datemode = self.book.datemode\\n        sheet = self.book.sheet_by_name(sheetname)\\n        data = []\\n        for i in range(sheet.nrows):\\n            row = []\\n            for value, typ in izip(sheet.row_values(i), sheet.row_types(i)):\\n                if typ == XL_CELL_DATE:\\n                    dt = xldate_as_tuple(value, datemode)\\n                    if dt[0] < MINYEAR: \\n                        value = time(*dt[3:])\\n                    else:\\n                        value = datetime(*dt)\\n                row.append(value)\\n            data.append(row)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_dates=False, date_parser=None, na_values=None,\\n                    thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    def _parse_xls(self, sheetname, header=0, skiprows=None, index_col=None,\\n                   parse_dates=False, date_parser=None, na_values=None,\\n                   thousands=None, chunksize=None):\\n        from datetime import MINYEAR, time, datetime\\n        from xlrd import xldate_as_tuple, XL_CELL_DATE\\n        datemode = self.book.datemode\\n        sheet = self.book.sheet_by_name(sheetname)\\n        data = []\\n        for i in range(sheet.nrows):\\n            row = []\\n            for value, typ in izip(sheet.row_values(i), sheet.row_types(i)):\\n                if typ == XL_CELL_DATE:\\n                    dt = xldate_as_tuple(value, datemode)\\n                    if dt[0] < MINYEAR: \\n                        value = time(*dt[3:])\\n                    else:\\n                        value = datetime(*dt)\\n                row.append(value)\\n            data.append(row)\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()\\n    @property"
  },
  {
    "code": "def verify_schema(self, dataset_id, table_id, schema):\\n        from apiclient.errors import HttpError\\n        try:\\n            return (self.service.tables().get(\\n                projectId=self.project_id,\\n                datasetId=dataset_id,\\n                tableId=table_id\\n            ).execute()['schema']) == schema\\n        except HttpError as ex:\\n            self.process_http_error(ex)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "In gbq, use googleapiclient instead of apiclient #13454 (#13458)\\n\\ncloses #13454",
    "fixed_code": "def verify_schema(self, dataset_id, table_id, schema):\\n        try:\\n            from googleapiclient.errors import HttpError\\n        except:\\n            from apiclient.errors import HttpError\\n        try:\\n            return (self.service.tables().get(\\n                projectId=self.project_id,\\n                datasetId=dataset_id,\\n                tableId=table_id\\n            ).execute()['schema']) == schema\\n        except HttpError as ex:\\n            self.process_http_error(ex)"
  },
  {
    "code": "class XportReader(abc.Iterator):\\n\\t__doc__ = _xport_reader_doc\\n\\tdef __init__(\\n\\t\\tself, filepath_or_buffer, index=None, encoding=\"ISO-8859-1\", chunksize=None\\n\\t):\\n\\t\\tself._encoding = encoding\\n\\t\\tself._lines_read = 0\\n\\t\\tself._index = index\\n\\t\\tself._chunksize = chunksize\\n\\t\\tif isinstance(filepath_or_buffer, str):\\n\\t\\t\\t(\\n\\t\\t\\t\\tfilepath_or_buffer,\\n\\t\\t\\t\\tencoding,\\n\\t\\t\\t\\tcompression,\\n\\t\\t\\t\\tshould_close,\\n\\t\\t\\t) = get_filepath_or_buffer(filepath_or_buffer, encoding=encoding)\\n\\t\\tif isinstance(filepath_or_buffer, (str, bytes)):\\n\\t\\t\\tself.filepath_or_buffer = open(filepath_or_buffer, \"rb\")\\n\\t\\telse:\\n\\t\\t\\tcontents = filepath_or_buffer.read()\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcontents = contents.encode(self._encoding)\\n\\t\\t\\texcept UnicodeEncodeError:\\n\\t\\t\\t\\tpass\\n\\t\\t\\tself.filepath_or_buffer = BytesIO(contents)\\n\\t\\tself._read_header()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: AttributeError when read_sas used with GCS (#33070)\\n\\n\\nWith GCSFS (and possibly other connectors), the output from they file\\nread is `bytes`, not a Unicode `str`. The `encode` call is unnecessary\\nin this case.",
    "fixed_code": "class XportReader(abc.Iterator):\\n\\t__doc__ = _xport_reader_doc\\n\\tdef __init__(\\n\\t\\tself, filepath_or_buffer, index=None, encoding=\"ISO-8859-1\", chunksize=None\\n\\t):\\n\\t\\tself._encoding = encoding\\n\\t\\tself._lines_read = 0\\n\\t\\tself._index = index\\n\\t\\tself._chunksize = chunksize\\n\\t\\tif isinstance(filepath_or_buffer, str):\\n\\t\\t\\t(\\n\\t\\t\\t\\tfilepath_or_buffer,\\n\\t\\t\\t\\tencoding,\\n\\t\\t\\t\\tcompression,\\n\\t\\t\\t\\tshould_close,\\n\\t\\t\\t) = get_filepath_or_buffer(filepath_or_buffer, encoding=encoding)\\n\\t\\tif isinstance(filepath_or_buffer, (str, bytes)):\\n\\t\\t\\tself.filepath_or_buffer = open(filepath_or_buffer, \"rb\")\\n\\t\\telse:\\n\\t\\t\\tself.filepath_or_buffer = filepath_or_buffer\\n\\t\\tself._read_header()"
  },
  {
    "code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tself._options = input_dataset.options()\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "[tf.data] Solve the stack overflow problem of getting all options on Python side.",
    "fixed_code": "class _OptionsDataset(UnaryUnchangedStructureDataset):\\n  def __init__(self, input_dataset, options):\\n\\tself._input_dataset = input_dataset\\n\\tvariant_tensor = input_dataset._variant_tensor  \\n\\tsuper(_OptionsDataset, self).__init__(input_dataset, variant_tensor)\\n\\tif self._options:\\n\\t  self._options = self._options.merge(options)\\n\\telse:\\n\\t  self._options = options"
  },
  {
    "code": "def _validate_insert_value(self, value):\\n        value = self._validate_scalar(value)\\n        return self._unbox(value, setitem=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _validate_insert_value(self, value):\\n        value = self._validate_scalar(value)\\n        return self._unbox(value, setitem=True)"
  },
  {
    "code": "def __getitem__(self, index):\\n        return list(self.object_list)[index]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #19261 -- Delayed Queryset evaluation in paginators\\n\\nThanks trbs for the report and the patch.",
    "fixed_code": "def __getitem__(self, index):\\n        if not isinstance(index, (slice,) + six.integer_types):\\n            raise TypeError\\n        return list(self.object_list)[index]"
  },
  {
    "code": "def plot_frame(frame=None, x=None, y=None, subplots=False, sharex=True,\\n               sharey=False, use_index=True, figsize=None, grid=False,\\n               legend=True, rot=None, ax=None, style=None, title=None,\\n               xlim=None, ylim=None, logy=False, xticks=None, yticks=None,\\n               kind='line', sort_columns=False, fontsize=None,\\n               secondary_y=False, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    if x is not None:\\n        if com.is_integer(x) and not frame.columns.holds_integer():\\n            x = frame.columns[x]\\n        frame = frame.set_index(x)\\n    if y is not None:\\n        if com.is_integer(y) and not frame.columns.holds_integer():\\n            y = frame.columns[y]\\n        label = kwds.pop('label', y)\\n        ser = frame[y]\\n        ser.index.name = label\\n        return plot_series(ser, label=label, kind=kind,\\n                           use_index=use_index,\\n                           rot=rot, xticks=xticks, yticks=yticks,\\n                           xlim=xlim, ylim=ylim, ax=ax, style=style,\\n                           grid=grid, logy=logy, secondary_y=secondary_y,\\n                           title=title, figsize=figsize, fontsize=fontsize,\\n                           legend=legend, **kwds)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, style=style, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logy=logy,\\n                     sort_columns=sort_columns, secondary_y=secondary_y,\\n                     **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def plot_frame(frame=None, x=None, y=None, subplots=False, sharex=True,\\n               sharey=False, use_index=True, figsize=None, grid=False,\\n               legend=True, rot=None, ax=None, style=None, title=None,\\n               xlim=None, ylim=None, logy=False, xticks=None, yticks=None,\\n               kind='line', sort_columns=False, fontsize=None,\\n               secondary_y=False, **kwds):\\n    kind = _get_standard_kind(kind.lower().strip())\\n    if kind == 'line':\\n        klass = LinePlot\\n    elif kind in ('bar', 'barh'):\\n        klass = BarPlot\\n    elif kind == 'kde':\\n        klass = KdePlot\\n    else:\\n        raise ValueError('Invalid chart type given %s' % kind)\\n    if x is not None:\\n        if com.is_integer(x) and not frame.columns.holds_integer():\\n            x = frame.columns[x]\\n        frame = frame.set_index(x)\\n    if y is not None:\\n        if com.is_integer(y) and not frame.columns.holds_integer():\\n            y = frame.columns[y]\\n        label = kwds.pop('label', y)\\n        ser = frame[y]\\n        ser.index.name = label\\n        return plot_series(ser, label=label, kind=kind,\\n                           use_index=use_index,\\n                           rot=rot, xticks=xticks, yticks=yticks,\\n                           xlim=xlim, ylim=ylim, ax=ax, style=style,\\n                           grid=grid, logy=logy, secondary_y=secondary_y,\\n                           title=title, figsize=figsize, fontsize=fontsize,\\n                           legend=legend, **kwds)\\n    plot_obj = klass(frame, kind=kind, subplots=subplots, rot=rot,\\n                     legend=legend, ax=ax, style=style, fontsize=fontsize,\\n                     use_index=use_index, sharex=sharex, sharey=sharey,\\n                     xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\\n                     title=title, grid=grid, figsize=figsize, logy=logy,\\n                     sort_columns=sort_columns, secondary_y=secondary_y,\\n                     **kwds)\\n    plot_obj.generate()\\n    plot_obj.draw()\\n    if subplots:\\n        return plot_obj.axes\\n    else:\\n        return plot_obj.axes[0]"
  },
  {
    "code": "def trigger_dag(dag_id):\\n    data = request.get_json(force=True)\\n    run_id = None\\n    if 'run_id' in data:\\n        run_id = data['run_id']\\n    conf = None\\n    if 'conf' in data:\\n        conf = data['conf']\\n    execution_date = None\\n    if 'execution_date' in data and data['execution_date'] is not None:\\n        execution_date = data['execution_date']\\n        try:\\n            execution_date = timezone.parse(execution_date)\\n        except ValueError:\\n            error_message = (\\n                'Given execution date, {}, could not be identified '\\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\\n                .format(execution_date))\\n            log.info(error_message)\\n            response = jsonify({'error': error_message})\\n            response.status_code = 400\\n            return response\\n    replace_microseconds = (execution_date is None)\\n    if 'replace_microseconds' in data:\\n        replace_microseconds = to_boolean(data['replace_microseconds'])\\n    try:\\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date, replace_microseconds)\\n    except AirflowException as err:\\n        log.error(err)\\n        response = jsonify(error=\"{}\".format(err))\\n        response.status_code = err.status_code\\n        return response\\n    if getattr(g, 'user', None):\\n        log.info(\"User %s created %s\", g.user, dr)\\n    response = jsonify(\\n        message=\"Created {}\".format(dr),\\n        execution_date=dr.execution_date.isoformat(),\\n        run_id=dr.run_id\\n    )\\n    return response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def trigger_dag(dag_id):\\n    data = request.get_json(force=True)\\n    run_id = None\\n    if 'run_id' in data:\\n        run_id = data['run_id']\\n    conf = None\\n    if 'conf' in data:\\n        conf = data['conf']\\n    execution_date = None\\n    if 'execution_date' in data and data['execution_date'] is not None:\\n        execution_date = data['execution_date']\\n        try:\\n            execution_date = timezone.parse(execution_date)\\n        except ValueError:\\n            error_message = (\\n                'Given execution date, {}, could not be identified '\\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\\n                .format(execution_date))\\n            log.info(error_message)\\n            response = jsonify({'error': error_message})\\n            response.status_code = 400\\n            return response\\n    replace_microseconds = (execution_date is None)\\n    if 'replace_microseconds' in data:\\n        replace_microseconds = to_boolean(data['replace_microseconds'])\\n    try:\\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date, replace_microseconds)\\n    except AirflowException as err:\\n        log.error(err)\\n        response = jsonify(error=\"{}\".format(err))\\n        response.status_code = err.status_code\\n        return response\\n    if getattr(g, 'user', None):\\n        log.info(\"User %s created %s\", g.user, dr)\\n    response = jsonify(\\n        message=\"Created {}\".format(dr),\\n        execution_date=dr.execution_date.isoformat(),\\n        run_id=dr.run_id\\n    )\\n    return response"
  },
  {
    "code": "def _needs_other_dtype(block, to_insert):\\n    if block.dtype == np.float64:\\n        return not issubclass(mat.dtype.type, (np.integer, np.floating))\\n    elif block.dtype == np.object_:\\n        return issubclass(mat.dtype.type, (np.integer, np.floating))\\n    else:\\n        raise Exception('have not handled this case yet')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "whack-a-mole with the test suite. we're getting there",
    "fixed_code": "def _needs_other_dtype(block, to_insert):\\n    if block.dtype == np.float64:\\n        return not issubclass(to_insert.dtype.type, (np.integer, np.floating))\\n    elif block.dtype == np.object_:\\n        return issubclass(to_insert.dtype.type, (np.integer, np.floating))\\n    else:\\n        raise Exception('have not handled this case yet')"
  },
  {
    "code": "def get_indent(line):\\n    return re.match(r\"^(\\s*)\", line).group()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #18226: Add docstrings and unittests for idlelib/FormatParagraph.py. Move comment code to a separate function so it can be separately tested. Original patches by Todd Rovito and Phil Webster.",
    "fixed_code": "def get_indent(line):\\n    return re.match(r\"^([ \\t]*)\", line).group()"
  },
  {
    "code": "def main():\\n    import argparse\\n    parser = argparse.ArgumentParser(\\n        description='Utilities to support installing Python libraries.')\\n    parser.add_argument('-l', action='store_const', const=0,\\n                        default=10, dest='maxlevels',\\n                        help=\"don't recurse into subdirectories\")\\n    parser.add_argument('-f', action='store_true', dest='force',\\n                        help='force rebuild even if timestamps are up to date')\\n    parser.add_argument('-q', action='store_true', dest='quiet',\\n                        help='output only error messages')\\n    parser.add_argument('-b', action='store_true', dest='legacy',\\n                        help='use legacy (pre-PEP3147) compiled file locations')\\n    parser.add_argument('-d', metavar='DESTDIR',  dest='ddir', default=None,\\n                        help=('directory to prepend to file paths for use in '\\n                              'compile-time tracebacks and in runtime '\\n                              'tracebacks in cases where the source file is '\\n                              'unavailable'))\\n    parser.add_argument('-x', metavar='REGEXP', dest='rx', default=None,\\n                        help=('skip files matching the regular expression; '\\n                              'the regexp is searched for in the full path '\\n                              'of each file considered for compilation'))\\n    parser.add_argument('-i', metavar='FILE', dest='flist',\\n                        help=('add all the files and directories listed in '\\n                              'FILE to the list considered for compilation; '\\n                              'if \"-\", names are read from stdin'))\\n    parser.add_argument('compile_dest', metavar='FILE|DIR', nargs='*',\\n                        help=('zero or more file and directory names '\\n                              'to compile; if no arguments given, defaults '\\n                              'to the equivalent of -l sys.path'))\\n    args = parser.parse_args()\\n    compile_dests = args.compile_dest\\n    if args.rx:\\n        import re\\n        args.rx = re.compile(args.rx)\\n    if args.flist:\\n        try:\\n            with (sys.stdin if args.flist=='-' else open(args.flist)) as f:\\n                for line in f:\\n                    compile_dests.append(line.strip())\\n        except OSError:\\n            print(\"Error reading file list {}\".format(args.flist))\\n            return False\\n    success = True\\n    try:\\n        if compile_dests:\\n            for dest in compile_dests:\\n                if os.path.isfile(dest):\\n                    if not compile_file(dest, args.ddir, args.force, args.rx,\\n                                        args.quiet, args.legacy):\\n                        success = False\\n                else:\\n                    if not compile_dir(dest, args.maxlevels, args.ddir,\\n                                       args.force, args.rx, args.quiet,\\n                                       args.legacy):\\n                        success = False\\n            return success\\n        else:\\n            return compile_path(legacy=args.legacy, force=args.force,\\n                                quiet=args.quiet)\\n    except KeyboardInterrupt:\\n        print(\"\\n[interrupted]\")\\n        return False\\n    return True",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    import argparse\\n    parser = argparse.ArgumentParser(\\n        description='Utilities to support installing Python libraries.')\\n    parser.add_argument('-l', action='store_const', const=0,\\n                        default=10, dest='maxlevels',\\n                        help=\"don't recurse into subdirectories\")\\n    parser.add_argument('-f', action='store_true', dest='force',\\n                        help='force rebuild even if timestamps are up to date')\\n    parser.add_argument('-q', action='store_true', dest='quiet',\\n                        help='output only error messages')\\n    parser.add_argument('-b', action='store_true', dest='legacy',\\n                        help='use legacy (pre-PEP3147) compiled file locations')\\n    parser.add_argument('-d', metavar='DESTDIR',  dest='ddir', default=None,\\n                        help=('directory to prepend to file paths for use in '\\n                              'compile-time tracebacks and in runtime '\\n                              'tracebacks in cases where the source file is '\\n                              'unavailable'))\\n    parser.add_argument('-x', metavar='REGEXP', dest='rx', default=None,\\n                        help=('skip files matching the regular expression; '\\n                              'the regexp is searched for in the full path '\\n                              'of each file considered for compilation'))\\n    parser.add_argument('-i', metavar='FILE', dest='flist',\\n                        help=('add all the files and directories listed in '\\n                              'FILE to the list considered for compilation; '\\n                              'if \"-\", names are read from stdin'))\\n    parser.add_argument('compile_dest', metavar='FILE|DIR', nargs='*',\\n                        help=('zero or more file and directory names '\\n                              'to compile; if no arguments given, defaults '\\n                              'to the equivalent of -l sys.path'))\\n    args = parser.parse_args()\\n    compile_dests = args.compile_dest\\n    if args.rx:\\n        import re\\n        args.rx = re.compile(args.rx)\\n    if args.flist:\\n        try:\\n            with (sys.stdin if args.flist=='-' else open(args.flist)) as f:\\n                for line in f:\\n                    compile_dests.append(line.strip())\\n        except OSError:\\n            print(\"Error reading file list {}\".format(args.flist))\\n            return False\\n    success = True\\n    try:\\n        if compile_dests:\\n            for dest in compile_dests:\\n                if os.path.isfile(dest):\\n                    if not compile_file(dest, args.ddir, args.force, args.rx,\\n                                        args.quiet, args.legacy):\\n                        success = False\\n                else:\\n                    if not compile_dir(dest, args.maxlevels, args.ddir,\\n                                       args.force, args.rx, args.quiet,\\n                                       args.legacy):\\n                        success = False\\n            return success\\n        else:\\n            return compile_path(legacy=args.legacy, force=args.force,\\n                                quiet=args.quiet)\\n    except KeyboardInterrupt:\\n        print(\"\\n[interrupted]\")\\n        return False\\n    return True"
  },
  {
    "code": "def _align_series(self, indexer, ser):\\n        if isinstance(indexer, slice):\\n            indexer = tuple([indexer])\\n        if isinstance(indexer, tuple):\\n            aligners = [not _is_null_slice(idx) for idx in indexer]\\n            sum_aligners = sum(aligners)\\n            single_aligner = sum_aligners == 1\\n            is_frame = self.obj.ndim == 2\\n            is_panel = self.obj.ndim >= 3\\n            obj = self.obj\\n            if is_frame:\\n                single_aligner = single_aligner and aligners[0]\\n            elif is_panel:\\n                single_aligner = (single_aligner and\\n                                  (aligners[1] or aligners[2]))\\n            if (sum_aligners == self.ndim and\\n                    all([com._is_sequence(_) for _ in indexer])):\\n                ser = ser.reindex(obj.axes[0][indexer[0].ravel()],\\n                                  copy=True).values\\n                l = len(indexer[1].ravel())\\n                ser = np.tile(ser, l).reshape(l, -1).T\\n                return ser\\n            for i, idx in enumerate(indexer):\\n                ax = obj.axes[i]\\n                if com._is_sequence(idx) or isinstance(idx, slice):\\n                    if single_aligner and _is_null_slice(idx):\\n                        continue\\n                    new_ix = ax[idx]\\n                    if not is_list_like(new_ix):\\n                        new_ix = Index([new_ix])\\n                    else:\\n                        new_ix = Index(new_ix.ravel())\\n                    if ser.index.equals(new_ix) or not len(new_ix):\\n                        return ser.values.copy()\\n                    return ser.reindex(new_ix).values\\n                elif single_aligner and is_frame:\\n                    ax = self.obj.axes[1]\\n                    if ser.index.equals(ax) or not len(ax):\\n                        return ser.values.copy()\\n                    return ser.reindex(ax).values\\n                elif single_aligner:\\n                    broadcast = []\\n                    for n, labels in enumerate(self.obj._get_plane_axes(i)):\\n                        if len(labels & ser.index):\\n                            ser = ser.reindex(labels)\\n                        else:\\n                            broadcast.append((n, len(labels)))\\n                    ser = ser.values.copy()\\n                    for (axis, l) in broadcast:\\n                        shape = [-1] * (len(broadcast) + 1)\\n                        shape[axis] = l\\n                        ser = np.tile(ser, l).reshape(shape)\\n                    if self.obj.ndim == 3:\\n                        ser = ser.T\\n                    return ser\\n        elif np.isscalar(indexer):\\n            ax = self.obj._get_axis(1)\\n            if ser.index.equals(ax):\\n                return ser.values.copy()\\n            return ser.reindex(ax).values\\n        raise ValueError('Incompatible indexer with Series')",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _align_series(self, indexer, ser):\\n        if isinstance(indexer, slice):\\n            indexer = tuple([indexer])\\n        if isinstance(indexer, tuple):\\n            aligners = [not _is_null_slice(idx) for idx in indexer]\\n            sum_aligners = sum(aligners)\\n            single_aligner = sum_aligners == 1\\n            is_frame = self.obj.ndim == 2\\n            is_panel = self.obj.ndim >= 3\\n            obj = self.obj\\n            if is_frame:\\n                single_aligner = single_aligner and aligners[0]\\n            elif is_panel:\\n                single_aligner = (single_aligner and\\n                                  (aligners[1] or aligners[2]))\\n            if (sum_aligners == self.ndim and\\n                    all([com._is_sequence(_) for _ in indexer])):\\n                ser = ser.reindex(obj.axes[0][indexer[0].ravel()],\\n                                  copy=True).values\\n                l = len(indexer[1].ravel())\\n                ser = np.tile(ser, l).reshape(l, -1).T\\n                return ser\\n            for i, idx in enumerate(indexer):\\n                ax = obj.axes[i]\\n                if com._is_sequence(idx) or isinstance(idx, slice):\\n                    if single_aligner and _is_null_slice(idx):\\n                        continue\\n                    new_ix = ax[idx]\\n                    if not is_list_like(new_ix):\\n                        new_ix = Index([new_ix])\\n                    else:\\n                        new_ix = Index(new_ix.ravel())\\n                    if ser.index.equals(new_ix) or not len(new_ix):\\n                        return ser.values.copy()\\n                    return ser.reindex(new_ix).values\\n                elif single_aligner and is_frame:\\n                    ax = self.obj.axes[1]\\n                    if ser.index.equals(ax) or not len(ax):\\n                        return ser.values.copy()\\n                    return ser.reindex(ax).values\\n                elif single_aligner:\\n                    broadcast = []\\n                    for n, labels in enumerate(self.obj._get_plane_axes(i)):\\n                        if len(labels & ser.index):\\n                            ser = ser.reindex(labels)\\n                        else:\\n                            broadcast.append((n, len(labels)))\\n                    ser = ser.values.copy()\\n                    for (axis, l) in broadcast:\\n                        shape = [-1] * (len(broadcast) + 1)\\n                        shape[axis] = l\\n                        ser = np.tile(ser, l).reshape(shape)\\n                    if self.obj.ndim == 3:\\n                        ser = ser.T\\n                    return ser\\n        elif np.isscalar(indexer):\\n            ax = self.obj._get_axis(1)\\n            if ser.index.equals(ax):\\n                return ser.values.copy()\\n            return ser.reindex(ax).values\\n        raise ValueError('Incompatible indexer with Series')"
  },
  {
    "code": "def numeric_tester(self, calc_type, calc_value, data_type, used_locale):\\t\\t\\t\\ttry:\\t\\t\\tset_locale = setlocale(LC_NUMERIC)\\t\\texcept Error:\\t\\t\\tset_locale = \"<not able to determine>\"\\t\\tknown_value = known_numerics.get(used_locale,\\t\\t\\t\\t\\t\\t\\t\\t\\t('', ''))[data_type is 'thousands_sep']\\t\\tif known_value and calc_value:\\t\\t\\tself.assertEquals(calc_value, known_value,\\t\\t\\t\\t\\t\\t\\t\\tself.lc_numeric_err_msg % (\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_value, known_value,\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_type, data_type, set_locale,\\t\\t\\t\\t\\t\\t\\t\\t\\tused_locale))",
    "label": 1,
    "bug_type": "binop",
    "bug_description": "Make test__locale pass.  Stupid bug in the original code: used 'is' for '=='.",
    "fixed_code": "def numeric_tester(self, calc_type, calc_value, data_type, used_locale):\\n\\t\\ttry:\\n\\t\\t\\tset_locale = setlocale(LC_NUMERIC)\\n\\t\\texcept Error:\\n\\t\\t\\tset_locale = \"<not able to determine>\"\\n\\t\\tknown_value = known_numerics.get(used_locale,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t('', ''))[data_type == 'thousands_sep']\\n\\t\\tif known_value and calc_value:\\n\\t\\t\\tself.assertEquals(calc_value, known_value,\\n\\t\\t\\t\\t\\t\\t\\t\\tself.lc_numeric_err_msg % (\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_value, known_value,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tcalc_type, data_type, set_locale,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tused_locale))"
  },
  {
    "code": "def _prep_ndarray(values, copy=True):\\n    if not isinstance(values, np.ndarray):\\n        arr = np.asarray(values)\\n        if issubclass(arr.dtype.type, basestring):\\n            arr = np.array(values, dtype=object, copy=True)\\n        values = arr\\n    else:\\n        if copy:\\n            values = values.copy()\\n    if values.ndim == 1:\\n        N = values.shape[0]\\n        if N == 0:\\n            values = values.reshape((values.shape[0], 0))\\n        else:\\n            values = values.reshape((values.shape[0], 1))\\n    elif values.ndim != 2:\\n        raise Exception('Must pass 2-d input')\\n    return values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: handling of Series input to DataFrame constructor",
    "fixed_code": "def _prep_ndarray(values, copy=True):\\n    if not isinstance(values, np.ndarray):\\n        arr = np.asarray(values)\\n        if issubclass(arr.dtype.type, basestring):\\n            arr = np.array(values, dtype=object, copy=True)\\n        values = arr\\n    else:\\n        values = np.asarray(values)\\n        if copy:\\n            values = values.copy()\\n    if values.ndim == 1:\\n        N = values.shape[0]\\n        if N == 0:\\n            values = values.reshape((values.shape[0], 0))\\n        else:\\n            values = values.reshape((values.shape[0], 1))\\n    elif values.ndim != 2:\\n        raise Exception('Must pass 2-d input')\\n    return values"
  },
  {
    "code": "def validation_curve(estimator, X, y, cv=None, scoring=None,\\n                     n_jobs=1, pre_dispatch=\"all\", verbose=0, **param_range):\\n    X, y = check_arrays(X, y, sparse_format='csr', allow_lists=True)\\n    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))\\n    scorer = check_scoring(estimator, scoring=scoring)\\n    if len(param_range) == 0:\\n        raise ValueError(\"You must provide a range for an estimator \"\\n                         \"parameter.\")\\n    elif len(param_range) > 1:\\n        raise ValueError(\"Only one parameter is allowed to change to \"\\n                         \"generate a validation curve.\")\\n    param_name = param_range.keys()[0]\\n    param_configs = [{param_name : v}\\n                     for v in param_range[param_name]]\\n    n_params = len(param_configs)\\n    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\\n                        verbose=verbose)\\n    out = parallel(delayed(_fit_and_score)(\\n        estimator, X, y, scorer, train, test, verbose, params, fit_params=None,\\n        return_train_score=True)\\n        for train, test in cv for params in param_configs)\\n    out = np.asarray(out)[:, :2]\\n    n_cv_folds = out.shape[0] / n_params\\n    out = out.reshape(n_cv_folds, n_params, 2)\\n    avg_over_cv = out.mean(axis=0).reshape(n_params, 2)\\n    return avg_over_cv[:, 0], avg_over_cv[:, 1]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Improve interface and documentation",
    "fixed_code": "def validation_curve(estimator, X, y, param_name, param_range, cv=None,\\n                     scoring=None, n_jobs=1, pre_dispatch=\"all\", verbose=0):\\n    X, y = check_arrays(X, y, sparse_format='csr', allow_lists=True)\\n    cv = _check_cv(cv, X, y, classifier=is_classifier(estimator))\\n    scorer = check_scoring(estimator, scoring=scoring)\\n    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\\n                        verbose=verbose)\\n    out = parallel(delayed(_fit_and_score)(\\n        estimator, X, y, scorer, train, test, verbose,\\n        parameters={param_name : v}, fit_params=None, return_train_score=True)\\n        for train, test in cv for v in param_range)\\n    out = np.asarray(out)[:, :2]\\n    n_params = len(param_range)\\n    n_cv_folds = out.shape[0] / n_params\\n    out = out.reshape(n_cv_folds, n_params, 2)\\n    avg_over_cv = out.mean(axis=0).reshape(n_params, 2)\\n    return avg_over_cv[:, 0], avg_over_cv[:, 1]"
  },
  {
    "code": "def modify_firewall_config(self, modify):\\n        net_firewall_config_obj = netapp_utils.zapi.NaElement(\"net-firewall-config-modify\")\\n        net_firewall_config_obj.add_new_child('node-name', self.parameters['node'])\\n        if modify.get('enable'):\\n            net_firewall_config_obj.add_new_child('is-enabled', self.change_status_to_bool(self.parameters['enable']))\\n        if modify.get('logging'):\\n            net_firewall_config_obj.add_new_child('is-logging', self.change_status_to_bool(self.parameters['logging']))\\n        try:\\n            self.server.invoke_successfully(net_firewall_config_obj, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg=\"Error modifying Firewall Config: %s\" % (to_native(error)),\\n                                  exception=traceback.format_exc())",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def modify_firewall_config(self, modify):\\n        net_firewall_config_obj = netapp_utils.zapi.NaElement(\"net-firewall-config-modify\")\\n        net_firewall_config_obj.add_new_child('node-name', self.parameters['node'])\\n        if modify.get('enable'):\\n            net_firewall_config_obj.add_new_child('is-enabled', self.change_status_to_bool(self.parameters['enable']))\\n        if modify.get('logging'):\\n            net_firewall_config_obj.add_new_child('is-logging', self.change_status_to_bool(self.parameters['logging']))\\n        try:\\n            self.server.invoke_successfully(net_firewall_config_obj, enable_tunneling=True)\\n        except netapp_utils.zapi.NaApiError as error:\\n            self.module.fail_json(msg=\"Error modifying Firewall Config: %s\" % (to_native(error)),\\n                                  exception=traceback.format_exc())"
  },
  {
    "code": "def run_query(self,\\n                  bql,\\n                  destination_dataset_table=False,\\n                  write_disposition='WRITE_EMPTY',\\n                  allow_large_results=False,\\n                  flatten_results=False,\\n                  udf_config=False,\\n                  use_legacy_sql=None,\\n                  maximum_billing_tier=None,\\n                  maximum_bytes_billed=None,\\n                  create_disposition='CREATE_IF_NEEDED',\\n                  query_params=None,\\n                  schema_update_options=(),\\n                  priority='INTERACTIVE',\\n                  time_partitioning={}):\\n        allowed_schema_update_options = [\\n            'ALLOW_FIELD_ADDITION', \"ALLOW_FIELD_RELAXATION\"\\n        ]\\n        if not set(allowed_schema_update_options).issuperset(\\n                set(schema_update_options)):\\n            raise ValueError(\\n                \"{0} contains invalid schema update options. \"\\n                \"Please only use one or more of the following options: {1}\"\\n                .format(schema_update_options, allowed_schema_update_options))\\n        if use_legacy_sql is None:\\n            use_legacy_sql = self.use_legacy_sql\\n        configuration = {\\n            'query': {\\n                'query': bql,\\n                'useLegacySql': use_legacy_sql,\\n                'maximumBillingTier': maximum_billing_tier,\\n                'maximumBytesBilled': maximum_bytes_billed,\\n                'priority': priority\\n            }\\n        }\\n        if destination_dataset_table:\\n            assert '.' in destination_dataset_table, (\\n                'Expected destination_dataset_table in the format of '\\n                '<dataset>.<table>. Got: {}').format(destination_dataset_table)\\n            destination_project, destination_dataset, destination_table = \\\\n                _split_tablename(table_input=destination_dataset_table,\\n                                 default_project_id=self.project_id)\\n            configuration['query'].update({\\n                'allowLargeResults': allow_large_results,\\n                'flattenResults': flatten_results,\\n                'writeDisposition': write_disposition,\\n                'createDisposition': create_disposition,\\n                'destinationTable': {\\n                    'projectId': destination_project,\\n                    'datasetId': destination_dataset,\\n                    'tableId': destination_table,\\n                }\\n            })\\n        if udf_config:\\n            assert isinstance(udf_config, list)\\n            configuration['query'].update({\\n                'userDefinedFunctionResources': udf_config\\n            })\\n        if query_params:\\n            if self.use_legacy_sql:\\n                raise ValueError(\"Query paramaters are not allowed when using \"\\n                                 \"legacy SQL\")\\n            else:\\n                configuration['query']['queryParameters'] = query_params\\n        time_partitioning = _cleanse_time_partitioning(\\n            destination_dataset_table,\\n            time_partitioning\\n        )\\n        if time_partitioning:\\n            configuration['query'].update({\\n                'timePartitioning': time_partitioning\\n            })\\n        if schema_update_options:\\n            if write_disposition not in [\"WRITE_APPEND\", \"WRITE_TRUNCATE\"]:\\n                raise ValueError(\"schema_update_options is only \"\\n                                 \"allowed if write_disposition is \"\\n                                 \"'WRITE_APPEND' or 'WRITE_TRUNCATE'.\")\\n            else:\\n                self.log.info(\\n                    \"Adding experimental \"\\n                    \"'schemaUpdateOptions': {0}\".format(schema_update_options))\\n                configuration['query'][\\n                    'schemaUpdateOptions'] = schema_update_options\\n        return self.run_with_configuration(configuration)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2513] Change `bql` to `sql` for BigQuery Hooks & Ops\\n\\n- Change `bql` to `sql` for BigQuery Hooks &\\nOperators for consistency\\n\\nCloses #3454 from kaxil/consistent-bq-lang",
    "fixed_code": "def run_query(self,\\n                  bql=None,\\n                  sql=None,\\n                  destination_dataset_table=False,\\n                  write_disposition='WRITE_EMPTY',\\n                  allow_large_results=False,\\n                  flatten_results=False,\\n                  udf_config=False,\\n                  use_legacy_sql=None,\\n                  maximum_billing_tier=None,\\n                  maximum_bytes_billed=None,\\n                  create_disposition='CREATE_IF_NEEDED',\\n                  query_params=None,\\n                  schema_update_options=(),\\n                  priority='INTERACTIVE',\\n                  time_partitioning={}):\\n        sql = bql if sql is None else sql\\n        if bql:\\n            import warnings\\n            warnings.warn('Deprecated parameter `bql` used in '\\n                          '`BigQueryBaseCursor.run_query` '\\n                          'Use `sql` parameter instead to pass the sql to be '\\n                          'executed. `bql` parameter is deprecated and '\\n                          'will be removed in a future version of '\\n                          'Airflow.',\\n                          category=DeprecationWarning)\\n        if sql is None:\\n            raise TypeError('`BigQueryBaseCursor.run_query` missing 1 required '\\n                            'positional argument: `sql`')\\n        allowed_schema_update_options = [\\n            'ALLOW_FIELD_ADDITION', \"ALLOW_FIELD_RELAXATION\"\\n        ]\\n        if not set(allowed_schema_update_options).issuperset(\\n                set(schema_update_options)):\\n            raise ValueError(\\n                \"{0} contains invalid schema update options. \"\\n                \"Please only use one or more of the following options: {1}\"\\n                .format(schema_update_options, allowed_schema_update_options))\\n        if use_legacy_sql is None:\\n            use_legacy_sql = self.use_legacy_sql\\n        configuration = {\\n            'query': {\\n                'query': sql,\\n                'useLegacySql': use_legacy_sql,\\n                'maximumBillingTier': maximum_billing_tier,\\n                'maximumBytesBilled': maximum_bytes_billed,\\n                'priority': priority\\n            }\\n        }\\n        if destination_dataset_table:\\n            assert '.' in destination_dataset_table, (\\n                'Expected destination_dataset_table in the format of '\\n                '<dataset>.<table>. Got: {}').format(destination_dataset_table)\\n            destination_project, destination_dataset, destination_table = \\\\n                _split_tablename(table_input=destination_dataset_table,\\n                                 default_project_id=self.project_id)\\n            configuration['query'].update({\\n                'allowLargeResults': allow_large_results,\\n                'flattenResults': flatten_results,\\n                'writeDisposition': write_disposition,\\n                'createDisposition': create_disposition,\\n                'destinationTable': {\\n                    'projectId': destination_project,\\n                    'datasetId': destination_dataset,\\n                    'tableId': destination_table,\\n                }\\n            })\\n        if udf_config:\\n            assert isinstance(udf_config, list)\\n            configuration['query'].update({\\n                'userDefinedFunctionResources': udf_config\\n            })\\n        if query_params:\\n            if self.use_legacy_sql:\\n                raise ValueError(\"Query paramaters are not allowed when using \"\\n                                 \"legacy SQL\")\\n            else:\\n                configuration['query']['queryParameters'] = query_params\\n        time_partitioning = _cleanse_time_partitioning(\\n            destination_dataset_table,\\n            time_partitioning\\n        )\\n        if time_partitioning:\\n            configuration['query'].update({\\n                'timePartitioning': time_partitioning\\n            })\\n        if schema_update_options:\\n            if write_disposition not in [\"WRITE_APPEND\", \"WRITE_TRUNCATE\"]:\\n                raise ValueError(\"schema_update_options is only \"\\n                                 \"allowed if write_disposition is \"\\n                                 \"'WRITE_APPEND' or 'WRITE_TRUNCATE'.\")\\n            else:\\n                self.log.info(\\n                    \"Adding experimental \"\\n                    \"'schemaUpdateOptions': {0}\".format(schema_update_options))\\n                configuration['query'][\\n                    'schemaUpdateOptions'] = schema_update_options\\n        return self.run_with_configuration(configuration)"
  },
  {
    "code": "def _validate_source(source, req):\\n\\t\\tif source:\\n\\t\\t\\tpieces = urlparse.urlparse(source)\\n\\t\\t\\tschemes = [scheme for scheme in store.get_known_schemes()\\n\\t\\t\\t\\t\\t   if scheme != 'file' and scheme != 'swift+config']\\n\\t\\t\\tfor scheme in schemes:\\n\\t\\t\\t\\tif pieces.scheme == scheme:\\n\\t\\t\\t\\t\\treturn source\\n\\t\\t\\tmsg = (\"External sourcing not supported for \"\\n\\t\\t\\t\\t   \"store '%s'\" % pieces.scheme)\\n\\t\\t\\tLOG.debug(msg)\\n\\t\\t\\traise HTTPBadRequest(explanation=msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t content_type=\"text/plain\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _validate_source(source, req):\\n\\t\\tif source:\\n\\t\\t\\tpieces = urlparse.urlparse(source)\\n\\t\\t\\tschemes = [scheme for scheme in store.get_known_schemes()\\n\\t\\t\\t\\t\\t   if scheme != 'file' and scheme != 'swift+config']\\n\\t\\t\\tfor scheme in schemes:\\n\\t\\t\\t\\tif pieces.scheme == scheme:\\n\\t\\t\\t\\t\\treturn source\\n\\t\\t\\tmsg = (\"External sourcing not supported for \"\\n\\t\\t\\t\\t   \"store '%s'\" % pieces.scheme)\\n\\t\\t\\tLOG.debug(msg)\\n\\t\\t\\traise HTTPBadRequest(explanation=msg,\\n\\t\\t\\t\\t\\t\\t\\t\\t request=req,\\n\\t\\t\\t\\t\\t\\t\\t\\t content_type=\"text/plain\")"
  },
  {
    "code": "def time_get_loc_dec(self):\\n        self.idx_dec.get_loc(100000)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "PERF: make RangeIndex iterate over ._range (#35676)",
    "fixed_code": "def time_get_loc_dec(self):\\n        self.idx_dec.get_loc(100_000)"
  },
  {
    "code": "def __init__(self, data=None, index=None, columns=None, default_kind=None,\\n\\t\\t\\t\\t default_fill_value=None, dtype=None, copy=False):\\n\\t\\tif isinstance(data, SparseDataFrame):\\n\\t\\t\\tif index is None:\\n\\t\\t\\t\\tindex = data.index\\n\\t\\t\\tif columns is None:\\n\\t\\t\\t\\tcolumns = data.columns\\n\\t\\t\\tif default_fill_value is None:\\n\\t\\t\\t\\tdefault_fill_value = data.default_fill_value\\n\\t\\t\\tif default_kind is None:\\n\\t\\t\\t\\tdefault_kind = data.default_kind\\n\\t\\telif isinstance(data, (SparseSeries, SparseArray)):\\n\\t\\t\\tif index is None:\\n\\t\\t\\t\\tindex = data.index\\n\\t\\t\\tif default_fill_value is None:\\n\\t\\t\\t\\tdefault_fill_value = data.fill_value\\n\\t\\t\\tif columns is None and hasattr(data, 'name'):\\n\\t\\t\\t\\tcolumns = [data.name]\\n\\t\\t\\tif columns is None:\\n\\t\\t\\t\\traise Exception(\"cannot pass a series w/o a name or columns\")\\n\\t\\t\\tdata = {columns[0]: data}\\n\\t\\tif default_fill_value is None:\\n\\t\\t\\tdefault_fill_value = np.nan\\n\\t\\tif default_kind is None:\\n\\t\\t\\tdefault_kind = 'block'\\n\\t\\tself._default_kind = default_kind\\n\\t\\tself._default_fill_value = default_fill_value\\n\\t\\tif is_scipy_sparse(data):\\n\\t\\t\\tmgr = self._init_spmatrix(data, index, columns, dtype=dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  fill_value=default_fill_value)\\n\\t\\telif isinstance(data, dict):\\n\\t\\t\\tmgr = self._init_dict(data, index, columns, dtype=dtype)\\n\\t\\telif isinstance(data, (np.ndarray, list)):\\n\\t\\t\\tmgr = self._init_matrix(data, index, columns, dtype=dtype)\\n\\t\\telif isinstance(data, SparseDataFrame):\\n\\t\\t\\tmgr = self._init_mgr(data._data,\\n\\t\\t\\t\\t\\t\\t\\t\\t dict(index=index, columns=columns),\\n\\t\\t\\t\\t\\t\\t\\t\\t dtype=dtype, copy=copy)\\n\\t\\telif isinstance(data, DataFrame):\\n\\t\\t\\tmgr = self._init_dict(data, data.index, data.columns, dtype=dtype)\\n\\t\\telif isinstance(data, Series):\\n\\t\\t\\tmgr = self._init_dict(data.to_frame(), data.index,\\n\\t\\t\\t\\t\\t\\t\\t\\t  columns=None, dtype=dtype)\\n\\t\\telif isinstance(data, BlockManager):\\n\\t\\t\\tmgr = self._init_mgr(data, axes=dict(index=index, columns=columns),\\n\\t\\t\\t\\t\\t\\t\\t\\t dtype=dtype, copy=copy)\\n\\t\\telif data is None:\\n\\t\\t\\tdata = DataFrame()\\n\\t\\t\\tif index is None:\\n\\t\\t\\t\\tindex = Index([])\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindex = _ensure_index(index)\\n\\t\\t\\tif columns is None:\\n\\t\\t\\t\\tcolumns = Index([])\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfor c in columns:\\n\\t\\t\\t\\t\\tdata[c] = SparseArray(np.nan, index=index,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  kind=self._default_kind,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  fill_value=self._default_fill_value)\\n\\t\\t\\tmgr = to_manager(data, columns, index)\\n\\t\\t\\tif dtype is not None:\\n\\t\\t\\t\\tmgr = mgr.astype(dtype)\\n\\t\\telse:\\n\\t\\t\\tmsg = ('SparseDataFrame called with unkown type \"{data_type}\" '\\n\\t\\t\\t\\t   'for data argument')\\n\\t\\t\\traise TypeError(msg.format(data_type=type(data).__name__))\\n\\t\\tgeneric.NDFrame.__init__(self, mgr)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, data=None, index=None, columns=None, default_kind=None,\\n\\t\\t\\t\\t default_fill_value=None, dtype=None, copy=False):\\n\\t\\tif isinstance(data, SparseDataFrame):\\n\\t\\t\\tif index is None:\\n\\t\\t\\t\\tindex = data.index\\n\\t\\t\\tif columns is None:\\n\\t\\t\\t\\tcolumns = data.columns\\n\\t\\t\\tif default_fill_value is None:\\n\\t\\t\\t\\tdefault_fill_value = data.default_fill_value\\n\\t\\t\\tif default_kind is None:\\n\\t\\t\\t\\tdefault_kind = data.default_kind\\n\\t\\telif isinstance(data, (SparseSeries, SparseArray)):\\n\\t\\t\\tif index is None:\\n\\t\\t\\t\\tindex = data.index\\n\\t\\t\\tif default_fill_value is None:\\n\\t\\t\\t\\tdefault_fill_value = data.fill_value\\n\\t\\t\\tif columns is None and hasattr(data, 'name'):\\n\\t\\t\\t\\tcolumns = [data.name]\\n\\t\\t\\tif columns is None:\\n\\t\\t\\t\\traise Exception(\"cannot pass a series w/o a name or columns\")\\n\\t\\t\\tdata = {columns[0]: data}\\n\\t\\tif default_fill_value is None:\\n\\t\\t\\tdefault_fill_value = np.nan\\n\\t\\tif default_kind is None:\\n\\t\\t\\tdefault_kind = 'block'\\n\\t\\tself._default_kind = default_kind\\n\\t\\tself._default_fill_value = default_fill_value\\n\\t\\tif is_scipy_sparse(data):\\n\\t\\t\\tmgr = self._init_spmatrix(data, index, columns, dtype=dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t  fill_value=default_fill_value)\\n\\t\\telif isinstance(data, dict):\\n\\t\\t\\tmgr = self._init_dict(data, index, columns, dtype=dtype)\\n\\t\\telif isinstance(data, (np.ndarray, list)):\\n\\t\\t\\tmgr = self._init_matrix(data, index, columns, dtype=dtype)\\n\\t\\telif isinstance(data, SparseDataFrame):\\n\\t\\t\\tmgr = self._init_mgr(data._data,\\n\\t\\t\\t\\t\\t\\t\\t\\t dict(index=index, columns=columns),\\n\\t\\t\\t\\t\\t\\t\\t\\t dtype=dtype, copy=copy)\\n\\t\\telif isinstance(data, DataFrame):\\n\\t\\t\\tmgr = self._init_dict(data, data.index, data.columns, dtype=dtype)\\n\\t\\telif isinstance(data, Series):\\n\\t\\t\\tmgr = self._init_dict(data.to_frame(), data.index,\\n\\t\\t\\t\\t\\t\\t\\t\\t  columns=None, dtype=dtype)\\n\\t\\telif isinstance(data, BlockManager):\\n\\t\\t\\tmgr = self._init_mgr(data, axes=dict(index=index, columns=columns),\\n\\t\\t\\t\\t\\t\\t\\t\\t dtype=dtype, copy=copy)\\n\\t\\telif data is None:\\n\\t\\t\\tdata = DataFrame()\\n\\t\\t\\tif index is None:\\n\\t\\t\\t\\tindex = Index([])\\n\\t\\t\\telse:\\n\\t\\t\\t\\tindex = _ensure_index(index)\\n\\t\\t\\tif columns is None:\\n\\t\\t\\t\\tcolumns = Index([])\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfor c in columns:\\n\\t\\t\\t\\t\\tdata[c] = SparseArray(np.nan, index=index,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  kind=self._default_kind,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  fill_value=self._default_fill_value)\\n\\t\\t\\tmgr = to_manager(data, columns, index)\\n\\t\\t\\tif dtype is not None:\\n\\t\\t\\t\\tmgr = mgr.astype(dtype)\\n\\t\\telse:\\n\\t\\t\\tmsg = ('SparseDataFrame called with unkown type \"{data_type}\" '\\n\\t\\t\\t\\t   'for data argument')\\n\\t\\t\\traise TypeError(msg.format(data_type=type(data).__name__))\\n\\t\\tgeneric.NDFrame.__init__(self, mgr)"
  },
  {
    "code": "def melt(frame, id_vars=None, value_vars=None,\\n         var_name='variable', value_name='value'):\\n    if id_vars is not None:\\n        if not isinstance(id_vars, (tuple, list, np.ndarray)):\\n            id_vars = [id_vars]\\n        else:\\n            id_vars = list(id_vars)\\n    else:\\n        id_vars = []\\n    if value_vars is not None:\\n        if not isinstance(value_vars, (tuple, list, np.ndarray)):\\n            value_vars = [value_vars]\\n        frame = frame.ix[:, id_vars + value_vars]\\n    else:\\n        frame = frame.copy()\\n    N, K = frame.shape\\n    K -= len(id_vars)\\n    mdata = {}\\n    for col in id_vars:\\n        mdata[col] = np.tile(frame.pop(col).values, K)\\n    mcolumns = id_vars + [var_name, value_name]\\n    mdata[value_name] = frame.values.ravel('F')\\n    mdata[var_name] = np.asarray(frame.columns).repeat(N)\\n    return DataFrame(mdata, columns=mcolumns)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH melt uses column name if available",
    "fixed_code": "def melt(frame, id_vars=None, value_vars=None,\\n         var_name=None, value_name='value'):\\n    if id_vars is not None:\\n        if not isinstance(id_vars, (tuple, list, np.ndarray)):\\n            id_vars = [id_vars]\\n        else:\\n            id_vars = list(id_vars)\\n    else:\\n        id_vars = []\\n    if value_vars is not None:\\n        if not isinstance(value_vars, (tuple, list, np.ndarray)):\\n            value_vars = [value_vars]\\n        frame = frame.ix[:, id_vars + value_vars]\\n    else:\\n        frame = frame.copy()\\n    if var_name is None:\\n        var_name = frame.columns.name if frame.columns.name is not None else 'variable'\\n    N, K = frame.shape\\n    K -= len(id_vars)\\n    mdata = {}\\n    for col in id_vars:\\n        mdata[col] = np.tile(frame.pop(col).values, K)\\n    mcolumns = id_vars + [var_name, value_name]\\n    mdata[value_name] = frame.values.ravel('F')\\n    mdata[var_name] = np.asarray(frame.columns).repeat(N)\\n    return DataFrame(mdata, columns=mcolumns)"
  },
  {
    "code": "def test_urlfield_10(self):\\n\\t\\tf = URLField(verify_exists=True)\\n\\t\\turl = u'http://\\u03b5\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac.idn.icann.org/\\u0391\\u03c1\\u03c7\\u03b9\\u03ba\\u03ae_\\u03c3\\u03b5\\u03bb\\u03af\\u03b4\\u03b1'\\n\\t\\tself.assertEqual(url, f.clean(url))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_urlfield_10(self):\\n\\t\\tf = URLField(verify_exists=True)\\n\\t\\turl = u'http://\\u03b5\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac.idn.icann.org/\\u0391\\u03c1\\u03c7\\u03b9\\u03ba\\u03ae_\\u03c3\\u03b5\\u03bb\\u03af\\u03b4\\u03b1'\\n\\t\\tself.assertEqual(url, f.clean(url))"
  },
  {
    "code": "def _check_params(self):\\n        if self.n_topics is not None:\\n            self._n_components = self.n_topics\\n            warnings.warn(\"n_topics has been renamed to n_components in \"\\n                          \"version 0.19 and will be removed in 0.21\",\\n                          DeprecationWarning)\\n        else:\\n            self._n_components = self.n_components\\n        if self._n_components <= 0:\\n            raise ValueError(\"Invalid 'n_components' parameter: %r\"\\n                             % self._n_components)\\n        if self.total_samples <= 0:\\n            raise ValueError(\"Invalid 'total_samples' parameter: %r\"\\n                             % self.total_samples)\\n        if self.learning_offset < 0:\\n            raise ValueError(\"Invalid 'learning_offset' parameter: %r\"\\n                             % self.learning_offset)\\n        if self.learning_method not in (\"batch\", \"online\", None):\\n            raise ValueError(\"Invalid 'learning_method' parameter: %r\"\\n                             % self.learning_method)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_params(self):\\n        if self.n_topics is not None:\\n            self._n_components = self.n_topics\\n            warnings.warn(\"n_topics has been renamed to n_components in \"\\n                          \"version 0.19 and will be removed in 0.21\",\\n                          DeprecationWarning)\\n        else:\\n            self._n_components = self.n_components\\n        if self._n_components <= 0:\\n            raise ValueError(\"Invalid 'n_components' parameter: %r\"\\n                             % self._n_components)\\n        if self.total_samples <= 0:\\n            raise ValueError(\"Invalid 'total_samples' parameter: %r\"\\n                             % self.total_samples)\\n        if self.learning_offset < 0:\\n            raise ValueError(\"Invalid 'learning_offset' parameter: %r\"\\n                             % self.learning_offset)\\n        if self.learning_method not in (\"batch\", \"online\", None):\\n            raise ValueError(\"Invalid 'learning_method' parameter: %r\"\\n                             % self.learning_method)"
  },
  {
    "code": "def time_days_in_month(self, tz):\\n        self.ts.days_in_month",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fastpaths for Timestamp properties (#18539)",
    "fixed_code": "def time_days_in_month(self, tz, freq):\\n        self.ts.days_in_month"
  },
  {
    "code": "def find_pod(self, namespace, context) -> Optional[k8s.V1Pod]:\\n        label_selector = self._build_find_pod_label_selector(context)\\n        pod_list = self.client.list_namespaced_pod(\\n            namespace=namespace,\\n            label_selector=label_selector,\\n        ).items\\n        pod = None\\n        num_pods = len(pod_list)\\n        if num_pods > 1:\\n            raise AirflowException(f'More than one pod running with labels {label_selector}')\\n        elif num_pods == 1:\\n            pod = pod_list[0]\\n            self.log.info(\"Found matching pod %s with labels %s\", pod.metadata.name, pod.metadata.labels)\\n            self.log.info(\"`try_number` of task_instance: %s\", context['ti'].try_number)\\n            self.log.info(\"`try_number` of pod: %s\", pod.metadata.labels['try_number'])\\n        return pod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Use \"remote\" pod when patching KPO pod as \"checked\" (#23676)\\n\\nWhen patching as \"checked\", we have to use the current version of the pod otherwise we may get an error when trying to patch it, e.g.:\\n\\n```\\nOperation cannot be fulfilled on pods \\\"test-kubernetes-pod-db9eedb7885c40099dd40cd4edc62415\\\": the object has been modified; please apply your changes to the latest version and try again\"\\n```\\n\\nThis error would not cause a failure of the task, since errors in `cleanup` are suppressed.  However, it would fail to patch.\\n\\nI believe one scenario when the pod may be updated is when retrieving xcom, since the sidecar is terminated after extracting the value.\\n\\nConcerning some changes in the tests re the \"already_checked\" label, it was added to a few \"expected pods\" recently, when we changed it to patch even in the case of a successful pod.\\n\\nSince we are changing the \"patch\" code to patch with the latest read on the pod that we have (i.e. using the `remote_pod` variable), and no longer the pod object stored on `k.pod`, the label no longer shows up in those tests (that's because in k.pod isn't actually a read of the remote pod, but just happens to get mutated in the patch function before it is used to actually patch the pod).\\n\\nFurther, since the `remote_pod` is a local variable, we can't observe it in tests.  So we have to read the pod using k8s api. _But_, our \"find pod\" function excludes \"already checked\" pods!  So we have to make this configurable.\\n\\nSo, now we have a proper integration test for the \"already_checked\" behavior (there was already a unit test).",
    "fixed_code": "def find_pod(self, namespace, context, *, exclude_checked=True) -> Optional[k8s.V1Pod]:\\n        label_selector = self._build_find_pod_label_selector(context, exclude_checked=exclude_checked)\\n        pod_list = self.client.list_namespaced_pod(\\n            namespace=namespace,\\n            label_selector=label_selector,\\n        ).items\\n        pod = None\\n        num_pods = len(pod_list)\\n        if num_pods > 1:\\n            raise AirflowException(f'More than one pod running with labels {label_selector}')\\n        elif num_pods == 1:\\n            pod = pod_list[0]\\n            self.log.info(\"Found matching pod %s with labels %s\", pod.metadata.name, pod.metadata.labels)\\n            self.log.info(\"`try_number` of task_instance: %s\", context['ti'].try_number)\\n            self.log.info(\"`try_number` of pod: %s\", pod.metadata.labels['try_number'])\\n        return pod"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        vrf=dict(required=False, type='str', default='default'),\\n        ospf=dict(required=True, type='str'),\\n        router_id=dict(required=False, type='str'),\\n        default_metric=dict(required=False, type='str'),\\n        log_adjacency=dict(required=False, type='str', choices=['log', 'detail', 'default']),\\n        timer_throttle_lsa_start=dict(required=False, type='str'),\\n        timer_throttle_lsa_hold=dict(required=False, type='str'),\\n        timer_throttle_lsa_max=dict(required=False, type='str'),\\n        timer_throttle_spf_start=dict(required=False, type='str'),\\n        timer_throttle_spf_hold=dict(required=False, type='str'),\\n        timer_throttle_spf_max=dict(required=False, type='str'),\\n        auto_cost=dict(required=False, type='str'),\\n        bfd=dict(required=False, type='str', choices=['enable', 'disable']),\\n        passive_interface=dict(required=False, type='bool'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False)\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, commands=[], warnings=warnings)\\n    state = module.params['state']\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args)\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key != 'interface':\\n            if str(value).lower() == 'true':\\n                value = True\\n            elif str(value).lower() == 'false':\\n                value = False\\n            elif str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n                if value is None:\\n                    value = 'default'\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    if state == 'absent' and existing:\\n        state_absent(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        result['commands'] = candidate\\n        if not module.check_mode:\\n            load_config(module, candidate)\\n            result['changed'] = True\\n    module.exit_json(**result)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        vrf=dict(required=False, type='str', default='default'),\\n        ospf=dict(required=True, type='str'),\\n        router_id=dict(required=False, type='str'),\\n        default_metric=dict(required=False, type='str'),\\n        log_adjacency=dict(required=False, type='str', choices=['log', 'detail', 'default']),\\n        timer_throttle_lsa_start=dict(required=False, type='str'),\\n        timer_throttle_lsa_hold=dict(required=False, type='str'),\\n        timer_throttle_lsa_max=dict(required=False, type='str'),\\n        timer_throttle_spf_start=dict(required=False, type='str'),\\n        timer_throttle_spf_hold=dict(required=False, type='str'),\\n        timer_throttle_spf_max=dict(required=False, type='str'),\\n        auto_cost=dict(required=False, type='str'),\\n        bfd=dict(required=False, type='str', choices=['enable', 'disable']),\\n        passive_interface=dict(required=False, type='bool'),\\n        state=dict(choices=['present', 'absent'], default='present', required=False)\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = dict(changed=False, commands=[], warnings=warnings)\\n    state = module.params['state']\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args)\\n    proposed_args = dict((k, v) for k, v in module.params.items()\\n                         if v is not None and k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key != 'interface':\\n            if str(value).lower() == 'true':\\n                value = True\\n            elif str(value).lower() == 'false':\\n                value = False\\n            elif str(value).lower() == 'default':\\n                value = PARAM_TO_DEFAULT_KEYMAP.get(key)\\n                if value is None:\\n                    value = 'default'\\n            if existing.get(key) != value:\\n                proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    if state == 'present':\\n        state_present(module, existing, proposed, candidate)\\n    if state == 'absent' and existing:\\n        state_absent(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        result['commands'] = candidate\\n        if not module.check_mode:\\n            load_config(module, candidate)\\n            result['changed'] = True\\n    module.exit_json(**result)"
  },
  {
    "code": "def construct_1d_arraylike_from_scalar(value, length, dtype):\\n    if is_datetimetz(dtype):\\n        from pandas import DatetimeIndex\\n        subarr = DatetimeIndex([value] * length, dtype=dtype)\\n    elif is_categorical_dtype(dtype):\\n        from pandas import Categorical\\n        subarr = Categorical([value] * length, dtype=dtype)\\n    else:\\n        if not isinstance(dtype, (np.dtype, type(np.dtype))):\\n            dtype = dtype.dtype\\n        if is_integer_dtype(dtype) and isna(value):\\n            dtype = np.float64\\n        subarr = np.empty(length, dtype=dtype)\\n        subarr.fill(value)\\n    return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG GH22858 When creating empty dataframe, only cast int to float if index given (#22963)",
    "fixed_code": "def construct_1d_arraylike_from_scalar(value, length, dtype):\\n    if is_datetimetz(dtype):\\n        from pandas import DatetimeIndex\\n        subarr = DatetimeIndex([value] * length, dtype=dtype)\\n    elif is_categorical_dtype(dtype):\\n        from pandas import Categorical\\n        subarr = Categorical([value] * length, dtype=dtype)\\n    else:\\n        if not isinstance(dtype, (np.dtype, type(np.dtype))):\\n            dtype = dtype.dtype\\n        if length and is_integer_dtype(dtype) and isna(value):\\n            dtype = np.float64\\n        subarr = np.empty(length, dtype=dtype)\\n        subarr.fill(value)\\n    return subarr"
  },
  {
    "code": "def _strip_annotations(t):\\n    if isinstance(t, _AnnotatedAlias):\\n        return _strip_annotations(t.__origin__)\\n    if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired):\\n        return _strip_annotations(t.__args__[0])\\n    if isinstance(t, _GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return t.copy_with(stripped_args)\\n    if isinstance(t, GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return GenericAlias(t.__origin__, stripped_args)\\n    if isinstance(t, types.UnionType):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return functools.reduce(operator.or_, stripped_args)\\n    return t",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _strip_annotations(t):\\n    if isinstance(t, _AnnotatedAlias):\\n        return _strip_annotations(t.__origin__)\\n    if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired):\\n        return _strip_annotations(t.__args__[0])\\n    if isinstance(t, _GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return t.copy_with(stripped_args)\\n    if isinstance(t, GenericAlias):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return GenericAlias(t.__origin__, stripped_args)\\n    if isinstance(t, types.UnionType):\\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\\n        if stripped_args == t.__args__:\\n            return t\\n        return functools.reduce(operator.or_, stripped_args)\\n    return t"
  },
  {
    "code": "def _get_offset(name: str) -> DateOffset:\\n    if name not in libfreqs._dont_uppercase:\\n        name = name.upper()\\n        name = libfreqs._lite_rule_alias.get(name, name)\\n        name = libfreqs._lite_rule_alias.get(name.lower(), name)\\n    else:\\n        name = libfreqs._lite_rule_alias.get(name, name)\\n    if name not in _offset_map:\\n        try:\\n            split = name.split(\"-\")\\n            klass = prefix_mapping[split[0]]\\n            offset = klass._from_name(*split[1:])\\n        except (ValueError, TypeError, KeyError) as err:\\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(name)) from err\\n        _offset_map[name] = offset\\n    return _offset_map[name]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_offset(name: str) -> DateOffset:\\n    if name not in libfreqs._dont_uppercase:\\n        name = name.upper()\\n        name = libfreqs._lite_rule_alias.get(name, name)\\n        name = libfreqs._lite_rule_alias.get(name.lower(), name)\\n    else:\\n        name = libfreqs._lite_rule_alias.get(name, name)\\n    if name not in _offset_map:\\n        try:\\n            split = name.split(\"-\")\\n            klass = prefix_mapping[split[0]]\\n            offset = klass._from_name(*split[1:])\\n        except (ValueError, TypeError, KeyError) as err:\\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(name)) from err\\n        _offset_map[name] = offset\\n    return _offset_map[name]"
  },
  {
    "code": "def copy(self, cursor, f):\\n\\t\\tlogger.info(\"Inserting file: %s\", f)\\n\\t\\tcolnames = ''\\n\\t\\tif self.columns and len(self.columns) > 0:\\n\\t\\t\\tcolnames = \",\".join([x[0] for x in self.columns])\\n\\t\\t\\tcolnames = '({})'.format(colnames)\\n\\t\\tcursor.execute(.format(\\n\\t\\t\\ttable=self.table,\\n\\t\\t\\tcolnames=colnames,\\n\\t\\t\\tsource=f,\\n\\t\\t\\tcreds=self._credentials(),\\n\\t\\t\\toptions=self.copy_options)\\n\\t\\t)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def copy(self, cursor, f):\\n\\t\\tlogger.info(\"Inserting file: %s\", f)\\n\\t\\tcolnames = ''\\n\\t\\tif self.columns and len(self.columns) > 0:\\n\\t\\t\\tcolnames = \",\".join([x[0] for x in self.columns])\\n\\t\\t\\tcolnames = '({})'.format(colnames)\\n\\t\\tcursor.execute(.format(\\n\\t\\t\\ttable=self.table,\\n\\t\\t\\tcolnames=colnames,\\n\\t\\t\\tsource=f,\\n\\t\\t\\tcreds=self._credentials(),\\n\\t\\t\\toptions=self.copy_options)\\n\\t\\t)"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        host=dict(required=True, type=\"str\"),\\n        username=dict(fallback=(env_fallback, [\"ANSIBLE_NET_USERNAME\"])),\\n        password=dict(fallback=(env_fallback, [\"ANSIBLE_NET_PASSWORD\"]), no_log=True),\\n        object=dict(required=True, type=\"str\", choices=[\"device\", \"cluster_nodes\", \"task\", \"custom\"]),\\n        custom_endpoint=dict(required=False, type=\"str\"),\\n        custom_dict=dict(required=False, type=\"dict\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_unique_name=dict(required=False, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\"),\\n        nodes=dict(required=False, type=\"list\"),\\n        task_id=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec, supports_check_mode=True, )\\n    host = module.params[\"host\"]\\n    username = module.params[\"username\"]\\n    if host is None or username is None:\\n        module.fail_json(msg=\"Host and username are required\")\\n    fmg = AnsibleFortiManager(module, module.params[\"host\"], module.params[\"username\"], module.params[\"password\"])\\n    response = fmg.login()\\n    if response[1]['status']['code'] != 0:\\n        module.fail_json(msg=\"Connection to FortiManager Failed\")\\n    paramgram = {\\n        \"adom\": module.params[\"adom\"],\\n        \"object\": module.params[\"object\"],\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"nodes\": module.params[\"nodes\"],\\n        \"task_id\": module.params[\"task_id\"],\\n        \"custom_endpoint\": module.params[\"custom_endpoint\"],\\n        \"custom_dict\": module.params[\"custom_dict\"]\\n    }\\n    if paramgram[\"object\"] == \"device\" and any(v is not None for v in [paramgram[\"device_unique_name\"],\\n                                               paramgram[\"device_serial\"], paramgram[\"device_ip\"]]):\\n        results = fmgr_get_device(fmg, paramgram)\\n        if results[0] not in [0]:\\n            module.fail_json(msg=\"Device query failed!\")\\n        elif len(results[1]) == 0:\\n            module.exit_json(msg=\"Device NOT FOUND!\")\\n        else:\\n            module.exit_json(msg=\"Device Found\", **results[1][0])\\n    if paramgram[\"object\"] == \"cluster_nodes\" and paramgram[\"nodes\"] is not None:\\n        results = fmgr_get_cluster_nodes(fmg, paramgram)\\n        if results[\"cluster_status\"] == \"MISSING\":\\n            module.exit_json(msg=\"No cluster device found!\", **results)\\n        elif results[\"query_status\"] == \"good\":\\n            module.exit_json(msg=\"Cluster Found - Showing Nodes\", **results)\\n        elif results is None:\\n            module.fail_json(msg=\"Query FAILED -- Check module or playbook syntax\")\\n    if paramgram[\"object\"] == \"task\":\\n        results = fmgr_get_task_status(fmg, paramgram)\\n        if results[0] != 0:\\n            module.fail_json(msg=\"QUERY FAILED -- Is FMGR online? Good Creds?\")\\n        if results[0] == 0:\\n            module.exit_json(msg=\"Task Found\", **results[1])\\n    if paramgram[\"object\"] == \"custom\":\\n        results = fmgr_get_custom(fmg, paramgram)\\n        if results[0] != 0:\\n            module.fail_json(msg=\"QUERY FAILED -- Please check syntax check JSON guide if needed.\")\\n        if results[0] == 0:\\n            results_len = len(results[1])\\n            if results_len > 0:\\n                results_combine = dict()\\n                if isinstance(results[1], dict):\\n                    results_combine[\"results\"] = results[1]\\n                if isinstance(results[1], list):\\n                    results_combine[\"results\"] = results[1][0:results_len]\\n                module.exit_json(msg=\"Custom Query Success\", **results_combine)\\n            else:\\n                module.exit_json(msg=\"NO RESULTS\")\\n    fmg.logout()\\n    return module.fail_json(msg=\"Parameters weren't right, logic tree didn't validate. Check playbook.\")",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FortiManager Plugin Module Conversion: fmgr_query (#52770)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        adom=dict(required=False, type=\"str\", default=\"root\"),\\n        object=dict(required=True, type=\"str\", choices=[\"device\", \"cluster_nodes\", \"task\", \"custom\"]),\\n        custom_endpoint=dict(required=False, type=\"str\"),\\n        custom_dict=dict(required=False, type=\"dict\"),\\n        device_ip=dict(required=False, type=\"str\"),\\n        device_unique_name=dict(required=False, type=\"str\"),\\n        device_serial=dict(required=False, type=\"str\"),\\n        nodes=dict(required=False, type=\"list\"),\\n        task_id=dict(required=False, type=\"str\")\\n    )\\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, )\\n    paramgram = {\\n        \"adom\": module.params[\"adom\"],\\n        \"object\": module.params[\"object\"],\\n        \"device_ip\": module.params[\"device_ip\"],\\n        \"device_unique_name\": module.params[\"device_unique_name\"],\\n        \"device_serial\": module.params[\"device_serial\"],\\n        \"nodes\": module.params[\"nodes\"],\\n        \"task_id\": module.params[\"task_id\"],\\n        \"custom_endpoint\": module.params[\"custom_endpoint\"],\\n        \"custom_dict\": module.params[\"custom_dict\"]\\n    }\\n    module.paramgram = paramgram\\n    fmgr = None\\n    if module._socket_path:\\n        connection = Connection(module._socket_path)\\n        fmgr = FortiManagerHandler(connection, module)\\n        fmgr.tools = FMGRCommon()\\n    else:\\n        module.fail_json(**FAIL_SOCKET_MSG)\\n    results = DEFAULT_RESULT_OBJ\\n    try:\\n        if paramgram[\"object\"] == \"device\" and any(v is not None for v in [paramgram[\"device_unique_name\"],\\n                                                                           paramgram[\"device_serial\"],\\n                                                                           paramgram[\"device_ip\"]]):\\n            results = fmgr_get_device(fmgr, paramgram)\\n            if results[0] not in [0]:\\n                module.fail_json(msg=\"Device query failed!\")\\n            elif len(results[1]) == 0:\\n                module.exit_json(msg=\"Device NOT FOUND!\")\\n            else:\\n                module.exit_json(msg=\"Device Found\", **results[1][0])\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"cluster_nodes\" and paramgram[\"nodes\"] is not None:\\n            results = fmgr_get_cluster_nodes(fmgr, paramgram)\\n            if results[\"cluster_status\"] == \"MISSING\":\\n                module.exit_json(msg=\"No cluster device found!\", **results)\\n            elif results[\"query_status\"] == \"good\":\\n                module.exit_json(msg=\"Cluster Found - Showing Nodes\", **results)\\n            elif results is None:\\n                module.fail_json(msg=\"Query FAILED -- Check module or playbook syntax\")\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"task\":\\n            results = fmgr_get_task_status(fmgr, paramgram)\\n            if results[0] != 0:\\n                module.fail_json(**results[1])\\n            if results[0] == 0:\\n                module.exit_json(**results[1])\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    try:\\n        if paramgram[\"object\"] == \"custom\":\\n            results = fmgr_get_custom(fmgr, paramgram)\\n            if results[0] != 0:\\n                module.fail_json(msg=\"QUERY FAILED -- Please check syntax check JSON guide if needed.\")\\n            if results[0] == 0:\\n                results_len = len(results[1])\\n                if results_len > 0:\\n                    results_combine = dict()\\n                    if isinstance(results[1], dict):\\n                        results_combine[\"results\"] = results[1]\\n                    if isinstance(results[1], list):\\n                        results_combine[\"results\"] = results[1][0:results_len]\\n                    module.exit_json(msg=\"Custom Query Success\", **results_combine)\\n                else:\\n                    module.exit_json(msg=\"NO RESULTS\")\\n    except Exception as err:\\n        raise FMGBaseException(err)\\n    return module.exit_json(**results[1])"
  },
  {
    "code": "def set_status(self, task, new_status, config=None):\\n\\t\\tif new_status == FAILED:\\n\\t\\t\\tassert config is not None\\n\\t\\tif new_status == DISABLED and task.status == RUNNING:\\n\\t\\t\\treturn\\n\\t\\tif task.status == DISABLED:\\n\\t\\t\\tif new_status == DONE:\\n\\t\\t\\t\\tself.re_enable(task)\\n\\t\\t\\telif task.scheduler_disable_time is not None:\\n\\t\\t\\t\\treturn\\n\\t\\tif new_status == FAILED and task.can_disable() and task.status != DISABLED:\\n\\t\\t\\ttask.add_failure()\\n\\t\\t\\tif task.has_excessive_failures():\\n\\t\\t\\t\\ttask.scheduler_disable_time = time.time()\\n\\t\\t\\t\\tnew_status = DISABLED\\n\\t\\t\\t\\tnotifications.send_error_email(\\n\\t\\t\\t\\t\\t'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\\n\\t\\t\\t\\t\\t'{task} failed {failures} times in the last {window} seconds, so it is being '\\n\\t\\t\\t\\t\\t'disabled for {persist} seconds'.format(\\n\\t\\t\\t\\t\\t\\tfailures=config.disable_failures,\\n\\t\\t\\t\\t\\t\\ttask=task.id,\\n\\t\\t\\t\\t\\t\\twindow=config.disable_window,\\n\\t\\t\\t\\t\\t\\tpersist=config.disable_persist,\\n\\t\\t\\t\\t\\t))\\n\\t\\telif new_status == DISABLED:\\n\\t\\t\\ttask.scheduler_disable_time = None\\n\\t\\tself._status_tasks[task.status].pop(task.id)\\n\\t\\tself._status_tasks[new_status][task.id] = task\\n\\t\\ttask.status = new_status",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def set_status(self, task, new_status, config=None):\\n\\t\\tif new_status == FAILED:\\n\\t\\t\\tassert config is not None\\n\\t\\tif new_status == DISABLED and task.status == RUNNING:\\n\\t\\t\\treturn\\n\\t\\tif task.status == DISABLED:\\n\\t\\t\\tif new_status == DONE:\\n\\t\\t\\t\\tself.re_enable(task)\\n\\t\\t\\telif task.scheduler_disable_time is not None:\\n\\t\\t\\t\\treturn\\n\\t\\tif new_status == FAILED and task.can_disable() and task.status != DISABLED:\\n\\t\\t\\ttask.add_failure()\\n\\t\\t\\tif task.has_excessive_failures():\\n\\t\\t\\t\\ttask.scheduler_disable_time = time.time()\\n\\t\\t\\t\\tnew_status = DISABLED\\n\\t\\t\\t\\tnotifications.send_error_email(\\n\\t\\t\\t\\t\\t'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\\n\\t\\t\\t\\t\\t'{task} failed {failures} times in the last {window} seconds, so it is being '\\n\\t\\t\\t\\t\\t'disabled for {persist} seconds'.format(\\n\\t\\t\\t\\t\\t\\tfailures=config.disable_failures,\\n\\t\\t\\t\\t\\t\\ttask=task.id,\\n\\t\\t\\t\\t\\t\\twindow=config.disable_window,\\n\\t\\t\\t\\t\\t\\tpersist=config.disable_persist,\\n\\t\\t\\t\\t\\t))\\n\\t\\telif new_status == DISABLED:\\n\\t\\t\\ttask.scheduler_disable_time = None\\n\\t\\tself._status_tasks[task.status].pop(task.id)\\n\\t\\tself._status_tasks[new_status][task.id] = task\\n\\t\\ttask.status = new_status"
  },
  {
    "code": "def main():\\n    argument_spec = dict(\\n        ssm_range=dict(required=True, type='list'),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = {'changed': False, 'commands': [], 'warnings': warnings}\\n    ssm_range_list = module.params['ssm_range']\\n    for item in ssm_range_list:\\n        splitted_ssm_range = item.split('.')\\n        if len(splitted_ssm_range) != 4 and item != 'none' and item != 'default':\\n            module.fail_json(msg=\"Valid ssm_range values are multicast addresses \"\\n                                 \"or the keyword 'none' or the keyword 'default'.\")\\n    args = PARAM_TO_COMMAND_KEYMAP.keys()\\n    existing = get_existing(module, args)\\n    proposed_args = dict((k, v) for k, v in module.params.items() if k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key == 'ssm_range':\\n            if value[0] == 'default':\\n                if existing.get(key):\\n                    proposed[key] = 'default'\\n            else:\\n                v = sorted(set([str(i) for i in value]))\\n                ex = sorted(set([str(i) for i in existing.get(key)]))\\n                if v != ex:\\n                    proposed[key] = ' '.join(str(s) for s in v)\\n    candidate = CustomNetworkConfig(indent=3)\\n    get_commands(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        result['commands'] = candidate\\n        result['changed'] = True\\n        load_config(module, candidate)\\n    module.exit_json(**result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "nxos_pim: Add bfd support (#56908)",
    "fixed_code": "def main():\\n    argument_spec = dict(\\n        bfd=dict(required=False, type='str', choices=['enable', 'disable']),\\n        ssm_range=dict(required=False, type='list', default=[]),\\n    )\\n    argument_spec.update(nxos_argument_spec)\\n    module = AnsibleModule(argument_spec=argument_spec,\\n                           supports_check_mode=True)\\n    warnings = list()\\n    check_args(module, warnings)\\n    result = {'changed': False, 'commands': [], 'warnings': warnings}\\n    params = module.params\\n    args = [k for k in PARAM_TO_COMMAND_KEYMAP.keys() if params[k] is not None]\\n    if 'ssm_range' in args:\\n        for item in params['ssm_range']:\\n            if re.search('none|default', item):\\n                break\\n            if len(item.split('.')) != 4:\\n                module.fail_json(msg=\"Valid ssm_range values are multicast addresses \"\\n                                     \"or the keyword 'none' or the keyword 'default'.\")\\n    existing = get_existing(module, args)\\n    proposed_args = dict((k, v) for k, v in params.items() if k in args)\\n    proposed = {}\\n    for key, value in proposed_args.items():\\n        if key == 'ssm_range':\\n            if value and value[0] == 'default':\\n                if existing.get(key):\\n                    proposed[key] = 'default'\\n            else:\\n                v = sorted(set([str(i) for i in value]))\\n                ex = sorted(set([str(i) for i in existing.get(key, [])]))\\n                if v != ex:\\n                    proposed[key] = ' '.join(str(s) for s in v)\\n        elif key == 'bfd':\\n            if value != existing.get('bfd', 'disable'):\\n                proposed[key] = value\\n        elif value != existing.get(key):\\n            proposed[key] = value\\n    candidate = CustomNetworkConfig(indent=3)\\n    get_commands(module, existing, proposed, candidate)\\n    if candidate:\\n        candidate = candidate.items_text()\\n        result['commands'] = candidate\\n        result['changed'] = True\\n        load_config(module, candidate)\\n    module.exit_json(**result)"
  },
  {
    "code": "def _ClassDef(self, t):\\n        self.write(\"\\n\")\\n        for deco in t.decorator_list:\\n            self.fill(\"@\")\\n            self.dispatch(deco)\\n        self.fill(\"class \"+t.name)\\n        self.write(\"(\")\\n        comma = False\\n        for e in t.bases:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.dispatch(e)\\n        for e in t.keywords:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.dispatch(e)\\n        if t.starargs:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.write(\"*\")\\n            self.dispatch(t.starargs)\\n        if t.kwargs:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.write(\"**\")\\n            self.dispatch(t.kwargs)\\n        self.write(\")\")\\n        self.enter()\\n        self.dispatch(t.body)\\n        self.leave()",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ClassDef(self, t):\\n        self.write(\"\\n\")\\n        for deco in t.decorator_list:\\n            self.fill(\"@\")\\n            self.dispatch(deco)\\n        self.fill(\"class \"+t.name)\\n        self.write(\"(\")\\n        comma = False\\n        for e in t.bases:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.dispatch(e)\\n        for e in t.keywords:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.dispatch(e)\\n        if t.starargs:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.write(\"*\")\\n            self.dispatch(t.starargs)\\n        if t.kwargs:\\n            if comma: self.write(\", \")\\n            else: comma = True\\n            self.write(\"**\")\\n            self.dispatch(t.kwargs)\\n        self.write(\")\")\\n        self.enter()\\n        self.dispatch(t.body)\\n        self.leave()"
  },
  {
    "code": "def kill_container(self):\\n\\t\\t''\\n\\t\\tcontainer_name = self.config.container_name\\n\\t\\tif container_name:\\n\\t\\t\\tcontainer_cli = self.config.process_isolation_executable\\n\\t\\t\\tcmd = [container_cli, 'kill', container_name]\\n\\t\\t\\tproc = Popen(cmd, stdout=PIPE, stderr=PIPE)\\n\\t\\t\\t_, stderr = proc.communicate()\\n\\t\\t\\tif proc.returncode:\\n\\t\\t\\t\\tlogger.info('Error from {} kill {} command:\\n{}'.format(container_cli, container_name, stderr))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlogger.info(\"Killed container {}\".format(container_name))\\n\\t@classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def kill_container(self):\\n\\t\\t''\\n\\t\\tcontainer_name = self.config.container_name\\n\\t\\tif container_name:\\n\\t\\t\\tcontainer_cli = self.config.process_isolation_executable\\n\\t\\t\\tcmd = [container_cli, 'kill', container_name]\\n\\t\\t\\tproc = Popen(cmd, stdout=PIPE, stderr=PIPE)\\n\\t\\t\\t_, stderr = proc.communicate()\\n\\t\\t\\tif proc.returncode:\\n\\t\\t\\t\\tlogger.info('Error from {} kill {} command:\\n{}'.format(container_cli, container_name, stderr))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlogger.info(\"Killed container {}\".format(container_name))\\n\\t@classmethod"
  },
  {
    "code": "def cartesian_product(X):\\n    ABC'), [1, 2]])\\n    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='|S1'),\\n    array([1, 2, 1, 2, 1, 2])]\\n    '''\\n    lenX = np.fromiter((len(x) for x in X), dtype=int)\\n    cumprodX = np.cumproduct(lenX)\\n    a = np.roll(cumprodX, 1)\\n    a[0] = 1\\n    b = cumprodX[-1] / cumprodX\\n    return [np.tile(np.repeat(np.asarray(com._values_from_object(x)), b[i]),\\n                    np.product(a[i]))\\n               for i, x in enumerate(X)]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def cartesian_product(X):\\n    ABC'), [1, 2]])\\n    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='|S1'),\\n    array([1, 2, 1, 2, 1, 2])]\\n    '''\\n    lenX = np.fromiter((len(x) for x in X), dtype=int)\\n    cumprodX = np.cumproduct(lenX)\\n    a = np.roll(cumprodX, 1)\\n    a[0] = 1\\n    b = cumprodX[-1] / cumprodX\\n    return [np.tile(np.repeat(np.asarray(com._values_from_object(x)), b[i]),\\n                    np.product(a[i]))\\n               for i, x in enumerate(X)]"
  },
  {
    "code": "def compiler_fixup(compiler_so, cc_args):\\n    stripArch = stripSysroot = False\\n    compiler_so = list(compiler_so)\\n    if not _supports_universal_builds():\\n        stripArch = stripSysroot = True\\n    else:\\n        stripArch = '-arch' in cc_args\\n        stripSysroot = '-isysroot' in cc_args\\n    if stripArch or 'ARCHFLAGS' in os.environ:\\n        while True:\\n            try:\\n                index = compiler_so.index('-arch')\\n                del compiler_so[index:index+2]\\n            except ValueError:\\n                break\\n    if 'ARCHFLAGS' in os.environ and not stripArch:\\n        compiler_so = compiler_so + os.environ['ARCHFLAGS'].split()\\n    if stripSysroot:\\n        while True:\\n            try:\\n                index = compiler_so.index('-isysroot')\\n                del compiler_so[index:index+2]\\n            except ValueError:\\n                break\\n    sysroot = None\\n    if '-isysroot' in cc_args:\\n        idx = cc_args.index('-isysroot')\\n        sysroot = cc_args[idx+1]\\n    elif '-isysroot' in compiler_so:\\n        idx = compiler_so.index('-isysroot')\\n        sysroot = compiler_so[idx+1]\\n    if sysroot and not os.path.isdir(sysroot):\\n        from distutils import log\\n        log.warn(\"Compiling with an SDK that doesn't seem to exist: %s\",\\n                sysroot)\\n        log.warn(\"Please check your Xcode installation\")\\n    return compiler_so",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-38360: macOS: support alternate form of -isysroot flag (GH-16480)\\n\\nIt is possible to use either '-isysroot /some/path' (with a space) or\\n'-isysroot/some/path' (no space in between). Support both forms in\\nplaces where special handling of -isysroot is done, rather than just\\nthe first form.",
    "fixed_code": "def compiler_fixup(compiler_so, cc_args):\\n    stripArch = stripSysroot = False\\n    compiler_so = list(compiler_so)\\n    if not _supports_universal_builds():\\n        stripArch = stripSysroot = True\\n    else:\\n        stripArch = '-arch' in cc_args\\n        stripSysroot = any(arg for arg in cc_args if arg.startswith('-isysroot'))\\n    if stripArch or 'ARCHFLAGS' in os.environ:\\n        while True:\\n            try:\\n                index = compiler_so.index('-arch')\\n                del compiler_so[index:index+2]\\n            except ValueError:\\n                break\\n    if 'ARCHFLAGS' in os.environ and not stripArch:\\n        compiler_so = compiler_so + os.environ['ARCHFLAGS'].split()\\n    if stripSysroot:\\n        while True:\\n            indices = [i for i,x in enumerate(compiler_so) if x.startswith('-isysroot')]\\n            if not indices:\\n                break\\n            index = indices[0]\\n            if compiler_so[index] == '-isysroot':\\n                del compiler_so[index:index+2]\\n            else:\\n                del compiler_so[index:index+1]\\n    sysroot = None\\n    argvar = cc_args\\n    indices = [i for i,x in enumerate(cc_args) if x.startswith('-isysroot')]\\n    if not indices:\\n        argvar = compiler_so\\n        indices = [i for i,x in enumerate(compiler_so) if x.startswith('-isysroot')]\\n    for idx in indices:\\n        if argvar[idx] == '-isysroot':\\n            sysroot = argvar[idx+1]\\n            break\\n        else:\\n            sysroot = argvar[idx][len('-isysroot'):]\\n            break\\n    if sysroot and not os.path.isdir(sysroot):\\n        from distutils import log\\n        log.warn(\"Compiling with an SDK that doesn't seem to exist: %s\",\\n                sysroot)\\n        log.warn(\"Please check your Xcode installation\")\\n    return compiler_so"
  },
  {
    "code": "def __str__(self):\\n        return str(self.data)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Handle vaulted non-ascii characters for Python2 (#58503)",
    "fixed_code": "def __str__(self):\\n        return to_native(self.data, errors='surrogate_or_strict')"
  },
  {
    "code": "def cov(self, min_periods=None) -> \"DataFrame\":\\n\\t\\tnumeric_df = self._get_numeric_data()\\n\\t\\tcols = numeric_df.columns\\n\\t\\tidx = cols.copy()\\n\\t\\tmat = numeric_df.values\\n\\t\\tif notna(mat).all():\\n\\t\\t\\tif min_periods is not None and min_periods > len(mat):\\n\\t\\t\\t\\tbaseCov = np.empty((mat.shape[1], mat.shape[1]))\\n\\t\\t\\t\\tbaseCov.fill(np.nan)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbaseCov = np.cov(mat.T)\\n\\t\\t\\tbaseCov = baseCov.reshape((len(cols), len(cols)))\\n\\t\\telse:\\n\\t\\t\\tbaseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\\n\\t\\treturn self._constructor(baseCov, index=idx, columns=cols)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Don't raise in DataFrame.corr with pd.NA (#33809)",
    "fixed_code": "def cov(self, min_periods=None) -> \"DataFrame\":\\n\\t\\tnumeric_df = self._get_numeric_data()\\n\\t\\tcols = numeric_df.columns\\n\\t\\tidx = cols.copy()\\n\\t\\tmat = numeric_df.astype(float, copy=False).to_numpy()\\n\\t\\tif notna(mat).all():\\n\\t\\t\\tif min_periods is not None and min_periods > len(mat):\\n\\t\\t\\t\\tbase_cov = np.empty((mat.shape[1], mat.shape[1]))\\n\\t\\t\\t\\tbase_cov.fill(np.nan)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tbase_cov = np.cov(mat.T)\\n\\t\\t\\tbase_cov = base_cov.reshape((len(cols), len(cols)))\\n\\t\\telse:\\n\\t\\t\\tbase_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\\n\\t\\treturn self._constructor(base_cov, index=idx, columns=cols)"
  },
  {
    "code": "def sanitize_array(\\n    data,\\n    index: Optional[Index],\\n    dtype: Optional[DtypeObj] = None,\\n    copy: bool = False,\\n    raise_cast_failure: bool = False,\\n) -> ArrayLike:\\n    if isinstance(data, ma.MaskedArray):\\n        data = sanitize_masked_array(data)\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray) and data.ndim == 0:\\n        if dtype is None:\\n            dtype = data.dtype\\n        data = lib.item_from_zerodim(data)\\n    if isinstance(data, np.ndarray):\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except ValueError:\\n                if copy:\\n                    subarr = data.copy()\\n                else:\\n                    subarr = np.array(data, copy=False)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n        return subarr\\n    elif isinstance(data, (list, tuple, abc.Set, abc.ValuesView)) and len(data) > 0:\\n        if isinstance(data, set):\\n            raise TypeError(\"Set type is unordered\")\\n        data = list(data)\\n        if dtype is not None:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n            subarr = maybe_cast_to_datetime(subarr, dtype)\\n    elif isinstance(data, range):\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\\n        subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\\n    elif not is_list_like(data):\\n        if index is None:\\n            raise ValueError(\"index must be specified when data is not list-like\")\\n        subarr = construct_1d_arraylike_from_scalar(data, len(index), dtype)\\n    else:\\n        subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    subarr = _sanitize_ndim(subarr, data, dtype, index)\\n    if not (is_extension_array_dtype(subarr.dtype) or is_extension_array_dtype(dtype)):\\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\\n        is_object_or_str_dtype = is_object_dtype(dtype) or is_string_dtype(dtype)\\n        if is_object_dtype(subarr.dtype) and not is_object_or_str_dtype:\\n            inferred = lib.infer_dtype(subarr, skipna=False)\\n            if inferred in {\"interval\", \"period\"}:\\n                subarr = array(subarr)\\n    return subarr",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REF: collected dtypes.cast and construction simplifications (#38629)",
    "fixed_code": "def sanitize_array(\\n    data,\\n    index: Optional[Index],\\n    dtype: Optional[DtypeObj] = None,\\n    copy: bool = False,\\n    raise_cast_failure: bool = False,\\n) -> ArrayLike:\\n    if isinstance(data, ma.MaskedArray):\\n        data = sanitize_masked_array(data)\\n    data = extract_array(data, extract_numpy=True)\\n    if isinstance(data, np.ndarray) and data.ndim == 0:\\n        if dtype is None:\\n            dtype = data.dtype\\n        data = lib.item_from_zerodim(data)\\n    if isinstance(data, np.ndarray):\\n        if dtype is not None and is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n            try:\\n                subarr = _try_cast(data, dtype, copy, True)\\n            except ValueError:\\n                subarr = np.array(data, copy=copy)\\n        else:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    elif isinstance(data, ABCExtensionArray):\\n        subarr = data\\n        if dtype is not None:\\n            subarr = subarr.astype(dtype, copy=copy)\\n        elif copy:\\n            subarr = subarr.copy()\\n        return subarr\\n    elif isinstance(data, (list, tuple, abc.Set, abc.ValuesView)) and len(data) > 0:\\n        if isinstance(data, set):\\n            raise TypeError(\"Set type is unordered\")\\n        data = list(data)\\n        if dtype is not None:\\n            subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n        else:\\n            subarr = maybe_convert_platform(data)\\n            subarr = maybe_cast_to_datetime(subarr, dtype)\\n    elif isinstance(data, range):\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\\n        subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\\n    elif not is_list_like(data):\\n        if index is None:\\n            raise ValueError(\"index must be specified when data is not list-like\")\\n        subarr = construct_1d_arraylike_from_scalar(data, len(index), dtype)\\n    else:\\n        subarr = _try_cast(data, dtype, copy, raise_cast_failure)\\n    subarr = _sanitize_ndim(subarr, data, dtype, index)\\n    if not (is_extension_array_dtype(subarr.dtype) or is_extension_array_dtype(dtype)):\\n        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)\\n        is_object_or_str_dtype = is_object_dtype(dtype) or is_string_dtype(dtype)\\n        if is_object_dtype(subarr.dtype) and not is_object_or_str_dtype:\\n            inferred = lib.infer_dtype(subarr, skipna=False)\\n            if inferred in {\"interval\", \"period\"}:\\n                subarr = array(subarr)\\n    return subarr"
  },
  {
    "code": "def predict_proba(self, X):\\n\\t\\tcheck_is_fitted(self, \"n_classes_\")\\n\\t\\tn_classes = self.n_classes_\\n\\t\\tX = self._validate_X_predict(X)\\n\\t\\tif self.algorithm == 'SAMME.R':\\n\\t\\t\\tproba = sum(_samme_proba(estimator, n_classes, X)\\n\\t\\t\\t\\t\\t\\tfor estimator in self.estimators_)\\n\\t\\telse:   \\n\\t\\t\\tproba = sum(estimator.predict_proba(X) * w\\n\\t\\t\\t\\t\\t\\tfor estimator, w in zip(self.estimators_,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself.estimator_weights_))\\n\\t\\tproba /= self.estimator_weights_.sum()\\n\\t\\tproba = np.exp((1. / (n_classes - 1)) * proba)\\n\\t\\tnormalizer = proba.sum(axis=1)[:, np.newaxis]\\n\\t\\tnormalizer[normalizer == 0.0] = 1.0\\n\\t\\tproba /= normalizer\\n\\t\\treturn proba",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[MRG+1] FIX AdaBoost ZeroDivisionError in proba #7501 (#8371)",
    "fixed_code": "def predict_proba(self, X):\\n\\t\\tcheck_is_fitted(self, \"n_classes_\")\\n\\t\\tn_classes = self.n_classes_\\n\\t\\tX = self._validate_X_predict(X)\\n\\t\\tif n_classes == 1:\\n\\t\\t\\treturn np.ones((X.shape[0], 1))\\n\\t\\tif self.algorithm == 'SAMME.R':\\n\\t\\t\\tproba = sum(_samme_proba(estimator, n_classes, X)\\n\\t\\t\\t\\t\\t\\tfor estimator in self.estimators_)\\n\\t\\telse:   \\n\\t\\t\\tproba = sum(estimator.predict_proba(X) * w\\n\\t\\t\\t\\t\\t\\tfor estimator, w in zip(self.estimators_,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tself.estimator_weights_))\\n\\t\\tproba /= self.estimator_weights_.sum()\\n\\t\\tproba = np.exp((1. / (n_classes - 1)) * proba)\\n\\t\\tnormalizer = proba.sum(axis=1)[:, np.newaxis]\\n\\t\\tnormalizer[normalizer == 0.0] = 1.0\\n\\t\\tproba /= normalizer\\n\\t\\treturn proba"
  },
  {
    "code": "def _from_derivatives(xi, yi, x, order=None, der=0, extrapolate=False):\\n    import scipy\\n    from scipy import interpolate\\n    if LooseVersion(scipy.__version__) < '0.18.0':\\n        try:\\n            method = interpolate.piecewise_polynomial_interpolate\\n            return method(xi, yi.reshape(-1, 1), x,\\n                          orders=order, der=der)\\n        except AttributeError:\\n            pass\\n    method = interpolate.BPoly.from_derivatives\\n    m = method(xi, yi.reshape(-1, 1),\\n               orders=order, extrapolate=extrapolate)\\n    return m(x)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Explicit LooseVersion comps (#18637)",
    "fixed_code": "def _from_derivatives(xi, yi, x, order=None, der=0, extrapolate=False):\\n    import scipy\\n    from scipy import interpolate\\n    if LooseVersion(scipy.__version__) < LooseVersion('0.18.0'):\\n        try:\\n            method = interpolate.piecewise_polynomial_interpolate\\n            return method(xi, yi.reshape(-1, 1), x,\\n                          orders=order, der=der)\\n        except AttributeError:\\n            pass\\n    method = interpolate.BPoly.from_derivatives\\n    m = method(xi, yi.reshape(-1, 1),\\n               orders=order, extrapolate=extrapolate)\\n    return m(x)"
  },
  {
    "code": "def unquote_to_bytes(string):\\n    if isinstance(string, str):\\n        string = string.encode('utf-8')\\n    res = string.split(b'%')\\n    res[0] = res[0]\\n    for i in range(1, len(res)):\\n        item = res[i]\\n        try:\\n            res[i] = bytes([int(item[:2], 16)]) + item[2:]\\n        except ValueError:\\n            res[i] = b'%' + item\\n    return b''.join(res)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 76719,81270-81272,83294,83319,84038-84039 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n................\\n  r76719 | antoine.pitrou | 2009-12-08 20:38:17 +0100 (mar., 08 d\u00e9c. 2009) | 9 lines\\n\\n  Merged revisions 76718 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r76718 | antoine.pitrou | 2009-12-08 20:35:12 +0100 (mar., 08 d\u00e9c. 2009) | 3 lines\\n\\n    Fix transient refleaks in test_urllib. Thanks to Florent Xicluna.\\n  ........\\n................\\n  r81270 | florent.xicluna | 2010-05-17 19:24:07 +0200 (lun., 17 mai 2010) | 9 lines\\n\\n  Merged revision 81259 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r81259 | florent.xicluna | 2010-05-17 12:39:07 +0200 (lun, 17 mai 2010) | 2 lines\\n\\n    Slight style cleanup.\\n  ........\\n................\\n  r81271 | florent.xicluna | 2010-05-17 19:33:07 +0200 (lun., 17 mai 2010) | 11 lines\\n\\n  Issue #1285086: Speed up urllib.parse functions: quote, quote_from_bytes, unquote, unquote_to_bytes.\\n\\n  Recorded merge of revisions 81265 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r81265 | florent.xicluna | 2010-05-17 15:35:09 +0200 (lun, 17 mai 2010) | 2 lines\\n\\n    Issue #1285086: Speed up urllib.quote and urllib.unquote for simple cases.\\n  ........\\n................\\n  r81272 | florent.xicluna | 2010-05-17 20:01:22 +0200 (lun., 17 mai 2010) | 2 lines\\n\\n  Inadvertently removed part of the comment in r81271.\\n................\\n  r83294 | senthil.kumaran | 2010-07-30 21:34:36 +0200 (ven., 30 juil. 2010) | 2 lines\\n\\n  Fix issue9301 - handle unquote({}) kind of case.\\n................\\n  r83319 | florent.xicluna | 2010-07-31 10:56:55 +0200 (sam., 31 juil. 2010) | 2 lines\\n\\n  Fix an oversight in r83294.  unquote() should reject bytes.  Issue #9301.\\n................\\n  r84038 | florent.xicluna | 2010-08-14 20:30:35 +0200 (sam., 14 ao\u00fbt 2010) | 1 line\\n\\n  Silence the BytesWarning, due to patch r83294 for #9301\\n................\\n  r84039 | florent.xicluna | 2010-08-14 22:51:58 +0200 (sam., 14 ao\u00fbt 2010) | 1 line\\n\\n  Silence BytesWarning while testing exception\\n................",
    "fixed_code": "def unquote_to_bytes(string):\\n    if not string:\\n        string.split\\n        return b''\\n    if isinstance(string, str):\\n        string = string.encode('utf-8')\\n    res = string.split(b'%')\\n    if len(res) == 1:\\n        return string\\n    string = res[0]\\n    for item in res[1:]:\\n        try:\\n            string += bytes([int(item[:2], 16)]) + item[2:]\\n        except ValueError:\\n            string += b'%' + item\\n    return string"
  },
  {
    "code": "def _resolve_output_field(self):\\n        sources_iter = (source for source in self.get_source_fields() if source is not None)\\n        for output_field in sources_iter:\\n            if any(not isinstance(output_field, source.__class__) for source in sources_iter):\\n                raise FieldError('Expression contains mixed types. You must set output_field.')\\n            return output_field",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _resolve_output_field(self):\\n        sources_iter = (source for source in self.get_source_fields() if source is not None)\\n        for output_field in sources_iter:\\n            if any(not isinstance(output_field, source.__class__) for source in sources_iter):\\n                raise FieldError('Expression contains mixed types. You must set output_field.')\\n            return output_field"
  },
  {
    "code": "def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\\n        if len(args) == 0 and len(kwargs) == 0:\\n            values = [self._get_level_values(i) for i in reversed(range(self.nlevels))]\\n            return np.lexsort(values)\\n        return self._values.argsort(*args, **kwargs)\\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/PERF: use lexsort_indexer in MultiIndex.argsort (#48495)",
    "fixed_code": "def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\\n        if len(args) == 0 and len(kwargs) == 0:\\n            target = self._sort_levels_monotonic(raise_if_incomparable=True)\\n            return lexsort_indexer(target._get_codes_for_sorting())\\n        return self._values.argsort(*args, **kwargs)\\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)"
  },
  {
    "code": "def _interleaved_dtype(blocks):\\n    if not len(blocks):\\n        return None\\n    counts = defaultdict(list)\\n    for x in blocks:\\n        counts[type(x)].append(x)\\n    have_int = len(counts[IntBlock]) > 0\\n    have_bool = len(counts[BoolBlock]) > 0\\n    have_object = len(counts[ObjectBlock]) > 0\\n    have_float = len(counts[FloatBlock]) > 0\\n    have_complex = len(counts[ComplexBlock]) > 0\\n    have_dt64 = len(counts[DatetimeBlock]) > 0\\n    have_dt64_tz = len(counts[DatetimeTZBlock]) > 0\\n    have_td64 = len(counts[TimeDeltaBlock]) > 0\\n    have_cat = len(counts[CategoricalBlock]) > 0\\n    have_sparse = len(counts[SparseBlock]) > 0  \\n    have_numeric = have_float or have_complex or have_int\\n    has_non_numeric = have_dt64 or have_dt64_tz or have_td64 or have_cat\\n    if (have_object or\\n        (have_bool and\\n         (have_numeric or have_dt64 or have_dt64_tz or have_td64)) or\\n        (have_numeric and has_non_numeric) or have_cat or have_dt64 or\\n            have_dt64_tz or have_td64):\\n        return np.dtype(object)\\n    elif have_bool:\\n        return np.dtype(bool)\\n    elif have_int and not have_float and not have_complex:\\n        lcd = _find_common_type([b.dtype for b in counts[IntBlock]])\\n        kinds = set([i.dtype.kind for i in counts[IntBlock]])\\n        if len(kinds) == 1:\\n            return lcd\\n        if lcd == 'uint64' or lcd == 'int64':\\n            return np.dtype('int64')\\n        if lcd.kind == 'u':\\n            return np.dtype('int%s' % (lcd.itemsize * 8 * 2))\\n        return lcd\\n    elif have_complex:\\n        return np.dtype('c16')\\n    else:\\n        introspection_blks = counts[FloatBlock] + counts[SparseBlock]\\n        return _find_common_type([b.dtype for b in introspection_blks])",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: upcasting on reshaping ops #13247\\n\\nOriginal work done by @jennolsen84, in #13337\\n\\ncloses #13247\\n\\nAuthor: Jaehoon Hwang <jaehoon217@gmail.com>\\nAuthor: Jae <jaehoon217@gmail.com>\\n\\nCloses #15594 from jaehoonhwang/Bug13247 and squashes the following commits:\\n\\n3cd1734 [Jaehoon Hwang] Pass the non-related tests in test_partial and test_reshape\\n1fa578b [Jaehoon Hwang] Applying request changes removing unnecessary test and renameing\\n6744636 [Jaehoon Hwang] Merge remote-tracking branch 'pandas-dev/master' into Bug13247\\n5bb72c7 [Jaehoon Hwang] Merge remote-tracking branch 'pandas-dev/master' into Bug13247\\na1d5d40 [Jaehoon Hwang] Completed pytest\\n8122359 [Jaehoon Hwang] Merge remote-tracking branch 'pandas-dev/master' into Bug13247\\n0e52b74 [Jaehoon Hwang] Working: Except for pytest\\n8fec07c [Jaehoon Hwang] Fix: test_concat.py and internals.py\\n4f6c03e [Jaehoon Hwang] Fix: is_float_dtypes and is_numeric_dtype wrong place\\nd3476c0 [Jaehoon Hwang] Merge branch 'master' into Bug13247\\nb977615 [Jaehoon Hwang] Merge remote-tracking branch 'pandas-dev/master'\\n4b1e5c6 [Jaehoon Hwang] Merge remote-tracking branch 'pandas-dev/master' into Bug13247\\n45f7ae9 [Jaehoon Hwang] Added pytest function\\n468baee [Jae] BUG: upcasting on reshaping ops #13247",
    "fixed_code": "def _interleaved_dtype(blocks):\\n    if not len(blocks):\\n        return None\\n    counts = defaultdict(list)\\n    for x in blocks:\\n        counts[type(x)].append(x)\\n    have_int = len(counts[IntBlock]) > 0\\n    have_bool = len(counts[BoolBlock]) > 0\\n    have_object = len(counts[ObjectBlock]) > 0\\n    have_float = len(counts[FloatBlock]) > 0\\n    have_complex = len(counts[ComplexBlock]) > 0\\n    have_dt64 = len(counts[DatetimeBlock]) > 0\\n    have_dt64_tz = len(counts[DatetimeTZBlock]) > 0\\n    have_td64 = len(counts[TimeDeltaBlock]) > 0\\n    have_cat = len(counts[CategoricalBlock]) > 0\\n    have_sparse = len(counts[SparseBlock]) > 0  \\n    have_numeric = have_float or have_complex or have_int\\n    has_non_numeric = have_dt64 or have_dt64_tz or have_td64 or have_cat\\n    if (have_object or\\n        (have_bool and\\n         (have_numeric or have_dt64 or have_dt64_tz or have_td64)) or\\n        (have_numeric and has_non_numeric) or have_cat or have_dt64 or\\n            have_dt64_tz or have_td64):\\n        return np.dtype(object)\\n    elif have_bool:\\n        return np.dtype(bool)\\n    elif have_int and not have_float and not have_complex:\\n        lcd = _find_common_type([b.dtype for b in counts[IntBlock]])\\n        kinds = set([i.dtype.kind for i in counts[IntBlock]])\\n        if len(kinds) == 1:\\n            return lcd\\n        if lcd == 'uint64' or lcd == 'int64':\\n            return np.dtype('int64')\\n        if lcd.kind == 'u':\\n            return np.dtype('int%s' % (lcd.itemsize * 8 * 2))\\n        return lcd\\n    elif have_int and have_float and not have_complex:\\n        return np.dtype('float64')\\n    elif have_complex:\\n        return np.dtype('c16')\\n    else:\\n        introspection_blks = counts[FloatBlock] + counts[SparseBlock]\\n        return _find_common_type([b.dtype for b in introspection_blks])"
  },
  {
    "code": "def get_dtypes(self):\\n        dtypes = np.array([blk.dtype for blk in self.blocks])\\n        return com.take_1d(dtypes, self._blknos, allow_fill=False)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_dtypes(self):\\n        dtypes = np.array([blk.dtype for blk in self.blocks])\\n        return com.take_1d(dtypes, self._blknos, allow_fill=False)"
  },
  {
    "code": "def _get_meta_from_src_dir(\\n        b_path,  \\n        require_build_metadata=True,  \\n):  \\n    galaxy_yml = os.path.join(b_path, _GALAXY_YAML)\\n    if not os.path.isfile(galaxy_yml):\\n        raise LookupError(\\n            \"The collection galaxy.yml path '{path!s}' does not exist.\".\\n            format(path=to_native(galaxy_yml))\\n        )\\n    with open(galaxy_yml, 'rb') as manifest_file_obj:\\n        try:\\n            manifest = yaml_load(manifest_file_obj)\\n        except yaml.error.YAMLError as yaml_err:\\n            raise_from(\\n                AnsibleError(\\n                    \"Failed to parse the galaxy.yml at '{path!s}' with \"\\n                    'the following error:\\n{err_txt!s}'.\\n                    format(\\n                        path=to_native(galaxy_yml),\\n                        err_txt=to_native(yaml_err),\\n                    ),\\n                ),\\n                yaml_err,\\n            )\\n    if not isinstance(manifest, dict):\\n        if require_build_metadata:\\n            raise AnsibleError(f\"The collection galaxy.yml at '{to_native(galaxy_yml)}' is incorrectly formatted.\")\\n        display.warning(f\"The collection galaxy.yml at '{to_native(galaxy_yml)}' is incorrectly formatted.\")\\n        raise ValueError(f\"The collection galaxy.yml at '{to_native(galaxy_yml)}' is incorrectly formatted.\")\\n    return _normalize_galaxy_yml_manifest(manifest, galaxy_yml, require_build_metadata)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_meta_from_src_dir(\\n        b_path,  \\n        require_build_metadata=True,  \\n):  \\n    galaxy_yml = os.path.join(b_path, _GALAXY_YAML)\\n    if not os.path.isfile(galaxy_yml):\\n        raise LookupError(\\n            \"The collection galaxy.yml path '{path!s}' does not exist.\".\\n            format(path=to_native(galaxy_yml))\\n        )\\n    with open(galaxy_yml, 'rb') as manifest_file_obj:\\n        try:\\n            manifest = yaml_load(manifest_file_obj)\\n        except yaml.error.YAMLError as yaml_err:\\n            raise_from(\\n                AnsibleError(\\n                    \"Failed to parse the galaxy.yml at '{path!s}' with \"\\n                    'the following error:\\n{err_txt!s}'.\\n                    format(\\n                        path=to_native(galaxy_yml),\\n                        err_txt=to_native(yaml_err),\\n                    ),\\n                ),\\n                yaml_err,\\n            )\\n    if not isinstance(manifest, dict):\\n        if require_build_metadata:\\n            raise AnsibleError(f\"The collection galaxy.yml at '{to_native(galaxy_yml)}' is incorrectly formatted.\")\\n        display.warning(f\"The collection galaxy.yml at '{to_native(galaxy_yml)}' is incorrectly formatted.\")\\n        raise ValueError(f\"The collection galaxy.yml at '{to_native(galaxy_yml)}' is incorrectly formatted.\")\\n    return _normalize_galaxy_yml_manifest(manifest, galaxy_yml, require_build_metadata)"
  },
  {
    "code": "def from_model(cls, model, exclude_rels=False):\\n        fields = []\\n        for field in model._meta.local_fields:\\n            if getattr(field, \"rel\", None) and exclude_rels:\\n                continue\\n            name, path, args, kwargs = field.deconstruct()\\n            field_class = import_string(path)\\n            try:\\n                fields.append((name, field_class(*args, **kwargs)))\\n            except TypeError as e:\\n                raise TypeError(\"Couldn't reconstruct field %s on %s.%s: %s\" % (\\n                    name,\\n                    model._meta.app_label,\\n                    model._meta.object_name,\\n                    e,\\n                ))\\n        if not exclude_rels:\\n            for field in model._meta.local_many_to_many:\\n                name, path, args, kwargs = field.deconstruct()\\n                field_class = import_string(path)\\n                try:\\n                    fields.append((name, field_class(*args, **kwargs)))\\n                except TypeError as e:\\n                    raise TypeError(\"Couldn't reconstruct m2m field %s on %s: %s\" % (\\n                        name,\\n                        model._meta.object_name,\\n                        e,\\n                    ))\\n        options = {}\\n        for name in DEFAULT_NAMES:\\n            if name in [\"apps\", \"app_label\"]:\\n                continue\\n            elif name in model._meta.original_attrs:\\n                if name == \"unique_together\":\\n                    ut = model._meta.original_attrs[\"unique_together\"]\\n                    options[name] = set(normalize_together(ut))\\n                elif name == \"index_together\":\\n                    it = model._meta.original_attrs[\"index_together\"]\\n                    options[name] = set(normalize_together(it))\\n                else:\\n                    options[name] = model._meta.original_attrs[name]",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def from_model(cls, model, exclude_rels=False):\\n        fields = []\\n        for field in model._meta.local_fields:\\n            if getattr(field, \"rel\", None) and exclude_rels:\\n                continue\\n            name, path, args, kwargs = field.deconstruct()\\n            field_class = import_string(path)\\n            try:\\n                fields.append((name, field_class(*args, **kwargs)))\\n            except TypeError as e:\\n                raise TypeError(\"Couldn't reconstruct field %s on %s.%s: %s\" % (\\n                    name,\\n                    model._meta.app_label,\\n                    model._meta.object_name,\\n                    e,\\n                ))\\n        if not exclude_rels:\\n            for field in model._meta.local_many_to_many:\\n                name, path, args, kwargs = field.deconstruct()\\n                field_class = import_string(path)\\n                try:\\n                    fields.append((name, field_class(*args, **kwargs)))\\n                except TypeError as e:\\n                    raise TypeError(\"Couldn't reconstruct m2m field %s on %s: %s\" % (\\n                        name,\\n                        model._meta.object_name,\\n                        e,\\n                    ))\\n        options = {}\\n        for name in DEFAULT_NAMES:\\n            if name in [\"apps\", \"app_label\"]:\\n                continue\\n            elif name in model._meta.original_attrs:\\n                if name == \"unique_together\":\\n                    ut = model._meta.original_attrs[\"unique_together\"]\\n                    options[name] = set(normalize_together(ut))\\n                elif name == \"index_together\":\\n                    it = model._meta.original_attrs[\"index_together\"]\\n                    options[name] = set(normalize_together(it))\\n                else:\\n                    options[name] = model._meta.original_attrs[name]"
  },
  {
    "code": "def perform_krb181_workaround(principal):\\n    cmdv = [configuration.conf.get('kerberos', 'kinit_path'),\\n            \"-c\", configuration.conf.get('kerberos', 'ccache'),\\n            \"-R\"]  \\n    log.info(\"Renewing kerberos ticket to work around kerberos 1.8.1: \" +\\n             \" \".join(cmdv))\\n    ret = subprocess.call(cmdv, close_fds=True)\\n    if ret != 0:\\n        principal = \"%s/%s\" % (principal or configuration.conf.get('kerberos', 'principal'),\\n                               socket.getfqdn())\\n        fmt_dict = dict(princ=principal,\\n                        ccache=configuration.conf.get('kerberos', 'principal'))\\n        log.error(\"Couldn't renew kerberos ticket in order to work around \"\\n                  \"Kerberos 1.8.1 issue. Please check that the ticket for \"\\n                  \"'%(princ)s' is still renewable:\\n\"\\n                  \"  $ kinit -f -c %(ccache)s\\n\"\\n                  \"If the 'renew until' date is the same as the 'valid starting' \"\\n                  \"date, the ticket cannot be renewed. Please check your KDC \"\\n                  \"configuration, and the ticket renewal policy (maxrenewlife) \"\\n                  \"for the '%(princ)s' and `krbtgt' principals.\" % fmt_dict)\\n        sys.exit(ret)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def perform_krb181_workaround(principal):\\n    cmdv = [configuration.conf.get('kerberos', 'kinit_path'),\\n            \"-c\", configuration.conf.get('kerberos', 'ccache'),\\n            \"-R\"]  \\n    log.info(\"Renewing kerberos ticket to work around kerberos 1.8.1: \" +\\n             \" \".join(cmdv))\\n    ret = subprocess.call(cmdv, close_fds=True)\\n    if ret != 0:\\n        principal = \"%s/%s\" % (principal or configuration.conf.get('kerberos', 'principal'),\\n                               socket.getfqdn())\\n        fmt_dict = dict(princ=principal,\\n                        ccache=configuration.conf.get('kerberos', 'principal'))\\n        log.error(\"Couldn't renew kerberos ticket in order to work around \"\\n                  \"Kerberos 1.8.1 issue. Please check that the ticket for \"\\n                  \"'%(princ)s' is still renewable:\\n\"\\n                  \"  $ kinit -f -c %(ccache)s\\n\"\\n                  \"If the 'renew until' date is the same as the 'valid starting' \"\\n                  \"date, the ticket cannot be renewed. Please check your KDC \"\\n                  \"configuration, and the ticket renewal policy (maxrenewlife) \"\\n                  \"for the '%(princ)s' and `krbtgt' principals.\" % fmt_dict)\\n        sys.exit(ret)"
  },
  {
    "code": "def model_from_yaml(yaml_string, custom_objects=None):\\n  raise RuntimeError(\\n\\t  'Method `model_from_yaml()` has been removed due to security risk of '\\n\\t  'arbitrary code execution. Please use `Model.to_json()` and '\\n\\t  '`model_from_json()` instead.'\\n  )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def model_from_yaml(yaml_string, custom_objects=None):\\n  raise RuntimeError(\\n\\t  'Method `model_from_yaml()` has been removed due to security risk of '\\n\\t  'arbitrary code execution. Please use `Model.to_json()` and '\\n\\t  '`model_from_json()` instead.'\\n  )"
  },
  {
    "code": "def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\\n        resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        if self.config:\\n            if not hasattr(self.config, 'resolve_expression'):\\n                resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n            else:\\n                resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        return resolved",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #30385 -- Restored SearchVector(config) immutability.\\n\\nRegression in 1a28dc3887e8d66d5e3ff08cf7fb0a6212b873e5.\\n\\nThe usage of CONCAT to allow SearchVector to deal with non-text fields\\nmade the generated expression non-IMMUTABLE which prevents a functional\\nindex to be created for it.\\n\\nUsing a combination of COALESCE and ::text makes sure the expression\\npreserves its immutability.\\n\\nRefs #29582. Thanks Andrew Brown for the report, Nick Pope for the\\nreview.",
    "fixed_code": "def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\\n        resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        resolved.set_source_expressions([\\n            Coalesce(\\n                expression\\n                if isinstance(expression.output_field, (CharField, TextField))\\n                else Cast(expression, TextField()),\\n                Value('')\\n            ) for expression in resolved.get_source_expressions()\\n        ])\\n        if self.config:\\n            if not hasattr(self.config, 'resolve_expression'):\\n                resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n            else:\\n                resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)\\n        return resolved"
  },
  {
    "code": "def _ipaddr_info(host, port, family, type, proto):\\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or host is None:\\n        return None\\n    type &= ~_SOCKET_TYPE_MASK\\n    if type == socket.SOCK_STREAM:\\n        proto = socket.IPPROTO_TCP\\n    elif type == socket.SOCK_DGRAM:\\n        proto = socket.IPPROTO_UDP\\n    else:\\n        return None\\n    if port is None:\\n        port = 0\\n    elif isinstance(port, bytes):\\n        if port == b'':\\n            port = 0\\n        else:\\n            try:\\n                port = int(port)\\n            except ValueError:\\n                port = socket.getservbyname(port.decode('ascii'))\\n    elif isinstance(port, str):\\n        if port == '':\\n            port = 0\\n        else:\\n            try:\\n                port = int(port)\\n            except ValueError:\\n                port = socket.getservbyname(port)\\n    if hasattr(socket, 'inet_pton'):\\n        if family == socket.AF_UNSPEC:\\n            afs = [socket.AF_INET, socket.AF_INET6]\\n        else:\\n            afs = [family]\\n        for af in afs:\\n            try:\\n                if af == socket.AF_INET6:\\n                    socket.inet_pton(af, host.partition('%')[0])\\n                else:\\n                    socket.inet_pton(af, host)\\n                return af, type, proto, '', (host, port)\\n            except OSError:\\n                pass\\n        return None\\n    try:\\n        addr = ipaddress.IPv4Address(host)\\n    except ValueError:\\n        try:\\n            addr = ipaddress.IPv6Address(host.partition('%')[0])\\n        except ValueError:\\n            return None\\n    af = socket.AF_INET if addr.version == 4 else socket.AF_INET6\\n    if family not in (socket.AF_UNSPEC, af):\\n        return None\\n    return af, type, proto, '', (host, port)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ipaddr_info(host, port, family, type, proto):\\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or host is None:\\n        return None\\n    type &= ~_SOCKET_TYPE_MASK\\n    if type == socket.SOCK_STREAM:\\n        proto = socket.IPPROTO_TCP\\n    elif type == socket.SOCK_DGRAM:\\n        proto = socket.IPPROTO_UDP\\n    else:\\n        return None\\n    if port is None:\\n        port = 0\\n    elif isinstance(port, bytes):\\n        if port == b'':\\n            port = 0\\n        else:\\n            try:\\n                port = int(port)\\n            except ValueError:\\n                port = socket.getservbyname(port.decode('ascii'))\\n    elif isinstance(port, str):\\n        if port == '':\\n            port = 0\\n        else:\\n            try:\\n                port = int(port)\\n            except ValueError:\\n                port = socket.getservbyname(port)\\n    if hasattr(socket, 'inet_pton'):\\n        if family == socket.AF_UNSPEC:\\n            afs = [socket.AF_INET, socket.AF_INET6]\\n        else:\\n            afs = [family]\\n        for af in afs:\\n            try:\\n                if af == socket.AF_INET6:\\n                    socket.inet_pton(af, host.partition('%')[0])\\n                else:\\n                    socket.inet_pton(af, host)\\n                return af, type, proto, '', (host, port)\\n            except OSError:\\n                pass\\n        return None\\n    try:\\n        addr = ipaddress.IPv4Address(host)\\n    except ValueError:\\n        try:\\n            addr = ipaddress.IPv6Address(host.partition('%')[0])\\n        except ValueError:\\n            return None\\n    af = socket.AF_INET if addr.version == 4 else socket.AF_INET6\\n    if family not in (socket.AF_UNSPEC, af):\\n        return None\\n    return af, type, proto, '', (host, port)"
  },
  {
    "code": "def _GetGrad(grads, t, unconnected_gradients):\\n  op = t.op\\n  op_grads = grads.get(op)\\n  if not op_grads:\\n\\tif unconnected_gradients == UnconnectedGradients.ZERO:\\n\\t  t_dtype = default_gradient.get_zeros_dtype(t)\\n\\t  return array_ops.zeros_like(t, dtype=t_dtype)\\n\\telif unconnected_gradients == UnconnectedGradients.NONE:\\n\\t  return None\\n\\telse:\\n\\t  raise ValueError(\\n\\t\\t  \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\\n  t_grad = op_grads[t.value_index]\\n  assert not isinstance(\\n\\t  t_grad, list), (\"gradients list should have been aggregated by now.\")\\n  return t_grad",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Create zeros grad of the correct shape for ResourceVariables.\\n\\nFixes #31297",
    "fixed_code": "def _GetGrad(grads, t, unconnected_gradients):\\n  op = t.op\\n  op_grads = grads.get(op)\\n  if not op_grads:\\n\\tif unconnected_gradients == UnconnectedGradients.ZERO:\\n\\t  t_dtype = default_gradient.get_zeros_dtype(t)\\n\\t  if t.dtype == dtypes.resource:\\n\\t\\treturn array_ops.zeros(\\n\\t\\t\\tresource_variable_ops.variable_shape(t), dtype=t_dtype)\\n\\t  else:\\n\\t\\treturn array_ops.zeros_like(t, dtype=t_dtype)\\n\\telif unconnected_gradients == UnconnectedGradients.NONE:\\n\\t  return None\\n\\telse:\\n\\t  raise ValueError(\\n\\t\\t  \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\\n  t_grad = op_grads[t.value_index]\\n  assert not isinstance(\\n\\t  t_grad, list), (\"gradients list should have been aggregated by now.\")\\n  return t_grad"
  },
  {
    "code": "def verify(mhash, key, S):\\n\\tmodBits = Crypto.Util.number.size(key.n)\\n\\tk = ceil_div(modBits,8) \\n\\tif len(S) != k:\\n\\t\\treturn 0\\n\\tm = key.encrypt(S, 0)[0]\\n\\tem1 = '\\x00'*(k-len(m)) + m\\n\\ttry:\\n\\t\\tem2 = EMSA_PKCS1_V1_5_ENCODE(mhash, k)\\n\\texcept ValueError:\\n\\t\\treturn 0\\n\\treturn em1==em2",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def verify(mhash, key, S):\\n\\tmodBits = Crypto.Util.number.size(key.n)\\n\\tk = ceil_div(modBits,8) \\n\\tif len(S) != k:\\n\\t\\treturn 0\\n\\tm = key.encrypt(S, 0)[0]\\n\\tem1 = '\\x00'*(k-len(m)) + m\\n\\ttry:\\n\\t\\tem2 = EMSA_PKCS1_V1_5_ENCODE(mhash, k)\\n\\texcept ValueError:\\n\\t\\treturn 0\\n\\treturn em1==em2"
  },
  {
    "code": "def match(to_match, values, na_sentinel=-1):\\n\\tvalues = com._asarray_tuplesafe(values)\\n\\tif issubclass(values.dtype.type, basestring):\\n\\t\\tvalues = np.array(values, dtype='O')\\n\\tf = lambda htype, caster: _match_generic(to_match, values, htype, caster)\\n\\treturn _hashtable_algo(f, values.dtype)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def match(to_match, values, na_sentinel=-1):\\n\\tvalues = com._asarray_tuplesafe(values)\\n\\tif issubclass(values.dtype.type, basestring):\\n\\t\\tvalues = np.array(values, dtype='O')\\n\\tf = lambda htype, caster: _match_generic(to_match, values, htype, caster)\\n\\treturn _hashtable_algo(f, values.dtype)"
  },
  {
    "code": "def modify_ports(self, current_ports):\\n        add_ports = set(self.parameters['ports']) - set(current_ports)\\n        remove_ports = set(current_ports) - set(self.parameters['ports'])\\n        for port in add_ports:\\n            self.add_port_to_if_grp(port)\\n        for port in remove_ports:\\n            self.remove_port_to_if_grp(port)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def modify_ports(self, current_ports):\\n        add_ports = set(self.parameters['ports']) - set(current_ports)\\n        remove_ports = set(current_ports) - set(self.parameters['ports'])\\n        for port in add_ports:\\n            self.add_port_to_if_grp(port)\\n        for port in remove_ports:\\n            self.remove_port_to_if_grp(port)"
  },
  {
    "code": "def get_list_or_404(klass, *args, **kwargs):\\n    queryset = _get_queryset(klass)\\n    if not hasattr(queryset, 'filter'):\\n        klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__\\n        raise ValueError(\\n            \"First argument to get_list_or_404() must be a Model, Manager, or \"\\n            \"QuerySet, not '%s'.\" % klass__name\\n        )\\n    obj_list = list(queryset.filter(*args, **kwargs))\\n    if not obj_list:\\n        raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)\\n    return obj_list",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_list_or_404(klass, *args, **kwargs):\\n    queryset = _get_queryset(klass)\\n    if not hasattr(queryset, 'filter'):\\n        klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__\\n        raise ValueError(\\n            \"First argument to get_list_or_404() must be a Model, Manager, or \"\\n            \"QuerySet, not '%s'.\" % klass__name\\n        )\\n    obj_list = list(queryset.filter(*args, **kwargs))\\n    if not obj_list:\\n        raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)\\n    return obj_list"
  },
  {
    "code": "def is_valid(cls, trigger_rule):\\n        return trigger_rule in cls.all_triggers()\\n    @classmethod",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactor TriggerRule & WeightRule classes to inherit from Enum (#21264)\\n\\ncloses: #19905\\nrelated: #5302,#18627",
    "fixed_code": "def is_valid(cls, trigger_rule: str) -> bool:\\n        return trigger_rule in cls.all_triggers()\\n    @classmethod"
  },
  {
    "code": "def visit(self, node, **kwargs):\\n        if isinstance(node, string_types):\\n            clean = self.preparser(node)\\n            node = ast.fix_missing_locations(ast.parse(clean))\\n        elif not isinstance(node, ast.AST):\\n            raise TypeError(\"Cannot visit objects of type {0!r}\"\\n                            \"\".format(node.__class__.__name__))\\n        method = 'visit_' + node.__class__.__name__\\n        visitor = getattr(self, method)\\n        return visitor(node, **kwargs)\\n    def visit_Module(self, node, **kwargs):\\n        if len(node.body) != 1:\\n            raise SyntaxError('only a single expression is allowed')\\n        expr = node.body[0]\\n        return self.visit(expr, **kwargs)\\n    def visit_Expr(self, node, **kwargs):\\n        return self.visit(node.value, **kwargs)\\n    def visit_BinOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        left = self.visit(node.left, side='left')\\n        right = self.visit(node.right, side='right')\\n        return op(left, right)\\n    def visit_Div(self, node, **kwargs):\\n        return lambda lhs, rhs: Div(lhs, rhs,\\n                                    truediv=self.env.locals['truediv'])\\n    def visit_UnaryOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        operand = self.visit(node.operand)\\n        return op(operand)\\n    def visit_Name(self, node, **kwargs):\\n        return self.term_type(node.id, self.env, **kwargs)\\n    def visit_Num(self, node, **kwargs):\\n        return self.const_type(node.n, self.env)\\n    def visit_Str(self, node, **kwargs):\\n        name = self.env.add_tmp(node.s)\\n        return self.term_type(name, self.env)\\n    def visit_List(self, node, **kwargs):\\n        name = self.env.add_tmp([self.visit(e).value for e in node.elts])\\n        return self.term_type(name, self.env)\\n    visit_Tuple = visit_List\\n    def visit_Index(self, node, **kwargs):\\n        return self.visit(node.value)\\n    def visit_Subscript(self, node, **kwargs):\\n        value = self.visit(node.value)\\n        slobj = self.visit(node.slice)\\n        expr = com.pprint_thing(slobj)\\n        result = pd.eval(expr, local_dict=self.env, engine=self.engine,\\n                         parser=self.parser)\\n        try:\\n            v = value.value[result]\\n        except AttributeError:\\n            lhs = pd.eval(com.pprint_thing(value), local_dict=self.env,\\n                          engine=self.engine, parser=self.parser)\\n            v = lhs[result]\\n        name = self.env.add_tmp(v)\\n        return self.term_type(name, env=self.env)\\n    def visit_Slice(self, node, **kwargs):\\n        lower = node.lower\\n        if lower is not None:\\n            lower = self.visit(lower).value\\n        upper = node.upper\\n        if upper is not None:\\n            upper = self.visit(upper).value\\n        step = node.step\\n        if step is not None:\\n            step = self.visit(step).value\\n        return slice(lower, upper, step)\\n    def visit_Assign(self, node, **kwargs):\\n        cmpr = ast.Compare(ops=[ast.Eq()], left=node.targets[0],\\n                           comparators=[node.value])\\n        return self.visit(cmpr)\\n    def visit_Attribute(self, node, **kwargs):\\n        attr = node.attr\\n        value = node.value\\n        ctx = node.ctx\\n        if isinstance(ctx, ast.Load):\\n            resolved = self.visit(value).value\\n            try:\\n                v = getattr(resolved, attr)\\n                name = self.env.add_tmp(v)\\n                return self.term_type(name, self.env)\\n            except AttributeError:\\n                if isinstance(value, ast.Name) and value.id == attr:\\n                    return resolved\\n        raise ValueError(\"Invalid Attribute context {0}\".format(ctx.__name__))\\n    def visit_Call(self, node, **kwargs):\\n        if isinstance(node.func, ast.Attribute):\\n            res = self.visit_Attribute(node.func)\\n        elif not isinstance(node.func, ast.Name):\\n            raise TypeError(\"Only named functions are supported\")\\n        else:\\n            res = self.visit(node.func)\\n        if res is None:\\n            raise ValueError(\"Invalid function call {0}\".format(node.func.id))\\n        if hasattr(res, 'value'):\\n            res = res.value\\n        args = [self.visit(targ).value for targ in node.args]\\n        if node.starargs is not None:\\n            args = args + self.visit(node.starargs).value\\n        keywords = {}\\n        for key in node.keywords:\\n            if not isinstance(key, ast.keyword):\\n                raise ValueError(\\n                    \"keyword error in function call '{0}'\".format(node.func.id))\\n            keywords[key.arg] = self.visit(key.value).value\\n        if node.kwargs is not None:\\n            keywords.update(self.visit(node.kwargs).value)\\n        return self.const_type(res(*args, **keywords), self.env)\\n    def visit_Compare(self, node, **kwargs):\\n        ops = node.ops\\n        comps = node.comparators",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def visit(self, node, **kwargs):\\n        if isinstance(node, string_types):\\n            clean = self.preparser(node)\\n            node = ast.fix_missing_locations(ast.parse(clean))\\n        elif not isinstance(node, ast.AST):\\n            raise TypeError(\"Cannot visit objects of type {0!r}\"\\n                            \"\".format(node.__class__.__name__))\\n        method = 'visit_' + node.__class__.__name__\\n        visitor = getattr(self, method)\\n        return visitor(node, **kwargs)\\n    def visit_Module(self, node, **kwargs):\\n        if len(node.body) != 1:\\n            raise SyntaxError('only a single expression is allowed')\\n        expr = node.body[0]\\n        return self.visit(expr, **kwargs)\\n    def visit_Expr(self, node, **kwargs):\\n        return self.visit(node.value, **kwargs)\\n    def visit_BinOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        left = self.visit(node.left, side='left')\\n        right = self.visit(node.right, side='right')\\n        return op(left, right)\\n    def visit_Div(self, node, **kwargs):\\n        return lambda lhs, rhs: Div(lhs, rhs,\\n                                    truediv=self.env.locals['truediv'])\\n    def visit_UnaryOp(self, node, **kwargs):\\n        op = self.visit(node.op)\\n        operand = self.visit(node.operand)\\n        return op(operand)\\n    def visit_Name(self, node, **kwargs):\\n        return self.term_type(node.id, self.env, **kwargs)\\n    def visit_Num(self, node, **kwargs):\\n        return self.const_type(node.n, self.env)\\n    def visit_Str(self, node, **kwargs):\\n        name = self.env.add_tmp(node.s)\\n        return self.term_type(name, self.env)\\n    def visit_List(self, node, **kwargs):\\n        name = self.env.add_tmp([self.visit(e).value for e in node.elts])\\n        return self.term_type(name, self.env)\\n    visit_Tuple = visit_List\\n    def visit_Index(self, node, **kwargs):\\n        return self.visit(node.value)\\n    def visit_Subscript(self, node, **kwargs):\\n        value = self.visit(node.value)\\n        slobj = self.visit(node.slice)\\n        expr = com.pprint_thing(slobj)\\n        result = pd.eval(expr, local_dict=self.env, engine=self.engine,\\n                         parser=self.parser)\\n        try:\\n            v = value.value[result]\\n        except AttributeError:\\n            lhs = pd.eval(com.pprint_thing(value), local_dict=self.env,\\n                          engine=self.engine, parser=self.parser)\\n            v = lhs[result]\\n        name = self.env.add_tmp(v)\\n        return self.term_type(name, env=self.env)\\n    def visit_Slice(self, node, **kwargs):\\n        lower = node.lower\\n        if lower is not None:\\n            lower = self.visit(lower).value\\n        upper = node.upper\\n        if upper is not None:\\n            upper = self.visit(upper).value\\n        step = node.step\\n        if step is not None:\\n            step = self.visit(step).value\\n        return slice(lower, upper, step)\\n    def visit_Assign(self, node, **kwargs):\\n        cmpr = ast.Compare(ops=[ast.Eq()], left=node.targets[0],\\n                           comparators=[node.value])\\n        return self.visit(cmpr)\\n    def visit_Attribute(self, node, **kwargs):\\n        attr = node.attr\\n        value = node.value\\n        ctx = node.ctx\\n        if isinstance(ctx, ast.Load):\\n            resolved = self.visit(value).value\\n            try:\\n                v = getattr(resolved, attr)\\n                name = self.env.add_tmp(v)\\n                return self.term_type(name, self.env)\\n            except AttributeError:\\n                if isinstance(value, ast.Name) and value.id == attr:\\n                    return resolved\\n        raise ValueError(\"Invalid Attribute context {0}\".format(ctx.__name__))\\n    def visit_Call(self, node, **kwargs):\\n        if isinstance(node.func, ast.Attribute):\\n            res = self.visit_Attribute(node.func)\\n        elif not isinstance(node.func, ast.Name):\\n            raise TypeError(\"Only named functions are supported\")\\n        else:\\n            res = self.visit(node.func)\\n        if res is None:\\n            raise ValueError(\"Invalid function call {0}\".format(node.func.id))\\n        if hasattr(res, 'value'):\\n            res = res.value\\n        args = [self.visit(targ).value for targ in node.args]\\n        if node.starargs is not None:\\n            args = args + self.visit(node.starargs).value\\n        keywords = {}\\n        for key in node.keywords:\\n            if not isinstance(key, ast.keyword):\\n                raise ValueError(\\n                    \"keyword error in function call '{0}'\".format(node.func.id))\\n            keywords[key.arg] = self.visit(key.value).value\\n        if node.kwargs is not None:\\n            keywords.update(self.visit(node.kwargs).value)\\n        return self.const_type(res(*args, **keywords), self.env)\\n    def visit_Compare(self, node, **kwargs):\\n        ops = node.ops\\n        comps = node.comparators"
  },
  {
    "code": "def maketables(trace=0):\\n    print \"--- Reading\", UNICODE_DATA, \"...\"\\n    unicode = UnicodeData(UNICODE_DATA, COMPOSITION_EXCLUSIONS)\\n    print len(filter(None, unicode.table)), \"characters\"\\n    makeunicodename(unicode, trace)\\n    makeunicodedata(unicode, trace)\\n    makeunicodetype(unicode, trace)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "- SF #962502: Add two more methods for unicode type; width() and iswide() for east asian width manipulation. (Inspired by David Goodger, Reviewed by Martin v. Loewis) - Move _PyUnicode_TypeRecord.flags to the end of the struct so that no padding is added for UCS-4 builds. (Suggested by Martin v. Loewis)",
    "fixed_code": "def maketables(trace=0):\\n    print \"--- Reading\", UNICODE_DATA, \"...\"\\n    unicode = UnicodeData(UNICODE_DATA, COMPOSITION_EXCLUSIONS,\\n                          EASTASIAN_WIDTH)\\n    print len(filter(None, unicode.table)), \"characters\"\\n    makeunicodename(unicode, trace)\\n    makeunicodedata(unicode, trace)\\n    makeunicodetype(unicode, trace)"
  },
  {
    "code": "def timeit(key, df, fn=None, remove=True, **kwargs):\\n    if fn is None:\\n        fn = 'timeit.h5'\\n    store = HDFStore(fn, mode='w')\\n    store.append(key, df, **kwargs)\\n    store.close()\\n    if remove:\\n        import os\\n        os.remove(fn)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: HDFStore.flush() to optionally perform fsync (GH5364)",
    "fixed_code": "def timeit(key, df, fn=None, remove=True, **kwargs):\\n    if fn is None:\\n        fn = 'timeit.h5'\\n    store = HDFStore(fn, mode='w')\\n    store.append(key, df, **kwargs)\\n    store.close()\\n    if remove:\\n        os.remove(fn)"
  },
  {
    "code": "def start_requests(self):\\n\\t\\tcls = self.__class__\\n\\t\\tif cls.make_requests_from_url is not Spider.make_requests_from_url:\\n\\t\\t\\twarnings.warn(\\n\\t\\t\\t\\t\"Spider.make_requests_from_url method is deprecated; it \"\\n\\t\\t\\t\\t\"won't be called in future Scrapy releases. Please \"\\n\\t\\t\\t\\t\"override Spider.start_requests method instead (see %s.%s).\" % (\\n\\t\\t\\t\\t\\tcls.__module__, cls.__name__\\n\\t\\t\\t\\t),\\n\\t\\t\\t)\\n\\t\\t\\tfor url in self.start_urls:\\n\\t\\t\\t\\tyield self.make_requests_from_url(url)\\n\\t\\telse:\\n\\t\\t\\tfor url in self.start_urls:\\n\\t\\t\\t\\tyield Request(url, dont_filter=True)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def start_requests(self):\\n\\t\\tcls = self.__class__\\n\\t\\tif cls.make_requests_from_url is not Spider.make_requests_from_url:\\n\\t\\t\\twarnings.warn(\\n\\t\\t\\t\\t\"Spider.make_requests_from_url method is deprecated; it \"\\n\\t\\t\\t\\t\"won't be called in future Scrapy releases. Please \"\\n\\t\\t\\t\\t\"override Spider.start_requests method instead (see %s.%s).\" % (\\n\\t\\t\\t\\t\\tcls.__module__, cls.__name__\\n\\t\\t\\t\\t),\\n\\t\\t\\t)\\n\\t\\t\\tfor url in self.start_urls:\\n\\t\\t\\t\\tyield self.make_requests_from_url(url)\\n\\t\\telse:\\n\\t\\t\\tfor url in self.start_urls:\\n\\t\\t\\t\\tyield Request(url, dont_filter=True)"
  },
  {
    "code": "def group_info(self):\\n        comp_ids, obs_group_ids = self._get_compressed_codes()\\n        ngroups = len(obs_group_ids)\\n        comp_ids = ensure_platform_int(comp_ids)\\n        return comp_ids, obs_group_ids, ngroups\\n    @final\\n    @cache_readonly",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TYP: GroupBy (#43806)",
    "fixed_code": "def group_info(self) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp], int]:\\n        comp_ids, obs_group_ids = self._get_compressed_codes()\\n        ngroups = len(obs_group_ids)\\n        comp_ids = ensure_platform_int(comp_ids)\\n        return comp_ids, obs_group_ids, ngroups\\n    @final\\n    @cache_readonly"
  },
  {
    "code": "def slice(self, start=None, stop=None, step=1):\\n        result = str_slice(self.series, start, stop)\\n        return self._wrap_result(result)\\n    @copy(str_slice)\\n    def slice_replace(self, i=None, j=None):\\n        raise NotImplementedError\\n    @copy(str_decode)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Implement step in slice StringMethod\\n\\nResolves #8754.",
    "fixed_code": "def slice(self, start=None, stop=None, step=1):\\n        result = str_slice(self.series, start, stop, step)\\n        return self._wrap_result(result)\\n    @copy(str_slice)\\n    def slice_replace(self, i=None, j=None):\\n        raise NotImplementedError\\n    @copy(str_decode)"
  },
  {
    "code": "def run(self, endpoint, data=None, headers=None, extra_options=None):\\n        extra_options = extra_options or {}\\n        session = self.get_conn(headers)\\n        url = self.base_url + endpoint\\n        req = None\\n        if self.method == 'GET':\\n            req = requests.Request(self.method,\\n                                   url,\\n                                   params=data,\\n                                   headers=headers)\\n        elif self.method == 'HEAD':\\n            req = requests.Request(self.method,\\n                                   url,\\n                                   headers=headers)\\n        else:\\n            req = requests.Request(self.method,\\n                                   url,\\n                                   data=data,\\n                                   headers=headers)\\n        prepped_request = session.prepare_request(req)\\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\\n        return self.run_and_check(session, prepped_request, extra_options)\\n    def run_and_check(self, session, prepped_request, extra_options):\\n        extra_options = extra_options or {}\\n        response = session.send(\\n            prepped_request,\\n            stream=extra_options.get(\"stream\", False),\\n            verify=extra_options.get(\"verify\", False),\\n            proxies=extra_options.get(\"proxies\", {}),\\n            cert=extra_options.get(\"cert\"),\\n            timeout=extra_options.get(\"timeout\"),\\n            allow_redirects=extra_options.get(\"allow_redirects\", True))\\n        try:\\n            response.raise_for_status()\\n        except requests.exceptions.HTTPError:\\n            self.log.error(\"HTTP error: %s\", response.reason)\\n            if self.method not in ('GET', 'HEAD'):\\n                self.log.error(response.text)\\n            raise AirflowException(str(response.status_code)+\":\"+response.reason)\\n        return response",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2110][AIRFLOW-2122] Enhance Http Hook\\n\\n- Use a header in passed in the \"extra\" argument and\\n  add tenacity retry\\n- Fix the tests with proper mocking\\n\\nCloses #3071 from albertocalderari/master",
    "fixed_code": "def run(self, endpoint, data=None, headers=None, extra_options=None):\\n        extra_options = extra_options or {}\\n        session = self.get_conn(headers)\\n        url = self.base_url + endpoint\\n        req = None\\n        if self.method == 'GET':\\n            req = requests.Request(self.method,\\n                                   url,\\n                                   params=data,\\n                                   headers=headers)\\n        elif self.method == 'HEAD':\\n            req = requests.Request(self.method,\\n                                   url,\\n                                   headers=headers)\\n        else:\\n            req = requests.Request(self.method,\\n                                   url,\\n                                   data=data,\\n                                   headers=headers)\\n        prepped_request = session.prepare_request(req)\\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\\n        return self.run_and_check(session, prepped_request, extra_options)"
  },
  {
    "code": "def maybe_make_parens_invisible_in_atom(\\n    node: LN,\\n    parent: LN,\\n    preview: bool = False,\\n) -> bool:\\n    if (\\n        preview\\n        and parent.type == syms.for_stmt\\n        and isinstance(node.prev_sibling, Leaf)\\n        and node.prev_sibling.type == token.NAME\\n        and node.prev_sibling.value == \"for\"\\n    ):\\n        for_stmt_check = False\\n    else:\\n        for_stmt_check = True\\n    if (\\n        node.type != syms.atom\\n        or is_empty_tuple(node)\\n        or is_one_tuple(node)\\n        or (is_yield(node) and parent.type != syms.expr_stmt)\\n        or (max_delimiter_priority_in_atom(node) >= COMMA_PRIORITY and for_stmt_check)\\n    ):\\n        return False\\n    if is_walrus_assignment(node):\\n        if parent.type in [\\n            syms.annassign,\\n            syms.expr_stmt,\\n            syms.assert_stmt,\\n            syms.return_stmt,\\n            syms.for_stmt,\\n            syms.del_stmt,\\n        ]:\\n            return False\\n    first = node.children[0]\\n    last = node.children[-1]\\n    if is_lpar_token(first) and is_rpar_token(last):\\n        middle = node.children[1]\\n        first.value = \"\"\\n        last.value = \"\"\\n        maybe_make_parens_invisible_in_atom(middle, parent=parent, preview=preview)\\n        if is_atom_with_invisible_parens(middle):\\n            middle.replace(middle.children[1])\\n        return False\\n    return True",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Remove unnecessary parentheses from `with` statements (#2926)",
    "fixed_code": "def maybe_make_parens_invisible_in_atom(\\n    node: LN,\\n    parent: LN,\\n    remove_brackets_around_comma: bool = False,\\n) -> bool:\\n    if (\\n        node.type != syms.atom\\n        or is_empty_tuple(node)\\n        or is_one_tuple(node)\\n        or (is_yield(node) and parent.type != syms.expr_stmt)\\n        or (\\n            not remove_brackets_around_comma\\n            and max_delimiter_priority_in_atom(node) >= COMMA_PRIORITY\\n        )\\n    ):\\n        return False\\n    if is_walrus_assignment(node):\\n        if parent.type in [\\n            syms.annassign,\\n            syms.expr_stmt,\\n            syms.assert_stmt,\\n            syms.return_stmt,\\n            syms.for_stmt,\\n            syms.del_stmt,\\n        ]:\\n            return False\\n    first = node.children[0]\\n    last = node.children[-1]\\n    if is_lpar_token(first) and is_rpar_token(last):\\n        middle = node.children[1]\\n        first.value = \"\"\\n        last.value = \"\"\\n        maybe_make_parens_invisible_in_atom(\\n            middle,\\n            parent=parent,\\n            remove_brackets_around_comma=remove_brackets_around_comma,\\n        )\\n        if is_atom_with_invisible_parens(middle):\\n            middle.replace(middle.children[1])\\n        return False\\n    return True"
  },
  {
    "code": "def f(self, other, axis=default_axis, level=None, fill_value=None):\\n\\t\\tif _should_reindex_frame_op(\\n\\t\\t\\tself, other, op, axis, default_axis, fill_value, level\\n\\t\\t):\\n\\t\\t\\treturn _frame_arith_method_with_reindex(self, other, op)\\n\\t\\tself, other = _align_method_FRAME(self, other, axis, flex=True, level=level)\\n\\t\\tif isinstance(other, ABCDataFrame):\\n\\t\\t\\tpass_op = op if should_series_dispatch(self, other, op) else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tnew_data = self._combine_frame(other, pass_op, fill_value)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\t\\telif isinstance(other, ABCSeries):\\n\\t\\t\\tpass_op = op if axis in [0, \"columns\", None] else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\traise NotImplementedError(f\"fill_value {fill_value} not supported.\")\\n\\t\\t\\taxis = self._get_axis_number(axis) if axis is not None else 1\\n\\t\\t\\treturn _combine_series_frame(\\n\\t\\t\\t\\tself, other, pass_op, axis=axis, str_rep=str_rep\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\tself = self.fillna(fill_value)\\n\\t\\t\\tnew_data = dispatch_to_series(self, other, op, str_rep)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\tf.__name__ = op_name\\n\\treturn f",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def f(self, other, axis=default_axis, level=None, fill_value=None):\\n\\t\\tif _should_reindex_frame_op(\\n\\t\\t\\tself, other, op, axis, default_axis, fill_value, level\\n\\t\\t):\\n\\t\\t\\treturn _frame_arith_method_with_reindex(self, other, op)\\n\\t\\tself, other = _align_method_FRAME(self, other, axis, flex=True, level=level)\\n\\t\\tif isinstance(other, ABCDataFrame):\\n\\t\\t\\tpass_op = op if should_series_dispatch(self, other, op) else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tnew_data = self._combine_frame(other, pass_op, fill_value)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\t\\telif isinstance(other, ABCSeries):\\n\\t\\t\\tpass_op = op if axis in [0, \"columns\", None] else na_op\\n\\t\\t\\tpass_op = pass_op if not is_logical else op\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\traise NotImplementedError(f\"fill_value {fill_value} not supported.\")\\n\\t\\t\\taxis = self._get_axis_number(axis) if axis is not None else 1\\n\\t\\t\\treturn _combine_series_frame(\\n\\t\\t\\t\\tself, other, pass_op, axis=axis, str_rep=str_rep\\n\\t\\t\\t)\\n\\t\\telse:\\n\\t\\t\\tif fill_value is not None:\\n\\t\\t\\t\\tself = self.fillna(fill_value)\\n\\t\\t\\tnew_data = dispatch_to_series(self, other, op, str_rep)\\n\\t\\t\\treturn self._construct_result(new_data)\\n\\tf.__name__ = op_name\\n\\treturn f"
  },
  {
    "code": "def size_internal(input, name=None, optimize=True, out_type=dtypes.int32):\\n  if context.executing_eagerly() and not isinstance(\\n\\t  input, (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)):\\n\\treturn np.prod(ops.convert_to_tensor(input)._shape_tuple())  \\n  with ops.name_scope(name, \"Size\", [input]) as name:\\n\\tif isinstance(input, (sparse_tensor.SparseTensor,\\n\\t\\t\\t\\t\\t\\t  sparse_tensor.SparseTensorValue)):\\n\\t  return gen_math_ops.prod(\\n\\t\\t  gen_math_ops.cast(input.dense_shape, out_type), 0, name=name)\\n\\telse:\\n\\t  input_tensor = ops.convert_to_tensor(input)\\n\\t  input_shape = input_tensor.get_shape()\\n\\t  if optimize:\\n\\t\\tif input_shape.is_fully_defined():\\n\\t\\t  return constant(input_shape.num_elements(), out_type, name=name)\\n\\t\\tif input_shape.dims and any(dim == 0 for dim in input_shape.dims):\\n\\t\\t  return constant(0, out_type, name=name)\\n\\t  return gen_array_ops.size(input, name=name, out_type=out_type)",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Fix #18180\\n\\ntf.size() was not respecting the `out_type` argument when eager execution was\\nenabled.",
    "fixed_code": "def size_internal(input, name=None, optimize=True, out_type=dtypes.int32):\\n  if context.executing_eagerly() and not isinstance(\\n\\t  input, (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)):\\n\\tinput = ops.convert_to_tensor(input)\\n\\tnp_out_type = out_type.as_numpy_dtype\\n\\tnum_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  \\n\\treturn ops.convert_to_tensor(num_elements, dtype=out_type)\\n  with ops.name_scope(name, \"Size\", [input]) as name:\\n\\tif isinstance(input, (sparse_tensor.SparseTensor,\\n\\t\\t\\t\\t\\t\\t  sparse_tensor.SparseTensorValue)):\\n\\t  return gen_math_ops.prod(\\n\\t\\t  gen_math_ops.cast(input.dense_shape, out_type), 0, name=name)\\n\\telse:\\n\\t  input_tensor = ops.convert_to_tensor(input)\\n\\t  input_shape = input_tensor.get_shape()\\n\\t  if optimize:\\n\\t\\tif input_shape.is_fully_defined():\\n\\t\\t  return constant(input_shape.num_elements(), out_type, name=name)\\n\\t\\tif input_shape.dims and any(dim == 0 for dim in input_shape.dims):\\n\\t\\t  return constant(0, out_type, name=name)\\n\\t  return gen_array_ops.size(input, name=name, out_type=out_type)"
  },
  {
    "code": "def _read_wide_table(self, group, where=None):\\n        t = create_table(self, group)\\n        return t.read(where)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _read_wide_table(self, group, where=None):\\n        t = create_table(self, group)\\n        return t.read(where)"
  },
  {
    "code": "def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\\n\\t\\tif is_period_dtype(self.dtype):\\n\\t\\t\\tif is_period_dtype(result.dtype):\\n\\t\\t\\t\\treturn self.freq\\n\\t\\t\\treturn None\\n\\t\\telif self.freq is None:\\n\\t\\t\\treturn None\\n\\t\\telif lib.is_scalar(other) and isna(other):\\n\\t\\t\\treturn None\\n\\t\\telif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n\\t\\t\\tnew_freq = None\\n\\t\\t\\tif isinstance(self.freq, Tick):\\n\\t\\t\\t\\tnew_freq = self.freq\\n\\t\\t\\treturn new_freq\\n\\t\\telif isinstance(other, DateOffset):\\n\\t\\t\\treturn None  \\n\\t\\telif isinstance(other, (datetime, np.datetime64)):\\n\\t\\t\\treturn self.freq\\n\\t\\telif is_timedelta64_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_object_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_datetime64_any_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telse:\\n\\t\\t\\traise NotImplementedError\\n\\t__add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\\n\\t__sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\\n\\t__radd__ = make_wrapped_arith_op(\"__radd__\")\\n\\t__rsub__ = make_wrapped_arith_op(\"__rsub__\")\\n\\t__pow__ = make_wrapped_arith_op(\"__pow__\")\\n\\t__rpow__ = make_wrapped_arith_op(\"__rpow__\")\\n\\t__mul__ = make_wrapped_arith_op(\"__mul__\")\\n\\t__rmul__ = make_wrapped_arith_op(\"__rmul__\")\\n\\t__floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\\n\\t__rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\\n\\t__mod__ = make_wrapped_arith_op(\"__mod__\")\\n\\t__rmod__ = make_wrapped_arith_op(\"__rmod__\")\\n\\t__divmod__ = make_wrapped_arith_op(\"__divmod__\")\\n\\t__rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\\n\\t__truediv__ = make_wrapped_arith_op(\"__truediv__\")\\n\\t__rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\\n\\t\\tif is_period_dtype(self.dtype):\\n\\t\\t\\tif is_period_dtype(result.dtype):\\n\\t\\t\\t\\treturn self.freq\\n\\t\\t\\treturn None\\n\\t\\telif self.freq is None:\\n\\t\\t\\treturn None\\n\\t\\telif lib.is_scalar(other) and isna(other):\\n\\t\\t\\treturn None\\n\\t\\telif isinstance(other, (Tick, timedelta, np.timedelta64)):\\n\\t\\t\\tnew_freq = None\\n\\t\\t\\tif isinstance(self.freq, Tick):\\n\\t\\t\\t\\tnew_freq = self.freq\\n\\t\\t\\treturn new_freq\\n\\t\\telif isinstance(other, DateOffset):\\n\\t\\t\\treturn None  \\n\\t\\telif isinstance(other, (datetime, np.datetime64)):\\n\\t\\t\\treturn self.freq\\n\\t\\telif is_timedelta64_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_object_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telif is_datetime64_any_dtype(other):\\n\\t\\t\\treturn None  \\n\\t\\telse:\\n\\t\\t\\traise NotImplementedError\\n\\t__add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\\n\\t__sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\\n\\t__radd__ = make_wrapped_arith_op(\"__radd__\")\\n\\t__rsub__ = make_wrapped_arith_op(\"__rsub__\")\\n\\t__pow__ = make_wrapped_arith_op(\"__pow__\")\\n\\t__rpow__ = make_wrapped_arith_op(\"__rpow__\")\\n\\t__mul__ = make_wrapped_arith_op(\"__mul__\")\\n\\t__rmul__ = make_wrapped_arith_op(\"__rmul__\")\\n\\t__floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\\n\\t__rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\\n\\t__mod__ = make_wrapped_arith_op(\"__mod__\")\\n\\t__rmod__ = make_wrapped_arith_op(\"__rmod__\")\\n\\t__divmod__ = make_wrapped_arith_op(\"__divmod__\")\\n\\t__rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\\n\\t__truediv__ = make_wrapped_arith_op(\"__truediv__\")\\n\\t__rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")"
  },
  {
    "code": "def get_array_facts(self):\\n        facts = dict(facts_from_proxy=False, ssid=self.ssid)\\n        controller_reference_label = self.get_controllers()\\n        array_facts = None\\n        try:\\n            rc, array_facts = self.request(\"storage-systems/%s/graph\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to obtain facts from storage array with id [%s]. Error [%s]\"\\n                                      % (self.ssid, str(error)))\\n        facts['netapp_storage_array'] = dict(\\n            name=array_facts['sa']['saData']['storageArrayLabel'],\\n            chassis_serial=array_facts['sa']['saData']['chassisSerialNumber'],\\n            firmware=array_facts['sa']['saData']['fwVersion'],\\n            wwn=array_facts['sa']['saData']['saId']['worldWideName'],\\n            segment_sizes=array_facts['sa']['featureParameters']['supportedSegSizes'],\\n            cache_block_sizes=array_facts['sa']['featureParameters']['cacheBlockSizes'])\\n        facts['netapp_controllers'] = [\\n            dict(\\n                name=controller_reference_label[controller['controllerRef']],\\n                serial=controller['serialNumber'].strip(),\\n                status=controller['status'],\\n            ) for controller in array_facts['controller']]\\n        facts['netapp_host_groups'] = [\\n            dict(\\n                id=group['id'],\\n                name=group['name']\\n            ) for group in array_facts['storagePoolBundle']['cluster']]\\n        facts['netapp_hosts'] = [\\n            dict(\\n                group_id=host['clusterRef'],\\n                hosts_reference=host['hostRef'],\\n                id=host['id'],\\n                name=host['name'],\\n                host_type_index=host['hostTypeIndex'],\\n                posts=host['hostSidePorts']\\n            ) for host in array_facts['storagePoolBundle']['host']]\\n        facts['netapp_host_types'] = [\\n            dict(\\n                type=host_type['hostType'],\\n                index=host_type['index']\\n            ) for host_type in array_facts['sa']['hostSpecificVals']\\n            if 'hostType' in host_type.keys() and host_type['hostType']\\n        ]\\n        facts['snapshot_images'] = [\\n            dict(\\n                id=snapshot['id'],\\n                status=snapshot['status'],\\n                pit_capacity=snapshot['pitCapacity'],\\n                creation_method=snapshot['creationMethod'],\\n                reposity_cap_utilization=snapshot['repositoryCapacityUtilization'],\\n                active_cow=snapshot['activeCOW'],\\n                rollback_source=snapshot['isRollbackSource']\\n            ) for snapshot in array_facts['highLevelVolBundle']['pit']]\\n        facts['netapp_disks'] = [\\n            dict(\\n                id=disk['id'],\\n                available=disk['available'],\\n                media_type=disk['driveMediaType'],\\n                status=disk['status'],\\n                usable_bytes=disk['usableCapacity'],\\n                tray_ref=disk['physicalLocation']['trayRef'],\\n                product_id=disk['productID'],\\n                firmware_version=disk['firmwareVersion'],\\n                serial_number=disk['serialNumber'].lstrip()\\n            ) for disk in array_facts['drive']]\\n        facts['netapp_management_interfaces'] = [\\n            dict(controller=controller_reference_label[controller['controllerRef']],\\n                 name=iface['ethernet']['interfaceName'],\\n                 alias=iface['ethernet']['alias'],\\n                 channel=iface['ethernet']['channel'],\\n                 mac_address=iface['ethernet']['macAddr'],\\n                 remote_ssh_access=iface['ethernet']['rloginEnabled'],\\n                 link_status=iface['ethernet']['linkStatus'],\\n                 ipv4_enabled=iface['ethernet']['ipv4Enabled'],\\n                 ipv4_address_config_method=iface['ethernet']['ipv4AddressConfigMethod'].lower().replace(\"config\", \"\"),\\n                 ipv4_address=iface['ethernet']['ipv4Address'],\\n                 ipv4_subnet_mask=iface['ethernet']['ipv4SubnetMask'],\\n                 ipv4_gateway=iface['ethernet']['ipv4GatewayAddress'],\\n                 ipv6_enabled=iface['ethernet']['ipv6Enabled'],\\n                 dns_config_method=iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsAcquisitionType'],\\n                 dns_servers=(iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers']\\n                              if iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers'] else []),\\n                 ntp_config_method=iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpAcquisitionType'],\\n                 ntp_servers=(iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers']\\n                              if iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers'] else [])\\n                 ) for controller in array_facts['controller'] for iface in controller['netInterfaces']]\\n        facts['netapp_hostside_interfaces'] = [\\n            dict(\\n                fc=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['fibre']['channel'],\\n                         link_status=iface['fibre']['linkStatus'],\\n                         current_interface_speed=strip_interface_speed(iface['fibre']['currentInterfaceSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['fibre']['maximumInterfaceSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'fc'],\\n                ib=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['ib']['channel'],\\n                         link_status=iface['ib']['linkState'],\\n                         mtu=iface['ib']['maximumTransmissionUnit'],\\n                         current_interface_speed=strip_interface_speed(iface['ib']['currentSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['ib']['supportedSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'ib'],\\n                iscsi=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                            iqn=iface['iscsi']['iqn'],\\n                            link_status=iface['iscsi']['interfaceData']['ethernetData']['linkStatus'],\\n                            ipv4_enabled=iface['iscsi']['ipv4Enabled'],\\n                            ipv4_address=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4Address'],\\n                            ipv4_subnet_mask=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4SubnetMask'],\\n                            ipv4_gateway=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4GatewayAddress'],\\n                            ipv6_enabled=iface['iscsi']['ipv6Enabled'],\\n                            mtu=iface['iscsi']['interfaceData']['ethernetData']['maximumFramePayloadSize'],\\n                            current_interface_speed=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                          ['ethernetData']['currentInterfaceSpeed']),\\n                            supported_interface_speeds=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                             ['ethernetData']\\n                                                                             ['supportedInterfaceSpeeds']))\\n                       for controller in array_facts['controller']\\n                       for iface in controller['hostInterfaces']\\n                       if iface['interfaceType'] == 'iscsi'],\\n                sas=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                          channel=iface['sas']['channel'],\\n                          current_interface_speed=strip_interface_speed(iface['sas']['currentInterfaceSpeed']),\\n                          maximum_interface_speed=strip_interface_speed(iface['sas']['maximumInterfaceSpeed']),\\n                          link_status=iface['sas']['iocPort']['state'])\\n                     for controller in array_facts['controller']\\n                     for iface in controller['hostInterfaces']\\n                     if iface['interfaceType'] == 'sas'])]\\n        facts['netapp_driveside_interfaces'] = [\\n            dict(\\n                controller=controller_reference_label[controller['controllerRef']],\\n                interface_type=interface['interfaceType'],\\n                interface_speed=strip_interface_speed(\\n                    interface[interface['interfaceType']]['maximumInterfaceSpeed']\\n                    if (interface['interfaceType'] == 'sata' or\\n                        interface['interfaceType'] == 'sas' or\\n                        interface['interfaceType'] == 'fibre')\\n                    else (\\n                        interface[interface['interfaceType']]['currentSpeed']\\n                        if interface['interfaceType'] == 'ib'\\n                        else (\\n                            interface[interface['interfaceType']]['interfaceData']['maximumInterfaceSpeed']\\n                            if interface['interfaceType'] == 'iscsi' else 'unknown'\\n                        ))),\\n            )\\n            for controller in array_facts['controller']\\n            for interface in controller['driveInterfaces']]\\n        facts['netapp_storage_pools'] = [\\n            dict(\\n                id=storage_pool['id'],\\n                name=storage_pool['name'],\\n                available_capacity=storage_pool['freeSpace'],\\n                total_capacity=storage_pool['totalRaidedSpace'],\\n                used_capacity=storage_pool['usedSpace']\\n            ) for storage_pool in array_facts['volumeGroup']]\\n        all_volumes = list(array_facts['volume'])\\n        facts['netapp_volumes'] = [\\n            dict(\\n                id=v['id'],\\n                name=v['name'],\\n                parent_storage_pool_id=v['volumeGroupRef'],\\n                capacity=v['capacity'],\\n                is_thin_provisioned=v['thinProvisioned'],\\n                workload=v['metadata'],\\n            ) for v in all_volumes]\\n        workload_tags = None\\n        try:\\n            rc, workload_tags = self.request(\"storage-systems/%s/workloads\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to retrieve workload tags. Array [%s].\" % self.ssid)\\n        facts['netapp_workload_tags'] = [\\n            dict(\\n                id=workload_tag['id'],\\n                name=workload_tag['name'],\\n                attributes=workload_tag['workloadAttributes']\\n            ) for workload_tag in workload_tags]\\n        facts['netapp_volumes_by_initiators'] = dict()\\n        for mapping in array_facts['storagePoolBundle']['lunMapping']:\\n            for host in facts['netapp_hosts']:\\n                if mapping['mapRef'] == host['hosts_reference'] or mapping['mapRef'] == host['group_id']:\\n                    if host['name'] not in facts['netapp_volumes_by_initiators'].keys():\\n                        facts['netapp_volumes_by_initiators'].update({host['name']: []})\\n                    for volume in all_volumes:\\n                        if mapping['id'] in [volume_mapping['id'] for volume_mapping in volume['listOfMappings']]:\\n                            workload_name = \"\"\\n                            metadata = dict()\\n                            for volume_tag in volume['metadata']:\\n                                if volume_tag['key'] == 'workloadId':\\n                                    for workload_tag in facts['netapp_workload_tags']:\\n                                        if volume_tag['value'] == workload_tag['id']:\\n                                            workload_name = workload_tag['name']\\n                                            metadata = dict((entry['key'], entry['value'])\\n                                                            for entry in workload_tag['attributes']\\n                                                            if entry['key'] != 'profileId')\\n                            facts['netapp_volumes_by_initiators'][host['name']].append(\\n                                dict(name=volume['name'],\\n                                     id=volume['id'],\\n                                     wwn=volume['wwn'],\\n                                     workload_name=workload_name,\\n                                     meta_data=metadata))\\n        features = [feature for feature in array_facts['sa']['capabilities']]\\n        features.extend([feature['capability'] for feature in array_facts['sa']['premiumFeatures']\\n                         if feature['isEnabled']])\\n        features = list(set(features))  \\n        features.sort()\\n        facts['netapp_enabled_features'] = features\\n        return facts",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix documentation and facts_from_proxy value in the netapp_e_facts module. (#61072)",
    "fixed_code": "def get_array_facts(self):\\n        facts = dict(facts_from_proxy=(not self.is_embedded()), ssid=self.ssid)\\n        controller_reference_label = self.get_controllers()\\n        array_facts = None\\n        try:\\n            rc, array_facts = self.request(\"storage-systems/%s/graph\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to obtain facts from storage array with id [%s]. Error [%s]\" % (self.ssid, str(error)))\\n        facts['netapp_storage_array'] = dict(\\n            name=array_facts['sa']['saData']['storageArrayLabel'],\\n            chassis_serial=array_facts['sa']['saData']['chassisSerialNumber'],\\n            firmware=array_facts['sa']['saData']['fwVersion'],\\n            wwn=array_facts['sa']['saData']['saId']['worldWideName'],\\n            segment_sizes=array_facts['sa']['featureParameters']['supportedSegSizes'],\\n            cache_block_sizes=array_facts['sa']['featureParameters']['cacheBlockSizes'])\\n        facts['netapp_controllers'] = [\\n            dict(\\n                name=controller_reference_label[controller['controllerRef']],\\n                serial=controller['serialNumber'].strip(),\\n                status=controller['status'],\\n            ) for controller in array_facts['controller']]\\n        facts['netapp_host_groups'] = [\\n            dict(\\n                id=group['id'],\\n                name=group['name']\\n            ) for group in array_facts['storagePoolBundle']['cluster']]\\n        facts['netapp_hosts'] = [\\n            dict(\\n                group_id=host['clusterRef'],\\n                hosts_reference=host['hostRef'],\\n                id=host['id'],\\n                name=host['name'],\\n                host_type_index=host['hostTypeIndex'],\\n                posts=host['hostSidePorts']\\n            ) for host in array_facts['storagePoolBundle']['host']]\\n        facts['netapp_host_types'] = [\\n            dict(\\n                type=host_type['hostType'],\\n                index=host_type['index']\\n            ) for host_type in array_facts['sa']['hostSpecificVals']\\n            if 'hostType' in host_type.keys() and host_type['hostType']\\n        ]\\n        facts['snapshot_images'] = [\\n            dict(\\n                id=snapshot['id'],\\n                status=snapshot['status'],\\n                pit_capacity=snapshot['pitCapacity'],\\n                creation_method=snapshot['creationMethod'],\\n                reposity_cap_utilization=snapshot['repositoryCapacityUtilization'],\\n                active_cow=snapshot['activeCOW'],\\n                rollback_source=snapshot['isRollbackSource']\\n            ) for snapshot in array_facts['highLevelVolBundle']['pit']]\\n        facts['netapp_disks'] = [\\n            dict(\\n                id=disk['id'],\\n                available=disk['available'],\\n                media_type=disk['driveMediaType'],\\n                status=disk['status'],\\n                usable_bytes=disk['usableCapacity'],\\n                tray_ref=disk['physicalLocation']['trayRef'],\\n                product_id=disk['productID'],\\n                firmware_version=disk['firmwareVersion'],\\n                serial_number=disk['serialNumber'].lstrip()\\n            ) for disk in array_facts['drive']]\\n        facts['netapp_management_interfaces'] = [\\n            dict(controller=controller_reference_label[controller['controllerRef']],\\n                 name=iface['ethernet']['interfaceName'],\\n                 alias=iface['ethernet']['alias'],\\n                 channel=iface['ethernet']['channel'],\\n                 mac_address=iface['ethernet']['macAddr'],\\n                 remote_ssh_access=iface['ethernet']['rloginEnabled'],\\n                 link_status=iface['ethernet']['linkStatus'],\\n                 ipv4_enabled=iface['ethernet']['ipv4Enabled'],\\n                 ipv4_address_config_method=iface['ethernet']['ipv4AddressConfigMethod'].lower().replace(\"config\", \"\"),\\n                 ipv4_address=iface['ethernet']['ipv4Address'],\\n                 ipv4_subnet_mask=iface['ethernet']['ipv4SubnetMask'],\\n                 ipv4_gateway=iface['ethernet']['ipv4GatewayAddress'],\\n                 ipv6_enabled=iface['ethernet']['ipv6Enabled'],\\n                 dns_config_method=iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsAcquisitionType'],\\n                 dns_servers=(iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers']\\n                              if iface['ethernet']['dnsProperties']['acquisitionProperties']['dnsServers'] else []),\\n                 ntp_config_method=iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpAcquisitionType'],\\n                 ntp_servers=(iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers']\\n                              if iface['ethernet']['ntpProperties']['acquisitionProperties']['ntpServers'] else [])\\n                 ) for controller in array_facts['controller'] for iface in controller['netInterfaces']]\\n        facts['netapp_hostside_interfaces'] = [\\n            dict(\\n                fc=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['fibre']['channel'],\\n                         link_status=iface['fibre']['linkStatus'],\\n                         current_interface_speed=strip_interface_speed(iface['fibre']['currentInterfaceSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['fibre']['maximumInterfaceSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'fc'],\\n                ib=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                         channel=iface['ib']['channel'],\\n                         link_status=iface['ib']['linkState'],\\n                         mtu=iface['ib']['maximumTransmissionUnit'],\\n                         current_interface_speed=strip_interface_speed(iface['ib']['currentSpeed']),\\n                         maximum_interface_speed=strip_interface_speed(iface['ib']['supportedSpeed']))\\n                    for controller in array_facts['controller']\\n                    for iface in controller['hostInterfaces']\\n                    if iface['interfaceType'] == 'ib'],\\n                iscsi=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                            iqn=iface['iscsi']['iqn'],\\n                            link_status=iface['iscsi']['interfaceData']['ethernetData']['linkStatus'],\\n                            ipv4_enabled=iface['iscsi']['ipv4Enabled'],\\n                            ipv4_address=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4Address'],\\n                            ipv4_subnet_mask=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4SubnetMask'],\\n                            ipv4_gateway=iface['iscsi']['ipv4Data']['ipv4AddressData']['ipv4GatewayAddress'],\\n                            ipv6_enabled=iface['iscsi']['ipv6Enabled'],\\n                            mtu=iface['iscsi']['interfaceData']['ethernetData']['maximumFramePayloadSize'],\\n                            current_interface_speed=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                          ['ethernetData']['currentInterfaceSpeed']),\\n                            supported_interface_speeds=strip_interface_speed(iface['iscsi']['interfaceData']\\n                                                                             ['ethernetData']\\n                                                                             ['supportedInterfaceSpeeds']))\\n                       for controller in array_facts['controller']\\n                       for iface in controller['hostInterfaces']\\n                       if iface['interfaceType'] == 'iscsi'],\\n                sas=[dict(controller=controller_reference_label[controller['controllerRef']],\\n                          channel=iface['sas']['channel'],\\n                          current_interface_speed=strip_interface_speed(iface['sas']['currentInterfaceSpeed']),\\n                          maximum_interface_speed=strip_interface_speed(iface['sas']['maximumInterfaceSpeed']),\\n                          link_status=iface['sas']['iocPort']['state'])\\n                     for controller in array_facts['controller']\\n                     for iface in controller['hostInterfaces']\\n                     if iface['interfaceType'] == 'sas'])]\\n        facts['netapp_driveside_interfaces'] = [\\n            dict(\\n                controller=controller_reference_label[controller['controllerRef']],\\n                interface_type=interface['interfaceType'],\\n                interface_speed=strip_interface_speed(\\n                    interface[interface['interfaceType']]['maximumInterfaceSpeed']\\n                    if (interface['interfaceType'] == 'sata' or\\n                        interface['interfaceType'] == 'sas' or\\n                        interface['interfaceType'] == 'fibre')\\n                    else (\\n                        interface[interface['interfaceType']]['currentSpeed']\\n                        if interface['interfaceType'] == 'ib'\\n                        else (\\n                            interface[interface['interfaceType']]['interfaceData']['maximumInterfaceSpeed']\\n                            if interface['interfaceType'] == 'iscsi' else 'unknown'\\n                        ))),\\n            )\\n            for controller in array_facts['controller']\\n            for interface in controller['driveInterfaces']]\\n        facts['netapp_storage_pools'] = [\\n            dict(\\n                id=storage_pool['id'],\\n                name=storage_pool['name'],\\n                available_capacity=storage_pool['freeSpace'],\\n                total_capacity=storage_pool['totalRaidedSpace'],\\n                used_capacity=storage_pool['usedSpace']\\n            ) for storage_pool in array_facts['volumeGroup']]\\n        all_volumes = list(array_facts['volume'])\\n        facts['netapp_volumes'] = [\\n            dict(\\n                id=v['id'],\\n                name=v['name'],\\n                parent_storage_pool_id=v['volumeGroupRef'],\\n                capacity=v['capacity'],\\n                is_thin_provisioned=v['thinProvisioned'],\\n                workload=v['metadata'],\\n            ) for v in all_volumes]\\n        workload_tags = None\\n        try:\\n            rc, workload_tags = self.request(\"storage-systems/%s/workloads\" % self.ssid)\\n        except Exception as error:\\n            self.module.fail_json(msg=\"Failed to retrieve workload tags. Array [%s].\" % self.ssid)\\n        facts['netapp_workload_tags'] = [\\n            dict(\\n                id=workload_tag['id'],\\n                name=workload_tag['name'],\\n                attributes=workload_tag['workloadAttributes']\\n            ) for workload_tag in workload_tags]\\n        facts['netapp_volumes_by_initiators'] = dict()\\n        for mapping in array_facts['storagePoolBundle']['lunMapping']:\\n            for host in facts['netapp_hosts']:\\n                if mapping['mapRef'] == host['hosts_reference'] or mapping['mapRef'] == host['group_id']:\\n                    if host['name'] not in facts['netapp_volumes_by_initiators'].keys():\\n                        facts['netapp_volumes_by_initiators'].update({host['name']: []})\\n                    for volume in all_volumes:\\n                        if mapping['id'] in [volume_mapping['id'] for volume_mapping in volume['listOfMappings']]:\\n                            workload_name = \"\"\\n                            metadata = dict()\\n                            for volume_tag in volume['metadata']:\\n                                if volume_tag['key'] == 'workloadId':\\n                                    for workload_tag in facts['netapp_workload_tags']:\\n                                        if volume_tag['value'] == workload_tag['id']:\\n                                            workload_name = workload_tag['name']\\n                                            metadata = dict((entry['key'], entry['value'])\\n                                                            for entry in workload_tag['attributes']\\n                                                            if entry['key'] != 'profileId')\\n                            facts['netapp_volumes_by_initiators'][host['name']].append(\\n                                dict(name=volume['name'],\\n                                     id=volume['id'],\\n                                     wwn=volume['wwn'],\\n                                     workload_name=workload_name,\\n                                     meta_data=metadata))\\n        features = [feature for feature in array_facts['sa']['capabilities']]\\n        features.extend([feature['capability'] for feature in array_facts['sa']['premiumFeatures']\\n                         if feature['isEnabled']])\\n        features = list(set(features))  \\n        features.sort()\\n        facts['netapp_enabled_features'] = features\\n        return facts"
  },
  {
    "code": "def _add_margins(table, data, values, rows, cols, aggfunc):\\n\\tgrand_margin = _compute_grand_margin(data, values, aggfunc)\\n\\tif not values and isinstance(table, Series):\\n\\t\\trow_key = ('All',) + ('',) * (len(rows) - 1) if len(rows) > 1 else 'All'\\n\\t\\treturn table.append(Series({row_key: grand_margin['All']}))\\n\\tif values:\\n\\t\\tmarginal_result_set = _generate_marginal_results(table, data, values, rows, cols, aggfunc, grand_margin)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\telse:\\n\\t\\tmarginal_result_set = _generate_marginal_results_without_values(table, data, rows, cols, aggfunc)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\tkey = ('All',) + ('',) * (len(rows) - 1) if len(rows) > 1 else 'All'\\n\\trow_margin = row_margin.reindex(result.columns)\\n\\tfor k in margin_keys:\\n\\t\\tif isinstance(k, compat.string_types):\\n\\t\\t\\trow_margin[k] = grand_margin[k]\\n\\t\\telse:\\n\\t\\t\\trow_margin[k] = grand_margin[k[0]]\\n\\tmargin_dummy = DataFrame(row_margin, columns=[key]).T\\n\\trow_names = result.index.names\\n\\tresult = result.append(margin_dummy)\\n\\tresult.index.names = row_names\\n\\treturn result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: quick fix for #10989\\n\\nTST: add test case from Issue #10989",
    "fixed_code": "def _add_margins(table, data, values, rows, cols, aggfunc):\\n\\tgrand_margin = _compute_grand_margin(data, values, aggfunc)\\n\\tdef convert_categorical(ind):\\n\\t\\t_convert = lambda ind: (ind.astype('object')\\n\\t\\t\\t\\t\\t\\t\\t\\tif ind.dtype.name == 'category' else ind)\\n\\t\\tif isinstance(ind, MultiIndex):\\n\\t\\t\\treturn ind.set_levels([_convert(lev) for lev in ind.levels])\\n\\t\\telse:\\n\\t\\t\\treturn _convert(ind)\\n\\ttable.index = convert_categorical(table.index)\\n\\tif hasattr(table, 'columns'):\\n\\t\\ttable.columns = convert_categorical(table.columns)\\n\\tif not values and isinstance(table, Series):\\n\\t\\trow_key = ('All',) + ('',) * (len(rows) - 1) if len(rows) > 1 else 'All'\\n\\t\\treturn table.append(Series({row_key: grand_margin['All']}))\\n\\tif values:\\n\\t\\tmarginal_result_set = _generate_marginal_results(table, data, values, rows, cols, aggfunc, grand_margin)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\telse:\\n\\t\\tmarginal_result_set = _generate_marginal_results_without_values(table, data, rows, cols, aggfunc)\\n\\t\\tif not isinstance(marginal_result_set, tuple):\\n\\t\\t\\treturn marginal_result_set\\n\\t\\tresult, margin_keys, row_margin = marginal_result_set\\n\\tkey = ('All',) + ('',) * (len(rows) - 1) if len(rows) > 1 else 'All'\\n\\trow_margin = row_margin.reindex(result.columns)\\n\\tfor k in margin_keys:\\n\\t\\tif isinstance(k, compat.string_types):\\n\\t\\t\\trow_margin[k] = grand_margin[k]\\n\\t\\telse:\\n\\t\\t\\trow_margin[k] = grand_margin[k[0]]\\n\\tmargin_dummy = DataFrame(row_margin, columns=[key]).T\\n\\trow_names = result.index.names\\n\\tresult = result.append(margin_dummy)\\n\\tresult.index.names = row_names\\n\\treturn result"
  },
  {
    "code": "def _round_floor(self, prec, expdiff, context):\\n        if not self._sign:\\n            return self._round_down(prec, expdiff, context)\\n        else:\\n            return self._round_up(prec, expdiff, context)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Decimal module and test cases were updated to their state of the art.  It now complies latest specification and tests.\\n\\nThe only difference of this version with the one in the trunk\\nis that a small subset that hash tests were removed, because\\nthey rely on modifications to core hash() function (see\\nissue 1182 for further details).",
    "fixed_code": "def _round_floor(self, prec):\\n        if not self._sign:\\n            return self._round_down(prec)\\n        else:\\n            return -self._round_down(prec)"
  },
  {
    "code": "def _refold_parse_tree(parse_tree, *, policy):\\n    maxlen = policy.max_line_length or float(\"+inf\")\\n    encoding = 'utf-8' if policy.utf8 else 'us-ascii'\\n    lines = ['']\\n    last_ew = None\\n    wrap_as_ew_blocked = 0\\n    want_encoding = False\\n    end_ew_not_allowed = Terminal('', 'wrap_as_ew_blocked')\\n    parts = list(parse_tree)\\n    while parts:\\n        part = parts.pop(0)\\n        if part is end_ew_not_allowed:\\n            wrap_as_ew_blocked -= 1\\n            continue\\n        tstr = str(part)\\n        try:\\n            tstr.encode(encoding)\\n            charset = encoding\\n        except UnicodeEncodeError:\\n            if any(isinstance(x, errors.UndecodableBytesDefect)\\n                   for x in part.all_defects):\\n                charset = 'unknown-8bit'\\n            else:\\n                charset = 'utf-8'\\n            want_encoding = True\\n        if part.token_type == 'mime-parameters':\\n            _fold_mime_parameters(part, lines, maxlen, encoding)\\n            continue\\n        if want_encoding and not wrap_as_ew_blocked:\\n            if not part.as_ew_allowed:\\n                want_encoding = False\\n                last_ew = None\\n                if part.syntactic_break:\\n                    encoded_part = part.fold(policy=policy)[:-1] \\n                    if policy.linesep not in encoded_part:\\n                        if len(encoded_part) > maxlen - len(lines[-1]):\\n                            newline = _steal_trailing_WSP_if_exists(lines)\\n                            lines.append(newline)\\n                        lines[-1] += encoded_part\\n                        continue\\n            if not hasattr(part, 'encode'):\\n                parts = list(part) + parts\\n            else:\\n                last_ew = _fold_as_ew(tstr, lines, maxlen, last_ew,\\n                                      part.ew_combine_allowed, charset)\\n            want_encoding = False\\n            continue\\n        if len(tstr) <= maxlen - len(lines[-1]):\\n            lines[-1] += tstr\\n            continue\\n        if (part.syntactic_break and\\n                len(tstr) + 1 <= maxlen):\\n            newline = _steal_trailing_WSP_if_exists(lines)\\n            if newline or part.startswith_fws():\\n                lines.append(newline + tstr)\\n                continue\\n        if not hasattr(part, 'encode'):\\n            newparts = list(part)\\n            if not part.as_ew_allowed:\\n                wrap_as_ew_blocked += 1\\n                newparts.append(end_ew_not_allowed)\\n            parts = newparts + parts\\n            continue\\n        if part.as_ew_allowed and not wrap_as_ew_blocked:\\n            parts.insert(0, part)\\n            want_encoding = True\\n            continue\\n        newline = _steal_trailing_WSP_if_exists(lines)\\n        if newline or part.startswith_fws():\\n            lines.append(newline + tstr)\\n        else:\\n            lines[-1] += tstr\\n    return policy.linesep.join(lines) + policy.linesep",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-34424: Handle different policy.linesep lengths correctly. (#8803)",
    "fixed_code": "def _refold_parse_tree(parse_tree, *, policy):\\n    maxlen = policy.max_line_length or float(\"+inf\")\\n    encoding = 'utf-8' if policy.utf8 else 'us-ascii'\\n    lines = ['']\\n    last_ew = None\\n    wrap_as_ew_blocked = 0\\n    want_encoding = False\\n    end_ew_not_allowed = Terminal('', 'wrap_as_ew_blocked')\\n    parts = list(parse_tree)\\n    while parts:\\n        part = parts.pop(0)\\n        if part is end_ew_not_allowed:\\n            wrap_as_ew_blocked -= 1\\n            continue\\n        tstr = str(part)\\n        try:\\n            tstr.encode(encoding)\\n            charset = encoding\\n        except UnicodeEncodeError:\\n            if any(isinstance(x, errors.UndecodableBytesDefect)\\n                   for x in part.all_defects):\\n                charset = 'unknown-8bit'\\n            else:\\n                charset = 'utf-8'\\n            want_encoding = True\\n        if part.token_type == 'mime-parameters':\\n            _fold_mime_parameters(part, lines, maxlen, encoding)\\n            continue\\n        if want_encoding and not wrap_as_ew_blocked:\\n            if not part.as_ew_allowed:\\n                want_encoding = False\\n                last_ew = None\\n                if part.syntactic_break:\\n                    encoded_part = part.fold(policy=policy)[:-len(policy.linesep)]\\n                    if policy.linesep not in encoded_part:\\n                        if len(encoded_part) > maxlen - len(lines[-1]):\\n                            newline = _steal_trailing_WSP_if_exists(lines)\\n                            lines.append(newline)\\n                        lines[-1] += encoded_part\\n                        continue\\n            if not hasattr(part, 'encode'):\\n                parts = list(part) + parts\\n            else:\\n                last_ew = _fold_as_ew(tstr, lines, maxlen, last_ew,\\n                                      part.ew_combine_allowed, charset)\\n            want_encoding = False\\n            continue\\n        if len(tstr) <= maxlen - len(lines[-1]):\\n            lines[-1] += tstr\\n            continue\\n        if (part.syntactic_break and\\n                len(tstr) + 1 <= maxlen):\\n            newline = _steal_trailing_WSP_if_exists(lines)\\n            if newline or part.startswith_fws():\\n                lines.append(newline + tstr)\\n                continue\\n        if not hasattr(part, 'encode'):\\n            newparts = list(part)\\n            if not part.as_ew_allowed:\\n                wrap_as_ew_blocked += 1\\n                newparts.append(end_ew_not_allowed)\\n            parts = newparts + parts\\n            continue\\n        if part.as_ew_allowed and not wrap_as_ew_blocked:\\n            parts.insert(0, part)\\n            want_encoding = True\\n            continue\\n        newline = _steal_trailing_WSP_if_exists(lines)\\n        if newline or part.startswith_fws():\\n            lines.append(newline + tstr)\\n        else:\\n            lines[-1] += tstr\\n    return policy.linesep.join(lines) + policy.linesep"
  },
  {
    "code": "def skew(self, axis=0, skipna=True):\\n        y, axis_labels = self._get_agg_data(axis, numeric_only=True)\\n        mask = np.isnan(y)\\n        count = (y.shape[axis] - mask.sum(axis)).astype(float)\\n        if skipna:\\n            np.putmask(y, mask, 0)\\n        A = y.sum(axis) / count\\n        B = (y ** 2).sum(axis) / count - A ** 2\\n        C = (y ** 3).sum(axis) / count - A ** 3 - 3 * A * B\\n        B = np.where(np.abs(B) < 1e-14, 0, B)\\n        C = np.where(np.abs(C) < 1e-14, 0, C)\\n        result = ((np.sqrt((count ** 2 - count)) * C) /\\n                  ((count - 2) * np.sqrt(B) ** 3))\\n        result = np.where(B == 0, 0, result)\\n        return Series(result, index=axis_labels)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def skew(self, axis=0, skipna=True):\\n        y, axis_labels = self._get_agg_data(axis, numeric_only=True)\\n        mask = np.isnan(y)\\n        count = (y.shape[axis] - mask.sum(axis)).astype(float)\\n        if skipna:\\n            np.putmask(y, mask, 0)\\n        A = y.sum(axis) / count\\n        B = (y ** 2).sum(axis) / count - A ** 2\\n        C = (y ** 3).sum(axis) / count - A ** 3 - 3 * A * B\\n        B = np.where(np.abs(B) < 1e-14, 0, B)\\n        C = np.where(np.abs(C) < 1e-14, 0, C)\\n        result = ((np.sqrt((count ** 2 - count)) * C) /\\n                  ((count - 2) * np.sqrt(B) ** 3))\\n        result = np.where(B == 0, 0, result)\\n        return Series(result, index=axis_labels)"
  },
  {
    "code": "def fix(x, y=None):\\n    x = asarray(x)\\n    if y is None:\\n        y = _nx.floor(x)\\n    else:\\n        _nx.floor(x,y)\\n    if x.ndim == 0:\\n        if (x<0):\\n            y += 1\\n    else:\\n        y[x<0] = y[x<0]+1\\n    return y",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Give ufunclike some TLC. Cleanup, add docstrings and unit tests. The unit tests are as doctests, and currently use the deprecated test_suite method of running.",
    "fixed_code": "def fix(x, y=None):\\n    x = asarray(x)\\n    if y is None:\\n        y = nx.floor(x)\\n    else:\\n        nx.floor(x, y)\\n    if x.ndim == 0:\\n        if (x<0):\\n            y += 1\\n    else:\\n        y[x<0] = y[x<0]+1\\n    return y"
  },
  {
    "code": "def discover_interpreter(action, interpreter_name, discovery_mode, task_vars):\\n    if interpreter_name != 'python':\\n        raise ValueError('Interpreter discovery not supported for {0}'.format(interpreter_name))\\n    host = task_vars.get('inventory_hostname', 'unknown')\\n    res = None\\n    platform_type = 'unknown'\\n    found_interpreters = [u'/usr/bin/python']  \\n    is_auto_legacy = discovery_mode.startswith('auto_legacy')\\n    is_silent = discovery_mode.endswith('_silent')\\n    try:\\n        platform_python_map = C.config.get_config_value('INTERPRETER_PYTHON_DISTRO_MAP', variables=task_vars)\\n        bootstrap_python_list = C.config.get_config_value('INTERPRETER_PYTHON_FALLBACK', variables=task_vars)\\n        display.vvv(msg=u\"Attempting {0} interpreter discovery\".format(interpreter_name), host=host)\\n        command_list = [\"command -v '%s'\" % py for py in bootstrap_python_list]\\n        shell_bootstrap = \"echo PLATFORM; uname; echo FOUND; {0}; echo ENDFOUND\".format('; '.join(command_list))\\n        res = action._low_level_execute_command(shell_bootstrap, sudoable=False)\\n        raw_stdout = res.get('stdout', u'')\\n        match = foundre.match(raw_stdout)\\n        if not match:\\n            display.debug(u'raw interpreter discovery output: {0}'.format(raw_stdout), host=host)\\n            raise ValueError('unexpected output from Python interpreter discovery')\\n        platform_type = match.groups()[0].lower().strip()\\n        found_interpreters = [interp.strip() for interp in match.groups()[1].splitlines() if interp.startswith('/')]\\n        display.debug(u\"found interpreters: {0}\".format(found_interpreters), host=host)\\n        if not found_interpreters:\\n            if not is_silent:\\n                action._discovery_warnings.append(u'No python interpreters found for '\\n                                                  u'host {0} (tried {1})'.format(host, bootstrap_python_list))\\n            return u'/usr/bin/python'\\n        if platform_type != 'linux':\\n            raise NotImplementedError('unsupported platform for extended discovery: {0}'.format(to_native(platform_type)))\\n        platform_script = pkgutil.get_data('ansible.executor.discovery', 'python_target.py')\\n        if action._connection.has_pipelining:\\n            res = action._low_level_execute_command(found_interpreters[0], sudoable=False, in_data=platform_script)\\n        else:\\n            raise NotImplementedError('pipelining support required for extended interpreter discovery')\\n        platform_info = json.loads(res.get('stdout'))\\n        distro, version = _get_linux_distro(platform_info)\\n        if not distro or not version:\\n            raise NotImplementedError('unable to get Linux distribution/version info')\\n        version_map = platform_python_map.get(distro.lower().strip())\\n        if not version_map:\\n            raise NotImplementedError('unsupported Linux distribution: {0}'.format(distro))\\n        platform_interpreter = to_text(_version_fuzzy_match(version, version_map), errors='surrogate_or_strict')\\n        if is_auto_legacy:\\n            if platform_interpreter != u'/usr/bin/python' and u'/usr/bin/python' in found_interpreters:\\n                if not is_silent:\\n                    action._discovery_deprecation_warnings.append(dict(\\n                        msg=u\"Distribution {0} {1} on host {2} should use {3}, but is using \"\\n                            u\"/usr/bin/python for backward compatibility with prior Ansible releases. \"\\n                            u\"A future Ansible release will default to using the discovered platform \"\\n                            u\"python for this host. See {4} for more information\"\\n                            .format(distro, version, host, platform_interpreter,\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')),\\n                        version='2.12'))\\n                return u'/usr/bin/python'\\n        if platform_interpreter not in found_interpreters:\\n            if platform_interpreter not in bootstrap_python_list:\\n                if not is_silent:\\n                    action._discovery_warnings \\\\n                        .append(u\"Platform interpreter {0} on host {1} is missing from bootstrap list\"\\n                                .format(platform_interpreter, host))\\n            if not is_silent:\\n                action._discovery_warnings \\\\n                    .append(u\"Distribution {0} {1} on host {2} should use {3}, but is using {4}, since the \"\\n                            u\"discovered platform python interpreter was not present. See {5} \"\\n                            u\"for more information.\"\\n                            .format(distro, version, host, platform_interpreter, found_interpreters[0],\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n            return found_interpreters[0]\\n        return platform_interpreter\\n    except NotImplementedError as ex:\\n        display.vvv(msg=u'Python interpreter discovery fallback ({0})'.format(to_text(ex)), host=host)\\n    except Exception as ex:\\n        if not is_silent:\\n            display.warning(msg=u'Unhandled error in Python interpreter discovery for host {0}: {1}'.format(host, to_text(ex)))\\n            display.debug(msg=u'Interpreter discovery traceback:\\n{0}'.format(to_text(format_exc())), host=host)\\n            if res and res.get('stderr'):\\n                display.vvv(msg=u'Interpreter discovery remote stderr:\\n{0}'.format(to_text(res.get('stderr'))), host=host)\\n    if not is_silent:\\n        action._discovery_warnings \\\\n            .append(u\"Platform {0} on host {1} is using the discovered Python interpreter at {2}, but future installation of \"\\n                    u\"another Python interpreter could change the meaning of that path. See {3} \"\\n                    u\"for more information.\"\\n                    .format(platform_type, host, found_interpreters[0],\\n                            get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n    return found_interpreters[0]",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Change interpreter discovery defaults to silently prefer Python 3 (#75049)\\n\\nPush /usr/bin/python to almost the bottom of the barrel. This makes the strategy to prefer\\nspecific versions of the \"mystery meat\" version.\\n\\n    Change description to match change in behavior.\\n    Change deprecation message to a warning.",
    "fixed_code": "def discover_interpreter(action, interpreter_name, discovery_mode, task_vars):\\n    if interpreter_name != 'python':\\n        raise ValueError('Interpreter discovery not supported for {0}'.format(interpreter_name))\\n    host = task_vars.get('inventory_hostname', 'unknown')\\n    res = None\\n    platform_type = 'unknown'\\n    found_interpreters = [u'/usr/bin/python']  \\n    is_auto_legacy = discovery_mode.startswith('auto_legacy')\\n    is_silent = discovery_mode.endswith('_silent')\\n    try:\\n        platform_python_map = C.config.get_config_value('INTERPRETER_PYTHON_DISTRO_MAP', variables=task_vars)\\n        bootstrap_python_list = C.config.get_config_value('INTERPRETER_PYTHON_FALLBACK', variables=task_vars)\\n        display.vvv(msg=u\"Attempting {0} interpreter discovery\".format(interpreter_name), host=host)\\n        command_list = [\"command -v '%s'\" % py for py in bootstrap_python_list]\\n        shell_bootstrap = \"echo PLATFORM; uname; echo FOUND; {0}; echo ENDFOUND\".format('; '.join(command_list))\\n        res = action._low_level_execute_command(shell_bootstrap, sudoable=False)\\n        raw_stdout = res.get('stdout', u'')\\n        match = foundre.match(raw_stdout)\\n        if not match:\\n            display.debug(u'raw interpreter discovery output: {0}'.format(raw_stdout), host=host)\\n            raise ValueError('unexpected output from Python interpreter discovery')\\n        platform_type = match.groups()[0].lower().strip()\\n        found_interpreters = [interp.strip() for interp in match.groups()[1].splitlines() if interp.startswith('/')]\\n        display.debug(u\"found interpreters: {0}\".format(found_interpreters), host=host)\\n        if not found_interpreters:\\n            if not is_silent:\\n                action._discovery_warnings.append(u'No python interpreters found for '\\n                                                  u'host {0} (tried {1})'.format(host, bootstrap_python_list))\\n            return u'/usr/bin/python'\\n        if platform_type != 'linux':\\n            raise NotImplementedError('unsupported platform for extended discovery: {0}'.format(to_native(platform_type)))\\n        platform_script = pkgutil.get_data('ansible.executor.discovery', 'python_target.py')\\n        if action._connection.has_pipelining:\\n            res = action._low_level_execute_command(found_interpreters[0], sudoable=False, in_data=platform_script)\\n        else:\\n            raise NotImplementedError('pipelining support required for extended interpreter discovery')\\n        platform_info = json.loads(res.get('stdout'))\\n        distro, version = _get_linux_distro(platform_info)\\n        if not distro or not version:\\n            raise NotImplementedError('unable to get Linux distribution/version info')\\n        version_map = platform_python_map.get(distro.lower().strip())\\n        if not version_map:\\n            raise NotImplementedError('unsupported Linux distribution: {0}'.format(distro))\\n        platform_interpreter = to_text(_version_fuzzy_match(version, version_map), errors='surrogate_or_strict')\\n        if is_auto_legacy:\\n            if platform_interpreter != u'/usr/bin/python' and u'/usr/bin/python' in found_interpreters:\\n                if not is_silent:\\n                    action._discovery_warnings.append(\\n                        u\"Distribution {0} {1} on host {2} should use {3}, but is using \"\\n                        u\"/usr/bin/python for backward compatibility with prior Ansible releases. \"\\n                        u\"See {4} for more information\"\\n                        .format(distro, version, host, platform_interpreter,\\n                                get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n                return u'/usr/bin/python'\\n        if platform_interpreter not in found_interpreters:\\n            if platform_interpreter not in bootstrap_python_list:\\n                if not is_silent:\\n                    action._discovery_warnings \\\\n                        .append(u\"Platform interpreter {0} on host {1} is missing from bootstrap list\"\\n                                .format(platform_interpreter, host))\\n            if not is_silent:\\n                action._discovery_warnings \\\\n                    .append(u\"Distribution {0} {1} on host {2} should use {3}, but is using {4}, since the \"\\n                            u\"discovered platform python interpreter was not present. See {5} \"\\n                            u\"for more information.\"\\n                            .format(distro, version, host, platform_interpreter, found_interpreters[0],\\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n            return found_interpreters[0]\\n        return platform_interpreter\\n    except NotImplementedError as ex:\\n        display.vvv(msg=u'Python interpreter discovery fallback ({0})'.format(to_text(ex)), host=host)\\n    except Exception as ex:\\n        if not is_silent:\\n            display.warning(msg=u'Unhandled error in Python interpreter discovery for host {0}: {1}'.format(host, to_text(ex)))\\n            display.debug(msg=u'Interpreter discovery traceback:\\n{0}'.format(to_text(format_exc())), host=host)\\n            if res and res.get('stderr'):\\n                display.vvv(msg=u'Interpreter discovery remote stderr:\\n{0}'.format(to_text(res.get('stderr'))), host=host)\\n    if not is_silent:\\n        action._discovery_warnings \\\\n            .append(u\"Platform {0} on host {1} is using the discovered Python interpreter at {2}, but future installation of \"\\n                    u\"another Python interpreter could change the meaning of that path. See {3} \"\\n                    u\"for more information.\"\\n                    .format(platform_type, host, found_interpreters[0],\\n                            get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\\n    return found_interpreters[0]"
  },
  {
    "code": "def do_related_class(self, other, cls):\\n        self.set_attributes_from_rel()\\n        self.related = RelatedObject(other, cls, self)\\n        if not cls._meta.abstract:\\n            self.contribute_to_related_class(other, self.related)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def do_related_class(self, other, cls):\\n        self.set_attributes_from_rel()\\n        self.related = RelatedObject(other, cls, self)\\n        if not cls._meta.abstract:\\n            self.contribute_to_related_class(other, self.related)"
  },
  {
    "code": "def exit_json(self, **kwargs):\\n        ''\\n        if 'changed' not in kwargs:\\n            kwargs['changed'] = False\\n        self.do_cleanup_files()\\n        self._return_formatted(kwargs)\\n        sys.exit(0)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "better backwards compat handling of status\\n\\nrestored 'rc' inspection but only when failed is not specified\\nremoved redundant changed from basic.py as task_executor already adds\\nremoved redundant filters, they are tests\\nadded aliases to tests removed from filters\\nfixed test to new rc handling",
    "fixed_code": "def exit_json(self, **kwargs):\\n        ''\\n        self.do_cleanup_files()\\n        self._return_formatted(kwargs)\\n        sys.exit(0)"
  },
  {
    "code": "def __mul__(self, someInt):\\n        return self.__class__(n=someInt * self.n, **self.kwds)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: offset normalize option may not work in addition/subtraction",
    "fixed_code": "def __mul__(self, someInt):\\n        return self.__class__(n=someInt * self.n, normalize=self.normalize, **self.kwds)"
  },
  {
    "code": "def setup(self, tz, freq):\\n        self.ts = Timestamp('2017-08-25 08:16:14', tzinfo=tz, freq=freq)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def setup(self, tz, freq):\\n        self.ts = Timestamp('2017-08-25 08:16:14', tzinfo=tz, freq=freq)"
  },
  {
    "code": "def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\\n        key = self.make_key(key, version=version)\\n        self.validate_key(key)\\n        try:\\n            pickled = pickle.dumps(value, pickle.HIGHEST_PROTOCOL)\\n        except pickle.PickleError:\\n            return False\\n        with self._lock.writer():\\n            exp = self._expire_info.get(key)\\n            if exp is None or exp <= time.time():\\n                self._set(key, pickled, timeout)\\n                return True\\n            return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Follow up to e112654fc81ddb3fbffbb8382b004d69367a85fe\\n\\nActually comitted the code now :\u00fe",
    "fixed_code": "def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\\n        key = self.make_key(key, version=version)\\n        self.validate_key(key)\\n        pickled = pickle.dumps(value, pickle.HIGHEST_PROTOCOL)\\n        with self._lock.writer():\\n            exp = self._expire_info.get(key)\\n            if exp is None or exp <= time.time():\\n                self._set(key, pickled, timeout)\\n                return True\\n            return False"
  },
  {
    "code": "def quote(file):\\n    ''\\n    for c in file:\\n        if c not in _safechars:\\n            break\\n    else:\\n        if not file:\\n            return \"''\"\\n        return file\\n    if '\\'' not in file:\\n        return '\\'' + file + '\\''\\n    res = ''\\n    for c in file:\\n        if c in _funnychars:\\n            c = '\\\\' + c\\n        res = res + c\\n    return '\"' + res + '\"'",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def quote(file):\\n    ''\\n    for c in file:\\n        if c not in _safechars:\\n            break\\n    else:\\n        if not file:\\n            return \"''\"\\n        return file\\n    if '\\'' not in file:\\n        return '\\'' + file + '\\''\\n    res = ''\\n    for c in file:\\n        if c in _funnychars:\\n            c = '\\\\' + c\\n        res = res + c\\n    return '\"' + res + '\"'"
  },
  {
    "code": "def _instance_type_get_query(context, session=None, read_deleted=None):\\n\\tquery = model_query(context, models.InstanceTypes, session=session,\\n\\t\\t\\t\\t\\t   read_deleted=read_deleted).\\\\n\\t\\t\\t\\t\\t   options(joinedload('extra_specs'))\\n\\tif not context.is_admin:\\n\\t\\tthe_filter = [models.InstanceTypes.is_public == True]\\n\\t\\tthe_filter.extend([\\n\\t\\t\\tmodels.InstanceTypes.projects.any(project_id=context.project_id)\\n\\t\\t])\\n\\t\\tquery = query.filter(or_(*the_filter))\\n\\treturn query",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _instance_type_get_query(context, session=None, read_deleted=None):\\n\\tquery = model_query(context, models.InstanceTypes, session=session,\\n\\t\\t\\t\\t\\t   read_deleted=read_deleted).\\\\n\\t\\t\\t\\t\\t   options(joinedload('extra_specs'))\\n\\tif not context.is_admin:\\n\\t\\tthe_filter = [models.InstanceTypes.is_public == True]\\n\\t\\tthe_filter.extend([\\n\\t\\t\\tmodels.InstanceTypes.projects.any(project_id=context.project_id)\\n\\t\\t])\\n\\t\\tquery = query.filter(or_(*the_filter))\\n\\treturn query"
  },
  {
    "code": "def get_template_env(self) -> jinja2.Environment:\\n        searchpath = [self.folder]\\n        if self.template_searchpath:\\n            searchpath += self.template_searchpath\\n        jinja_env_options = {\\n            'loader': jinja2.FileSystemLoader(searchpath),\\n            'undefined': self.template_undefined,\\n            'extensions': [\"jinja2.ext.do\"],\\n            'cache_size': 0,\\n        }\\n        if self.jinja_environment_kwargs:\\n            jinja_env_options.update(self.jinja_environment_kwargs)\\n        env: jinja2.Environment\\n        if self.render_template_as_native_obj:\\n            env = airflow.templates.NativeEnvironment(**jinja_env_options)\\n        else:\\n            env = airflow.templates.SandboxedEnvironment(**jinja_env_options)\\n        if self.user_defined_macros:\\n            env.globals.update(self.user_defined_macros)\\n        if self.user_defined_filters:\\n            env.filters.update(self.user_defined_filters)\\n        return env",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix `email_on_failure` with `render_template_as_native_obj` (#22770)",
    "fixed_code": "def get_template_env(self, *, force_sandboxed: bool = False) -> jinja2.Environment:\\n        searchpath = [self.folder]\\n        if self.template_searchpath:\\n            searchpath += self.template_searchpath\\n        jinja_env_options = {\\n            'loader': jinja2.FileSystemLoader(searchpath),\\n            'undefined': self.template_undefined,\\n            'extensions': [\"jinja2.ext.do\"],\\n            'cache_size': 0,\\n        }\\n        if self.jinja_environment_kwargs:\\n            jinja_env_options.update(self.jinja_environment_kwargs)\\n        env: jinja2.Environment\\n        if self.render_template_as_native_obj and not force_sandboxed:\\n            env = airflow.templates.NativeEnvironment(**jinja_env_options)\\n        else:\\n            env = airflow.templates.SandboxedEnvironment(**jinja_env_options)\\n        if self.user_defined_macros:\\n            env.globals.update(self.user_defined_macros)\\n        if self.user_defined_filters:\\n            env.filters.update(self.user_defined_filters)\\n        return env"
  },
  {
    "code": "def to_gbq(self, destination_table, project_id=None, chunksize=None,\\n               reauth=False, if_exists='fail', private_key=None,\\n               auth_local_webserver=False, table_schema=None, location=None,\\n               progress_bar=True, verbose=None):\\n        from pandas.io import gbq\\n        return gbq.to_gbq(\\n            self, destination_table, project_id=project_id,\\n            chunksize=chunksize, reauth=reauth,\\n            if_exists=if_exists, private_key=private_key,\\n            auth_local_webserver=auth_local_webserver,\\n            table_schema=table_schema, location=location,\\n            progress_bar=progress_bar, verbose=verbose)\\n    @classmethod",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def to_gbq(self, destination_table, project_id=None, chunksize=None,\\n               reauth=False, if_exists='fail', private_key=None,\\n               auth_local_webserver=False, table_schema=None, location=None,\\n               progress_bar=True, verbose=None):\\n        from pandas.io import gbq\\n        return gbq.to_gbq(\\n            self, destination_table, project_id=project_id,\\n            chunksize=chunksize, reauth=reauth,\\n            if_exists=if_exists, private_key=private_key,\\n            auth_local_webserver=auth_local_webserver,\\n            table_schema=table_schema, location=location,\\n            progress_bar=progress_bar, verbose=verbose)\\n    @classmethod"
  },
  {
    "code": "def _bool_method_SERIES(cls, op, special):\\n\\tdef na_op(x, y):\\n\\t\\ttry:\\n\\t\\t\\tresult = op(x, y)\\n\\t\\texcept TypeError:\\n\\t\\t\\tif isinstance(y, list):\\n\\t\\t\\t\\ty = construct_1d_object_array_from_listlike(y)\\n\\t\\t\\tif isinstance(y, (np.ndarray, ABCSeries, ABCIndexClass)):\\n\\t\\t\\t\\tif (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype)):\\n\\t\\t\\t\\t\\tresult = op(x, y)  \\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tx = ensure_object(x)\\n\\t\\t\\t\\t\\ty = ensure_object(y)\\n\\t\\t\\t\\t\\tresult = libops.vec_binop(x, y, op)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif not isna(y):\\n\\t\\t\\t\\t\\ty = bool(y)\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tresult = libops.scalar_binop(x, y, op)\\n\\t\\t\\t\\texcept:\\n\\t\\t\\t\\t\\traise TypeError(\"cannot compare a dtyped [{dtype}] array \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"with a scalar of type [{typ}]\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t.format(dtype=x.dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttyp=type(y).__name__))\\n\\t\\treturn result\\n\\tfill_int = lambda x: x.fillna(0)\\n\\tfill_bool = lambda x: x.fillna(False).astype(bool)\\n\\tdef wrapper(self, other):\\n\\t\\tis_self_int_dtype = is_integer_dtype(self.dtype)\\n\\t\\tself, other = _align_method_SERIES(self, other, align_asobject=True)\\n\\t\\tif isinstance(other, ABCDataFrame):\\n\\t\\t\\treturn NotImplemented\\n\\t\\telif isinstance(other, ABCSeries):\\n\\t\\t\\tname = get_op_result_name(self, other)\\n\\t\\t\\tis_other_int_dtype = is_integer_dtype(other.dtype)\\n\\t\\t\\tother = fill_int(other) if is_other_int_dtype else fill_bool(other)\\n\\t\\t\\tfiller = (fill_int if is_self_int_dtype and is_other_int_dtype\\n\\t\\t\\t\\t\\t  else fill_bool)\\n\\t\\t\\tres_values = na_op(self.values, other.values)\\n\\t\\t\\tunfilled = self._constructor(res_values,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t index=self.index, name=name)\\n\\t\\t\\treturn filler(unfilled)\\n\\t\\telse:\\n\\t\\t\\tfiller = (fill_int if is_self_int_dtype and\\n\\t\\t\\t\\t\\t  is_integer_dtype(np.asarray(other)) else fill_bool)\\n\\t\\t\\tres_values = na_op(self.values, other)\\n\\t\\t\\tunfilled = self._constructor(res_values, index=self.index)\\n\\t\\t\\treturn filler(unfilled).__finalize__(self)\\n\\treturn wrapper",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _bool_method_SERIES(cls, op, special):\\n\\tdef na_op(x, y):\\n\\t\\ttry:\\n\\t\\t\\tresult = op(x, y)\\n\\t\\texcept TypeError:\\n\\t\\t\\tif isinstance(y, list):\\n\\t\\t\\t\\ty = construct_1d_object_array_from_listlike(y)\\n\\t\\t\\tif isinstance(y, (np.ndarray, ABCSeries, ABCIndexClass)):\\n\\t\\t\\t\\tif (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype)):\\n\\t\\t\\t\\t\\tresult = op(x, y)  \\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tx = ensure_object(x)\\n\\t\\t\\t\\t\\ty = ensure_object(y)\\n\\t\\t\\t\\t\\tresult = libops.vec_binop(x, y, op)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif not isna(y):\\n\\t\\t\\t\\t\\ty = bool(y)\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tresult = libops.scalar_binop(x, y, op)\\n\\t\\t\\t\\texcept:\\n\\t\\t\\t\\t\\traise TypeError(\"cannot compare a dtyped [{dtype}] array \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"with a scalar of type [{typ}]\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t.format(dtype=x.dtype,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttyp=type(y).__name__))\\n\\t\\treturn result\\n\\tfill_int = lambda x: x.fillna(0)\\n\\tfill_bool = lambda x: x.fillna(False).astype(bool)\\n\\tdef wrapper(self, other):\\n\\t\\tis_self_int_dtype = is_integer_dtype(self.dtype)\\n\\t\\tself, other = _align_method_SERIES(self, other, align_asobject=True)\\n\\t\\tif isinstance(other, ABCDataFrame):\\n\\t\\t\\treturn NotImplemented\\n\\t\\telif isinstance(other, ABCSeries):\\n\\t\\t\\tname = get_op_result_name(self, other)\\n\\t\\t\\tis_other_int_dtype = is_integer_dtype(other.dtype)\\n\\t\\t\\tother = fill_int(other) if is_other_int_dtype else fill_bool(other)\\n\\t\\t\\tfiller = (fill_int if is_self_int_dtype and is_other_int_dtype\\n\\t\\t\\t\\t\\t  else fill_bool)\\n\\t\\t\\tres_values = na_op(self.values, other.values)\\n\\t\\t\\tunfilled = self._constructor(res_values,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t index=self.index, name=name)\\n\\t\\t\\treturn filler(unfilled)\\n\\t\\telse:\\n\\t\\t\\tfiller = (fill_int if is_self_int_dtype and\\n\\t\\t\\t\\t\\t  is_integer_dtype(np.asarray(other)) else fill_bool)\\n\\t\\t\\tres_values = na_op(self.values, other)\\n\\t\\t\\tunfilled = self._constructor(res_values, index=self.index)\\n\\t\\t\\treturn filler(unfilled).__finalize__(self)\\n\\treturn wrapper"
  },
  {
    "code": "def name(self):\\n        if self._column is None:\\n            return None \\n        else:\\n            return self._column\\n    @property",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: can pass multiple columns to GroupBy.__getitem__, close #383",
    "fixed_code": "def name(self):\\n        if self._selection is None:\\n            return None \\n        else:\\n            return self._selection\\n    @property"
  },
  {
    "code": "def _init_fn(fields, frozen, has_post_init, self_name, globals):\\n    seen_default = False\\n    for f in fields:\\n        if f.init:\\n            if not (f.default is MISSING and f.default_factory is MISSING):\\n                seen_default = True\\n            elif seen_default:\\n                raise TypeError(f'non-default argument {f.name!r} '\\n                                'follows default argument')\\n    locals = {f'_type_{f.name}': f.type for f in fields}\\n    locals.update({\\n        'MISSING': MISSING,\\n        '_HAS_DEFAULT_FACTORY': _HAS_DEFAULT_FACTORY,\\n    })\\n    body_lines = []\\n    for f in fields:\\n        line = _field_init(f, frozen, locals, self_name)\\n        if line:\\n            body_lines.append(line)\\n    if has_post_init:\\n        params_str = ','.join(f.name for f in fields\\n                              if f._field_type is _FIELD_INITVAR)\\n        body_lines.append(f'{self_name}.{_POST_INIT_NAME}({params_str})')\\n    if not body_lines:\\n        body_lines = ['pass']\\n    return _create_fn('__init__',\\n                      [self_name] + [_init_param(f) for f in fields if f.init],\\n                      body_lines,\\n                      locals=locals,\\n                      globals=globals,\\n                      return_type=None)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _init_fn(fields, frozen, has_post_init, self_name, globals):\\n    seen_default = False\\n    for f in fields:\\n        if f.init:\\n            if not (f.default is MISSING and f.default_factory is MISSING):\\n                seen_default = True\\n            elif seen_default:\\n                raise TypeError(f'non-default argument {f.name!r} '\\n                                'follows default argument')\\n    locals = {f'_type_{f.name}': f.type for f in fields}\\n    locals.update({\\n        'MISSING': MISSING,\\n        '_HAS_DEFAULT_FACTORY': _HAS_DEFAULT_FACTORY,\\n    })\\n    body_lines = []\\n    for f in fields:\\n        line = _field_init(f, frozen, locals, self_name)\\n        if line:\\n            body_lines.append(line)\\n    if has_post_init:\\n        params_str = ','.join(f.name for f in fields\\n                              if f._field_type is _FIELD_INITVAR)\\n        body_lines.append(f'{self_name}.{_POST_INIT_NAME}({params_str})')\\n    if not body_lines:\\n        body_lines = ['pass']\\n    return _create_fn('__init__',\\n                      [self_name] + [_init_param(f) for f in fields if f.init],\\n                      body_lines,\\n                      locals=locals,\\n                      globals=globals,\\n                      return_type=None)"
  },
  {
    "code": "def init_dict(data, index, columns, dtype=None):\\n    if columns is not None:\\n        from pandas.core.series import Series\\n        arrays = Series(data, index=columns, dtype=object)\\n        data_names = arrays.index\\n        missing = arrays.isnull()\\n        if index is None:\\n            index = extract_index(arrays[~missing])\\n        else:\\n            index = ensure_index(index)\\n        if missing.any() and not is_integer_dtype(dtype):\\n            if dtype is None or np.issubdtype(dtype, np.flexible):\\n                nan_dtype = object\\n            else:\\n                nan_dtype = dtype\\n            v = construct_1d_arraylike_from_scalar(np.nan, len(index),\\n                                                   nan_dtype)\\n            arrays.loc[missing] = [v] * missing.sum()\\n    else:\\n        keys = com.dict_keys_to_ordered_list(data)\\n        columns = data_names = Index(keys)\\n        arrays = [data[k] for k in keys]\\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix mutation of DTI backing Series/DataFrame (#24096)",
    "fixed_code": "def init_dict(data, index, columns, dtype=None):\\n    if columns is not None:\\n        from pandas.core.series import Series\\n        arrays = Series(data, index=columns, dtype=object)\\n        data_names = arrays.index\\n        missing = arrays.isnull()\\n        if index is None:\\n            index = extract_index(arrays[~missing])\\n        else:\\n            index = ensure_index(index)\\n        if missing.any() and not is_integer_dtype(dtype):\\n            if dtype is None or np.issubdtype(dtype, np.flexible):\\n                nan_dtype = object\\n            else:\\n                nan_dtype = dtype\\n            v = construct_1d_arraylike_from_scalar(np.nan, len(index),\\n                                                   nan_dtype)\\n            arrays.loc[missing] = [v] * missing.sum()\\n    else:\\n        for key in data:\\n            if (isinstance(data[key], ABCDatetimeIndex) and\\n                    data[key].tz is not None):\\n                data[key] = data[key].copy(deep=True)\\n        keys = com.dict_keys_to_ordered_list(data)\\n        columns = data_names = Index(keys)\\n        arrays = [data[k] for k in keys]\\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)"
  },
  {
    "code": "def searchsorted(self, value, side=\"left\", sorter=None):\\n        if isinstance(value, (np.ndarray, Index)):\\n            if not type(self._data)._is_recognized_dtype(value):\\n                raise TypeError(\\n                    \"searchsorted requires compatible dtype or scalar, \"\\n                    f\"not {type(value).__name__}\"\\n                )\\n            value = type(self._data)(value)\\n            self._data._check_compatible_with(value)\\n        elif isinstance(value, self._data._recognized_scalars):\\n            self._data._check_compatible_with(value)\\n            value = self._data._scalar_type(value)\\n        elif not isinstance(value, TimedeltaArray):\\n            raise TypeError(\\n                \"searchsorted requires compatible dtype or scalar, \"\\n                f\"not {type(value).__name__}\"\\n            )\\n        return self._data.searchsorted(value, side=side, sorter=sorter)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def searchsorted(self, value, side=\"left\", sorter=None):\\n        if isinstance(value, (np.ndarray, Index)):\\n            if not type(self._data)._is_recognized_dtype(value):\\n                raise TypeError(\\n                    \"searchsorted requires compatible dtype or scalar, \"\\n                    f\"not {type(value).__name__}\"\\n                )\\n            value = type(self._data)(value)\\n            self._data._check_compatible_with(value)\\n        elif isinstance(value, self._data._recognized_scalars):\\n            self._data._check_compatible_with(value)\\n            value = self._data._scalar_type(value)\\n        elif not isinstance(value, TimedeltaArray):\\n            raise TypeError(\\n                \"searchsorted requires compatible dtype or scalar, \"\\n                f\"not {type(value).__name__}\"\\n            )\\n        return self._data.searchsorted(value, side=side, sorter=sorter)"
  },
  {
    "code": "def unquote_to_bytes(string):\\n    if isinstance(string, str):\\n        string = string.encode('utf-8')\\n    res = string.split(b'%')\\n    res[0] = res[0]\\n    for i in range(1, len(res)):\\n        item = res[i]\\n        try:\\n            res[i] = bytes([int(item[:2], 16)]) + item[2:]\\n        except ValueError:\\n            res[i] = b'%' + item\\n    return b''.join(res)\\ndef unquote(string, encoding='utf-8', errors='replace'):\\n    if encoding is None: encoding = 'utf-8'\\n    if errors is None: errors = 'replace'\\n    pct_sequence = []\\n    res = string.split('%')\\n    for i in range(1, len(res)):\\n        item = res[i]\\n        try:\\n            if not item: raise ValueError\\n            pct_sequence.append(bytes.fromhex(item[:2]))\\n            rest = item[2:]\\n        except ValueError:\\n            rest = '%' + item\\n        if not rest:\\n            res[i] = ''\\n        else:\\n            res[i] = b''.join(pct_sequence).decode(encoding, errors) + rest\\n            pct_sequence = []\\n    if pct_sequence:\\n        assert not res[-1], \"string=%r, res=%r\" % (string, res)\\n        res[-1] = b''.join(pct_sequence).decode(encoding, errors)\\n    return ''.join(res)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 76719,81270-81272,83294,83319,84038-84039 via svnmerge from svn+ssh://pythondev@svn.python.org/python/branches/py3k\\n\\n................\\n  r76719 | antoine.pitrou | 2009-12-08 20:38:17 +0100 (mar., 08 d\u00e9c. 2009) | 9 lines\\n\\n  Merged revisions 76718 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r76718 | antoine.pitrou | 2009-12-08 20:35:12 +0100 (mar., 08 d\u00e9c. 2009) | 3 lines\\n\\n    Fix transient refleaks in test_urllib. Thanks to Florent Xicluna.\\n  ........\\n................\\n  r81270 | florent.xicluna | 2010-05-17 19:24:07 +0200 (lun., 17 mai 2010) | 9 lines\\n\\n  Merged revision 81259 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r81259 | florent.xicluna | 2010-05-17 12:39:07 +0200 (lun, 17 mai 2010) | 2 lines\\n\\n    Slight style cleanup.\\n  ........\\n................\\n  r81271 | florent.xicluna | 2010-05-17 19:33:07 +0200 (lun., 17 mai 2010) | 11 lines\\n\\n  Issue #1285086: Speed up urllib.parse functions: quote, quote_from_bytes, unquote, unquote_to_bytes.\\n\\n  Recorded merge of revisions 81265 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r81265 | florent.xicluna | 2010-05-17 15:35:09 +0200 (lun, 17 mai 2010) | 2 lines\\n\\n    Issue #1285086: Speed up urllib.quote and urllib.unquote for simple cases.\\n  ........\\n................\\n  r81272 | florent.xicluna | 2010-05-17 20:01:22 +0200 (lun., 17 mai 2010) | 2 lines\\n\\n  Inadvertently removed part of the comment in r81271.\\n................\\n  r83294 | senthil.kumaran | 2010-07-30 21:34:36 +0200 (ven., 30 juil. 2010) | 2 lines\\n\\n  Fix issue9301 - handle unquote({}) kind of case.\\n................\\n  r83319 | florent.xicluna | 2010-07-31 10:56:55 +0200 (sam., 31 juil. 2010) | 2 lines\\n\\n  Fix an oversight in r83294.  unquote() should reject bytes.  Issue #9301.\\n................\\n  r84038 | florent.xicluna | 2010-08-14 20:30:35 +0200 (sam., 14 ao\u00fbt 2010) | 1 line\\n\\n  Silence the BytesWarning, due to patch r83294 for #9301\\n................\\n  r84039 | florent.xicluna | 2010-08-14 22:51:58 +0200 (sam., 14 ao\u00fbt 2010) | 1 line\\n\\n  Silence BytesWarning while testing exception\\n................",
    "fixed_code": "def unquote_to_bytes(string):\\n    if not string:\\n        string.split\\n        return b''\\n    if isinstance(string, str):\\n        string = string.encode('utf-8')\\n    res = string.split(b'%')\\n    if len(res) == 1:\\n        return string\\n    string = res[0]\\n    for item in res[1:]:\\n        try:\\n            string += bytes([int(item[:2], 16)]) + item[2:]\\n        except ValueError:\\n            string += b'%' + item\\n    return string\\ndef unquote(string, encoding='utf-8', errors='replace'):\\n    if string == '':\\n        return string\\n    res = string.split('%')\\n    if len(res) == 1:\\n        return string\\n    if encoding is None:\\n        encoding = 'utf-8'\\n    if errors is None:\\n        errors = 'replace'\\n    pct_sequence = b''\\n    string = res[0]\\n    for item in res[1:]:\\n        try:\\n            if not item:\\n                raise ValueError\\n            pct_sequence += bytes.fromhex(item[:2])\\n            rest = item[2:]\\n            if not rest:\\n                continue\\n        except ValueError:\\n            rest = '%' + item\\n        string += pct_sequence.decode(encoding, errors) + rest\\n        pct_sequence = b''\\n    if pct_sequence:\\n        string += pct_sequence.decode(encoding, errors)\\n    return string"
  },
  {
    "code": "def _test_google_api_imports():\\n    try:\\n        import httplib2  \\n        try:\\n            from googleapiclient.discovery import build  \\n            from googleapiclient.errors import HttpError  \\n        except:\\n            from apiclient.discovery import build  \\n            from apiclient.errors import HttpError  \\n        from oauth2client.client import AccessTokenRefreshError  \\n        from oauth2client.client import OAuth2WebServerFlow  \\n        from oauth2client.file import Storage  \\n        from oauth2client.tools import run_flow, argparser  \\n    except ImportError as e:\\n        raise ImportError(\"Missing module required for Google BigQuery \"\\n                          \"support: {0}\".format(str(e)))\\nlogger = logging.getLogger('pandas.io.gbq')\\nlogger.setLevel(logging.ERROR)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _test_google_api_imports():\\n    try:\\n        import httplib2  \\n        try:\\n            from googleapiclient.discovery import build  \\n            from googleapiclient.errors import HttpError  \\n        except:\\n            from apiclient.discovery import build  \\n            from apiclient.errors import HttpError  \\n        from oauth2client.client import AccessTokenRefreshError  \\n        from oauth2client.client import OAuth2WebServerFlow  \\n        from oauth2client.file import Storage  \\n        from oauth2client.tools import run_flow, argparser  \\n    except ImportError as e:\\n        raise ImportError(\"Missing module required for Google BigQuery \"\\n                          \"support: {0}\".format(str(e)))\\nlogger = logging.getLogger('pandas.io.gbq')\\nlogger.setLevel(logging.ERROR)"
  },
  {
    "code": "def make_column_transformer(*transformers, **kwargs):\\n    n_jobs = kwargs.pop('n_jobs', None)\\n    remainder = kwargs.pop('remainder', 'drop')\\n    sparse_threshold = kwargs.pop('sparse_threshold', 0.3)\\n    verbose = kwargs.pop('verbose', False)\\n    if kwargs:\\n        raise TypeError('Unknown keyword arguments: \"{}\"'\\n                        .format(list(kwargs.keys())[0]))\\n    transformer_list = _get_transformer_list(transformers)\\n    return ColumnTransformer(transformer_list, n_jobs=n_jobs,\\n                             remainder=remainder,\\n                             sparse_threshold=sparse_threshold,\\n                             verbose=verbose)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Replace *kwargs with named arguments in make_column_transformer (#17623)",
    "fixed_code": "def make_column_transformer(*transformers,\\n                            remainder='drop',\\n                            sparse_threshold=0.3,\\n                            n_jobs=None,\\n                            verbose=False):\\n    transformer_list = _get_transformer_list(transformers)\\n    return ColumnTransformer(transformer_list, n_jobs=n_jobs,\\n                             remainder=remainder,\\n                             sparse_threshold=sparse_threshold,\\n                             verbose=verbose)"
  },
  {
    "code": "def source_file_def_line(self):\\n        try:\\n            return inspect.getsourcelines(self.code_obj)[-1]\\n        except (OSError, TypeError):\\n            pass\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def source_file_def_line(self):\\n        try:\\n            return inspect.getsourcelines(self.code_obj)[-1]\\n        except (OSError, TypeError):\\n            pass\\n    @property"
  },
  {
    "code": "def __init__(self, default: Any = NOTSET, description: Optional[str] = None, **kwargs):\\n        self.value = default\\n        self.description = description\\n        self.schema = kwargs.pop('schema') if 'schema' in kwargs else kwargs",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Don't validate that Params are JSON when NOTSET (#22000)\\n\\nIf params are NOTSET, then JSON validation will of course fail.",
    "fixed_code": "def __init__(self, default: Any = NOTSET, description: Optional[str] = None, **kwargs):\\n        if default is not NOTSET:\\n            self._warn_if_not_json(default)\\n        self.value = default\\n        self.description = description\\n        self.schema = kwargs.pop('schema') if 'schema' in kwargs else kwargs"
  },
  {
    "code": "def read_certificate_fingerprint(module, openssl_bin, certificate_path):\\n    current_certificate_fingerprint_cmd = \"%s x509 -noout -in %s -fingerprint -sha1\" % (openssl_bin, certificate_path)\\n    (rc, current_certificate_fingerprint_out, current_certificate_fingerprint_err) = run_commands(module, current_certificate_fingerprint_cmd)\\n    if rc != 0:\\n        return module.fail_json(msg=current_certificate_fingerprint_out,\\n                                err=current_certificate_fingerprint_err,\\n                                rc=rc,\\n                                cmd=current_certificate_fingerprint_cmd)\\n    current_certificate_match = re.search(r\"=([\\w:]+)\", current_certificate_fingerprint_out)\\n    if not current_certificate_match:\\n        return module.fail_json(\\n            msg=\"Unable to find the current certificate fingerprint in %s\" % current_certificate_fingerprint_out,\\n            rc=rc,\\n            cmd=current_certificate_fingerprint_err\\n        )\\n    return current_certificate_match.group(1)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "java_keystore - Prefer SHA256 and solve SHA256 keytool in java11 version (#57302)",
    "fixed_code": "def read_certificate_fingerprint(module, openssl_bin, certificate_path):\\n    current_certificate_fingerprint_cmd = \"%s x509 -noout -in %s -fingerprint -sha256\" % (openssl_bin, certificate_path)\\n    (rc, current_certificate_fingerprint_out, current_certificate_fingerprint_err) = run_commands(module, current_certificate_fingerprint_cmd)\\n    if rc != 0:\\n        return module.fail_json(msg=current_certificate_fingerprint_out,\\n                                err=current_certificate_fingerprint_err,\\n                                rc=rc,\\n                                cmd=current_certificate_fingerprint_cmd)\\n    current_certificate_match = re.search(r\"=([\\w:]+)\", current_certificate_fingerprint_out)\\n    if not current_certificate_match:\\n        return module.fail_json(\\n            msg=\"Unable to find the current certificate fingerprint in %s\" % current_certificate_fingerprint_out,\\n            rc=rc,\\n            cmd=current_certificate_fingerprint_err\\n        )\\n    return current_certificate_match.group(1)"
  },
  {
    "code": "def __init__(self, values, columns, ref_columns):\\n        values = _convert_if_1d(values)\\n        if issubclass(values.dtype.type, basestring):\\n            values = np.array(values, dtype=object)\\n        self.values = values\\n        assert(len(columns) == values.shape[1])\\n        self.columns = _ensure_index(columns)\\n        self.ref_columns = _ensure_index(ref_columns)\\n    _ref_locs = None\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, values, columns, ref_columns):\\n        values = _convert_if_1d(values)\\n        if issubclass(values.dtype.type, basestring):\\n            values = np.array(values, dtype=object)\\n        self.values = values\\n        assert(len(columns) == values.shape[1])\\n        self.columns = _ensure_index(columns)\\n        self.ref_columns = _ensure_index(ref_columns)\\n    _ref_locs = None\\n    @property"
  },
  {
    "code": "def make_sparse(series, kind='block', sparse_value=np.NaN):\\nclass SparseSeries(Series):",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "broke something. sigh",
    "fixed_code": "def make_sparse(series, kind='block', sparse_value=np.NaN):\\n    assert(isinstance(series, Series))\\n    if kind == 'block':\\n        pass\\n    elif kind == 'integer':\\n        pass\\n    else:\\n        raise ValueError('must be block or integer type')\\nclass SparseSeries(Series):"
  },
  {
    "code": "def parse_table_schema(json, precise_float):\\n    table = loads(json, precise_float=precise_float)\\n    col_order = [field[\"name\"] for field in table[\"schema\"][\"fields\"]]\\n    df = DataFrame(table[\"data\"], columns=col_order)[col_order]\\n    dtypes = {\\n        field[\"name\"]: convert_json_field_to_pandas_type(field)\\n        for field in table[\"schema\"][\"fields\"]\\n    }\\n    if \"timedelta64\" in dtypes.values():\\n        raise NotImplementedError(\\n            'table=\"orient\" can not yet read ISO-formatted Timedelta data'\\n        )\\n    df = df.astype(dtypes)\\n    if \"primaryKey\" in table[\"schema\"]:\\n        df = df.set_index(table[\"schema\"][\"primaryKey\"])\\n        if len(df.index.names) == 1:\\n            if df.index.name == \"index\":\\n                df.index.name = None\\n        else:\\n            df.index.names = [\\n                None if x.startswith(\"level_\") else x for x in df.index.names\\n            ]\\n    return df",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def parse_table_schema(json, precise_float):\\n    table = loads(json, precise_float=precise_float)\\n    col_order = [field[\"name\"] for field in table[\"schema\"][\"fields\"]]\\n    df = DataFrame(table[\"data\"], columns=col_order)[col_order]\\n    dtypes = {\\n        field[\"name\"]: convert_json_field_to_pandas_type(field)\\n        for field in table[\"schema\"][\"fields\"]\\n    }\\n    if \"timedelta64\" in dtypes.values():\\n        raise NotImplementedError(\\n            'table=\"orient\" can not yet read ISO-formatted Timedelta data'\\n        )\\n    df = df.astype(dtypes)\\n    if \"primaryKey\" in table[\"schema\"]:\\n        df = df.set_index(table[\"schema\"][\"primaryKey\"])\\n        if len(df.index.names) == 1:\\n            if df.index.name == \"index\":\\n                df.index.name = None\\n        else:\\n            df.index.names = [\\n                None if x.startswith(\"level_\") else x for x in df.index.names\\n            ]\\n    return df"
  },
  {
    "code": "def run(self, args, opts):\\n        if opts.verbose:\\n            import lxml.etree\\n            lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\\n            libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\\n            print \"Scrapy  : %s\" % scrapy.__version__\\n            print \"lxml    : %s\" % lxml_version\\n            print \"libxml2 : %s\" % libxml2_version\\n            print \"Twisted : %s\" % twisted.version.short()\\n            print \"Python  : %s\" % sys.version.replace(\"\\n\", \"- \")\\n            print \"Platform: %s\" % platform.platform()\\n        else:\\n            print \"Scrapy %s\" % scrapy.__version__",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def run(self, args, opts):\\n        if opts.verbose:\\n            import lxml.etree\\n            lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\\n            libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\\n            print \"Scrapy  : %s\" % scrapy.__version__\\n            print \"lxml    : %s\" % lxml_version\\n            print \"libxml2 : %s\" % libxml2_version\\n            print \"Twisted : %s\" % twisted.version.short()\\n            print \"Python  : %s\" % sys.version.replace(\"\\n\", \"- \")\\n            print \"Platform: %s\" % platform.platform()\\n        else:\\n            print \"Scrapy %s\" % scrapy.__version__"
  },
  {
    "code": "def tsplot(series, plotf, **kwargs):\\n    if 'ax' in kwargs:\\n        ax = kwargs.pop('ax')\\n    else:\\n        import matplotlib.pyplot as plt\\n        ax = plt.gca()\\n    freq = _get_freq(ax, series)\\n    if freq is None:  \\n        raise ValueError('Cannot use dynamic axis without frequency info')\\n    else:\\n        if isinstance(series.index, DatetimeIndex):\\n            series = series.to_period(freq=freq)\\n        freq, ax_freq, series = _maybe_resample(series, ax, freq, plotf,\\n                                                kwargs)\\n    _decorate_axes(ax, freq, kwargs)\\n    args = _maybe_mask(series)\\n    if not hasattr(ax, '_plot_data'):\\n        ax._plot_data = []\\n    ax._plot_data.append((series, kwargs))\\n    style = kwargs.pop('style', None)\\n    if style is not None:\\n        args.append(style)\\n    lines = plotf(ax, *args, **kwargs)\\n    label = kwargs.get('label', None)\\n    format_dateaxis(ax, ax.freq)\\n    left, right = _get_xlim(ax.get_lines())\\n    ax.set_xlim(left, right)\\n    ax.format_coord = lambda t, y: \"t = {}  y = {:8f}\".format(Period(ordinal=int(t), freq=ax.freq), y)\\n    return lines",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "COMPAT: 2.6 format spec compat\\n\\nBUG: TestCase instances cant assertRaisesRegexp so use tm",
    "fixed_code": "def tsplot(series, plotf, **kwargs):\\n    if 'ax' in kwargs:\\n        ax = kwargs.pop('ax')\\n    else:\\n        import matplotlib.pyplot as plt\\n        ax = plt.gca()\\n    freq = _get_freq(ax, series)\\n    if freq is None:  \\n        raise ValueError('Cannot use dynamic axis without frequency info')\\n    else:\\n        if isinstance(series.index, DatetimeIndex):\\n            series = series.to_period(freq=freq)\\n        freq, ax_freq, series = _maybe_resample(series, ax, freq, plotf,\\n                                                kwargs)\\n    _decorate_axes(ax, freq, kwargs)\\n    args = _maybe_mask(series)\\n    if not hasattr(ax, '_plot_data'):\\n        ax._plot_data = []\\n    ax._plot_data.append((series, kwargs))\\n    style = kwargs.pop('style', None)\\n    if style is not None:\\n        args.append(style)\\n    lines = plotf(ax, *args, **kwargs)\\n    format_dateaxis(ax, ax.freq)\\n    left, right = _get_xlim(ax.get_lines())\\n    ax.set_xlim(left, right)\\n    ax.format_coord = lambda t, y: (\"t = {0}  \"\\n                                    \"y = {1:8f}\".format(Period(ordinal=int(t),\\n                                                               freq=ax.freq),\\n                                                        y))\\n    return lines"
  },
  {
    "code": "def open(filename, mode='rb', encoding=None, errors='strict', buffering=1):\\n    if encoding is not None:\\n        if 'U' in mode:\\n' is done on reading and writing\\n            mode = mode.strip().replace('U', '')\\n            if mode[:1] not in set('rwa'):\\n                mode = 'r' + mode\\n        if 'b' not in mode:\\n            mode = mode + 'b'\\n    file = __builtin__.open(filename, mode, buffering)\\n    if encoding is None:\\n        return file\\n    info = lookup(encoding)\\n    srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)\\n    srw.encoding = encoding\\n    return srw",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def open(filename, mode='rb', encoding=None, errors='strict', buffering=1):\\n    if encoding is not None:\\n        if 'U' in mode:\\n' is done on reading and writing\\n            mode = mode.strip().replace('U', '')\\n            if mode[:1] not in set('rwa'):\\n                mode = 'r' + mode\\n        if 'b' not in mode:\\n            mode = mode + 'b'\\n    file = __builtin__.open(filename, mode, buffering)\\n    if encoding is None:\\n        return file\\n    info = lookup(encoding)\\n    srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)\\n    srw.encoding = encoding\\n    return srw"
  },
  {
    "code": "def floatformat(text, arg=-1):\\n\\tforce_grouping = False\\n\\tuse_l10n = True\\n\\tif isinstance(arg, str):\\n\\t\\tlast_char = arg[-1]\\n\\t\\tif arg[-2:] in {\"gu\", \"ug\"}:\\n\\t\\t\\tforce_grouping = True\\n\\t\\t\\tuse_l10n = False\\n\\t\\t\\targ = arg[:-2] or -1\\n\\t\\telif last_char == \"g\":\\n\\t\\t\\tforce_grouping = True\\n\\t\\t\\targ = arg[:-1] or -1\\n\\t\\telif last_char == \"u\":\\n\\t\\t\\tuse_l10n = False\\n\\t\\t\\targ = arg[:-1] or -1\\n\\ttry:\\n\\t\\tinput_val = str(text)\\n\\t\\td = Decimal(input_val)\\n\\texcept InvalidOperation:\\n\\t\\ttry:\\n\\t\\t\\td = Decimal(str(float(text)))\\n\\t\\texcept (ValueError, InvalidOperation, TypeError):\\n\\t\\t\\treturn \"\"\\n\\ttry:\\n\\t\\tp = int(arg)\\n\\texcept ValueError:\\n\\t\\treturn input_val\\n\\ttry:\\n\\t\\tm = int(d) - d\\n\\texcept (ValueError, OverflowError, InvalidOperation):\\n\\t\\treturn input_val\\n\\tif not m and p < 0:\\n\\t\\treturn mark_safe(\\n\\t\\t\\tformats.number_format(\\n\\t\\t\\t\\t\"%d\" % (int(d)),\\n\\t\\t\\t\\t0,\\n\\t\\t\\t\\tuse_l10n=use_l10n,\\n\\t\\t\\t\\tforce_grouping=force_grouping,\\n\\t\\t\\t)\\n\\t\\t)\\n\\texp = Decimal(1).scaleb(-abs(p))\\n\\ttupl = d.as_tuple()\\n\\tunits = len(tupl[1])\\n\\tunits += -tupl[2] if m else tupl[2]\\n\\tprec = abs(p) + units + 1\\n\\trounded_d = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec))\\n\\tsign, digits, exponent = rounded_d.as_tuple()\\n\\tdigits = [str(digit) for digit in reversed(digits)]\\n\\twhile len(digits) <= abs(exponent):\\n\\t\\tdigits.append(\"0\")\\n\\tdigits.insert(-exponent, \".\")\\n\\tif sign and rounded_d:\\n\\t\\tdigits.append(\"-\")\\n\\tnumber = \"\".join(reversed(digits))\\n\\treturn mark_safe(\\n\\t\\tformats.number_format(\\n\\t\\t\\tnumber,\\n\\t\\t\\tabs(p),\\n\\t\\t\\tuse_l10n=use_l10n,\\n\\t\\t\\tforce_grouping=force_grouping,\\n\\t\\t)\\n\\t)",
    "label": 1,
    "bug_type": "security",
    "bug_description": "Fixed #34272 -- Fixed floatformat crash on zero with trailing zeros to zero decimal places.\\n\\nRegression in 08c5a787262c1ae57f6517d4574b54a5fcaad124.\\n\\nThanks Andrii Lahuta for the report.",
    "fixed_code": "def floatformat(text, arg=-1):\\n\\tforce_grouping = False\\n\\tuse_l10n = True\\n\\tif isinstance(arg, str):\\n\\t\\tlast_char = arg[-1]\\n\\t\\tif arg[-2:] in {\"gu\", \"ug\"}:\\n\\t\\t\\tforce_grouping = True\\n\\t\\t\\tuse_l10n = False\\n\\t\\t\\targ = arg[:-2] or -1\\n\\t\\telif last_char == \"g\":\\n\\t\\t\\tforce_grouping = True\\n\\t\\t\\targ = arg[:-1] or -1\\n\\t\\telif last_char == \"u\":\\n\\t\\t\\tuse_l10n = False\\n\\t\\t\\targ = arg[:-1] or -1\\n\\ttry:\\n\\t\\tinput_val = str(text)\\n\\t\\td = Decimal(input_val)\\n\\texcept InvalidOperation:\\n\\t\\ttry:\\n\\t\\t\\td = Decimal(str(float(text)))\\n\\t\\texcept (ValueError, InvalidOperation, TypeError):\\n\\t\\t\\treturn \"\"\\n\\ttry:\\n\\t\\tp = int(arg)\\n\\texcept ValueError:\\n\\t\\treturn input_val\\n\\ttry:\\n\\t\\tm = int(d) - d\\n\\texcept (ValueError, OverflowError, InvalidOperation):\\n\\t\\treturn input_val\\n\\tif not m and p <= 0:\\n\\t\\treturn mark_safe(\\n\\t\\t\\tformats.number_format(\\n\\t\\t\\t\\t\"%d\" % (int(d)),\\n\\t\\t\\t\\t0,\\n\\t\\t\\t\\tuse_l10n=use_l10n,\\n\\t\\t\\t\\tforce_grouping=force_grouping,\\n\\t\\t\\t)\\n\\t\\t)\\n\\texp = Decimal(1).scaleb(-abs(p))\\n\\ttupl = d.as_tuple()\\n\\tunits = len(tupl[1])\\n\\tunits += -tupl[2] if m else tupl[2]\\n\\tprec = abs(p) + units + 1\\n\\trounded_d = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec))\\n\\tsign, digits, exponent = rounded_d.as_tuple()\\n\\tdigits = [str(digit) for digit in reversed(digits)]\\n\\twhile len(digits) <= abs(exponent):\\n\\t\\tdigits.append(\"0\")\\n\\tdigits.insert(-exponent, \".\")\\n\\tif sign and rounded_d:\\n\\t\\tdigits.append(\"-\")\\n\\tnumber = \"\".join(reversed(digits))\\n\\treturn mark_safe(\\n\\t\\tformats.number_format(\\n\\t\\t\\tnumber,\\n\\t\\t\\tabs(p),\\n\\t\\t\\tuse_l10n=use_l10n,\\n\\t\\t\\tforce_grouping=force_grouping,\\n\\t\\t)\\n\\t)"
  },
  {
    "code": "def _nanmax(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    dtype = values.dtype\\n    if skipna and not issubclass(dtype.type, (np.integer, np.datetime64)):\\n        values = values.copy()\\n        np.putmask(values, mask, -np.inf)\\n    if issubclass(dtype.type, np.datetime64):\\n        values = values.view(np.int64)\\n    if (values.dtype == np.object_\\n        and sys.version_info[0] >= 3):  \\n        import __builtin__\\n        if values.ndim > 1:\\n            apply_ax = axis if axis is not None else 0\\n            result = np.apply_along_axis(__builtin__.max, apply_ax, values)\\n        else:\\n            result = __builtin__.max(values)\\n    else:\\n        if ((axis is not None and values.shape[axis] == 0)\\n             or values.size == 0):\\n            result = values.sum(axis)\\n            result.fill(np.nan)\\n        else:\\n            result = values.max(axis)\\n    if issubclass(dtype.type, np.datetime64):\\n        if not isinstance(result, np.ndarray):\\n            result = lib.Timestamp(result)\\n        else:\\n            result = result.view(dtype)\\n    return _maybe_null_out(result, axis, mask)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: nanmin/nanmax fails for empty int dtype #2610",
    "fixed_code": "def _nanmax(values, axis=None, skipna=True):\\n    mask = isnull(values)\\n    dtype = values.dtype\\n    if skipna and not issubclass(dtype.type, (np.integer, np.datetime64)):\\n        values = values.copy()\\n        np.putmask(values, mask, -np.inf)\\n    if issubclass(dtype.type, np.datetime64):\\n        values = values.view(np.int64)\\n    if (values.dtype == np.object_\\n        and sys.version_info[0] >= 3):  \\n        import __builtin__\\n        if values.ndim > 1:\\n            apply_ax = axis if axis is not None else 0\\n            result = np.apply_along_axis(__builtin__.max, apply_ax, values)\\n        else:\\n            result = __builtin__.max(values)\\n    else:\\n        if ((axis is not None and values.shape[axis] == 0)\\n             or values.size == 0):\\n            result = com.ensure_float(values.sum(axis))\\n            result.fill(np.nan)\\n        else:\\n            result = values.max(axis)\\n    if issubclass(dtype.type, np.datetime64):\\n        if not isinstance(result, np.ndarray):\\n            result = lib.Timestamp(result)\\n        else:\\n            result = result.view(dtype)\\n    return _maybe_null_out(result, axis, mask)"
  },
  {
    "code": "def get_all_dependencies(self):\\n        child_deps  = []\\n        direct_deps = self.get_direct_dependencies()\\n        for dep in direct_deps:\\n            dep_deps = dep.get_all_dependencies()\\n            for dep_dep in dep_deps:\\n                if dep_dep not in child_deps:\\n                    child_deps.append(dep_dep)\\n        return direct_deps + child_deps",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_all_dependencies(self):\\n        child_deps  = []\\n        direct_deps = self.get_direct_dependencies()\\n        for dep in direct_deps:\\n            dep_deps = dep.get_all_dependencies()\\n            for dep_dep in dep_deps:\\n                if dep_dep not in child_deps:\\n                    child_deps.append(dep_dep)\\n        return direct_deps + child_deps"
  },
  {
    "code": "def _ewma(v):\\n        result = tseries.ewma(v, com)\\n        first_index = _first_valid_index(v)\\n        result[first_index : first_index + min_periods] = NaN\\n        return result\\n    return_hook, values = _process_data_structure(arg)\\n    output = np.apply_along_axis(_ewma, 0, values)\\n    return return_hook(output)\\newma.__doc__ = _ewm_doc % (\"Moving exponentially-weighted moving average\",\\n                           _unary_arg, \"\")",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _ewma(v):\\n        result = tseries.ewma(v, com)\\n        first_index = _first_valid_index(v)\\n        result[first_index : first_index + min_periods] = NaN\\n        return result\\n    return_hook, values = _process_data_structure(arg)\\n    output = np.apply_along_axis(_ewma, 0, values)\\n    return return_hook(output)\\newma.__doc__ = _ewm_doc % (\"Moving exponentially-weighted moving average\",\\n                           _unary_arg, \"\")"
  },
  {
    "code": "def timesince(d, now=None, reversed=False, time_strings=None, depth=2):\\n    if time_strings is None:\\n        time_strings = TIME_STRINGS\\n    if depth <= 0:\\n        raise ValueError('depth must be greater than 0.')\\n    if not isinstance(d, datetime.datetime):\\n        d = datetime.datetime(d.year, d.month, d.day)\\n    if now and not isinstance(now, datetime.datetime):\\n        now = datetime.datetime(now.year, now.month, now.day)\\n    now = now or datetime.datetime.now(utc if is_aware(d) else None)\\n    if reversed:\\n        d, now = now, d\\n    delta = now - d\\n    leapdays = calendar.leapdays(d.year, now.year)\\n    if leapdays != 0:\\n        if calendar.isleap(d.year):\\n            leapdays -= 1\\n        elif calendar.isleap(now.year):\\n            leapdays += 1\\n    delta -= datetime.timedelta(leapdays)\\n    since = delta.days * 24 * 60 * 60 + delta.seconds\\n    if since <= 0:\\n        return avoid_wrapping(time_strings['minute'] % 0)\\n    for i, (seconds, name) in enumerate(TIMESINCE_CHUNKS):\\n        count = since // seconds\\n        if count != 0:\\n            break\\n    else:\\n        return avoid_wrapping(time_strings['minute'] % 0)\\n    result = []\\n    current_depth = 0\\n    while i < len(TIMESINCE_CHUNKS) and current_depth < depth:\\n        seconds, name = TIMESINCE_CHUNKS[i]\\n        count = since // seconds\\n        if count == 0:\\n            break\\n        result.append(avoid_wrapping(time_strings[name] % count))\\n        since -= seconds * count\\n        current_depth += 1\\n        i += 1\\n    return gettext(', ').join(result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #31937 -- Eased translation of time strings in some languages.",
    "fixed_code": "def timesince(d, now=None, reversed=False, time_strings=None, depth=2):\\n    if time_strings is None:\\n        time_strings = TIME_STRINGS\\n    if depth <= 0:\\n        raise ValueError('depth must be greater than 0.')\\n    if not isinstance(d, datetime.datetime):\\n        d = datetime.datetime(d.year, d.month, d.day)\\n    if now and not isinstance(now, datetime.datetime):\\n        now = datetime.datetime(now.year, now.month, now.day)\\n    now = now or datetime.datetime.now(utc if is_aware(d) else None)\\n    if reversed:\\n        d, now = now, d\\n    delta = now - d\\n    leapdays = calendar.leapdays(d.year, now.year)\\n    if leapdays != 0:\\n        if calendar.isleap(d.year):\\n            leapdays -= 1\\n        elif calendar.isleap(now.year):\\n            leapdays += 1\\n    delta -= datetime.timedelta(leapdays)\\n    since = delta.days * 24 * 60 * 60 + delta.seconds\\n    if since <= 0:\\n        return avoid_wrapping(time_strings['minute'] % {'num': 0})\\n    for i, (seconds, name) in enumerate(TIMESINCE_CHUNKS):\\n        count = since // seconds\\n        if count != 0:\\n            break\\n    else:\\n        return avoid_wrapping(time_strings['minute'] % {'num': 0})\\n    result = []\\n    current_depth = 0\\n    while i < len(TIMESINCE_CHUNKS) and current_depth < depth:\\n        seconds, name = TIMESINCE_CHUNKS[i]\\n        count = since // seconds\\n        if count == 0:\\n            break\\n        result.append(avoid_wrapping(time_strings[name] % {'num': count}))\\n        since -= seconds * count\\n        current_depth += 1\\n        i += 1\\n    return gettext(', ').join(result)"
  },
  {
    "code": "def clip(self, lower=None, upper=None):\\n        return self.apply(lambda x: x.clip(lower=lower, upper=upper))\\n    def clip_upper(self, threshold):\\n        return self.apply(lambda x: x.clip_upper(threshold))\\n    def clip_lower(self, threshold):\\n        return self.apply(lambda x: x.clip_lower(threshold))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: provide backwards compatibility to DataFrame.clip if arguments are reversed",
    "fixed_code": "def clip(self, lower=None, upper=None):\\n        if lower is not None and upper is not None:\\n            lower, upper = min(lower,upper), max(lower,upper)\\n        return self.apply(lambda x: x.clip(lower=lower, upper=upper))\\n    def clip_upper(self, threshold):\\n        return self.apply(lambda x: x.clip_upper(threshold))\\n    def clip_lower(self, threshold):\\n        return self.apply(lambda x: x.clip_lower(threshold))"
  },
  {
    "code": "def info(self, to_stdout=True):\\n        if len(self._series) == 0:\\n            output = 'DataFrame is empty!\\n'\\n            output += self.index.__repr__()\\n            return output\\n        output = 'Index: %s entries, %s to %s\\n' % (len(self.index),\\n                                                    min(self.index),\\n                                                    max(self.index))\\n        output += 'Columns:\\n'\\n        series = self._series\\n        skeys = sorted(self.cols())\\n        space = max([len(str(k)) for k in skeys]) + 4\\n        for k in skeys:\\n            out = _pfixed(k, space)\\n            N = notnull(series[k]).sum()\\n            out += '%d  non-null values\\n' % N\\n            output += out\\n        if to_stdout:\\n            print output\\n        else:\\n            return output",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "not weighting untransformed data in PanelOLS. changed DataFrame and DataMatrix.toString to take a buffer instead of a to_stdout flag",
    "fixed_code": "def info(self, buffer=sys.stdout):\\n        if len(self._series) == 0:\\n            print >> buffer, 'DataFrame is empty!'\\n            print >> buffer, repr(self.index)\\n        print >> buffer, 'Index: %s entries, %s to %s' % (len(self.index),\\n                                                          min(self.index),\\n                                                          max(self.index))\\n        print >> buffer, 'Data columns:'\\n        series = self._series\\n        columns = sorted(self.cols())\\n        space = max([len(str(k)) for k in columns]) + 4\\n        for k in columns:\\n            out = _pfixed(k, space)\\n            N = notnull(series[k]).sum()\\n            out += '%d  non-null values' % N\\n            print >> buffer, out"
  },
  {
    "code": "def __init__(self):\\n        self.float_format = None\\n        self.column_space = 12\\n        self.precision = 4\\n        self.max_rows = 500\\n        self.max_columns = 0\\n        self.column_justify = 'right'\\nGlobalPrintConfig = _GlobalPrintConfig()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: common.py cleanup, test coverage",
    "fixed_code": "def __init__(self, precision=None, use_eng_prefix=False):\\n        self.precision = precision\\n        self.use_eng_prefix = use_eng_prefix"
  },
  {
    "code": "def discover_device(fmgr, paramgram):\\n    datagram = {\\n        \"odd_request_form\": \"True\",\\n        \"device\": {\"adm_usr\": paramgram[\"device_username\"],\\n                   \"adm_pass\": paramgram[\"device_password\"],\\n                   \"ip\": paramgram[\"device_ip\"]}\\n    }\\n    url = '/dvm/cmd/discover/device/'\\n    response = fmgr.process_request(url, datagram, FMGRMethods.EXEC)\\n    return response",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def discover_device(fmgr, paramgram):\\n    datagram = {\\n        \"odd_request_form\": \"True\",\\n        \"device\": {\"adm_usr\": paramgram[\"device_username\"],\\n                   \"adm_pass\": paramgram[\"device_password\"],\\n                   \"ip\": paramgram[\"device_ip\"]}\\n    }\\n    url = '/dvm/cmd/discover/device/'\\n    response = fmgr.process_request(url, datagram, FMGRMethods.EXEC)\\n    return response"
  },
  {
    "code": "def __or__(self, otherseq, *rest):\\n' % (self, (otherseq,) + rest))\\n        if type(otherseq) != type(self):\\n            otherseq = bitvec(otherseq, *rest)\\n        return BitVec(self._data | otherseq._data, \\\\n                  max(self._len, otherseq._len))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __or__(self, otherseq, *rest):\\n' % (self, (otherseq,) + rest))\\n        if type(otherseq) != type(self):\\n            otherseq = bitvec(otherseq, *rest)\\n        return BitVec(self._data | otherseq._data, \\\\n                  max(self._len, otherseq._len))"
  },
  {
    "code": "def __init__(self):\\n    try:\\n      self.spinner_proc = subprocess.Popen([\"./spinner\"],\\n                                           stdin=subprocess.PIPE,\\n                                           cwd=os.path.join(BASEDIR, \"selfdrive\", \"ui\"),\\n                                           close_fds=True)\\n    except OSError:\\n      self.spinner_proc = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self):\\n    try:\\n      self.spinner_proc = subprocess.Popen([\"./spinner\"],\\n                                           stdin=subprocess.PIPE,\\n                                           cwd=os.path.join(BASEDIR, \"selfdrive\", \"ui\"),\\n                                           close_fds=True)\\n    except OSError:\\n      self.spinner_proc = None"
  },
  {
    "code": "def staged_predict(self, X, n_estimators=-1):\\n        if n_estimators == 0:\\n            raise ValueError(\"``n_estimators`` must not equal zero\")\\n        if not self.estimators_:\\n            raise RuntimeError(\\n                (\"{0} is not initialized. \"\\n                 \"Perform a fit first\").format(self.__class__.__name__))\\n        n_classes = self.n_classes_\\n        classes = self.classes_\\n        pred = None\\n        for i, (weight, estimator) in enumerate(\\n                zip(self.weights_, self.estimators_)):\\n            if i == n_estimators:\\n                break\\n            if self.real:\\n                current_pred = estimator.predict_proba(X) + 1e-200\\n                current_pred = (n_classes - 1) * (\\n                    np.log(current_pred) -\\n                    (1. / n_classes) *\\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\\n            else:\\n                current_pred = estimator.predict(X)\\n                current_pred = (\\n                    current_pred == classes[:, np.newaxis]).T * weight\\n            if pred is None:\\n                pred = current_pred\\n            else:\\n                pred += current_pred\\n            yield np.array(classes.take(\\n                np.argmax(pred, axis=1), axis=0))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def staged_predict(self, X, n_estimators=-1):\\n        if n_estimators == 0:\\n            raise ValueError(\"``n_estimators`` must not equal zero\")\\n        if not self.estimators_:\\n            raise RuntimeError(\\n                (\"{0} is not initialized. \"\\n                 \"Perform a fit first\").format(self.__class__.__name__))\\n        n_classes = self.n_classes_\\n        classes = self.classes_\\n        pred = None\\n        for i, (weight, estimator) in enumerate(\\n                zip(self.weights_, self.estimators_)):\\n            if i == n_estimators:\\n                break\\n            if self.real:\\n                current_pred = estimator.predict_proba(X) + 1e-200\\n                current_pred = (n_classes - 1) * (\\n                    np.log(current_pred) -\\n                    (1. / n_classes) *\\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\\n            else:\\n                current_pred = estimator.predict(X)\\n                current_pred = (\\n                    current_pred == classes[:, np.newaxis]).T * weight\\n            if pred is None:\\n                pred = current_pred\\n            else:\\n                pred += current_pred\\n            yield np.array(classes.take(\\n                np.argmax(pred, axis=1), axis=0))"
  },
  {
    "code": "def _fit_binary(self, X, y, sample_weight, n_iter):\\n        if sp.issparse(X):\\n            X = _tocsr(X)\\n        coef, intercept = fit_binary(self, 1, X, y, n_iter,\\n                                     self._expanded_class_weight[1],\\n                                     self._expanded_class_weight[0],\\n                                     sample_weight)\\n        self.coef_ = coef.reshape(1, -1)\\n        self.intercept_ = np.atleast_1d(intercept)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _fit_binary(self, X, y, sample_weight, n_iter):\\n        if sp.issparse(X):\\n            X = _tocsr(X)\\n        coef, intercept = fit_binary(self, 1, X, y, n_iter,\\n                                     self._expanded_class_weight[1],\\n                                     self._expanded_class_weight[0],\\n                                     sample_weight)\\n        self.coef_ = coef.reshape(1, -1)\\n        self.intercept_ = np.atleast_1d(intercept)"
  },
  {
    "code": "def contribute_to_class(self, cls, name):\\n        self.set_attributes_from_name(name)\\n        self.model = cls\\n        cls._meta.add_field(self)\\n        if self.choices:\\n            setattr(cls, 'get_%s_display' % self.name, curry(cls._get_FIELD_display, field=self))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def contribute_to_class(self, cls, name):\\n        self.set_attributes_from_name(name)\\n        self.model = cls\\n        cls._meta.add_field(self)\\n        if self.choices:\\n            setattr(cls, 'get_%s_display' % self.name, curry(cls._get_FIELD_display, field=self))"
  },
  {
    "code": "def __init__(self, obj, func, broadcast, raw, reduce, result_type,\\n\\t\\t\\t\\t ignore_failures, args, kwds):\\n\\t\\tself.obj = obj\\n\\t\\tself.raw = raw\\n\\t\\tself.ignore_failures = ignore_failures\\n\\t\\tself.args = args or ()\\n\\t\\tself.kwds = kwds or {}\\n\\t\\tif result_type not in [None, 'reduce', 'broadcast', 'expand']:\\n\\t\\t\\traise ValueError(\"invalid value for result_type, must be one \"\\n\\t\\t\\t\\t\\t\\t\\t \"of {None, 'reduce', 'broadcast', 'expand'}\")\\n\\t\\tif broadcast is not None:\\n\\t\\t\\twarnings.warn(\"The broadcast argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='broadcast' to broadcast the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif broadcast:\\n\\t\\t\\t\\tresult_type = 'broadcast'\\n\\t\\tif reduce is not None:\\n\\t\\t\\twarnings.warn(\"The reduce argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='reduce' to try to reduce the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif reduce:\\n\\t\\t\\t\\tif result_type is not None:\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\t\"cannot pass both reduce=True and result_type\")\\n\\t\\t\\t\\tresult_type = 'reduce'\\n\\t\\tself.result_type = result_type\\n\\t\\tif ((kwds or args) and\\n\\t\\t\\t\\tnot isinstance(func, (np.ufunc, compat.string_types))):\\n\\t\\t\\tdef f(x):\\n\\t\\t\\t\\treturn func(x, *args, **kwds)\\n\\t\\telse:\\n\\t\\t\\tf = func\\n\\t\\tself.f = f\\n\\t\\tself.result = None\\n\\t\\tself.res_index = None\\n\\t\\tself.res_columns = None",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, obj, func, broadcast, raw, reduce, result_type,\\n\\t\\t\\t\\t ignore_failures, args, kwds):\\n\\t\\tself.obj = obj\\n\\t\\tself.raw = raw\\n\\t\\tself.ignore_failures = ignore_failures\\n\\t\\tself.args = args or ()\\n\\t\\tself.kwds = kwds or {}\\n\\t\\tif result_type not in [None, 'reduce', 'broadcast', 'expand']:\\n\\t\\t\\traise ValueError(\"invalid value for result_type, must be one \"\\n\\t\\t\\t\\t\\t\\t\\t \"of {None, 'reduce', 'broadcast', 'expand'}\")\\n\\t\\tif broadcast is not None:\\n\\t\\t\\twarnings.warn(\"The broadcast argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='broadcast' to broadcast the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif broadcast:\\n\\t\\t\\t\\tresult_type = 'broadcast'\\n\\t\\tif reduce is not None:\\n\\t\\t\\twarnings.warn(\"The reduce argument is deprecated and will \"\\n\\t\\t\\t\\t\\t\\t  \"be removed in a future version. You can specify \"\\n\\t\\t\\t\\t\\t\\t  \"result_type='reduce' to try to reduce the result \"\\n\\t\\t\\t\\t\\t\\t  \"to the original dimensions\",\\n\\t\\t\\t\\t\\t\\t  FutureWarning, stacklevel=4)\\n\\t\\t\\tif reduce:\\n\\t\\t\\t\\tif result_type is not None:\\n\\t\\t\\t\\t\\traise ValueError(\\n\\t\\t\\t\\t\\t\\t\"cannot pass both reduce=True and result_type\")\\n\\t\\t\\t\\tresult_type = 'reduce'\\n\\t\\tself.result_type = result_type\\n\\t\\tif ((kwds or args) and\\n\\t\\t\\t\\tnot isinstance(func, (np.ufunc, compat.string_types))):\\n\\t\\t\\tdef f(x):\\n\\t\\t\\t\\treturn func(x, *args, **kwds)\\n\\t\\telse:\\n\\t\\t\\tf = func\\n\\t\\tself.f = f\\n\\t\\tself.result = None\\n\\t\\tself.res_index = None\\n\\t\\tself.res_columns = None"
  },
  {
    "code": "def parse_time_string(arg):\\n    if not isinstance(arg, basestring):\\n        return arg\\n    try:\\n        parsed = _dtparser._parse(arg)\\n        default = datetime(1,1,1).replace(hour=0, minute=0,\\n                                          second=0, microsecond=0)\\n        if parsed is None:\\n            raise DateParseError(\"Could not parse %s\" % arg)\\n        repl = {}\\n        reso = 'year'\\n        stopped = False\\n        for attr in [\"year\", \"month\", \"day\", \"hour\",\\n                     \"minute\", \"second\", \"microsecond\"]:\\n            value = getattr(parsed, attr)\\n            if value is not None:\\n                repl[attr] = value\\n                if not stopped:\\n                    reso = attr\\n                else:\\n                    raise DateParseError(\"Missing attribute before %s\" % attr)\\n            else:\\n                stopped = True\\n        ret = default.replace(**repl)\\n        return ret, parsed, reso  \\n    except Exception, e:\\n        raise DateParseError(e)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix treatment of 'how' in asfreq having multiples",
    "fixed_code": "def parse_time_string(arg):\\n    if not isinstance(arg, basestring):\\n        return arg\\n    try:\\n        parsed = _dtparser._parse(arg)\\n        default = datetime(1,1,1).replace(hour=0, minute=0,\\n                                          second=0, microsecond=0)\\n        if parsed is None:\\n            raise DateParseError(\"Could not parse %s\" % arg)\\n        repl = {}\\n        reso = 'year'\\n        stopped = False\\n        for attr in [\"year\", \"month\", \"day\", \"hour\",\\n                     \"minute\", \"second\", \"microsecond\"]:\\n            value = getattr(parsed, attr)\\n            if value is not None and value != 0:\\n                repl[attr] = value\\n                if not stopped:\\n                    reso = attr\\n                else:\\n                    raise DateParseError(\"Missing attribute before %s\" % attr)\\n            else:\\n                stopped = True\\n        ret = default.replace(**repl)\\n        return ret, parsed, reso  \\n    except Exception, e:\\n        raise DateParseError(e)"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            subscription=dict(required=True),\\n            token=dict(required=True, no_log=True),\\n            room=dict(required=True),\\n            msg=dict(required=True),\\n            notify=dict(required=False,\\n                        choices=[\"56k\", \"bell\", \"bezos\", \"bueller\",\\n                                 \"clowntown\", \"cottoneyejoe\",\\n                                 \"crickets\", \"dadgummit\", \"dangerzone\",\\n                                 \"danielsan\", \"deeper\", \"drama\",\\n                                 \"greatjob\", \"greyjoy\", \"guarantee\",\\n                                 \"heygirl\", \"horn\", \"horror\",\\n                                 \"inconceivable\", \"live\", \"loggins\",\\n                                 \"makeitso\", \"noooo\", \"nyan\", \"ohmy\",\\n                                 \"ohyeah\", \"pushit\", \"rimshot\",\\n                                 \"rollout\", \"rumble\", \"sax\", \"secret\",\\n                                 \"sexyback\", \"story\", \"tada\", \"tmyk\",\\n                                 \"trololo\", \"trombone\", \"unix\",\\n                                 \"vuvuzela\", \"what\", \"whoomp\", \"yeah\",\\n                                 \"yodel\"]),\\n        ),\\n        supports_check_mode=False\\n    )\\n    subscription = module.params[\"subscription\"]\\n    token = module.params[\"token\"]\\n    room = module.params[\"room\"]\\n    msg = module.params[\"msg\"]\\n    notify = module.params[\"notify\"]\\n    URI = \"https://%s.campfirenow.com\" % subscription\\n    NSTR = \"<message><type>SoundMessage</type><body>%s</body></message>\"\\n    MSTR = \"<message><body>%s</body></message>\"\\n    AGENT = \"Ansible/1.2\"\\n    module.params['url_username'] = token\\n    module.params['url_password'] = 'X'\\n    target_url = '%s/room/%s/speak.xml' % (URI, room)\\n    headers = {'Content-Type': 'application/xml',\\n               'User-agent': AGENT}\\n    if notify:\\n        response, info = fetch_url(module, target_url, data=NSTR % html_escape(notify), headers=headers)\\n        if info['status'] not in [200, 201]:\\n            module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                             \" returned error code: '%s'\" %\\n                                 (notify, info['status']))\\n    response, info = fetch_url(module, target_url, data=MSTR % html_escape(msg), headers=headers)\\n    if info['status'] not in [200, 201]:\\n        module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                         \" returned error code: '%s'\" %\\n                             (msg, info['status']))\\n    module.exit_json(changed=True, room=room, msg=msg, notify=notify)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            subscription=dict(required=True),\\n            token=dict(required=True, no_log=True),\\n            room=dict(required=True),\\n            msg=dict(required=True),\\n            notify=dict(required=False,\\n                        choices=[\"56k\", \"bell\", \"bezos\", \"bueller\",\\n                                 \"clowntown\", \"cottoneyejoe\",\\n                                 \"crickets\", \"dadgummit\", \"dangerzone\",\\n                                 \"danielsan\", \"deeper\", \"drama\",\\n                                 \"greatjob\", \"greyjoy\", \"guarantee\",\\n                                 \"heygirl\", \"horn\", \"horror\",\\n                                 \"inconceivable\", \"live\", \"loggins\",\\n                                 \"makeitso\", \"noooo\", \"nyan\", \"ohmy\",\\n                                 \"ohyeah\", \"pushit\", \"rimshot\",\\n                                 \"rollout\", \"rumble\", \"sax\", \"secret\",\\n                                 \"sexyback\", \"story\", \"tada\", \"tmyk\",\\n                                 \"trololo\", \"trombone\", \"unix\",\\n                                 \"vuvuzela\", \"what\", \"whoomp\", \"yeah\",\\n                                 \"yodel\"]),\\n        ),\\n        supports_check_mode=False\\n    )\\n    subscription = module.params[\"subscription\"]\\n    token = module.params[\"token\"]\\n    room = module.params[\"room\"]\\n    msg = module.params[\"msg\"]\\n    notify = module.params[\"notify\"]\\n    URI = \"https://%s.campfirenow.com\" % subscription\\n    NSTR = \"<message><type>SoundMessage</type><body>%s</body></message>\"\\n    MSTR = \"<message><body>%s</body></message>\"\\n    AGENT = \"Ansible/1.2\"\\n    module.params['url_username'] = token\\n    module.params['url_password'] = 'X'\\n    target_url = '%s/room/%s/speak.xml' % (URI, room)\\n    headers = {'Content-Type': 'application/xml',\\n               'User-agent': AGENT}\\n    if notify:\\n        response, info = fetch_url(module, target_url, data=NSTR % html_escape(notify), headers=headers)\\n        if info['status'] not in [200, 201]:\\n            module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                             \" returned error code: '%s'\" %\\n                                 (notify, info['status']))\\n    response, info = fetch_url(module, target_url, data=MSTR % html_escape(msg), headers=headers)\\n    if info['status'] not in [200, 201]:\\n        module.fail_json(msg=\"unable to send msg: '%s', campfire api\"\\n                         \" returned error code: '%s'\" %\\n                             (msg, info['status']))\\n    module.exit_json(changed=True, room=room, msg=msg, notify=notify)"
  },
  {
    "code": "def get(self, key, default=None):\\n        fname = self._key_to_file(key)\\n        try:\\n            f = open(fname, 'rb')\\n            exp = pickle.load(f)\\n            now = time.time()\\n            if exp < now:\\n                f.close()\\n                os.remove(fname)\\n            else:\\n                return pickle.load(f)\\n        except (IOError, OSError, EOFError, pickle.PickleError):\\n            pass\\n        return default",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #6099: the filebased cache backend now uses md5 hashes of keys instead of sanitized filenames. For good measure, keys are partitioned into subdirectories using the first few bits of the hash. Thanks, sherbang.",
    "fixed_code": "def get(self, key, default=None):\\n        fname = self._key_to_file(key)\\n        try:\\n            f = open(fname, 'rb')\\n            exp = pickle.load(f)\\n            now = time.time()\\n            if exp < now:\\n                f.close()\\n                self._delete(fname)\\n            else:\\n                return pickle.load(f)\\n        except (IOError, OSError, EOFError, pickle.PickleError):\\n            pass\\n        return default"
  },
  {
    "code": "def create_volume_clone(self):\\n        clone_obj = netapp_utils.zapi.NaElement('volume-clone-create')\\n        clone_obj.add_new_child(\"parent-volume\", self.parent_volume)\\n        clone_obj.add_new_child(\"volume\", self.volume)\\n        if self.qos_policy_group_name:\\n            clone_obj.add_new_child(\"qos-policy-group-name\", self.qos_policy_group_name)\\n        if self.space_reserve:\\n            clone_obj.add_new_child(\"space-reserve\", self.space_reserve)\\n        if self.parent_snapshot:\\n            clone_obj.add_new_child(\"parent-snapshot\", self.parent_snapshot)\\n        if self.parent_vserver:\\n            clone_obj.add_new_child(\"parent-vserver\", self.parent_vserver)\\n        if self.volume_type:\\n            clone_obj.add_new_child(\"volume-type\", self.volume_type)\\n        if self.junction_path:\\n            clone_obj.add_new_child(\"junction-path\", self.junction_path)\\n        self.server.invoke_successfully(clone_obj, True)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Add uid, and gid to volume clone (#57371)",
    "fixed_code": "def create_volume_clone(self):\\n        clone_obj = netapp_utils.zapi.NaElement('volume-clone-create')\\n        clone_obj.add_new_child(\"parent-volume\", self.parameters['parent_volume'])\\n        clone_obj.add_new_child(\"volume\", self.parameters['volume'])\\n        if self.parameters.get('qos_policy_group_name'):\\n            clone_obj.add_new_child(\"qos-policy-group-name\", self.parameters['qos_policy_group_name'])\\n        if self.parameters.get('space_reserve'):\\n            clone_obj.add_new_child(\"space-reserve\", self.parameters['space_reserve'])\\n        if self.parameters.get('parent_snapshot'):\\n            clone_obj.add_new_child(\"parent-snapshot\", self.parameters['parent_snapshot'])\\n        if self.parameters.get('parent_vserver'):\\n            clone_obj.add_new_child(\"parent-vserver\", self.parameters['parent_vserver'])\\n        if self.parameters.get('volume_type'):\\n            clone_obj.add_new_child(\"volume-type\", self.parameters['volume_type'])\\n        if self.parameters.get('junction_path'):\\n            clone_obj.add_new_child(\"junction-path\", self.parameters['junction_path'])\\n        if self.parameters.get('uid'):\\n            clone_obj.add_new_child(\"uid\", str(self.parameters['uid']))\\n            clone_obj.add_new_child(\"gid\", str(self.parameters['gid']))\\n        try:\\n            self.server.invoke_successfully(clone_obj, True)\\n        except netapp_utils.zapi.NaApiError as exc:\\n            self.module.fail_json(msg='Error creating volume clone: %s: %s' %\\n                                      (self.parameters['volume'], to_native(exc)), exception=traceback.format_exc())"
  },
  {
    "code": "def searchsorted(self, value, side=\"left\", sorter=None):\\n\\t\\tif isinstance(value, Period) or value is NaT:\\n\\t\\t\\tself._data._check_compatible_with(value)\\n\\t\\telif isinstance(value, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tvalue = Period(value, freq=self.freq)\\n\\t\\t\\texcept DateParseError:\\n\\t\\t\\t\\traise KeyError(f\"Cannot interpret '{value}' as period\")\\n\\t\\telif not isinstance(value, PeriodArray):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\\n\\t\\t\\t)\\n\\t\\treturn self._data.searchsorted(value, side=side, sorter=sorter)\\n\\t@property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def searchsorted(self, value, side=\"left\", sorter=None):\\n\\t\\tif isinstance(value, Period) or value is NaT:\\n\\t\\t\\tself._data._check_compatible_with(value)\\n\\t\\telif isinstance(value, str):\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tvalue = Period(value, freq=self.freq)\\n\\t\\t\\texcept DateParseError:\\n\\t\\t\\t\\traise KeyError(f\"Cannot interpret '{value}' as period\")\\n\\t\\telif not isinstance(value, PeriodArray):\\n\\t\\t\\traise TypeError(\\n\\t\\t\\t\\t\"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\\n\\t\\t\\t)\\n\\t\\treturn self._data.searchsorted(value, side=side, sorter=sorter)\\n\\t@property"
  },
  {
    "code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    from pandas.tseries.period import PeriodIndex\\n    values = Series(values).values\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    if com.is_categorical_dtype(values.dtype):\\n        result = values.value_counts(dropna)\\n    else:\\n        dtype = values.dtype\\n        is_period = com.is_period_arraylike(values)\\n        if com.is_datetime_or_timedelta_dtype(dtype) or is_period:\\n            if is_period:\\n                values = PeriodIndex(values)\\n            values = values.view(np.int64)\\n            keys, counts = htable.value_count_int64(values)\\n            if dropna:\\n                from pandas.tslib import iNaT\\n                msk = keys != iNaT\\n                keys, counts = keys[msk], counts[msk]\\n            keys = keys.astype(dtype)\\n        elif com.is_integer_dtype(dtype):\\n            values = com._ensure_int64(values)\\n            keys, counts = htable.value_count_int64(values)\\n        else:\\n            values = com._ensure_object(values)\\n            mask = com.isnull(values)\\n            keys, counts = htable.value_count_object(values, mask)\\n            if not dropna and mask.any():\\n                keys = np.insert(keys, 0, np.NaN)\\n                counts = np.insert(counts, 0, mask.sum())\\n        result = Series(counts, index=com._values_from_object(keys))\\n        if bins is not None:\\n            result = result.reindex(np.arange(len(cat.categories)), fill_value=0)\\n            result.index = bins[:-1]\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def value_counts(values, sort=True, ascending=False, normalize=False,\\n                 bins=None, dropna=True):\\n    from pandas.core.series import Series\\n    from pandas.tools.tile import cut\\n    from pandas.tseries.period import PeriodIndex\\n    values = Series(values).values\\n    if bins is not None:\\n        try:\\n            cat, bins = cut(values, bins, retbins=True)\\n        except TypeError:\\n            raise TypeError(\"bins argument only works with numeric data.\")\\n        values = cat.codes\\n    if com.is_categorical_dtype(values.dtype):\\n        result = values.value_counts(dropna)\\n    else:\\n        dtype = values.dtype\\n        is_period = com.is_period_arraylike(values)\\n        if com.is_datetime_or_timedelta_dtype(dtype) or is_period:\\n            if is_period:\\n                values = PeriodIndex(values)\\n            values = values.view(np.int64)\\n            keys, counts = htable.value_count_int64(values)\\n            if dropna:\\n                from pandas.tslib import iNaT\\n                msk = keys != iNaT\\n                keys, counts = keys[msk], counts[msk]\\n            keys = keys.astype(dtype)\\n        elif com.is_integer_dtype(dtype):\\n            values = com._ensure_int64(values)\\n            keys, counts = htable.value_count_int64(values)\\n        else:\\n            values = com._ensure_object(values)\\n            mask = com.isnull(values)\\n            keys, counts = htable.value_count_object(values, mask)\\n            if not dropna and mask.any():\\n                keys = np.insert(keys, 0, np.NaN)\\n                counts = np.insert(counts, 0, mask.sum())\\n        result = Series(counts, index=com._values_from_object(keys))\\n        if bins is not None:\\n            result = result.reindex(np.arange(len(cat.categories)), fill_value=0)\\n            result.index = bins[:-1]\\n    if sort:\\n        result.sort()\\n        if not ascending:\\n            result = result[::-1]\\n    if normalize:\\n        result = result / float(values.size)\\n    return result"
  },
  {
    "code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    _validate_flavor_parameter(flavor)\\n    con = _engine_builder(con)\\n    if _is_sqlalchemy_connectable(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    elif isinstance(con, string_types):\\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\\n    else:\\n        return SQLiteDatabase(con, is_cursor=is_cursor)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def pandasSQL_builder(con, flavor=None, schema=None, meta=None,\\n                      is_cursor=False):\\n    _validate_flavor_parameter(flavor)\\n    con = _engine_builder(con)\\n    if _is_sqlalchemy_connectable(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    elif isinstance(con, string_types):\\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\\n    else:\\n        return SQLiteDatabase(con, is_cursor=is_cursor)"
  },
  {
    "code": "def __get_builtin_constructor(name):\\n    cache = __builtin_constructor_cache\\n    constructor = cache.get(name)\\n    if constructor is not None:\\n        return constructor\\n    try:\\n        if name in ('SHA1', 'sha1'):\\n            import _sha1\\n            cache['SHA1'] = cache['sha1'] = _sha1.sha1\\n        elif name in ('MD5', 'md5'):\\n            import _md5\\n            cache['MD5'] = cache['md5'] = _md5.md5\\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\\n            import _sha256\\n            cache['SHA224'] = cache['sha224'] = _sha256.sha224\\n            cache['SHA256'] = cache['sha256'] = _sha256.sha256\\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\\n            import _sha512\\n            cache['SHA384'] = cache['sha384'] = _sha512.sha384\\n            cache['SHA512'] = cache['sha512'] = _sha512.sha512\\n        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512',\\n                      'SHA3_224', 'SHA3_256', 'SHA3_384', 'SHA3_512'}:\\n            import _sha3\\n            cache['SHA3_224'] = cache['sha3_224'] = _sha3.sha3_224\\n            cache['SHA3_256'] = cache['sha3_256'] = _sha3.sha3_256\\n            cache['SHA3_384'] = cache['sha3_384'] = _sha3.sha3_384\\n            cache['SHA3_512'] = cache['sha3_512'] = _sha3.sha3_512\\n    except ImportError:\\n        pass  \\n    constructor = cache.get(name)\\n    if constructor is not None:\\n        return constructor\\n    raise ValueError('unsupported hash type ' + name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __get_builtin_constructor(name):\\n    cache = __builtin_constructor_cache\\n    constructor = cache.get(name)\\n    if constructor is not None:\\n        return constructor\\n    try:\\n        if name in ('SHA1', 'sha1'):\\n            import _sha1\\n            cache['SHA1'] = cache['sha1'] = _sha1.sha1\\n        elif name in ('MD5', 'md5'):\\n            import _md5\\n            cache['MD5'] = cache['md5'] = _md5.md5\\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\\n            import _sha256\\n            cache['SHA224'] = cache['sha224'] = _sha256.sha224\\n            cache['SHA256'] = cache['sha256'] = _sha256.sha256\\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\\n            import _sha512\\n            cache['SHA384'] = cache['sha384'] = _sha512.sha384\\n            cache['SHA512'] = cache['sha512'] = _sha512.sha512\\n        elif name in {'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512',\\n                      'SHA3_224', 'SHA3_256', 'SHA3_384', 'SHA3_512'}:\\n            import _sha3\\n            cache['SHA3_224'] = cache['sha3_224'] = _sha3.sha3_224\\n            cache['SHA3_256'] = cache['sha3_256'] = _sha3.sha3_256\\n            cache['SHA3_384'] = cache['sha3_384'] = _sha3.sha3_384\\n            cache['SHA3_512'] = cache['sha3_512'] = _sha3.sha3_512\\n    except ImportError:\\n        pass  \\n    constructor = cache.get(name)\\n    if constructor is not None:\\n        return constructor\\n    raise ValueError('unsupported hash type ' + name)"
  },
  {
    "code": "def start_template_dataflow(self, job_name, variables, parameters, dataflow_template,\\n                                append_job_name=True):\\n        name = self._build_dataflow_job_name(job_name, append_job_name)\\n        self._start_template_dataflow(\\n            name, variables, parameters, dataflow_template)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-2715] Use region setting when launching Dataflow templates (#4139)\\n\\nTo launch an instance of a Dataflow template in the configured region,\\nthe API service.projects().locations().teplates() instead of\\nservice.projects().templates() has to be used. Otherwise, all jobs will\\nalways be started in us-central1.\\n\\nIn case there is no region configured, the default region `us-central1`\\nwill get picked up.\\n\\nTo make it even worse, the polling for the job status already honors the\\nregion parameter and will search for the job in the wrong region in the\\ncurrent implementation. Because the job's status is not found, the\\ncorresponding Airflow task will hang.",
    "fixed_code": "def start_template_dataflow(self, job_name, variables, parameters, dataflow_template,\\n                                append_job_name=True):\\n        variables = self._set_variables(variables)\\n        name = self._build_dataflow_job_name(job_name, append_job_name)\\n        self._start_template_dataflow(\\n            name, variables, parameters, dataflow_template)"
  },
  {
    "code": "def _build_vectors_and_vocab(self, raw_documents):\\n        term_counts_per_doc = []\\n        term_counts = {}\\n        document_counts = {}\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_dict = {}  \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n                term_counts[term] = term_counts.get(term, 0) + 1\\n            if max_df is not None:\\n                for term in term_count_dict.iterkeys():\\n                    document_counts[term] = document_counts.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        n_doc = len(term_counts_per_doc)\\n        stop_words = set()\\n        if max_df is not None:\\n            max_document_count = max_df * n_doc\\n            for t, dc in sorted(document_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if dc <= max_document_count:\\n                    break\\n                stop_words.add(t)\\n        if max_features is not None:\\n            terms = set()\\n            for t, tc in sorted(term_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        else:\\n            terms = set(term_counts.keys())\\n            terms -= stop_words\\n        vocabulary = dict(((t, i) for i, t in enumerate(terms)))  \\n        matrix = self._term_count_dicts_to_matrix(term_counts_per_doc, vocabulary)\\n        return matrix, vocabulary\\n    def _build_vectors(self, raw_documents):\\n        term_counts_per_doc = []\\n        for doc in raw_documents:\\n            term_count_dict = {}  \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        return self._term_count_dicts_to_matrix(\\n            term_counts_per_doc, self.vocabulary)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _build_vectors_and_vocab(self, raw_documents):\\n        term_counts_per_doc = []\\n        term_counts = {}\\n        document_counts = {}\\n        max_df = self.max_df\\n        max_features = self.max_features\\n        for doc in raw_documents:\\n            term_count_dict = {}  \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n                term_counts[term] = term_counts.get(term, 0) + 1\\n            if max_df is not None:\\n                for term in term_count_dict.iterkeys():\\n                    document_counts[term] = document_counts.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        n_doc = len(term_counts_per_doc)\\n        stop_words = set()\\n        if max_df is not None:\\n            max_document_count = max_df * n_doc\\n            for t, dc in sorted(document_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if dc <= max_document_count:\\n                    break\\n                stop_words.add(t)\\n        if max_features is not None:\\n            terms = set()\\n            for t, tc in sorted(term_counts.iteritems(), key=itemgetter(1),\\n                                reverse=True):\\n                if t not in stop_words:\\n                    terms.add(t)\\n                if len(terms) >= max_features:\\n                    break\\n        else:\\n            terms = set(term_counts.keys())\\n            terms -= stop_words\\n        vocabulary = dict(((t, i) for i, t in enumerate(terms)))  \\n        matrix = self._term_count_dicts_to_matrix(term_counts_per_doc, vocabulary)\\n        return matrix, vocabulary\\n    def _build_vectors(self, raw_documents):\\n        term_counts_per_doc = []\\n        for doc in raw_documents:\\n            term_count_dict = {}  \\n            for term in self.analyzer.analyze(doc):\\n                term_count_dict[term] = term_count_dict.get(term, 0) + 1\\n            term_counts_per_doc.append(term_count_dict)\\n        return self._term_count_dicts_to_matrix(\\n            term_counts_per_doc, self.vocabulary)"
  },
  {
    "code": "def _check_reg_targets(y_true, y_pred, multioutput):\\n    check_consistent_length(y_true, y_pred)\\n    y_true = check_array(y_true, ensure_2d=False)\\n    y_pred = check_array(y_pred, ensure_2d=False)\\n    if y_true.ndim == 1:\\n        y_true = y_true.reshape((-1, 1))\\n    if y_pred.ndim == 1:\\n        y_pred = y_pred.reshape((-1, 1))\\n    if y_true.shape[1] != y_pred.shape[1]:\\n        raise ValueError(\"y_true and y_pred have different number of output \"\\n                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\\n    n_outputs = y_true.shape[1]\\n    allowed_multioutput_str = ('raw_values', 'uniform_average',\\n                               'variance_weighted')\\n    if isinstance(multioutput, str):\\n        if multioutput not in allowed_multioutput_str:\\n            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\\n                             \"You provided multioutput={!r}\".format(\\n                                 allowed_multioutput_str,\\n                                 multioutput))\\n    elif multioutput is not None:\\n        multioutput = check_array(multioutput, ensure_2d=False)\\n        if n_outputs == 1:\\n            raise ValueError(\"Custom weights are useful only in \"\\n                             \"multi-output cases.\")\\n        elif n_outputs != len(multioutput):\\n            raise ValueError((\"There must be equally many custom weights \"\\n                              \"(%d) as outputs (%d).\") %\\n                             (len(multioutput), n_outputs))\\n    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\\n    return y_type, y_true, y_pred, multioutput",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Add Poisson, Gamma and Tweedie deviances to regression metrics (#14263)",
    "fixed_code": "def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\\n    check_consistent_length(y_true, y_pred)\\n    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\\n    if y_true.ndim == 1:\\n        y_true = y_true.reshape((-1, 1))\\n    if y_pred.ndim == 1:\\n        y_pred = y_pred.reshape((-1, 1))\\n    if y_true.shape[1] != y_pred.shape[1]:\\n        raise ValueError(\"y_true and y_pred have different number of output \"\\n                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\\n    n_outputs = y_true.shape[1]\\n    allowed_multioutput_str = ('raw_values', 'uniform_average',\\n                               'variance_weighted')\\n    if isinstance(multioutput, str):\\n        if multioutput not in allowed_multioutput_str:\\n            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\\n                             \"You provided multioutput={!r}\".format(\\n                                 allowed_multioutput_str,\\n                                 multioutput))\\n    elif multioutput is not None:\\n        multioutput = check_array(multioutput, ensure_2d=False)\\n        if n_outputs == 1:\\n            raise ValueError(\"Custom weights are useful only in \"\\n                             \"multi-output cases.\")\\n        elif n_outputs != len(multioutput):\\n            raise ValueError((\"There must be equally many custom weights \"\\n                              \"(%d) as outputs (%d).\") %\\n                             (len(multioutput), n_outputs))\\n    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\\n    return y_type, y_true, y_pred, multioutput"
  },
  {
    "code": "def trace(self, offset=0, axis1=0, axis2=1, dtype=None, out=None):\\n\\t\\tm = self._mask\\n\\t\\tif m is nomask:\\n\\t\\t\\tresult = super(MaskedArray, self).trace(offset=offset, axis1=axis1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\taxis2=axis2, out=out)\\n\\t\\t\\treturn result.astype(dtype)\\n\\t\\telse:\\n\\t\\t\\tD = self.diagonal(offset=offset, axis1=axis1, axis2=axis2)\\n\\t\\t\\treturn D.astype(dtype).filled(0).sum(axis=None, out=out)\\n\\ttrace.__doc__ = ndarray.trace.__doc__",
    "label": 1,
    "bug_type": "corruption - memory",
    "bug_description": "BUG: np.ma.trace gives the wrong result on ND arrays\\n\\nFixes #5560",
    "fixed_code": "def trace(self, offset=0, axis1=0, axis2=1, dtype=None, out=None):\\n\\t\\tm = self._mask\\n\\t\\tif m is nomask:\\n\\t\\t\\tresult = super(MaskedArray, self).trace(offset=offset, axis1=axis1,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\taxis2=axis2, out=out)\\n\\t\\t\\treturn result.astype(dtype)\\n\\t\\telse:\\n\\t\\t\\tD = self.diagonal(offset=offset, axis1=axis1, axis2=axis2)\\n\\t\\t\\treturn D.astype(dtype).filled(0).sum(axis=-1, out=out)\\n\\ttrace.__doc__ = ndarray.trace.__doc__"
  },
  {
    "code": "def __getattr__(self, attr):\\n        return getattr(self.socket, attr)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Merged revisions 80875 via svnmerge from svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n........\\n  r80875 | giampaolo.rodola | 2010-05-06 19:57:06 +0200 (gio, 06 mag 2010) | 1 line\\n\\n  Fix asyncore issues 8573 and 8483: _strerror might throw ValueError; asyncore.__getattr__ cheap inheritance caused confusing error messages when accessing undefined class attributes; added an alias for __str__ which now is used as a fallback for __repr__\\n........",
    "fixed_code": "def __getattr__(self, attr):\\n        try:\\n            return getattr(self.socket, attr)\\n        except AttributeError:\\n            raise AttributeError(\"%s instance has no attribute '%s'\"\\n                                 %(self.__class__.__name__, attr))"
  },
  {
    "code": "def __init__(self, key=None, sep=':', salt=None):\\n        self.key = key or settings.SECRET_KEY\\n        self.sep = force_str(sep)\\n        if _SEP_UNSAFE.match(self.sep):\\n            warnings.warn('Unsafe Signer separator: %r (cannot be empty or consist of only A-z0-9-_=)' % sep,\\n                          RemovedInDjango110Warning)\\n        self.salt = force_str(salt or\\n            '%s.%s' % (self.__class__.__module__, self.__class__.__name__))",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, key=None, sep=':', salt=None):\\n        self.key = key or settings.SECRET_KEY\\n        self.sep = force_str(sep)\\n        if _SEP_UNSAFE.match(self.sep):\\n            warnings.warn('Unsafe Signer separator: %r (cannot be empty or consist of only A-z0-9-_=)' % sep,\\n                          RemovedInDjango110Warning)\\n        self.salt = force_str(salt or\\n            '%s.%s' % (self.__class__.__module__, self.__class__.__name__))"
  },
  {
    "code": "def _make_fixed_width(strings, justify='right'):\\n    if len(strings) == 0:\\n        return strings\\n    max_len = max(_strlen(x) for x in strings)\\n    conf_max = print_config.max_colwidth\\n    if conf_max is not None and max_len > conf_max:\\n        max_len = conf_max\\n    if justify == 'left':\\n        justfunc = lambda self, x: self.ljust(x)\\n    else:\\n        justfunc = lambda self, x: self.rjust(x)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: DataFrame column formatting issue in length-truncated column close #1906",
    "fixed_code": "def _make_fixed_width(strings, justify='right', minimum=None):\\n    if len(strings) == 0:\\n        return strings\\n    max_len = max(_strlen(x) for x in strings)\\n    if minimum is not None:\\n        max_len = max(minimum, max_len)\\n    conf_max = print_config.max_colwidth\\n    if conf_max is not None and max_len > conf_max:\\n        max_len = conf_max\\n    if justify == 'left':\\n        justfunc = lambda self, x: self.ljust(x)\\n    else:\\n        justfunc = lambda self, x: self.rjust(x)"
  },
  {
    "code": "def default_collection(cls):\\n        return cls._default_collection\\n    @default_collection.setter\\n    def default_collection(cls, value):\\n        cls._default_collection = value\\n    @property",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def default_collection(cls):\\n        return cls._default_collection\\n    @default_collection.setter\\n    def default_collection(cls, value):\\n        cls._default_collection = value\\n    @property"
  },
  {
    "code": "def mad(self, axis=0, skipna=True):\\n        if axis == 0:\\n            demeaned = self - self.mean(axis=0)\\n        else:\\n            demeaned = self.sub(self.mean(axis=1), axis=0)\\n        return np.abs(demeaned).mean(axis=axis, skipna=skipna)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def mad(self, axis=0, skipna=True):\\n        if axis == 0:\\n            demeaned = self - self.mean(axis=0)\\n        else:\\n            demeaned = self.sub(self.mean(axis=1), axis=0)\\n        return np.abs(demeaned).mean(axis=axis, skipna=skipna)"
  },
  {
    "code": "def subscribe_by_pool_ids(self, pool_ids):\\n\\t\\tavailable_pools = RhsmPools(self.module)\\n\\t\\tavailable_pool_ids = [p.get_pool_id() for p in available_pools]\\n\\t\\tfor pool_id, quantity in sorted(pool_ids.items()):\\n\\t\\t\\tif pool_id in available_pool_ids:\\n\\t\\t\\t\\targs = [SUBMAN_CMD, 'attach', '--pool', pool_id]\\n\\t\\t\\t\\tif quantity is not None:\\n\\t\\t\\t\\t\\targs.extend(['--quantity', to_native(quantity)])\\n\\t\\t\\t\\trc, stderr, stdout = self.module.run_command(args, check_rc=True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)\\n\\t\\treturn pool_ids",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def subscribe_by_pool_ids(self, pool_ids):\\n\\t\\tavailable_pools = RhsmPools(self.module)\\n\\t\\tavailable_pool_ids = [p.get_pool_id() for p in available_pools]\\n\\t\\tfor pool_id, quantity in sorted(pool_ids.items()):\\n\\t\\t\\tif pool_id in available_pool_ids:\\n\\t\\t\\t\\targs = [SUBMAN_CMD, 'attach', '--pool', pool_id]\\n\\t\\t\\t\\tif quantity is not None:\\n\\t\\t\\t\\t\\targs.extend(['--quantity', to_native(quantity)])\\n\\t\\t\\t\\trc, stderr, stdout = self.module.run_command(args, check_rc=True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)\\n\\t\\treturn pool_ids"
  },
  {
    "code": "def help_text_for_field(name, model):\\n    help_text = \"\"\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n    except FieldDoesNotExist:\\n        pass\\n    else:\\n        if hasattr(field, 'help_text'):\\n            help_text = field.help_text\\n    return smart_text(help_text)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixing #26524 -- Made a foreign key id reference in ModelAdmin.list_display display the id.",
    "fixed_code": "def help_text_for_field(name, model):\\n    help_text = \"\"\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):\\n        pass\\n    else:\\n        if hasattr(field, 'help_text'):\\n            help_text = field.help_text\\n    return smart_text(help_text)"
  },
  {
    "code": "def __init__(self, data=None, index=None, columns=None, dtype=None):\\n        if isinstance(data, dict):\\n            self._series, self.index = self._initDict(data, index,\\n                                                      columns, dtype)\\n        elif isinstance(data, np.ndarray):\\n            if columns is None:\\n                raise Exception('Must pass column names with ndarray!')\\n            if index is None:\\n                raise Exception('Must pass index with ndarray!')\\n            if data.ndim == 1:\\n                data = data.reshape((len(data), 1))\\n            elif data.ndim != 2:\\n                raise Exception('Must pass 2-d input!')\\n            self._series, self.index = self._initMatrix(data, index,\\n                                                        columns, dtype)\\n        elif data is None:\\n            if index is None:\\n                index = NULL_INDEX\\n            self._series, self.index = {}, index",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __init__(self, data=None, index=None, columns=None, dtype=None):\\n        if isinstance(data, dict):\\n            self._series, self.index = self._initDict(data, index,\\n                                                      columns, dtype)\\n        elif isinstance(data, np.ndarray):\\n            if columns is None:\\n                raise Exception('Must pass column names with ndarray!')\\n            if index is None:\\n                raise Exception('Must pass index with ndarray!')\\n            if data.ndim == 1:\\n                data = data.reshape((len(data), 1))\\n            elif data.ndim != 2:\\n                raise Exception('Must pass 2-d input!')\\n            self._series, self.index = self._initMatrix(data, index,\\n                                                        columns, dtype)\\n        elif data is None:\\n            if index is None:\\n                index = NULL_INDEX\\n            self._series, self.index = {}, index"
  },
  {
    "code": "def get_list_or_404(klass, *args, **kwargs):\\n    queryset = _get_queryset(klass)\\n    try:\\n        obj_list = list(queryset.filter(*args, **kwargs))\\n    except AttributeError:\\n        klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__\\n        raise ValueError(\\n            \"First argument to get_list_or_404() must be a Model, Manager, or \"\\n            \"QuerySet, not '%s'.\" % klass__name\\n        )\\n    if not obj_list:\\n        raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)\\n    return obj_list",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #29135 -- Prevented get_object/list_or_404() from hiding AttributeError raised by QuerySet filtering.",
    "fixed_code": "def get_list_or_404(klass, *args, **kwargs):\\n    queryset = _get_queryset(klass)\\n    if not hasattr(queryset, 'filter'):\\n        klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__\\n        raise ValueError(\\n            \"First argument to get_list_or_404() must be a Model, Manager, or \"\\n            \"QuerySet, not '%s'.\" % klass__name\\n        )\\n    obj_list = list(queryset.filter(*args, **kwargs))\\n    if not obj_list:\\n        raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)\\n    return obj_list"
  },
  {
    "code": "def _clean_interp_method(method, order=None, **kwargs):\\n    valid = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\\n             'quadratic', 'cubic', 'barycentric', 'polynomial',\\n             'krogh', 'piecewise_polynomial',\\n             'pchip', 'spline']\\n    if method in ('spline', 'polynomial') and order is None:\\n        raise ValueError(\"You must specify the order of the spline or \"\\n                         \"polynomial.\")\\n    if method not in valid:\\n        raise ValueError(\"method must be one of {0}.\"\\n                         \"Got '{1}' instead.\".format(valid, method))\\n    return method",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _clean_interp_method(method, order=None, **kwargs):\\n    valid = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\\n             'quadratic', 'cubic', 'barycentric', 'polynomial',\\n             'krogh', 'piecewise_polynomial',\\n             'pchip', 'spline']\\n    if method in ('spline', 'polynomial') and order is None:\\n        raise ValueError(\"You must specify the order of the spline or \"\\n                         \"polynomial.\")\\n    if method not in valid:\\n        raise ValueError(\"method must be one of {0}.\"\\n                         \"Got '{1}' instead.\".format(valid, method))\\n    return method"
  },
  {
    "code": "def reset_cache(cls) -> None:\\n        cls._cache_dtypes = {}",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def reset_cache(cls) -> None:\\n        cls._cache_dtypes = {}"
  },
  {
    "code": "def _create_sql_schema(self, frame, table_name):\\n        table = PandasSQLTable(table_name, self, frame=frame)\\n        return str(table.compile())",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG/TST: fix get_schema + add tests\\n\\n- adapt signature of get_schema to what it was before (for backwards compatibility), possibility to call function without connection object\\n- fix get_schema for sqlalchemy mode\\n- add tests",
    "fixed_code": "def _create_sql_schema(self, frame, table_name):\\n        table = PandasSQLTable(table_name, self, frame=frame)\\n        return str(table.sql_schema())"
  },
  {
    "code": "def _delete(self, req, id):\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tauthorize(context)\\n\\t\\ttry:\\n\\t\\t\\tflavor = flavors.get_flavor_by_flavor_id(\\n\\t\\t\\t\\t\\tid, read_deleted=\"no\")\\n\\t\\texcept exception.NotFound as e:\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=e.format_message())\\n\\t\\tflavors.destroy(flavor['name'])\\n\\t\\treturn webob.Response(status_int=202)\\n\\t@wsgi.action(\"create\")\\n\\t@wsgi.serializers(xml=flavors_api.FlavorTemplate)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make flavors is_public option actually work\\n\\nWhen you create a flavor, you can set an is_public flag to be True or\\nFalse.  It is True by default.  When False, the intention is that the\\nflavor is only accessible by an admin, unless you use the flavor_access\\nAPI extension to grant access to specific tenants.\\n\\nUnfortunately, the only place in the code where this was being enforced\\nwas when listing flavors through the API.  It would filter out the\\nnon-public ones for a non-admin.  Otherwise, the flavor was accessible.\\nYou could get the details, and you could boot an instance with it, if\\nyou figured out a valid flavor ID.\\n\\nThis patch adds enforcement down in the db layer.  It also fixes one\\nplace in the API where the context wasn't passed down to enable the\\nenforcement to happen.\\n\\nFix bug 1194093.",
    "fixed_code": "def _delete(self, req, id):\\n\\t\\tcontext = req.environ['nova.context']\\n\\t\\tauthorize(context)\\n\\t\\ttry:\\n\\t\\t\\tflavor = flavors.get_flavor_by_flavor_id(\\n\\t\\t\\t\\t\\tid, ctxt=context, read_deleted=\"no\")\\n\\t\\texcept exception.NotFound as e:\\n\\t\\t\\traise webob.exc.HTTPNotFound(explanation=e.format_message())\\n\\t\\tflavors.destroy(flavor['name'])\\n\\t\\treturn webob.Response(status_int=202)\\n\\t@wsgi.action(\"create\")\\n\\t@wsgi.serializers(xml=flavors_api.FlavorTemplate)"
  },
  {
    "code": "def _log_reg_scoring_path(X, y, train, test, Cs=10, scoring=None,\\n                          fit_intercept=False,\\n                          max_iter=100, gtol=1e-4,\\n                          tol=1e-4, verbose=0, method='liblinear'):\\n    log_reg = LogisticRegression(fit_intercept=fit_intercept)\\n    log_reg._enc = LabelEncoder()\\n    log_reg._enc.fit_transform([-1, 1])\\n    coefs, Cs = logistic_regression_path(X[train], y[train], Cs=Cs,\\n                                         fit_intercept=fit_intercept,\\n                                         method=method,\\n                                         max_iter=max_iter,\\n                                         gtol=gtol, verbose=verbose)\\n    scores = list()\\n    X_test = X[test]\\n    y_test = y[test]\\n    if isinstance(scoring, six.string_types):\\n        scoring = SCORERS[scoring]\\n    for w in coefs:\\n        if fit_intercept:\\n            log_reg.coef_ = w[np.newaxis, :-1]\\n            log_reg.intercept_ = w[-1]\\n        else:\\n            log_reg.coef_ = w[np.newaxis, :]\\n        if scoring is None:\\n            scores.append(log_reg.score(X_test, y_test))\\n        else:\\n            scores.append(scoring(log_reg, X_test, y_test))\\n    return coefs, Cs, np.array(scores)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactor and some bug fixing",
    "fixed_code": "def _log_reg_scoring_path(X, y, train, test, Cs=10, scoring=None,\\n                          fit_intercept=False,\\n                          max_iter=100, gtol=1e-4,\\n                          tol=1e-4, verbose=0, method='liblinear'):\\n    log_reg = LogisticRegression(fit_intercept=fit_intercept)\\n    log_reg._enc = LabelEncoder()\\n    log_reg._enc.fit_transform([-1, 1])\\n    coefs, Cs = logistic_regression_path(X[train], y[train], Cs=Cs,\\n                                         fit_intercept=fit_intercept,\\n                                         solver=method,\\n                                         max_iter=max_iter,\\n                                         gtol=gtol, verbose=verbose)\\n    scores = list()\\n    X_test = X[test]\\n    y_test = y[test]\\n    if isinstance(scoring, six.string_types):\\n        scoring = SCORERS[scoring]\\n    for w in coefs:\\n        if fit_intercept:\\n            log_reg.coef_ = w[np.newaxis, :-1]\\n            log_reg.intercept_ = w[-1]\\n        else:\\n            log_reg.coef_ = w[np.newaxis, :]\\n        if scoring is None:\\n            scores.append(log_reg.score(X_test, y_test))\\n        else:\\n            scores.append(scoring(log_reg, X_test, y_test))\\n    return coefs, Cs, np.array(scores)"
  },
  {
    "code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in seq:\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "bpo-31961: Fix support of path-like executables in subprocess. (GH-5914)",
    "fixed_code": "def list2cmdline(seq):\\n    result = []\\n    needquote = False\\n    for arg in map(os.fsdecode, seq):\\n        bs_buf = []\\n        if result:\\n            result.append(' ')\\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or not arg\\n        if needquote:\\n            result.append('\"')\\n        for c in arg:\\n            if c == '\\\\':\\n                bs_buf.append(c)\\n            elif c == '\"':\\n                result.append('\\\\' * len(bs_buf)*2)\\n                bs_buf = []\\n                result.append('\\\\\"')\\n            else:\\n                if bs_buf:\\n                    result.extend(bs_buf)\\n                    bs_buf = []\\n                result.append(c)\\n        if bs_buf:\\n            result.extend(bs_buf)\\n        if needquote:\\n            result.extend(bs_buf)\\n            result.append('\"')\\n    return ''.join(result)"
  },
  {
    "code": "def predict(self, X):\\n        if self.voting == 'soft':\\n            maj = np.argmax(self.predict_proba(X), axis=1)\\n        else:  \\n            predictions = self._predict(X)\\n            maj = np.apply_along_axis(\\n                                      lambda x:\\n                                      np.argmax(np.bincount(x,\\n                                                weights=self.weights)),\\n                                      axis=1,\\n                                      arr=predictions)\\n        maj = self.le_.inverse_transform(maj)\\n        return maj",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def predict(self, X):\\n        if self.voting == 'soft':\\n            maj = np.argmax(self.predict_proba(X), axis=1)\\n        else:  \\n            predictions = self._predict(X)\\n            maj = np.apply_along_axis(\\n                                      lambda x:\\n                                      np.argmax(np.bincount(x,\\n                                                weights=self.weights)),\\n                                      axis=1,\\n                                      arr=predictions)\\n        maj = self.le_.inverse_transform(maj)\\n        return maj"
  },
  {
    "code": "def set_info(self):\\n        self.attrs.pandas_type = self.pandas_kind\\n        self.attrs.pandas_version = _version\\n        self.set_version()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: support timezone data_columns in HDFStore (GH2852)\\n\\nDOC: update release notes/whatsnew, added whatsnew 0.11.1 to index.rst\\n\\nENH: warn a FrequencyWarning if appending with a different frequency that existing",
    "fixed_code": "def set_info(self, info):\\n        idx = info.get(self.name)\\n        if idx is not None:\\n            self.__dict__.update(idx)"
  },
  {
    "code": "def __init__(self, fullname, path, file=None):\\n        super().__init__(fullname, path)\\n        self.file = file",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #13959: Re-implement imp.NullImporter in Lib/imp.py.",
    "fixed_code": "def __init__(self, path):\\n        if path == '':\\n            raise ImportError('empty pathname', path='')\\n        elif os.path.isdir(path):\\n            raise ImportError('existing directory', path=path)"
  },
  {
    "code": "def _astype(self, dtype, copy=False, errors='raise', values=None,\\n                klass=None, mgr=None, **kwargs):\\n        errors_legal_values = ('raise', 'ignore')\\n        if errors not in errors_legal_values:\\n            invalid_arg = (\"Expected value of kwarg 'errors' to be one of {}. \"\\n                           \"Supplied value is '{}'\".format(\\n                               list(errors_legal_values), errors))\\n            raise ValueError(invalid_arg)\\n        if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\\n            msg = (\"Expected an instance of {}, but got the class instead. \"\\n                   \"Try instantiating 'dtype'.\".format(dtype.__name__))\\n            raise TypeError(msg)\\n        if self.is_categorical_astype(dtype):\\n            if ('categories' in kwargs or 'ordered' in kwargs):\\n                if isinstance(dtype, CategoricalDtype):\\n                    raise TypeError(\\n                        \"Cannot specify a CategoricalDtype and also \"\\n                        \"`categories` or `ordered`. Use \"\\n                        \"`dtype=CategoricalDtype(categories, ordered)`\"\\n                        \" instead.\")\\n                warnings.warn(\"specifying 'categories' or 'ordered' in \"\\n                              \".astype() is deprecated; pass a \"\\n                              \"CategoricalDtype instead\",\\n                              FutureWarning, stacklevel=7)\\n            kwargs = kwargs.copy()\\n            categories = getattr(dtype, 'categories', None)\\n            ordered = getattr(dtype, 'ordered', False)\\n            kwargs.setdefault('categories', categories)\\n            kwargs.setdefault('ordered', ordered)\\n            return self.make_block(Categorical(self.values, **kwargs))\\n        dtype = np.dtype(dtype)\\n        if self.dtype == dtype:\\n            if copy:\\n                return self.copy()\\n            return self\\n        if klass is None:\\n            if dtype == np.object_:\\n                klass = ObjectBlock\\n        try:\\n            if values is None:\\n                if issubclass(dtype.type,\\n                              (compat.text_type, compat.string_types)):\\n                    if self.is_datelike:\\n                        values = self.to_native_types()\\n                    else:\\n                        values = self.values\\n                else:\\n                    values = self.get_values(dtype=dtype)\\n                values = astype_nansafe(values.ravel(), dtype, copy=True)\\n                values = values.reshape(self.shape)\\n            newb = make_block(values, placement=self.mgr_locs, dtype=dtype,\\n                              klass=klass)\\n        except:\\n            if errors == 'raise':\\n                raise\\n            newb = self.copy() if copy else self\\n        if newb.is_numeric and self.is_numeric:\\n            if newb.shape != self.shape:\\n                raise TypeError(\\n                    \"cannot set astype for copy = [{copy}] for dtype \"\\n                    \"({dtype} [{itemsize}]) with smaller itemsize than \"\\n                    \"current ({newb_dtype} [{newb_size}])\".format(\\n                        copy=copy, dtype=self.dtype.name,\\n                        itemsize=self.itemsize, newb_dtype=newb.dtype.name,\\n                        newb_size=newb.itemsize))\\n        return newb",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: Fix Series.astype and Categorical.astype to update existing Categorical data (#18710)",
    "fixed_code": "def _astype(self, dtype, copy=False, errors='raise', values=None,\\n                klass=None, mgr=None, **kwargs):\\n        errors_legal_values = ('raise', 'ignore')\\n        if errors not in errors_legal_values:\\n            invalid_arg = (\"Expected value of kwarg 'errors' to be one of {}. \"\\n                           \"Supplied value is '{}'\".format(\\n                               list(errors_legal_values), errors))\\n            raise ValueError(invalid_arg)\\n        if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\\n            msg = (\"Expected an instance of {}, but got the class instead. \"\\n                   \"Try instantiating 'dtype'.\".format(dtype.__name__))\\n            raise TypeError(msg)\\n        if self.is_categorical_astype(dtype):\\n            if ('categories' in kwargs or 'ordered' in kwargs):\\n                if isinstance(dtype, CategoricalDtype):\\n                    raise TypeError(\\n                        \"Cannot specify a CategoricalDtype and also \"\\n                        \"`categories` or `ordered`. Use \"\\n                        \"`dtype=CategoricalDtype(categories, ordered)`\"\\n                        \" instead.\")\\n                warnings.warn(\"specifying 'categories' or 'ordered' in \"\\n                              \".astype() is deprecated; pass a \"\\n                              \"CategoricalDtype instead\",\\n                              FutureWarning, stacklevel=7)\\n            categories = kwargs.get('categories', None)\\n            ordered = kwargs.get('ordered', None)\\n            if _any_not_none(categories, ordered):\\n                dtype = CategoricalDtype(categories, ordered)\\n            if is_categorical_dtype(self.values):\\n                return self.make_block(self.values.astype(dtype, copy=copy))\\n            return self.make_block(Categorical(self.values, dtype=dtype))\\n        dtype = np.dtype(dtype)\\n        if self.dtype == dtype:\\n            if copy:\\n                return self.copy()\\n            return self\\n        if klass is None:\\n            if dtype == np.object_:\\n                klass = ObjectBlock\\n        try:\\n            if values is None:\\n                if issubclass(dtype.type,\\n                              (compat.text_type, compat.string_types)):\\n                    if self.is_datelike:\\n                        values = self.to_native_types()\\n                    else:\\n                        values = self.values\\n                else:\\n                    values = self.get_values(dtype=dtype)\\n                values = astype_nansafe(values.ravel(), dtype, copy=True)\\n                values = values.reshape(self.shape)\\n            newb = make_block(values, placement=self.mgr_locs, dtype=dtype,\\n                              klass=klass)\\n        except:\\n            if errors == 'raise':\\n                raise\\n            newb = self.copy() if copy else self\\n        if newb.is_numeric and self.is_numeric:\\n            if newb.shape != self.shape:\\n                raise TypeError(\\n                    \"cannot set astype for copy = [{copy}] for dtype \"\\n                    \"({dtype} [{itemsize}]) with smaller itemsize than \"\\n                    \"current ({newb_dtype} [{newb_size}])\".format(\\n                        copy=copy, dtype=self.dtype.name,\\n                        itemsize=self.itemsize, newb_dtype=newb.dtype.name,\\n                        newb_size=newb.itemsize))\\n        return newb"
  },
  {
    "code": "class HTTPSClientAuthConnection(httplib.HTTPSConnection):\\n\\tdef __init__(self, host, port, key_file, cert_file,\\n\\t\\t\\t\\t ca_file, timeout=None):\\n\\t\\thttplib.HTTPSConnection.__init__(self, host, port, key_file=key_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t cert_file=cert_file)\\n\\t\\tself.key_file = key_file\\n\\t\\tself.cert_file = cert_file\\n\\t\\tself.ca_file = ca_file\\n\\t\\tself.timeout = timeout",
    "label": 1,
    "bug_type": "security",
    "bug_description": "New -k/--insecure command line option\\n\\nFix for bug 929591.\\n\\nChange glance to require server certificate validation\\nby default when using https. The standard system\\nCA file will be used if available (and an alternative was not\\nprovided).\\n\\nThe --insecure option can be used by clients to skip server\\ncertificate validation if appropriate.\\n\\n  If the standard CA file is not suitable they will need to provide\\n  a CA file or else create an 'insecure' glance client.\\n  certificate validation.\\n  system CA file is installed then that file will be used by default.\\n  It probably makes sense for the glance package to have a\\n  dependency on whichever package provides the default CA bundle.\\n  (In Ubuntu this is 'ca-certificates')",
    "fixed_code": "class HTTPSClientAuthConnection(httplib.HTTPSConnection):\\n\\tdef __init__(self, host, port, key_file, cert_file,\\n\\t\\t\\t\\t ca_file, timeout=None, insecure=False):\\n\\t\\thttplib.HTTPSConnection.__init__(self, host, port, key_file=key_file,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t cert_file=cert_file)\\n\\t\\tself.key_file = key_file\\n\\t\\tself.cert_file = cert_file\\n\\t\\tself.ca_file = ca_file\\n\\t\\tself.timeout = timeout\\n\\t\\tself.insecure = insecure"
  },
  {
    "code": "def do_deactivate_user(\\n\\tuser_profile: UserProfile, _cascade: bool = True, *, acting_user: Optional[UserProfile]\\n) -> None:\\n\\tif not user_profile.is_active:\\n\\t\\treturn\\n\\tif _cascade:\\n\\t\\tbot_profiles = get_active_bots_owned_by_user(user_profile)\\n\\t\\tfor profile in bot_profiles:\\n\\t\\t\\tdo_deactivate_user(profile, _cascade=False, acting_user=acting_user)\\n\\twith transaction.atomic():\\n\\t\\tif user_profile.realm.is_zephyr_mirror_realm:  \\n\\t\\t\\tuser_profile.is_mirror_dummy = True\\n\\t\\t\\tuser_profile.save(update_fields=[\"is_mirror_dummy\"])\\n\\t\\tchange_user_is_active(user_profile, False)\\n\\t\\tdelete_user_sessions(user_profile)\\n\\t\\tclear_scheduled_emails(user_profile.id)\\n\\t\\trevoke_invites_generated_by_user(user_profile)\\n\\t\\tevent_time = timezone_now()\\n\\t\\tRealmAuditLog.objects.create(\\n\\t\\t\\trealm=user_profile.realm,\\n\\t\\t\\tmodified_user=user_profile,\\n\\t\\t\\tacting_user=acting_user,\\n\\t\\t\\tevent_type=RealmAuditLog.USER_DEACTIVATED,\\n\\t\\t\\tevent_time=event_time,\\n\\t\\t\\textra_data=orjson.dumps(\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tRealmAuditLog.ROLE_COUNT: realm_user_count_by_role(user_profile.realm),\\n\\t\\t\\t\\t}\\n\\t\\t\\t).decode(),\\n\\t\\t)\\n\\t\\tdo_increment_logging_stat(\\n\\t\\t\\tuser_profile.realm,\\n\\t\\t\\tCOUNT_STATS[\"active_users_log:is_bot:day\"],\\n\\t\\t\\tuser_profile.is_bot,\\n\\t\\t\\tevent_time,\\n\\t\\t\\tincrement=-1,\\n\\t\\t)\\n\\t\\tif settings.BILLING_ENABLED:\\n\\t\\t\\tupdate_license_ledger_if_needed(user_profile.realm, event_time)\\n\\tevent = dict(\\n\\t\\ttype=\"realm_user\",\\n\\t\\top=\"remove\",\\n\\t\\tperson=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t)\\n\\tsend_event(user_profile.realm, event, active_user_ids(user_profile.realm_id))\\n\\tif user_profile.is_bot:\\n\\t\\tevent = dict(\\n\\t\\t\\ttype=\"realm_bot\",\\n\\t\\t\\top=\"remove\",\\n\\t\\t\\tbot=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t\\t)\\n\\t\\tsend_event(user_profile.realm, event, bot_owner_user_ids(user_profile))",
    "label": 1,
    "bug_type": "security",
    "bug_description": "CVE-2022-24751: Clear sessions outside of the transaction.\\n\\nClearing the sessions inside the transaction makes Zulip vulnerable to\\na narrow window where the deleted session has not yet been committed,\\nbut has been removed from the memcached cache.  During this window, a\\nrequest with the session-id which has just been deleted can\\nsuccessfully re-fill the memcached cache, as the in-database delete is\\nnot yet committed, and thus not yet visible.  After the delete\\ntransaction commits, the cache will be left with a cached session,\\nwhich allows further site access until it expires (after\\nSESSION_COOKIE_AGE seconds), is ejected from the cache due to memory\\npressure, or the server is upgraded.\\n\\nMove the session deletion outside of the transaction.\\n\\nBecause the testsuite runs inside of a transaction, it is impossible\\nto test this is CI; the testsuite uses the non-caching\\n`django.contrib.sessions.backends.db` backend, regardless.  The test\\nadded in this commit thus does not fail before this commit; it is\\nmerely a base expression that the session should be deleted somehow,\\nand does not exercise the assert added in the previous commit.",
    "fixed_code": "def do_deactivate_user(\\n\\tuser_profile: UserProfile, _cascade: bool = True, *, acting_user: Optional[UserProfile]\\n) -> None:\\n\\tif not user_profile.is_active:\\n\\t\\treturn\\n\\tif _cascade:\\n\\t\\tbot_profiles = get_active_bots_owned_by_user(user_profile)\\n\\t\\tfor profile in bot_profiles:\\n\\t\\t\\tdo_deactivate_user(profile, _cascade=False, acting_user=acting_user)\\n\\twith transaction.atomic():\\n\\t\\tif user_profile.realm.is_zephyr_mirror_realm:  \\n\\t\\t\\tuser_profile.is_mirror_dummy = True\\n\\t\\t\\tuser_profile.save(update_fields=[\"is_mirror_dummy\"])\\n\\t\\tchange_user_is_active(user_profile, False)\\n\\t\\tclear_scheduled_emails(user_profile.id)\\n\\t\\trevoke_invites_generated_by_user(user_profile)\\n\\t\\tevent_time = timezone_now()\\n\\t\\tRealmAuditLog.objects.create(\\n\\t\\t\\trealm=user_profile.realm,\\n\\t\\t\\tmodified_user=user_profile,\\n\\t\\t\\tacting_user=acting_user,\\n\\t\\t\\tevent_type=RealmAuditLog.USER_DEACTIVATED,\\n\\t\\t\\tevent_time=event_time,\\n\\t\\t\\textra_data=orjson.dumps(\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tRealmAuditLog.ROLE_COUNT: realm_user_count_by_role(user_profile.realm),\\n\\t\\t\\t\\t}\\n\\t\\t\\t).decode(),\\n\\t\\t)\\n\\t\\tdo_increment_logging_stat(\\n\\t\\t\\tuser_profile.realm,\\n\\t\\t\\tCOUNT_STATS[\"active_users_log:is_bot:day\"],\\n\\t\\t\\tuser_profile.is_bot,\\n\\t\\t\\tevent_time,\\n\\t\\t\\tincrement=-1,\\n\\t\\t)\\n\\t\\tif settings.BILLING_ENABLED:\\n\\t\\t\\tupdate_license_ledger_if_needed(user_profile.realm, event_time)\\n\\tdelete_user_sessions(user_profile)\\n\\tevent = dict(\\n\\t\\ttype=\"realm_user\",\\n\\t\\top=\"remove\",\\n\\t\\tperson=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t)\\n\\tsend_event(user_profile.realm, event, active_user_ids(user_profile.realm_id))\\n\\tif user_profile.is_bot:\\n\\t\\tevent = dict(\\n\\t\\t\\ttype=\"realm_bot\",\\n\\t\\t\\top=\"remove\",\\n\\t\\t\\tbot=dict(user_id=user_profile.id, full_name=user_profile.full_name),\\n\\t\\t)\\n\\t\\tsend_event(user_profile.realm, event, bot_owner_user_ids(user_profile))"
  },
  {
    "code": "def _ensure_arraylike(values):\\n    if not isinstance(values, (np.ndarray, ABCCategorical,\\n                               ABCIndexClass, ABCSeries)):\\n        values = np.array(values)\\n    return values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "REGR: assure .unique of mixed strings does not stringize (#16108)\\n\\ncloses #16107",
    "fixed_code": "def _ensure_arraylike(values):\\n    if not isinstance(values, (np.ndarray, ABCCategorical,\\n                               ABCIndexClass, ABCSeries)):\\n        inferred = lib.infer_dtype(values)\\n        if inferred in ['mixed', 'string', 'unicode']:\\n            values = np.asarray(values, dtype=object)\\n        else:\\n            values = np.asarray(values)\\n    return values"
  },
  {
    "code": "def _release_save(self):\\n        if __debug__:\\n            self._note(\"%s._release_save()\", self)\\n        if self._count == 0:\\n            raise RuntimeError(\"cannot release un-acquired lock\")\\n        count = self._count\\n        self._count = 0\\n        owner = self._owner\\n        self._owner = None\\n        self._block.release()\\n        return (count, owner)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _release_save(self):\\n        if __debug__:\\n            self._note(\"%s._release_save()\", self)\\n        if self._count == 0:\\n            raise RuntimeError(\"cannot release un-acquired lock\")\\n        count = self._count\\n        self._count = 0\\n        owner = self._owner\\n        self._owner = None\\n        self._block.release()\\n        return (count, owner)"
  },
  {
    "code": "def getfqdn(name=''):\\n    name = name.strip()\\n    if not name or name == '0.0.0.0':\\n        name = gethostname()\\n    try:\\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\\n    except error:\\n        pass\\n    else:\\n        aliases.insert(0, hostname)\\n        for name in aliases:\\n            if '.' in name:\\n                break\\n        else:\\n            name = hostname\\n    return name",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def getfqdn(name=''):\\n    name = name.strip()\\n    if not name or name == '0.0.0.0':\\n        name = gethostname()\\n    try:\\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\\n    except error:\\n        pass\\n    else:\\n        aliases.insert(0, hostname)\\n        for name in aliases:\\n            if '.' in name:\\n                break\\n        else:\\n            name = hostname\\n    return name"
  },
  {
    "code": "def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True,\\n             **eval_kwargs):\\n    use_numexpr = use_numexpr and _bool_arith_check(op_str, a, b)\\n    if use_numexpr:\\n        return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\\n                         **eval_kwargs)\\n    return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True,\\n             **eval_kwargs):\\n    use_numexpr = use_numexpr and _bool_arith_check(op_str, a, b)\\n    if use_numexpr:\\n        return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\\n                         **eval_kwargs)\\n    return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)"
  },
  {
    "code": "def _aggregate_named(self, arg):\\n        result = {}\\n        for name in self.primary:\\n            grp = self.get_group(name)\\n            grp.name = name\\n            output = arg(grp)\\n            result[name] = output\\n        return result",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: multi-key groupby now works with multiple functions. starting to add *args, **kwargs. address GH #133",
    "fixed_code": "def _aggregate_named(self, func, *args, **kwargs):\\n        result = {}\\n        for name in self.primary:\\n            grp = self.get_group(name)\\n            grp.name = name\\n            output = func(grp, *args, **kwargs)\\n            result[name] = output\\n        return result"
  },
  {
    "code": "def to_datetime(arg, errors='ignore', dayfirst=False, utc=None, box=True,\\n                format=None, coerce=False, unit='ns'):\\n    from pandas.core.series import Series\\n    from pandas.tseries.index import DatetimeIndex",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "TST: to_datetime format fixes\\n\\nCLN/TST: to_datetime cleanup\\n\\nTST: tests for dayfirst=True",
    "fixed_code": "def to_datetime(arg, errors='ignore', dayfirst=False, utc=None, box=True,\\n                format=None, coerce=False, unit='ns'):\\n    from pandas import Timestamp\\n    from pandas.core.series import Series\\n    from pandas.tseries.index import DatetimeIndex"
  },
  {
    "code": "def main(fixer_pkg, args=None):\\n    parser = optparse.OptionParser(usage=\"2to3 [options] file|dir ...\")\\n    parser.add_option(\"-d\", \"--doctests_only\", action=\"store_true\",\\n                      help=\"Fix up doctests only\")\\n    parser.add_option(\"-f\", \"--fix\", action=\"append\", default=[],\\n                      help=\"Each FIX specifies a transformation; default: all\")\\n    parser.add_option(\"-j\", \"--processes\", action=\"store\", default=1,\\n                      type=\"int\", help=\"Run 2to3 concurrently\")\\n    parser.add_option(\"-x\", \"--nofix\", action=\"append\", default=[],\\n                      help=\"Prevent a transformation from being run\")\\n    parser.add_option(\"-l\", \"--list-fixes\", action=\"store_true\",\\n                      help=\"List available transformations\")\\n    parser.add_option(\"-p\", \"--print-function\", action=\"store_true\",\\n                      help=\"Modify the grammar so that print() is a function\")\\n    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\",\\n                      help=\"More verbose logging\")\\n    parser.add_option(\"--no-diffs\", action=\"store_true\",\\n                      help=\"Don't show diffs of the refactoring\")\\n    parser.add_option(\"-w\", \"--write\", action=\"store_true\",\\n                      help=\"Write back modified files\")\\n    parser.add_option(\"-n\", \"--nobackups\", action=\"store_true\", default=False,\\n                      help=\"Don't write backups for modified files\")\\n    refactor_stdin = False\\n    flags = {}\\n    options, args = parser.parse_args(args)\\n    if not options.write and options.no_diffs:\\n        warn(\"not writing files and not printing diffs; that's not very useful\")\\n    if not options.write and options.nobackups:\\n        parser.error(\"Can't use -n without -w\")\\n    if options.list_fixes:\\n        print \"Available transformations for the -f/--fix option:\"\\n        for fixname in refactor.get_all_fix_names(fixer_pkg):\\n            print fixname\\n        if not args:\\n            return 0\\n    if not args:\\n        print >> sys.stderr, \"At least one file or directory argument required.\"\\n        print >> sys.stderr, \"Use --help to show usage.\"\\n        return 2\\n    if \"-\" in args:\\n        refactor_stdin = True\\n        if options.write:\\n            print >> sys.stderr, \"Can't write to stdin.\"\\n            return 2\\n    if options.print_function:\\n        flags[\"print_function\"] = True\\n    level = logging.DEBUG if options.verbose else logging.INFO\\n    logging.basicConfig(format='%(name)s: %(message)s', level=level)\\n    avail_fixes = set(refactor.get_fixers_from_package(fixer_pkg))\\n    unwanted_fixes = set(fixer_pkg + \".fix_\" + fix for fix in options.nofix)\\n    explicit = set()\\n    if options.fix:\\n        all_present = False\\n        for fix in options.fix:\\n            if fix == \"all\":\\n                all_present = True\\n            else:\\n                explicit.add(fixer_pkg + \".fix_\" + fix)\\n        requested = avail_fixes.union(explicit) if all_present else explicit\\n    else:\\n        requested = avail_fixes.union(explicit)\\n    fixer_names = requested.difference(unwanted_fixes)\\n    rt = StdoutRefactoringTool(sorted(fixer_names), flags, sorted(explicit),\\n                               options.nobackups, not options.no_diffs)\\n    if not rt.errors:\\n        if refactor_stdin:\\n            rt.refactor_stdin()\\n        else:\\n            try:\\n                rt.refactor(args, options.write, options.doctests_only,\\n                            options.processes)\\n            except refactor.MultiprocessingUnsupported:\\n                assert options.processes > 1\\n                print >> sys.stderr, \"Sorry, -j isn't \" \\\\n                    \"supported on this platform.\"\\n                return 1\\n        rt.summarize()\\n    return int(bool(rt.errors))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Issue #13930: Adds ability for 2to3 to write its output to a different directory tree instead of overwriting the input files.  Adds three command line options: -o/--output-dir, -W/--write-unchanged-files and --add-suffix.\\n\\nFeature backports into stable release branches for 2to3 are allowed by\\na special exemption:",
    "fixed_code": "def main(fixer_pkg, args=None):\\n    parser = optparse.OptionParser(usage=\"2to3 [options] file|dir ...\")\\n    parser.add_option(\"-d\", \"--doctests_only\", action=\"store_true\",\\n                      help=\"Fix up doctests only\")\\n    parser.add_option(\"-f\", \"--fix\", action=\"append\", default=[],\\n                      help=\"Each FIX specifies a transformation; default: all\")\\n    parser.add_option(\"-j\", \"--processes\", action=\"store\", default=1,\\n                      type=\"int\", help=\"Run 2to3 concurrently\")\\n    parser.add_option(\"-x\", \"--nofix\", action=\"append\", default=[],\\n                      help=\"Prevent a transformation from being run\")\\n    parser.add_option(\"-l\", \"--list-fixes\", action=\"store_true\",\\n                      help=\"List available transformations\")\\n    parser.add_option(\"-p\", \"--print-function\", action=\"store_true\",\\n                      help=\"Modify the grammar so that print() is a function\")\\n    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\",\\n                      help=\"More verbose logging\")\\n    parser.add_option(\"--no-diffs\", action=\"store_true\",\\n                      help=\"Don't show diffs of the refactoring\")\\n    parser.add_option(\"-w\", \"--write\", action=\"store_true\",\\n                      help=\"Write back modified files\")\\n    parser.add_option(\"-n\", \"--nobackups\", action=\"store_true\", default=False,\\n                      help=\"Don't write backups for modified files\")\\n    parser.add_option(\"-o\", \"--output-dir\", action=\"store\", type=\"str\",\\n                      default=\"\", help=\"Put output files in this directory \"\\n                      \"instead of overwriting the input files.  Requires -n.\")\\n    parser.add_option(\"-W\", \"--write-unchanged-files\", action=\"store_true\",\\n                      help=\"Also write files even if no changes were required\"\\n                      \" (useful with --output-dir); implies -w.\")\\n    parser.add_option(\"--add-suffix\", action=\"store\", type=\"str\", default=\"\",\\n                      help=\"Append this string to all output filenames.\"\\n                      \" Requires -n if non-empty.  \"\\n                      \"ex: --add-suffix='3' will generate .py3 files.\")\\n    refactor_stdin = False\\n    flags = {}\\n    options, args = parser.parse_args(args)\\n    if options.write_unchanged_files:\\n        flags[\"write_unchanged_files\"] = True\\n        if not options.write:\\n            warn(\"--write-unchanged-files/-W implies -w.\")\\n        options.write = True\\n    if options.output_dir and not options.nobackups:\\n        parser.error(\"Can't use --output-dir/-o without -n.\")\\n    if options.add_suffix and not options.nobackups:\\n        parser.error(\"Can't use --add-suffix without -n.\")\\n    if not options.write and options.no_diffs:\\n        warn(\"not writing files and not printing diffs; that's not very useful\")\\n    if not options.write and options.nobackups:\\n        parser.error(\"Can't use -n without -w\")\\n    if options.list_fixes:\\n        print \"Available transformations for the -f/--fix option:\"\\n        for fixname in refactor.get_all_fix_names(fixer_pkg):\\n            print fixname\\n        if not args:\\n            return 0\\n    if not args:\\n        print >> sys.stderr, \"At least one file or directory argument required.\"\\n        print >> sys.stderr, \"Use --help to show usage.\"\\n        return 2\\n    if \"-\" in args:\\n        refactor_stdin = True\\n        if options.write:\\n            print >> sys.stderr, \"Can't write to stdin.\"\\n            return 2\\n    if options.print_function:\\n        flags[\"print_function\"] = True\\n    level = logging.DEBUG if options.verbose else logging.INFO\\n    logging.basicConfig(format='%(name)s: %(message)s', level=level)\\n    logger = logging.getLogger('lib2to3.main')\\n    avail_fixes = set(refactor.get_fixers_from_package(fixer_pkg))\\n    unwanted_fixes = set(fixer_pkg + \".fix_\" + fix for fix in options.nofix)\\n    explicit = set()\\n    if options.fix:\\n        all_present = False\\n        for fix in options.fix:\\n            if fix == \"all\":\\n                all_present = True\\n            else:\\n                explicit.add(fixer_pkg + \".fix_\" + fix)\\n        requested = avail_fixes.union(explicit) if all_present else explicit\\n    else:\\n        requested = avail_fixes.union(explicit)\\n    fixer_names = requested.difference(unwanted_fixes)\\n    input_base_dir = os.path.commonprefix(args)\\n    if (input_base_dir and not input_base_dir.endswith(os.sep)\\n        and not os.path.isdir(input_base_dir)):\\n        input_base_dir = os.path.dirname(input_base_dir)\\n    if options.output_dir:\\n        input_base_dir = input_base_dir.rstrip(os.sep)\\n        logger.info('Output in %r will mirror the input directory %r layout.',\\n                    options.output_dir, input_base_dir)\\n    rt = StdoutRefactoringTool(\\n            sorted(fixer_names), flags, sorted(explicit),\\n            options.nobackups, not options.no_diffs,\\n            input_base_dir=input_base_dir,\\n            output_dir=options.output_dir,\\n            append_suffix=options.add_suffix)\\n    if not rt.errors:\\n        if refactor_stdin:\\n            rt.refactor_stdin()\\n        else:\\n            try:\\n                rt.refactor(args, options.write, options.doctests_only,\\n                            options.processes)\\n            except refactor.MultiprocessingUnsupported:\\n                assert options.processes > 1\\n                print >> sys.stderr, \"Sorry, -j isn't \" \\\\n                    \"supported on this platform.\"\\n                return 1\\n        rt.summarize()\\n    return int(bool(rt.errors))"
  },
  {
    "code": "def __str__(self) -> str:\\n        return (\\n            f\"Timetable class {self.type_string!r} is not registered or \"\\n            \"you have a top level database access that disrupted the session. \"\\n            \"Please check the airflow best practices documentation.\"\\n        )",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def __str__(self) -> str:\\n        return (\\n            f\"Timetable class {self.type_string!r} is not registered or \"\\n            \"you have a top level database access that disrupted the session. \"\\n            \"Please check the airflow best practices documentation.\"\\n        )"
  },
  {
    "code": "def _eval_type(t, globalns, localns, recursive_guard=frozenset()):\\n    if isinstance(t, ForwardRef):\\n        return t._evaluate(globalns, localns, recursive_guard)\\n    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):\\n        if isinstance(t, GenericAlias):\\n            args = tuple(\\n                ForwardRef(arg) if isinstance(arg, str) else arg\\n                for arg in t.__args__\\n            )\\n            if _should_unflatten_callable_args(t, args):\\n                t = t.__origin__[(args[:-1], args[-1])]\\n            else:\\n                t = t.__origin__[args]\\n        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\\n        if ev_args == t.__args__:\\n            return t\\n        if isinstance(t, GenericAlias):\\n            return GenericAlias(t.__origin__, ev_args)\\n        if isinstance(t, types.UnionType):\\n            return functools.reduce(operator.or_, ev_args)\\n        else:\\n            return t.copy_with(ev_args)\\n    return t",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _eval_type(t, globalns, localns, recursive_guard=frozenset()):\\n    if isinstance(t, ForwardRef):\\n        return t._evaluate(globalns, localns, recursive_guard)\\n    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):\\n        if isinstance(t, GenericAlias):\\n            args = tuple(\\n                ForwardRef(arg) if isinstance(arg, str) else arg\\n                for arg in t.__args__\\n            )\\n            if _should_unflatten_callable_args(t, args):\\n                t = t.__origin__[(args[:-1], args[-1])]\\n            else:\\n                t = t.__origin__[args]\\n        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\\n        if ev_args == t.__args__:\\n            return t\\n        if isinstance(t, GenericAlias):\\n            return GenericAlias(t.__origin__, ev_args)\\n        if isinstance(t, types.UnionType):\\n            return functools.reduce(operator.or_, ev_args)\\n        else:\\n            return t.copy_with(ev_args)\\n    return t"
  },
  {
    "code": "def post(self):\\n\\t\\ttyped_password = self.get_argument('password', default=u'')\\n\\t\\tnew_password = self.get_argument('new_password', default=u'')\\n\\t\\tif self.get_login_available(self.settings):\\n\\t\\t\\tif self.passwd_check(self.hashed_password, typed_password) and not new_password:\\n\\t\\t\\t\\tself.set_login_cookie(self, uuid.uuid4().hex)\\n\\t\\t\\telif self.token and self.token == typed_password:\\n\\t\\t\\t\\tself.set_login_cookie(self, uuid.uuid4().hex)\\n\\t\\t\\t\\tif new_password and self.settings.get('allow_password_change'):\\n\\t\\t\\t\\t\\tconfig_dir = self.settings.get('config_dir')\\n\\t\\t\\t\\t\\tconfig_file = os.path.join(config_dir, 'jupyter_server_config.json')\\n\\t\\t\\t\\t\\tset_password(new_password, config_file=config_file)\\n\\t\\t\\t\\t\\tself.log.info(\"Wrote hashed password to %s\" % config_file)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.set_status(401)\\n\\t\\t\\t\\tself._render(message={'error': 'Invalid credentials'})\\n\\t\\t\\t\\treturn",
    "label": 1,
    "bug_type": "security",
    "bug_description": "sync _redirect_safe with upstream",
    "fixed_code": "def post(self):\\n\\t\\ttyped_password = self.get_argument('password', default=u'')\\n\\t\\tnew_password = self.get_argument('new_password', default=u'')\\n\\t\\tif self.get_login_available(self.settings):\\n\\t\\t\\tif self.passwd_check(self.hashed_password, typed_password) and not new_password:\\n\\t\\t\\t\\tself.set_login_cookie(self, uuid.uuid4().hex)\\n\\t\\t\\telif self.token and self.token == typed_password:\\n\\t\\t\\t\\tself.set_login_cookie(self, uuid.uuid4().hex)\\n\\t\\t\\t\\tif new_password and self.settings.get(\"allow_password_change\"):\\n\\t\\t\\t\\t\\tconfig_dir = self.settings.get(\"config_dir\")\\n\\t\\t\\t\\t\\tconfig_file = os.path.join(\\n\\t\\t\\t\\t\\t\\tconfig_dir, \"jupyter_notebook_config.json\"\\n\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\tset_password(new_password, config_file=config_file)\\n\\t\\t\\t\\t\\tself.log.info(\"Wrote hashed password to %s\" % config_file)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.set_status(401)\\n\\t\\t\\t\\tself._render(message={'error': 'Invalid credentials'})\\n\\t\\t\\t\\treturn"
  },
  {
    "code": "def _selection_list(self):\\n        if not isinstance(self._selection, (list, tuple, ABCSeries,\\n                                            ABCIndexClass, np.ndarray)):\\n            return [self._selection]\\n        return self._selection\\n    @cache_readonly",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _selection_list(self):\\n        if not isinstance(self._selection, (list, tuple, ABCSeries,\\n                                            ABCIndexClass, np.ndarray)):\\n            return [self._selection]\\n        return self._selection\\n    @cache_readonly"
  },
  {
    "code": "def _partial_tup_index(self, tup, side='left'):\\n        if len(tup) > self.lexsort_depth:\\n            raise KeyError('MultiIndex lexsort depth %d, key was length %d' %\\n                           (self.lexsort_depth, len(tup)))\\n        n = len(tup)\\n        start, end = 0, len(self)\\n        zipped = zip(tup, self.levels, self.labels)\\n        for k, (lab, lev, labs) in enumerate(zipped):\\n            section = labs[start:end]\\n            if lab not in lev:\\n                if not lev.is_type_compatible(lib.infer_dtype([lab])):\\n                    raise Exception('Level type mismatch: %s' % lab)\\n                loc = lev.searchsorted(lab, side=side)\\n                if side == 'right' and loc >= 0:\\n                    loc -= 1\\n                return start + section.searchsorted(loc, side=side)\\n            idx = lev.get_loc(lab)\\n            if k < n - 1:\\n                end = start + section.searchsorted(idx, side='right')\\n                start = start + section.searchsorted(idx, side='left')\\n            else:\\n                return start + section.searchsorted(idx, side=side)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: Make core/index exceptions more descriptive\\n\\n  produce copies that are not the same object. (uses `assert_almost_equal`\\n  under the hood).\\n  after iterable check)",
    "fixed_code": "def _partial_tup_index(self, tup, side='left'):\\n        if len(tup) > self.lexsort_depth:\\n            raise KeyError('Key length (%d) was greater than MultiIndex'\\n                           ' lexsort depth (%d)' %\\n                           (len(tup), self.lexsort_depth))\\n        n = len(tup)\\n        start, end = 0, len(self)\\n        zipped = zip(tup, self.levels, self.labels)\\n        for k, (lab, lev, labs) in enumerate(zipped):\\n            section = labs[start:end]\\n            if lab not in lev:\\n                if not lev.is_type_compatible(lib.infer_dtype([lab])):\\n                    raise TypeError('Level type mismatch: %s' % lab)\\n                loc = lev.searchsorted(lab, side=side)\\n                if side == 'right' and loc >= 0:\\n                    loc -= 1\\n                return start + section.searchsorted(loc, side=side)\\n            idx = lev.get_loc(lab)\\n            if k < n - 1:\\n                end = start + section.searchsorted(idx, side='right')\\n                start = start + section.searchsorted(idx, side='left')\\n            else:\\n                return start + section.searchsorted(idx, side=side)"
  },
  {
    "code": "def pandasSQL_builder(con, flavor=None, meta=None):\\n    try:\\n        import sqlalchemy\\n        if isinstance(con, sqlalchemy.engine.Engine):\\n            return PandasSQLAlchemy(con, meta=meta)\\n        else:\\n            warnings.warn(\"Not an SQLAlchemy engine, \"\\n                          \"attempting to use as legacy DBAPI connection\")\\n            if flavor is None:\\n                raise ValueError(\\n                    \"PandasSQL must be created with an SQLAlchemy engine \"\\n                    \"or a DBAPI2 connection and SQL flavor\")\\n            else:\\n                return PandasSQLLegacy(con, flavor)\\n    except ImportError:\\n        warnings.warn(\"SQLAlchemy not installed, using legacy mode\")\\n        if flavor is None:\\n            raise SQLAlchemyRequired\\n        else:\\n            return PandasSQLLegacy(con, flavor)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "FIX: execute() works on a cursor.",
    "fixed_code": "def pandasSQL_builder(con, flavor=None, meta=None, is_cursor=False):\\n    try:\\n        import sqlalchemy\\n        if isinstance(con, sqlalchemy.engine.Engine):\\n            return PandasSQLAlchemy(con, meta=meta)\\n        else:\\n            warnings.warn(\"Not an SQLAlchemy engine, \"\\n                          \"attempting to use as legacy DBAPI connection\")\\n            if flavor is None:\\n                raise ValueError(\\n                    \"PandasSQL must be created with an SQLAlchemy engine \"\\n                    \"or a DBAPI2 connection and SQL flavor\")\\n            else:\\n                return PandasSQLLegacy(con, flavor, is_cursor=is_cursor)\\n    except ImportError:\\n        warnings.warn(\"SQLAlchemy not installed, using legacy mode\")\\n        if flavor is None:\\n            raise SQLAlchemyRequired\\n        else:\\n            return PandasSQLLegacy(con, flavor, is_cursor=is_cursor)"
  },
  {
    "code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n              parse_dates=False, date_parser=None, na_values=None,\\n              chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: parsing numbers with commas in read_csv/table/clipboard/fwf #796 VB: read_csv with/without thousands separator parsing",
    "fixed_code": "def _parse_xlsx(self, sheetname, header=0, skiprows=None, index_col=None,\\n                    parse_dates=False, date_parser=None, na_values=None,\\n                    thousands=None, chunksize=None):\\n        sheet = self.book.get_sheet_by_name(name=sheetname)\\n        data = []\\n        for row in sheet.iter_rows():\\n            data.append([cell.internal_value for cell in row])\\n        if header is not None:\\n            data[header] = _trim_excel_header(data[header])\\n        parser = TextParser(data, header=header, index_col=index_col,\\n                            na_values=na_values,\\n                            thousands=thousands,\\n                            parse_dates=parse_dates,\\n                            date_parser=date_parser,\\n                            skiprows=skiprows,\\n                            chunksize=chunksize)\\n        return parser.get_chunk()"
  },
  {
    "code": "def ensure_timezone(self):\\n        if not self.is_usable():\\n            return False\\n        conn_timezone_name = self.connection.get_parameter_status('TimeZone')\\n        timezone_name = self.timezone_name\\n        if timezone_name and conn_timezone_name != timezone_name:\\n            with self.connection.cursor() as cursor:\\n                cursor.execute(self.ops.set_time_zone_sql(), [timezone_name])\\n            return True\\n        return False",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #30193, Refs #28478 -- Avoided PostgreSQL connection health checks on initialization.\\n\\nThis addressed a regression introduced by a96b9019320ed8236659ee520a7a017c1bafbc6f as identified by Ran Benita.",
    "fixed_code": "def ensure_timezone(self):\\n        if self.connection is None:\\n            return False\\n        conn_timezone_name = self.connection.get_parameter_status('TimeZone')\\n        timezone_name = self.timezone_name\\n        if timezone_name and conn_timezone_name != timezone_name:\\n            with self.connection.cursor() as cursor:\\n                cursor.execute(self.ops.set_time_zone_sql(), [timezone_name])\\n            return True\\n        return False"
  },
  {
    "code": "def replace(self, to_replace=None, value=None, inplace=False, limit=None,\\n                regex=False, infer_types=False, method=None, axis=None):\\n        if not isinstance(regex, bool) and to_replace is not None:\\n            raise AssertionError(\"'to_replace' must be 'None' if 'regex' is \"\\n                                 \"not a bool\")\\n        if method is not None:\\n            from warnings import warn\\n            warn('the \"method\" argument is deprecated and will be removed in'\\n                 'v0.12; this argument has no effect')\\n        if axis is not None:\\n            from warnings import warn\\n            warn('the \"axis\" argument is deprecated and will be removed in'\\n                 'v0.12; this argument has no effect')\\n        self._consolidate_inplace()\\n        if value is None:\\n            if not isinstance(to_replace, (dict, Series)):\\n                if not isinstance(regex, (dict, Series)):\\n                    raise TypeError('If \"to_replace\" and \"value\" are both None'\\n                                    ' then regex must be a mapping')\\n                to_replace = regex\\n                regex = True\\n            items = to_replace.items()\\n            keys, values = itertools.izip(*items)\\n            are_mappings = [isinstance(v, (dict, Series)) for v in values]\\n            if any(are_mappings):\\n                if not all(are_mappings):\\n                    raise TypeError(\"If a nested mapping is passed, all values\"\\n                                    \" of the top level mapping must be \"\\n                                    \"mappings\")\\n                to_rep_dict = {}\\n                value_dict = {}\\n                for k, v in items:\\n                    to_rep_dict[k] = v.keys()\\n                    value_dict[k] = v.values()\\n                to_replace, value = to_rep_dict, value_dict\\n            else:\\n                to_replace, value = keys, values\\n            return self.replace(to_replace, value, inplace=inplace,\\n                                limit=limit, regex=regex,\\n                                infer_types=infer_types)\\n        else:\\n            if not len(self.columns):\\n                return self\\n            new_data = self._data\\n            if isinstance(to_replace, (dict, Series)):\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for c, src in to_replace.iteritems():\\n                        if c in value and c in self:\\n                            new_data = new_data.replace(src, value[c],\\n                                                        filter=[c],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data\\n                    for k, src in to_replace.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(src, value,\\n                                                        filter=[k],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                else:\\n                    raise TypeError('Fill value must be scalar, dict, or '\\n                                    'Series')\\n            elif isinstance(to_replace, (list, np.ndarray)):\\n                if isinstance(value, (list, np.ndarray)):\\n                    if len(to_replace) != len(value):\\n                        raise ValueError('Replacement lists must match '\\n                                         'in length. Expecting %d got %d ' %\\n                                         (len(to_replace), len(value)))\\n                    new_data = self._data.replace_list(to_replace, value,\\n                                                       inplace=inplace,\\n                                                       regex=regex)\\n                else:  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n            elif to_replace is None:\\n                if not (_re_compilable(regex) or\\n                        isinstance(regex, (list, dict, np.ndarray, Series))):\\n                    raise TypeError(\"'regex' must be a string or a compiled \"\\n                                    \"regular expression or a list or dict of \"\\n                                    \"strings or regular expressions, you \"\\n                                    \"passed a {0}\".format(type(regex)))\\n                return self.replace(regex, value, inplace=inplace, limit=limit,\\n                                    regex=True, infer_types=infer_types)\\n            else:\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for k, v in value.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(to_replace, v,\\n                                                        filter=[k],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n                else:\\n                    raise TypeError('Invalid \"to_replace\" type: '\\n                                    '{0}'.format(type(to_replace)))  \\n        if infer_types:\\n            new_data = new_data.convert()\\n        if inplace:\\n            self._data = new_data\\n        else:\\n            return self._constructor(new_data)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def replace(self, to_replace=None, value=None, inplace=False, limit=None,\\n                regex=False, infer_types=False, method=None, axis=None):\\n        if not isinstance(regex, bool) and to_replace is not None:\\n            raise AssertionError(\"'to_replace' must be 'None' if 'regex' is \"\\n                                 \"not a bool\")\\n        if method is not None:\\n            from warnings import warn\\n            warn('the \"method\" argument is deprecated and will be removed in'\\n                 'v0.12; this argument has no effect')\\n        if axis is not None:\\n            from warnings import warn\\n            warn('the \"axis\" argument is deprecated and will be removed in'\\n                 'v0.12; this argument has no effect')\\n        self._consolidate_inplace()\\n        if value is None:\\n            if not isinstance(to_replace, (dict, Series)):\\n                if not isinstance(regex, (dict, Series)):\\n                    raise TypeError('If \"to_replace\" and \"value\" are both None'\\n                                    ' then regex must be a mapping')\\n                to_replace = regex\\n                regex = True\\n            items = to_replace.items()\\n            keys, values = itertools.izip(*items)\\n            are_mappings = [isinstance(v, (dict, Series)) for v in values]\\n            if any(are_mappings):\\n                if not all(are_mappings):\\n                    raise TypeError(\"If a nested mapping is passed, all values\"\\n                                    \" of the top level mapping must be \"\\n                                    \"mappings\")\\n                to_rep_dict = {}\\n                value_dict = {}\\n                for k, v in items:\\n                    to_rep_dict[k] = v.keys()\\n                    value_dict[k] = v.values()\\n                to_replace, value = to_rep_dict, value_dict\\n            else:\\n                to_replace, value = keys, values\\n            return self.replace(to_replace, value, inplace=inplace,\\n                                limit=limit, regex=regex,\\n                                infer_types=infer_types)\\n        else:\\n            if not len(self.columns):\\n                return self\\n            new_data = self._data\\n            if isinstance(to_replace, (dict, Series)):\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for c, src in to_replace.iteritems():\\n                        if c in value and c in self:\\n                            new_data = new_data.replace(src, value[c],\\n                                                        filter=[c],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data\\n                    for k, src in to_replace.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(src, value,\\n                                                        filter=[k],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                else:\\n                    raise TypeError('Fill value must be scalar, dict, or '\\n                                    'Series')\\n            elif isinstance(to_replace, (list, np.ndarray)):\\n                if isinstance(value, (list, np.ndarray)):\\n                    if len(to_replace) != len(value):\\n                        raise ValueError('Replacement lists must match '\\n                                         'in length. Expecting %d got %d ' %\\n                                         (len(to_replace), len(value)))\\n                    new_data = self._data.replace_list(to_replace, value,\\n                                                       inplace=inplace,\\n                                                       regex=regex)\\n                else:  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n            elif to_replace is None:\\n                if not (_re_compilable(regex) or\\n                        isinstance(regex, (list, dict, np.ndarray, Series))):\\n                    raise TypeError(\"'regex' must be a string or a compiled \"\\n                                    \"regular expression or a list or dict of \"\\n                                    \"strings or regular expressions, you \"\\n                                    \"passed a {0}\".format(type(regex)))\\n                return self.replace(regex, value, inplace=inplace, limit=limit,\\n                                    regex=True, infer_types=infer_types)\\n            else:\\n                if isinstance(value, (dict, Series)):  \\n                    new_data = self._data\\n                    for k, v in value.iteritems():\\n                        if k in self:\\n                            new_data = new_data.replace(to_replace, v,\\n                                                        filter=[k],\\n                                                        inplace=inplace,\\n                                                        regex=regex)\\n                elif not isinstance(value, (list, np.ndarray)):  \\n                    new_data = self._data.replace(to_replace, value,\\n                                                  inplace=inplace, regex=regex)\\n                else:\\n                    raise TypeError('Invalid \"to_replace\" type: '\\n                                    '{0}'.format(type(to_replace)))  \\n        if infer_types:\\n            new_data = new_data.convert()\\n        if inplace:\\n            self._data = new_data\\n        else:\\n            return self._constructor(new_data)"
  },
  {
    "code": "def __init__(self, instream=None, infile=None, posix=False):\\n        if isinstance(instream, str):\\n            instream = StringIO(instream)\\n        if instream is not None:\\n            self.instream = instream\\n            self.infile = infile\\n        else:\\n            self.instream = sys.stdin\\n            self.infile = None\\n        self.posix = posix\\n        if posix:\\n            self.eof = None\\n        else:\\n            self.eof = ''\\n        self.commenters = '\\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\\n        if self.posix:\\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\\n        self.whitespace = ' \\t\\r\\n'\\n        self.whitespace_split = False\\n        self.quotes = '\\'\"'\\n        self.escape = '\\\\'\\n        self.escapedquotes = '\"'\\n        self.state = ' '\\n        self.pushback = deque()\\n        self.lineno = 1\\n        self.debug = 0\\n        self.token = ''\\n        self.filestack = deque()\\n        self.source = None",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Closes #1521950: Made shlex parsing more shell-like.",
    "fixed_code": "def __init__(self, instream=None, infile=None, posix=False,\\n                 punctuation_chars=False):\\n        if isinstance(instream, str):\\n            instream = StringIO(instream)\\n        if instream is not None:\\n            self.instream = instream\\n            self.infile = infile\\n        else:\\n            self.instream = sys.stdin\\n            self.infile = None\\n        self.posix = posix\\n        if posix:\\n            self.eof = None\\n        else:\\n            self.eof = ''\\n        self.commenters = '\\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\\n        if self.posix:\\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\\n        self.whitespace = ' \\t\\r\\n'\\n        self.whitespace_split = False\\n        self.quotes = '\\'\"'\\n        self.escape = '\\\\'\\n        self.escapedquotes = '\"'\\n        self.state = ' '\\n        self.pushback = deque()\\n        self.lineno = 1\\n        self.debug = 0\\n        self.token = ''\\n        self.filestack = deque()\\n        self.source = None\\n        if not punctuation_chars:\\n            punctuation_chars = ''\\n        elif punctuation_chars is True:\\n            punctuation_chars = '();<>|&'\\n        self.punctuation_chars = punctuation_chars\\n        if punctuation_chars:\\n            self._pushback_chars = deque()\\n            self.wordchars += '~-./*?='\\n            t = self.wordchars.maketrans(dict.fromkeys(punctuation_chars))\\n            self.wordchars = self.wordchars.translate(t)"
  },
  {
    "code": "def _convert_index(index, encoding=None):\\n    index_name = getattr(index,'name',None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(index.values, 'integer', atom, freq=getattr(index,'freq',None),\\n                        index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                               v.microsecond / 1E6) for v in values],\\n                             dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                             dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(converted, 'string', _tables().StringCol(itemsize), itemsize=itemsize,\\n                        index_name=index_name)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _convert_index(index, encoding=None):\\n    index_name = getattr(index,'name',None)\\n    if isinstance(index, DatetimeIndex):\\n        converted = index.asi8\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif isinstance(index, (Int64Index, PeriodIndex)):\\n        atom = _tables().Int64Col()\\n        return IndexCol(index.values, 'integer', atom, freq=getattr(index,'freq',None),\\n                        index_name=index_name)\\n    if isinstance(index, MultiIndex):\\n        raise Exception('MultiIndex not supported here!')\\n    inferred_type = lib.infer_dtype(index)\\n    values = np.asarray(index)\\n    if inferred_type == 'datetime64':\\n        converted = values.view('i8')\\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\\n                        freq=getattr(index,'freq',None), tz=getattr(index,'tz',None),\\n                        index_name=index_name)\\n    elif inferred_type == 'datetime':\\n        converted = np.array([(time.mktime(v.timetuple()) +\\n                               v.microsecond / 1E6) for v in values],\\n                             dtype=np.float64)\\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'date':\\n        converted = np.array([time.mktime(v.timetuple()) for v in values],\\n                             dtype=np.int32)\\n        return IndexCol(converted, 'date', _tables().Time32Col(),\\n                        index_name=index_name)\\n    elif inferred_type == 'string':\\n        converted = _convert_string_array(values, encoding)\\n        itemsize = converted.dtype.itemsize\\n        return IndexCol(converted, 'string', _tables().StringCol(itemsize), itemsize=itemsize,\\n                        index_name=index_name)\\n    elif inferred_type == 'unicode':\\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'integer':\\n        atom = _tables().Int64Col()\\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\\n                        index_name=index_name)\\n    elif inferred_type == 'floating':\\n        atom = _tables().Float64Col()\\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\\n                        index_name=index_name)\\n    else:  \\n        atom = _tables().ObjectAtom()\\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\\n                        index_name=index_name)"
  },
  {
    "code": "def changelist_view(self, request, extra_context=None):\\n        \"The 'change list' admin view for this model.\"\\n        from django.contrib.admin.views.main import ERROR_FLAG\\n        opts = self.model._meta\\n        app_label = opts.app_label\\n        if not self.has_change_permission(request, None):\\n            raise PermissionDenied\\n        actions = self.get_actions(request)\\n        list_display = list(self.list_display)\\n        if not actions:\\n            try:\\n                list_display.remove('action_checkbox')\\n            except ValueError:\\n                pass\\n        ChangeList = self.get_changelist(request)\\n        try:\\n            cl = ChangeList(request, self.model, list_display, self.list_display_links, self.list_filter,\\n                self.date_hierarchy, self.search_fields, self.list_select_related, self.list_per_page, self.list_editable, self)\\n        except IncorrectLookupParameters:\\n            if ERROR_FLAG in request.GET.keys():\\n                return render_to_response('admin/invalid_setup.html', {'title': _('Database error')})\\n            return HttpResponseRedirect(request.path + '?' + ERROR_FLAG + '=1')\\n        if actions and request.method == 'POST':\\n            response = self.response_action(request, queryset=cl.get_query_set())\\n            if response:\\n                return response\\n        formset = cl.formset = None\\n        if request.method == \"POST\" and self.list_editable:\\n            FormSet = self.get_changelist_formset(request)\\n            formset = cl.formset = FormSet(request.POST, request.FILES, queryset=cl.result_list)\\n            if formset.is_valid():\\n                changecount = 0\\n                for form in formset.forms:\\n                    if form.has_changed():\\n                        obj = self.save_form(request, form, change=True)\\n                        self.save_model(request, obj, form, change=True)\\n                        form.save_m2m()\\n                        change_msg = self.construct_change_message(request, form, None)\\n                        self.log_change(request, obj, change_msg)\\n                        changecount += 1\\n                if changecount:\\n                    if changecount == 1:\\n                        name = force_unicode(opts.verbose_name)\\n                    else:\\n                        name = force_unicode(opts.verbose_name_plural)\\n                    msg = ungettext(\"%(count)s %(name)s was changed successfully.\",\\n                                    \"%(count)s %(name)s were changed successfully.\",\\n                                    changecount) % {'count': changecount,\\n                                                    'name': name,\\n                                                    'obj': force_unicode(obj)}\\n                    self.message_user(request, msg)\\n                return HttpResponseRedirect(request.get_full_path())\\n        elif self.list_editable:\\n            FormSet = self.get_changelist_formset(request)\\n            formset = cl.formset = FormSet(queryset=cl.result_list)\\n        if formset:\\n            media = self.media + formset.media\\n        else:\\n            media = self.media\\n        if actions:\\n            action_form = self.action_form(auto_id=None)\\n            action_form.fields['action'].choices = self.get_action_choices(request)\\n        else:\\n            action_form = None\\n        context = {\\n            'title': cl.title,\\n            'is_popup': cl.is_popup,\\n            'cl': cl,\\n            'media': media,\\n            'has_add_permission': self.has_add_permission(request),\\n            'root_path': self.admin_site.root_path,\\n            'app_label': app_label,\\n            'action_form': action_form,\\n            'actions_on_top': self.actions_on_top,\\n            'actions_on_bottom': self.actions_on_bottom,\\n        }\\n        context.update(extra_context or {})\\n        context_instance = template.RequestContext(request, current_app=self.admin_site.name)\\n        return render_to_response(self.change_list_template or [\\n            'admin/%s/%s/change_list.html' % (app_label, opts.object_name.lower()),\\n            'admin/%s/change_list.html' % app_label,\\n            'admin/change_list.html'\\n        ], context, context_instance=context_instance)\\n    @csrf_protect",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def changelist_view(self, request, extra_context=None):\\n        \"The 'change list' admin view for this model.\"\\n        from django.contrib.admin.views.main import ERROR_FLAG\\n        opts = self.model._meta\\n        app_label = opts.app_label\\n        if not self.has_change_permission(request, None):\\n            raise PermissionDenied\\n        actions = self.get_actions(request)\\n        list_display = list(self.list_display)\\n        if not actions:\\n            try:\\n                list_display.remove('action_checkbox')\\n            except ValueError:\\n                pass\\n        ChangeList = self.get_changelist(request)\\n        try:\\n            cl = ChangeList(request, self.model, list_display, self.list_display_links, self.list_filter,\\n                self.date_hierarchy, self.search_fields, self.list_select_related, self.list_per_page, self.list_editable, self)\\n        except IncorrectLookupParameters:\\n            if ERROR_FLAG in request.GET.keys():\\n                return render_to_response('admin/invalid_setup.html', {'title': _('Database error')})\\n            return HttpResponseRedirect(request.path + '?' + ERROR_FLAG + '=1')\\n        if actions and request.method == 'POST':\\n            response = self.response_action(request, queryset=cl.get_query_set())\\n            if response:\\n                return response\\n        formset = cl.formset = None\\n        if request.method == \"POST\" and self.list_editable:\\n            FormSet = self.get_changelist_formset(request)\\n            formset = cl.formset = FormSet(request.POST, request.FILES, queryset=cl.result_list)\\n            if formset.is_valid():\\n                changecount = 0\\n                for form in formset.forms:\\n                    if form.has_changed():\\n                        obj = self.save_form(request, form, change=True)\\n                        self.save_model(request, obj, form, change=True)\\n                        form.save_m2m()\\n                        change_msg = self.construct_change_message(request, form, None)\\n                        self.log_change(request, obj, change_msg)\\n                        changecount += 1\\n                if changecount:\\n                    if changecount == 1:\\n                        name = force_unicode(opts.verbose_name)\\n                    else:\\n                        name = force_unicode(opts.verbose_name_plural)\\n                    msg = ungettext(\"%(count)s %(name)s was changed successfully.\",\\n                                    \"%(count)s %(name)s were changed successfully.\",\\n                                    changecount) % {'count': changecount,\\n                                                    'name': name,\\n                                                    'obj': force_unicode(obj)}\\n                    self.message_user(request, msg)\\n                return HttpResponseRedirect(request.get_full_path())\\n        elif self.list_editable:\\n            FormSet = self.get_changelist_formset(request)\\n            formset = cl.formset = FormSet(queryset=cl.result_list)\\n        if formset:\\n            media = self.media + formset.media\\n        else:\\n            media = self.media\\n        if actions:\\n            action_form = self.action_form(auto_id=None)\\n            action_form.fields['action'].choices = self.get_action_choices(request)\\n        else:\\n            action_form = None\\n        context = {\\n            'title': cl.title,\\n            'is_popup': cl.is_popup,\\n            'cl': cl,\\n            'media': media,\\n            'has_add_permission': self.has_add_permission(request),\\n            'root_path': self.admin_site.root_path,\\n            'app_label': app_label,\\n            'action_form': action_form,\\n            'actions_on_top': self.actions_on_top,\\n            'actions_on_bottom': self.actions_on_bottom,\\n        }\\n        context.update(extra_context or {})\\n        context_instance = template.RequestContext(request, current_app=self.admin_site.name)\\n        return render_to_response(self.change_list_template or [\\n            'admin/%s/%s/change_list.html' % (app_label, opts.object_name.lower()),\\n            'admin/%s/change_list.html' % app_label,\\n            'admin/change_list.html'\\n        ], context, context_instance=context_instance)\\n    @csrf_protect"
  },
  {
    "code": "def _get_handle(path, mode, encoding=None):\\n    if py3compat.PY3:  \\n        if encoding:\\n            f = open(path, mode, encoding=encoding)\\n        else:\\n            f = open(path, mode, errors='replace')\\n    else:\\n        f = open(path, mode)\\n        if encoding is not None and 'r' in mode:\\n            f = UTF8Recoder(f, encoding)\\n    return f\\nif py3compat.PY3:",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: move utf8 recoding elsewhere, add test case to deal with #2298",
    "fixed_code": "def _get_handle(path, mode, encoding=None):\\n    if py3compat.PY3:  \\n        if encoding:\\n            f = open(path, mode, encoding=encoding)\\n        else:\\n            f = open(path, mode, errors='replace')\\n    else:\\n        f = open(path, mode)\\n    return f\\nif py3compat.PY3:"
  },
  {
    "code": "def main():\\n    spec = ArgumentSpec()\\n    module = AnsibleModule(\\n        argument_spec=spec.argument_spec,\\n        supports_check_mode=spec.supports_check_mode\\n    )\\n    client = F5RestClient(**module.params)\\n    try:\\n        mm = ModuleManager(module=module, client=client)\\n        results = mm.exec_module()\\n        cleanup_tokens(client)\\n        exit_json(module, results, client)\\n    except F5ModuleError as ex:\\n        cleanup_tokens(client)\\n        fail_json(module, ex, client)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactors main() function and module manager in multiple modules in line with recent changes (#53981)\\n\\nAdds variable types to docs\\nRefactors unit tests to remove deprecated parameters",
    "fixed_code": "def main():\\n    spec = ArgumentSpec()\\n    module = AnsibleModule(\\n        argument_spec=spec.argument_spec,\\n        supports_check_mode=spec.supports_check_mode\\n    )\\n    try:\\n        mm = ModuleManager(module=module)\\n        results = mm.exec_module()\\n        module.exit_json(**results)\\n    except F5ModuleError as ex:\\n        module.fail_json(msg=str(ex))"
  },
  {
    "code": "def _hash_fn(fields, globals):\\n    self_tuple = _tuple_str('self', fields)\\n    return _create_fn('__hash__',\\n                      ('self',),\\n                      [f'return hash({self_tuple})'],\\n                      globals=globals)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _hash_fn(fields, globals):\\n    self_tuple = _tuple_str('self', fields)\\n    return _create_fn('__hash__',\\n                      ('self',),\\n                      [f'return hash({self_tuple})'],\\n                      globals=globals)"
  },
  {
    "code": "def _align_method_SERIES(left, right, align_asobject=False):\\n    if isinstance(right, ABCSeries):\\n        if not left.index.equals(right.index):\\n            if align_asobject:\\n                left = left.astype(object)\\n                right = right.astype(object)\\n            left, right = left.align(right, copy=False)\\n    return left, right",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _align_method_SERIES(left, right, align_asobject=False):\\n    if isinstance(right, ABCSeries):\\n        if not left.index.equals(right.index):\\n            if align_asobject:\\n                left = left.astype(object)\\n                right = right.astype(object)\\n            left, right = left.align(right, copy=False)\\n    return left, right"
  },
  {
    "code": "def set_name(self, n):\\n        self.name = n\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refactor of PyTables support to allow multiple table types.\\n\\n  This commit allows for support of multiple table types in a pytables hdf file,\\n  supporting the existing infrastructure in a backwards compatible manner (LegacyTable)\\n  while extending to a slightly modified format to support AppendableTables and future support of WORMTables\\n\\n  AppendableTables are implementations of the current table format with two enhancements:\\n     1) mixed dtype support\\n     2) writing routines in cython for enhanced performance\\n\\n  WORMTables (not implemented - but pretty straightforward)\\n    these tables can support a fixed 'table' (meaning not-appendable), that is searchable via queries\\n    this would have greatly enhanced write performance compared with AppendableTables, and a similar read performance profile\\n\\n  In addition, the tables allow for arbitrary axes to be indexed (e.g. you could save a panel that allows indexing on major_axis,minor_axis AND items),\\n    so all dimensions are queryable (currently only major/minor axes allow this query)\\n\\n  all tests pass (with 1 exception)\\n    a frame table round-trip - we fail on a comparison of a sorted index of the frame vs the index of the table (which is as written), not sure why this should be the case?",
    "fixed_code": "def set_name(self, name, kind_attr = None):\\n        self.name      = name\\n        self.kind_attr = kind_attr or \"%s_kind\" % name\\n        if self.cname is None:\\n            self.cname = name\\n        return self"
  },
  {
    "code": "def __setitem__(self, key, item):\\n            self.putenv(key, item)\\n            self.data[self.keymap(key)] = item",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Following an idea by Ron Adam, make sure keys and values in the environ dict are strings (in particular, not 8-bit strings).",
    "fixed_code": "def __setitem__(self, key, value):\\n            self.putenv(key, str(value))\\n            self.data[self.keymap(key)] = str(value)"
  },
  {
    "code": "def _instance_type_get_query(context, session=None, read_deleted=None):\\n\\treturn model_query(context, models.InstanceTypes, session=session,\\n\\t\\t\\t\\t\\t   read_deleted=read_deleted).\\\\n\\t\\t\\t\\t\\toptions(joinedload('extra_specs'))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Make flavors is_public option actually work\\n\\nWhen you create a flavor, you can set an is_public flag to be True or\\nFalse.  It is True by default.  When False, the intention is that the\\nflavor is only accessible by an admin, unless you use the flavor_access\\nAPI extension to grant access to specific tenants.\\n\\nUnfortunately, the only place in the code where this was being enforced\\nwas when listing flavors through the API.  It would filter out the\\nnon-public ones for a non-admin.  Otherwise, the flavor was accessible.\\nYou could get the details, and you could boot an instance with it, if\\nyou figured out a valid flavor ID.\\n\\nThis patch adds enforcement down in the db layer.  It also fixes one\\nplace in the API where the context wasn't passed down to enable the\\nenforcement to happen.\\n\\nFix bug 1194093.",
    "fixed_code": "def _instance_type_get_query(context, session=None, read_deleted=None):\\n\\tquery = model_query(context, models.InstanceTypes, session=session,\\n\\t\\t\\t\\t\\t   read_deleted=read_deleted).\\\\n\\t\\t\\t\\t\\t   options(joinedload('extra_specs'))\\n\\tif not context.is_admin:\\n\\t\\tthe_filter = [models.InstanceTypes.is_public == True]\\n\\t\\tthe_filter.extend([\\n\\t\\t\\tmodels.InstanceTypes.projects.any(project_id=context.project_id)\\n\\t\\t])\\n\\t\\tquery = query.filter(or_(*the_filter))\\n\\treturn query"
  },
  {
    "code": "def formatMessage(self, record):\\n        style = self._style\\n        if style == '%':\\n            s = self._fmt % record.__dict__\\n        elif style == '{':\\n            s = self._fmt.format(**record.__dict__)\\n        else:\\n            from string import Template\\n            s = Template(self._fmt).substitute(**record.__dict__)\\n        return s",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "logging: Improved Formatter implementation.",
    "fixed_code": "def formatMessage(self, record):\\n        return self._style.format(record)"
  },
  {
    "code": "def get_select2_language():\\n    lang_code = get_language()\\n    supported_code = SELECT2_TRANSLATIONS.get(lang_code)\\n    if supported_code is None and lang_code is not None:\\n        i = None\\n        while (i := lang_code.rfind(\"-\", 0, i)) > -1:\\n            if supported_code := SELECT2_TRANSLATIONS.get(lang_code[:i]):\\n                return supported_code\\n    return supported_code",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_select2_language():\\n    lang_code = get_language()\\n    supported_code = SELECT2_TRANSLATIONS.get(lang_code)\\n    if supported_code is None and lang_code is not None:\\n        i = None\\n        while (i := lang_code.rfind(\"-\", 0, i)) > -1:\\n            if supported_code := SELECT2_TRANSLATIONS.get(lang_code[:i]):\\n                return supported_code\\n    return supported_code"
  },
  {
    "code": "def combine_expression(self, connector, sub_expressions):\\n        if connector == '^':\\n            return 'POW(%s)' % ','.join(sub_expressions)\\n        elif connector in ('&', '|'):\\n            return 'CONVERT(%s, SIGNED)' % connector.join(sub_expressions)\\n        return super(DatabaseOperations, self).combine_expression(connector, sub_expressions)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def combine_expression(self, connector, sub_expressions):\\n        if connector == '^':\\n            return 'POW(%s)' % ','.join(sub_expressions)\\n        elif connector in ('&', '|'):\\n            return 'CONVERT(%s, SIGNED)' % connector.join(sub_expressions)\\n        return super(DatabaseOperations, self).combine_expression(connector, sub_expressions)"
  },
  {
    "code": "def _sparse_array_op(left, right, op, name):\\n    if np.isnan(left.fill_value):\\n        sparse_op = lambda a, b: _sparse_nanop(a, b, name)\\n    else:\\n        sparse_op = lambda a, b: _sparse_fillop(a, b, name)\\n    if left.sp_index.equals(right.sp_index):\\n        result = op(left.sp_values, right.sp_values)\\n        result_index = left.sp_index\\n    else:\\n        result, result_index = sparse_op(left, right)\\n    try:\\n        fill_value = op(left.fill_value, right.fill_value)\\n    except:\\n        fill_value = nan\\n    return SparseArray(result, sparse_index=result_index,\\n                       fill_value=fill_value)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _sparse_array_op(left, right, op, name):\\n    if np.isnan(left.fill_value):\\n        sparse_op = lambda a, b: _sparse_nanop(a, b, name)\\n    else:\\n        sparse_op = lambda a, b: _sparse_fillop(a, b, name)\\n    if left.sp_index.equals(right.sp_index):\\n        result = op(left.sp_values, right.sp_values)\\n        result_index = left.sp_index\\n    else:\\n        result, result_index = sparse_op(left, right)\\n    try:\\n        fill_value = op(left.fill_value, right.fill_value)\\n    except:\\n        fill_value = nan\\n    return SparseArray(result, sparse_index=result_index,\\n                       fill_value=fill_value)"
  },
  {
    "code": "def __enter__(self):\\n\\t\\tself._template_dict = self._get_template_data(self._template_file)\\n\\t\\tself._function_provider = SamFunctionProvider(self._template_dict, self.parameter_overrides)\\n\\t\\tself._env_vars_value = self._get_env_vars_value(self._env_vars_file)\\n\\t\\tself._log_file_handle = self._setup_log_file(self._log_file)\\n\\t\\tself._debug_context = self._get_debug_context(self._debug_ports, self._debug_args, self._debugger_path)\\n\\t\\tself._container_manager = self._get_container_manager(self._docker_network, self._skip_pull_image)\\n\\t\\tif not self._container_manager.is_docker_reachable:\\n\\t\\t\\traise InvokeContextException(\"Running AWS SAM projects locally requires Docker. Have you got it installed?\")\\n\\t\\treturn self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fix #1392: Catch pywintypes.error when determining if docker is reachable  (#2215)",
    "fixed_code": "def __enter__(self):\\n\\t\\tself._template_dict = self._get_template_data(self._template_file)\\n\\t\\tself._function_provider = SamFunctionProvider(self._template_dict, self.parameter_overrides)\\n\\t\\tself._env_vars_value = self._get_env_vars_value(self._env_vars_file)\\n\\t\\tself._log_file_handle = self._setup_log_file(self._log_file)\\n\\t\\tself._debug_context = self._get_debug_context(self._debug_ports, self._debug_args, self._debugger_path)\\n\\t\\tself._container_manager = self._get_container_manager(self._docker_network, self._skip_pull_image)\\n\\t\\tif not self._container_manager.is_docker_reachable:\\n\\t\\t\\traise InvokeContextException(\\n\\t\\t\\t\\t\"Running AWS SAM projects locally requires Docker. Have you got it installed and running?\"\\n\\t\\t\\t)\\n\\t\\treturn self"
  },
  {
    "code": "def _set_index(self, index):\\n        if not isinstance(index, _INDEX_TYPES):\\n            raise TypeError(\"Expected index to be in %s; was %s.\"\\n                            % (_INDEX_TYPES, type(index)))\\n        if len(self) != len(index):\\n            raise AssertionError('Lengths of index and values did not match!')\\n        self._index = _ensure_index(index)\\n    index = property(fget=_get_index, fset=_set_index)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _set_index(self, index):\\n        if not isinstance(index, _INDEX_TYPES):\\n            raise TypeError(\"Expected index to be in %s; was %s.\"\\n                            % (_INDEX_TYPES, type(index)))\\n        if len(self) != len(index):\\n            raise AssertionError('Lengths of index and values did not match!')\\n        self._index = _ensure_index(index)\\n    index = property(fget=_get_index, fset=_set_index)"
  },
  {
    "code": "def int_list_validator(sep=',', message=None, code='invalid', allow_negative=False):\\n\\tregexp = _lazy_re_compile('^%(neg)s\\d+(?:%(sep)s%(neg)s\\d+)*\\Z' % {\\n\\t\\t'neg': '(-)?' if allow_negative else '',\\n\\t\\t'sep': re.escape(sep),\\n\\t})\\n\\treturn RegexValidator(regexp, message=message, code=code)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def int_list_validator(sep=',', message=None, code='invalid', allow_negative=False):\\n\\tregexp = _lazy_re_compile('^%(neg)s\\d+(?:%(sep)s%(neg)s\\d+)*\\Z' % {\\n\\t\\t'neg': '(-)?' if allow_negative else '',\\n\\t\\t'sep': re.escape(sep),\\n\\t})\\n\\treturn RegexValidator(regexp, message=message, code=code)"
  },
  {
    "code": "def _dual_gap(emp_cov, precision_, alpha):\\n    gap = np.sum(emp_cov * precision_)\\n    gap -= precision_.shape[0]\\n    gap += alpha*np.abs(precision_).sum()\\n    return gap",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH Glasso: don't penalize the diagonal",
    "fixed_code": "def _dual_gap(emp_cov, precision_, alpha):\\n    gap = np.sum(emp_cov * precision_)\\n    gap -= precision_.shape[0]\\n    gap += alpha*(np.abs(precision_).sum()\\n                        - np.abs(np.diag(precision_)).sum())\\n    return gap"
  },
  {
    "code": "def load_pkcs12(buffer, passphrase=None):\\n\\tpassphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\\n\\tif isinstance(buffer, _text_type):\\n\\t\\tbuffer = buffer.encode(\"ascii\")\\n\\tbio = _new_mem_buf(buffer)\\n\\tif not passphrase:\\n\\t\\tpassphrase = _ffi.NULL\\n\\tp12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\\n\\tif p12 == _ffi.NULL:\\n\\t\\t_raise_current_error()\\n\\tp12 = _ffi.gc(p12, _lib.PKCS12_free)\\n\\tpkey = _ffi.new(\"EVP_PKEY**\")\\n\\tcert = _ffi.new(\"X509**\")\\n\\tcacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\\n\\tparse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\\n\\tif not parse_result:\\n\\t\\t_raise_current_error()\\n\\tcacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\\n\\ttry:\\n\\t\\t_raise_current_error()\\n\\texcept Error:\\n\\t\\tpass\\n\\tif pkey[0] == _ffi.NULL:\\n\\t\\tpykey = None\\n\\telse:\\n\\t\\tpykey = PKey.__new__(PKey)\\n\\t\\tpykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\\n\\tif cert[0] == _ffi.NULL:\\n\\t\\tpycert = None\\n\\t\\tfriendlyname = None\\n\\telse:\\n\\t\\tpycert = X509._from_raw_x509_ptr(cert[0])\\n\\t\\tfriendlyname_length = _ffi.new(\"int*\")\\n\\t\\tfriendlyname_buffer = _lib.X509_alias_get0(\\n\\t\\t\\tcert[0], friendlyname_length\\n\\t\\t)\\n\\t\\tfriendlyname = _ffi.buffer(\\n\\t\\t\\tfriendlyname_buffer, friendlyname_length[0]\\n\\t\\t)[:]\\n\\t\\tif friendlyname_buffer == _ffi.NULL:\\n\\t\\t\\tfriendlyname = None\\n\\tpycacerts = []\\n\\tfor i in range(_lib.sk_X509_num(cacerts)):\\n\\t\\tx509 = _lib.sk_X509_value(cacerts, i)\\n\\t\\tpycacert = X509._from_raw_x509_ptr(x509)\\n\\t\\tpycacerts.append(pycacert)\\n\\tif not pycacerts:\\n\\t\\tpycacerts = None\\n\\tpkcs12 = PKCS12.__new__(PKCS12)\\n\\tpkcs12._pkey = pykey\\n\\tpkcs12._cert = pycert\\n\\tpkcs12._cacerts = pycacerts\\n\\tpkcs12._friendlyname = friendlyname\\n\\treturn pkcs12",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def load_pkcs12(buffer, passphrase=None):\\n\\tpassphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\\n\\tif isinstance(buffer, _text_type):\\n\\t\\tbuffer = buffer.encode(\"ascii\")\\n\\tbio = _new_mem_buf(buffer)\\n\\tif not passphrase:\\n\\t\\tpassphrase = _ffi.NULL\\n\\tp12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\\n\\tif p12 == _ffi.NULL:\\n\\t\\t_raise_current_error()\\n\\tp12 = _ffi.gc(p12, _lib.PKCS12_free)\\n\\tpkey = _ffi.new(\"EVP_PKEY**\")\\n\\tcert = _ffi.new(\"X509**\")\\n\\tcacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\\n\\tparse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\\n\\tif not parse_result:\\n\\t\\t_raise_current_error()\\n\\tcacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\\n\\ttry:\\n\\t\\t_raise_current_error()\\n\\texcept Error:\\n\\t\\tpass\\n\\tif pkey[0] == _ffi.NULL:\\n\\t\\tpykey = None\\n\\telse:\\n\\t\\tpykey = PKey.__new__(PKey)\\n\\t\\tpykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\\n\\tif cert[0] == _ffi.NULL:\\n\\t\\tpycert = None\\n\\t\\tfriendlyname = None\\n\\telse:\\n\\t\\tpycert = X509._from_raw_x509_ptr(cert[0])\\n\\t\\tfriendlyname_length = _ffi.new(\"int*\")\\n\\t\\tfriendlyname_buffer = _lib.X509_alias_get0(\\n\\t\\t\\tcert[0], friendlyname_length\\n\\t\\t)\\n\\t\\tfriendlyname = _ffi.buffer(\\n\\t\\t\\tfriendlyname_buffer, friendlyname_length[0]\\n\\t\\t)[:]\\n\\t\\tif friendlyname_buffer == _ffi.NULL:\\n\\t\\t\\tfriendlyname = None\\n\\tpycacerts = []\\n\\tfor i in range(_lib.sk_X509_num(cacerts)):\\n\\t\\tx509 = _lib.sk_X509_value(cacerts, i)\\n\\t\\tpycacert = X509._from_raw_x509_ptr(x509)\\n\\t\\tpycacerts.append(pycacert)\\n\\tif not pycacerts:\\n\\t\\tpycacerts = None\\n\\tpkcs12 = PKCS12.__new__(PKCS12)\\n\\tpkcs12._pkey = pykey\\n\\tpkcs12._cert = pycert\\n\\tpkcs12._cacerts = pycacerts\\n\\tpkcs12._friendlyname = friendlyname\\n\\treturn pkcs12"
  },
  {
    "code": "def help_text_for_field(name, model):\\n    help_text = \"\"\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):\\n        pass\\n    else:\\n        if hasattr(field, 'help_text'):\\n            help_text = field.help_text\\n    return smart_text(help_text)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def help_text_for_field(name, model):\\n    help_text = \"\"\\n    try:\\n        field = _get_non_gfk_field(model._meta, name)\\n    except (FieldDoesNotExist, FieldIsAForeignKeyColumnName):\\n        pass\\n    else:\\n        if hasattr(field, 'help_text'):\\n            help_text = field.help_text\\n    return smart_text(help_text)"
  },
  {
    "code": "def _aggregate_item_by_item(self, func, *args, **kwargs):\\n        obj = self._obj_with_exclusions\\n        result = {}\\n        cannot_agg = []\\n        for item in obj:\\n            try:\\n                colg = SeriesGroupBy(obj[item], column=item,\\n                                     grouper=self.grouper)\\n                result[item] = colg.aggregate(func, *args, **kwargs)\\n            except (ValueError, TypeError):\\n                cannot_agg.append(item)\\n                continue\\n        result_columns = obj.columns\\n        if cannot_agg:\\n            result_columns = result_columns.drop(cannot_agg)\\n        return DataFrame(result, columns=result_columns)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _aggregate_item_by_item(self, func, *args, **kwargs):\\n        obj = self._obj_with_exclusions\\n        result = {}\\n        cannot_agg = []\\n        for item in obj:\\n            try:\\n                colg = SeriesGroupBy(obj[item], column=item,\\n                                     grouper=self.grouper)\\n                result[item] = colg.aggregate(func, *args, **kwargs)\\n            except (ValueError, TypeError):\\n                cannot_agg.append(item)\\n                continue\\n        result_columns = obj.columns\\n        if cannot_agg:\\n            result_columns = result_columns.drop(cannot_agg)\\n        return DataFrame(result, columns=result_columns)"
  },
  {
    "code": "def _fit_full(self, X, n_components):\\n        n_samples, n_features = X.shape\\n        if n_components == \"mle\":\\n            if n_samples < n_features:\\n                raise ValueError(\\n                    \"n_components='mle' is only supported if n_samples >= n_features\"\\n                )\\n        elif not 0 <= n_components <= min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be between 0 and \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='full'\" % (n_components, min(n_samples, n_features))\\n            )\\n        elif n_components >= 1:\\n            if not isinstance(n_components, numbers.Integral):\\n                raise ValueError(\\n                    \"n_components=%r must be of type int \"\\n                    \"when greater than or equal to 1, \"\\n                    \"was of type=%r\" % (n_components, type(n_components))\\n                )\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        U, S, Vt = linalg.svd(X, full_matrices=False)\\n        U, Vt = svd_flip(U, Vt)\\n        components_ = Vt\\n        explained_variance_ = (S**2) / (n_samples - 1)\\n        total_var = explained_variance_.sum()\\n        explained_variance_ratio_ = explained_variance_ / total_var\\n        singular_values_ = S.copy()  \\n        if n_components == \"mle\":\\n            n_components = _infer_dimension(explained_variance_, n_samples)\\n        elif 0 < n_components < 1.0:\\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\\n            n_components = np.searchsorted(ratio_cumsum, n_components, side=\"right\") + 1\\n        if n_components < min(n_features, n_samples):\\n            self.noise_variance_ = explained_variance_[n_components:].mean()\\n        else:\\n            self.noise_variance_ = 0.0\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = components_[:n_components]\\n        self.n_components_ = n_components\\n        self.explained_variance_ = explained_variance_[:n_components]\\n        self.explained_variance_ratio_ = explained_variance_ratio_[:n_components]\\n        self.singular_values_ = singular_values_[:n_components]\\n        return U, S, Vt",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "MAINT Use _validate_params in PCA  (#23485)",
    "fixed_code": "def _fit_full(self, X, n_components):\\n        n_samples, n_features = X.shape\\n        if n_components == \"mle\":\\n            if n_samples < n_features:\\n                raise ValueError(\\n                    \"n_components='mle' is only supported if n_samples >= n_features\"\\n                )\\n        elif not 0 <= n_components <= min(n_samples, n_features):\\n            raise ValueError(\\n                \"n_components=%r must be between 0 and \"\\n                \"min(n_samples, n_features)=%r with \"\\n                \"svd_solver='full'\" % (n_components, min(n_samples, n_features))\\n            )\\n        self.mean_ = np.mean(X, axis=0)\\n        X -= self.mean_\\n        U, S, Vt = linalg.svd(X, full_matrices=False)\\n        U, Vt = svd_flip(U, Vt)\\n        components_ = Vt\\n        explained_variance_ = (S**2) / (n_samples - 1)\\n        total_var = explained_variance_.sum()\\n        explained_variance_ratio_ = explained_variance_ / total_var\\n        singular_values_ = S.copy()  \\n        if n_components == \"mle\":\\n            n_components = _infer_dimension(explained_variance_, n_samples)\\n        elif 0 < n_components < 1.0:\\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\\n            n_components = np.searchsorted(ratio_cumsum, n_components, side=\"right\") + 1\\n        if n_components < min(n_features, n_samples):\\n            self.noise_variance_ = explained_variance_[n_components:].mean()\\n        else:\\n            self.noise_variance_ = 0.0\\n        self.n_samples_, self.n_features_ = n_samples, n_features\\n        self.components_ = components_[:n_components]\\n        self.n_components_ = n_components\\n        self.explained_variance_ = explained_variance_[:n_components]\\n        self.explained_variance_ratio_ = explained_variance_ratio_[:n_components]\\n        self.singular_values_ = singular_values_[:n_components]\\n        return U, S, Vt"
  },
  {
    "code": "def get_data_google(symbols=None, start=None, end=None, retry_count=3, pause=0,\\n                   chunksize=25, **kwargs):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_data_google(symbols=None, start=None, end=None, retry_count=3, pause=0,\\n                   chunksize=25, **kwargs):"
  },
  {
    "code": "def _maybe_cache(arg, format, cache, convert_listlike):\\n    from pandas import Series\\n    cache_array = Series()\\n    if cache:\\n        from pandas import Index\\n        if not Index(arg).is_unique:\\n            unique_dates = algorithms.unique(arg)\\n            cache_dates = convert_listlike(unique_dates, True, format)\\n            cache_array = Series(cache_dates, index=unique_dates)\\n    return cache_array",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: pd.to_datetime() throws if caching is on with Null-like arguments (#26078)",
    "fixed_code": "def _maybe_cache(arg, format, cache, convert_listlike):\\n    from pandas import Series\\n    cache_array = Series()\\n    if cache:\\n        from pandas import Index\\n        unique_dates = Index(arg).unique()\\n        if len(unique_dates) < len(arg):\\n            cache_dates = convert_listlike(unique_dates.to_numpy(),\\n                                           True, format)\\n            cache_array = Series(cache_dates, index=unique_dates)\\n    return cache_array"
  },
  {
    "code": "def predict(self, X, **params):\\n        X = np.atleast_2d(X)\\n        self._set_params(**params)\\n        if self.ball_tree is None:\\n            if self.algorithm == 'brute_inplace':\\n                neigh_ind = knn_brute(self._fit_X, X, self.n_neighbors)\\n            else:\\n                from .metrics import euclidean_distances\\n                dist = euclidean_distances(\\n                    X, self._fit_X, squared=True)\\n                neigh_ind = dist.argsort(axis=1)[:, :self.n_neighbors]\\n        else:\\n            neigh_ind = self.ball_tree.query(\\n                X, self.n_neighbors, return_distance=False)\\n        pred_labels = self._y[neigh_ind]\\n        from scipy import stats\\n        mode, _ = stats.mode(pred_labels, axis=1)\\n        return mode.flatten().astype(np.int)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: fix kneighbors method in high dim",
    "fixed_code": "def predict(self, X, **params):\\n        X = np.atleast_2d(X)\\n        self._set_params(**params)\\n        neigh_ind = self.kneighbors(X, return_distance=False)\\n        pred_labels = self._y[neigh_ind]\\n        from scipy import stats\\n        mode, _ = stats.mode(pred_labels, axis=1)\\n        return mode.flatten().astype(np.int)"
  },
  {
    "code": "def get_supported_language_variant(lang_code, strict=False):\\n    if lang_code == settings.LANGUAGE_CODE:\\n        return lang_code\\n    else:\\n        raise LookupError(lang_code)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Fixed #23689 -- Made parsing HTTP Accept-Language header case-insensitive.\\n\\nThank you Daniel Samuels for test project.",
    "fixed_code": "def get_supported_language_variant(lang_code, strict=False):\\n    if lang_code and lang_code.lower() == settings.LANGUAGE_CODE.lower():\\n        return lang_code\\n    else:\\n        raise LookupError(lang_code)"
  },
  {
    "code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            state=dict(type='str', default='present',\\n                       choices=['absent', 'active', 'deleted', 'present', 'restarted', 'started', 'stopped']),\\n            api_key=dict(type='str', no_log=True),\\n            name=dict(type='str', required=True),\\n            alert_bwin_enabled=dict(type='bool'),\\n            alert_bwin_threshold=dict(type='int'),\\n            alert_bwout_enabled=dict(type='bool'),\\n            alert_bwout_threshold=dict(type='int'),\\n            alert_bwquota_enabled=dict(type='bool'),\\n            alert_bwquota_threshold=dict(type='int'),\\n            alert_cpu_enabled=dict(type='bool'),\\n            alert_cpu_threshold=dict(type='int'),\\n            alert_diskio_enabled=dict(type='bool'),\\n            alert_diskio_threshold=dict(type='int'),\\n            backupsenabled=dict(type='int'),\\n            backupweeklyday=dict(type='int'),\\n            backupwindow=dict(type='int'),\\n            displaygroup=dict(type='str', default=''),\\n            plan=dict(type='int'),\\n            additional_disks=dict(type='list'),\\n            distribution=dict(type='int'),\\n            datacenter=dict(type='int'),\\n            kernel_id=dict(type='int'),\\n            linode_id=dict(type='int', aliases=['lid']),\\n            payment_term=dict(type='int', default=1, choices=[1, 12, 24]),\\n            password=dict(type='str', no_log=True),\\n            private_ip=dict(type='bool'),\\n            ssh_pub_key=dict(type='str'),\\n            swap=dict(type='int', default=512),\\n            wait=dict(type='bool', default=True),\\n            wait_timeout=dict(default=300),\\n            watchdog=dict(type='bool', default=True),\\n        ),\\n    )\\n    if not HAS_LINODE:\\n        module.fail_json(msg='linode-python required for this module')\\n    state = module.params.get('state')\\n    api_key = module.params.get('api_key')\\n    name = module.params.get('name')\\n    alert_bwin_enabled = module.params.get('alert_bwin_enabled')\\n    alert_bwin_threshold = module.params.get('alert_bwin_threshold')\\n    alert_bwout_enabled = module.params.get('alert_bwout_enabled')\\n    alert_bwout_threshold = module.params.get('alert_bwout_threshold')\\n    alert_bwquota_enabled = module.params.get('alert_bwquota_enabled')\\n    alert_bwquota_threshold = module.params.get('alert_bwquota_threshold')\\n    alert_cpu_enabled = module.params.get('alert_cpu_enabled')\\n    alert_cpu_threshold = module.params.get('alert_cpu_threshold')\\n    alert_diskio_enabled = module.params.get('alert_diskio_enabled')\\n    alert_diskio_threshold = module.params.get('alert_diskio_threshold')\\n    backupsenabled = module.params.get('backupsenabled')\\n    backupweeklyday = module.params.get('backupweeklyday')\\n    backupwindow = module.params.get('backupwindow')\\n    displaygroup = module.params.get('displaygroup')\\n    plan = module.params.get('plan')\\n    additional_disks = module.params.get('additional_disks')\\n    distribution = module.params.get('distribution')\\n    datacenter = module.params.get('datacenter')\\n    kernel_id = module.params.get('kernel_id')\\n    linode_id = module.params.get('linode_id')\\n    payment_term = module.params.get('payment_term')\\n    password = module.params.get('password')\\n    private_ip = module.params.get('private_ip')\\n    ssh_pub_key = module.params.get('ssh_pub_key')\\n    swap = module.params.get('swap')\\n    wait = module.params.get('wait')\\n    wait_timeout = int(module.params.get('wait_timeout'))\\n    watchdog = int(module.params.get('watchdog'))\\n    kwargs = dict()\\n    check_items = dict(\\n        alert_bwin_enabled=alert_bwin_enabled,\\n        alert_bwin_threshold=alert_bwin_threshold,\\n        alert_bwout_enabled=alert_bwout_enabled,\\n        alert_bwout_threshold=alert_bwout_threshold,\\n        alert_bwquota_enabled=alert_bwquota_enabled,\\n        alert_bwquota_threshold=alert_bwquota_threshold,\\n        alert_cpu_enabled=alert_cpu_enabled,\\n        alert_cpu_threshold=alert_cpu_threshold,\\n        alert_diskio_enabled=alert_diskio_enabled,\\n        alert_diskio_threshold=alert_diskio_threshold,\\n        backupweeklyday=backupweeklyday,\\n        backupwindow=backupwindow,\\n    )\\n    for key, value in check_items.items():\\n        if value is not None:\\n            kwargs[key] = value\\n    if not api_key:\\n        try:\\n            api_key = os.environ['LINODE_API_KEY']\\n        except KeyError as e:\\n            module.fail_json(msg='Unable to load %s' % e.message)\\n    try:\\n        api = linode_api.Api(api_key)\\n        api.test_echo()\\n    except Exception as e:\\n        module.fail_json(msg='%s' % e.value[0]['ERRORMESSAGE'])\\n    linodeServers(module, api, state, name,\\n                  displaygroup, plan,\\n                  additional_disks, distribution, datacenter, kernel_id, linode_id,\\n                  payment_term, password, private_ip, ssh_pub_key, swap, wait,\\n                  wait_timeout, watchdog, **kwargs)",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            state=dict(type='str', default='present',\\n                       choices=['absent', 'active', 'deleted', 'present', 'restarted', 'started', 'stopped']),\\n            api_key=dict(type='str', no_log=True),\\n            name=dict(type='str', required=True),\\n            alert_bwin_enabled=dict(type='bool'),\\n            alert_bwin_threshold=dict(type='int'),\\n            alert_bwout_enabled=dict(type='bool'),\\n            alert_bwout_threshold=dict(type='int'),\\n            alert_bwquota_enabled=dict(type='bool'),\\n            alert_bwquota_threshold=dict(type='int'),\\n            alert_cpu_enabled=dict(type='bool'),\\n            alert_cpu_threshold=dict(type='int'),\\n            alert_diskio_enabled=dict(type='bool'),\\n            alert_diskio_threshold=dict(type='int'),\\n            backupsenabled=dict(type='int'),\\n            backupweeklyday=dict(type='int'),\\n            backupwindow=dict(type='int'),\\n            displaygroup=dict(type='str', default=''),\\n            plan=dict(type='int'),\\n            additional_disks=dict(type='list'),\\n            distribution=dict(type='int'),\\n            datacenter=dict(type='int'),\\n            kernel_id=dict(type='int'),\\n            linode_id=dict(type='int', aliases=['lid']),\\n            payment_term=dict(type='int', default=1, choices=[1, 12, 24]),\\n            password=dict(type='str', no_log=True),\\n            private_ip=dict(type='bool'),\\n            ssh_pub_key=dict(type='str'),\\n            swap=dict(type='int', default=512),\\n            wait=dict(type='bool', default=True),\\n            wait_timeout=dict(default=300),\\n            watchdog=dict(type='bool', default=True),\\n        ),\\n    )\\n    if not HAS_LINODE:\\n        module.fail_json(msg='linode-python required for this module')\\n    state = module.params.get('state')\\n    api_key = module.params.get('api_key')\\n    name = module.params.get('name')\\n    alert_bwin_enabled = module.params.get('alert_bwin_enabled')\\n    alert_bwin_threshold = module.params.get('alert_bwin_threshold')\\n    alert_bwout_enabled = module.params.get('alert_bwout_enabled')\\n    alert_bwout_threshold = module.params.get('alert_bwout_threshold')\\n    alert_bwquota_enabled = module.params.get('alert_bwquota_enabled')\\n    alert_bwquota_threshold = module.params.get('alert_bwquota_threshold')\\n    alert_cpu_enabled = module.params.get('alert_cpu_enabled')\\n    alert_cpu_threshold = module.params.get('alert_cpu_threshold')\\n    alert_diskio_enabled = module.params.get('alert_diskio_enabled')\\n    alert_diskio_threshold = module.params.get('alert_diskio_threshold')\\n    backupsenabled = module.params.get('backupsenabled')\\n    backupweeklyday = module.params.get('backupweeklyday')\\n    backupwindow = module.params.get('backupwindow')\\n    displaygroup = module.params.get('displaygroup')\\n    plan = module.params.get('plan')\\n    additional_disks = module.params.get('additional_disks')\\n    distribution = module.params.get('distribution')\\n    datacenter = module.params.get('datacenter')\\n    kernel_id = module.params.get('kernel_id')\\n    linode_id = module.params.get('linode_id')\\n    payment_term = module.params.get('payment_term')\\n    password = module.params.get('password')\\n    private_ip = module.params.get('private_ip')\\n    ssh_pub_key = module.params.get('ssh_pub_key')\\n    swap = module.params.get('swap')\\n    wait = module.params.get('wait')\\n    wait_timeout = int(module.params.get('wait_timeout'))\\n    watchdog = int(module.params.get('watchdog'))\\n    kwargs = dict()\\n    check_items = dict(\\n        alert_bwin_enabled=alert_bwin_enabled,\\n        alert_bwin_threshold=alert_bwin_threshold,\\n        alert_bwout_enabled=alert_bwout_enabled,\\n        alert_bwout_threshold=alert_bwout_threshold,\\n        alert_bwquota_enabled=alert_bwquota_enabled,\\n        alert_bwquota_threshold=alert_bwquota_threshold,\\n        alert_cpu_enabled=alert_cpu_enabled,\\n        alert_cpu_threshold=alert_cpu_threshold,\\n        alert_diskio_enabled=alert_diskio_enabled,\\n        alert_diskio_threshold=alert_diskio_threshold,\\n        backupweeklyday=backupweeklyday,\\n        backupwindow=backupwindow,\\n    )\\n    for key, value in check_items.items():\\n        if value is not None:\\n            kwargs[key] = value\\n    if not api_key:\\n        try:\\n            api_key = os.environ['LINODE_API_KEY']\\n        except KeyError as e:\\n            module.fail_json(msg='Unable to load %s' % e.message)\\n    try:\\n        api = linode_api.Api(api_key)\\n        api.test_echo()\\n    except Exception as e:\\n        module.fail_json(msg='%s' % e.value[0]['ERRORMESSAGE'])\\n    linodeServers(module, api, state, name,\\n                  displaygroup, plan,\\n                  additional_disks, distribution, datacenter, kernel_id, linode_id,\\n                  payment_term, password, private_ip, ssh_pub_key, swap, wait,\\n                  wait_timeout, watchdog, **kwargs)"
  },
  {
    "code": "def _init_dict(self, data, index, columns, dtype):\\n        if columns is not None:\\n            if not isinstance(columns, Index):\\n                columns = Index(columns)\\n            data = dict((k, v) for k, v in data.iteritems() if k in columns)\\n        else:\\n            columns = Index(try_sort(data.keys()))\\n        index = extract_index(data, index)\\n        sdict = {}\\n        for k, v in data.iteritems():\\n            if isinstance(v, Series):\\n                sdict[k] = v.reindex(index)\\n            else:\\n                if isinstance(v, dict):\\n                    v = [v.get(i, NaN) for i in index]\\n                try:\\n                    v = Series(v, dtype=dtype, index=index)\\n                except Exception:\\n                    v = Series(v, index=index)\\n                sdict[k] = v.copy()\\n        for c in columns:\\n            if c not in sdict:\\n                sdict[c] = Series(np.NaN, index=index)\\n        return sdict, columns, index",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "working on SparseWidePanel. reduced some code dup in core classes",
    "fixed_code": "def _init_dict(self, data, index, columns, dtype):\\n        if columns is not None:\\n            columns = _ensure_index(columns)\\n            data = dict((k, v) for k, v in data.iteritems() if k in columns)\\n        else:\\n            columns = Index(try_sort(data.keys()))\\n        index = extract_index(data, index)\\n        sdict = {}\\n        for k, v in data.iteritems():\\n            if isinstance(v, Series):\\n                sdict[k] = v.reindex(index)\\n            else:\\n                if isinstance(v, dict):\\n                    v = [v.get(i, NaN) for i in index]\\n                try:\\n                    v = Series(v, dtype=dtype, index=index)\\n                except Exception:\\n                    v = Series(v, index=index)\\n                sdict[k] = v.copy()\\n        for c in columns:\\n            if c not in sdict:\\n                sdict[c] = Series(np.NaN, index=index)\\n        return sdict, columns, index"
  },
  {
    "code": "def _from_components(self, tensor_list):\\n\\tret = TensorArray(\\n\\t\\tdtype=self._dtype,\\n\\t\\tflow=tensor_list[0],\\n\\t\\tdynamic_size=self._dynamic_size,\\n\\t\\tinfer_shape=self._infer_shape)\\n\\tret._element_shape = [self._element_shape]  \\n\\treturn ret",
    "label": 1,
    "bug_type": "semantic",
    "bug_description": "Preserve element shape across TensorArray component serde\\n\\nThis change addresses the problem raised in #30685, where passing a TensorArray out of a tf.reduce loop would cause it to lose its inferred shape.\\n\\nThe issue was that when restoring the TensorArray with _from_components, we would set the _element_shape of the TensorArray wrapper class, but this field is never used. We need to set the _element_shape of the wrapped TenseorArray implementation, either _GraphTensorArray, _GraphTensorArrayV2, or _EagerTensorArray.",
    "fixed_code": "def _from_components(self, tensor_list):\\n\\tret = TensorArray(\\n\\t\\tdtype=self._dtype,\\n\\t\\tflow=tensor_list[0],\\n\\t\\tdynamic_size=self._dynamic_size,\\n\\t\\tinfer_shape=self._infer_shape)\\n\\tret._implementation._element_shape = [self._element_shape]  \\n\\treturn ret"
  },
  {
    "code": "def from_blocks(cls, blocks, index):\\n        items = _union_block_items(blocks)\\n        return BlockManager(blocks, [items, index])",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def from_blocks(cls, blocks, index):\\n        items = _union_block_items(blocks)\\n        return BlockManager(blocks, [items, index])"
  },
  {
    "code": "def __init__(self, body, aws_conn_id='aws_default'):\\n        self.body = body\\n        self.aws_conn_id = aws_conn_id",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "[AIRFLOW-5104] Set default schedule for GCP Transfer operators (#5726)\\n\\nThe GCS Transfer Service REST API requires that a schedule be set, even for\\none-time immediate runs. This adds code to\\n`S3ToGoogleCloudStorageTransferOperator` and\\n`GoogleCloudStorageToGoogleCloudStorageTransferOperator` to set a default\\none-time immediate run schedule when no `schedule` argument is passed.",
    "fixed_code": "def __init__(self, body, aws_conn_id='aws_default', default_schedule=False):\\n        self.body = body\\n        self.aws_conn_id = aws_conn_id\\n        self.default_schedule = default_schedule"
  },
  {
    "code": "def get_user(self, uidb64):\\n\\t\\ttry:\\n\\t\\t\\tuid = urlsafe_base64_decode(uidb64).decode()\\n\\t\\t\\tuser = UserModel._default_manager.get(pk=uid)\\n\\t\\texcept (TypeError, ValueError, OverflowError, UserModel.DoesNotExist, ValidationError):\\n\\t\\t\\tuser = None\\n\\t\\treturn user",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def get_user(self, uidb64):\\n\\t\\ttry:\\n\\t\\t\\tuid = urlsafe_base64_decode(uidb64).decode()\\n\\t\\t\\tuser = UserModel._default_manager.get(pk=uid)\\n\\t\\texcept (TypeError, ValueError, OverflowError, UserModel.DoesNotExist, ValidationError):\\n\\t\\t\\tuser = None\\n\\t\\treturn user"
  },
  {
    "code": "def replace_metacharacters(pattern):\\n    return pattern.replace('^', '').replace('$', '').replace('?', '')",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "Refs #32499 -- Fixed escaped metacharacters in simplify_regex().",
    "fixed_code": "def replace_metacharacters(pattern):\\n    return re.sub(\\n        r'((?:^|(?<!\\\\))(?:\\\\\\\\)*)(\\\\?)([?^$])',\\n        lambda m: m[1] + m[3] if m[2] else m[1],\\n        pattern,\\n    )"
  },
  {
    "code": "def test_invalid_keys(self):",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def test_invalid_keys(self):"
  },
  {
    "code": "def read_csv(filepath_or_buffer,\\n             sep=',',\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             verbose=False,\\n             delimiter=None,\\n             encoding=None,\\n             squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_table_doc)",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: add dialect keyword to parsers to expose quoting specifications #1363",
    "fixed_code": "def read_csv(filepath_or_buffer,\\n             sep=',',\\n             dialect=None,\\n             header=0,\\n             index_col=None,\\n             names=None,\\n             skiprows=None,\\n             na_values=None,\\n             thousands=None,\\n             comment=None,\\n             parse_dates=False,\\n             keep_date_col=False,\\n             dayfirst=False,\\n             date_parser=None,\\n             nrows=None,\\n             iterator=False,\\n             chunksize=None,\\n             skip_footer=0,\\n             converters=None,\\n             verbose=False,\\n             delimiter=None,\\n             encoding=None,\\n             squeeze=False):\\n    kwds = locals()\\n    sep = kwds.pop('sep')\\n    if kwds.get('delimiter', None) is None:\\n        kwds['delimiter'] = sep\\n    return _read(TextParser, filepath_or_buffer, kwds)\\n@Appender(_read_table_doc)"
  },
  {
    "code": "def _check_subparams(self, n_samples, n_features):\\n        if self.fit_intercept:\\n            n_dim = n_features + 1\\n        else:\\n            n_dim = n_features\\n        n_subsamples = self.n_subsamples\\n        if n_subsamples is not None:\\n            if n_subsamples > n_samples:\\n                raise ValueError(\"Invalid parameter since n_subsamples > \"\\n                                 \"n_samples ({0} > {1}).\".format(n_subsamples,\\n                                                                 n_samples))\\n            if n_samples >= n_features:\\n                if n_dim > n_subsamples:\\n                    plus_1 = \"+1\" if self.fit_intercept else \"\"\\n                    raise ValueError(\"Invalid parameter since n_features{0} \"\\n                                     \"> n_subsamples ({1} > {2}).\"\\n                                     \"\".format(plus_1, n_dim, n_samples))\\n            else:  \\n                if n_subsamples != n_samples:\\n                    raise ValueError(\"Invalid parameter since n_subsamples != \"\\n                                     \"n_samples ({0} != {1}) while n_samples \"\\n                                     \"< n_features.\".format(n_subsamples,\\n                                                            n_samples))\\n        else:\\n            n_subsamples = min(n_dim, n_samples)\\n        if self.max_subpopulation <= 0:\\n            raise ValueError(\"Subpopulation must be positive.\")\\n        n_all = max(1, np.rint(binom(n_samples, n_subsamples)))\\n        n_subpop = int(min(self.max_subpopulation, n_all))\\n        return n_subsamples, n_subpop",
    "label": 0,
    "bug_type": "none",
    "bug_description": "",
    "fixed_code": "def _check_subparams(self, n_samples, n_features):\\n        if self.fit_intercept:\\n            n_dim = n_features + 1\\n        else:\\n            n_dim = n_features\\n        n_subsamples = self.n_subsamples\\n        if n_subsamples is not None:\\n            if n_subsamples > n_samples:\\n                raise ValueError(\"Invalid parameter since n_subsamples > \"\\n                                 \"n_samples ({0} > {1}).\".format(n_subsamples,\\n                                                                 n_samples))\\n            if n_samples >= n_features:\\n                if n_dim > n_subsamples:\\n                    plus_1 = \"+1\" if self.fit_intercept else \"\"\\n                    raise ValueError(\"Invalid parameter since n_features{0} \"\\n                                     \"> n_subsamples ({1} > {2}).\"\\n                                     \"\".format(plus_1, n_dim, n_samples))\\n            else:  \\n                if n_subsamples != n_samples:\\n                    raise ValueError(\"Invalid parameter since n_subsamples != \"\\n                                     \"n_samples ({0} != {1}) while n_samples \"\\n                                     \"< n_features.\".format(n_subsamples,\\n                                                            n_samples))\\n        else:\\n            n_subsamples = min(n_dim, n_samples)\\n        if self.max_subpopulation <= 0:\\n            raise ValueError(\"Subpopulation must be positive.\")\\n        n_all = max(1, np.rint(binom(n_samples, n_subsamples)))\\n        n_subpop = int(min(self.max_subpopulation, n_all))\\n        return n_subsamples, n_subpop"
  },
  {
    "code": "def radius_neighbors_graph(\\n        self, X=None, radius=None, mode=\"connectivity\", sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if radius is None:\\n            radius = self.radius\\n        if mode == \"connectivity\":\\n            A_ind = self.radius_neighbors(X, radius, return_distance=False)\\n            A_data = None\\n        elif mode == \"distance\":\\n            dist, A_ind = self.radius_neighbors(\\n                X, radius, return_distance=True, sort_results=sort_results\\n            )\\n            A_data = np.concatenate(list(dist))\\n        else:\\n            raise ValueError(\\n                'Unsupported mode, must be one of \"connectivity\", '\\n                'or \"distance\" but got %s instead' % mode\\n            )\\n        n_queries = A_ind.shape[0]\\n        n_samples_fit = self.n_samples_fit_\\n        n_neighbors = np.array([len(a) for a in A_ind])\\n        A_ind = np.concatenate(list(A_ind))\\n        if A_data is None:\\n            A_data = np.ones(len(A_ind))\\n        A_indptr = np.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))\\n        return csr_matrix((A_data, A_ind, A_indptr), shape=(n_queries, n_samples_fit))",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "DOC Ensures that NearestNeighbors passes numpydoc validation (#20446)",
    "fixed_code": "def radius_neighbors_graph(\\n        self, X=None, radius=None, mode=\"connectivity\", sort_results=False\\n    ):\\n        check_is_fitted(self)\\n        if radius is None:\\n            radius = self.radius\\n        if mode == \"connectivity\":\\n            A_ind = self.radius_neighbors(X, radius, return_distance=False)\\n            A_data = None\\n        elif mode == \"distance\":\\n            dist, A_ind = self.radius_neighbors(\\n                X, radius, return_distance=True, sort_results=sort_results\\n            )\\n            A_data = np.concatenate(list(dist))\\n        else:\\n            raise ValueError(\\n                'Unsupported mode, must be one of \"connectivity\", '\\n                'or \"distance\" but got %s instead' % mode\\n            )\\n        n_queries = A_ind.shape[0]\\n        n_samples_fit = self.n_samples_fit_\\n        n_neighbors = np.array([len(a) for a in A_ind])\\n        A_ind = np.concatenate(list(A_ind))\\n        if A_data is None:\\n            A_data = np.ones(len(A_ind))\\n        A_indptr = np.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))\\n        return csr_matrix((A_data, A_ind, A_indptr), shape=(n_queries, n_samples_fit))"
  },
  {
    "code": "def _setitem_copy(self, copy):\\n        self._is_copy = copy\\n        return self",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "API: change _is_copy to is_copy attribute on pandas objects GH(5650)",
    "fixed_code": "def _setitem_copy(self, copy):\\n        self.is_copy = copy\\n        return self"
  },
  {
    "code": "def _get_counts_nanvar(mask, axis, ddof):\\n    count = _get_counts(mask, axis)\\n    d = count-ddof\\n    if np.isscalar(count):\\n        if count <= ddof:\\n            count = np.nan\\n            d = np.nan\\n    else:\\n        mask2 = count <= ddof\\n        if mask2.any():\\n            np.putmask(d, mask2, np.nan)\\n            np.putmask(count, mask2, np.nan)\\n    return count, d",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "ENH: make sure return dtypes for nan funcs are consistent",
    "fixed_code": "def _get_counts_nanvar(mask, axis, ddof, dtype=float):\\n    dtype = _get_dtype(dtype)\\n    count = _get_counts(mask, axis, dtype=dtype)\\n    d = count - dtype.type(ddof)\\n    if np.isscalar(count):\\n        if count <= ddof:\\n            count = np.nan\\n            d = np.nan\\n    else:\\n        mask2 = count <= ddof\\n        if mask2.any():\\n            np.putmask(d, mask2, np.nan)\\n            np.putmask(count, mask2, np.nan)\\n    return count, d"
  },
  {
    "code": "def _concatenate_join_units(\\n    join_units: List[JoinUnit], concat_axis: int, copy: bool\\n) -> ArrayLike:\\n    if concat_axis == 0 and len(join_units) > 1:\\n        raise AssertionError(\"Concatenating join units along axis0\")\\n    empty_dtype, upcasted_na = _get_empty_dtype_and_na(join_units)\\n    to_concat = [\\n        ju.get_reindexed_values(empty_dtype=empty_dtype, upcasted_na=upcasted_na)\\n        for ju in join_units\\n    ]\\n    if len(to_concat) == 1:\\n        concat_values = to_concat[0]\\n        if copy:\\n            if isinstance(concat_values, np.ndarray):\\n                if concat_values.base is not None:\\n                    concat_values = concat_values.copy()\\n            else:\\n                concat_values = concat_values.copy()\\n    elif any(isinstance(t, ExtensionArray) for t in to_concat):\\n        to_concat = [t if isinstance(t, ExtensionArray) else t[0, :] for t in to_concat]\\n        concat_values = concat_compat(to_concat, axis=0)\\n        if not isinstance(concat_values, ExtensionArray) or (\\n            isinstance(concat_values, DatetimeArray) and concat_values.tz is None\\n        ):\\n            concat_values = np.atleast_2d(concat_values)\\n    else:\\n        concat_values = concat_compat(to_concat, axis=concat_axis)\\n    return concat_values",
    "label": 1,
    "bug_type": "unknown",
    "bug_description": "BUG: casting on concat with empties (#38907)",
    "fixed_code": "def _concatenate_join_units(\\n    join_units: List[JoinUnit], concat_axis: int, copy: bool\\n) -> ArrayLike:\\n    if concat_axis == 0 and len(join_units) > 1:\\n        raise AssertionError(\"Concatenating join units along axis0\")\\n    nonempties = [\\n        x for x in join_units if x.block is None or x.block.shape[concat_axis] > 0\\n    ]\\n    if nonempties:\\n        join_units = nonempties\\n    empty_dtype, upcasted_na = _get_empty_dtype_and_na(join_units)\\n    to_concat = [\\n        ju.get_reindexed_values(empty_dtype=empty_dtype, upcasted_na=upcasted_na)\\n        for ju in join_units\\n    ]\\n    if len(to_concat) == 1:\\n        concat_values = to_concat[0]\\n        if copy:\\n            if isinstance(concat_values, np.ndarray):\\n                if concat_values.base is not None:\\n                    concat_values = concat_values.copy()\\n            else:\\n                concat_values = concat_values.copy()\\n    elif any(isinstance(t, ExtensionArray) for t in to_concat):\\n        to_concat = [t if isinstance(t, ExtensionArray) else t[0, :] for t in to_concat]\\n        concat_values = concat_compat(to_concat, axis=0)\\n        if not isinstance(concat_values, ExtensionArray) or (\\n            isinstance(concat_values, DatetimeArray) and concat_values.tz is None\\n        ):\\n            concat_values = np.atleast_2d(concat_values)\\n    else:\\n        concat_values = concat_compat(to_concat, axis=concat_axis)\\n    return concat_values"
  }
]